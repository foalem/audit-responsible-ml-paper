title,body
fit error  help,"model = Sequential()

model.add(Conv2D(32, kernel_size=(5, 9),
                 activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 4)))

model.add(Conv2D(16, kernel_size=(5, 7), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 3)))

model.add(Flatten())

#num_classes*4 =104 

model.add(Dense(num_classes*4, activation='sigmoid'))


model.compile(loss=keras.losses.binary_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

ValueError: Error when checking target: expected dense_1 to have shape (104,) but got array with shape (26,)

why？ help me~~~~thks~"
验证码重叠,
add a font file and fix the writing style,
4-6个字符能用一个模型实现吗？,你好，你的模型我成功复制了，4个字符，6个字符单独测试都识别率很高，有没有办法一个模型既支持4个字符又支持6个字符。
write labels err,"    writer.writerows(labels)
TypeError: a bytes-like object is required, not 'str'"
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,按照你上面的代码运行发现报这个错误，查了下说明超出了范围，不大理解为什么连接相加的时候出错了，谢谢告知
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,"装好依赖，生成图片后，直接运行，报如下错误
picnum :  6000
6000 (20, 80)
6000 4
Traceback (most recent call last):
  File ""cnn_end2end_ocr.py"", line 76, in <module>
    c = np.concatenate((c0,c1,c2,c3),axis=1)
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1"
Only extract one word from gumbel softmax,"In the code https://github.com/yala/text_nn/blob/master/rationale_net/utils/learn.py#L71-L85

```python
def get_hard_mask(z, return_ind=False):
    '''
        -z: torch Tensor where each element probablity of element
        being selected
        -args: experiment level config
        returns: A torch variable that is binary mask of z >= .5
    '''
    max_z, ind = torch.max(z, dim=-1)
    if return_ind:
        del z
        return ind
    masked = torch.ge(z, max_z.unsqueeze(-1)).float()
    del z
    return masked

```

because we take the max, usually, only one position will have the max value. 
In this case, if we have 100 words in the sentence, we only select one word as the rationale?
I thought we should select independently and choose those words with >0.5 probability.

Maybe we should change
```python
masked = torch.ge(z, max_z.unsqueeze(-1)).float()
```
to 
```python
masked = torch.ge(z, 0.5).float()
```
instead?"
Multiple GPUs is broken,"Hi Yala! 

Great package. Just letting you know, though, that computation on multiple GPU's is broken for two reasons:

1. The `model.py` file does not import 
``import torch.nn as nn``
that's an easy fix.

2. You have some class-attribute dependencies that are single-thread bound.
https://github.com/pytorch/pytorch/issues/8637

I'm not sure exactly what they are, but here is my error message, which matches the one in the issue I linked to above:

```
Traceback (most recent call last):
  File ""scripts/main.py"", line 35, in <module>
    epoch_stats, model, gen = train.train_model(train_data, dev_data, model, gen, args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 59, in train_model
    args=args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 198, in run_epoch
    mask, z = gen(x_indx)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/_utils.py"", line 369, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker
    output = module(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/generator.py"", line 55, in forward
    activ = self.cnn(x)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 55, in forward
    activ = self._conv(x)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 41, in _conv
    next_activ.append( conv(padded_activ) )
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 200, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)```

Alex"
Gumbel softmax instead of Reinforce,"Hi,

Thank you for your code ! I have seen you wrote Gumbel instead of Reinforce but I don't understand exactly how Reinforce was implemented before as I don't see any average over Z samples. Here I see that Z is sampled only once https://github.com/yala/text_nn/blob/master/rationale_net/models/generator.py#L40 .

Did I miss something ?

Thank you very much for your help"
Is log_softmax function missing in the Line 39 of generator.py?,"I think log_softmax function should be added in Line 39 if we want to use the Gumbel Softmax.

`logits = F.log_softrmax(self.hidden(activ))`

If I'm wrong, please let me know. 
Thanks.

![image](https://user-images.githubusercontent.com/17995697/54492275-0d17c500-4900-11e9-8bee-addd81d35934.png)
"
Can this be applied to a regression problem?,Can this be applied to a regression problem?
Is it possible to share the pre-trained embedding for beer reviews? ,"From https://github.com/yala/text_nn/blob/master/rationale_net/utils/embedding.py#L38 , it's trying to find `review+wiki.filtered.200.txt.gz`, is it possible to share the pre-trained embedding for beer reviews? "
Tutorial,Added tutorial
gpu-trained on cpu fix,
Refactor,Add rationale export in results file and clean up code
FileNotFoundError: [Errno 2] No such file or directory: 'pickle_files/embeddings.p',"In dataset.py we read
    embedding_path = 'pickle_files/embeddings.p'
    word_to_indx_path = 'pickle_files/vocabIndxDict.p'
    embedding_tensor = pickle.load(open(embedding_path,'rb'))
    word_to_indx = pickle.load(open(word_to_indx_path,'rb'))
However, the two files are not in the zip-code?"
AttributeError: 'Namespace' object has no attribute 'use_as_tagger',"Running from command line:
`CUDA_VISIBLE_DEVICES=2 python -u scripts/main.py  --batch_size 64 --cuda --dataset full_beer --embedding 
glove --dropout 0.05 --weight_decay 5e-06 --num_layers 1 --model_form cnn --hidden_dim 100 --epochs 50 --init_lr 0.0001 --num_workers
 0 --objective cross_entropy --patience 5 --save_dir snapshot --train --test --results_path logs/adhoc_0.results  --gumbel_decay 1e-5 --get_rationales
 --aspect aroma --selection_lambda .005 --continuity_lambda .01`

Gives an error:

`
Traceback (most recent call last):
  File ""scripts/main.py"", line 30, in <module>
    gen, model = model_utils.get_model(args, embeddings, train_data)
  File ""/home/mglowacki/Desktop/RNN_yala_pytorch_2/rationale_net/utils/model.py"", line 12, in get_model
    if args.use_as_tagger == True:
AttributeError: 'Namespace' object has no attribute 'use_as_tagger'
`
I've checked sourcecode but there is no --use_as_tagger parameter in args.
Btw. in the same file `model.py` there is an additional problem because `nn` is not imported.

"
fixing 0-dim tensor (scalar) access,"New versions of PyTorch will require proper usage for accessing 0 dimensional Tensors:
`tensor.item()` instead of `tensor.data[0]`"
Tagging,
added confusion matrix info,
How can I be able to learn with your repository,"How can I be able to learn with your repository

Note: I am a complete Noobie in the world of C# Programming but with Knowledge in JavaScript"
HPO wiki page,"Small edit - in the code example on the wiki page for hyperparameter optimization, `transform.Logarithmic` should now be `transform.Log10`"
how can i train the Neural Network with my own Training Pictures?,"let's say i have a list of Images ..how do i convert my images into F64matrix Form so i can train them with my Neural Network ..and how do i test the Network with an Image at the End
could you please make an example of this because you didn't mention that in the examples, you also didn't mention how to test the Network using a real Image..

Thanks in Advance "
Access OOB data and OOB error calculations of Random Forest,"Hi
Can you add access to the Out-of-Bag data and/or Out-of-Data error calculations for Random Forests?

Love this project,
Thanks

"
A way to Save Bayesian Optimizer progress and continue later.,"Hello, 
I would like to use the Bayesian Optimizer for Hyperparameter tuning.
Is there a way to save the current status of the optimizer and then resume later. I could not find one... 
Also I could not figure out a way to pass a cancellation token.
Great project.
Thank you."
Code sharing,"Hi @mdabros,

For about 15 years, I was the main maintainer of a project called the Accord.NET Framework, which was mainly a machine learning/statistics processing framework for .NET. I have recently archived the project as I couldn't keep up updating it. If there is anything that you would ever find useful in Accord/its codebase, I just wanted to let you know that I have granted license to anyone who would like to, to reuse any piece of code I have written myself (as per noted in the headers of each file of the project) under the MIT or BSD licenses.

I still continue to receive requests on how to use/adapt existing features, even after the project has been archived, so I guess there are still useful things that have been implemented in that framework. If you would like to adapt any of those into SharpLearning, please let me know.

All the best and merry Christmas if you celebrate it!

Cesar

"
Exception when serializing neural net to XML,"Hello,

I am using SharpLearning.Neural version 0.31.8 in a .Net Core 3.1 project. After training a neural network and obtaining the predictor model, I try to serialize it to an XML file using the suggested procedure:

```csharp
GenericXmlDataContractSerializer xmlSerializer = new GenericXmlDataContractSerializer();
xmlSerializer.Serialize<IPredictorModel<double>>(_model, () => new StreamWriter(filePath));
```

It fails with the following SerializationException:

System.Runtime.Serialization.SerializationException: An object of type 'SharpLearning.InputOutput.Serialization.GenericXmlDataContractSerializer+GenericResolver' which derives from DataContractResolver returned false from its TryResolveType method when attempting to resolve the name for an object of type 'MathNet.Numerics.LinearAlgebra.Storage.DenseVectorStorage`1[[System.Single, System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]]', indicating that the resolution failed. Change the TryResolveType implementation to return true.

Any help?
"
SharpLearning.XGBoost.dll is not compatible with .net core,"Hi,
SharpLearning.XGBoost.dll is not compatible with .net core due to PicNet.XGBoost (0.2.1) dependency.
Checked on 0.31.8 version. And  Core 2.1 version
Do you plan to fix it?"
Continuously improving a neural network over time using small batches.,"Hey, first of thanks for a fantastic library!

The library is really easy and simple to use if you have a large dataset and want to train a network in one go.

But I'm building a DQN and I want to continuously improve a neural network from small batches of training data, with as little overhead as possible. Is that something that's easily possible in SharpLearning?

Right now, the only way I see it can be achive, is by doing something like this:

    var net = new NeuralNet();
    // ...

    while (true)
    {
        var learner = new NeuralNetLearner(net, new CopyTargetEncoder(), new SquareLoss());
        // ...
        net = learner.Learn(observations, targets);
    }

However there's a lot of overhead and data copying going on there. Are there better ways to go about it?

Thanks  :)

**Edit 1** : Seems like my example doesn't work either since the weights are randomized when a learning begins.

**Edit 2**: My seccond attempt, throws a nullreference exception on net.Forward(input, output). (Allthough I imagine that this is not a very good way to go about it either? And probably wrong on many levels 😊)

    var delta = Matrix<float>.Build.Dense(1, 1);
    var input = Matrix<float>.Build.Dense(inputCount, 1);
    var output = Matrix<float>.Build.Dense(1, 1);

    while (true)
    {
            PopulateInput(input)
            net.Forward(input, output);
            var expected = GetExpected();
            delta[0, 0] = (float)(expected - output[0, 0]);
            net.Backward(delta);
    }"
Serialization Exception,"Hi. 

Not sure what happened, but something happened :) 

1. I updated all projects from .NET Core 3.0 to 3.1 
2. Made some changes in MySQL DB 

Now, I get various errors from `GenericXmlDataContractSerializer`. Surprisingly, exception happens only when I build project the second time, after the first build it works fine. I mentioned DB, because I serialize trained model using `MemoryStream` and save it as a byte[] to the MySQL column of type LongBlob. I'm also using models from `ML.NET` and export / import them from DB the same way and they work fine, so probably DB is not an issue. All projects in the solution are built as x64. Serializer fails on any model, either `RandomForest` or `AdaBoost`, with the same exception. 

**The issue** 

1. build the project and start debugging 
2. create, train model, and save it to DB as byte array using `GetPredictor` method below 
3. select byte array from DB, deserialize to a model, provide test data and get estimate - **OK** 
4. stop debugging, repeat steps 1-3, now prediction method fails with the exception below - **NOT OK**

**The question**

Maybe somebody knows what could be the reason for serializer to fall with the exception? Also, can I serialize trained model to `MemoryStream` using different serializer, without `GenericXmlDataContractSerializer`? 

**Most common exception**
```
System.Runtime.Serialization.SerializationException: Element 'http://schemas.datacontract.org/2004/07/Core.Learners.SharpLearning.EngineSpace:Model' contains data from a type that maps to the name 'SharpLearning.RandomForest.Models:ClassificationForestModel'. The deserializer has no knowledge of any type that maps to this name. Consider changing the implementation of the ResolveName method on your DataContractResolver to return a non-null value for name 'ClassificationForestModel' and namespace 'SharpLearning.RandomForest.Models' 
```

**After updating all Nuget packages I got another exception only once** 
```
Invalid XML at line 1 or something like that
```

**Serializing trained model to byte array and save to DB** 

```C#
public virtual ResponseModel<byte> GetPredictor(IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<byte>();

  using (var memoryStream = new MemoryStream())
  {
    var processor = GetInput(columns, inputs, nameof(PredictorLabelsEnum.Emotion));
    var learner = new ClassificationRandomForestLearner();
    var serializer = new GenericXmlDataContractSerializer();
    var container = new MapModel<int, string>
    {
      Map = processor.Map,
      Model = learner.Learn(processor.Input.Observations, processor.Input.Targets)
    };
    
    serializer.Serialize(container, () => new StreamWriter(memoryStream));
    responseModel.Items = memoryStream.ToArray().ToList();
  }

  return responseModel;
}
```

**Deserializing model from DB stream and getting prediction** 

```C#
public virtual ResponseModel<string> GetEstimate(IEnumerable<byte> predictor, IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<string>();

  using (var memoryStream = new MemoryStream(predictor.ToArray()))
  {
    var processor = GetInput(columns, inputs);
    var serializer = new GenericXmlDataContractSerializer();
    var model = serializer.Deserialize<MapModel<int, string>>(() => new StreamReader(memoryStream));
    var predictions = model.Predict(processor.Input.Observations);

    responseModel.Items.Add(predictions.OrderByDescending(o => o.Key).First().Value);
  }

  return responseModel;
}
```

Method `GetInput` in the code above is just a conversion from `IDataView` format in ML.NET to `ObservationSet` format in `SharpLearning`. `MapModel` is a [wrapper](https://github.com/mdabros/SharpLearning/issues/132) that allows to save text labels along with numeric ones. "
[PR] The proj files have been updated to enable SourceLink,"CSProj files have been updated to enable SourceLink in your nuget
---

*[This pull request was created with an automated workflow]*

I noticed that your repository and Nuget package are important for our .NET community, but you still haven't enabled SourceLink.

**We have to take 2 steps:**
1) Please approve this pull request and make .NET a better place for .NET developers and their debugging.
2) **Then just upload the .snupkg file** to https://www.nuget.org/ (now you can find the snupkg file along with the .nuget file)

You can find more information about SourceLine at the following links  
https://github.com/dotnet/sourcelink
https://www.hanselman.com/blog/ExploringNETCoresSourceLinkSteppingIntoTheSourceCodeOfNuGetPackagesYouDontOwn.aspx

If you are interesting about this automated workflow and how it works  
https://github.com/JTOne123/GitHubMassUpdater

*If you notice any flaws, please comment and I will try to make fixes manually*
"
Is there a way to keep textual labels / targets as a part of the trained model?,"First of all, thank you for sharing this library. 
Second, would be great to make mapping between columns and feature names mode obvious. 
If model was serialized and saved on one computer and deserialized and loaded on the other one, then second computer will have no idea what's the meaning of labels / targets, because model keeps them as double values. 

**Save model**

```C#

var labels = new[] { ""Good"", ""Bad"", ""Average"" ...  };
var labelKeys = labels.Select((v, i) => (double) i);  // take label key instead of name 
var learner = new ClassificationDecisionTreeLearner();
var model = learner.Learn(items, labelKeys); // is there any reason not to use string labels instead of doubles?

using (var memoryStream = new MemoryStream())
{
  var serializer = new GenericXmlDataContractSerializer();
  serializer.Serialize<IPredictorModel<double>>(model, () => new StreamWriter(memoryStream));
  db.Save(memoryStream.ToArray());  // convert XML to byte[] and save as Blob to DB
}
```

**Load model**

```C#
var xmlModel = db.Get(...).AsBlob().GetBytes(); // load saved model from blob column in DB

using (var memoryStream = new MemoryStream(xmlModel))
{
  var serializer = new GenericXmlDataContractSerializer();
  var xml = serializer.Deserialize<IPredictorModel<double>>(() => new StreamReader(memoryStream));
}
```

As a result, loaded model has property `Targets` that contains some double values, like 1, 2, 3, 4 and there is no way to understand that initially they meant ""Good"", ""Bad"", etc 

**Question**

Is there a way to save original **string labels / targets** as a part of the model to make prediction results human-readable? "
0.31.8.0: Ensure deterministic order of results from multithreaded optimizers,"Fix #130 and make order of results deterministic for all optimizers when running with parallel execution. Results will now also be the same between single threaded and multithreaded execution.

This affects all optimizers supporting parallel execution:
 - `BayesianOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`
 - `GridSearchOptimizer`
 - `ParticleSwarmOptimizer`
 - `RandomSearchOptimizer`
"
Order of results from RandomSearch is not deterministic with different iteration counts.,"Running the `RandomSearchOptimizer` with `runPrallel=false`, so not multithreading, with 100 iterations and 120 iterations seem to provide different order of results. Expectation would be that the fist 100 iteration would be the same, and this is not currently the case.
This is most likely caused by the use of `ConcurrentBag` to collect the results, which does not guarantee order.

This might also affect other Optimizers supporting parallel execution,"
Issue with loading model using GenericXmlDataContractSerializer: The deserializer has no knowledge of any type that maps to this name,"Thank you so much for developing this package. It's been working smoothly on my computer, but I might need some help on generating a dll file for others to run on their computers. I tried using this nuget package [https://github.com/Fody/Costura/graphs/contributors](url) to compile sharplearning dlls into the project dll and adding all sharplearning xml files to embedded resources. But I keep getting the same error saying that ""Element 'http://schemas.microsoft.com/2003/10/Serialization/:anyType' contains data from a type that maps to the name 'SharpLearning.GradientBoost.Models:RegressionGradientBoostModel'. The deserializer has no knowledge of any type that maps to this name. "" I was wondering where the deserializer knowledge is stored and how do I add them to the project dll file. Any comment or suggestion is appreciated. Thank you!"
0.31.7.0: Refactor BaysianOptimizer and add parallel computation,"This pull request refactors the `BayesianOptimizer` implementation to be use the same principles as the `SMACOptimizer`. The two optimizers are both model based optimizers, and should therefore be very similar in implementation. The `BayesianOptimizer` can be viewed a basic implementation of model based optimization, which the `SMACOptimizer` builds a few tricks on top of.
A base class for model based optimizers seems to be the next logical step, but that will follow in a later pull request.

The refactoring enables use of the `BayesianOptimizer` in an ""open loop"" style just like the `SMACOptimizer`. See the unit tests for an example.

This pull request also adds the option of parallel computation to the `BayesianOptimizer`. This work was originally added in #119.

Note that when running in parallel, and using the `Optimize(Func<double[], OptimizerResult> functionToMinimize)` method, the order of the results will not be reproducible. The individuel results will remain the same, but the order of the results will vary between runs.

I recommend only using the parallel version if the provided `functionToMinimize` is running serial computation, and is slow to compute."
"Minor typo fixes in layers (release postponed, so no version incrementation)",Fix typo in comment of DropoutLayer and SoftMaxLayer. Misspelled gradients.
0.31.5.0: Add sigmoid activation function,"Added sigmoid activation function, sigmoid short derivative, sigmoid test.
"
"TrimSplitLineTrimColumnsToDictionary throws a ""key already exists"" exception","Hi guys!
First off, great job! SharpLearning is very useful and well built.

One small bug I found while mistakenly creating a dataset based on a CSV file without the headers line (when calling 'ToF64Matrix()').

In SharpLearning.InputOutput.Csv.CsvParser -> Dictionary<string, int> TrimSplitLineTrimColumnsToDictionary(string line)
there's an iteration over the headers line, but it assumes all headers are distinct (and also that it is the headers line) - therefore an exception of ""key already exists in dictionary"" is thrown.
I think it should check if there's a duplication and throw a more explanatory error message in such case.

Let me know if you want me to fix it and add a pull request."
0.31.6.0: Adds implicit and explicit conversions from double[][] to F64Matrix,"This is a simple way to address the need to expose an API surface that accepts double[][] rather than an F64Matrix. Instead of changing all interfaces and implementations, I have made double[][] implicitly convertible to F64Matrix. This can be considered a convenience and a temporary workaround for [#20](https://github.com/mdabros/SharpLearning/issues/20) and [#115 ](https://github.com/mdabros/SharpLearning/issues/115)."
Add parallelism to Bayesian Optimizer. Also allow resampling non-deterministic algorithms,"[https://github.com/mdabros/SharpLearning/pull/119](https://github.com/mdabros/SharpLearning/pull/119)
"
retrain a Model,"How to Retain a model with new data ?
"
For Multiple files ,"is there a way of loading multiple files of same schema ,or do i have to combine  all files in to one big giant file ?"
Excellent Work,Thanks for x boost GPU learners .
"Adds parellelism to Bayesian optimizer by default and adds support for non-deterministic algorithms (release postponed, so no version incrementation)","I've had a second look at the Bayesian Optimizer and have introduced parallelism by default as with other optimizers. I've also introduced support for non-deterministic algorithms that may return different results for identical parameters. This also entailed a change to the standard serial behaviour so that instead of skipping an evaluation if the parameters did not change from the previous run, it will now store the results for all evaluations and skip any that have been run before. This should result in a performance improvement for the serial behaviour but will consume some memory."
"Hard dependency from Microsoft.IdentityModel.Clients.ActiveDirectory, version 3.17.2.31801",In my test project by saving of gradient boosting model I’ve encounter strange implicit dependency from Microsoft.IdentityModel.Clients.ActiveDirectory. It doesn’t work (load assembly exception) in all referenced versions of this assembly except I reference pretty old one version 3.17.2. Even if I use dependentAssembly construction it doesn’t help. My project is net461.
"0.31.4.0: Add CrossValidationUtilities.GetKFoldCrossValidationIndexSets, Refactor CrossValidation.","Extract the internal `GetKFoldCrossValidationIndexSets` method form the `CrossValidation<T>` class. 
This enables calculation of KFold CrossValidation IndexSets for use outside the `CrossValidation<T>` it self.

Usage: 

```csharp
// Targets to create KFold Index Sets from.
var targets = new double[] { 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3 };
// Sampler to control the sampling of the sets. In this case Stratified.
var sampler = new StratifiedIndexSampler<double>(seed: 242);

var indexSets = CrossValidationUtilities.GetKFoldCrossValidationIndexSets(sampler,
    foldCount: 4, targets: targets);

foreach (var (trainingIndices, validationIndices) in indexSets)
{
    // Do model training and accumulate predictions,
    // to form a fully k-fold cross validated prediction array.
}
```

Note, that in the case of remainders from `samplesPerFold = targets.Length / foldCount`, the last validationIndices will contain the remaining values (making it larger compared to the others), and the last trainingIndices will exclude these (making it smaller than the others)."
SharpLearning.Core: Merge SharpLearning.Containers and SharpLearning.Common.Interfaces,"Once #20, and #112 has been completed, the code in SharpLearning.Containers should be reduced quite a bit, and only contain a few basic types and support extension methods. For this reason it would make sense to merge this with the common interfaces assembly into a SharpLearning.Core project that holds the basic essentials for SharpLearning."
Add multidimensional array extensions to replace the current properties and methods on the Matrix class,Switching from the matrix class to multidimensional arrays requires adding extension methods to provide to expose the same members that the current matrix class does.
Consolidate common project settings into Directory.Build.props,"Where possible, add common project settings to Directory.Build.props"
Update projects to target c# 7.3 to enable new language features,
Remove/clean unused types and classes,"Currently there are a few which contains functionality that is rarely used, or that doesn't quite fit into the current direction of the package. This includes:
 - `SharpLearning.Containers.Arithmetic`: MatrixF64 is mostly used as a container, and more efficient matrix arithmetic can found in other libraries, like mathnet.numerics.
 - `SharpLearning.Containers.ObservationTargetSet`: This can be replaced by using a value tuple instead.
 - `SharpLearning.Containers.ArrayExtensions`: Several methods are unused.
 - `SharpLearning.CrossValidation.ContinuousMungeAugmentator`: 
 - `SharpLearning.CrossValidation.NominalMungeAugmentator`:
"
"Update code style, line length, member order, etc.","Part of 2019 sommer cleaning to get the code base cleaned and more up to date. This pull request contains mainly code style changes. This includes:
 - Line lengths around 100 chars (unless specific circumstances, like constructor checks, and expected test results)
 - Remove and order usings.
 - Class member ordering to follow standard guidelines.
 - Use expression bodied methods where applicable.

The pull request also includes code changes to avoid duplicated code in a few places.

A few more exotic metrics has also been deleted:
 - DiscreteTargetMeanErrorRegressionMetric 
 - RocAucRegressionMetric

If anybody uses these, let me know, and I can readd them."
Make SharpLearning.Neural support .net core 2.0/.net standard 2.0 (update mathnet numerics to latest version (.NETStandard 2.0 compatible) ),"This pull request updates mathnet numerics to latest version (.NETStandard 2.0 compatible). This enables SharpLearning.Neural to have support for .net core 2.0/.net standard 2.0, and solves #26. "
Add azure pipelines yaml configuration,"This pull request adds `azure-pipelines.yaml` configuration to control CI and PR validation. This also adds both debug and release validation. Previously, only the release version was build and tested via CI. 

This addition is the bare minimum of the yaml configuration. BuildPlatform has not been added to the configuration yet. I could not make it work together with the dotnet build command. Information on this can be found here: [dotnet issue 10421](https://github.com/dotnet/cli/issues/10421). So currently, the build platform from the project files is used.

I am currently using 'tasks' for the individual steps, but it seems that 'scripts' are generally more popular for dotnet core pipelines. Here is a guide using scripts: [yaml-build-pipeline-net-core-azure-devops-tutorial)](https://www.nankov.com/posts/yaml-build-pipeline-net-core-azure-devops-tutorial)

Another example is the [TorchSharp pipeline](https://github.com/xamarin/TorchSharp/blob/master/azure-pipelines.yml), but that also handles additional steps for getting external resources etc.. 'Scripts' definitely seems more flexible than 'tasks', so I might switch to 'scripts' in the future when I have some more experience with them."
"Can this project be called under xamarin forms and run on android? I tried, loading the model failed.",
Wow! Amazing work! Thanks a lot!,"Hi!

I also wanted to thank you for this wonderful library! The API is super clean and love it so far.

One question I have is regarding the accuracy score like the accuracy_score() function in scikit learn that can be seen here: https://www.kaggle.com/mathvv/prediction-of-red-wine-quality-93-215
Like this:
`print('Random Forest:', accuracy_score(y_test, rf_pred)*100,'%')`
Which outputs this:
`Random Forest: 91.875 %`

Many thanks and congrats again!

Flo"
Understand class prediction results,"Hi,
maybe some silly questions...
I'm trying to train a random forest model with **two different classes**. I think I understood that the number of rows of the target vector must be equal to observations matrix (therefore regardless of the number of classes). So in the rows of the output vector I set the value 0 for the first class and 1 for the second class. **This is right?** 
I would also like to understand how to interpret the results, for example if for a set of features I have the prediction value 0.6 I must consider it a class of type ""0"" or a class of type ""1""? Do I have to cast to integer or I must to round it? Finally the ""variance"" values ​​contained in CertaintyPrediction indicates the probability of the prediction (greater is better)?
many thanks
"
Add support for simple linear/logistic regression,"You've done a great job with the more sophisticated algorithms: would it be possible, for completeness, to throw in linear/logistic regression? I imagine it would be fairly quick comparatively."
"0.31.1.0: Add SmacOptimizer, update argument names on HyperbandOptimizer and BayesianOptimizer, clean up SharpLearning.Optimization.Test project","This pull request adds the `SmacOptimizer` to `SharpLearning.Optimization`. The implementation is based on the paper: [Sequential Model-Based Optimization for General Algorithm Configuration](https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf).

The algorithm combines bayesian optimization with greedy local search based on the current top solutions.

The `SmacOptimizer` implements the regular `IOptimizer` interface, but also surfaces the two primary methods for running the algorithm `ProposeParameterSets` and `RunParameterSets`. This makes it possible to use the optimizer in an ""open-loop"" style, and allows the optimizer to be easily used from or combined with other optimizers. An examples could be using the scheduling technique from the `HyberbandOptimizer` together with the model based sampling from the `SmacOptimizer`.

An example showing ""open-loop"" use can be found in the `SmacOptimizerTest` class."
Remove resources and add culture initialiser for test projects,"This should fix issue #34, and fix the current issue with azure dev ops pipelines.

This adds an `AssemblyInitializeCultureTest` class for all unit test project, which sets the culture settings to `CultureInfo.InvariantCulture`. This should make the unit tests pass on machines with different culture settings.
```CSharp
    [TestClass]
    public class AssemblyInitializeCultureTest
    {
        [AssemblyInitialize]
        public static void AssemblyInitializeCultureTest_InvariantCulture(TestContext c)
        {
            CultureInfo culture = CultureInfo.InvariantCulture;
            CultureInfo.DefaultThreadCurrentCulture = culture;
            CultureInfo.DefaultThreadCurrentUICulture = culture;
            Thread.CurrentThread.CurrentCulture = culture;
            Thread.CurrentThread.CurrentUICulture = culture;
        }
    }
```

This pull request also removes the use of resources in all test projects, and instead uses a `DataSetUtilities` class to handle small test datasets. This also cleans up a lot of CsvParser code for loading the data."
Make individual Tree models public on ForestModels. This is a breaking change. Also clean up SharpLearning.RandomForest.Test,"This pull request makes the individual tree models public on the forest models: `RegressionForestModel` and `ClassificationForestModel`. This makes it possible to use the individual trees from the model for custom predictions, or for calculating statistics. For instance, using a different ensemble strategy than average for regression and majority vote for classification. Getting the predictions for the individual trees also makes it possible to calculate statistics on the predictions to see how much the ensemble of models agrees or disagress.

The tree models are accessed through the `.Trees` property of the forest models:

```CSharp
var learner = new RegressionRandomForestLearner();
var forest = learner.Learn(observations, targets);
var trees = forest.Trees;
```

The trees can then be used individually afterwards:

```CSharp
var prediction = trees.Select(t => t.Predict(observation)).Average();
```

Note that this is a **breaking change**, since the trees have been promoted from a private member to a public property. This means that it will not be possible to load ForestModels trained with earlier versions of SharpLearning into this version. So retraining of models from the `SharpLearning.RandomForest` project is mandatory if updating to 0.31.0.0 and newer versions. This is sadly the downside of serializing the model code directly instead of using a custom format, which is the current strategy in SharpLearning.

This should solve #101 and #94 "
 Individual trees prediction of the classification RF model," #94
Add an additional methods which provides individual trees prediction of the classification RF model.

I ‘m not sure the code is right .Please help me to check it.I need the methods a little hurry.Thanks very much!
JinHJ"
0.30.2: Add HyperbandOptimizer.OptimizeBest method,Add missing `OptimizeBest` method for the `HyperbandOptimizer`. This method returns the best result found by the optimizer
"0.30.1.0: Add HyperbandOptimizer, and update test adapters","This pull request adds the `HyperbandOptimizer` to `SharpLearning.Optimization`. The implementation is based on the original article [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) and the implementation by [fastml](http://fastml.com/tuning-hyperparams-fast-with-hyperband/).

Compared to the other optimizers from SharpLearning, Hyperband includes an extra parameter in the objective function, `unitsOfCompute`. Hyperband uses the `unitsOfCompute` parameter to control a budget of compute for each set of hyperparameters. Initially it will run each parameter set with very little compute budget to get a taste of how they perform. Then it takes the best performers and runs them on a larger budget.

The `unitOfCompute` parameter is used in the objective function, and could for instance be used to control the size of the training set, the number of trees in gradient boost, or the number of epochs for neural nets. One unit of compute could for instance be defined as 1000 samples in the training set. The `maximumUnitsOfCompute` is provided as an argument to the `HyperbandOptimizer`, and the optimzer will define a schedule for evaluating the hyperparameters on a budget.

A small experiment comparing the results and runtime of the `HyperbandOptimizer` vs. the `RandomSearchOptimizer`, optimizing a neural net (using CNTK) on the CIFAR-10 dataset:

**System**
**CPU: i7-4770**
**GPU: GTX1070**

The Hyperband optimizers uses the default parameters except for the `skipLastIterationOfEachRound` which is enabled for one of the runs. The default parameters result in a total of 209 different parameter sets tried with the Hyberband optimizers. The `unitsOfCompute` parameter in the objective function is used to control the size of the training set, where 1 unit of compute is set to 740 samples, which corresponds to the full training set size of 60.000 samples, when `maximumUnitsOfCompute` is set to 81, which is the default maximum. The full test set is used to track the test loss/accuracy in all rounds/iterations.

| Optimizer        | Time (hours)           | Test accuracy (%)  |
| ------------- |:-------------:| -----:|
| `RandomSearchOptimizer(iterations=100)`|  45.34 |  88.47 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=false)` |  10.46  |   87.93 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=true)`|  6.56  | 87.93 |

As can be seen the RandomSearchOptimizer finds a slightly better parameter set. However, the two Hyperband runs uses significant less time to find a solution that is nearly as good. In this case, skipping the last iteration of each round results in finding the same parameter set as when including all iterations. This will not be the case for all problem types, but it does provide a nice speed up for large problems.

In the end, Hyperband finds a solution that is almost as good, and reduces the time required by a factor of 4-7.

A future extension to the hyperband optimizer could be to use bayesian optimization instead of random search to select the parameter sets. This combination has proven very useful in: [Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/pdf/1807.01774.pdf)."
"Add support for Reinforcement Learning algorithms: QLearning, Sarsa etc.","There is currently support for most of the common (and some less common) ML algorithms in Sharp Learning. However, there does appear to be a lack in the area of Reinforcement Leaning and some might observe these algorithms are beginning to gain some traction.

If there is any appetite for extending into this area, I would propose as an initial baseline provision for QLearning and Sarsa, backed by Epsilon Greedy and Boltzmann approaches. A second stage could then continue with Thompson and UCB1 exploration, and finally the existing Neural Net and ensemble interfaces could probably produce a compound that resembled Deep Q Networks."
0.30.0: Optimizers Optimize method returns unfiltered results in chronological order,"The `Optimize ` method on the optimizers from `SharpLearning.Optimization` will now return all results, unfiltered and in chronological order. Before, the results would be filtered for `NaN` values and ordered from smallest to largest error. This change makes it easier to compare the iterations required to get a good solution between the optimizers. The `OptimizeBest` method, which returns the single best result from the optimizers, is unchanged and will provide exactly the same result as previously.

This is a potential breaking change, so if relaying on the order of the results from the `Optimize` method, these should now be sorted and/or filtered after the call to the optimizer. Like it is also done in the `OptimizeBest` method:

```CSharp
Optimize(functionToMinimize).Where(v => !double.IsNaN(v.Error)).OrderBy(r => r.Error).First();
```

Note, that the particle swarm optimizer only returns the latest result from each particle."
"0.29.1: Rename logarithmic transform to Log10 transform, and release contribution of parallel ParticleSwarm and parallel GlobalizedBoundedNelderMeadOptimizer.","This pull request renames the `Logarithmic` transform to `Log10` transform. This should fix issue #93.

This pull request also releases the contributions made by @jameschch in pull request #95, which adds parallel execution to the following optimizers:

 - `ParticleSwarmOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`

and adds selection of the degree of parallelism to:

 - `GridSearchOptimizer`
 - `RandomSearchOptimizer`"
"Adds parellelism to Particle Swarm optimizer, Adds configurable maxim…",This introduces parallel evaluation to the particle swarm and Nelder Mead optimizers. This also introduces a configurable maximum degree of parallelism to Random and Grid optimizers. The automatic scheduler is not effective for long-running problems that themselves are multi-threaded. This setting allows the user to strictly control the number of parallel operations.
Random Forest: how can I get each tree's prediction ?,"hi,Mads,
I hope to know how can I get each tree's prediction or which category it choose when I finished trained a RF model for my classify task？It is useful for my task.
thanks！
"
Optimization: Rename Transforms.Logarithmic to Transforms.Log10,"Currently, the name of the logarithmic transform is misleading, since in the .Net world log refers to log2, and the [LogarithmicTransform](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.Optimization/Transforms/LogarithmicTransform.cs) from SharpLearning uses Log10. 

So the transform, and the enum should be renamed to illustrate the use of Log10. For instance:
 - `Transforms.Log10`
 - `Log10Transform`"
Add IParameterSpec to SharpLearning.Optimization,"Primary change is the addition of an `IParameterSpec` interface, that replaces the previous `ParameterBounds` type. Two concrete implementations have been added:
 - *GridParameterSpec*:  Usable when a fixed set of parameters, needs to be searched.
 - *MinMaxParameterSpec*: Direct replacement of `ParameterBounds`. used for sampling values in the range [min;max].

The addition of the `IParameterSpec` and the  `GridParameterSpec`, makes the `GridSearchOptimizer` use a similar set of bounds as all the other optimizers. This makes it easier to switch to the `GridSearchOptimizer` in scenarios where autofac or other dependency injection frameworks are used.

The addition of the `GridParameterSpec`, also makes it possible to limit the sampling of the `RandomSearchOptimizer` to a fixed set of values. For instance:

```csharp
var parameterSpecs = new IParameterSpec[] 
{
    new GridParameterSpec(1, 10, 15, 20, 25),
    new MinMaxParameterSpec(0.0, 100.0, Transform.Linear)
};
var optimizer = new RandomSearchOptimizer(parameterSpecs, iterations: 100);
var actual = optimizer.OptimizeBest(Minimize);
```

In the above, the `RandomSearchOptimizer` will only sample randomly between the fixed values `1, 10, 15, 20, 25`, for the first parameter."
Add serializable feature transforms,"This pull request enables serialization of the features transforms available in the *SharpLearning.FeatureTransformations* project.

This is related to issue #90"
Serialisation of MinMaxTransformer,"Firstly, thanks for this excellent library - it is pretty well exactly what i have been looking for.

I have not found a way to serialise a MinMaxTransformer after use during training of a regression model 

Is there some other approach i should be using to normalise new data points for prediction in subsequent processing?"
"Error when training with GPU -  ""Unknown linear updater grow_gpu_hist""","I have downloaded and installed CUDA, my GPU is benchmark ""ASUS ROG strix OC 1080TI"",
when the learning starts some console prints are shown (attached screenshots) some error messages and sometimes the program even crashes,
should i build the program in certain way/download CUB or something else i missed?
Please help!

thx,
![capture](https://user-images.githubusercontent.com/13719129/47250162-af9d4c00-d425-11e8-8f29-ed3c8f3526ba.PNG)

"
Add Activation Functions other than ReLU please!,
SEHException when using xgboost with gpu,"I have installed latest Cuda version (9.2), and when setting the tree method to GPU* i get an SEHException ""external component has thrown an exception"", should i compile the program in a certain way after cuda installation? What should i do?, thanks!"
"Add overload for CsvRowExtensions.ToF64Matrix and ToF64Vector, to support other converters ",
Multiple output regression,"Most regression models are hardcoded with learning method: Learn(F64Matrix observations, double[] targets), what if we had multiple predicted variables.
I want to add method Learn(F64Matrix observations, F64Matrix targets)"
Full access to trained model structures,"Currently the trained models only expose members from IPredictorModel. For NNs, its impossible to get to the layered structures of the best model. Extend models to expose this"
GBM prediction confidence,"Hi, is it possible to add an option for getting the confidence of a prediction of a GBM?
To know how much the prediction ""can be trusted""
How can i do it by my self?
Thanks!"
Sample weight for XGBoost,"In Python XGBoost one can provide weights for each row of the data, see http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit. I tried to look for a way to specify such weights in SharpLearning, but could not find it. Is this possible?"
 Add check to GBMDecisionTreeLearner so we wont use more features than we can,
CrossValidation CrossValidate ProbabilityPredictions error means?,"Hi,
when I run ""CrossValidation_CrossValidate_ProbabilityPredictions"" example with my data, it gives these values:
**Cross-validation error: 6.88921441267999**
**Training error: 6.86281040890985**
I searched a lot but couldn't find any documantation abput it? Could you explain what's this values mean, and for best prediction what they should be?
Thanks.
Regards!"
F# unit tests,Let’s „contaminate“ SharpLearning with some F# code. For data processing F# is quite good alternative to c#  
Text Classification,"I am pretty new to Machine Learning and all of these things
I am want to ask a question about text classification: 
Can I use this library for text classification and how? 



P.S I need to split one string and classify substrings by Categories\Groups\etc, for example ""Ray's Potato Chips with Ketchup taste 80g"" I need to split into 
Category ""Potato Chips"" 
Groups:""Ketchup"" 
Brand(or smthng):""Ray's"""
How to vectorize text?,"Hi, thanks for the great library!

My CSV has text in some of the columns. Some of them are categorical (e.g. month of the year) and some have free text (e.g., book title). Looks like `SharpLearning.InputOutput.Csv.CsvRowExtensions.ToF64Matrix` is trying to parse stringified numbers. What if my CSV consists of non-number values? Is there a recommended way or should I wire another lib to do TF-IDF/word2vec/char embedding/etc?"
How to load data from SQL server table,"All given samples contain CSV method only.

```
            #region Read data

            // Use StreamReader(filepath) when running from filesystem
            var parser = new CsvParser(() => new StringReader(Resources.winequality_white));
            var targetName = ""quality"";

            // read feature matrix (all columns different from the targetName)
            F64Matrix observations = parser.EnumerateRows(c => c != targetName).ToF64Matrix();

            // read targets
            var targets = parser.EnumerateRows(targetName).ToF64Vector();
```

I would like to load data from a List<Dto> object; how it can be possible?
Thanks!

edit:

```
    /// <summary>
    /// Parses the CsvRows to a double array. Only CsvRows with a single column can be used
    /// </summary>
    /// <param name=""dataRows""></param>
    /// <returns></returns>
    public static double[] ToF64Vector(this IEnumerable<CsvRow> dataRows)
    {
      if (dataRows.First<CsvRow>().ColumnNameToIndex.Count != 1)
        throw new ArgumentException(""Vector can only be genereded from a single column"");
      return dataRows.SelectMany<CsvRow, double>((Func<CsvRow, IEnumerable<double>>) (values => (IEnumerable<double>) values.Values.AsF64())).ToArray<double>();
    }
```

This code only works with CsvRow list."
Which model type should I use for financial price prediction?,"First of all thank you for the great library!
My question is simple: I want to predict next period price with pre-computed history values.
I have over 30 rows data for each price.
Price and datas are decimal.

**For example history:**
**Indicator1** - **Indicator 2** - **Indicator 3** - **Price**         - **Trend**
10,01121  - 23,56540    - 12.00001     - 12,23321   - UP
9,00001    - 3,00040    -   2.00001       - 1,23300     - DOWN
...
...
**And data to predict coming like** 
8,11211    - 1,00020    -   0.00021       - 3,5555     - **?**
I want to get TREND field.

Which model should I use? Any example will be perfect?
Regards!
"
Implement Coefficient of Determination (r-squared) metric,
Add ParameterType to optimization ParameterBound,"This pull request adds a `ParameterType` to the `ParameterBound` class in `SharpLearning.Optimization`. The parameter type specifies if the parameter is discrete or continous. This enables the parameter samplers used in the optimizers to sample from either a continous range or from a discrete range. Before this, all samplers would sample from a continous range.

For instance, during hyperparameter optimization of a gradient boost model, it makes more sense to sample the number of trees on a discrete range, and the learning rate on a continous range. If the number of trees is sampled on a continous range, the optimizer will try many continous values which will be cast to the same integer values in the learner. For instance 15.2325, 15.9343, and so on, will all be cast to 15. Using the discrete range will make the search more efficient.

The parameter type is specified on the `ParameterBound` class, together with the other options:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 10, max: 1000, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // iterations
    
    new ParameterBounds(min: 0.001, max:  0.2, transform: Transform.Logarithmic, 
         parameterType: ParameterType.Continuous), // learning rate
    
    new ParameterBounds(min: 1, max: 25, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // maximumTreeDepth
    
    new ParameterBounds(min: 0.5, max: 1.0, transform: Transform.Linear, 
         parameterType: ParameterType.Continuous), // subSampleRatio
};
```

The default value of the `ParameterType` on `ParameterBounds` is still `Continous`, so the default behavior of the optimizers will not change. 
"
Add proper optimizer seeding to all optimizers in SharpLearning.Optimization,"This pull request will add proper seeding to all optimizer algorithms in `SharpLearning.Optimization`. The technique used is a single seed is set through the constructor, this seed is used to create a `random` generator, which will then create seeds for all underlying algorithms used in the given optimizer.

Note, that since this PR will change default seeding of the optimizers, the default behavior of the optimizers will not be the same as before this addition.

This should solve issue #69 . "
Nuget package for SharpLearning.XGBoost can't install,"
Currently there is an issue when installing the SharpLearning.XGBoost package, Nuget will try to add a reference for the native xgboost dll:

![image](https://user-images.githubusercontent.com/9001637/40295861-298b3238-5cdb-11e8-9426-f7646974f015.png)

The reason for this is how the nuget package for PicNet.XGBoost.Net has been created. I have openened an issue to get this solved: https://github.com/PicNet/XGBoost.Net/issues/24
"
[Question] How would you use the model to make predictions on new data?,"I have read all the examples and gone through the source code, but haven't been able to answer the question.

I have setup a data set, trained and tested the model, but now I would like to use the model to make predictions on new data. How would I achieve this?

Example:
Target value has 3 classifications: good, bad, average

New data comes in -> use trained/tested model to make a prediction on the target value. Also, would it be possible to get a probability/confidence of the prediction of the target value i.e. 25% good, 50% average, 25% bad.
"
Most implementations of IOptimizer don't properly pass on the random seed to all internally used algorithms,"Hi mdabros,

I love the Optimizer classes of SharpDevelop, I use them heavily for hyperparameter tuning.
But I would like to use differend random seeds and parts of the Optimizer classes don't allow using them like that.
Example:
If you look at the constructor of your BayesianOptimizer you can see that it doesn't pass on the ""seed"" parameter to all other classes that BayesianOptimizer creates instances of, sometimes it will just forward a hardcoded 42 instead. 
I know, 42 is the answer, but I would prefer ""seed"" in this case... ;)
The other IOptimizer implementations have similar issues.
Would be nice if you could modify that some time... 

Thank you!

Best regards
Florian
"
Add SharpLearning.XGBoost project,"Add a more efficient alternative to `SharpLearning.GradientBoost`. XGBoost is faster on CPU and also supports GPU learning. However, it does have native dependencies, so might not be ideal for all platforms and situations.

A small test comparing the `RegressionXGBoostLearner` and the `RegressionGradientBoostLearner` from SharpLearning on a medium sized regression task. 

Dataset: [YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)
Rows: 515345
Cols: 90

Hardware:
CPU: Core i7-4770
GPU: GTX-1070

Model parameters:
`MaximumTreeDepth`: 7
`Estimators`: 152
`colSampleByTree`: 0.45
`colSampleByLevel`: 0.77

Training time compared using XGBoost in `histogram` and `exact` mode on GPU and CPU:

![image](https://user-images.githubusercontent.com/9001637/39971261-51276bf6-56f8-11e8-9d59-b50c46d4af7f.png)


As can be seen, XGBoost can be up to 70 times faster, when using the histogram based tree method. Using the exact method, which is more similar to the method from SharpLearning.GradientBoost, the speed up is still around 10 when using GPU, and 5 when using CPU. 

Missing tasks before the PR can be completed:
 - [x] Add argument checks to learners.
 - [x] Add unit test of conversion class.
 - [x] Add index support for learners, and input checks.
 - [x] Add probability support for classifier model.
 - [x] Add more learner and model tests.
 - [x] In the classification model, consider removing the targetNameToTargetIndex member, and adopt XGBoost´s requirement of sequntial class labels starting at 0. Checks can be added to alert users before learning starts.
 - [x] Complete pull request to XGBoost.Net to enable GPU use and Booster selection.
 - [x] Add VariableImportance support to XGBoost models.
 - [x] Split objectives into regression and classification, so only compatible objectives are available for the learners .
 - [x] Consider splitting learners into `Linear`, `Tree` and `Dart`, to only show relevant hyperparameters for each in the constructors.
 - [x] Add enums for the DART specific parameters. 
 - [x] Complete pull request to XGBoost.Net to add DART parameters.
 - [x] Complete pull request to XGBoost.Net to fix `Booster.Dispose()`.
 - [x] Get XGBoost.Net to publish new nuget package.
 - [x] Change from local reference to updated XGBoost.Net package.
 - [x] Update readme.
 - [x] Check cross-validation and learning curves loops with XGBoost models (disposable).
 - [x] Package SharpLearing.XGBoost during build to avoid issue with ""dotnet pack"" and how the native dll is included in picnet.xgboost.net
 - [x] Add probability interfaces to xgboost classification learner.
 - [x] Add model converter from XGBoost to SharpLearing.GradientBoost. (Added but not completed).
 - [x] Consider using the SharpLearing.GradientBoost.Models instead of the XGBoost equivilants. This would enable standard serialization and features, and avoid having to deal with native resources when using the XGBoost models. (For now, it has been decided to use the XGBoostModels, and leave the conversion for another pull request)."
Fix the non-deterministic behaviour of the random forest and extra trees learners when running multi-threaded.,"This pull request will fix the non-deterministic behaviour of the random forest and extra trees learners. It will also ensure that the learners produce the same models when running single threaded and multi threaded.

The main changes to the learners are:
 - The random generator used for each tree is created before the learning process starts.
 - Each random generator is associated with a specific tree index.
 - The final order of the learned trees is made using the tree index.    

This pull request should solve issue #65 and is related to the previous pull request on that issue: #66 "
correct random seed problems in random forest learners,fix bug described in #65 
Random Forest: m_random and parallel RNG,"Hello, I would first like to thank you for making such an excellent and accessible repo. We're getting quite a bit of use out of it in multiple projects.

I've noticed that despite setting the seed in the Random Forest learner, I get slightly varying results from run-to-run given identical inputs. I suspect the problem is in the following code block:

```
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, new Random(m_random.Next())));
});
```

The parallel random number generation is a problem; the same random numbers are generated because the seed is set in the constructor, but there is a ""race"" between the threads to grab the next random number. Locking m_random would not help, I think. This should be fixed by generating a random number for each tree prior to entering the Parallel.ForEach loop, like so:

```
int[] randomNumbers = new int[m_trees];
for(int i = 0; i < randomNumbers.Length; i++)
{
   randomNumbers[i] = m_random.Next();
}
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, randomNumbers[work]));
});
```

Let me know if I'm overlooking something. I'd be happy to fork and make a pull request.

Rob"
An item with the same key has already been added,"Hello,

Might not be an actual issue, but more of a question on how to handle my error. I want to feed data into the parser that is in the SharpLearning.InputOutput.Csv package. Below is my code:

`string rawTarget = Transformations.ReturnColumnAsCSVString(Data, OutputColumn);

System.Windows.Forms.Clipboard.SetText(rawTarget);

var targetparser = new CsvParser(() => new StringReader(rawTarget));

var targets2 = targetparser.EnumerateRows(OutputColumn).ToF64Vector();`

So, first I pull my data into a string, this results in a string looking like this:
`""Vwap"";7049.4;6983.3;6981.8;6871.0;6846.7;6811.0`
(obviously there is a lot more)

Then I use the Stringreader to read my string and parse it to an F64 vector.  The error I get is:
`An item with the same key has already been added.`

I have also tried to convert the string to a stream, then use the Streamreader but this results in the exact same error. I am at a loss at how to solve this. 

Hope anyone can provide a solution! Thanks in advance!
"
Strongly-named assemblies,"First, let me say thanks for the great package: it works much better in our app than our previous (non-learning) solution.

We've got one issue to report: in our next release, all the various subsystems need to be in signed assemblies, which means we can't at the moment use SharpLearning since it would need to be called by a signed assembly and is itself unsigned. Any chance we could see signed versions of the SharpLearning Nuget packages?

Thanks,

Alistair
"
Possible evolution: trained neuralnet predict C output,"Hello,
I created a NeuralNet and trained it on my computer. 
I wish to port it on an embedded target now and i asked myself if a C code output of predict for a trained network could be possible. Is that the case ? If yes could you tell me some advises where to look at ?

Thank you"
change unsafe code to safe code,
Add support for enabling/disabling messages from learners during training.,"Currently, some of the learners in SharpLearning will output information during the training period. This includes the early stopping with gradient boost and neural networks. It should be possible to enable and disable these messages, and preferably, also possible to choose where the messages should be outputted. For instance to  `Console`, `Trace`, or alternatively a log file. 

This could be made by adding an `Action` to receive the message. This should probably be part of a separate interface, for learners supporting this. Something like:

```csharp
public interface ILogger
{
   public Action<string> Log { get; set }
}
```

The learners would then add the message to the log. Likewise, other algorithms like the optimizers could also implement this interface."
OutOfMemory Exception in F64Matrix constructor - maximum array bounds exceeded,"Hi there!

First: Thanks for the great work, excellent design you have there!

I am experiencing an OutOfMemory Exception in the constructor of F64Matrix that does not really come from memory shortage but from the fact that F64Matrix internally uses a single one dimensional double array that can quite easily exceed .NETs internal boundaries of maximum array dimensions.

In my case I tried to create a F64Matrix with 10 mio rows and 55 columns.

My preferred suggestion would be to either abstract the matrix to an IF64Matrix interface that probably only consists of the At() method overloads. This would enable users to provide a custom implementation that is capable of handling larger amounts of data, if needed even by swapping data from and to disk.
Another solution could be to change the internal implementation of F64Matrix to use an array of double arrays, which I believe could also help.

Thanks for your help and keep up the excellent work!

Best regards

Florian"
Add ParameterBounds and hyper-parameter scale transform to optimization ,"This pull request adds a ParameterBounds type to optimization to make it more clear to the user how to setup min and max bounds for the optimization parameters. This is illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
};
 ```
Note that the `GridSearchOptimizer` still uses jagged arrayes for defining parameter ranges. This is intended since a grid search typically invovles outlining all hyperparameters for the grid, and jagged arrays fits this purpose nicely.

It is now also possible to add a scale transform to the sampling of the hyper-parameters. This can be usefull when dealing with hyper-parameters like learning rate, that covers a large range close to zero, like 0.0001 to 1.0. In this case it would be better to sample using the logarithmic scale to get a better distribution of values across the entire range. Default transform is linear, and logarithmic scale can be applied like illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 100, max: 300, transform: Transform.Linear),
    new ParameterBounds(min: 0.001, max: 1.0, transform: Transform.Logarithmic),
};
 ```

Running the hyper-parameter tuning example from [SharpLearning.Examples](https://github.com/mdabros/SharpLearning.Examples/blob/master/src/Guides/HyperparameterTuningGuide.cs) before and after introducing logarithmic scale for the learning rate and subSampleRatio shows improved results:

**With linear scale**
Train error: 0.0174 - Test error: 0.3905

**With logarithmic scale**
Train error: 0.0011 - Test error: 0.3843

This will solve issue #57 .


"
Optimization: Add option for how to sample hyper parameters,"Currently, all optimizers in SharpLearning.Optimization use random uniform sampling for sampling hyper parameters from the provided min/max boundaries. This is not always optimal, for instance when dealing with a hyper parameters like learning rate that can span a large range of values, like 0.0001 to 1.0. Using random uniform sampling in this case might result in only sampling values in a small part of the range. In this case it would be much better to sample at uniform in the log space. Hence, it should be possible to select which space to sample from for each hyper parameter when setting og an optimizer. 

This should include at least random uniform form:
 - Linear (current method)
 - Logarithmic
 - Exponential

At the same time, setup of the hyper-parameter ranges could be changed from setting op an array of arrays to using a type to guide the user better:

**Current method**
```csharp
var parameters = new double[][]
{
   new double[] { 80, 300 }, // iterations (min: 80, max: 300)
   new double[] { 0.02, 0.2 }, // learning rate (min: 0.02, max: 0.2)
};
 ```

**Proposed method**
```csharp
var parameters = new OptimizerParameter[]
{
   new OptimizerParameter(min: 80, max: 300,  SamplingMethod.Linear), // iterations (min: 80, max: 300)
   new OptimizerParameter(min: 0.02, max: 0.2, SamplingMethod.Logarithmic),, // learning rate (min: 0.02, max: 0.2)
};
 ```"
[WIP] Backends testing and C# TF batch running,
[WIP] Add cntk cnn example (C# and Python),"Adding cntk convolutional neural network example in C# and Python [WIP].
 - [x] Examples should return the same result
 - [x] Examples should be as similar as possible


"
Add preserveObjectReferences option to GenericXmlDataContractSerializer,"The default settings for the GenericXmlDataContractSerializer is to perserve object references. This is required for the SharpLearning.Neural.Models to be serialized and deserialized correctly. However, for serializing simple types like a list of items, the resulting xml can be slightly more complicated. Therefore this pull request adds the option to not preserve object references when using the GenericXmlDataContractSerializer.

The recommended setting for serializing SharpLearning models is the default (true).

**XML with  perserveObjectReferences = true (default):**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" z:Id=""1"" z:Size=""5"" xmlns:z=""http://schemas.microsoft.com/2003/10/Serialization/"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key z:Id=""2"">Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""3"">Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""4"">Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""5"">Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""6"">Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```

**XML with  perserveObjectReferences = false:**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key>Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```"
Random Forest Regression generates constant values for prediction,"Hi,

I am using random Forest for learning. The output I get results into the initial 40 or so values varying (Float values) but after that it's just constant. 

I was wondering if you have seen this behavior. The data has 24 features and 500 observations. I get the 42 first prediction varying but after that it's just constant. 

I can provide the data and the code if you would like that.

regards,
Avi"
Add cntk python simple mnist examples,"As a starting point for simple python mnist example using cntk, copied the original example from [cntk](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)

Currently, I have not found a method for running/debugging this from visual studio, only from command prompt. More information can be found in the [readme](https://github.com/mdabros/SharpLearning/blob/backends-cntk-python/python/src/CntkPython/README.md)

@nietras Currently, the simple mnist example from CNTK is quite different from corresponding example in Tensorflow. We should decide how much we want to modify the CNTK example, and/or the tensorflow example, to make them as similar as possible.

Some differences: 
 - The CNTK example uses text files as input and tensorflow the raw data.
 - The CNTK example also uses the Dense operator from the layer API.
 - The CNTK exmaple uses all 60k examples for training and tensorflow only 10k."
Consider switching to XorShift for RNG,"XorShift is a simple alternative to the built-in Random class in C#, which provide better performance and randomness. I benchmarked the RNG implementations of Math.net as seen in the picture. The XorHome in the list is my own quick port of xoroshiro128+ from here: http://xoroshiro.di.unimi.it/xoroshiro128plus.c

![image](https://user-images.githubusercontent.com/657616/35411685-ec14329a-0219-11e8-8e19-82716a4361ef.png)

As seen in the picture, the Math.Net XorShift is much faster than the built-in random."
Backends Tensorflow Deep Mnist Raw (to backends!),
[WIP] Backends Tensorflow Deep Mnist Raw,
Backends TensorFlow prototyping,"Create PR to more easily follow changes.

I made the mistake of merging with master, so perhaps ""backends"" should merge with master too?"
Add CntkRawMnistTest. Showing a simple cnn using the cntk api,Training a simple cnn using the cntk api. 
Backends new cntk test project,"Changing the `SharpLearning.Backend.Cntk.Test` project format from new csproj to old style .net framework csproj. Also, change platform target for the project to x64 to work with CNTK.

"
*NOMERGE* Cntk experiment,Create PR to easier review the changes in this.
Failing unit test ClassificationGradientBoostLearner_LearnWithEarlyStopping,"This always fails on when I run `all.ps1`
cc: @mdabros 
```
Failed   ClassificationGradientBoostLearner_LearnWithEarlyStopping
Error Message:
   Assert.AreEqual failed. Expected a difference no greater than <1E-06> between expected value <0.162790697674419> and actual value <0.13953488372093>.
Stack Trace:
   at SharpLearning.GradientBoost.Test.Learners.ClassificationGradientBoostLearnerTest.ClassificationGradientBoostLearner_LearnWithEarlyStopping() in E:\oss\SharpLearning\src\SharpLearning.GradientBoost.Test\Learners\ClassificationGradientBoostLearnerTest.cs:line 120
Standard Output Messages:


Debug Trace:
Iteration 1 Validation Error: 0.674418604651163
   Iteration 11 Validation Error: 0.290697674418605
   Iteration 21 Validation Error: 0.244186046511628
   Iteration 31 Validation Error: 0.22093023255814
   Iteration 41 Validation Error: 0.186046511627907
   Iteration 51 Validation Error: 0.186046511627907
   Iteration 61 Validation Error: 0.197674418604651
   Iteration 71 Validation Error: 0.174418604651163
   Iteration 81 Validation Error: 0.13953488372093
   Iteration 91 Validation Error: 0.162790697674419
```
Both for `Debug` and `Release`.
"
0.26.7.0: TimeSeriesCrossValidation,"With most time series data, it is not possible to use traditional cross-validation methods, like the CrossValidators available in SharpLearning. The reason for this is, that shuffling the data will result in the learner and model using future data to predict past observations.
While it is possible to use the NoShuffleTrainingTestSplitter to create a single split without chainging the order, this will limit the size of test set and for smaller datasets reduce the robustness of the test set error/generalization error.

For this reason, this pull request introduces the TimeSeriesCrossValidation<T> class. Time series cross-validation is based on rolling validation, where the original order of the data is kept, and new observations in the test interval are predicted as hold-out samples and following included in the model one at a time. A nice illustration of this can be found here: [Cross-validation for time series](https://robjhyndman.com/hyndsight/tscv/). 

The TimeSeriesCrossValidation<T> class supports the following features:
 - InitialTrainingSetSize: Specify how much data the initial learner/model should use.
 - maxTrainingSetSize: Specify a max size for the training interval. If no max size is specified, this will correspond to an expading training interval. If a max is specified, this will correspond to a sliding training interval.
 - retrainInterval: Specify how often the model being validated should be retrained. If no interval is specified, the model will be retrained each time a new time step is predicted and included. If an interval is specified, the model will only be retrained at the specified interval and the existing model will be used for validation predictions inbetween the retrain intervals.

More information can be found in the documentation of the code and the unit tests.

A short example showing how to measure the mean square error using The TimeSeriesCrossValidation<T>class:

```c#
var tsCv = new TimeSeriesCrossValidation<double>(initialTrainingSize: 30);

// Calculate the validated predictions.
var timeSeriesPredictions = tsCv.Validate(new RegressionDecisionTreeLearner(), observations, targets);
// Get the targets corresponding to the validation predictions. 
var timeSeriesTargets = tsCv.GetValidationTargets(targets);

// Measure the mean square error
var metric = new MeanSquaredErrorRegressionMetric();
var mse = metric.Error(timeSeriesTargets, timeSeriesPredictions);
```


 "
Improve SequentialModelBasedOptimizer (now BayesianOptimizer),"While attending the NIPS 2017 conference I was lucky enough to have Frank Hutter, the author and co-author of several [Bayesian Optimization papers](http://ml.informatik.uni-freiburg.de/people/hutter/publications.html), explain me some insightful details about this type of optimization.

This pull request contains improvements to the SequentialModelBasedOptimizer, now renamed to BayesianOptimizer, based on some of the insights provided by Frank Hutter.

The main changes are:

- Model type changed from RandomForest to ExtraTrees. The important change here is how the split in the decision trees are calculated. RF: (v1 + v2)/ 2 vs. ET: random * (max - min)  + min, where random is between 0 and 1.
- Optimizer type for finding the maximum of the acquisition function changed to RandomSearchOptimizer. This should matter less, but compared to the ParticleSwarmOptimizer, this seemed to work better in practice.
- Refactored the BayesianOptimizer to be easier to extend with other model types, optimizers and acquisition functions in the future.
  - Currently the model type, optimizer type and acquisition function is hardcoded:
    - Model type: ExtraTrees
    - Optimizer type: RandomSearchOptimizer
    - Acquisition function: Expected improvement
  - A natural extension would be to make it possible to inject other types. Like a Gaussian Process instead of the ExtraTrees model and so forth. These changes will be included in a later pull request.

I ran a small test to illustrate the improvements, optimizing the hyper parameters of a classification decision tree learner on the [landsat satellite dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)) from UCI. As can be seen on the attached image, the updated BayesianOptimizer uses significant less function evaluations compared with the old implementation (SequentialModelBasedOptimizer), and the RandomSearchOptimizer, while also finding a better minimum.

![image](https://user-images.githubusercontent.com/9001637/34309947-faac473c-e754-11e7-8c50-e51d42c00670.png)
"
Fix for AccessViolationException in F64MatrixView and F64MatrixColumnView for large matrices,"#38 

Changed the pointer offsets applied in `F64MatrixColumnView.RowPtr(row)` and `F64MatrixView.this[row]` to use `long` instead of `int` - integer overflows were occurring when the length of the underlying `double[]` in `F64Matrix` passed `int.MaxValue / sizeof(double)`."
0.26.5.1: Add .net461 to target frameworks,"When using .net standard 2.0 class libraries from a .net framework application, all system dependencies will be copied to the build output. To avoid this, a net461 build has been added to the target frameworks. This means that future SharpLearning nuget packages will contain both a .netstandard2.0 build and a .net461 build.

This might change in the future if .net standard changes the way dependencies are handled."
Add contribution guide,"Adding a contribution guide to make it easier for other developers to contribute to SharpLearning. Some parts of the guide could use more details, but this should provide a start and make the contribution process more clear."
System.AccessViolationException when retrieving data from a large dataset,"In `F64MatrixColumnView.RowPtr`, integer overflow can occur when `row * m_strideInBytes` is larger than `int.MaxValue`, resulting in an invalid offset being applied to `m_dataPtr`:

    double* RowPtr(int row)
    {
        return (double*)((byte*)m_dataPtr + row * m_strideInBytes);
    }

I am happy to fix this (it just requires a cast to `long`), but I'm not sure how to contribute - do I branch from master, then push and create a pull request from the GitHub website? In the future, if I find a simple bug like this, should I raise an issue, or can I just push with details and let you decide whether it's a good fix?

I haven't contributed to an open source project before!"
Unnecessary System Files Generated,"For our project [MetaMorpheus](https://github.com/smith-chem-wisc/MetaMorpheus), after adding Sharplearning NuGet Package to our EngineLayer and TaskLayer, in the GUI WPF project, there is an excessive amount of unnecessary system dll files generated in the output folder after building (No matter release or debug). Here is a list of these [files](https://github.com/smith-chem-wisc/MetaMorpheus/issues/767#issuecomment-349014733). We really couldn't determine where is the problem since there is no trace in the .csproj files and references of GUI nor related projects. So please help us if you have any idea! Thanks a lot.
"
"CS0012	The type 'Object' is defined in an assembly that is not referenced. You must add a reference to assembly 'netstandard, Version=2.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.","Fresh download, after restoring packages via NuGet.

How to solve it????"
Backends prototyping,"Creating a PR for the backends stuff to more easily see the changes.

cc: @mdabros "
Unittests fail because of localization settings,"Some unittests compare against hardcoded strings written in the test method. These fail on systems that use ,(comma) as decimal separator instead of .(dot). These strings should probably be loaded from a resource or the entire library should work with invariant culture unless otherwise specified."
Predict overload for multiple observations added to IPredictor<TPrediction>,
Add TPrediction[] Predict(F64Matrix observations) to public interface IPredictor<TPrediction>,"I think it would be a good idea to add the `F64Matrix `overload to the `IPredictor `interface as it would make it easier to use the `IPredictorModel `interface in your code. The models seems to implement it already. 

It would add a dependency for `SharpLearning.Containers.Matrices` in `SharpLearning.Common.Interfaces`, but I think it is unlikely that you would use the SharpLearning library without referencing `SharpLearning.Containers.Matrices` anyway."
Duplicate efforts,"Hi @mdabros!

I've just found your library a couple days ago and couldn't help but notice the similarity between both of our projects, SharpLearning and [Accord.NET](https://github.com/accord-net/framework). Since we both share the same goal (bring serious machine learning to .NET), and instead of duplicating our efforts, wouldn't you be willing to join the Accord.NET project as well? 

Seeing your extremely well-organized repository and coding skills, you would be more than welcome in joining Accord.NET as one of its authors. 

Regards,
Cesar"
Change to unified project versioning,"After the migration from .net framework/desktop to .net core, I decided to switch to individual versioning for each project in SharpLearning. However, since vsts continuous integration/delivery currently does not support skipping already published nuget packages, I have decided to switch back to unified versioning across all projects. This is done differently with .net core projects compared to .net framework projects. I followed the advice from this answer on stackoverflow: [sharedassemblyinfo-equivalent-in-net-core-projects](https://stackoverflow.com/questions/42790536/sharedassemblyinfo-equivalent-in-net-core-projects). 

This solution also allows to have assembly attributes shared among the projects in one location."
Added dictionary version of KeyCombine which is a lot faster,moved unittests from CsvParserTests to CsvRowExtensionsTests
Add better error messages to learners when input data is not valid,"This pull request should provide better error messages and feedback from the learners when invalid input data is provided and solve issue #27. 

Added checks for all learners includes: 
 - Observations: Verify that row and column count is larger than zero.
 - Targets: Verify that row count is larger than zero
 - Observations and Targets: Verify that the row count of observations and targets are equal
 - Indices: Verify that there are no negative indices provided. Verify that the max index does not exceed the row count of observations and targets.  "
Better error messages from learners in case of dimensionality mismatch,"Currently, there are no checks to verify that the dimensions of the observation matrix and the target array matches before learning is started. This results in error messages from somewhere in the learner implementation, providing poor error messages and feedback to the user.

Checks should be added to all learners to ensure that the provided arguments and data is valid, before starting the learning process."
SharpLearning.Neural full .net core2.0/standard 2.0 support,"SharpLearning.Neural depends on [math.net](https://github.com/mathnet/mathnet-numerics), which does not currently support .net core 2.0/.net standard 2.0. Hence, SharpLearning.Neural will only work on .NET Desktop/Windows. 

Support for .net core 2.0/.net standard 2.0 is planned for math.net, so full support for SharpLearning.Neural will also be possible once this is implemented. Eventually, #9 might also solve this. "
Vsts continuous integration,"Merging the move to vsts continuous integration. This will also add continuous delivery, packing and pushing nuget packages with each commit to the master branch. The nuget steps are disabled for pull requests against the master branch."
Migrate everything to .NET Core 2 (.NET Standard 2.0),"In the end it was easier for me to start from scratch, so I didn't use the branch you had prepared (hopefully that's ok).

Here are a few things from the migration that I thought I should mention:
* I disabled the ""auto-generate AssemblyInfo"" feature of the new .csproj files so that existing `AssmeblyInfo.cs` continue to be used.
* I left the old `.nuspec` files in place.  These take precedence over the NuGet info in the new .csproj files.
* Updated the `SerializationString` value used in the `GenericBinarySerializer_Serialize` and `GenericBinarySerializer_Deserialize` tests to reflect the .NET Core behaviour of the `BinaryFormatter`."
AdaBoostLearners: Add subsample ratio pr. tree as hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, subSampleRatio, which controls how many training samples are forwarded to each tree in the ensemble. When subsampling is active, samples from the training data will be drawn with replacement, leading to more variation among the trees in the ensemble. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

In the RandomForest implementation of this feature, there is sampling with replacement, even if subSamplingRatio=1.0, this is part of the algorithms design. However, for the AdaBoost implementation of this feature, if subsampling is off (subSampleRatio=1.0), no sampling with replacement should be introduced, and the whole training set should be considered in each tree of the ensemble. This will result in the 'classic', AdaBoost algorithm, if the subSamplingRatio is set to 1.0. 

Besides the difference when subSampleRatio=1.0, the AdaBoost implementation should be very similar to the RandomForest implementation, which can be found here [RandomForest](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.RandomForest/Learners/ClassificationRandomForestLearner.cs)."
AdaBoostLearners: Add features pr. split as regularization hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, featuresPrSplit, which controls how many randomly selected features are considered during the decision trees search for a new split. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

Sine the DecisionTreeLearner used in adaboost already supports 'featuresPrSplit', the implementation should simply add the hyper parameter to the adaboost learner contructors and forward the parameter to the DecisionTreeLearner."
Better default parameters for DecisionTreeLearners,"Currently, the DecisionTreeLearners ([ClassificationDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/ClassificationDecisionTreeLearner.cs) and [RegressionDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/RegressionDecisionTreeLearner.cs)) does not have very good default parameters. With a maximumTreeDepth=2000, using the default paramters will, in most cases, result in a model that overfits the problem. Hence, a better set of default paramters should be found, that, in more cases results in a better regularized model."
Replace SharpLearning.Containers.Matrices.F64Matrix with multidimensional array,"In SharpLearning, the F64Matrix class, which is part of the [Learner interfaces](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Common.Interfaces), is mostly used as a container for holding the features for a learning problem. While SharpLearning does contain some [arithmetic extensions](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Containers/Arithmetic) for the F64Matrix, the arithmetic is not used by any of the learners. Also, more efficient implementations can be found in [Math.net](https://github.com/mathnet/mathnet-numerics). 

Therefore it might indicate, that the primary container for features in SharpLearning should rather be a standard .net type like multidimensional array (double[,]) or jagged array (double[][]), with some extension methods to add the current functionality of the F64Matrix. 

An alternative, also suggested in #6, would be to replace the F64Matrix directly by using Math.net as the matrix provider. However, since only the SharpLearning.Neural project is using matrix arithmetic and with the plan of using [CNTK as backend](https://github.com/mdabros/SharpLearning/issues/9), math.net is a large dependency to take, if only using the matrix class as a feature container. So currently, I am leaning more towards replacing F64Matrix with a standard .net type. However, to better handle integration between Math.Net and SharpLearning, maybe a separate project, SharpLearning.MathNet, could be added with efficient conversions between Math.net and SharpLearning containers (both copy and shared memory). This of course depends on what data structure ends up replacing F64Matrix, if any.

These are my current thoughts, and people are very welcome to discuss and pitch in with suggestions. 
"
Metrics: Consider adding support for sample weighted metrics,"When dealing with imbalanced data sets, it can be beneficial to use sample weighted metrics. This task should be split into several tasks, one for each metric, if sample weights are to be supported in the [metrics project](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Metrics).  "
LearningCurves: Add support for weighted learners,"Extend the [ILearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/ILearningCurvesCalculator.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
Implement support for sample weights in [LearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/LearningCurvesCalculator.cs). "
CrossValidation: Add support for weighted learners,"- Extend the [ICrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/ICrossValidation.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
- Implement support for sample weights in [CrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/CrossValidation.cs)."
Ensemble learners: Add support for sample weights,"Add support for sample weights to the Ensemble learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This task requires #14 to be done first, since the ensemble learners needs to be extended to also support weighted learners in the constructor.

Following the learners must implement weighted learner interfaces and should simply forward the sample weights the learners in the ensemble. The ensemble learners can be found here in the ensemble project: [Ensemble learners](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Ensemble/Learners)"
NeuralNet: Add support for sample weights,"Add support for sample weights to the NeuralNet learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This is currently on-hold until #9 has been decided."
Add IWeigtedLearner and IWeigtedIndexedLearner interfaces to Common.Interfaces,"Add interface for learners supporting sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights);
```

Add interface for learners supporting sample indices and sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights, int[] indices);
```
"
GradientBoost: Add support for sample weights,"Add support for sample weights to the GradientBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

The GBMDecisionTreeLearner, used by GradientBoost does not support sample weights, so adding sample weight support to the GradientBoost learners requires first adding it to the GBMDecisionTreeLearner. Adding sample weight support to the GBMDecisionTreeLearner, primarely requires using the wieghts in the loss functions: [GradientBoost Loss](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Loss)

Following, sample wieght support can be added to the GradientBoost learners. The learners can be found here in the GradientBoost project: [GradientBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Learners)"
ExtremelyRandomizedTrees: Add support for sample weights,"Add support for sample weights to the ExtremelyRandomizedTrees learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by ExtremelyRandomizedTrees, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project:
 [ExtremelyRandomizedTrees](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
RandomForest: Add support for sample weights,"Add support for sample weights to the RandomForest learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by RandomForest, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project: [RandomForest](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
AdaBoost: Add support for sample weights,"Add support for sample weights to the AdaBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by AdaBoost, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner in each boosting iteration. The learners can be found here in the AdaBoost project: [AdaBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.AdaBoost/Learners)

The work is currently in progress in the branch [adaboost-sample-weight-support](https://github.com/mdabros/SharpLearning/tree/adaboost-sample-weight-support)"
CNTK as backend for SharpLearning.Neural,"The Microsoft team working on [CNTK](https://github.com/Microsoft/CNTK) has recently released the initial version of the C#/.Net API with support for both evaluation and training of neural networks. A more feature complete version, with support for layers and other helpful features, should arrive before the end of the year. Currently, there seems to be a few performance related issues (https://github.com/Microsoft/CNTK/issues/2374 and https://github.com/Microsoft/CNTK/issues/2386) but hopefully these will be also be solved in the next release.

Using CNTK as backend for SharpLearning.Neural will add operators for more layer and network types, while also enabling GPU training and evaluation. Using a well supported deep learning toolkit as backend will also help to ensure that future operator, layer and network types will be available faster.

This task will require a large rewrite of SharpLearning.Neural, most likely only keeping the top level interface. However, since all the core operations are availible from CNTK, most of the hard work is already completed.

This task should be split into multiple others when a design of how CNTK should be integrated has been completed. A few considerations:
- Should the integrations be ""simple"", i.e. only have a NeuralNetLearner and NeuralNetModel in SharpLearning and use the layer construction and related functionality from CNTK directly?
- Should the integration hide CNTK behind an adapter to make it easier to support other deep learning toolkits like TensorFlow(Sharp)? "
.Net Core and .Net Standard support,"Add .NET Core and .Net Standard support to make SharpLearning available on more platforms. Porting to .Net Standard involves the following tasks:

- Retargeting the projects .NET Framework version to .NET Framework 4.6.2.
- Determining the portability of the code using API Portability Analyzer. This has been done, and only the GenericXmlDataContractSerializer from SharpLearning.InputOutput uses unsupported API calls.
- Change the implementation of GenericXmlDataContractSerializer to conform with .net standard 2.0. This is possible with the available API calls, however there are issues with serializing some of the Math.net containers used in the NeuralNet models. This might be solved together with #9, since CNTK will most likely replace math.net in the SharpLearning.Neural project.
- Change project format to .net core.

After the porting process the continuous integration on appveyor must be updated."
"Exception: Source array was not long enough. Check srcIndex and length, and the array's lower bounds","The following code throws a System.IndexOutOfRangeException on line 328 in GBMDecisionTreeLearner.cs

```
            var sut = new RegressionSquareLossGradientBoostLearner();

            Random rnd = new Random(42);
            var rows = 10000;
            var columns = 1;
            double[] values = new double[rows * columns];
            for (int i = 0; i < rows * columns; i++)
                values[i] = rnd.NextDouble();
            Containers.Matrices.F64Matrix observations = new Containers.Matrices.F64Matrix(values, 1, 10000);
            double[] targets = new double[rows];
            for (int i = 0; i < rows; i++)
                targets[i] = rnd.NextDouble();

            var model = sut.Learn(observations, targets);
```"
Math.NET Matrices,"This is a really great library. Was there a specific reason why you chose to roll your own Matrix class, rather than leveraging Math.NET?

Ideally I'd like to marry the two (not only for consistency with modules I've already written, but even for smaller things like using Matrix<float> rather than Matrix<double>). Before I jump in and start changing anything, though, I thought I'd check with the author to see if there was a specific reason behind it.

If I do proceed with integrating the two, more than happy to submit back a PR too, just let me know."
Thanks for developing and sharing this brilliant machine learning package in C#,
Please share your vision of .NET deep Learning,"@mdabros Pls apologize if I hijack your excellent work here.

Daniel from MSFT is gathering [a broad vision for .NET Deep Learning here.](https://github.com/Microsoft/CNTK/issues/960#issuecomment-315049580)  I think you may have unique view on this.  "
[Question] 2d - 3d output in neural networks,"Is it possible ? 
And if it's possible, how to train the network ?"
Feature Suggestion,"
First of all, I want to congratulate you for this project. I have a suggestion, couldn't figure where to write it other than issues on GitHub.

My suggestion is, number of observations (or better their indexes, one can count them) that fall to the left and right child of a node.
"
"Restructure repo, add scripts and nuget packaging",
请问数据集来源是？,
python demo_single.py ---->TypeError: can't pickle weakref objects,"Traceback (most recent call last):
  File ""demo_single.py"", line 36, in <module>
    pickle.dump(clf, f)
TypeError: can't pickle weakref objects
"
抱歉看错了,为什么二分类任务用categorical_crossentropy，多分类任务用binary_creossentory
predict的label严重不准确的问题,"道友你好~
使用DEMO的法律数据进行测试发现,predict的label几乎(95%以上)全部是一样的
我查看了前三位的索引值
prediction.argsort()[-3:][::-1]
类似:
[139,34,55]
[139,34,55]
........
[139,55,34]

基本完全一样

我发现prediction内的值是不一样的,但最高的几个值都是在固定位置
换成我自己用的数据(标签更丰富)进行测试,同样的问题,每次都还是几个固定的label
不明白是哪里出了问题





"
验证,可以画个roc-auc曲线对模型的性能做个评估
请问一下，Ali打标签中，org_code指的是什么？,是regionID还是？
Gutenberg file naming changed,"I believe the files changed a little bit on the Gutenberg website. 
The ASCII versions are now (mostly) found under an ""old"" folder. 
Also, this might be due to how my internet is set-up locally, but downloading via http creates empty files, hence I changed to https.

For the Pride and Prejudice book I couldn't find the same file, This one seems to be in UTF-8. Nonetheless everything seems to be fine with the pushed code. 

Best,"
Add soft K means,"http://www.inference.org.uk/mackay/itprnn/ps/284.292.pdf
http://www.inference.org.uk/mackay/itprnn/code/kmeans/

Example code:
```q
\l funq.q
\l iris.q
X:iris.X
\S -314159i
c:.ml.forgy[3] X
/ 
these all return the same answer (within epsilon 5e-5)
5.006    3.428    1.462    0.246   
5.901613 2.748387 4.393548 1.433871
6.85     3.073684 5.742105 2.071053
\
asc flip .ml.kmeans[X] over c
asc flip .ml.kmeanss[X] over c
asc flip .ml.kmeans1[100;X] over c1
```"
How to compile the code?,"I work under win10, win32-bit q . Useing MSYS2 to Compile., which is armed by the following packages:

- tar
- make
- patch
- mingw-w64-i686-gcc-fortran

under the target folder, type make much errors appear, as the attach file. NEED HELP!
[error.txt](https://github.com/psaris/funq/files/6866296/error.txt)
"
possible typo in book,"First congrats on the book! Looks great, very excited to go through it. Not sure if this is the right place to report typos but on p 16, section 2.4 in it refers to ""3...independent features (petal and sepal measurements..."" and ""4...dependent feature (species label)"" and then at the bottom of the page it says ""we have access to the independent (species labels) and dependent (sepal and petal measurements)""  The independent/dependent labels are reversed. I believe the first labels are correct but figured i'd let you know."
fixed weighted odds,Added missing multiplication operation.
Take advantage of symmetry of cNk (Pascal),More stable and faster for large n
fixing typos,
fix typos in comments,
fix typo in comments,
fix two typos in comments,
fix two typos,
Remove trailing whitespace,
fix correlated matrix in linreg.q,"was `X[0]` which produced results for `rho` as if it was `1-rho`. 
Test: plot for `rho:1f` should show a line but was showing circle"
Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5×3 positions and 5×4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5×3 positions and 5×4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4×3 positions and 4×4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter network。
"
Problem in training the segmentation network using the provided dataset,
Pose estimation has huge position and orientation error for one object and does not appear to decrease,"Hello,

So I generated my own training set and am attempting to train the pose estimation portion with just one object, I was able to go through all the steps to get the CAD model properly setup, the PCD files and do all the stuff with the redis-server. I ended up attempting to train the model, and after 3000 epochs, I find the error is quite large, roughly the same as what it was at the start (37889.62 m for position, 127.81 m for orientation). Any advice on what I might be doing incorrectly?
"
How can I get ground truth pose and the segmentation mask?,"Hi Jimmy

I have already downloaded the oil change dataset, I would like to ask how to get the ground truth poses and the segmentation masks from the .json file? (specifically, I want to get the ground truth pose and the segmentation mask of the blue oil funnel)

Thank you so much!"
Why do we need blender for demo?,"Hi, thanks for your great work. I wanted to see the results of your work, i.e. I will give a RGB image and expect 6dposes. I believe that blender was used to create the synthetic dataset for training Pose interpreter network. My question is why do we need blender for seeing the results.

My other question is if we require blender then how to use it on some online platform like google collab? "
end_to_end_eval position estimation (own dataset),"Hey there,

when I evaluate the data using eval.ipynb on the small images, which where used for training the pose estimation network, the position error is abot 1.5 cm. So that's good.

But when I use the end to end evaluation, the position error ist about 2 m. The error is only this big in the z coordinate. The x and y coordinates are fine.

Do you have any idea why it is estimating the position an the small pictures right, but on the masks it is getting from the segmentation network wrong.

Thanks in advance!

Kind regard!"
steps to train model again with another DRN,"Hello, 
I want to train another drn model on the same dataset. Please guide me through steps for end to end evaluation. Also, I can see DRNSeg being called during training. But where is drn22 being called by DRNSeg ?"
I think [this line](https://github.com/jimmyyhwu/pose-interpreter-networks/blob/master/pose_estimation/render_pose.py#L6) is supposed to take care of the import problem. I did not have to add any workaround to run the notebooks. Maybe you could try opening `jupyter notebook` in the same directory that the notebook is in?,
Rendering folder calling issue,"the code for segmentation works fine but creates a problem for me in end_to_end_visualization.py and pose_estimation/demo.py in importing 
from dataset import oilchange_scene

I think when at folder pose_estimation it needs to access oilchange.py file which is in dataset folder, and from folder I cannot import this way. Hence, it shows and error. Please suggest a work around for this."
pose estimation demo runtime error,"While running demo.ipynb the following error comes up. I am unsure is it the sound card error or CUDA error ? Please help.

ALSA lib confmisc.c:768:(parse_card) cannot find card '0'
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory
ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default
AL lib: (EE) ALCplaybackAlsa_open: Could not open playback device 'default': No such file or directory
found bundled python: /opt/blender/2.79/python
Import finished in 1.6221 sec.
Fra:1 Mem:56.16M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Creating Shadowbuffers
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Raytree.. preparing
Fra:1 Mem:91.02M (0.00M, Peak 91.02M) | Time:00:00.10 | Raytree.. building
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Raytree finished
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Creating Environment maps
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Caching Point Densities
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Loading voxel datasets
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Volume preprocessing
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:91.04M (0.00M, Peak 143.57M) | Time:00:01.32 | Scene, Part 13-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 3-20
Fra:1 Mem:91.37M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 11-20
Fra:1 Mem:91.30M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 9-20
Fra:1 Mem:91.21M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 7-20
Fra:1 Mem:91.48M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 19-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 15-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 5-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 18-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.40 | Scene, Part 10-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 1-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 4-20
Fra:1 Mem:91.91M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 17-20
Fra:1 Mem:91.82M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 2-20
Fra:1 Mem:91.36M (0.00M, Peak 143.57M) | Time:00:01.43 | Scene, Part 14-20
Fra:1 Mem:90.95M (0.00M, Peak 143.57M) | Time:00:01.44 | Scene, Part 8-20
Fra:1 Mem:90.57M (0.00M, Peak 143.57M) | Time:00:01.45 | Scene, Part 12-20
Fra:1 Mem:90.19M (0.00M, Peak 143.57M) | Time:00:01.46 | Scene, Part 20-20
Fra:1 Mem:89.99M (0.00M, Peak 143.57M) | Time:00:01.49 | Scene, Part 16-20
Fra:1 Mem:89.28M (0.00M, Peak 143.57M) | Time:00:01.56 | Scene, Part 6-20
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Determining resolution
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Initializing execution
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Tile 1-2
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.58 | Compositing | Tile 2-2
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Compositing | De-initializing execution
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Sce: Scene Ve:126775 Fa:253749 La:1
Saved: '/tmp/tmpz3aqpi1j/render.png'
 Time: 00:01.75 (Saving: 00:00.05)


Blender quit
Traceback (most recent call last):
  File ""<stdin>"", line 14, in <module>
  File ""<stdin>"", line 3, in visualize_batch
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 112, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 52, in forward
    x = self.resnet18(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 199, in forward
    x = self.conv1(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDNN_STATUS_MAPPING_ERROR"
No such file or directory: 'data/OilChangeDataset\\annotations\\val_20171103_OilChange.json',"I can not unzip the oil change dataset. Its showing corrupted file. 
Kindly help, please !!!!"
blender linking issue,"It seems to be easy but I am facing trouble with the process of linking blender. Kindly let me know where am I going wrong.
Step 1:  I downloaded blender-2.79b-linux-glibc219-x86_64.tar.bz2  from the link : https://download.blender.org/release/Blender2.79/

Step 2: created an empty directory blender in /usr/local/bin/blender 

Step 3: extracted .tar file at the downloaded location.

Step 4: to link the binary file : lm -s /blender_download_folder/blender /usr/local/bin/blender

This gives me an error ln: failed to create symbolic link '/usr/local/bin/blender/blender': File exists. I could not find any executable file in the downloaded folders. Please let me know if I am wrong or missing out something.

"
end_to_end_visualize.ipynb HTTP 403 forbidden error ,"I loaded all the pretrained models and dataset required. While running end_to_end_visualize.ipynb I get an error at line 
segm_model = segm_models.DRNSeg(segm_cfg.arch, segm_cfg.data.classes, None, pretrained=True)

Downloading: ""https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth"" 
 line 65, in load_url
    _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
in _download_url_to_file
    u = urlopen(url)
  File ""/usr/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.6/urllib/request.py"", line 532, in open
    response = meth(req, response)
  File ""/usr/lib/python3.6/urllib/request.py"", line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/usr/lib/python3.6/urllib/request.py"", line 570, in error
    return self._call_chain(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
"
Issue with Segmentation Eval 403 Forbidden,"Hi Jimmy,

I have downloaded the pretrained datasets, however when I attempt to run the segmentation eval Jupyter notebook. I receive an error in the third block. It seems that the code is downloading a checkpoint from a princeton website. Attempting to visit this site gives me a 403 Forbidden error. The address is: https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth. 

Do users need special permission for this?

Thanks"
ROS Dependencies,"in order to run end_to_end_visualize.ipynb  I installed all the packages from conda and tried to run the ipynb file. Earlier it gave me pypcd error. I cloned the github repo of the same. But again got stuck somewhere and got an error 
RuntimeError: invalid hash value (expected ""4bd2f8ea"", got ""f22d7c4219e69774cf0e6a9ebcaeb1a200420ae084036bf122712107be6576bb"")

Please let me know how to resolve these ROS dependency and make the code run just to see how it works."
Problem about evaluation with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object and I trained it, I had problems because my images hadn't the same size as others images used for models pretained, so I changed the train code and it worked. But now I have problems when I try to use eval.ipynb, at the line model.load_state_dict(checkpoint['state_dict']) I have this error :
RuntimeError: Error(s) in loading state_dict for DataParallel:
size mismatch for module.fc1.weight: copying a param of torch.Size([256, 40960]) from checkpoint, where the shape is torch.Size([256, 86528]) in current model.
size mismatch for module.fc_p1.weight: copying a param of torch.Size([15, 256]) from checkpoint, where the shape is torch.Size([3, 256]) in current model.
size mismatch for module.fc_p1.bias: copying a param of torch.Size([15]) from checkpoint, where the shape is torch.Size([3]) in current model.
size mismatch for module.fc_o1.weight: copying a param of torch.Size([20, 256]) from checkpoint, where the shape is torch.Size([4, 256]) in current model.
size mismatch for module.fc_o1.bias: copying a param of torch.Size([20]) from checkpoint, where the shape is torch.Size([4]) in current model.

I don't understand how I could change these sizes.

Thanks in advance.

"
Problem about pose_estimation training with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object to train the pose estimation. When I ran train.py there is the error : 
RuntimeError: invalid argument 2: size '[-1 x 40960]' is invalid for input with 2768896 elements at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THStorage.cpp:80

I went in the debug mode and I found that the problem is that the matrix x in models.py file at line 52 don't have the right size. Normally I should have matrix of shape [32,512,8,10] but I have [32,512,13,13] when I look x.shape. I don't understand how this matrix is created and so where does the problem come from. 

Could you give me any suggestion?

Thanks in advance."
Problem about orientation_error of validation set is extremely high compare to train set.,"Hi!
I used my own dataset which contains four objects to train the pose_interpreter_network and I found the orientation_error of val dosen't decline well when error of train set converged as the chart below.

![火狐截图_2019-05-07T11-28-27 356Z](https://user-images.githubusercontent.com/38148405/57296727-44d10b00-7100-11e9-9665-a0cd94afaaa1.png)

Consequently, the result of end_to_end_eval is unsatisfactory as well. Despite the positions of object are predicted correctly, most the orientations are wrong.
Could you give me any suggestion?
Thanks in advance."
Error when training pose_estimation about load pcd file.,"Hi, Jimmy. 
I have made my own dataset that contains 4 objects and pcd files. When I run:

> python train.py config/kinect1_mask.yml

There is an error occured:

> Traceback (most recent call last):
  File ""train.py"", line 295, in <module>
    main(cfg)
  File ""train.py"", line 215, in main
    criterion = PointsL1Loss(numpy_pcs).cuda()
  File ""train.py"", line 63, in __init__
    self._pcs = torch.from_numpy(np.array(numpy_pcs)).cuda()
ValueError: could not broadcast input array from shape (4,38720) into shape (4)

There are 38720 points in my first pcd file, and the format is same with your pcd files. It's like:

> VERSION 0.7
FIELDS x y z
SIZE 4 4 4
TYPE F F F
COUNT 1 1 1
WIDTH 38720
HEIGHT 1
VIEWPOINT 0 0 0 1 0 0 0
POINTS 38720
DATA ascii
0 0.00193 0
0 0 0
0.0019499999 0 0
0.0038999999 0 0
0.0058499998 0 0
0.0077999998 0 0
0.0097500002 0 0
...

My config file is like :

> data:
        root: data/kinect1_mask
        pcd_root: objects
        num_subsets: 10
        val_subset_num: 101
        objects: [
            box,
            bottle,
            cola_can,
            power_bank
        ]
        batch_size: 32
        workers: 4
arch:
        num_input_channels: 1
        num_shared_fc_layers: 1
        num_shared_fc_nodes: 256
        num_position_fc_layers: 1
        num_position_fc_nodes: 256
        num_orientation_fc_layers: 1
        num_orientation_fc_nodes: 256
loss: points # l1 | posecnn | points_simple | points
optimizer:
        lr: 0.01
        lr_decay_epochs: [700, 1400]
        momentum: 0.9
        weight_decay: 0.0001
training:
        logs_dir: logs/
        checkpoints_dir: checkpoints/
        experiment_name: kinect1_mask
        print_freq: 10
        checkpoint_epochs: 100
        epochs: 2100
        log_dir:
        resume:

Could you give me any suggestions?
Thanks in advance."
About ‘end to end eval’ error,"I add get_filtered_cat_ids() in the ‘datasets.py':

> def get_filtered_cat_ids(coco, img_ids):
    object_instances = coco.loadAnns(coco.getAnnIds(imgIds=img_ids))
    def catgory_filter():
        return lambda x: x['category_id'] in [1, 2, 4, 5, 6]
    return [object_instances for object_instances in filter(catgory_filter(), object_instances)]

Then I changed the __init__() in ’EvalDataset‘class:

> def __init__(self, data_root, ann_file, camera_name, object_names, transform):
        self.data_root = data_root
        self.coco = COCO(os.path.join(self.data_root, 'annotations', ann_file))

        img_ids = get_filtered_img_ids(self.coco, camera_name)
        self.object_instances = get_filtered_cat_ids(self.coco, img_ids)

        self.object_names_map = {cat['id']: cat['name'] for cat in self.coco.dataset['categories']}
        #self.object_indices_map = {object_name: i for i, object_name in enumerate(object_names)}
        self.object_indices_map = {'blue_funnel':6,'funnel':4,'oil_bottle':1,'fluid_bottle':2,'engine':5}
        self.object_ids_map = {cat['name']: cat['id'] for cat in self.coco.dataset['categories']}
        #self.object_ids_map = self.object_indices_map

        self.transform = transform

The 'end to end eval' can read the dataset:
> loading annotations into memory...
Done (t=0.72s)
creating index...
index created!
using camera: kinect2

But, I get thsi error:

> RuntimeError                              Traceback (most recent call last)
<ipython-input-11-fed55fe84c4c> in <module>
      3 with torch.no_grad():
      4     for input, target, object_index, object_id in tqdm(val_loader):
----> 5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)
      7         orientation_errors.extend(orientation_error)

<ipython-input-10-ebee32e8a8cf> in forward_batch(model, input, target, object_index, object_id)
      5 
      6     position, orientation = model(input, object_index, object_id)
----> 7     print(target)
      8     position_error = (target[:, :3] - position).pow(2).sum(dim=1).sqrt()
      9     orientation_error = 180.0 / np.pi * pose_utils.batch_rotation_angle(target[:, 3:], orientation)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/tensor.py in __repr__(self)
     55         # characters to replace unicode characters with.
     56         if sys.version_info > (3,):
---> 57             return torch._tensor_str._str(self)
     58         else:
     59             if hasattr(sys.stdout, 'encoding'):

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in _str(self)
    254             suffix += ', dtype=' + str(self.dtype)
    255 
--> 256         formatter = _Formatter(get_summarized_data(self) if summarize else self)
    257         tensor_str = _tensor_str(self, indent, formatter, summarize)
    258 

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in __init__(self, tensor)
     80 
     81         else:
---> 82             copy = torch.empty(tensor.size(), dtype=torch.float64).copy_(tensor).view(tensor.nelement())
     83             copy_list = copy.tolist()
     84             try:

RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorCopy.cpp:70
"
a question about making own datasets ,"Hello, I now mark each image in my dataset with ”labelme“ and convert the annotation.json format to coco format, but these files are one by one, how to properly merge a annotation file, just like the annotations file you provide in Oilchangedatasets?
![image](https://user-images.githubusercontent.com/34231078/56178671-a805db00-6035-11e9-95d3-7ed905db812d.png)

"
"Error about ""end to end pose estimator"" use ""kinect2"" in Oilchange datasets ","When testing “end to end system” in Oilchange dataset，I used “kinect2“，but  I met  the key error. 

> KeyError                                  Traceback (most recent call last)
<ipython-input-169-fed55fe84c4c> in <module>
      2 orientation_errors = []
      3 with torch.no_grad():
----> 4     for input, target, object_index, object_id in tqdm(val_loader):
      5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py in __iter__(self, *args, **kwargs)
    219     def __iter__(self, *args, **kwargs):
    220         try:
--> 221             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    222                 # return super(tqdm...) will not catch exception
    223                 yield obj

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)
   1020                 """"""), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1021 
-> 1022             for obj in iterable:
   1023                 yield obj
   1024                 # Update and possibly print the progressbar.

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in <listcomp>(.0)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/codedisk/dsl_py/pose-interpreter-networks/datasets.py in __getitem__(self, index)
     50 
     51         object_name = self.object_names_map[ann['category_id']]
---> 52         object_index = self.object_indices_map[object_name]
     53         object_id = self.object_ids_map[object_name]
     54 

KeyError: 'oilfilter'

How to read  annotations in datasets which only  used for pose estimation?"
About trainning pose estimator ,"Hello, I changed the model in pose estimator from  resnet18 to  resnet50, I use the points loss, but when I train the estimator on th mask dataset, the orientation loss does not converge，could you give me some suggestion？
![image](https://user-images.githubusercontent.com/34231078/56078641-fb511100-5e1c-11e9-9bcd-8bac773f74c1.png)
![image](https://user-images.githubusercontent.com/34231078/56078642-fe4c0180-5e1c-11e9-9361-346168bec827.png)
![image](https://user-images.githubusercontent.com/34231078/56078643-0015c500-5e1d-11e9-81e7-800a13289c46.png)
"
Error about ros package,"Hello, when I run ""roslaunch pose_interpreter_networks pose_estimator.launch"", I met this error, as follow:

> [ERROR] [1554901200.082931]: bad callback: <bound method Subscriber.callback of <message_filters.Subscriber object at 0x7fb60d16e910>>
Traceback (most recent call last):
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/topics.py"", line 750, in _invoke_callback
    cb(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 75, in callback
    self.signalMessage(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 224, in add
    self.signalMessage(*msgs)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/home/dsl/catkin_ws/src/pose_interpreter_networks/src/pose_estimator.py"", line 117, in callback
    segm, object_names, positions, orientations = self.model(input)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 124, in forward
    return self.gather(outputs, self.output_device)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 136, in gather
    return gather(outputs, output_device, dim=self.dim)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 67, in gather
    return gather_map(outputs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
TypeError: zip argument #1 must support iteration

How to do to solve this problem?
"
How to use the pretrained object pose estimation model for end to end eval.,"Hi, Jimmy.
When I loaded the checkpoint of floating_kinect1_object in the end_to_end_eval.ipynb. There is a error 

> RuntimeError: Given groups=1, weight[64, 3, 7, 7], so expected input[1, 1, 240, 320] to have 3 channels, but got 1 channels instead

So I wander could the object chekpoint be used to end_to_end_eval/visualize.
Thanks you."
Where to download provided val?,"Hello, in poseprocess_wraper.py:
“parser.set_defaults(process_val=False)  # should download provided val set to match numbers in paper”
Where to download val set??
I set process_val=True to create val dataset ， but when I train my pose estimator：
![image](https://user-images.githubusercontent.com/34231078/55678340-6d5fbc80-592a-11e9-99bc-62128ddebdf1.png)
I use kinect2.  I just foget to change ""experiment_name"""
Issue about  training pose estimation models.,"Hi, Jimmy.
When I ran 

> python train.py config/floating_kinect1_mask.yml

There was a error

> File ""/home/huo/virtuallist/py36/lib/python3.6/site-packages/pypcd/pypcd.py"", line 282, in point_cloud_from_fileobj
> if ln.startswith('DATA'):
> TypeError: startswith first arg must be bytes or a tuple of bytes, not str 

came out.

It seems because of loading pcd file incorrectly. I have tried to use the two fomat of pcd file in binary and ascii, but the same error still occurs. And if I modified the code `ln.startswith('DATA')` with `ln.startswith(b'DATA')`, there will be a error 

> File ""/home/huo/virtuallist/py36/lib/python3.6/re.py"", line 172, in match
>     return _compile(pattern, flags).match(string)
> TypeError: cannot use a string pattern on a bytes-like object

come out subsequently.

Could you please give some adcices to solve the peoblems above?
Thank you."
Segmentation training issues: StopIteration,"HI! Thank you for the great job.
I created a json file of my own annotated data by following the coco mask.py.
The part of the json file as below: 

> {...,
> ""image"": [{""license"": 0, ""file_name"": ""0000007149_rgb.png"", ""coco_url"": """", ""height"": 480, ""width"": 640, ""date_captured"": 1544011401.0, ""camera_id"": 0, ""flickr_url"": """", ""id"": 0},...],
> ""annotations"": [{""segmentation"": {""size"": [480, 640], ""counts"": ""QgY37h>f0ZOe0\\Od0\\Od0\\Od0[Of0[Oc0]O`0@3M2N2N3M2N2N2N3L3N2N2N3M2N2N2N2N1O001O0O2O0000001O000000O1000O10000000O10000O100000O0100000000O100000000O1000O10O1000000O100000000O0100000O100000000O1000O10O1000000O1000000O1000O1000O1000000O1000000O10O100000O1000000O10000000O0100000000O1000000O1000O10O100000000O100000O0100000000O10000O100000001N1000001N10001O0O101O000O2O0000001N101hMfFmLn0a0m;]OUDa0U=01N2O1O1O1N101O1O1N2O1O1N2N2MQeW3""}, ""area"": 34310, ""pose"": {""position"": {""x"": -0.0007623627074502259, ""y"": 0.34552469760243687, ""z"": 1.0687215939143497}, ""orientation"": {""x"": 0.7472581634432386, ""y"": -0.14419430357527752, ""z"": 0.10985900520900949, ""w"": 0.6393310871202543}}, ""iscrowd"": 0, ""image_id"": 0, ""bbox"": [225.0, 242.0, 193.0, 219.0], ""category_id"": 1, ""id"": 0},...],
> ""categories"": [{""supercategory"": ""objects"", ""mesh"": ""charge_pile.stl"", ""id"": 1, ""name"": ""charge_pile""}]}

Then I modified the config file(drn_d_22_ChargePile.yml) and utils.py related to the object classes in ""segmentation"" folder.
But when I run

>  python train.py config/drn_d_22_ChargePile.yml

There is a error ""StopIteration"" came out at:
`first_input_batch, first_target_batch = iter(val_loader).next()` in the train.py:217
and RuntimeWarning: invalid value encountered in true_divide at:
`return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))` in the utils.py:25
It seems my dataset wasn't loaded correctly.
Can you give me any suggestions about how to solve these problem？Thanks."
How to Create a RGB segmentation dataset for your environment,"How, I am a tiro，I don‘t konw how to create a RGB segmentation datase for translate learning. I just use labelme to buile .json file for  a single RGB image, just like this  
![image](https://user-images.githubusercontent.com/34231078/54470325-cd0df080-47e0-11e9-9c0b-be1a38439b1d.png)
However, It didn't like your annotation file

![image](https://user-images.githubusercontent.com/34231078/54470333-e747ce80-47e0-11e9-8502-fcfb4adc1b4a.png)
"
blender in pose estimation,"Hello, when I run the demo, visualization, and evaluation code in ""pose estimation"", the program has been rendering in the background of the blender, unable to proceed to the next step, and there has been no result. I encountered this problem for the first time and put the “mesh” folder in the Oil Change dataset. The permissions of the .stl file in the file are changed to executable files, which can run normally. 
However, after a few days, I met the same problem that the program has been running can not produce results, I don't know how to solve this time."
Problem about training on LineMod dataset,"I want to train your network on linemod dataset, however, I have question writing the config file such as 
20171103_OilChange.json. I cannot understand the messages in the json file like the following...

{""segmentation"":{""counts"":""mYVe08]Q14M4L3L4N1N3M3L3N3N1M4N2L3N3mLZOPUOh0jj0ElTO?Qk0HhTO;Tk0LfTO7Wk02aTO0\\k08\\TOKbk0?STODjk0j0hSOYOUl0W3O1N2O00001O00O100O1O1O1O1O1O1N2O1O1O1O1O1O100O1O1O1O1O1O100O1O1O1O1O1O1O1O1O1O1O1O1O1000000O10000000000O100000000000000001O000000001O000000000000001O0000000000000000O10000O100O1O1O1N2O1O1O1O1O1N2O1O1N2N2O1O1O1O1O1N2O1O1N2O1O1O1N2O1O1O1N2O1O1O1O1O1N2N2O1O1N200O100O100O100O100O100O1O1N3O2J6D<E`0Ca0\\Oc0BQ[\\T1"",""size"":[1080,1920]},""area"":21399,""pose"":{""position"":{""y"":-0.3352892126408012,""x"":-0.1564572835692801,""z"":0.7772331158642385},""orientation"":{""y"":0.9502802730465292,""x"":-0.303411278174998,""z"":-0.01268932429862474,""w"":0.06890558746337358}},""iscrowd"":0,""image_id"":0,""bbox"":[643.0,0.0,173.0,179.0],""category_id"":6,""id"":0},

Could you please explain how did you create these .json files and what should I do to my own dataset such as LineMod to get these segmentation ground truth?
Thank you !!!


"
A bug,"File ""train.py"", line 52, in train
    output = model(input)
RuntimeError: CuDNN error: CUDNN_STATUS_MAPPING_ERROR
I run the code and It seems a bug here."
A BUG,"I run the code and there's a  ""CUDNN_STATUS_MAPPING_ERROR"" 
File ""train.py"", line 52, in train
    output = model(input)
it seems from here.

"
PermissionError: Permission denied: '/usr/local/bin/blender',"When I used anaconda3's python to run end_to_end_visualize.ipynb, this problem occurred. I copied the system's installed Blender library file to my anaconda3/share/ folder, but it didn't work. I don't know how to solve this problem.

The error message is as follows：
---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
<ipython-input-11-a2f62c5336e7> in <module>()
     21         all_objects = np.zeros_like(resized_image)
     22         for i, object_name in enumerate(object_names):
---> 23             single_object = pose_renderers[object_names[i]].render(positions[i], orientations[i])
     24             all_objects = np.maximum(all_objects, single_object * object_colors[object_names[i]])
     25         rendered_pose = (1 - alpha) * resized_image + alpha * all_objects

~/pose-interpreter-networks/pose_estimation/utils.py in render(self, position, orientation)
     89                                    str(self.camera_parameters['p_x']), str(self.camera_parameters['p_y']),
     90                                    str(self.camera_scale),
---> 91                                    position, orientation])
     92             assert ret == 0
     93             image = np.asarray(Image.open(output_path))

~/anaconda3/lib/python3.6/subprocess.py in call(timeout, *popenargs, **kwargs)
    265     retcode = call([""ls"", ""-l""])
    266     """"""
--> 267     with Popen(*popenargs, **kwargs) as p:
    268         try:
    269             return p.wait(timeout=timeout)

~/anaconda3/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    707                                 c2pread, c2pwrite,
    708                                 errread, errwrite,
--> 709                                 restore_signals, start_new_session)
    710         except:
    711             # Cleanup if the child failed starting.

~/anaconda3/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)
   1342                         if errno_num == errno.ENOENT:
   1343                             err_msg += ': ' + repr(err_filename)
-> 1344                     raise child_exception_type(errno_num, err_msg, err_filename)
   1345                 raise child_exception_type(err_msg)
   1346 

PermissionError: [Errno 13] Permission denied: '/usr/local/bin/blender'"
Support for kernel mean embeddings and kernels on distributions,"My work involves embedding distributions in RKHS using [kernel mean embeddings](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions) or equivalently kernels on distributions [Ch. 2, especially the generalized RBF kernel](http://reports-archive.adm.cs.cmu.edu/anon/2016/CMU-CS-16-128.pdf).

Are there any plans of extending this library to this case (or other structured domains for that matter)?

In the near future I'll be implement algorithms that uses this and if this sounds interesting I'd be happy to clean up and contribute the resulting code!

Thanks for the hard work :)"
Using GPU to accelerate the eigen decomposition in the psuedo-inverse calculation in nystrom.jl,"Here is my implementation:
```julia
using CuArrays, CUDAnative, LinearAlgebra, MLKernel

function make_symmetric(A::Mat, uplo::Char='U') where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    return LinearAlgebra.copytri!(A |> Matrix{T}, uplo)
end

function nystrom_inv_gpu!(A::Mat) where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    A = cu(A)
    vals, vectors = CuArrays.CUSOLVER.syevd!('V', 'U', A)
    tol = eps(T)*size(A,1)
    max_eig = maximum(vals)
    # for i in eachindex(vals)
    #     vals[i] = abs(vals[i]) <= max_eig * tol ? zero(T) : one(T) / sqrt(vals[i])
    # end
    predicate = one(T) .* (vals .>= max_eig * tol)
    vals .= predicate .* CUDAnative.rsqrt.(vals .^ predicate)
    QD = CuArrays.CUBLAS.dgmm!('R', vectors, vals, vectors)
    W = CuArrays.CUBLAS.syrk('U', 'N', QD)
    return make_symmetric(W)
end
```"
nystrom factorization for sparse matrix,"``` julia
fac = nystrom(k, X)
```
```
ERROR: MethodError: no method matching nystrom(::PolynomialKernel{Float64,Int64}, ::SparseMatrixCSC{Float64,Int64})
Closest candidates are:
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}) where T<:Union{Float32, Float64} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}, ::Array{U<:Integer,1}) where {T<:Union{Float32, Float64}, U<:Integer} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
```"
add constructor for RBFKernel,"Hi, folks

I'm just wondering if it make more sense to use the convention in wiki ([here](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)) instead of just define it as an alias

I found it a little bit confusing (not very intuitive) while searching wiki, if this name does refer to RBF kernel, since it is completely the same to `SquaredExponentialKernel`..."
Add missing negative signs in the documentation,In the documentation of the exponential kernels some negative signs are missing.
ForwardDiff.jl Compatibility,"[ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) has the following constraints:

  - [ ] The target function can only be composed of generic Julia functions. ForwardDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of Ax_mul_Bx-style functions.
  - [ ] The target function must be written generically enough to accept numbers of type T<:Real as input (or arrays of these numbers). The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well."
Implement Automatic Relevance Determination,"Implement Automatic Relevance Determination as described in Rasmussen (page 2 of pdf, 106 of text):

> http://www.gaussianprocess.org/gpml/chapters/RW5.pdf"
ARD Implementation and Automatic Differentation compatibility,"Hello,

I made relatively heavy changes to your package with the objective to be able to have Automatic Relevance Determination and to be compatible with Automatic Differentation packages such as ForwardDiff.jl.

I changed the way distances were computed by directly applying the scale factor (appropriately) to the vectors. It might make the algorithm slower, I have not made benchmarks so far.
However it is still retrocompatible and using a scalar scaling work with everything.

I did not add tests yet for the ARD case, and I had to remove some cause of the needed transformation of the parameters (in the ""Testing constructors"" part) cause I did not know if the best option is to keep a copy of the original parameters or ignore this issue. Let me know if you would like other modifications.

Cheers, "
Remove hyperparameters,Addresses #65 
Remove integer parameter from PolynomialKernel,"Having a second parameter complicates conversions and adds little value since parameter `d` is typically small (usually 3). Therefore, the practical set of values that would be used can be represented by a floating point number."
Remove LinearKernel,The LinearKernel is used infrequently (never) and can be modelled with the PolynomialKernel with d=1.
Remove PeriodicKernel,"This is a construction of the form k(f(x), f(y)) which can be left to data preprocessing."
Rename GammaRationalKernel to GammaRationalQuadraticKernel,Keep naming consistent with Rasmussen's Gaussian Processes book where the extension of the Exponential Kernel is referred to as the Gamma Exponential Kernel
Error tagging new release,"The tag name ""0.4.0"" is not of the appropriate SemVer form (vX.Y.Z).
cc: @trthatcher"
Dev,
Wrong metric for Matern kernel,"I am only half sure about what I advanced but I think the metric used for the Matern Kernel is not correct. Currently it is using a Squared Euclidean (hence ||x-x'||^2).
But in the documentation (and in the definition of the Matern kernel according to Rasmussen and Williams) the distance used is the Euclidean distance (||x-x'||), also in the documentation the equation of Matern kernel should have the 2's in the square roots. 
"
Issue with new release,"I tried to `add` the package with Julia 1.0.1 but get the following error :
```julia
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ├─possible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ├─restricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 └─restricted by julia compatibility requirements to versions: uninstalled — no versions left
```

I think it has to do with the tag version in Project.toml compared to the actual github release tag"
Dev,
Dev 2,
Dev,Refactored code to support Documenter.jl
Update dependencies for 1.0 compatibility.,"Installing MLKernels on Julia 1.0 currently fails due to a dependency issue.
```
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ├─possible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ├─restricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 └─restricted by julia compatibility requirements to versions: uninstalled — no versions left
```

With the newest release of SpecialFunctions.jl (https://github.com/JuliaLang/METADATA.jl/pull/19167) we can now update the dependencies of MLKernels.jl. This enables full compatibility for Julia 1.0 (CI passes).

Please trigger a new release on METADATA.jl after merging this to allow users to finally use this great package with Julia 1.0.

A test coverage drop may occur, see the bug and discussion here: https://github.com/JuliaLang/julia/issues/28192."
Update requirements for doc build,
Move HyperParameters module into deprecated file. Add deprecation war…,…nings.
Dev,
Remove HyperParameter?,"Hej!

I like this package, and in principle I think it could be a nice building block for different GP packages such as https://github.com/PieterjanRobbe/GaussianRandomFields.jl or https://github.com/STOR-i/GaussianProcesses.jl. But I'm not sure whether the use of `HyperParameter`s is actually needed; to me it seems it adds an additional layer of complexity that might not always be desired. My suggestion would be to follow the same approach as https://github.com/JuliaStats/Distributions.jl: restrict parameters `<:Real` of immutable kernel functions to certain intervals such as positive real line with the help of inner constructors. This would provide users with a very simple and completely flexible way of updating parameters by just creating new kernel functions with updated parameter values. For increased convenience one could even think about providing default transformations from https://github.com/tpapp/ContinuousTransformations.jl that would provide an easy way to map real values to required intervals and transforming likelihoods etc in a correct way."
Add Julia 1.0 to CI.,"Add testing for Julia 1.0.
Add windows testing."
more Julia 0.7 compat,Test has to be a dependency now
increase version to 0.4.0,was 0.3.9999
Julia 07 compat,
0.7 compat,Is there any work for 0.7 compatibility on the way?
Kernel combination implementation and derivatives,"Hello,
It would be amazing for the package to feature simple kernel combination such as kernel sum and product.
I wrote my own kernel function module because I needed it imperatively but yours is a lot more robust, maybe I could implement it given some directions.
Thanks!"
Kernels for HAC estimators?,"I am unsure if this package is the right place to implement these, but wanted to ask. For robust variance covariance estimators sometimes kernels are implemented to account for spatial or temporal correlation of an assumed or estimated form (Heteroscedastic and Autocorrelation Consistent / HAC variance covariance estimators). Would this package be a good place to have these kernels implemented? Here is a reference of the R implementation of these [Sandwich](https://www.jstatsoft.org/article/view/v011i10/v11i10.pdf) and here is another implementation in Julia [CovarianceMatrices.jl](https://github.com/gragusa/CovarianceMatrices.jl)."
Dev,
WIP: Fixes for Julia v0.6,"- [x] Fix deprecation warnings
- [ ] Fix kernel macros"
Using Distances.jl and automatic differentiation,"I just made a PR JuliaStats/Distances.jl#72 to make distances compatible with automatic differentiation. By using Distances.jl we could get

- ARD kernels using the weighted metrics
- easy higher order derivatives with automatic differentiation

Using a more general `Metric` type in the kernel computations could also enable kernels to be computed over other metric spaces than R^n (e.g. graphs).

Relevant issues:
#46 #53 #52 "
threading support,"I am doing some tests with the `@threads` macro, in case you are interested:

https://github.com/gdkrmr/MLKernels.jl/tree/threads

For the Gaussian kernel I get around 60% speedup for two threads."
DomainError in tests,"in `v0.2.0`
```
INFO: Testing MLKernels.kernelmatrix!
ERROR: LoadError: LoadError: DomainError:
 in exponentialkernel at /home/me/.julia/v0.5/MLKernels/src/kernel.jl:110 [inlined]
```
`z` can be smaller than zero, I think this is due to calculating the squared euclidean distance as x^2 - 2xy + y^2"
pairwise_aggregate inconsistencies,"sometimes you use `f::Type` and sometimes only `::Type` in `pairwise_aggregate`:

https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L19
https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L27"
julia 0.6 compatibility,What are your plans on supporting Julia 0.6?
tag a new version,"I am developing a package that depends on MLKernels, but the latest tagged version is 2 years old, which makes it hard to specify dependencies.

If you are interested: https://github.com/gdkrmr/KernelRidgeRegression.jl"
More detailed categorization of kernels,"I think it would be useful to have a more detailed categorization of the kernels. In chapter 4 of [this book](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) kernels (covariance functions) are categorized by the types of their inputs. This would enable specifying the pairwise functions in the type hierarchy instead of specifying them for each kernel separately.

For example:
- stationary kernels satisfy `K(x, y) = k(x-y)`
- isotropic kernels satisfy `K(x, y) = k(|x-y|)`"
Random Fourier Features,"Random Fourier features is a technique for approximating inner products in the RKHS using a randomized feature map. It would be great to have this in MLKernels.

Here's a paper introducing them:
[Random Features for Large-Scale Kernel Machines](http://pages.cs.wisc.edu/~brecht/papers/07.rah.rec.nips.pdf)

A blog post demonstrating their use:
[Random Fourier Features for Kernel Density Estimation](https://mlstat.wordpress.com/2010/10/04/random-fourier-features-for-kernel-density-estimation/)

The kernel being approximated needs to have some specific properties. Mainly it needs to be stationary (shift-invariant) and scaled properly.

I need to look into this a bit more, but it seems like it might not be too difficult to implement. The main part being a function `spectraldensity`, which takes a kernel as an argument and returns a distribution to sample from."
LoadError with 0.5,"I get the following syntax error and deprecation warnings with version 0.5.1 of Julia.

```julia
julia> Pkg.free(""MLKernels"")
INFO: Freeing MLKernels
INFO: No packages to install, update or remove

julia> using MLKernels
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:14
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:17
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
ERROR: LoadError: LoadError: syntax: space before ""("" not allowed in ""? (""
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/pairwise.jl, in expression starting on line 45
while loading .../.julia/v0.5/MLKernels/src/MLKernels.jl, in expression starting on line 57
```
These do not appear in the `dev` branch or when using version 0.4.5 of Julia."
Kernel Derivatives,"There's two components to this enhancement.

### Optimization
Define a `theta` and `eta` (inverse `theta`) function to transform parameters between an open bounded interval to a closed bounded interval (or eliminate the bounds entirely) for use in optimization methods. This is similar to how link functions work in logistic regression - unconstrained optimization is used to set a parameter value in the interval (0,1) using the logit link function.
- [x] `theta` - given an interval and a value, applies a transformation that eliminates finite open bounds
- [x] `eta` - given an interval and a value, reverses the value back to the original parameter space
- [x] `gettheta` returns the theta transformed variable when applied to `HyperParameters` and a vector of theta transformed variables when used on a  `Kernel`
- [x] `settheta!` this function is used to update `HyperParameter`s or `Kernel`s given a vector of theta-transformed variables
- [x] `checktheta` used to check if the provided vector (or scalar if working with a HyperParameter) is a valid update
- [x] `upperboundtheta` returns the theta-transformed upper bound. For example, in the case that a parameter is restricted to (0,1], the transformed upper bound will be log(1)
- [x] `lowerboundtheta` returns the theta-transformed lower bound. For example, in the case that a parameter is restricted to (0,1], the transformed lower bound will be -Infinity

### Derivatives
Derivatives will be with respect to `theta` as described above.
- [ ] `gradeta` derivative of `eta` function. Using chain rule, this is applied to `gradkappa` to get the derivative with respect to theta. Not exported.
- [ ] `gradkappa` derivative of the scalar part of a `Kernel`. This must be defined for each kernel. It will be manual, so the derivative will be analytical or a hand coded numerical derivative. It will only be defined for parameters of the kernel. Not exported. Ex. `dkappa(k, Val{:alpha}, z)`
- [ ] `gradkernel` derivative of `kernel`. Second argument will be the variable the derivative is with respect to. A value type with the field name as a parameter will be used. Ex. `dkernel(k, Val{:alpha}, x, y)`
- [ ] `gradkernelmatrix` derivative matrix."
Equality test for Kernels,"```julia
julia> GaussianKernel(1.0) == GaussianKernel(1.0)
false
```

my best guess:

```julia
julia> isimmutable(GaussianKernel(1.0))
true

julia> isimmutable(GaussianKernel(1.0).alpha)
false
```
confuses julia"
performance,"The following code shaves off more than 0.5 seconds when X has `20_000` columns and `10` rows:

```julia
n = 20000
x = randn(10, n)

function gauss{T}(X::Matrix{T}, alpha::T)
    n = size(X, 2)
    xy = LinAlg.BLAS.syrk('U', 'T', one(T), X)
    x2 = [ sum(X[:, i] .^ 2) for i in 1:n ]
    
    LinAlg.BLAS.syr2k!('U', 'N', one(T), x2, ones(T, n), T(-2), xy)
    
    @inbounds for i in 1:n
        for j in 1:i
            xy[j, i] = exp(-alpha * xy[j, i])
        end
    end
    LinAlg.copytri!(xy, 'U')
end
gauss(x, 1.0)
```

compared to

```julia
kernelmatrix!(
    ColumnMajor(),
    Matrix{Float64}(n, n), 
    GaussianKernel(1.0), 
    x, true
)
```
I think one of the reasons is that `LinAlg.syrk_wrapper!` always copies the triangle in the matrix, even though you try to avoid that. I am not sure how much using `syr2k` saves.
"
MLKernels with julia v0.5,"`MLKernels` is nice, but there are a few issues when using with julia 0.5.
Here is what I've found:
- In `kernelapproximation.jl`: replace `Base.blasfunc` with `Base.LinAlg.BLAS.@basefunc` and `chksquare` with `checksquare`
- In `meta.jl`: replace `super` with `supertype`
- In`pairwise.jl`: replace text like `i = store_upper ? (1:j) : (j:n)` with `i = (store_upper ? (1:j) : (j:n))`
- In `kernelfunctions.jl`: in the first 4 `call` statements, there is the issue of adding methods to an abstract type in Julia 0.5 e.g. sisl/BayesNets.jl#28 . What functionality in `MLKernels` would be lost if these statements were excluded? 
"
kernelmatrix function,"<img width=""1087"" alt=""screenshot 2016-06-30 01 26 14"" src=""https://cloud.githubusercontent.com/assets/1314675/16481762/c665400c-3e61-11e6-8ea9-cc8db4caaba7.png"">

I also get this error with `kernel` function sometimes.
"
kernel function,"I just started using your latest version of MLKernels.  I'm having an issue which may just be a syntax problem on my end.  I'm attempting to use the kernel function like I have in the past but here's the result:

``` julia
julia> kernel(GaussianKernel(), 2., 4.)
```

```
ERROR: type KernelComposition has no field k
 in kernel at /Users/joshualeond/.julia/v0.4/MLKernels/src/kernelfunction.jl:14
```

I usually use the kernel to compare arrays and have the same result except the error is at line 16.  The version that isn't working for me is v0.1.0+ which I cloned from Github and the previous version I was using was v0.1.0 installed from Metadata. 
"
Hyperparameters,"The parameters for Kernels should be abstracted to a `HyperParameter` type that can be used to aid in optimization and ensure consistency of constraints.
"
Update to 0.4 compatibility,"This PR makes the full move to work in 0.4+
"
Positive Definite vs Mercer,"I see that you treat positive definite kernels as being synonymous to mercer kernels, while as far as I understand there is a slight difference in that a mercer is more restrictive (e.g. continuity).

Was this an oversight, or a conscious design decision (for simplification maybe)?

source: [here at 36:40](http://videolectures.net/mlss09uk_schoelkopf_km/)
"
Use of triangular dispatch invalid,"This definition for ARD seems to invalid in Julia 0.4, and I think should be invalid in 0.3 too but might not be due to a change.

https://github.com/trthatcher/MLKernels.jl/blob/c1532ce1d3b43806664296b82b4a0bd0b281190c/src/kernels.jl#L62

```
immutable ARD{T<:FloatingPoint,K<:StandardKernel{T}} <: SimpleKernel{T}
```

In particular the `K` part. I can't find exactly where it was changed, but here is related discussion

https://github.com/JuliaLang/julia/issues/6620
https://github.com/JuliaLang/julia/issues/3766
https://github.com/JuliaLang/julia/issues/8974#issuecomment-62552208

I'm a bit confused on how this all works internally, might be worth posting on julia-users if you want to get it working on 0.4 at some point.
"
Kernel/Kernel Matrix Computation,"@st-- 

So I was thinking a bit about the approach to the kernel/kernel matrix computation.

If you have a function f:RxR -> R, then you can apply it element-wise to two vectors and sum the results. So for x = [x1,x2] and y = [y1,y2] the dot product the function would be f(x,y) = xy. Then k(x,y) = f(x1,y1) + f(x2,y2) = x1y1 + x2y2. The squared distance is just f(x,y) = (x-y)^2

If f is a valid positive or negative definite kernel in RxR, then we have a way to extend the kernel to R^n x R^2 by summing the element-wise results.

I was thinking we could take a more modular approach. Technically, the kernels we have now are a composition of a positive definite kernel (the polynomial kernel takes the dot product) or a negative definite positive-valued kernel (the euclidean distance in what we have implemented) and we could define these 'input' kernels in terms of `f`.

I was thinking we could abstract `kernelmatrix` into two functions: generic `kernelize` (the way it currently operates) and a generic `pairwise` which operates on two vectors/matrices. We would create a new class of kernels and `pairwise` would dispatch on those types. Examples of those new base kernels would be `SquaredDistance` and `ScalarProduct`. However, we can easily extend it:
- `f(x,y) = sin(x-y)^2` (sine squared kernel - also negative definite)
- `f(x,y) = (x-y)^2/(x+y)` (chi squared kernel for R+ x R+)

These can all be extended naturally to cover the periodic kernel weights... I was thinking two weight vectors and `f` could be defined like this:

k(x,y) = u1_sin(w1_x1 - w1_y1)^2 + u2_sin(w2_x2 - w2_y2)^2

Where u and w are your weight vectors. (I know this could add redundancy - just an idea)

Long story short, three levels:
- 'Base' simple kernels that will be defined for `pairwise` - ARD would be defined at this level
- Derived kernels that are a function of the matrix that pairwise returns
- Composite kernels as we have them now.

Anyway, I think an approach along these lines would also give us a nice modular approach to the derivatives. We can always ensure there's generic fall-back methods basically exactly how we have them now. Let me know your thoughts
"
Consistency in Exceptions: ArgumentError vs DimensionMismatch,"`kernelmatrix!` for StandardKernel throws ArgumentError, `kernelmatrix!` for SquaredDistanceKernel or ScalarProductKernel throws DimensionMismatch (by `scprodmatrix!`/`sqdistmatrix!`). `matrix_prod!` and `matrix_sum!` for generic Arrays call error(), but for matrices with is_upper argument the former throws DimensionMismatch and the latter throws ArgumentError. I think this should all be made a bit more consistent.

Is there a specific reason to use error() rather than throw(<some exception>) in some cases ?

(NB. there should be `@test_throw`s as well, which I'm about to add.)
"
Merge Development into Master,
Kernel derivatives need to be a separate package,"@st-- 

I'm trying to be pragmatic when it comes to this package. I originally envisioned that this package would provide the following:
- A vetted set of mainstream machine learning kernels (with some simple combination rules)
- The ability to compute a kernel matrix quickly
- The ability to compute a kernel matrix approximation

Unfortunately with the kernel derivatives, I feel the package moves too far away from that. My concerns are that: 
- Derivatives may not be defined and vector parameters add a layer of complexity
- Many derivatives that do exist are intractable - reliance on analytic derivatives is complex/unfeasible in many cases and raises floating point accuracy in others
- Derivatives appear to be catering to a very specific need - it's a specialized component being added to a generic package. 

As such, it's bespoke code and necessitates its own package. 

That being said, I appreciate all the help and I'm more than willing to help you out where ever I can.
"
MultiQuadraticKernel missing,"Maybe that's intentional, but I just noticed that the MultiQuadraticKernel in master doesn't have any equivalent in developments. The InverseMultiQuadraticKernel maps to RationalQuadraticKernel with beta=0.5, but the MultiQuadraticKernel would be beta=-0.5 and you only allow positive values for beta...
"
@inbounds,"Various functions, particularly in vectorfunctions.jl and kernelderiv.jl, use `@inbounds` without making sure all the arguments have the right length etc. Is this fine, because we know how we call them, and we do check array dimensions etc. in the parent routines? Or should we add additional bounds checks in there?
"
ARD parameter (weights) derivative,"Just realised that `kernelmatrix_dp` was missing, and added it in ab3c09258fdcc322f0748ac9467f0b8ac86f518c -- piggy-backing on `kernelmatrix` seemed easiest. The only issue with this is that, currently, `kernel_dp(k::ARD, :weights, x, y)` returns an array with the length of `k.weights`. From a computational POV it makes sense to calculate all dk/dw[i]s at once (and usually you'd want all derivatives, not just one of them, I think), but from an interface/API POV it would be better if `kernel_dp` always returned a scalar... how to reconcile that ?
"
Segfaults,"I was going to add `@test_throw` tests for the various ArgumentErrors e.g. in kernelmatrix, and to figure out which way around things should go I played around in the REPL with things like `MLKernels.kernelmatrix!(zeros(4,4), ExponentialKernel(), ones(4,6), true)`, transposed dimensions for X, different values for is_trans, leaving out is_trans entirely - which sometimes led to instant segfaults, sometimes with a bit of delay. Don't have the time anymore to investigate, but I suspect it's something where we used `@inbounds` but didn't have enough checks to make sure we weren't going to exceed the array bounds!
"
"Limits on parameter ranges: PolynomialKernel,","PolynomialKernel required d to be integer. ExponentialKernel, RationalQuadraticKernel, PowerKernel, LogKernel all require gamma to be <= 1. What's the reason for that ? (Maybe a kernel would not be Mercer if gamma > 1 (or d real, for PolynomialKernel), but it might still be useful. We could just amend ismercer() so that it actually checks the parameter value?)
"
Approach to derivatives,"Types can be parameterized by symbols, which means we can take a different approach to derivatives. I've included a [gist](https://gist.github.com/trthatcher/acd86f5a9b911737d419) to illustrate how we could take advantage of this (""Approach 2"") 

It would cut down on the number of functions and it seems to perform better on my machine (and definitely no worse).
"
Definity of kernels,"I had assumed the Sigmoid kernel was c.p.d., but according to the [paper I just read](http://vip.uwaterloo.ca/files/publications/Carrington%20et%20al%20-%20A%20New%20Mercer%20Sigmoid%20Kernel.pdf) about the Mercer Sigmoid kernel apparently that's only true for some parameter values, depending on the data set... so I suppose we should have iscondposdef(::SigmoidKernel) = false?

Also, I think we might've been working with `isposdef(::Kernel)` meaning ""positive semi-definite"", but Base.isposdef is ""strictly positive definite"" (e.g. isposdef(0) = false...), and if we extend a Base function we should follow that. For kernels it doesn't seem to make much difference whether it's positive semi-definite or strictly positive definite, so maybe move to our own function? We could have ismercer()... as positive semi-definity is sufficient and necessary for a kernel being a Mercer kernel.
"
Remove Separable Kernel (Mercer Sigmoid Kernel),"The separable kernel is the dot product kernel applied to an element-wise transformation of the original data vectors. The element-wise transformation is 100% equivalent to pre-processing one's data.

I don't think it's adding any value - any opposition to removal of this type/kernel?
"
Nystrom kernel approximation,"I believe the Nystrom kernel approximation should satisfy the following invariant:
`nystrom(kernel, X, [1:size(X,1)]) == kernelmatrix(kernel, X)` (up to floating point inaccuracies)
The current implementation is definitely broken.
If I use just Base routines, as follows, I get what I believe is the correct approximation:

```
function basenystrom(kernel::Kernel, X::Matrix, xs::Vector)
    C = kernelmatrix(kernel, X, X[xs,:])
    D = C[xs,:]
    SVD = svdfact(D)
    DVC = diagm(1./sqrt(SVD[:S])) * SVD[:Vt] * C'
    MLKernels.syml(BLAS.syrk('U', 'T', 1, DVC))
end
```

But I don't understand your optimised version well enough to figure out where it's going wrong...

I've added this as test/test_approx.jl; have a look and have fun fiddling with your implementation until that test passes. ;-) It's not yet added to runtests.jl so it doesn't break the overall test.
"
Edge cases,"LogKernel is not pos.def. anyway; constructor says gamma has to be in (0,1] but only checks for gamma>0. Is there a reason for gamma<=1? If not, error message should be adjusted, otherwise code & test...
"
GammaRationalQuadraticKernel parameters,"So GammaRationalQuadraticKernel(alpha,beta,gamma) is a generalised form of the RationalQuadraticKernel(alpha,beta) is a generalised form of the InverseQuadraticKernel(alpha).
InverseQuadraticKernel(alpha) = RationalQuadraticKernel(alpha,beta=1), and
RationalQuadraticKernel(alpha,beta) = GammaRationalQuadraticKernel(alpha,beta,gamma=1).
But the default value of GammaRationalQuadraticKernel is gamma=0.5. Just checking if that is intentional?
"
Why split up test_standardkernels?,"The kernels are all very similar in their tests - wouldn't splitting it up risk missing some tests for some of them / testing them vaguely differently? It's a lot of code duplication for which I can't see the gain... maybe you've got a good reason though ? Mainly curious.
"
Periodic Kernel does not appear to be a Squared Distance kernel,"I was trying to find some documentation on the periodic kernel. In all the papers I look at, it seems like the sin() function is applied to the element-wise distances and squared rather than taking the squared sine of the distance.

sum sin(xi - yi)^2 for all i

rather than sin(||x-y||)^2

Here's a paper for example:

http://jmlr.org/proceedings/papers/v33/hajighassemi14.pdf
"
Release,"@st-- 

I would _really_ like to finish up the non-derivative portion of the package ASAP (next 10 days or so) so that I may use it in my other projects and so that I can release a version of this package to http://pkg.julialang.org/. Originally, this would have happened last week, but the derivatives and ARD obviously added to that time. That's okay (:

I'm going through all the standard kernel definitions and correcting/refining all definitions to ensure they are clean and correct. I'm also going to revisit the standard kernel tests (broken on my last commit - don't worry, will fix up). The composite kernels will need to be sorted for the recursive definition.

Regarding the kernel derivatives, there's a couple options since they probably won't be ready. First, exclude them from the first release and just keep them in a development branch. Alternatively, they could be included in the package code, but not extracted/documented. You would need to explicitly import them. Lastly, they could be broken out into their own package with this one as a dependency (you would be the owner of the deriv package - but I could still co-maintain).

How would you like to approach it?
"
sqdist_d*() and scprod_d*(),"It seems like each version is used in only one place (the derivative for the corresponding class of kernel including ARD). Making use of these functions uses at least two - currently three - scans of the array that the kernel function returns.

Are these functions providing any real value? It seems like there's two definitions with multiple array scans when only a single definition and one scan is required.
"
Only integer exponents?,"Does the exponent e.g. for PowerKernel need to be integer? Can it not interpolate with real exponents?
"
Re-implementing Base functions,"In one of your recent commits you made use of (Base.)scale!() which I hadn't come across before, but it looks like it's basically the same functionality as gdmm!/dgmm! in matrixfunctions.jl - the only difference is that the Base version doesn't use `@inbounds`... does that macro make things so much faster that it's worth keeping an in-package version of matrix-vector scaling ?
"
N vs. T,"How useful is it to basically duplicate have the code to allow both row-wise and column-wise data matrices ? From an implementation point of view it'd be a lot simpler to just decide on one, and require users to call e.g. kernelmatrix(k, X', Y') when needed, which is a one-off overhead.

The only difference is whether the access is X[i,:] and Y[j,:] or X[:,i] and Y[:,j]... so I played around with macros and came up with one which takes a condition, a tuple of symbols, and a code block and transforms all array references to objects listed in the tuple of symbols (84ca8bffcc825aaea48708b4d8fc9e9c6984c446, 1564f8a3c4a68ac97bcd5eeda4464a6120fa4e7b). But that doesn't work so well for the cases where you have e.g. N_sqdist and T_sqdist...
"
scprod and sqdist including weighted versions and derivatives and tests,"Sorry, I had added this as well but hadn't put it in there. Merging now...
"
"kernelmatrix(k, X, Y) calculates full matrix, not only upper-right triangle as commented","Now the question is, are there going to be any kernels which are non-symmetric, for which k(x, y) != k(y, x)? If not, then kernelmatrix(k, X, Y) should only calculate half the matrix, as already the case for kernelmatrix(k, X) [which calculates kernelmatrix(k, X, X)].

On second thought, why is there a special case for kernelmatrix(k, X, X)? Should be the same code as for kernelmatrix(k, X, Y)...
"
Roadmap,"@st-- 

I think we should establish a roadmap. For the most part, I already have what I need from this package. All I need to do is expand/finish the approximations.

We have a good foundation for the kernel derivatives defined for the Gaussian kernel. I can expand it to include all the other kernels, it's just a matter of sinking some time into it. 

What are you priorities in terms of the features? How do you want to approach it?
"
Change norm2 -> sqdist & add derivatives for ScaledKernel,
positive definiteness of composite kernels,"Are you sure that a kernel sum of one positive definite and one NOT positive definite kernel is still positive definite ? That's what the code says: `isposdef(ψ::KernelSum) = isposdef(ψ.k1) | isposdef(ψ.k2)`, but I really wouldn't've thought so..
"
Kernel function discussion/kernels with variable-dimensional parameters (ARD),"I just pushed an implementation of integer-based parameter derivatives based on calling names() on the kernel object to get its field. After all that I remembered an important use case in Gaussian Processes: the ""Automatic Relevance Determination"" (ARD) kernel. This is basically a Gaussian kernel, but with a sigma _vector_ (same length as data dimension):

Instead of the 1D Gaussian, `exp(- vecnorm(x - y)^2 / 2k.sigma^2)`, you would use `exp(- vecnorm((x - y) ./ k.sigmas)^2 / 2)`, where k.sigmas is a vector of the same dimensions as x and y.

This crops up not just for euclidean distance kernels, but also scalar product kernels - e.g. a LinearKernel with different scaling for each dimension...

Any idea on how to best implement that ?
"
Memory usage / in-place covariance matrix calculations,"Reading up on Coverage.jl I was curious to check the memory allocation behaviour of the code. Most of it is constructing Kernels, which I think is fine. But it occurred to me that e.g. the second derivatives construct n x n matrices, and memory allocation is one of the biggest speed hits. So in the future it might be worth rewriting the kernel functions such that they can write directly into the covariance matrix. (This will require some more careful thought, and isn't urgent, but I wanted to bring it up now before I forget again!)
"
Composited composite kernels,"Currently, a CompositeKernel can only be made out of StandardKernels, which means something like GaussianKernel()+GaussianKernel()*GaussianKernel() doesn't work. Is there a good reason for that?
"
Fix GaussianKernel kernelize* functions,"Parentheses are significant :)
"
Speed vs code clarity,"So quite a bit of the code looks like it's rather optimised for some things - e.g. euclidean_distance() with BLAS.axpy! and BLAS.dot - is that actually that much faster than Base.dot(x, y) that it's worth it ? And what happens if x and y happen to have different lengths (_shouldn't_ happen, but you never know)? Base.dot() would just throw a DimensionMismatch. The BLAS stuff doesn't complain, just computes a different number...

On a very similar note, is it worth writing convert(T,2) instead of just 2? Doesn't the compiler automatically convert types?

And on a not so related note, but too small to bother opening a separate issue: the Coveralls thing seems to think that kernelize_scalar() for the MercerSigmoidKernel (separablekernels.jl) isn't covered - but I think it should be ? When I add a println() statement to the function, it gets printed out lots of times. Any idea what's going on?
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
Derivatives (wrt parameters and input values),"Hi,

to be able to use this kernel module in my Gaussian Process regression code - instead of rolling my own covariance kernels, which do basically the same, just not as fancy as your code - I will need the derivatives of kernels, both with respect to the parameters (for optimizing log marginal likelihoods) and with respect to input values (for training from and prediction of derivatives of function values, not just the function values themselves). What are your thoughts (if any) on the interface for that ?

Have a look at https://github.com/st--/juliagp/blob/master/covariances.jl for how I've implemented it so far...

-ST
"
Refactor conversion loop to reduce code redundancy,"Hi,
I just came across your Kernel package for Julia, which I'd like to incorporate into my Gaussian Process code - rather than duplicating all the covariance kernels! To get a bit used to things I just refactored the loops at the bottom of the kernel files, removing code redundancy. All tests running fine.
"
can I use this module in virtual environment?,"Now, I try to install in virtual-env.
but I couldn't find the way for install.
could you help me?"
Fix absl compatibility issues in TF 1.13,"Ref https://github.com/tensorflow/tensorflow/issues/22766
Ref https://github.com/tensorflow/tensorflow/issues/22113"
How to use extra loss with Keras model?,"Hi

I am using Keras api for model training and testing. May I know how to use the loss function in this project together with Keras api?

Thank you!"
error in 'make',"When I run 'make' in build directory , there some errors:

[100%] Linking CXX shared library libextra_losses.so
/usr/bin/ld: 找不到 -ltensorflow_framework
collect2: error: ld returned 1 exit status
CMakeFiles/extra_losses.dir/build.make:166: recipe for target 'libextra_losses.so' failed
make[2]: *** [libextra_losses.so] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/extra_losses.dir/all' failed
make[1]: *** [CMakeFiles/extra_losses.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2

do you know the reason? thank you for your help.

ubuntu16.04+python3.6(anaconda3)+tensorflow1.6
thank you very much."
Where is cuda_config.h?,"      In the ""readme.md"", there is ""copy the header file ""cuda\_config.h"" from ""your\_python\_path/site-packages/external/local\_config\_cuda/cuda/cuda/cuda\_config.h"" to ""your\_python\_path/site-packages/tensorflow/include/tensorflow/stream\_executor/cuda/cuda\_config.h"".
      But  I can't find the path or ""cuda_config.h"" anywhere. It may lead to the failure of ""make"" command.  Can you fix it?"
 undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv,"I have Ubuntu 16.04, Python 3.5.2 in a virtual env, and Tensorflow 1.6 GPU.
I have 2 1080ti.
I have run  the ""cd build && cmake .."" and the output is:

> `-- The C compiler identification is GNU 5.4.0
> -- The CXX compiler identification is GNU 5.4.0
> -- Check for working C compiler: /usr/bin/cc
> -- Check for working C compiler: /usr/bin/cc -- works
> -- Detecting C compiler ABI info
> -- Detecting C compiler ABI info - done
> -- Detecting C compile features
> -- Detecting C compile features - done
> -- Check for working CXX compiler: /usr/bin/c++
> -- Check for working CXX compiler: /usr/bin/c++ -- works
> -- Detecting CXX compiler ABI info
> -- Detecting CXX compiler ABI info - done
> -- Detecting CXX compile features
> -- Detecting CXX compile features - done
> -- Looking for pthread.h
> -- Looking for pthread.h - found
> -- Looking for pthread_create
> -- Looking for pthread_create - not found
> -- Looking for pthread_create in pthreads
> -- Looking for pthread_create in pthreads - not found
> -- Looking for pthread_create in pthread
> -- Looking for pthread_create in pthread - found
> -- Found Threads: TRUE  
> -- Found CUDA: /usr/local/cuda-9.0 (found version ""9.0"") 
> -- Found CWD: /home/vision/matthew/tf.extra_losses/build
> -- Found GPU_CAPABILITY: gencode arch=compute_61,code=sm_61
> 
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> -- Found TF_INC: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include
> -- Found TF_INC_EXTERNAL: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/external/nsync/public
> -- Found TF_LIB: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow
> -- Configuring done
> -- Generating done
> -- Build files have been written to: /home/vision/matthew/tf.extra_losses/build
> `

**And Also 'make' command:**

> `[ 16%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_grad_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> [ 33%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> Scanning dependencies of target extra_losses
> [ 50%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:274:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:268:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:291:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:285:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:62:40: warning: variable ‘num_dimensions’ set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:93:40: warning: variable ‘num_dimensions’ set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> [ 66%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_grad_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:293:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:287:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:310:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:304:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> [ 83%] Building CXX object CMakeFiles/extra_losses.dir/common.cc.o
> In file included from /home/vision/matthew/tf.extra_losses/common.cc:22:0:
> /home/vision/matthew/tf.extra_losses/common.h:46:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_pow2(const T x)
>                                      ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) Target binary_cast(Source s)
>                                        ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
> /home/vision/matthew/tf.extra_losses/common.h:52:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_aligned(const T ptr, const size_t alignment)
>                                      ^
> [100%] Linking CXX shared library libextra_losses.so
> [100%] Built target extra_losses
> `

**but unfortunately  got an error in test_op.py execution:**

 

> 
> `/home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> Traceback (most recent call last):
>   File ""test_op.py"", line 36, in <module>
>     op_module = load_op_module(LIB_NAME)
>   File ""test_op.py"", line 33, in load_op_module
>     oplib = tf.load_op_library(lib_path)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
>     lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: /home/vision/matthew/tf.extra_losses/build/libextra_losses.so: undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv
> `
> 

What should  do?!"
How to get results from embedding training?,"Hi

 I've successfully made the model train, but I'm having trouble printing results, can you please elaborate on how to:

""use these embeddings, place them in the same folder as main.py, load the embeddings and use them.""

Specifically, what does ""loading and using them mean"" in the context of getting H@1, MRR etc.?"
How is background information used in the model ?,"Your paper mentions about using background information for filtering training dataset and tying parameters of relation in ""SimplE-bk"".  Is the code of this model available?"
Can I use GPU accelerate the training process?,"The training process (except the early stop process and test process) take me 7 hours on wn18 dataset using the default parameters on a Linux machine (One GTX 1080 ti). Is this normal?

And I find the early stop process also need 6 hours on wn18 dataset, I guess the time consuming is mainly from ""self.reader.replace_raw"" and ""get_rank"" step. Is this normal?"
a question about simplE_ignr.py,"I really like this work, but I have one thing confused.
In the 34th line of simplE_ignr.py, you use the code ""self.labels = tf.tile(self.y, [2])"", why do you use this code (what's its effect), and why don't you use the code in simplE_avg.py?
Looking forward to your kind feedback."
The evaluations,"I really appreciate your work but I'm not sure about one thing: In `Reader.py next_batch()`, if the `neg_ratio` is larger than 1, the size of `all_triples_and_labels` would be `(1+neg_ratio) * len(bp_triples)`，then the evaluations should all be divided by  `(1+neg_ratio) * len(triples)`. Looking forward to your reply.
"
GPU not being used,i used --cuda on google colab but gpu is not being used how to fix it
Hi! the training can not convergence!,"I train the model on val2017 dataset and my own dataset, but the valid result is about 21db, have you tested them ?Thank you!"
Methods for adding poission noise in official implementations,"Hi Authors,

I  found the official implementations of adding poission noise use the following methods:
https://github.com/NVlabs/noise2noise/blob/c40a0481198bb524d0b70c2cc452f21bd7aec85c/train.py#L47

        return np.random.poisson(chi*(x+0.5))/chi - 0.5

Do you think we can utilize this method?"
Using my own noisy-noisy pairs,"Dear Authors,

There are no instructions on how to use the code for my own noisy-noisy pairs. The current code takes a dataset of images as input and applies two independent noises on each instance, leading to pairs of noisy-noisy train set. But what if I already have my own pairs of noisy-noisy images and want to train the network on them? Is this possible with current architecture?

Best,
Ali. "
just for the test.py,"Hello, I would like to ask, why the test image I input is 1024 * 1024, but the output is indeed 256 * 256, and I did not see where the picture was cropped, I look forward to your reply, thank you."
"Confused about the unet.py code, is it reasonable?","Hi I checked the code feel confused with the code in unet.py:
```
    def forward(self, x):
        """"""Through encoder, then decoder by adding U-skip connections. """"""

        # Encoder
        pool1 = self._block1(x)
        pool2 = self._block2(pool1)
        pool3 = self._block2(pool2)
        pool4 = self._block2(pool3)
        pool5 = self._block2(pool4)
```

pool2-pool5 is computed by self._block2, So it means pool2-pool5 re-use the same conv weight and bias. Does it accepted? I think the u-net should use different conv weight of different layer."
Sharing Results from Monto Carlo Renderings,"Hi ,

Can you please share the results of Monte Carlo Renderings N2N ? 

Regards
Muzahid"
OpenEXR Error while running the train files.,"Traceback (most recent call last):
  File ""train.py"", line 7, in <module>
    from datasets import load_dataset
  File ""/Users/snehagathani/Desktop/noise/src/datasets.py"", line 9, in <module>
    from utils import load_hdr_as_tensor
  File ""/Users/snehagathani/Desktop/noise/src/utils.py"", line 12, in <module>
    import OpenEXR
ImportError: dlopen(/anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so, 2): Symbol not found: __ZN7Imf_2_314TypedAttributeISsE13readValueFromERNS_7IStreamEii
  Referenced from: /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so
  Expected in: flat namespace
 in /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so


This error comes for all the training files."
datasets.py  issues?,"Hello,there may be problems with this place
`raise ValueError('Invalid noise type: {}'.format(noise_type))` 
 should be 
`raise ValueError('Invalid noise type: {}'.format(self.noise_type))`"
something wrong with your endcoder?,"https://github.com/joeylitalien/noise2noise-pytorch/blob/7942c06f924e2244d91fc8c1aff1c2e3991e0eae/src/unet.py#L82
I think enconv(i), i=2,...,5 should be defined separatedly or they will share the same weights.
"
About U-Net model and data pre-processing,"I am curious about the differences from the paper model
1) Why did you choose to use ConvTranspose instead of Upsampling
2) You used RELU in all parts and Leaky RELU after the last layer. But from the paper I think the author meant Leaky RELU everywhere except the last layer. And In the last layer only linear activation

Unrelated question
3) Did you use any pre-processing for the images e.g. means subtraction, normalization etc. I think that would be needed since we don't have BN layers


I am trying to implement the model in Tensorflow and having the problem of INF loss that starts in the 2nd epoch. So I hope your answers would help me. Thank you in advance!

Update: I seem to have solved the problem by adding Batch Norm layers to the UNET model. But still I am puzzled how the authors managed to get a stable training without Batch Norm"
Create LICENSE,
Failed to load https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.19041-SP0
Python: 3.8.11
PyTorch: 1.9.1+cpu
HanLP: 2.1.0-beta.50
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
d:\PycharmProjects\mednlp_project\mednlp_train\mishu_entity_extract.ipynb 单元格 64 in ()
      1 import hanlp
      3 HanLP = hanlp.pipeline() \
      4     .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
      5     .append(hanlp.load('FINE_ELECTRA_SMALL_ZH'), output_key='tok') \
      6     .append(hanlp.load('CTB9_POS_ELECTRA_SMALL'), output_key='pos') \
      7     .append(hanlp.load('MSRA_NER_ELECTRA_SMALL_ZH'), output_key='ner', input_key='tok') \
----> 8     .append(hanlp.load('PMT1_DEP_ELECTRA_SMALL', conll=0), output_key='dep', input_key='tok')\
      9     .append(hanlp.load('CTB9_CON_ELECTRA_SMALL'), output_key='con', input_key='tok')

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\__init__.py:43, in load(save_dir, verbose, **kwargs)
     41     from hanlp_common.constant import HANLP_VERBOSE
     42     verbose = HANLP_VERBOSE
---> 43 return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\utils\component_util.py:186, in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    184 except:
    185     pass
--> 186 raise e from None

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\utils\component_util.py:106, in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    104 else:
    105     if os.path.isfile(os.path.join(save_dir, 'config.json')):
...
    459     if not _raise_exceptions_for_missing_entries:

OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like hfl/chinese-electra-180g-small-discriminator is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
=================================ERROR LOG ENDS=================================
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
hanlp.load(SIGHAN2005_MSR_CONVSEG) 卡住了,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
第一次import hanlp
hanlp.load(SIGHAN2005_MSR_CONVSEG) 卡住了

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
from hanlp.utils.rules import split_sentence
from hanlp.pretrained.tok import SIGHAN2005_MSR_CONVSEG

tok = hanlp.load(SIGHAN2005_MSR_CONVSEG)
```

输出
```
2023-09-08 17:42:46.670747: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-08 17:42:46.716386: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-08 17:42:46.716815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-08 17:42:47.808801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
看之前的输出，下载已经完成

输入ctrl-d，会输出：
```
terminate called after throwing an instance of 'std::runtime_error'
  what():  random_device could not be read
```

试了另一个tok.SIGHAN2005_PKU_CONVSEG，也是这样
COARSE_ELECTRA_SMALL_ZH没问题

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
os: Fedora Linux 38 (Workstation Edition)
kernel: 6.4.14
python: 3.11.4
hanlp: 2.1.0b50

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
始终报file is not a zip file,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
A clear and concise description of what the bug is. hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)报错，说load_from_meta_file raise e from None,等于是load（）函数下载的东西有问题，在zipfile解压的时候说不是zip文件。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)

```python
```

**Describe the current behavior**
A clear and concise description of what happened.在.hanlp/tok/安装网上下载的包也还是不能跑通

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.8.17
- HanLP version:2.1.0-beta.50
- Pytorch 2.0.1+cu117
- linux-4.15.0-142-generate-x86_64-with-glibc2.17

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
a bug,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
运行recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)时报错

Failed to load https://file.hankcs.com/hanlp/ner/msra_ner_albert_base_20211228_173323.zip
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.22621-SP0
Python: 3.9.13
PyTorch: 2.0.1+cu118
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50



RuntimeError: Failed to import transformers.models.albert.modeling_tf_albert because of the following error (look up to see its traceback):
No module named 'keras.engine'

**Code to reproduce the issue**
import hanlp
recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)

```python
```

**Describe the current behavior**
已经成功下载了模型，但是在加载的时候报错
hanlp版本为2.1.0b50

**Expected behavior**
希望能够将下载的模型加载进来使得代码能够成功运行

**System information**
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.22621-SP0
Python: 3.9.13
PyTorch: 2.0.1+cu118
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50

**Other info / logs**
源代码：
import hanlp
recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
希望可以增加自定义词典功能，对于分错的词语可以人为纠正。,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
目前没有发现存在自定义词典的功能。在一些特定场景中，如：螺栓紧固，会被分词为 螺栓、紧、固。但是在该场景中，我们更希望得到螺栓、紧固。
**Will this change the current api? How?**
也许需要增加一个add_word()接口去实现该功能。
**Who will benefit with this feature?**
每一个使用hanlp并希望hanlp能越来越好的人都会获益。
**Are you willing to contribute it (Yes/No):**
力有不逮
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Dedian
- Python version: 3.7.4
- HanLP version: 2.1.0b50

**Any other info**
no
* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
修复ViterbiSegment分词器中加载自定义词典时未替换DoubleArrayTrie导致分词不符合预期的问题,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ✨.
⚠️Changes must be made on dev branch.
-->

# 修复ViterbiSegment分词器中加载自定义词典时未替换DoubleArrayTrie导致分词不符合预期的问题

## Description

ViterbiSegment加载自定义词典时未正确替换DoubleArrayTrie, 导致应该被切分出的词条未被切分

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?
com/hankcs/hanlp/seg/SegmentTest.java
```java
    public void testExtendViterbi() throws Exception
    {
        HanLP.Config.enableDebug(false);
        String path = System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/CustomDictionary.txt;"" +
            System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/全国地名大全.txt"";
        path = path.replace(""\\"", ""/"");
        String text = ""一半天帕克斯曼是走不出丁字桥镇的"";
        Segment segment = HanLP.newSegment().enableCustomDictionary(false);
        Segment seg = new ViterbiSegment(path);
        System.out.println(""不启用字典的分词结果："" + segment.seg(text));
        System.out.println(""默认分词结果："" + HanLP.segment(text));
        seg.enableCustomDictionaryForcing(true).enableCustomDictionary(true);
        List<Term> termList = seg.seg(text);
        System.out.println(""自定义字典的分词结果："" + termList);
    }
```
![image](https://github.com/hankcs/HanLP/assets/27196120/73ae3c04-c3dc-4a87-ba0c-287c127b9829)


## Checklist

Check all items that apply.

- [x] ⚠️Changes **must** be made on `dev` branch instead of `master`
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
ViterbiSegment加载自定义词典时未正确替换DoubleArrayTrie,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
ViterbiSegment加载自定义词典时未正确替换DoubleArrayTrie

**Code to reproduce the issue**
com/hankcs/hanlp/seg/Viterbi/ViterbiSegment.java
```java
    private void loadCustomDic(String customPath, boolean isCache)
    {
        if (TextUtility.isBlank(customPath))
        {
            return;
        }
        logger.info(""开始加载自定义词典:"" + customPath);
        DoubleArrayTrie<CoreDictionary.Attribute> dat = new DoubleArrayTrie<CoreDictionary.Attribute>();
        String path[] = customPath.split("";"");
        String mainPath = path[0];
        StringBuilder combinePath = new StringBuilder();
        for (String aPath : path)
        {
            combinePath.append(aPath.trim());
        }
        File file = new File(mainPath);
        mainPath = file.getParent() + ""/"" + Math.abs(combinePath.toString().hashCode());
        mainPath = mainPath.replace(""\\"", ""/"");
        DynamicCustomDictionary.loadMainDictionary(mainPath, path, dat, isCache, config.normalization);
    }
```
com/hankcs/hanlp/seg/SegmentTest.java
```java
    public void testExtendViterbi() throws Exception
    {
        HanLP.Config.enableDebug(false);
        String path = System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/CustomDictionary.txt;"" +
            System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/全国地名大全.txt"";
        path = path.replace(""\\"", ""/"");
        String text = ""一半天帕克斯曼是走不出丁字桥镇的"";
        Segment segment = HanLP.newSegment().enableCustomDictionary(false);
        Segment seg = new ViterbiSegment(path);
        System.out.println(""不启用字典的分词结果："" + segment.seg(text));
        System.out.println(""默认分词结果："" + HanLP.segment(text));
        seg.enableCustomDictionaryForcing(true).enableCustomDictionary(true);
        List<Term> termList = seg.seg(text);
        System.out.println(""自定义字典的分词结果："" + termList);
    }
```

**Describe the current behavior**
加载CustomDictionary.txt与全国地名大全.txt中, 应该包含'丁字桥镇'词条, 但实际的分词中并未切出
![image](https://github.com/hankcs/HanLP/assets/27196120/fa80c919-a984-446e-a2fa-bf03d27b5147)
![image](https://github.com/hankcs/HanLP/assets/27196120/f25fff1a-1e95-47aa-bc9a-f5957292fae1)

**Expected behavior**
'丁字桥镇'词条应被切出

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macos 13.3.1 (a) (22E772610a)
- Python version: n/a
- HanLP version: 1.8.4

**Other info / logs**
com/hankcs/hanlp/seg/Viterbi/ViterbiSegment.java中的loadCustomDic(String customPath, boolean isCache)在加载完DoubleArrayTrie后应替换对应词典
![image](https://github.com/hankcs/HanLP/assets/27196120/fa42b9bf-77c4-4489-b07a-adb17772f8a3)


* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
调用粗粒度分词API疑是存在内存泄漏？,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
针对一批法律文书（民事判决书，文件大小从2k-10k不等），循环调用粗粒度分词API进行全文分词（CPU mode），发现内存会持续上涨，最终内存溢出。

**Code to reproduce the issue**

```python
import os
import hanlp
import time
import psutil

def batch_parse_info(root_path):
    tok_model = hanlp.load('COARSE_ELECTRA_SMALL_ZH')

    fs_list = os.listdir(root_path)
    for fs in fs_list:
        full_fs = root_path + os.sep + fs

        content = ''
        with open(full_fs, 'r', encoding='utf-8') as f:
            for line in f:
                content += line + '\n'

        process = psutil.Process()
        memory_info1 = process.memory_info()
        result = tok_model(content)
        memory_info = process.memory_info()

        print(f""Memory info：{memory_info1.rss/1024.0/1024.0} MB, {memory_info.rss/1024.0/1024.0} MB, {(memory_info.rss-memory_info1.rss)/1024.0/1024.0} MB"")

if __name__ == '__main__':
    root_path = r'C:\Users\Jerson\MySource\anaconda\private-projects\data\final\liuyahaofile\民事判决书\原告'

    tm_beg = time.time()
    batch_parse_info(root_path)
    tm_end = time.time()
    
    print(f'Cost time: {tm_end-tm_beg}s')
```

**Describe the current behavior**

以下是部分日志：
Memory info：194.4296875 MB, 212.2265625 MB, 17.796875 MB
Memory info：212.2265625 MB, 212.35546875 MB, 0.12890625 MB
Memory info：212.35546875 MB, 213.3359375 MB, 0.98046875 MB
Memory info：213.3359375 MB, 214.21484375 MB, 0.87890625 MB
Memory info：214.21484375 MB, 215.55859375 MB, 1.34375 MB
Memory info：215.55859375 MB, 215.63671875 MB, 0.078125 MB
Memory info：215.63671875 MB, 216.0234375 MB, 0.38671875 MB
Memory info：216.0234375 MB, 216.02734375 MB, 0.00390625 MB
Memory info：216.02734375 MB, 216.03515625 MB, 0.0078125 MB
Memory info：216.03515625 MB, 216.0625 MB, 0.02734375 MB
Memory info：216.0625 MB, 216.06640625 MB, 0.00390625 MB
Memory info：216.06640625 MB, 216.0703125 MB, 0.00390625 MB
Memory info：216.0703125 MB, 216.0625 MB, -0.0078125 MB
Memory info：216.06640625 MB, 216.0703125 MB, 0.00390625 MB
Memory info：216.07421875 MB, 216.08203125 MB, 0.0078125 MB
Memory info：216.08203125 MB, 216.078125 MB, -0.00390625 MB
Memory info：216.078125 MB, 221.6015625 MB, 5.5234375 MB
Memory info：221.6015625 MB, 221.87890625 MB, 0.27734375 MB
Memory info：221.87890625 MB, 221.875 MB, -0.00390625 MB
Memory info：221.875 MB, 222.41015625 MB, 0.53515625 MB
Memory info：222.41015625 MB, 223.0 MB, 0.58984375 MB
Memory info：223.0 MB, 223.16015625 MB, 0.16015625 MB
Memory info：223.1640625 MB, 223.15625 MB, -0.0078125 MB
Memory info：223.15625 MB, 225.859375 MB, 2.703125 MB
Memory info：225.859375 MB, 225.6171875 MB, -0.2421875 MB
Memory info：225.6171875 MB, 225.37109375 MB, -0.24609375 MB
Memory info：225.37109375 MB, 225.36328125 MB, -0.0078125 MB
Memory info：225.3671875 MB, 225.37890625 MB, 0.01171875 MB
Memory info：225.37890625 MB, 225.390625 MB, 0.01171875 MB
Memory info：225.390625 MB, 225.41796875 MB, 0.02734375 MB
Memory info：225.42578125 MB, 227.04296875 MB, 1.6171875 MB
Memory info：227.046875 MB, 226.80078125 MB, -0.24609375 MB
Memory info：226.80078125 MB, 226.8125 MB, 0.01171875 MB
Memory info：226.8125 MB, 226.81640625 MB, 0.00390625 MB
Memory info：226.81640625 MB, 226.80859375 MB, -0.0078125 MB
Memory info：225.80859375 MB, 226.3125 MB, 0.50390625 MB
Memory info：226.3125 MB, 226.44140625 MB, 0.12890625 MB
Memory info：226.44140625 MB, 226.43359375 MB, -0.0078125 MB
Memory info：226.43359375 MB, 226.4375 MB, 0.00390625 MB
Memory info：226.4375 MB, 226.375 MB, -0.0625 MB
Memory info：226.375 MB, 226.58203125 MB, 0.20703125 MB
Memory info：226.58203125 MB, 226.73046875 MB, 0.1484375 MB
Memory info：226.73046875 MB, 226.71875 MB, -0.01171875 MB
Memory info：226.71875 MB, 226.734375 MB, 0.015625 MB
Memory info：226.734375 MB, 226.7265625 MB, -0.0078125 MB
Memory info：226.7265625 MB, 229.109375 MB, 2.3828125 MB
Memory info：229.109375 MB, 227.86328125 MB, -1.24609375 MB
Memory info：227.86328125 MB, 229.11328125 MB, 1.25 MB
Memory info：229.11328125 MB, 229.14453125 MB, 0.03125 MB
Memory info：229.14453125 MB, 228.89453125 MB, -0.25 MB
Memory info：228.89453125 MB, 228.90625 MB, 0.01171875 MB
Memory info：228.90625 MB, 228.90625 MB, 0.0 MB
Memory info：228.90625 MB, 228.91015625 MB, 0.00390625 MB
Memory info：228.91015625 MB, 230.1484375 MB, 1.23828125 MB
Memory info：230.1484375 MB, 230.1484375 MB, 0.0 MB
Memory info：230.1484375 MB, 230.16015625 MB, 0.01171875 MB
Memory info：230.16015625 MB, 230.171875 MB, 0.01171875 MB
Memory info：230.171875 MB, 229.9296875 MB, -0.2421875 MB
Memory info：229.9296875 MB, 229.9296875 MB, 0.0 MB
Memory info：229.9296875 MB, 229.94140625 MB, 0.01171875 MB
Memory info：229.94140625 MB, 229.9453125 MB, 0.00390625 MB
Memory info：229.9453125 MB, 231.2109375 MB, 1.265625 MB
Memory info：231.2109375 MB, 231.21484375 MB, 0.00390625 MB
Memory info：231.21484375 MB, 230.96484375 MB, -0.25 MB
Memory info：230.96484375 MB, 230.9765625 MB, 0.01171875 MB
Memory info：230.9765625 MB, 230.984375 MB, 0.0078125 MB
Memory info：230.984375 MB, 230.9765625 MB, -0.0078125 MB
Memory info：230.9765625 MB, 230.9765625 MB, 0.0 MB
Memory info：230.9765625 MB, 230.98046875 MB, 0.00390625 MB
Memory info：230.98046875 MB, 230.984375 MB, 0.00390625 MB
Memory info：230.984375 MB, 230.98046875 MB, -0.00390625 MB
Memory info：230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory info：230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory info：230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory info：230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory info：230.98046875 MB, 230.984375 MB, 0.00390625 MB
Memory info：230.984375 MB, 230.98828125 MB, 0.00390625 MB
Memory info：230.98828125 MB, 230.9921875 MB, 0.00390625 MB
Memory info：230.9921875 MB, 230.99609375 MB, 0.00390625 MB
Memory info：230.99609375 MB, 231.0 MB, 0.00390625 MB
Memory info：231.0 MB, 231.00390625 MB, 0.00390625 MB
Memory info：231.00390625 MB, 231.00390625 MB, 0.0 MB
Memory info：231.00390625 MB, 231.0078125 MB, 0.00390625 MB
Memory info：231.0078125 MB, 231.01171875 MB, 0.00390625 MB
Memory info：231.015625 MB, 231.015625 MB, 0.0 MB
Memory info：231.015625 MB, 231.0078125 MB, -0.0078125 MB
Memory info：231.0078125 MB, 231.01953125 MB, 0.01171875 MB
Memory info：231.01953125 MB, 231.0234375 MB, 0.00390625 MB
Memory info：231.0234375 MB, 231.0078125 MB, -0.015625 MB
Memory info：231.0078125 MB, 231.0078125 MB, 0.0 MB
Memory info：231.0078125 MB, 231.0078125 MB, 0.0 MB
Memory info：231.0078125 MB, 231.01171875 MB, 0.00390625 MB
Memory info：231.01171875 MB, 231.01953125 MB, 0.0078125 MB
Memory info：231.01953125 MB, 231.61328125 MB, 0.59375 MB
Memory info：231.61328125 MB, 231.62890625 MB, 0.015625 MB
Memory info：231.62890625 MB, 231.64453125 MB, 0.015625 MB
Memory info：231.64453125 MB, 231.64453125 MB, 0.0 MB
Memory info：231.64453125 MB, 231.65234375 MB, 0.0078125 MB
Memory info：231.65234375 MB, 231.640625 MB, -0.01171875 MB
Memory info：231.640625 MB, 232.265625 MB, 0.625 MB
Memory info：232.265625 MB, 232.28125 MB, 0.015625 MB
Memory info：232.28125 MB, 232.48828125 MB, 0.20703125 MB
Memory info：232.48828125 MB, 232.50390625 MB, 0.015625 MB
Memory info：232.50390625 MB, 232.26953125 MB, -0.234375 MB
Memory info：232.26953125 MB, 232.265625 MB, -0.00390625 MB
Memory info：232.265625 MB, 232.26171875 MB, -0.00390625 MB
Memory info：232.26171875 MB, 232.265625 MB, 0.00390625 MB
Memory info：232.265625 MB, 232.265625 MB, 0.0 MB
Memory info：232.265625 MB, 232.26953125 MB, 0.00390625 MB
Memory info：232.26953125 MB, 232.265625 MB, -0.00390625 MB
Memory info：232.265625 MB, 232.265625 MB, 0.0 MB


**Expected behavior**

内存较稳定，不会持续上涨

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10/ Linux version 5.10.102.1-microsoft-standard-WSL2
- Python version: 3.8.5
- HanLP version: 2.1.0b42. 也尝试升级到2.1.0b50，问题依旧

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
"a finer tokenizer than current FINE, to be able cut people names to first and last name","<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
Current behavior is always treat Personal name as one word,

https://github.com/hankcs/HanLP/issues/1829#issuecomment-1653800092

>2.2.1 Personal name
> Treat it as one word. Don’t give the internal structure unless there is a space between two names (in foreign alphabet).

but this isn't enough for applications scenarios where need a finer tokenizer, e.g. in a search index'ing application, need search query phrase `孔明` be able to match out `諸葛孔明` and also `パリピ孔明` in pre-index'ed vectors,

need some config to be able set, let user of the tokenizer to decide always treat names as one word, or to cut first/last in half,
especially for 

need a config to set minimum length for this behavior, for short names `張飛` `岳飛` might be okay to treat as one word,
but should have a way to cut longer names as `諸葛青云` `龐青云` `刘青云` to two parts, enable end user can search by `青云` to find out all text talking about `青云`

to even longer foreign names `米蘭昆德拉` `唐納德川普` `米歇爾·奥巴馬` there should have a way to cut into halves,

compare with the alternative library `nodejieba`, which has no problem to cut `米兰·昆德拉` to 2 parts:

```js
> var nodejieba = require('nodejieba');
> nodejieba.cut('为什么余华说米兰·昆德拉是三流小说家？')
[
  '为什么', '余',
  '华',     '说',
  '米兰',   '·',
  '昆德拉', '是',
  '三流',   '小说家',
  '？'
]
```

```py

In [107]: tok([""米蘭昆德拉"", ""米蘭·昆德拉"", ""为什么余华说米兰·昆德拉是三流小说家？"", ""米蘭昆德拉"", ""唐納德川普"", ""米歇爾·奥巴馬"", ""諸葛亮"", ""諸葛孔明"", ""諸葛亮先生"", ""パリピ孔明""])
Out[107]: 
[['米蘭昆德拉'],
 ['米蘭·昆德拉'],
 ['为什么', '余华', '说', '米兰·昆德拉', '是', '三流', '小说家', '？'],
 ['米蘭昆德拉'],
 ['唐納德川普'],
 ['米歇爾·奥巴馬'],
 ['諸葛亮'],
 ['諸葛孔明'],
 ['諸葛亮', '先生'],
 ['パリピ', '孔明']]


from hanlp_restful import HanLPClient
HanLP = HanLPClient('https://hanlp.hankcs.com/api', auth=None, language='zh') # auth不填则匿名，zh中文，mul多语种
HanLP([""米蘭昆德拉"", ""米蘭·昆德拉"", ""为什么余华说米兰·昆德拉是三流小说家？"", ""米蘭昆德拉"", ""唐納德川普"", ""米歇爾·奥巴馬"", ""諸葛亮"", ""諸葛孔明"", ""諸葛亮先生"", ""周恩来"", ""米蘭市"", ""洛杉磯縣"", ""南京市"",]) .pretty_print()

Dep 
─── 
┌─► 
└── 	Tok 
─── 
米蘭  
昆德拉 	Rela 
──── 
name 
root 	Po 
── 
NR 
NR 	Tok 
─── 
米蘭  
昆德拉 	NER Type     
──────────── 
───►LOCATION 
───►PERSON   	Tok 
─── 
米蘭  
昆德拉 	Po    3  
─────────
NR──┐    
NR──┴►TOP

 
 
 	Token  
────── 
米蘭·昆德拉 	Rela 
──── 
root 	Po 
── 
NR 	Token  
────── 
米蘭·昆德拉 	NER Type   
────────── 
───►PERSON 	Token  
────── 
米蘭·昆德拉 	Po    3 
────────
NR───►NP

Dep Tree  
───────── 
     ┌──► 
     │┌─► 
┌┬───┴┴── 
││  ┌───► 
││  │┌──► 
││  ││┌─► 
│└─►└┴┴── 
└───────► 	Token  
────── 
为什么    
余华     
说      
米兰·昆德拉 
是      
三流     
小说家    
？      	Relati 
────── 
advmod 
nsubj  
root   
nsubj  
cop    
amod   
ccomp  
punct  	Po 
── 
AD 
NR 
VV 
NR 
VC 
JJ 
NN 
PU 	Token  
────── 
为什么    
余华     
说      
米兰·昆德拉 
是      
三流     
小说家    
？      	NER Type   
────────── 
           
───►PERSON 
           
───►PERSON 
           
           
           
           	Token  
────── 
为什么    
余华     
说      
米兰·昆德拉 
是      
三流     
小说家    
？      	SRL PA1      
──────────── 
───►ARGM-ADV 
───►ARG0     
╟──►PRED     
◄─┐          
  │          
  ├►ARG1     
◄─┘          
             	Token  
────── 
为什么    
余华     
说      
米兰·昆德拉 
是      
三流     
小说家    
？      	SRL PA2  
──────── 
         
         
         
───►ARG0 
╟──►PRED 
◄─┐      
◄─┴►ARG1 
         	Token  
────── 
为什么    
余华     
说      
米兰·昆德拉 
是      
三流     
小说家    
？      	Po    3       4       5       6       7       8 
────────────────────────────────────────────────
AD───────────────────────────────────►ADVP──┐   
NR───────────────────────────────────►NP────┤   
VV──────────────────────────────────┐       │   
NR───────────────────►NP ───┐       ├►VP────┤   
VC──────────────────┐       ├►IP ───┘       ├►IP
JJ───►ADJP──┐       ├►VP ───┘               │   
NN───►NP ───┴►NP ───┘                       │   
PU──────────────────────────────────────────┘   

Dep 
─── 
┌─► 
└── 	Tok 
─── 
米蘭  
昆德拉 	Rela 
──── 
name 
root 	Po 
── 
NR 
NR 	Tok 
─── 
米蘭  
昆德拉 	NER Type     
──────────── 
───►LOCATION 
───►PERSON   	Tok 
─── 
米蘭  
昆德拉 	Po    3  
─────────
NR──┐    
NR──┴►TOP

 
 
 	Token 
───── 
唐納德川普 	Rela 
──── 
root 	Po 
── 
NR 	Token 
───── 
唐納德川普 	NER Type   
────────── 
───►PERSON 	Token 
───── 
唐納德川普 	Po    3 
────────
NR───►NP

 
 
 	Token   
─────── 
米歇爾·奥巴馬 	Rela 
──── 
root 	Po 
── 
NR 	Token   
─────── 
米歇爾·奥巴馬 	NER Type   
────────── 
───►PERSON 	Token   
─────── 
米歇爾·奥巴馬 	PoS
───
NR 

 
 
 	Tok 
─── 
諸葛亮 	Rela 
──── 
root 	Po 
── 
NR 	Tok 
─── 
諸葛亮 	NER Type   
────────── 
───►PERSON 	Tok 
─── 
諸葛亮 	Po    3 
────────
NR───►NP

 
 
 	Toke 
──── 
諸葛孔明 	Rela 
──── 
root 	Po 
── 
NR 	Toke 
──── 
諸葛孔明 	NER Type   
────────── 
───►PERSON 	Toke 
──── 
諸葛孔明 	Po    3 
────────
NR───►NP

Dep 
─── 
┌─► 
└── 	Tok 
─── 
諸葛亮 
先生  	Relation    
─────────── 
compound:nn 
root        	Po 
── 
NR 
NN 	Tok 
─── 
諸葛亮 
先生  	NER Type   
────────── 
───►PERSON 
           	Tok 
─── 
諸葛亮 
先生  	Po    3 
────────
NR──┐   
NN──┴►NP

 
 
 	Tok 
─── 
周恩来 	Rela 
──── 
root 	Po 
── 
NR 	Tok 
─── 
周恩来 	NER Type   
────────── 
───►PERSON 	Tok 
─── 
周恩来 	Po    3 
────────
NR───►NP

 
 
 	Tok 
─── 
米蘭市 	Rela 
──── 
root 	Po 
── 
NR 	Tok 
─── 
米蘭市 	NER Type     
──────────── 
───►LOCATION 	Tok 
─── 
米蘭市 	Po    3     4   
────────────────
NR───►NP───►FRAG

 
 
 	Toke 
──── 
洛杉磯縣 	Rela 
──── 
root 	Po 
── 
NR 	Toke 
──── 
洛杉磯縣 	NER Type     
──────────── 
───►LOCATION 	Toke 
──── 
洛杉磯縣 	Po    3     4   
────────────────
NR───►NP───►FRAG

 
 
 	Tok 
─── 
南京市 	Rela 
──── 
root 	Po 
── 
NR 	Tok 
─── 
南京市 	NER Type     
──────────── 
───►LOCATION 	Tok 
─── 
南京市 	Po    3     4   
────────────────
NR───►NP───►FRAG
```

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
anyone who need a finer tokenizer,

**Are you willing to contribute it (Yes/No):**
Yes, let me know how

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.10
- HanLP version:
In [87]: hanlp.version
Out[87]: '2.1.0-beta.50'

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
加载ner模型报错,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
使用MSRA_NER_BERT_BASE_ZH模型调用ner，会去下载模型，但是加载路径下模型文件是存在的.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
ner = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
sen_ner = tok(sentence)
print(sen_ner)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
OS: Windows-10-10.0.22621-SP0
Python: 3.8.12
PyTorch: 2.0.1+cpu
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50

**Other info / logs**
Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\PowerChen\work\workspace\Idea\python\hanlp-server\test\server\hanlp_api.py"", line 18, in <module>
    ner_handle = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 186, in load_from_meta_file
    raise e from None
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 106, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 215, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 225, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\components\taggers\transformers\transformer_tagger_tf.py"", line 34, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\layers\transformers\loader_tf.py"", line 11, in build_transformer
    tokenizer = AutoTokenizer_.from_pretrained(transformer)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\layers\transformers\pt_imports.py"", line 65, in from_pretrained
    tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\models\auto\tokenization_auto.py"", line 658, in from_pretrained
    config = AutoConfig.from_pretrained(
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 944, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\configuration_utils.py"", line 574, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\configuration_utils.py"", line 629, in _get_config_dict
    resolved_config_file = cached_file(
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\utils\hub.py"", line 452, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like bert-base-chinese is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
find a category of words that HanLP can't tokenize well: 米蘭昆德拉 / 唐納德川普 / 米歇爾·奥巴馬 /etc,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
find a category of words (外國人名) that HanLP can't tokenize well: 米蘭昆德拉 / 唐納德川普 / 米歇爾·奥巴馬 /etc

**Code to reproduce the issue**

```python
In [88]: tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
                                   
In [89]: tok([""米蘭昆德拉"", ""米蘭·昆德拉"", ""为什么余华说米兰·昆德拉是三流小说家？"", ""米蘭昆德拉"", ""唐納德川普"", ""米歇爾·奥巴馬""])
Out[89]: 
[['米蘭昆德拉'],
 ['米蘭·昆德拉'],
 ['为什么', '余华', '说', '米兰·昆德拉', '是', '三流', '小说家', '？'],
 ['米蘭昆德拉'],
 ['唐納德川普'],
 ['米歇爾·奥巴馬']]

In [84]: tok = hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH)
Downloading https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip to /home/ubuntu/.hanlp/tok/fine_electra_small_20220615_231803.zip
100%  43.5 MiB   2.6 MiB/s ETA:  0 s [=============================================================]
Decompressing /home/ubuntu/.hanlp/tok/fine_electra_small_20220615_231803.zip to /home/ubuntu/.hanlp/tok
                                   
In [85]: tok([""米蘭昆德拉"", ""米蘭·昆德拉"", ""为什么余华说米兰·昆德拉是三流小说家？"", ""米蘭昆德拉"", ""唐納德川普"", ""米歇爾·奥巴馬""])
Out[85]: 
[['米蘭昆德拉'],
 ['米蘭·昆德拉'],
 ['为什么', '余华', '说', '米兰·昆德拉', '是', '三流', '小说家', '？'],
 ['米蘭昆德拉'],
 ['唐納德', '川普'],
 ['米歇爾·奥巴馬']]
```

**Describe the current behavior**
all names seem to be always not cut

**Expected behavior**
those foreign full names should be tokenized between their first name and last name, the FINE_ one seems be able to cut '唐納德' '川普' but not much better, many other foreign full names are still not cut

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.10
- HanLP version:

In [87]: hanlp.__version__
Out[87]: '2.1.0-beta.50'


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
Unable to run in Docker; Can HanLP have some way to pre-install the pretrained models files? at pip install time?,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
When running inside docker with another username, it can't find the pretrained model files,

**Code to reproduce the issue**
the HanLP is trying to download the pretrained on first-time, this may not work under environments with no Internet access, (e.g. behind a Corporate firewall,)

so I use a little trick to run a this part code in docker building phase, to pre-warm up HanLP to not have to download again, it does download the necessary pretrained models files, and indeed work if run the container with same root username,
```log
Step 22/26 : RUN python3 ./app/text/tokenizer.py
 ---> Running in 241d468253cd
Downloading https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip to /root/.hanlp/tok/fine_electra_small_20220615_231803.zip
100%  43.5 MiB 440.1 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/tok/fine_electra_small_20220615_231803.zip to /root/.hanlp/tok
Downloading https://file.hankcs.com/hanlp/utils/char_table_20210602_202632.json.zip to /root/.hanlp/utils/char_table_20210602_202632.json.zip
100%  26.7 KiB  26.7 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/utils/char_table_20210602_202632.json.zip to /root/.hanlp/utils
Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_small_20210706_125427.zip to /root/.hanlp/transformers/electra_zh_small_20210706_125427.zip
100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/transformers/electra_zh_small_20210706_125427.zip to /root/.hanlp/transformers
```

**Describe the current behavior**
however, it downloaded those files to `/root/.hanlp/...`, and at run-time it often be run under another username, (e.g. AWS Lambda Container use another temporary generated username), then the same code would try to download again to `/home/username/.hanlp/...`

**Expected behavior**
Can HanLP have some way to pre-install the pretrained models files? at pip install time?

Or support a common path like `PRETRAIN_FILES_DIR=/common/path/`, so any user run this would get it from a common path;

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- Python version: `3.10`
- HanLP version: `hanlp==2.1.0b50`  (we use this but this is not HanLP version specific)

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
"
python3.10时，tf支持版本最低是2.8.0，setup中却只能等于2.6.0,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
- python3.10时，安装`extras_require=tf`时，`tensorflow==2.6.0`不能被满足
https://github.com/hankcs/HanLP/blob/31c34ec86f71fe91f1fe6d86e7ca8575c80e2306/setup.py#L24

- 因为[pypi](https://pypi.org/project/tensorflow/2.6.0/#files)中最多支持到python3.9

    ![image](https://github.com/hankcs/HanLP/assets/28639377/f5a7632b-193b-43f9-8f4b-358c5d1eca4e)

**Expected behavior**
- 期待库检查时，可适当放松版本限制

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- Python version: 3.10
- HanLP version: 2.1.0b49

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
branch:portable - com/hankcs/hanlp/collection/trie/bintrie/BinTrie.java全切分问题,"同:https://bbs.hankcs.com/t/2-4-5/5393
**Describe the bug**
在2.4.5 前缀树的妙用代码com/hankcs/hanlp/collection/trie/bintrie/BinTrie.java
中, 假设词典为[中华, 中华人"", 华人], 输入文本为[中华人], 那么i这个变量会不断自增 最终切出的词只有[中华, 中华人], 但是这个方法是全切分吗？是否应该切出[华人]这个词, 不知道是否我理解有误, 感谢解答

**Code to reproduce the issue**
`public class BinTriedTest {
    public static void main(String[] args) throws IOException {
        // 加载词典
        TreeMap<String, CoreDictionary.Attribute> dictionary =
            IOUtil.loadDictionary(""data/dictionary/TestDictionary.txt"");
        final BinTrie<CoreDictionary.Attribute> binTrie = new BinTrie<CoreDictionary.Attribute>(dictionary);
        List<String> wordlist = segmentFully(""中华人"", binTrie);
        System.out.println(wordlist);
    }

    public static List<String> segmentFully(final String text, BinTrie<CoreDictionary.Attribute> dictionary)
    {
        final List<String> wordList = new LinkedList<String>();
        dictionary.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
        {
            @Override
            public void hit(int begin, int end, CoreDictionary.Attribute value)
            {
                wordList.add(text.substring(begin, end));
            }
        });
        return wordList;
    }}`
![image](https://github.com/hankcs/HanLP/assets/27196120/6020e0a2-6813-485a-9c1d-dfe16fc9a43c)
![image](https://github.com/hankcs/HanLP/assets/27196120/9a431398-ec6c-44eb-88e8-8f504e10a05a)
![image](https://github.com/hankcs/HanLP/assets/27196120/b7eb537e-955b-4415-94e9-c5be6fa92f0f)


**Describe the current behavior**
![image](https://github.com/hankcs/HanLP/assets/27196120/eecbc8a8-7071-4d31-9ed6-2f97226a19e3)

output: [中华, 中华人]

**Expected behavior**
expected output: [中华, 中华人,华人]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:portable

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
在使用文本相似度比较时，两个字符串交换一下位置，得出来的文本相似度不一样,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
在使用文本相似度比较时，两个字符串交换一下位置，得出来的文本相似度不一样

**Code to reproduce the issue**
![image](https://user-images.githubusercontent.com/77441902/236605940-f5f62acf-d3f8-4a9b-adb1-b2d2edf342d3.png)


```python
```

**Describe the current behavior**
点击运行后，发现文本相似度的值不同

**Expected behavior**
交换字符串的位置，应该得到的文本相似度是一样的。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
Create NLP assignment,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ✨.
⚠️Changes must be made on dev branch.
-->

# Title of Your Pull Request

## Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

## Checklist

Check all items that apply.

- [ ] ⚠️Changes **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have checked my code and corrected any misspellings
"
成分句法分析的sampler_builder、batch_size参数,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
指定batch_size参数不起作用

**Code to reproduce the issue**
```python
import hanlp
from hanlp import pretrained

tokenizer_model = pretrained.tok.COARSE_ELECTRA_SMALL_ZH
tokenizer = hanlp.load(tokenizer_model, devices=0)
constituency_model = pretrained.constituency.CTB9_CON_ELECTRA_SMALL
parser = hanlp.load(constituency_model)

s = ""栏目下的一个节目，这个节目主要探讨的是英语口语的常用表达、词汇的起源和其背后的故事。""
sentences = [s] * 100
tokens = tokenizer(sentences, batch_size=bs)
parseing_tree = parser(batch_tokens, batch_size=bs)
```

**Describe the current behavior**
指定batch_size是不生效的，会自动加载config中的sampler_builder。
sampler_builder中：
- Tokenizer的batch_size是32
- constituency的batch_size是10

对于Tokenizer，添加`sampler_builder=None`就可以使batch_size生效: 
```
tokens = tokenizer(sentences, sampler_builder=None, batch_size=bs)
```
对于constituency，即使添加`sampler_builder=None`，也不会使batch_size生效，
```
parseing_tree = parser(batch_tokens, sampler_builder=None, batch_size=bs)
```

**Expected behavior**
对于Tokenizer和constituency：
- 指定了batch_size参数就起作用，不需要和sampler_builder=None配合
     - （即当指定了batch_size就要屏蔽config中的sampler_builder）

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 LTS
- Python version: Python 3.8.16
- HanLP version: 2.1.0b48

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
成分句法分析的标注标准,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
严格意义上，这不算一个Bug，是一个问题。（https://bbs.hankcs.com/ 上没有注册成功）

Github Readme中，说明的“成分句法分析”的标注标准是Chinese Tree Bank：https://hanlp.hankcs.com/docs/annotations/constituency/ctb.html
上述页面上的所有Tag：
```
'ADJP', 'ADVP', 'CLP', 'CP', 'DNP', 'DP', 'DVP', 'FRAG', 'INTJ',
'IP', 'LCP', 'LST', 'MSP', 'NN', 'NP', 'PP', 'PRN', 'QP', 'ROOT', 'UCP',
'VCD', 'VCP', 'VNV', 'VP', 'VPT', 'VRD', 'VSB'
```

而我在自己数据集上生成的所有Tag：
```
'CLP', 'QP', 'NP', 'ADJP', 'ADVP', 'IP', 'VP', 'PP', 'VSB',
'TOP', 'DP', 'LCP', 'CP', 'DNP', 'PRN', 'DVP', 'VRD', 'VCP',
'UCP', 'VCD', 'VPT', 'VNV', 'DFL', 'FLR', 'INTJ', 'LST', 'INC',
'EMO', 'FRAG', 'SKIP', 'IMG', 'IP=9', 'OTH', 'NP=2', 'NP=3', 'TYPO', 'IP=4'
```
实际生成的Tag与网页上的不一致：
- ROOT -> TOP
- 没有生成过'MSP'和'NN'（数据量不小）
- 生成的'DFL'、'EMO'、'FLR'、'IMG'、'INC'、'IP=4'、'IP=9'、'NP=2'、'NP=3'、'OTH'、'SKIP'、'TYPO'不在网页上


**Code to reproduce the issue**
None

**Describe the current behavior**
None

**Expected behavior**
希望得到一个所有可能生成的Tag（标签）列表。
更新网页上的标注标准。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): None
- Python version: None
- HanLP version: None

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
成分句法分析会修改原始文本，如修改中文的标点符号？,"<!--
感谢找出bug，请认真填写下表：
-->

**成分句法分析修改原始文本**

A clear and concise description of what the bug is.

**Code to reproduce the issue**
```python
import hanlp
from hanlp import pretrained

tokenizer_model = pretrained.tok.COARSE_ELECTRA_SMALL_ZH
tokenizer = hanlp.load(tokenizer_model)

toks = tokenizer(""栏目下的一个节目，这个节目主要探讨的是英语口语的常用表达、词汇的起源和其背后的故事。"")
print(toks)

constituency_model = pretrained.constituency.CTB9_CON_ELECTRA_SMALL
parser = hanlp.load(constituency_model)
tree = parser(toks)
print(tree)
```

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/53685945/230709750-3b17e2c4-4eb3-4fa2-a491-4da61f79a4d3.png)
为了好展示，修改了token是输出格式（为了代码的简洁，没有包含在演示代码中）



**Expected behavior**
保持原来的中文标点。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.9.2009 (Core)
- Python version: Python 3.8.16
- HanLP version: 2.1.0b47

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
加载MSRA_NER_ALBERT_BASE_ZH失败,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
使用pip install --upgrade hanlp[full]安装最新版本的hanlp，并使用ner = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)进行模型加载，但报如下错误：
![image](https://user-images.githubusercontent.com/20340633/227485545-c2209a84-f2f8-4be4-afa7-dfbac81b528d.png)


**Expected behavior**
期望能正常加载

**System information**
- OS Platform and Distribution:inux-5.4.0-139-generic-x86_64-with-glibc2.27
- Python version:3.8.0
- HanLP version:2.1.0-beta.46

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
調用Pretrained TOK跟NER的時候報錯,"**Describe the bug**
調用部分Pretrained TOK跟NER的時候報錯，已試過直接調用已下載到本地的Model也是出現相同的報錯
NER:
hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_CASED_EN
hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH
hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH
TOK:
hanlp.pretrained.tok.CTB6_CONVSEG
hanlp.pretrained.tok.LARGE_ALBERT_BASE
hanlp.pretrained.tok.PKU_NAME_MERGED_SIX_MONTHS_CONVSEG
hanlp.pretrained.tok.SIGHAN2005_MSR_CONVSEG
hanlp.pretrained.tok.SIGHAN2005_PKU_CONVSEG

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
NER_1 = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(hanlp.load('UD_TOK_MMINILMV2L12'), output_key='tok') \
    .append(hanlp.load('CONLL03_NER_BERT_BASE_CASED_EN'), output_key='ner', input_key='tok')
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS: Windows-10-10.0.19041-SP0
- Python: 3.9.0
- PyTorch: 1.13.1+cpu
- HanLP version: hanlp==2.1.0b45

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Decompressing C:\Users\AppData\Roaming\hanlp\ner/ner_conll03_bert_base_cased_en_20211227_121443.zip to C:\Users\AppData\Roaming\hanlp\ner
Failed to load https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_cased_en_20211227_121443.zip
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
......
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
文件流未正确关闭,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
A clear and concise description of what the bug is.
VectorsReader类的readVectorFile()方法未正确关闭文件流，导致资源泄漏
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
使用Word2VecTrainer的train方法，或者new 一个WordVectorModel对象，进程进行中，删除模型文件比如model.txt，会无法删除
```java
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.
我在使用该jar进行词向量模型训练或者文档转换为向量后，模型文件始终删除不掉，文件资源积压
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):任何系统都会出现
- Python version:
- HanLP version:目前发现1.5-1.8.3的版本都会出现该问题

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[VectorsReader.java](https://github.com/hankcs/HanLP/blob/v1.8.3/src/main/java/com/hankcs/hanlp/mining/word2vec/VectorsReader.java)
* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
增加快速构建配置,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ✨.
⚠️Changes must be made on dev branch.
-->

# 增加快速构建配置

## Description

增加快速构建配置：enableFastBuild，打开后构建时nextCheckPos会加1，产生更多空闲空间以降低构建耗时。

Fixes https://github.com/hankcs/HanLP/issues/1801

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

详见单测DoubleArrayTrieTest.testEnableFastBuild和AhoCorasickDoubleArrayTrieTest.testEnableFastBuild

## Checklist

Check all items that apply.

- [x] ⚠️Changes **must** be made on `dev` branch instead of `master`
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
希望增加tok保存空格的选项，以便分词后还原文本,"**Describe the feature and the current behavior/state.**

文本的空格（全形和半形）会在tok舍弃

**Will this change the current api? How?**

不知道

**Who will benefit with this feature?**

使用简繁转换的人

**Are you willing to contribute it (Yes/No):**

力有不逮

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Python version: 3.10.9
- HanLP version: 2.1.0b45，用`pip install hanlp`安装

**Any other info**


我主要是想用hanlp来进行文本简繁转换

因为opencc的简繁转换有时会出现问题（例如`只`和`隻`的转换）
在其github [#224 (comment)](https://github.com/BYVoid/OpenCC/issues/224#issuecomment-283668276)的讨论中，看到有人使用HanLP分词再丢给opencc
所以试了一整天，感觉不错
但是因为tok未能保存空格以文本未能成功还原

例子

```python
import hanlp
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
print(tok(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种Neuro-linguistic programming技术。', '阿婆主来到北京立方庭参观自然语义科技公司。']))
```
输出为：
```python
[['2021年', 'HanLPv2.1', '为', '生产', '环境', '带来', '次世代', '最', '先进', '的', '多', '语种', 'Neuro-linguistic', 'programming', '技术', '。'], ['阿婆', '主', '来到', '北京立方庭', '参观', '自然语义科技公司', '。']]
```

`Neuro-linguistic programming` 两个词中的空格消失了
把这段输出丢给opencc再还原后
就会变成`Neuro-linguisticprogramming`

因为我编程能力极度有限
现在我只是使用python读取txt档
再像上面那样python的hanlp的tok分词
再使用json.dumps掉进terminal
在terminal用`opencc`进行简繁转换
再使用`jq`,`sed`等工具还原文本

或者有没有什么更有效的分词简繁转换方法？
谢谢！


* [x] I've carefully completed this form."
修改一行代码可以让DAT构建速度提升N倍,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**

https://github.com/hankcs/HanLP/blob/1323221c38e9188b19cef3f770eec40148a459ac/src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java#L167

`int pos = Math.max(siblings.get(0).code + 1, nextCheckPos) - 1;`
改成
`int pos = Math.max(siblings.get(0).code, nextCheckPos);`

构建速度可以提升很多而且表现稳定，缺点是最终构建出的DAT大小微增。下面是我的测试数据：

<img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1414826/213084924-2dd7248d-d0ee-4476-b110-74b93be7a1d9.png"">

其中保留nextCheckPos为原版代码，去掉nextCheckPos用作对比，核心循环计数是在循环体内做了一个`count++`计数：
![image](https://user-images.githubusercontent.com/1414826/213085116-83c7c0c8-fa47-4b14-95fa-9307af0d067a.png)


**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Anyone who uses DAT

**Are you willing to contribute it (Yes/No):**

Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur
- Python version:
- HanLP version: hanlp-portable-1.8.3.jar

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
为分词器指定一份不同的词典不生效,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
https://github.com/hankcs/HanLP/blob/1.x/src/test/java/com/hankcs/demo/DemoCustomDictionary.java
代码72行，词典指定不生效

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Java
// 每个分词器都有一份词典，默认公用 CustomDictionary.DEFAULT，你可以为任何分词器指定一份不同的词典
        DynamicCustomDictionary myDictionary = new DynamicCustomDictionary(""data/dictionary/custom/CustomDictionary.txt"", ""data/dictionary/custom/机构名词典.txt"");
```

**Describe the current behavior**
A clear and concise description of what happened.
无法指定不同的词典，源代码词典加载只加载`HanLP.Config.CustomDictionaryPath`配置目录下的用户词典，该方法传参未使用
![image](https://user-images.githubusercontent.com/71128756/212274068-98991256-c1bc-46dd-9d29-351f5e6bf5c0.png)
`src/main/java/com/hankcs/hanlp/dictionary/DynamicCustomDictionary.loadMainDictionary`。
获取本地词典更新状态也只从`HanLP.Config.CustomDictionaryPath`目录判断，未使用方法参数。
`src/main/java/com/hankcs/hanlp/dictionary/DynamicCustomDictionary.isDicNeedUpdate`。
![image](https://user-images.githubusercontent.com/71128756/212274394-d7b09aca-2378-4685-9fb4-1783d0ce6a1d.png)


**Expected behavior**
希望指定词典可以生效

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- JDK version:1.8
- HanLP version:portable-1.8.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
人名识别对姓“张”识别不太准确,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
抽取了一些短语发现张特别容易没识别出来。
如下是具体的例子
张先生对接城西 分词: [张先生/nz, 对接/v, 城西/d]
张先生开封 分词: [张先生/nz, 开封/ns]
张阿姨 分词: [张/q, 阿姨/n]

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```   Segment segment = HanLP.newSegment().enableAllNamedEntityRecognize(true);
        List<Term> list = segment.seg(str);
        log.info(""##{} 分词: {}"", str, ArrayUtils.toString(list));
        CoreStopWordDictionary.apply(list);
```

**Describe the current behavior**
很多识别出来

**Expected behavior**
张先生 识别出 张 nr，或者 张先生 nr

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mocos 11.3.1
- Java version: 8
- HanLP version: portable-1.8.3

**Other info / logs**
无

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
load函数出现错误日志,"<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
加载不了下载到本地路径的预训练模型，不知道是否是版本问题？

**Code to reproduce the issue**
```
import hanlp

tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
```

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**System information**
OS: Linux-4.4.0-210-generic-x86_64-with-debian-buster-sid
Python: 3.7.13
PyTorch: 1.13.0+cu117
HanLP: 2.1.0-beta.44
---------------------------------------------------------------------------
BadZipFile                                Traceback (most recent call last)
/tmp/ipykernel_25466/3441843643.py in <module>
      1 import hanlp
      2 
----> 3 tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
      4 tok(['商品和服务。', '晓美焰来到北京立方庭参观自然语义科技公司'])

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    180         except:
    181             pass
--> 182         raise e from None
    183 
    184 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    104             else:
    105                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
--> 106                     obj.load(save_dir, verbose=verbose, **kwargs)
    107                 else:
    108                     obj.load(metapath, **kwargs)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/torch_component.py in load(self, save_dir, devices, verbose, **kwargs)
    171         if devices is None and self.model:
    172             devices = self.devices
--> 173         self.load_config(save_dir, **kwargs)
    174         self.load_vocabs(save_dir)
    175         if verbose:

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/torch_component.py in load_config(self, save_dir, filename, **kwargs)
    123         for k, v in self.config.items():
    124             if isinstance(v, dict) and 'classpath' in v:
--> 125                 self.config[k] = Configurable.from_config(v)
    126         self.on_config_ready(**self.config, save_dir=save_dir)
    127 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp_common/configurable.py in from_config(config, **kwargs)
     30             return cls(**deserialized_config)
     31         else:
---> 32             return cls.from_config(deserialized_config)
     33 
     34 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/transform.py in from_config(cls, config)
    256         config = dict(config)
    257         config.pop('classpath')
--> 258         return cls(**config)
    259 
    260 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/transform.py in __init__(self, mapper, src, dst)
    478         self.mapper = mapper
    479         if isinstance(mapper, str):
--> 480             mapper = get_resource(mapper)
    481         if isinstance(mapper, str):
    482             self._table = load_json(mapper)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in get_resource(path, save_dir, extract, prefix, append_location, verbose)
    339             path = realpath
    340     if extract and compressed:
--> 341         path = uncompress(path, verbose=verbose)
    342         if anchor:
    343             path = path_join(path, anchor)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in uncompress(path, dest, remove, verbose)
    258                 elif os.path.isdir(prefix):
    259                     shutil.rmtree(prefix)
--> 260             raise e
    261     if remove:
    262         remove_file(path)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in uncompress(path, dest, remove, verbose)
    223     else:
    224         try:
--> 225             with zipfile.ZipFile(path, ""r"") if ext == '.zip' else tarfile.open(path, 'r:*') as archive:
    226                 if not dest:
    227                     namelist = sorted(archive.namelist() if file_is_zip else archive.getnames())

~/anaconda3/envs/topic/lib/python3.7/zipfile.py in __init__(self, file, mode, compression, allowZip64, compresslevel)
   1256         try:
   1257             if mode == 'r':
-> 1258                 self._RealGetContents()
   1259             elif mode in ('w', 'x'):
   1260                 # set the modified flag so central directory gets written

~/anaconda3/envs/topic/lib/python3.7/zipfile.py in _RealGetContents(self)
   1323             raise BadZipFile(""File is not a zip file"")
   1324         if not endrec:
-> 1325             raise BadZipFile(""File is not a zip file"")
   1326         if self.debug > 1:
   1327             print(endrec)

BadZipFile: File is not a zip file

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
fix(sec): upgrade com.fasterxml.jackson.core:jackson-databind to 2.14.0-rc1,"### What happened？
There are 1 security vulnerabilities found in com.fasterxml.jackson.core:jackson-databind 2.13.2.2
- [CVE-2022-42004](https://www.oscs1024.com/hd/CVE-2022-42004)


### What did I do？
Upgrade com.fasterxml.jackson.core:jackson-databind from 2.13.2.2 to 2.14.0-rc1 for vulnerability fix

### What did you expect to happen？
Ideally, no insecure libs should be used.

### The specification of the pull request
[PR Specification](https://www.oscs1024.com/docs/pr-specification/) from OSCS"
HanLP语义相似度，希望可以输出句子的embedding以便做存储，提高效率,"**Describe the feature and the current behavior/state.**
当前使用sts，输入两个句子，对于大量句子比较，效率太低，虽然可以batch来做，但效率还是不够

**Will this change the current api? How?**
可以在sts里增加一个输出

**Who will benefit with this feature?**
sts使用者

**Are you willing to contribute it (Yes/No):**
No

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
HanLP语义相似度比较的效果不错，非常感谢作者的贡献，但现在有大量句子需要比较，希望HanLP能增加输出句子embedding的功能，先存储，使用时算cos距离，提高实际使用中的比较效率

* [x] I've carefully completed this form."
"Java API, com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer is not serializable.","**Describe the bug**
Java API, while com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.class be used as a member variable in Spark, it throws ""Caused by: java.io.NotSerializableException: com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer"".
That is because spark try to serialize `PerceptronLexicalAnalyzer `, but this class didn`t implements Serializable interface.
**Code to reproduce the issue**

public class Test{
    private final PerceptronLexicalAnalyzer analyzer;
    public Test() throws IOException {
        analyzer = new PerceptronLexicalAnalyzer(""./cws.bin"",
                ""./pos.bin"",
                ""./ner.bin"");
    }
    public static void main(String[] args) throws IOException {
        Test a = new Test();
        try {
            FileOutputStream fileOut = new FileOutputStream(""./test.ser"");
            ObjectOutputStream out = new ObjectOutputStream(fileOut);
            out.writeObject(a);
            out.close();
            fileOut.close();
            System.out.println(""Serialized data is saved in ./test.ser"");
        } catch (IOException i) {
            i.printStackTrace();
        }
    }
}

**Describe the current behavior**
Caused by: java.io.NotSerializableException: com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.

**Expected behavior**
No Exception been threw.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- HanLP version: portable-1.8.3


* [x] I've completed this form and searched the web for solutions."
指代消解的算法／模型什么时候会开源？,"**Describe the feature and the current behavior/state.**
指代消解算法／模型的开源，目前只有rest api

**Will this change the current api? How?**
不会

**Who will benefit with this feature?**
需要大量执行任务的用户

**Are you willing to contribute it (Yes/No):**
No

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
HanLP的指代消解效果太惊人了，不知道是因为训练数据的规模大与高质量，还是算法的改进？
虽然即使不开源，有免费的api调用，也非常不错了。但如果能详细地介绍一下数据和算法相关的细节，那就太好了。

* [x] I've carefully completed this form."
"SpringBoot加载相对路径data,报数组越界异常","<!--
感谢找出bug，请认真填写下表：
-->

**Describe the bug**
SpringBoot使用portable-1.8.3版本,修改了root路径为相对路径,放置在了resources/nlp目录,自定义了IOAdapter,使用NLPTokenizer时报错,报错信息如下
```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.<clinit>(AbstractLexicalAnalyzer.java:57)
	at com.hankcs.hanlp.tokenizer.NLPTokenizer.<clinit>(NLPTokenizer.java:39)
	at com.holly.top.springframework.config.TestNlp.main(TestNlp.java:21)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 32621
	at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToChar(ByteUtil.java:255)
	at com.hankcs.hanlp.corpus.io.ByteArray.nextChar(ByteArray.java:87)
	at com.hankcs.hanlp.dictionary.other.CharType.<clinit>(CharType.java:94)
	... 3 more
```

**Code to reproduce the issue**
```
public static void main(String[] args) {
        List<Term> ff=NLPTokenizer.segment(""我的名字叫邓欣雨"");
        for (Term term : ff) {
            System.out.println(term);
        }
    }
```


**Describe the current behavior**
IO适配器,已在hanlp.properties中打开
```
public class ResourceFileIoAdapter implements IIOAdapter {
    @Override
    public InputStream open(String path) throws IOException {
        ClassPathResource resource = new ClassPathResource(path);
        InputStream is = new FileInputStream(resource.getFile());
        return is;
    }

    @Override
    public OutputStream create(String path) throws IOException {
        ClassPathResource resource = new ClassPathResource(path);
        OutputStream os = new FileOutputStream(resource.getFile());
        return os;
    }
}
```


**Expected behavior**
希望能将data打包到jar包中,使用Nlp分词器

**System information**
- HanLP version: portable-1.8.3

**Other info / logs**
我的配置信息如下
```
root=nlp/
#IO适配器，实现com.hankcs.hanlp.corpus.io.IIOAdapter接口以在不同的平台（Hadoop、Redis等）上运行HanLP
#默认的IO适配器如下，该适配器是基于普通文件系统的。
IOAdapter=com.holly.top.springframework.config.ResourceFileIoAdapter
```
文件目录
![image](https://user-images.githubusercontent.com/76762269/194991248-675e17b4-16d7-4889-aba0-ba4c647f1d81.png)

* [x] I've completed this form and searched the web for solutions.
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->
<!-- ⬆️此处务必勾选，否则你的issue会被机器人自动删除！ -->"
add TSL cert verify switch to support network env behind private TSL …,"…gateway

<!--
Thank you for being interested in contributing to HanLP! You are awesome ✨.
⚠️Changes must be made on dev branch.
-->

# Allow user to ignore TSL cert check when using hanlp_restful API

## Description
When user is working in a network behind a gateway, which blocks / modifies the TSL certification, user need to add a cert file or ignore TSL cert check in order to user hanlp_restful API.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

## How Has This Been Tested?
1. It is used including this modification in the really world use case and works fine.
2. It failed to pass unittest `python -m unittest discover ./tests` due to API access limit (2 times/min allowed only ~~)

## Checklist

Check all items that apply.

- [x] ⚠️Changes **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
CSV 词典中不能包含逗号,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**

使用 CSV 文件作为词典时，由于部分词含有逗号会导致词典失败。

从代码上看，HanLP 只是单纯的使用逗号切分每一行，并没有处理 CSV 转义的情况。

列数据中存在 `""`, `,` 符号时会将该列使用 `""""` 进行转义。

**Code to reproduce the issue**

将以下文本直接保存为 csv 文件并加载词典。

```csv
19th century music
20 century British history
21st Century Music
21st century science & technology
2D Materials
3 Biotech
3D Printing and Additive Manufacturing
3D Printing in Medicine
3D Research
""3L: Language, Linguistics, Literature""
```

**Describe the current behavior**

```
Exception in thread ""main"" java.lang.NumberFormatException: For input string: "" Linguistics""
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.base/java.lang.Integer.parseInt(Integer.java:638)
	at java.base/java.lang.Integer.parseInt(Integer.java:770)
	at com.hankcs.hanlp.corpus.io.IOUtil.loadDictionary(IOUtil.java:794)
	at com.hankcs.hanlp.corpus.io.IOUtil.loadDictionary(IOUtil.java:752)
	at com.hankcs.hanlp.seg.Other.DoubleArrayTrieSegment.<init>(DoubleArrayTrieSegment.java:68)
	at org.grobid.core.lexicon.DictSegmenterKt.main(DictSegmenter.kt:6)
	at org.grobid.core.lexicon.DictSegmenterKt.main(DictSegmenter.kt)
```

**Expected behavior**

正常加载

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.1 LTS
- HanLP version:  com.hankcs:hanlp:portable-1.8.3


* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
BinTrie和DoubleArrayTrie序列化失败,"**Describe the bug**
这边使用HanLP 1.8.3的jar进行学习测试时，发现BinTrie和DoubleArrayTrie都是可序列化的，但是执行序列化时，提示hanlp.corpus.tag.Nature不是序列化实现~进行开发时会导致异常结束。


**Describe the current behavior**
序列化时抛出异常，序列化失败。

**Expected behavior**
希望可以序列化正常~

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Java version:JDK 11
- HanLP version:1.8.3

**Other info / logs**
java.io.NotSerializableException: com.hankcs.hanlp.corpus.tag.Nature

* [x] I've completed this form and searched the web for solutions."
日文成分句法分析结果错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
在使用hanlp_restful或hanlp时，使用示例代码无法复刻doc中日语文本的成分句法分析结果，且结果大部分都是错的，英文中文正常。
麻烦开发者大大看一下，谢谢！


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
代码
```
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)
print(HanLP(['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.',
             '2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。',
             '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。']))
```
目标日语文本
```
2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。
```

**Describe the current behavior**
A clear and concise description of what happened.
文档中描述的该示例中日语文本的成分句法分析结果应为：
```
 [""TOP"", [[""IP"", [[""NUM"", [""2021""]], [""NOUN"", [""年""]], [""PUNCT"", [""、""]], [""NOUN"", [""HanLPv2.1""]], [""IP"", [[""VP"", [[""VP"", [[""ADP"", [""は""]], [""NOUN"", [""次""]], [""NOUN"", [""世代""]], [""ADP"", [""の""]], [""ADJP"", [[""ADJP"", [[""ADJP"", [[""NOUN"", [""最""]]]], [""ADJP"", [[""NOUN"", [""先端""]]]]]], [""ADJP"", [[""NOUN"", [""多""]]]]]]]]]]]], [""NP"", [[""NP"", [[""NP"", [[""NP"", [[""NP"", [[""NOUN"", [""言語""]], [""NOUN"", [""NLP""]], [""NOUN"", [""技術""]]]], [""ADP"", [""を""]]]], [""NOUN"", [""本番""]], [""NOUN"", [""環境""]]]], [""PP"", [[""ADP"", [""に""]]]]]], [""VP"", [[""VERB"", [""導入""]], [""AUX"", [""します""]]]]]], [""PUNCT"", [""。""]]]]]],
```
实际运行结果为：
```
[""TOP"", [[""S"", [[""NP"", [[""NP"", [[""NP"", [[""NOUN"", [""2021年""]]]], [""PUNCT"", [""、""]], [""PROPN"", [""HanLPv2.1""]], [""ADP"", [""は""]], [""NP"", [[""NOUN"", [""次世代""]]]]]], [""ADP"", [""の""]], [""NOUN"", [""最""]], [""NOUN"", [""先端""]], [""NOUN"", [""多言語""]], [""NP"", [[""NOUN"", [""NLP""]], [""NOUN"", [""技術""]], [""NP"", [[""NP"", [[""NOUN"", [""を本番環境""]]]], [""IP"", [[""VP"", [[""VPT"", [[""ADP"", [""に""]], [""NP"", [[""NOUN"", [""導入""]]]]]]]]]]]], [""VERB"", [""し""]]]]]], [""NP"", [[""AUX"", [""ます""]]]], [""PUNCT"", [""。""]]]]]]
```
**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7.10
- HanLP version: 2.1.0b41

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
本人主要用于分析日语中的名词短语（NP）。
上面描述的示例日语文本分析出的结果为：
```
2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入し
2021年、HanLPv2.1は次世代
2021年
次世代
NLP技術を本番環境に導入し
を本番環境に導入
を本番環境
導入
ます
```
只有两条正确。是不是模型版本迭代时，日语句法成分分析任务漏掉了？麻烦开发者大大看一下，谢谢！

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
java  DynamicCustomDictionary load 词典时候不生效,"
**Describe the bug**
DynamicCustomDictionary

**Code to reproduce the issue**
  dictionary.load(""/home/duanfa/trash/845.txt"");
  DynamicCustomDictionary load 词典时候不生效 
845.txt 文件内容

爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791 nz 1
爱慕官方花 nz 1


 用  insert 生效
  dictionary.insert(""爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791"", ""N 1"");

或者在hanlp.properties里面配置
CustomDictionaryPath=data/dictionary/custom/duanfa/845.txt 
生效后，把 data/dictionary/custom/duanfa/845.txt.bin  拷贝到 /home/duanfa/trash/  目录下，dictionary.load(""/home/duanfa/trash/845.txt"");就可以生效了

版本
 <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.8.3</version>
        </dependency>

```import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.DynamicCustomDictionary;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;

public class LoadCustomFile {
	public DynamicCustomDictionary dictionary = new DynamicCustomDictionary();
	public Segment hanlpSegmenter;

	public LoadCustomFile() {
		try {
			hanlpSegmenter = HanLP.newSegment();
			hanlpSegmenter.enableCustomDictionary(dictionary).enableCustomDictionaryForcing(true);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	public static void main(String[] args) {
		String text = ""爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791"";
		LoadCustomFile lcf = new LoadCustomFile();
		lcf.dictionary.load(""/home/duanfa/trash/845.txt"");
		for (Term term : lcf.hanlpSegmenter.seg(text)) {
			System.out.println(term);
		}
	}
}
```
代码打印结果：
爱慕/vn
官方/n
花园/n
速写/n
内衣/n
女/b
无/v
钢圈/n
中/f
厚/a
杯/q
小/a
胸/ng
聚拢/v
蕾/ng
丝/q
文/ng
胸/ng
AM/nx
171791/m



**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
关于 java 版本中使用静态变量存储字典路径的问题,"**Describe the feature and the current behavior/state.**
当前 java 版本使用静态变量储存字典路径, 其有个弊端是无法在一个 classloader 下同时开启多个使用不同路径的 hanlp 的 instance. 
比如说我想开启两个实例, 一个用它来处理繁体, 一个用其处理简体. 这个在当前实现下很难实现, 因为两者的词典路径会冲突. 

**Will this change the current api? How?**
不会更改api,属于内部实现更改

**Who will benefit with this feature?**
所有的 Java 版本用户

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 与 OS 无关
- Python version: xxx
- HanLP version: 1.8.3-portable

**Any other info**
 No

* [x] I've carefully completed this form."
Create .travis.yml,"Sorry, I selected the wrong option while following the tutorial! Please discard this change attempt. Thank you.

<!--
Thank you for being interested in contributing to HanLP! You are awesome ✨.
⚠️Changes must be made on dev branch.
-->

# Title of Your Pull Request

## Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

## Checklist

Check all items that apply.

- [ ] ⚠️Changes **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have checked my code and corrected any misspellings
"
列表形式的输入，不同模型的结果长度不一致,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
处理列表形式的输入，tok对于空字符串或单个标点是有返回值的，pos 和dep则没有返回值，导致tok的结果不能和pos 和 dep的结果一一对齐
**Code to reproduce the issue**

```python
import hanlp
tok  = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
pos = hanlp.load(hanlp.pretrained.pos.PKU_POS_ELECTRA_SMALL)
dep = hanlp.load(hanlp.pretrained.dep.PMT1_DEP_ELECTRA_SMALL)

text = ['你好', '']
tok_r = tok(text)
# [['你好'], ['']]
pos_r = pos(tok_r)
# [['l']]  期望的返回应该是 [['l'], []]
dep_r = dep(tok_r)
# [[{'id': 1, 'form': '你好', 'cpos': None, 'pos': None, 'head': 0, 'deprel': 'HED', 'lemma': None, 'feats': None, 'phead': None, 'pdeprel': None}]]
len(tok_r) != len(pos_r) ==len(dep_r)
# True
```

**Describe the current behavior**
处理列表形式的输入，tok对于空字符串或单个标点是有返回值的，pos 和dep则没有返回值，导致tok的结果不能和pos 和 dep的结果一一对齐

**Expected behavior**
对于空字符串或单个标点等，pos 和dep也需要有个空列表的结果进行占位，用于对文本进行标注。

**System information**
- macos 11.6 
- Python version: 3.8.12
- HanLP version: 2.1.0b37

**Other info / logs**
no

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
bugfix: 修复 bintrie 树全分词时 提前跳出循环 bug,"# bintrie 不能完全分词

## Description

使用 BinTrie 的代码某些场景不能进行有效的完全切词


下面是 bug 演示:

```java
import com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie;
import org.junit.Before;
import org.junit.Test;

public class BinTrieParseTextTest {

    private BinTrie<Integer> trie;

    @Before
    public void setup() {
        this.trie = new BinTrie<Integer>();
        String[] words = new String[]{""溜"", ""儿"", ""溜儿"", ""一溜儿"", ""一溜""};
        /*构建一个简单的词典， 从 core dict 文件中扣出的一部分*/
        for (int i = 0; i < words.length; i++) {
            this.trie.put(words[i], i);
        }
    }


    @Test
    public void justForShowBugs() {
        showParseText(""一溜儿"");

        /*我们在 一溜儿后面随便+一个字符，这里我们加一个空格 会完全不同*/
        showParseText(""一溜儿"" + "" "");

    }


    private void showParseText(final String text) {
        System.out.printf(""========进行完全切词%s的演示======\n"", text);
        this.trie.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<Integer>() {
            @Override
            public void hit(int begin, int end, Integer value) {
                System.out.println(text.substring(begin, end));
            }
        });

        System.out.println(""==========================="");


    }
}

```

输出结果如下:

```
========进行完全切词一溜儿的演示======
一溜
一溜儿
===========================
========进行完全切词一溜儿 的演示======
一溜
一溜儿
溜
溜儿
儿
===========================
```


- 现象: 发现在 ""一溜儿"" 的情况分词不完全， 而把 BinTrie 改为 DoubleArrayTrie 则没有问题.
- 原因: debug 发现 bintrie 在分词命中了最后一个字符的时候 会提前跳出循环.


## How Has This Been Tested?

测试代码见 `com.hankcs.hanlp.collection.trie.bintrie.BinTrieParseTextTest.java`


"
demo脚本无法复现sota效果,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
执行sota脚本无法复现效果

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
1、下载sota脚本 HanLP/ plugins / hanlp_demo / hanlp_demo / zh / train_sota_bert_pku.py 
2、打开python shell 执行 脚本中的python代码 代码如下：
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer
from hanlp.datasets.tokenization.sighan2005.pku import SIGHAN2005_PKU_TRAIN_ALL, SIGHAN2005_PKU_TEST
from tests import cdroot

cdroot()
tokenizer = TransformerTaggingTokenizer()
save_dir = 'data/model/cws/sighan2005_pku_bert_base_96.66'
tokenizer.fit(
    SIGHAN2005_PKU_TRAIN_ALL,
    SIGHAN2005_PKU_TEST,  # Conventionally, no devset is used. See Tian et al. (2020).
    save_dir,
    'bert-base-chinese',
    max_seq_len=300,
    char_level=True,
    hard_constraint=True,
    sampler_builder=SortingSamplerBuilder(batch_size=32),
    epochs=10,
    adam_epsilon=1e-6,
    warmup_steps=0.1,
    weight_decay=0.01,
    word_dropout=0.1,
    seed=1609422632,
)
tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
- Python version: 3.8.8
- HanLP version: 2.1.0b37

**Other info / logs**
623/623 loss: 1416.3794 P: 59.23% R: 65.39% F1: 62.16% ET: 2 m 6 s  
  63/63 loss: 451.0509 P: 89.72% R: 89.54% F1: 89.63% ET: 4 s   
2 m 9 s / 21 m 33 s ETA: 19 m 24 s (saved)
Epoch 2 / 10:
623/623 loss: 286.6173 P: 31.52% R: 63.34% F1: 42.09% ET: 2 m 8 s   
  63/63 loss: 486.3771 P: 0.37% R: 8.27% F1: 0.71% ET: 4 s   
4 m 22 s / 21 m 50 s ETA: 17 m 28 s (1)
Epoch 3 / 10:
623/623 loss: 211.5965 P: 21.55% R: 61.44% F1: 31.91% ET: 2 m 8 s   
  63/63 loss: 470.1510 P: 0.39% R: 8.66% F1: 0.75% ET: 4 s   
6 m 34 s / 21 m 53 s ETA: 15 m 19 s (2)
Epoch 4 / 10:
623/623 loss: 173.5003 P: 16.42% R: 59.68% F1: 25.75% ET: 2 m 9 s   
  63/63 loss: 469.0070 P: 0.37% R: 8.32% F1: 0.71% ET: 4 s   
8 m 47 s / 21 m 57 s ETA: 13 m 10 s (3)
Epoch 5 / 10:
623/623 loss: 149.7656 P: 13.30% R: 58.04% F1: 21.63% ET: 2 m 9 s  
  63/63 loss: 482.1117 P: 0.38% R: 8.61% F1: 0.73% ET: 4 s   
10 m 59 s / 21 m 59 s ETA: 10 m 59 s (4)
Epoch 6 / 10:
623/623 loss: 131.2736 P: 11.19% R: 56.51% F1: 18.69% ET: 2 m 9 s  
  63/63 loss: 530.2560 P: 0.36% R: 8.13% F1: 0.69% ET: 4 s   
13 m 12 s / 22 m 0 s ETA: 8 m 48 s (5) early stop
Max score of dev is P: 89.72% R: 89.54% F1: 89.63% at epoch 1
Average time of each epoch is 2 m 12 s
13 m 12 s elapsed
P: 89.72% R: 89.54% F1: 89.63%
>>> tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)
Pruned 0 (0.0%) samples out of 2004.                             
63/63 loss: 451.0509 P: 89.72% R: 89.54% F1: 89.63% ET: 4 s   
speed: 531 samples/second
(P: 89.72% R: 89.54% F1: 89.63%, (451.0508732871404, P: 89.72% R: 89.54% F1: 89.63%))

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
自定义词典强制模式不起作用,"**Describe the bug**
添加自定义词典，强制模式不起作用

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import hanlp
HanLP = hanlp.load(“component”)
text = “海航haihangMU3456航班”
HanLP.dict_force = {“haihang”}
print(""-->"", HanLP([text])[“tok/fine”])
--> [[‘海航’, ‘haihangMU3456’, ‘航班’]]
```

**System information**
- OS Platform and Distribution: macOS big sur 11.3.1
- Python version: 3.7
- HanLP version: 2.1


* [x] I've completed this form and searched the web for solutions."
如何加载multi task learning预训练的某个模型,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->

您好，非常不好意思打扰，在readme里面有这么一句话：
```
根据我们的[最新研究](https://aclanthology.org/2021.emnlp-main.451)，单任务学习的性能往往优于多任务学习。在乎精度甚于速度的话，建议使用[单任务模型](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/index.html)。
```

所以我将SRL模型从multi task learning预训练模型抽出来，但是预测的结果并不一样，所以麻烦作者大大帮忙看下具体问题在哪，应该怎么修改。更具体代码请看：[bbs hankcs 如何将multi task训练的某个模型抽取出来 ](https://bbs.hankcs.com/t/multi-task/4874)。


非常感谢！！

"
[非bug]transformer_encode进行gather index span时不合理,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
你好，我在看到[transformer_tokenizer.py](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/transform/transformer_tokenizer.py#L244)时，你对CLS和SEP也计算了`token_token_span`，接着你会调用[pick_tensor_for_each_token](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/layers/transformers/utils.py#L82)这个函数来获取首字向量或者平均向量，随后你会在每一个任务里面获取句子向量时都是通过`encoder_hidden[:, 1:-1:, ]`来表示，其中`-1`在有padding的情况下不会是[SEP]。


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
hanlp.common.transform.NormalizeCharacter转换后和原句不同,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.

[这里](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/common/transform.py#L493)，比如使用句子`与此同时，我们也为用户提供了客服申诉渠道、文档找回路径。`经过此函数转换后变成了`与此同时,我们也为用户提供了客服申诉渠道,文档找回路径。`


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python

```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
求教：load BERT 模型报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20211227_114712.zip.

**Code to reproduce the issue**
recog = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

```python
```

**Describe the current behavior**
报错如下traceback

**Expected behavior**
消除报错

**System information**
OS: Windows-10-10.0.19041-SP0
Python: 3.8.3
PyTorch: 1.12.0+cpu
TensorFlow: 2.6.0
HanLP: 2.1.0-beta.36

**Other info / logs**

OSError                                   Traceback (most recent call last)
<ipython-input-6-0638e9495b92> in <module>
----> 1 recog = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

~\AppData\Roaming\Python\Python38\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    173         except:
    174             pass
--> 175         raise e from None
    176 
    177 

~\AppData\Roaming\Python\Python38\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     97             else:
     98                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 99                     obj.load(save_dir, verbose=verbose, **kwargs)
    100                 else:
    101                     obj.load(metapath, **kwargs)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\common\keras_component.py in load(self, save_dir, logger, **kwargs)
    212         self.load_config(save_dir)
    213         self.load_vocabs(save_dir)
--> 214         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    215         self.load_weights(save_dir, **kwargs)
    216         self.load_meta(save_dir)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\common\keras_component.py in build(self, logger, **kwargs)
    222     def build(self, logger, **kwargs):
    223         self.transform.build_config()
--> 224         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
    225                                                    loss=kwargs.get('loss', None)))
    226         self.transform.lock_vocabs()

~\AppData\Roaming\Python\Python38\site-packages\hanlp\components\taggers\transformers\transformer_tagger_tf.py in build_model(self, transformer, max_seq_length, **kwargs)
     32 
     33     def build_model(self, transformer, max_seq_length, **kwargs) -> tf.keras.Model:
---> 34         model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
     35         self.transform.tokenizer = tokenizer
     36         return model

~\AppData\Roaming\Python\Python38\site-packages\hanlp\layers\transformers\loader_tf.py in build_transformer(transformer, max_seq_length, num_labels, tagging, tokenizer_only)
      9 
     10 def build_transformer(transformer, max_seq_length, num_labels, tagging=True, tokenizer_only=False):
---> 11     tokenizer = AutoTokenizer_.from_pretrained(transformer)
     12     if tokenizer_only:
     13         return tokenizer

~\AppData\Roaming\Python\Python38\site-packages\hanlp\layers\transformers\pt_imports.py in from_pretrained(cls, pretrained_model_name_or_path, use_fast, do_basic_tokenize)
     66         if use_fast and not do_basic_tokenize:
     67             warnings.warn('`do_basic_tokenize=False` might not work when `use_fast=True`')
---> 68         tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
     69                                         do_basic_tokenize=do_basic_tokenize,
     70                                         **additional_config)

~\AppData\Roaming\Python\Python38\site-packages\transformers\models\auto\tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    533         if config_tokenizer_class is None:
    534             if not isinstance(config, PretrainedConfig):
--> 535                 config = AutoConfig.from_pretrained(
    536                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs
    537                 )

~\AppData\Roaming\Python\Python38\site-packages\transformers\models\auto\configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    703         kwargs[""name_or_path""] = pretrained_model_name_or_path
    704         trust_remote_code = kwargs.pop(""trust_remote_code"", False)
--> 705         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    706         if ""auto_map"" in config_dict and ""AutoConfig"" in config_dict[""auto_map""]:
    707             if not trust_remote_code:

~\AppData\Roaming\Python\Python38\site-packages\transformers\configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    551         original_kwargs = copy.deepcopy(kwargs)
    552         # Get config dict associated with the base config file
--> 553         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
    554 
    555         # That config file may point us toward another config file to use.

~\AppData\Roaming\Python\Python38\site-packages\transformers\configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    639             )
    640         except EnvironmentError:
--> 641             raise EnvironmentError(
    642                 f""Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from ""
    643                 ""'https://huggingface.co/models', make sure you don't have a local directory with the same name. ""

OSError: Can't load config for 'bert-base-chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-chinese' is the correct path to a directory containing a config.json file
=================================ERROR LOG ENDS=================================


* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Hanlp native无法下载模型,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**

无法下载hanlp native模型 failed to download a hanlp native model

Download failed due to ConnectionError(MaxRetryError(""HTTPSConnectionPool(host='file.hankcs.workers.dev', port=443): Max retries exceeded with url: /hanlp/transformers/electra_zh_base_20210706_125233.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024B8D5CA4C0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))"")).

Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip.



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



```python
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)
```


**Describe the current behavior**
模型下载失败

**Expected behavior**
模型成功下载

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version:  Python 3.8.8 
- HanLP version: 2.1.0b35

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ConnectionError                           Traceback (most recent call last)
<ipython-input-33-7a52ebc07181> in <module>
----> 1 HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)

E:\anacoda\lib\site-packages\hanlp\__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

E:\anacoda\lib\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     30     identifier = save_dir
     31     load_path = save_dir
---> 32     save_dir = get_resource(save_dir)
     33     if save_dir.endswith('.json'):
     34         meta_filename = os.path.basename(save_dir)

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in get_resource(path, save_dir, extract, prefix, append_location, verbose)
    338             realpath += compressed
    339         if not os.path.isfile(realpath):
--> 340             path = download(url=path, save_path=realpath, verbose=verbose)
    341         else:
    342             path = realpath

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in download(url, save_path, save_dir, prefix, append_location, verbose)
    183             elif hasattr(e, 'args') and e.args and isinstance(e.args, tuple) and isinstance(e.args[0], str):
    184                 e.args = (e.args[0] + '\n' + remove_color_tag(message),) + e.args[1:]
--> 185             raise e from None
    186         remove_file(save_path)
    187         os.rename(tmp_path, save_path)

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in download(url, save_path, save_dir, prefix, append_location, verbose)
    153             if verbose:
    154                 downloader.subscribe(DownloadCallback(show_header=False))
--> 155             downloader.start_sync()
    156         except BaseException as e:
    157             remove_file(tmp_path)

E:\anacoda\lib\site-packages\hanlp_downloader\down.py in start_sync(self)
    112             raise RuntimeError('Download has already been started.')
    113 
--> 114         self.run()
    115 
    116     def pause(self):

E:\anacoda\lib\site-packages\hanlp_downloader\down.py in run(self)
    150         s.mount('https://', adapter)
    151 
--> 152         r = s.get(self.url, stream=True, headers=self.headers)
    153         if r.status_code != 200:
    154             raise HTTPError(self.url, r.status_code, f'Internet error', r.headers, None)

E:\anacoda\lib\site-packages\requests\sessions.py in get(self, url, **kwargs)
    553 
    554         kwargs.setdefault('allow_redirects', True)
--> 555         return self.request('GET', url, **kwargs)
    556 
    557     def options(self, url, **kwargs):

E:\anacoda\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    540         }
    541         send_kwargs.update(settings)
--> 542         resp = self.send(prep, **send_kwargs)
    543 
    544         return resp

E:\anacoda\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    675             # Redirect resolving generator.
    676             gen = self.resolve_redirects(r, request, **kwargs)
--> 677             history = [resp for resp in gen]
    678         else:
    679             history = []

E:\anacoda\lib\site-packages\requests\sessions.py in <listcomp>(.0)
    675             # Redirect resolving generator.
    676             gen = self.resolve_redirects(r, request, **kwargs)
--> 677             history = [resp for resp in gen]
    678         else:
    679             history = []

E:\anacoda\lib\site-packages\requests\sessions.py in resolve_redirects(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)
    235             else:
    236 
--> 237                 resp = self.send(
    238                     req,
    239                     stream=stream,

E:\anacoda\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    653 
    654         # Send the request
--> 655         r = adapter.send(request, **kwargs)
    656 
    657         # Total elapsed time of the request (approximately)

E:\anacoda\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='file.hankcs.workers.dev', port=443): Max retries exceeded with url: /hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024B8D6288E0>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE模型处理文本时报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE模型处理文本时报错

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

model5 = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)
model5('很好br ~%?…;# *’☆&℃$︿★?')
```

**Describe the current behavior**
A clear and concise description of what happened.
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-20-3ec54f8a3e83> in <module>
----> 1 model5('很好br ~%?…;# *’☆&℃$︿★?')

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in __call__(self, data, **kwargs)
    769 
    770     def __call__(self, data, **kwargs) -> Document:
--> 771         return super().__call__(data, **kwargs)
    772 
    773     def __getitem__(self, task_name: str) -> Task:

/opt/app/conda/lib/python3.6/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---> 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in __call__(self, *args, **kwargs)
    636             **kwargs: Used in sub-classes.
    637         """"""
--> 638         return super().__call__(*args, **merge_dict(self.config, overwrite=True, **kwargs))

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in __call__(self, *args, **kwargs)
     34 
     35         """"""
---> 36         return self.predict(*args, **kwargs)

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict(self, data, tasks, skip_tasks, resolved_tasks, **kwargs)
    503             # Run the first task, let it make the initial batch for the successors
    504             output_dict = self.predict_task(first_task, first_task_name, batch, results, run_transform=True,
--> 505                                             cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
    506             # Run each task group in order
    507             for group_id, group in enumerate(target_tasks):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict_task(self, task, output_key, batch, results, output_dict, run_transform, cls_is_bos, sep_is_eos)
    593         output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
    594                                              results)
--> 595         self.decode_output(output_dict, batch, output_key)
    596         results[output_key].extend(task.prediction_to_result(output_dict[output_key]['prediction'], batch))
    597         return output_dict

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in decode_output(self, output_dict, batch, task_name)
    733                 output_per_task['mask'],
    734                 batch,
--> 735                 self.model.decoders[task_name])
    736 
    737     def update_metrics(self, batch: Dict[str, Any], output_dict: Dict[str, Any], metrics: MetricDict, task_name):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/tasks/tok/tag_tok.py in decode_output(self, output, mask, batch, decoder, **kwargs)
    122     def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],
    123                       mask: torch.BoolTensor, batch: Dict[str, Any], decoder, **kwargs) -> Union[Dict[str, Any], Any]:
--> 124         return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
    125 
    126     def update_metrics(self, batch: Dict[str, Any],

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in decode_output(self, logits, mask, batch, model)
    111             output = output.tolist()
    112         prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
--> 113         return self.tag_to_span(prediction, batch)
    114 
    115     def tag_to_span(self, batch_tags, batch: dict):

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in tag_to_span(self, batch_tags, batch)
    129                     for start, end, label in custom_words:
    130                         if end - start == 1:
--> 131                             tags[start] = S
    132                         else:
    133                             tags[start] = 'B'

IndexError: list assignment index out of range

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.6.11
- HanLP version: 2.1.0-beta.34

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6模型处理文本时报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6模型处理文本时报错

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

model4 = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6)
model4(""~(=^‥^)_ ~(=^‥^)_ ~(=^‥^)_"")
```

**Describe the current behavior**
A clear and concise description of what happened.
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-12-d6c0a38fb7b1> in <module>
----> 1 model4(""~(=^‥^)_ ~(=^‥^)_ ~(=^‥^)_"")

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in __call__(self, data, **kwargs)
    769 
    770     def __call__(self, data, **kwargs) -> Document:
--> 771         return super().__call__(data, **kwargs)
    772 
    773     def __getitem__(self, task_name: str) -> Task:

/opt/app/conda/lib/python3.6/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---> 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in __call__(self, *args, **kwargs)
    636             **kwargs: Used in sub-classes.
    637         """"""
--> 638         return super().__call__(*args, **merge_dict(self.config, overwrite=True, **kwargs))

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in __call__(self, *args, **kwargs)
     34 
     35         """"""
---> 36         return self.predict(*args, **kwargs)

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict(self, data, tasks, skip_tasks, resolved_tasks, **kwargs)
    503             # Run the first task, let it make the initial batch for the successors
    504             output_dict = self.predict_task(first_task, first_task_name, batch, results, run_transform=True,
--> 505                                             cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
    506             # Run each task group in order
    507             for group_id, group in enumerate(target_tasks):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict_task(self, task, output_key, batch, results, output_dict, run_transform, cls_is_bos, sep_is_eos)
    593         output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
    594                                              results)
--> 595         self.decode_output(output_dict, batch, output_key)
    596         results[output_key].extend(task.prediction_to_result(output_dict[output_key]['prediction'], batch))
    597         return output_dict

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in decode_output(self, output_dict, batch, task_name)
    733                 output_per_task['mask'],
    734                 batch,
--> 735                 self.model.decoders[task_name])
    736 
    737     def update_metrics(self, batch: Dict[str, Any], output_dict: Dict[str, Any], metrics: MetricDict, task_name):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/tasks/tok/tag_tok.py in decode_output(self, output, mask, batch, decoder, **kwargs)
    122     def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],
    123                       mask: torch.BoolTensor, batch: Dict[str, Any], decoder, **kwargs) -> Union[Dict[str, Any], Any]:
--> 124         return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
    125 
    126     def update_metrics(self, batch: Dict[str, Any],

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in decode_output(self, logits, mask, batch, model)
    111             output = output.tolist()
    112         prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
--> 113         return self.tag_to_span(prediction, batch)
    114 
    115     def tag_to_span(self, batch_tags, batch: dict):

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in tag_to_span(self, batch_tags, batch)
    129                     for start, end, label in custom_words:
    130                         if end - start == 1:
--> 131                             tags[start] = S
    132                         else:
    133                             tags[start] = 'B'

IndexError: list assignment index out of range

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: Python 3.6.11
- HanLP version: 2.1.0-beta.34

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
自己训练的pos.bin不能加载,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
自己训练的crf模型文件，pos.bin不能加载，但是同时生成的pos.bin.txt就可以加载成功。


**Code to reproduce the issue**


```java
CRFPOSTagger tagger = new CRFPOSTagger(null); // 创建空白标注器
//        tagger = new CRFPOSTagger(PKU.POS_MODEL); // 加载
         tagger = new CRFPOSTagger(""/root/repo/hanlp-java/HanLP/data/test/pos.bin""); // 加载
        System.out.println(Arrays.toString(tagger.tag(""他"", ""的"", ""希望"", ""是"", ""希望"", ""上学""))); // 预测
        AbstractLexicalAnalyzer analyzer = new AbstractLexicalAnalyzer(new PerceptronSegmenter(), tagger); // 构造词法分析器
        System.out.println(analyzer.analyze(""李狗蛋的希望是希望上学"")); // 分词+词性标注
```
报错如下：
java.lang.ArrayIndexOutOfBoundsException: 1677721600

	at com.hankcs.hanlp.model.perceptron.feature.FeatureMap.loadTagSet(FeatureMap.java:99)
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:92)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421)
	at com.hankcs.hanlp.model.crf.LogLinearModel.load(LogLinearModel.java:58)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388)
	at com.hankcs.hanlp.model.crf.LogLinearModel.<init>(LogLinearModel.java:83)
	at com.hankcs.hanlp.model.crf.CRFTagger.<init>(CRFTagger.java:41)
	at com.hankcs.hanlp.model.crf.CRFPOSTagger.<init>(CRFPOSTagger.java:45)
	at com.hankcs.hanlp.model.crf.CRFPOSTaggerTest.testTrain(CRFPOSTaggerTest.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:255)
	at junit.framework.TestSuite.run(TestSuite.java:250)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)

**Describe the current behavior**
100%出错

**Expected behavior**
能正常加载pos.bin

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- jkd version:jdk 1.8
- HanLP version: 1.7.5 and 1.8.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
CrfppTrainHanLPLoad执行时报 “[0/n 构造单词时失败”的错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
CrfppTrainHanLPLoad程序在执行时，如果文本中出现""[0\n""这样的字符串，会报空指针错误：


debug发现，对HanLP-1.8.3/src/main/java/com/hankcs/hanlp/model/perceptron/utility/IOUtility.java，第73行进行修改
```
# if (sentence.wordList.size() == 0) continue;
if (sentence==null || sentence.wordList.size() == 0) continue;
```
但是又报：[0/n 构造单词时失败
debug发现：
/root/repo/1.8.3/HanLP-1.8.3/src/main/java/com/hankcs/hanlp/corpus/document/sentence/word/WordFactory.java

```java
public static IWord create(String param)
    {
        if (param == null) return null;
        if (param.startsWith(""["") && !param.startsWith(""[/""))
        {
            return CompoundWord.create(param);
        }
        else
        {
            return Word.create(param);
        }
    }
```
这个意思是如果以""[""开头的话，调用CompoundWord.create，继续debug发现，在CompoundWord类中的create方法中的这段代码

```
public static CompoundWord create(String param)
    {
        if (param == null) return null;
        int cutIndex = param.lastIndexOf(']');
        if (cutIndex <= 2 || cutIndex == param.length() - 1) return null;
```
我理解如果包含了这种的分词结果：""[0/n""，那执行的结果肯定是null，这里为什么这样处理，能解释一下么？我想应该是有理由的，但是这里执行上有问题。能解释一下非常感谢！！

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
public void testCwsTest() {

        
        str = ""操作/vn 时间/n ：/w 20210115/m 15/m :/w 19/m :/w 操作人/n ：/w 刘X/nr (/w 1390000005/m )/w 重新处理/n "" +
                ""说明/v ：/w 角色/n 内/f 产品/n 总/b 数量/n 为/p [0/n ]/w ,/w 不/d 满足/v 角色/n [/w 201908011652/m :/w :/w"" +
                "" IPTV/nz 设备/n 产品/n 分组/n ]/w 设定/v 的/u 最/d 小值/n 为/v [/w 1/m ]/w 的/u 限制/vn ，/w 请/v 对/p 错误/n"" +
                "" 界面/n 进行/v 截图/vn 并/c 发送给/n 系统管理员/nnt ;/w"";
        Sentence sentence;
        if (str == null) {
            System.out.println(""nok"");
        } else {
            str = str.trim();
            if (str.isEmpty()) {
                System.out.println(""nok"");
            } else {
                Pattern pattern = Pattern.compile(""(\\[(([^\\s\\]]+/[0-9a-zA-Z]+)\\s+)+?([^\\s\\]]+/[0-9a-zA-Z]+)]/?[0-9a-zA-Z]+)|([^\\s]+/[0-9a-zA-Z]+)"");
                Matcher matcher = pattern.matcher(str);
                List<IWord> wordList = new LinkedList();

                while (matcher.find()) {
                    String single = matcher.group();
                    IWord word = WordFactory.create(single);
                    if (word == null) {
                        System.out.println(""在用 "" + single + "" 构造单词时失败，句子构造参数为 "" + str);

                    }

                    wordList.add(word);
                }


            }

        }
    }
```

**Describe the current behavior**
100%重现问题

**Expected behavior**
执行成功

**SystemX information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
- Python version:
- jdk version: jdk1.8
- HanLP version: 1.7.5 和1.8.3

**Other info / logs**


* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
UD 2.10 数据集依存句法分析模型训练报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD 2.10 数据集训练报错

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
ud = UniversalDependenciesParser()
ud.fit(
    trn_data=UD_210_MULTILINGUAL_TRAIN,
    dev_data=UD_210_MULTILINGUAL_DEV,
    save_dir=save_dir,
    transformer=ContextualWordEmbedding('token',
                            ""nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large"",
                            average_subwords=True,
                            max_sequence_length=256,
                            word_dropout=0.2),
    max_seq_len=256,
    sampler_builder=SortingSamplerBuilder(batch_size=16),
    epochs=10,
    seed=42,
    tree=True,
    dependencies='tok',
    mix_embedding=0,
    transform=NormalizeToken(dst=""token"",
                             src=""token"",
                             mapper={
                              ""-LRB-"": ""("",
                              ""-RRB-"": "")"",
                              ""-LCB-"": ""{"",
                              ""-RCB-"": ""}"",
                              ""-LSB-"": ""["",
                              ""-RSB-"": ""]"",
                              ""``"": ""\"""",
                              ""''"": ""\"""",
                              ""`"": ""'"",
                              ""«"": ""\"""",
                              ""»"": ""\"""",
                              ""‘"": ""'"",
                              ""’"": ""'"",
                              ""“"": ""\"""",
                              ""”"": ""\"""",
                              ""„"": ""\"""",
                              ""‹"": ""'"",
                              ""›"": ""'"",
                              ""–"": ""--"",
                              ""—"": ""--""}),
)
```

**Describe the current behavior**
A clear and concise description of what happened.

RuntimeError                              Traceback (most recent call last)
<ipython-input-6-d2a750e8a8b0> in <module>
     37                               ""›"": ""'"",
     38                               ""–"": ""--"",
---> 39                               ""—"": ""--""}),
     40 )

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in fit(self, trn_data, dev_data, save_dir, transformer, sampler_builder, mix_embedding, layer_dropout, n_mlp_arc, n_mlp_rel, mlp_dropout, lr, transformer_lr, patience, batch_size, epochs, gradient_accumulation, adam_epsilon, weight_decay, warmup_steps, grad_norm, tree, proj, punct, logger, verbose, devices, **kwargs)
    210             verbose=True,
    211             devices: Union[float, int, List[int]] = None, **kwargs):
--> 212         return super().fit(**merge_locals_kwargs(locals(), kwargs))
    213 
    214     def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in fit(self, trn_data, dev_data, save_dir, batch_size, epochs, devices, logger, seed, finetune, eval_trn, _device_placeholder, **kwargs)
    293                                                        dev_data=dev_data,
    294                                                        eval_trn=eval_trn,
--> 295                                                        overwrite=True))
    296 
    297     def build_logger(self, name, save_dir):

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in execute_training_loop(self, trn, dev, epochs, criterion, optimizer, metric, save_dir, logger, devices, ratio_width, patience, eval_trn, **kwargs)
    222             logger.info(f""[yellow]Epoch {epoch} / {epochs}:[/yellow]"")
    223             self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,
--> 224                                 eval_trn=eval_trn, **self.config)
    225             loss, dev_metric = self.evaluate_dataloader(dev, criterion, metric, logger=logger, ratio_width=ratio_width)
    226             timer.update()

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in fit_dataloader(self, trn, criterion, optimizer, metric, logger, history, gradient_accumulation, grad_norm, ratio_width, eval_trn, **kwargs)
    271             total_loss += loss.item()
    272             if eval_trn:
--> 273                 self.decode_output(out, mask, batch)
    274                 self.update_metrics(metric, batch, out, mask)
    275             if history.step(gradient_accumulation):

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in decode_output(self, outputs, mask, batch)
    285         arc_scores, rel_scores = outputs['class_probabilities']['deps']['s_arc'], \
    286                                  outputs['class_probabilities']['deps']['s_rel']
--> 287         arc_preds, rel_preds = BiaffineDependencyParser.decode(self, arc_scores, rel_scores, mask, batch)
    288         outputs['arc_preds'], outputs['rel_preds'] = arc_preds, rel_preds
    289         return outputs

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/biaffine/biaffine_dep.py in decode(self, arc_scores, rel_scores, mask, batch)
    544         tree, proj = self.config.tree, self.config.get('proj', False)
    545         if tree:
--> 546             arc_preds = decode_dep(arc_scores, mask, tree, proj)
    547         else:
    548             arc_preds = arc_scores.argmax(-1)

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in decode_dep(s_arc, mask, tree, proj)
    757             alg = mst
    758             s_arc.diagonal(0, 1, 2)[1:].fill_(float('-inf'))
--> 759         arc_preds[bad] = alg(s_arc[bad], mask[bad])
    760 
    761     return arc_preds

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in mst(scores, mask, multiroot)
    555                 s[:, 0] = float('-inf')
    556                 s[root, 0] = s_root[root]
--> 557                 t = chuliu_edmonds(s)
    558                 s_tree = s[1:].gather(1, t[1:].unsqueeze(-1)).sum()
    559                 if s_tree > s_best:

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    448     cycle = torch.tensor(cycle)
    449     # indices of noncycle in the original tree
--> 450     noncycle = torch.ones(len(s)).index_fill_(0, cycle, 0)
    451     noncycle = torch.where(noncycle.gt(0))[0]
    452 

RuntimeError: unknown parameter type

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.6 LTS
- Python version: 3.6.5
- HanLP version: 2.1.0b32

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
FINE_ELECTRA_SMALL_ZH 解析部分面积单位出错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
tok模型: **FINE_ELECTRA_SMALL_ZH**   (close_tok_fine_electra_small_20220612_114112.zip)
例1 输入： 中国面积是960万平方千米
结果: ""中国"", ""面积"",  ""是"", **""960万"",  _""平方"",  ""千"",  ""米""_** 
_错误_：""平方千米"" 被 分为 **_""平方"",  ""千"", ""米""_**

例2 输入： 中国面积是960万千米²
结果: ""中国"", ""面积"",  ""是"", **""960万"", _""千"",  ""米"",  ""²""_** 
_错误_: ""千米²"" 被 分为 **_""千"",  ""米"",  ""²""_**

例3 输入:  中国面积是9600000km²
结果: ""中国"", ""面积"",  ""是"", **""_9600000km²""_** 
_错误_: ""9600000km²"" 应 分为 **_""9600000"",  ""km²""_**

**Current behavior**
改用模型 CTB9_TOK_ELECTRA_BASE_CRF 后，例1、例2 结果正确， 类似例子还有 ""中国面积是9600000公里²"", ""教室面积是180米²""
例3结果则仍然相同。

**Expected behavior**
对类似度量单位实现正确分词

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b31

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
m1系统安装hanlp出错 can't find Rust compiler,"安装失败

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac m1 
- Python version:3.8.9
- HanLP version:1.8.3

**Other info / logs**

```python
  Building wheel for tokenizers (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for tokenizers (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [51 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.14-arm64-cpython-38
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      running build_ext
      running build_rust
      error: can't find Rust compiler
      
      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
      
      To update pip, run:
      
          pip install --upgrade pip
      
      and then retry package installation.
      
      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for tokenizers
Successfully built pyyaml
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects

```
* [x] I've completed this form and searched the web for solutions.
"
amr解析日期错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
amr模型: MRP2020_AMR_ZHO_MENGZI_BASE
输入 [""1972年5月"", ""火星"", ""发生"", ""地震"", ""。""]  
（使用分词模型 http://download.hanlp.com/tok/extra/msra_crf_electra_base_20220507_113936.zip）

**Describe the current behavior**
执行结果中，**1972年5月** 不能被完全解析
```
{
        ""id"": ""0"",
        ""input"": ""1972年5月 火星 发生 地震 。"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""date-entity"",
                ""anchors"": []
            },
            {
                ""id"": 1,
                ""label"": ""1972"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 7
                    }
                ]
            },
            {
                ""id"": 2,
                ""label"": ""planet"",
                ""anchors"": []
            },
            {
                ""id"": 3,
                ""label"": ""name"",
                ""properties"": [
                    ""op1""
                ],
                ""values"": [
                    ""火星""
                ],
                ""anchors"": [
                    {
                        ""from"": 8,
                        ""to"": 10
                    }
                ]
            },
            {
                ""id"": 4,
                ""label"": ""发生-01"",
                ""anchors"": [
                    {
                        ""from"": 11,
                        ""to"": 13
                    }
                ]
            },
            {
                ""id"": 5,
                ""label"": ""地震"",
                ""anchors"": [
                    {
                        ""from"": 14,
                        ""to"": 16
                    }
                ]
            }
        ],
        ""edges"": [
            {
                ""source"": 0,
                ""target"": 1,
                ""label"": ""year""
            },
            {
                ""source"": 2,
                ""target"": 3,
                ""label"": ""name""
            },
            {
                ""source"": 4,
                ""target"": 2,
                ""label"": ""arg1""
            },
            {
                ""source"": 4,
                ""target"": 5,
                ""label"": ""arg0""
            },
            {
                ""source"": 4,
                ""target"": 0,
                ""label"": ""time""
            }
        ],
        ""tops"": [
            4
        ],
        ""framework"": ""amr""
    }

```
分词改为  [""1972年"", ""5月"", ""火星"", ""发生"", ""地震"", ""。""]
（分词模型: http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_crf_20220426_161255.zip）
日期可以正确解析。

**Expected behavior**
提高 对粗粒度分词结果的日期解析精度。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie#getState 长时间占用线程 CPU99.9,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
堆栈显示 com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie#getState(AhoCorasickDoubleArrayTrie.java:402)长时间占用线程(时间超过一周)，线程状态一直RUNNABLE,CPU占用达99.9.


**Code to reproduce the issue**
暂不清楚怎么触发的

``JAVA``

**Describe the current behavior**
无

**Expected behavior**
无

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Python version:
- HanLP version: portable-1.8.2

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
已搜索
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
pypi上未更新 CTB9_TOK_ELECTRA_BASE_CRF、CTB9_TOK_ELECTRA_BASE、MSR_TOK_ELECTRA_BASE_CRF,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
加载 CTB9_TOK_ELECTRA_BASE_CRF、CTB9_TOK_ELECTRA_BASE、MSR_TOK_ELECTRA_BASE_CRF
模型报错

**Code to reproduce the issue**
执行代码

```python
tok = hanlp.load(hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE_CRF)
```

**Describe the current behavior**
报错信息:
`AttributeError: module 'hanlp.pretrained.tok' has no attribute 'CTB9_TOK_ELECTRA_BASE_CRF'`

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version: python3.9
- HanLP version: 2.1.0b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
amr部分node缺失anchors信息、label错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
amr模型:  MRP2020_AMR_ZHO_MENGZI_BASE

例1 输入  [""我"", ""不"", ""吃饭""]
执行结果中， 吃饭 对应的  ""anchors"": []  

```
{
        ""id"": ""0"",
        ""input"": ""我 不 吃饭"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""我"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 1
                    }
                ]
            },
            {
                ""id"": 1,
                ""label"": ""-"",
                ""anchors"": [
                    {
                        ""from"": 2,
                        ""to"": 3
                    }
                ]
            },
            {
                ""id"": 2,
                ""label"": ""吃饭-01"",
                ""anchors"": []
            }
        ],
        ""edges"": [
            {
                ""source"": 2,
                ""target"": 1,
                ""label"": ""polarity""
            },
            {
                ""source"": 2,
                ""target"": 0,
                ""label"": ""arg0""
            }
        ],
        ""tops"": [
            2
        ],
        ""framework"": ""amr""
    }
```

例2， 输入 [""我"", ""吃饭""]
吃饭 对应的 ""label"": ""**死-01**"",  ""anchors"": []
```
{
        ""id"": ""0"",
        ""input"": ""我 吃饭"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""我"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 1
                    }
                ]
            },
            {
                ""id"": 1,
                ""label"": ""死-01"",
                ""anchors"": []
            }
        ],
        ""edges"": [
            {
                ""source"": 1,
                ""target"": 0,
                ""label"": ""arg0""
            }
        ],
        ""tops"": [
            1
        ],
        ""framework"": ""amr""
    }
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
上述例子在线版测试时label没有问题

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Failed to load https://file.hankcs.com/hanlp/tok/coarse_electra_small_20220220_013548.zip.,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
模型加载失败

**Code to reproduce the issue**
hanlp已升级到最新版本
```
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
```

**Describe the current behavior**
模型已下载完成，但加载失败

**Expected behavior**
希望完成加载模型

**System information**
报错信息仅提供了以下三行，无额外其他信息
OS: macOS-10.15.5-x86_64-i386-64bit
Python: 3.8.12
PyTorch: 1.11.0
HanLP: 2.1.0-beta.27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)报错，无法加载模型,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
我运行了
`hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)`然后出现报错，无法加载模型

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)`
```python
```
```
import hanlp
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
```
**Describe the current behavior**
A clear and concise description of what happened.
目前，加载模型后报错，我分别在自己本地和google的colab上都不行，尝试pip upgrade hanlp[full] -U也后同样会报错
**Expected behavior**
A clear and concise description of what you expected to happen.
能够正常加载模型，完成ner任务
**System information**
- OS Platform and Distribution:Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
- Python version:3.7.13
- HanLP version:2.1.0-beta.27

**Other info / logs**


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20211227_114712.zip.
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.13
PyTorch: 1.11.0+cu113
TensorFlow: 2.8.0
HanLP: 2.1.0-beta.27

OSError                                   Traceback (most recent call last)
[<ipython-input-26-b5faac023b11>](https://localhost:8080/#) in <module>()
----> 1 hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

11 frames
[/usr/local/lib/python3.7/dist-packages/hanlp/__init__.py](https://localhost:8080/#) in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

[/usr/local/lib/python3.7/dist-packages/hanlp/utils/component_util.py](https://localhost:8080/#) in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    169         except:
    170             pass
--> 171         raise e from None
    172 
    173 

[/usr/local/lib/python3.7/dist-packages/hanlp/utils/component_util.py](https://localhost:8080/#) in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     97             else:
     98                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 99                     obj.load(save_dir, verbose=verbose, **kwargs)
    100                 else:
    101                     obj.load(metapath, **kwargs)

[/usr/local/lib/python3.7/dist-packages/hanlp/common/keras_component.py](https://localhost:8080/#) in load(self, save_dir, logger, **kwargs)
    212         self.load_config(save_dir)
    213         self.load_vocabs(save_dir)
--> 214         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    215         self.load_weights(save_dir, **kwargs)
    216         self.load_meta(save_dir)

[/usr/local/lib/python3.7/dist-packages/hanlp/common/keras_component.py](https://localhost:8080/#) in build(self, logger, **kwargs)
    223         self.transform.build_config()
    224         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
--> 225                                                    loss=kwargs.get('loss', None)))
    226         self.transform.lock_vocabs()
    227         optimizer = self.build_optimizer(**self.config)

[/usr/local/lib/python3.7/dist-packages/hanlp/components/taggers/transformers/transformer_tagger_tf.py](https://localhost:8080/#) in build_model(self, transformer, max_seq_length, **kwargs)
     32 
     33     def build_model(self, transformer, max_seq_length, **kwargs) -> tf.keras.Model:
---> 34         model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
     35         self.transform.tokenizer = tokenizer
     36         return model

[/usr/local/lib/python3.7/dist-packages/hanlp/layers/transformers/loader_tf.py](https://localhost:8080/#) in build_transformer(transformer, max_seq_length, num_labels, tagging, tokenizer_only)
      9 
     10 def build_transformer(transformer, max_seq_length, num_labels, tagging=True, tokenizer_only=False):
---> 11     tokenizer = AutoTokenizer_.from_pretrained(transformer)
     12     if tokenizer_only:
     13         return tokenizer

[/usr/local/lib/python3.7/dist-packages/hanlp/layers/transformers/pt_imports.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, use_fast, do_basic_tokenize)
     68         tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
     69                                         do_basic_tokenize=do_basic_tokenize,
---> 70                                         **additional_config)
     71         tokenizer.name_or_path = transformer
     72         return tokenizer

[/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    483             if not isinstance(config, PretrainedConfig):
    484                 config = AutoConfig.from_pretrained(
--> 485                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs
    486                 )
    487             config_tokenizer_class = config.tokenizer_class

[/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    650         kwargs[""name_or_path""] = pretrained_model_name_or_path
    651         trust_remote_code = kwargs.pop(""trust_remote_code"", False)
--> 652         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    653         if ""auto_map"" in config_dict and ""AutoConfig"" in config_dict[""auto_map""]:
    654             if not trust_remote_code:

[/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    546         original_kwargs = copy.deepcopy(kwargs)
    547         # Get config dict associated with the base config file
--> 548         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
    549 
    550         # That config file may point us toward another config file to use.

[/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py](https://localhost:8080/#) in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    635         except EnvironmentError:
    636             raise EnvironmentError(
--> 637                 f""Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from ""
    638                 ""'https://huggingface.co/models', make sure you don't have a local directory with the same name. ""
    639                 f""Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory ""

OSError: Can't load config for 'bert-base-chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-chinese' is the correct path to a directory containing a config.json file
=================================ERROR LOG ENDS=================================
* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
TupleTrieDict的len不一致,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
TupleTrieDict 初始化前合并数据，与初始化后更新数据，len(TupleTrieDict)的结果不一致

**Code to reproduce the issue**
分批添加数据key有643142（dict1方式），合并后key的量为632400（dict2方式）。
程序运行时，发现 dict1方式 节省了时间，请问 TupleTrieDict 的此种现象，是特性，还是bug？

```python
    from hanlp_trie.dictionary import TupleTrieDict

    dict1 = TupleTrieDict({""aa"": 1})
    print(len(dict1), dict1[""aa""])  # 输出 1 1

    dict1[""aa""] += 2
    print(len(dict1), dict1[""aa""])  # 输出 2 3

    dict2 = TupleTrieDict({""aa"": 4})
    print(len(dict2), dict2[""aa""])  # 输出 1 4
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.9.7
- HanLP version: 2.1.0b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
启用用户词典，分词结果中的坐标是错误的,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
启用用户词典，分词结果中的坐标是错误的。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
tok = hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH)

tok.config.output_spans = True

tok.dict_force = None
tok.dict_combine = None
sent = '我先去看医生'
print(tok(sent))

tok.dict_combine = set({'先去'})
print(tok(sent))

# 输出，先去 的坐标应该是 1,3：
[['我', 0, 1], ['先', 1, 2], ['去', 2, 3], ['看', 3, 4], ['医生', 4, 6]]
[['我', 0, 1], ['先去', 1, 2], ['看', 2, 3], ['医生', 3, 4]]
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  win10
- Python version: python3.9.7
- HanLP version: hanlp-2.1.0b26

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
在线版分词错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
例：马斯克突然放话收购推特，出价2700亿让它退市。
![屏幕快照 2022-04-16 下午1 35 30](https://user-images.githubusercontent.com/10105704/163663066-839e5542-25e5-47e9-8a6d-7e82bebffed1.png)

""**推特，**""   


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
不只这一个句子，在线版上已见过多次 **把词与标点** 分在一起的情况。

---
使用本地版hanlp-2.1.0b24， tok模型 **FINE_ELECTRA_SMALL_ZH**， 对例句分词结果**正确**。

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1.0b24

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
在线版分词错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
例：马斯克突然放话收购推特，出价2700亿让它退市。
![屏幕快照 2022-04-16 下午1 35 30](https://user-images.githubusercontent.com/10105704/163663066-839e5542-25e5-47e9-8a6d-7e82bebffed1.png)

""**推特，**""   


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
不只这一个句子，在线版上已见过多次 **把词与标点** 分在一起的情况。

---
使用本地版hanlp-2.1.0b24， tok模型 **FINE_ELECTRA_SMALL_ZH**， 对例句分词结果**正确**。

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1.0b24

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
amr解析部分数字出错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
例1：我给了他15万元。
amr 解析结果如下图:
![bug](https://user-images.githubusercontent.com/10105704/163555082-0d6052d6-20c6-4a75-be86-26dc3a2035a3.png)
“**15万**” 未被正确解析

---
例2:  我给了他十五点八万元。
![bug2](https://user-images.githubusercontent.com/10105704/163556640-02c5c566-6d1a-401b-b102-c2982947f36f.png)

“**十五点八万**” 未被正确解析

---
例3: 我给了他十元三角八分钱。
![屏幕快照 2022-04-15 下午5 54 34](https://user-images.githubusercontent.com/10105704/163557085-a1f0bb31-15d9-43cf-8af9-430a8dae10f8.png)
“**十元三角八分**” 未被正确解析

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
**将“15万”改为“十五万”后，可解析为 “150000”**
错误应出自数字转换的过程。 可以参考 https://github.com/microsoft/Recognizers-Text 

**Expected behavior**
能正确显示 label。
当然了，输出数据里的 anchors 标记了原文位置，所以问题也不是特别的大😄


看了下输出的数据，anchors是保留了原文的位置，所以问题也不是特别的大。

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1b23

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
HanLP1.x 文本推荐 逻辑错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
在 Suggester 类的suggest方法中对于不同评价器求和出错，Suggester 位于package com.hankcs.hanlp.suggest;
 ` scoreMap.put(entry.getKey(), score / max + entry.getValue() * scorer.boost);`
其中max表示当前评价器的最优得分，被错误的除在了之前评价器得分之和上，导致最终推荐结果不准确
应改为
` scoreMap.put(entry.getKey(), score  + entry.getValue() * scorer.boost/ max);`
**Code to reproduce the issue**
测试代码
```public class TEST {
   public static void main(String[] args)
    {
        Suggester suggester = new Suggester();
        String[] titleArray =
            (
                ""wuqi\n"" +""服务器""
            ).split(""\\n"");
        for (String title : titleArray)
        {
            suggester.addSentence(title);
        }
        System.out.println(suggester.suggest(""务器"", 1));    
    }
}
```
实际运行结果推荐为wuqi

**Describe the current behavior**
因为对于评价器分数求和错误导致了模型得分发生偏差，更倾向于选择拼音检查得分高的句子


**Expected behavior**
期望运行结果应如下所示，每轮得到的评分应该位于（0,1）之间，最后测试结果应该为服务器
```
---IdVectorScorer------
当前总得分 1.0 当前评价器给出得分 1.1457421786266213E-10 当前评价器给出得分最大值 1.1457421786266213E-10 候选句子内容 服务器 本轮增长分数 1.0
---EditDistanceScorer------
当前总得分 2.0 当前评价器给出得分 0.5 当前评价器给出得分最大值 0.5 候选句子内容 服务器 本轮增长分数 1.0
当前总得分 0.4 当前评价器给出得分 0.2 当前评价器给出得分最大值 0.5 候选句子内容 wuqi 本轮增长分数 0.4
---PinyinScorer------
当前总得分 2.7 当前评价器给出得分 1.1666666666666665 当前评价器给出得分最大值 1.6666666666666665 候选句子内容 服务器 本轮增长分数 0.7000000000000002
当前总得分 1.4 当前评价器给出得分 1.6666666666666665 当前评价器给出得分最大值 1.6666666666666665 候选句子内容 wuqi 本轮增长分数 0.9999999999999999
```
**System information**
- Windows
- jdk11.0.5
- HanLP version:1.x

**Other info / logs**
实际运行结果，出现了轮次评分大于1的情况。评价器评分叠加异常
```
---IdVectorScorer------
当前总得分 1.1457421786266213E-10 当前评价器给出得分 1.1457421786266213E-10 当前评价器给出得分最大值 1.1457421786266213E-10 候选句子内容 服务器 本轮增长分数 1.1457421786266213E-10
---EditDistanceScorer------
当前总得分 0.5000000002291485 当前评价器给出得分 0.5 当前评价器给出得分最大值 0.5 候选句子内容 服务器 本轮增长分数 0.5000000001145742
当前总得分 0.2 当前评价器给出得分 0.2 当前评价器给出得分最大值 0.5 候选句子内容 wuqi 本轮增长分数 0.2
---PinyinScorer------
当前总得分 1.4666666668041557 当前评价器给出得分 1.1666666666666665 当前评价器给出得分最大值 1.6666666666666665 候选句子内容 服务器 本轮增长分数 0.9666666665750072
当前总得分 1.7866666666666666 当前评价器给出得分 1.6666666666666665 当前评价器给出得分最大值 1.6666666666666665 候选句子内容 wuqi 本轮增长分数 1.5866666666666667
```
* [x] I've completed this form and searched the web for solutions.
"
fix typo in document,
fix typo in document,
ViterbiSegment 添加是否进行 Normalize 的配置方法,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
目前只有一种方式开关 Normalization，且影响全局，不能以单个对象控制。 
`HanLP.Config.Normalization = true`
 
**Will this change the current api? How?**
希望 ViterbiSegment 添加配置方法，能够单独配置是否进行 Normalize。eg:  
`new ViterbiSegment().enableNormalize(bool)`

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
执行demo 404,"**Describe the bug**
<img width=""1322"" alt=""image"" src=""https://user-images.githubusercontent.com/6836030/155309692-da5a3604-55d7-43c2-b875-d8f38749e538.png"">

**Code to reproduce the issue**
```python
# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2021-04-29 11:06
import hanlp
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.utils.io_util import get_resource

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)
ner: TaggingNamedEntityRecognition = HanLP['ner/msra']
ner.dict_whitelist = {'午饭后': 'TIME'}
doc = HanLP('2021年测试高血压是138，时间是午饭后2点45，低血压是44', tasks='ner/msra')
doc.pretty_print()
print(doc['ner/msra'])

ner.dict_tags = {('名字', '叫', '金华'): ('O', 'O', 'S-PERSON')}
HanLP('他在浙江金华出生，他的名字叫金华。', tasks='ner/msra').pretty_print()

# HanLP.save(get_resource(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH))

# 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php
# See https://hanlp.hankcs.com/docs/api/hanlp/components/mtl/tasks/ner/tag_ner.html
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC
- Python version: 3.9
- HanLP version: 2.1.0B16

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
ViterbiSegment 自定义 CustomDictionaryForcing 时启用 enableCustomDictionaryForcing 不生效,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
ViterbiSegment 使用 DynamicCustomDictionary 时，同时启用 enableCustomDictionaryForcing 不生效。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
val dictionary = DynamicCustomDictionary()
dictionary.insert(""fuck"", ""ZH_CN_COMMON"")

val segment = ViterbiSegment()
    .enableCustomDictionaryForcing(true)
    .enableCustomDictionary(dictionary)
val text = ""fucking,fuckkkkkk,fuck123""

//自定义Dictionary未生效
println(segment.seg(text))

//全局Dictionary才会生效
CustomDictionary.add(""fuck"")
println(segment.seg(text))
```

输出：
[fucking/nx, ,/w, fuckkkkkk/nx, ,/w, fuck/ZH_CN_COMMON, 123/m]
[fuck/ZH_CN_COMMON, ing/nx, ,/w, fuck/ZH_CN_COMMON, kkkkk/nx, ,/w, fuck/ZH_CN_COMMON, 123/m]

**Describe the current behavior**
自定义DynamicCustomDictionary不生效，使用全局的CustomDictionary才会生效。

**Expected behavior**
自定义DynamicCustomDictionary生效。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
- Python version:
- HanLP version: <img width=""219"" alt=""image"" src=""https://user-images.githubusercontent.com/10527522/154885763-bd0172d4-ac0f-4811-b279-a83dcb7a2410.png"">


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
py38环境下无法加载模型,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
py38环境下无法加载模型。但py36可以加载，并且之前模型已经在本地下载好了。
按照提示，将其升级到15版本，依然出现同样的问题

**Code to reproduce the issue**
```python
model= hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows-10-10.0.19042-SP0
- Python version:  3.8.12
- HanLP version:  2.1.0-beta.14

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip.
Please upgrade HanLP with:

	pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.19042-SP0
Python: 3.8.12
PyTorch: 1.10.1+cpu
HanLP: 2.1.0-beta.14
Traceback (most recent call last):
  File ""D:/data/10 gitlab/information_extract/src/model_quant.py"", line 202, in <module>
    obj = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\utils\component_util.py"", line 170, in load_from_meta_file
    raise e from None
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\utils\component_util.py"", line 91, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp_common\reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp_common\reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 27, in <module>
    from hanlp.components.mtl.tasks import Task
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\components\mtl\tasks\__init__.py"", line 23, in <module>
    from hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\transform\transformer_tokenizer.py"", line 9, in <module>
    from hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, PretrainedConfig, AutoTokenizer_
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\layers\transformers\pt_imports.py"", line 13, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, \
  File ""<frozen importlib._bootstrap>"", line 1039, in _handle_fromlist
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\__init__.py"", line 2709, in __getattr__
    return super().__getattr__(name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\file_utils.py"", line 1822, in __getattr__
    value = getattr(module, name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\file_utils.py"", line 1821, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\auto\__init__.py"", line 202, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\auto\tokenization_auto.py"", line 36, in <module>
    from ..flaubert.tokenization_flaubert import FlaubertTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\flaubert\tokenization_flaubert.py"", line 23, in <module>
    from ..xlm.tokenization_xlm import XLMTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\xlm\tokenization_xlm.py"", line 25, in <module>
    import sacremoses as sm
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\__init__.py"", line 2, in <module>
    from sacremoses.tokenize import *
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\tokenize.py"", line 10, in <module>
    from sacremoses.util import is_cjk
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\util.py"", line 11, in <module>
    from joblib import Parallel, delayed
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\__init__.py"", line 119, in <module>
    from .parallel import Parallel
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\parallel.py"", line 28, in <module>
    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\_parallel_backends.py"", line 22, in <module>
    from .executor import get_memmapping_executor
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\executor.py"", line 14, in <module>
    from .externals.loky.reusable_executor import get_reusable_executor
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\loky\__init__.py"", line 12, in <module>
    from .backend.reduction import set_loky_pickler
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 125, in <module>
    from joblib.externals import cloudpickle  # noqa: F401
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\__init__.py"", line 3, in <module>
    from .cloudpickle import *
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 152, in <module>
    _cell_set_template_code = _make_cell_set_template_code()
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 133, in _make_cell_set_template_code
    return types.CodeType(
TypeError: an integer is required (got type bytes)
=================================ERROR LOG ENDS=================================

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
[linux] fail to pip install hanlp ,"**Describe the bug**
I install hanlp on windows successfully but fail to install hanlp on linux.

**Code to reproduce the issue**
pip install hanlp

**Describe the current behavior**
installation take hours and finally failed. see logs below.

**Expected behavior**
install successfully.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux 8e9e71981ec3 5.3.0-51-generic #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Python version:
3.10
- HanLP version: 2.1

**Other info / logs**
INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.
  Downloading regex-2021.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)
     |████████████████████████████████| 763 kB 114.8 MB/s 
  Downloading regex-2021.10.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)
     |████████████████████████████████| 763 kB 93.0 MB/s 
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking

ERROR: Exception:
Traceback (most recent call last):
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/cli/base_command.py"", line 173, in _main
    status = self.run(options, args)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/cli/req_command.py"", line 203, in wrapper
    return func(self, options, args)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/commands/install.py"", line 315, in run
    requirement_set = resolver.resolve(
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 94, in resolve
    result = self._result = resolver.resolve(
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 472, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 383, in resolve
    raise ResolutionTooDeep(max_rounds)
pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 2000000

* [x] I've completed this form and searched the web for solutions.
https://stackoverflow.com/questions/66008760/pips-dependency-resolver-takes-way-too-long-to-solve-the-conflict
使用pip install hanlp --use-deprecated=legacy-resolver可以解决问题
此外，要先安装pytorch>=1.6，再执行上述命令。

虽然通过上述办法解决了问题，但希望能够提供方便的安装，以及说明需要什么依赖。"
mtl.evaluate报出 AssertionError: No samples loaded,"**Describe the bug**
mtl.evaluate报出 AssertionError: No samples loaded


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import os
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.common.transform import NormalizeCharacter
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.components.mtl.tasks.pos import TransformerTagging
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding
from hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder
from hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON
from hanlp.utils.log_util import cprint


root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))


def cdroot():
    os.chdir(root)


F_LEARN_RATE = 1e-3
N_EPOCH = 1
N_BATCH_SIZE = 16
N_MAX_SEQ_LEN = 510

CTB8_CWS_TRAIN = r""/home/_tmp/data/ctb8_cn/tasks/cws/train.txt""
CTB8_CWS_DEV = r""/home/_tmp/data/ctb8_cn/tasks/cws/dev.txt""
CTB8_CWS_TEST = r""/home/_tmp/data/ctb8_cn/tasks/cws/test.txt""

CTB8_POS_TRAIN = r""/home/_tmp/data/ctb8_cn/tasks/pos/train.txt""
CTB8_POS_DEV = r""/home/_tmp/data/ctb8_cn/tasks/pos/dev.txt""
CTB8_POS_TEST = r""/home/_tmp/data/ctb8_cn/tasks/pos/test.txt""

S_NER_DATA_DIR = r""/home/_tmp/data/msra_ner_token_level_cn/""
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN = os.path.join(S_NER_DATA_DIR, ""word_level.train.tsv"")
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV = os.path.join(S_NER_DATA_DIR, ""word_level.dev.tsv"")
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST = os.path.join(S_NER_DATA_DIR, ""word_level.test.tsv"")

S_SAV_MODEL_DIR = '/home/_tmp/data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small'
S_PERTRAINED_DIR = r""/home/data_sync/pretrain_model/hfl__chinese-electra-180g-small-discriminator""

print(CTB8_CWS_TRAIN)
print(CTB8_CWS_DEV)
print(CTB8_CWS_TEST)
print()
print(CTB8_POS_TRAIN)
print(CTB8_POS_DEV)
print(CTB8_POS_TEST)
print()
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN)
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV)
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST)

tasks = {
    'tok': TaggingTokenization(  # 分词
        CTB8_CWS_TRAIN,
        CTB8_CWS_DEV,
        CTB8_CWS_TEST,
        SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
        max_seq_len=N_MAX_SEQ_LEN,
        hard_constraint=True,
        char_level=True,
        tagging_scheme='BMES',
        lr=F_LEARN_RATE,
        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),
    ),
    # 'pos': TransformerTagging(  # 词性
    #     CTB8_POS_TRAIN,
    #     CTB8_POS_DEV,
    #     CTB8_POS_TEST,
    #     SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
    #     hard_constraint=True,
    #     max_seq_len=N_MAX_SEQ_LEN,
    #     char_level=True,
    #     dependencies='tok',
    #     lr=1e-3,
    # ),
    'ner': TaggingNamedEntityRecognition(  # 实体  mat multi mat erro
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,
        SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
        lr=F_LEARN_RATE,
        # secondary_encoder=RelativeTransformerEncoder(768, k_as_x=True),  # base
        secondary_encoder=RelativeTransformerEncoder(256, k_as_x=True),   # small
        dependencies='tok',
    ),
    # 'dep': BiaffineDependencyParsing(
    #     CTB8_SD330_TRAIN,
    #     CTB8_SD330_DEV,
    #     CTB8_SD330_TEST,
    #     SortingSamplerBuilder(batch_size=32),
    #     lr=1e-3,
    #     tree=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'con': CRFConstituencyParsing(
    #     CTB8_BRACKET_LINE_NOEC_TRAIN,
    #     CTB8_BRACKET_LINE_NOEC_DEV,
    #     CTB8_BRACKET_LINE_NOEC_TEST,
    #     SortingSamplerBuilder(batch_size=32),
    #     lr=1e-3,
    #     dependencies='tok',
    # )
}

mtl = MultiTaskLearning()

mtl.load(S_SAV_MODEL_DIR)
print(mtl('华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动'))
for k, v in tasks.items():
    v.trn = tasks[k].trn
    v.dev = tasks[k].dev
    v.tst = tasks[k].tst
metric, *_ = mtl.evaluate(S_SAV_MODEL_DIR)
for k, v in tasks.items():
    print(metric[k], end=' ')
print()

```

**Describe the current behavior**
运行报错
File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/dataset.py"", line 129, in __init__
    assert data, 'No samples loaded'
AssertionError: No samples loaded

**Expected behavior**
程序正常输出样本集上P R F

**System information**
- centos-release-7-9.2009.1.el7.centos.x86_64:
- python=3.6.4:
- hanlp=2.1.0a65

**Other info / logs**
/home/_tmp/data/ctb8_cn/tasks/cws/train.txt
/home/_tmp/data/ctb8_cn/tasks/cws/dev.txt
/home/_tmp/data/ctb8_cn/tasks/cws/test.txt

/home/_tmp/data/ctb8_cn/tasks/pos/train.txt
/home/_tmp/data/ctb8_cn/tasks/pos/dev.txt
/home/_tmp/data/ctb8_cn/tasks/pos/test.txt

/home/_tmp/data/msra_ner_token_level_cn/word_level.train.tsv
/home/_tmp/data/msra_ner_token_level_cn/word_level.dev.tsv
/home/_tmp/data/msra_ner_token_level_cn/word_level.test.tsv
{
  ""tok"": [
    ""华纳"",
    ""音乐"",
    ""旗下"",
    ""的"",
    ""新垣结衣"",
    ""在"",
    ""12月"",
    ""21日"",
    ""于"",
    ""日本"",
    ""武道馆"",
    ""举办"",
    ""歌手"",
    ""出道"",
    ""活动""
  ],
  ""ner"": [
    [""华纳音乐"", ""ORGANIZATION"", 0, 2],
    [""新垣结衣"", ""PERSON"", 4, 5],
    [""12月"", ""DATE"", 6, 7],
    [""21日"", ""DATE"", 7, 8],
    [""日本"", ""LOCATION"", 9, 10],
    [""武道馆"", ""LOCATION"", 10, 11]
  ]
}
1 / 2 Building tst dataset for ner ...
Traceback (most recent call last):
  File ""hanlp_train_cn.py"", line 134, in <module>
    metric, *_ = mtl.evaluate(S_SAV_MODEL_DIR)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 753, in evaluate
    rets = super().evaluate('tst', save_dir, logger, batch_size, output, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 469, in evaluate
    device=self.devices[0], logger=logger, overwrite=True))
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 156, in build_dataloader
    cache=isinstance(data, str), **config)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py"", line 123, in build_dataloader
    dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/ner/transformer_ner.py"", line 216, in build_dataset
    dataset = super().build_dataset(data, transform, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 170, in build_dataset
    return TSVTaggingDataset(data, transform=transform, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/datasets/ner/tsv.py"", line 45, in __init__
    super().__init__(data, transform, cache, generate_idx)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/dataset.py"", line 129, in __init__
    assert data, 'No samples loaded'
AssertionError: No samples loaded


* [x] I've completed this form and searched the web for solutions.
"
"open_base.py训练有bug , mat1 and mat2 shapes cannot be multiplied","**Describe the bug**
open_base.py训练有bug

 mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)

**Code to reproduce the issue**

```
# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2020-12-03 14:24

import os
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.common.transform import NormalizeCharacter
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing
from hanlp.components.mtl.tasks.dep import BiaffineDependencyParsing
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.components.mtl.tasks.pos import TransformerTagging
from hanlp.components.mtl.tasks.sdp import BiaffineSemanticDependencyParsing
from hanlp.components.mtl.tasks.srl.bio_srl import SpanBIOSemanticRoleLabeling
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from hanlp.datasets.ner.msra import MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, \
    MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST
from hanlp.datasets.parsing.ctb8 import CTB8_POS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_SD330_TEST, CTB8_SD330_DEV, \
    CTB8_SD330_TRAIN, CTB8_CWS_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_BRACKET_LINE_NOEC_TRAIN, \
    CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TEST
from hanlp.datasets.parsing.semeval16 import SEMEVAL2016_TEXT_TRAIN_CONLLU, SEMEVAL2016_TEXT_TEST_CONLLU, \
    SEMEVAL2016_TEXT_DEV_CONLLU
# from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_DEV, \
#     ONTONOTES5_CONLL12_CHINESE_TRAIN
from hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding
from hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder
from hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON
from hanlp.utils.log_util import cprint


root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))


def cdroot():
    """"""
    cd to project root, so models are saved in the root folder
    """"""
    os.chdir(root)


n_batch_size = 8


tasks = {
    'tok': TaggingTokenization(  # 分词
        CTB8_CWS_TRAIN,
        CTB8_CWS_DEV,
        CTB8_CWS_TEST,
        SortingSamplerBuilder(batch_size=n_batch_size),
        max_seq_len=510,
        hard_constraint=True,
        char_level=True,
        tagging_scheme='BMES',
        lr=1e-3,
        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),
    ),
    # 'pos': TransformerTagging(  # 词性
    #     CTB8_POS_TRAIN,
    #     CTB8_POS_DEV,
    #     CTB8_POS_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     hard_constraint=True,
    #     max_seq_len=510,
    #     char_level=True,
    #     dependencies='tok',
    #     lr=1e-3,
    # ),
    'ner': TaggingNamedEntityRecognition(  # 实体  mat multi mat erro
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,
        SortingSamplerBuilder(batch_size=n_batch_size),
        lr=1e-3,
        secondary_encoder=RelativeTransformerEncoder(768, k_as_x=True),
        dependencies='tok',
    ),
    # 'srl': SpanBIOSemanticRoleLabeling(  # 依存句法  download error
    #     ONTONOTES5_CONLL12_CHINESE_TRAIN,
    #     ONTONOTES5_CONLL12_CHINESE_DEV,
    #     ONTONOTES5_CONLL12_CHINESE_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size, batch_max_tokens=2048),
    #     lr=1e-3,
    #     crf=True,
    #     dependencies='tok',
    # ),
    # 'dep': BiaffineDependencyParsing(  # 成分句法
    #     CTB8_SD330_TRAIN,
    #     CTB8_SD330_DEV,
    #     CTB8_SD330_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     tree=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'sdp': BiaffineSemanticDependencyParsing(  # 语义依存
    #     SEMEVAL2016_TEXT_TRAIN_CONLLU,
    #     SEMEVAL2016_TEXT_DEV_CONLLU,
    #     SEMEVAL2016_TEXT_TEST_CONLLU,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     apply_constraint=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'con': CRFConstituencyParsing(  # 语义角色 memory out
    #     CTB8_BRACKET_LINE_NOEC_TRAIN,
    #     CTB8_BRACKET_LINE_NOEC_DEV,
    #     CTB8_BRACKET_LINE_NOEC_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     dependencies='tok',
    # )
}

mtl = MultiTaskLearning()
save_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small'
# save_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base'
# save_dir = 'data/model/mtl/open_tok_pos_ner_bert_base'
mtl.fit(
    ContextualWordEmbedding('token',
                            # ""bert-base-chinese"",
                            ""hfl/chinese-electra-180g-small-discriminator"",
                            # ""hfl/chinese-electra-180g-base-discriminator"",
                            average_subwords=True,
                            max_sequence_length=510,
                            word_dropout=.1),
    tasks,
    save_dir,
    1,  # 30
    lr=1e-3,
    encoder_lr=5e-5,
    grad_norm=1,
    gradient_accumulation=2,
    eval_trn=False,
)
cprint(f'Model saved in [cyan]{save_dir}[/cyan]')
mtl.load(save_dir)
for k, v in tasks.items():
    v.trn = tasks[k].trn
    v.dev = tasks[k].dev
    v.tst = tasks[k].tst
metric, *_ = mtl.evaluate(save_dir)
for k, v in tasks.items():
    print(metric[k], end=' ')
print()
print(mtl('华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动'))
```

**Describe the current behavior**
python open_base.py 运行报错
RuntimeError: mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)


**Expected behavior**
程序正常训练


**System information**
- centos-release-7-9.2009.1.el7.centos.x86_64
- python=3.6.4
- hanlp=2.1.0a65

**Other info / logs**
Using GPUs: [0]
Epoch 1 / 1:
    1/24919 loss: 0.7001 ETA: 43 m 14 sTraceback (most recent call last):
  File ""hanlp_train.py"", line 135, in <module>
    eval_trn=False,
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 644, in fit
    **tasks)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 295, in fit
    overwrite=True))
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 287, in execute_training_loop
    **self.config)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 346, in fit_dataloader
    output_dict, _ = self.feed_batch(batch, task_name)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 687, in feed_batch
    decoder=self.model.decoders[task_name]),
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 182, in feed_batch
    return decoder(h, batch=batch, mask=mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py"", line 35, in forward
    contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 309, in forward
    x = layer(x, mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 263, in forward
    x = self.self_attn(x, mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 135, in forward
    qv = self.qv_linear(x)  # batch_size x max_len x d_model2
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/functional.py"", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)


* [x] I've completed this form and searched the web for solutions."
Update DoubleArrayTrie.java,"原书page78
寻找一个起始下标b，使得所有<img src=""https://latex.codecogs.com/svg.image?check[base[b]&space;&plus;&space;s_{i}.code]&space;==&space;0"" title=""check[base[b] + s_{i}.code] == 0"" />
"
关于constituency tree数据标注,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
everyone
**Are you willing to contribute it (Yes/No):**
Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- Python version: Python3.*
- HanLP version: master branch

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->



你好，关于constituency tree标注，从标注上来看：
```
                      TOP                 
                       |                   
                     IP-HLN               
                 ______|________________   
              IP-TPC              |     | 
     ___________|______           |     |  
    |                  VP         |     | 
    |            ______|_____     |     |  
    |         PP-DIR         |    |     | 
    |       ____|______      |    |     |  
NP-PN-SBJ  |           NP    VP NP-SBJ  VP
    |      |           |     |    |     |  
    NR     P           NN    VV   NN    VV
    |      |           |     |    |     |  
    广西     对           外     开放   成绩    斐然

```
它的形式貌似和人正常理解是相反的，比如下面可能会更好：


![123321](https://user-images.githubusercontent.com/11239531/143535834-28e93b8a-7589-4fd5-bf0c-2345661c9328.png)


我的问题在于：

有没有现成的标注工具可以实现类似上图效果，或者说有没有相关的标注工具推荐。

非常感谢！！！

"
crf模型在处理emoji时报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**问题描述:**
crf模型在处理emoji时报错(wordbasedsegmenter不会报错), 样例代码如下:


**复现代码**

```
from jpype import java

from pyhanlp import *

text = '😱😱😱你好，欢迎在😱Pytho😱😱n中😱调用HanLP的API   😱😱😱😱😱😱😱😱😱😱'

String = java.lang.String
java_string = String(text.encode(), 'UTF8')
string_from_java = str(java_string).encode('utf-16', errors='surrogatepass').decode('utf-16')
print('原文：' + string_from_java)
print('HanLP分词结果')
crflex = HanLP.newSegment(""crf"")
for term in crflex.seg(java_string):
    print(str(term.word).encode('utf-16', errors='surrogatepass').decode('utf-16'))
```

**报错信息**
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte


**System information**
系统: MAC Big Sur, I7.
python 3.6.8
HanLP Jar version: 1.8.2
pyhanlp version: 0.1.79

**其他信息**
crf模型在输入八字节的单emoji时, 会拆成两个4字节的字返回
![image](https://user-images.githubusercontent.com/16161025/141046032-da8161a6-d3a0-4db0-ab28-f3f32c729024.png)

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
failed to load a pretrained model,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
Failed to load a pretrained model.

**Code to reproduce the issue**
(first do _pip install hanlp[full]_)
```python
import hanlp
hanlp.load(hanlp.pretrained.tok.CTB6_CONVSEG)
```

**Describe the current behavior**
```
2021-11-07 23:34:22.992023: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers
Failed to load https://file.hankcs.com/hanlp/tok/ctb6_convseg_nowe_nocrf_20200110_004046.zip.
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
```

**Expected behavior**
Successful loading.

**System information**
OS: Windows-10-10.0.22000-SP0
Python: 3.9.7
PyTorch: 1.9.1
HanLP: 2.1.0-alpha.63

**Other info / logs**
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 126, in load_from_meta_file
    raise e from None
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp_common\reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp_common\reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""C:\Programming\miniconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 850, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\components\tok_tf.py"", line 9, in <module>
    from hanlp.common.keras_component import KerasComponent
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 15, in <module>
    from hanlp.callbacks.fine_csv_logger import FineCSVLogger
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\callbacks\fine_csv_logger.py"", line 33, in <module>
    class FineCSVLogger(tf.keras.callbacks.History):
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""C:\Programming\miniconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Programming\miniconda3\lib\site-packages\keras\__init__.py"", line 25, in <module>
    from keras import models
  File ""C:\Programming\miniconda3\lib\site-packages\keras\models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""C:\Programming\miniconda3\lib\site-packages\keras\metrics.py"", line 26, in <module>
    from keras import activations
  File ""C:\Programming\miniconda3\lib\site-packages\keras\activations.py"", line 20, in <module>
    from keras.layers import advanced_activations
  File ""C:\Programming\miniconda3\lib\site-packages\keras\layers\__init__.py"", line 23, in <module>
    from keras.engine.input_layer import Input
  File ""C:\Programming\miniconda3\lib\site-packages\keras\engine\input_layer.py"", line 21, in <module>
    from keras.engine import base_layer
  File ""C:\Programming\miniconda3\lib\site-packages\keras\engine\base_layer.py"", line 43, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""C:\Programming\miniconda3\lib\site-packages\keras\mixed_precision\loss_scale_optimizer.py"", line 18, in <module>
    from keras import optimizers
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizers.py"", line 26, in <module>
    from keras.optimizer_v2 import adadelta as adadelta_v2
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizer_v2\adadelta.py"", line 22, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 36, in <module>
    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\eager\monitoring.py"", line 360, in __init__
    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\eager\monitoring.py"", line 135, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.
```

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
func 'input_is_single_sample()' has a bug ,"**Describe the bug**
In 'components/taggers/transformers/transform_tf.py' file, the function in line 117 maybe wrong.

def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:
        return isinstance(input[0], str)

**Code to reproduce the issue**

import hanlp
import unittest
ner1 = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

class TestNER(unittest.TestCase):
    def test_ner_multi_sent(self):
        texts = [
            '北京市海淀区海淀大街天使大厦5层小邮局， 收件人王小二',
            '北京市海淀区清华大学明德楼， 收件人王小二']
        doc = ner1(texts)
        print(doc1)

if __name__ == '__main__':
    unittest.main()


**Describe the current behavior**
with the current behavior, the func 'input_is_single_sample()' will return True for the input, which is not expected.  
As a result, the NER result is wrong.
[('北京市海淀区海淀大街天使大厦5层小邮局， 收件人王小二', 'NS', 0, 1), ('北京市海淀区清华大学明德楼， 收件人王小二', 'NS', 1, 2)]


**Expected behavior**
The func 'input_is_single_sample()' should  return False. And NER result should be like this:
[[('北京市', 'NS', 0, 3), ('海淀区', 'NS', 3, 6), ('海淀大街', 'NS', 6, 10), ('天使大厦', 'NS', 10, 14), ('王小二', 'NR', 24, 27)], [('北京市', 'NS', 0, 3), ('海淀区', 'NS', 3, 6), ('清华大学', 'NT', 6, 10), ('明德楼', 'NS', 10, 13), ('王小二', 'NR', 18, 21)]]

**System information**
No Need.
The bug is in Master brach and doc-zh brach.

**Other info / logs**
No Need.

* [x] I've completed this form and searched the web for solutions."
hanlp 运行报错,"
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)


**Describe the current behavior**
A clear and concise description of what happened.
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip.


**System information**
OS: Darwin-17.6.0-x86_64-i386-64bit
Python: 3.7.2
PyTorch: 1.1.0
HanLP: 2.1.0-alpha.61

**Other info / logs**

Exception has occurred: ImportError (note: full exception trace is shown but execution is paused at: )
cannot import name 'IterableDataset' from 'torch.utils.data.dataset' (/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataset.py)

* [x] I've completed this form and searched the web for solutions."
batch size 无法设置,"**Describe the bug**
如题，bs 无法设置，一直为默认的 32，导致显存占用和利用率无法上升



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
from hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning

# hanlp.load()
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)  # 世界最大中文语料库
res = HanLP(['结构为XP+地的短评，用于修饰动词短语VP'] * 128, batch_size=64, tasks=['con'])
```

debug 可以看到
![image](https://user-images.githubusercontent.com/25661058/136928619-6904a18a-9b71-4c7f-841b-0be312f249bc.png)

![image](https://user-images.githubusercontent.com/25661058/136928645-a11ee331-3c24-4a40-bf77-61ebe6416bd2.png)



**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- HanLP version: '2.1.0-alpha.61'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Correct the order of the returned P/R values,
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

class Node method 

def _add_child(self, char, value, overwrite=False):
        child = self._children.get(char)
        if child is None:
            child = Node(value)
            self._children[char] = child
        elif overwrite:
            child._value = value
        return child

error for ""elif overwrite:""
should be  ""if overwrite""

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py```python
```

**Describe the current behavior**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

**Expected behavior**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: all
- HanLP version: all

**Other info / logs**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

* [x] I've completed this form and searched the web for solutions.
"
Correct the seeding behavior for `seed=0`,
Correct the seeding behavior for `seed=0`,
srl输出漏了一个分词结果，或者是有一个合并词的起始位置错了,"**Describe the bug**
A clear and concise description of what the bug is.
在做示例程序里输入某语句后得到的返回结果里，srl指示位组合起来比原语句少
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, devices=0) # 世界最大中文语料库
r3 = HanLP.predict([""""""今天继续玩新生模拟器，这个子弹大家都知道吧？
                把子弹数量改成二十六，然后咱们再召唤就会召唤出一群战斗机。
                更好玩的是，如果咱们在瞄准模式下再召唤的话，就会召唤出一架可以驾驶的战斗。
                可别小看这架战斗机，最高速度五秒就可以跨越太平洋秒杀目前人类制造的所有飞行器。
                快来新建模拟器，尝试一下吧。""""""])
print(r3)
```

**Describe the current behavior**
""srl"": [
    [[[""今天"", ""ARGM-TMP"", 0, 1], [""继续"", ""PRED"", 1, 2], [""玩新生模拟器"", ""ARG1"", 2, 5]], **_[[""今天"", ""ARGM-TMP"", 0, 1], [""玩"", ""PRED"", 2, 3],_** [""新生模拟器"", ""ARG1"", 3, 5]], [[""大家"", ""ARG0"", 8, 9], [""都"", ""ARGM-ADV"", 9, 10], [""知道"", ""PRED"", 10, 11]], [[""子弹数量"", ""ARG1"", 14, 16], [""改"", ""PRED"", 16, 17], [""二十六"", ""ARG2"", 18, 19]], [[""然后"", ""ARGM-DIS"", 20, 21], [""咱们"", ""ARG0"", 21, 22], [""再"", ""ARGM-ADV"", 22, 23], [""召唤"", ""PRED"", 23, 24]], [[""更"", ""ARGM-ADV"", 32, 33], [""好玩"", ""PRED"", 33, 34]], [[""更好玩的"", ""ARG0"", 32, 35], [""是"", ""PRED"", 35, 36], [""如果咱们在瞄准模式下再召唤的话，就会召唤出一架可以驾驶的战斗"", ""ARG1"", 37, 57]], [[""如果"", ""ARGM-DIS"", 37, 38], [""咱们"", ""ARG0"", 38, 39], [""在瞄准模式下"", ""ARGM-LOC"", 39, 43], [""再"", ""ARGM-ADV"", 43, 44], [""召唤"", ""PRED"", 44, 45]], [[""驾驶"", ""PRED"", 54, 55], [""战斗"", ""ARG1"", 56, 57]], [[""可"", ""ARGM-ADV"", 58, 59], [""别"", ""ARGM-ADV"", 59, 60], [""小看"", ""PRED"", 60, 61], [""这架战斗机"", ""ARG1"", 61, 64]], [[""跨越"", ""PRED"", 71, 72], [""太平洋"", ""ARG1"", 72, 73]], [[""秒杀"", ""PRED"", 73, 74], [""目前人类制造的所有飞行器"", ""ARG1"", 74, 80]], [[""目前"", ""ARGM-TMP"", 74, 75], [""人类"", ""ARG0"", 75, 76], [""制造"", ""PRED"", 76, 77]], [[""快"", ""ARGM-ADV"", 81, 82], [""来"", ""PRED"", 82, 83], [""新建模拟器"", ""ARG1"", 83, 85]], [[""快"", ""ARGM-ADV"", 81, 82], [""新建"", ""PRED"", 83, 84], [""模拟器"", ""ARG1"", 84, 85]], [[""尝试"", ""PRED"", 86, 87], [""一下"", ""ARGM-ADV"", 87, 88]]]

**Expected behavior**
""srl"": [
    [[[""今天"", ""ARGM-TMP"", 0, 1], [""继续"", ""PRED"", 1, 2], [""玩新生模拟器"", ""ARG1"", 2, 5]], [[""今天"", ""ARGM-TMP"", 0, 1], **_[""玩"", ""PRED"", 1, 3]_**, [""新生模拟器"", ""ARG1"", 3, 5]], [[""大家"", ""ARG0"", 8, 9], [""都"", ""ARGM-ADV"", 9, 10], [""知道"", ""PRED"", 10, 11]], [[""子弹数量"", ""ARG1"", 14, 16], [""改"", ""PRED"", 16, 17], [""二十六"", ""ARG2"", 18, 19]], [[""然后"", ""ARGM-DIS"", 20, 21], [""咱们"", ""ARG0"", 21, 22], [""再"", ""ARGM-ADV"", 22, 23], [""召唤"", ""PRED"", 23, 24]], [[""更"", ""ARGM-ADV"", 32, 33], [""好玩"", ""PRED"", 33, 34]], [[""更好玩的"", ""ARG0"", 32, 35], [""是"", ""PRED"", 35, 36], [""如果咱们在瞄准模式下再召唤的话，就会召唤出一架可以驾驶的战斗"", ""ARG1"", 37, 57]], [[""如果"", ""ARGM-DIS"", 37, 38], [""咱们"", ""ARG0"", 38, 39], [""在瞄准模式下"", ""ARGM-LOC"", 39, 43], [""再"", ""ARGM-ADV"", 43, 44], [""召唤"", ""PRED"", 44, 45]], [[""驾驶"", ""PRED"", 54, 55], [""战斗"", ""ARG1"", 56, 57]], [[""可"", ""ARGM-ADV"", 58, 59], [""别"", ""ARGM-ADV"", 59, 60], [""小看"", ""PRED"", 60, 61], [""这架战斗机"", ""ARG1"", 61, 64]], [[""跨越"", ""PRED"", 71, 72], [""太平洋"", ""ARG1"", 72, 73]], [[""秒杀"", ""PRED"", 73, 74], [""目前人类制造的所有飞行器"", ""ARG1"", 74, 80]], [[""目前"", ""ARGM-TMP"", 74, 75], [""人类"", ""ARG0"", 75, 76], [""制造"", ""PRED"", 76, 77]], [[""快"", ""ARGM-ADV"", 81, 82], [""来"", ""PRED"", 82, 83], [""新建模拟器"", ""ARG1"", 83, 85]], [[""快"", ""ARGM-ADV"", 81, 82], [""新建"", ""PRED"", 83, 84], [""模拟器"", ""ARG1"", 84, 85]], [[""尝试"", ""PRED"", 86, 87], [""一下"", ""ARGM-ADV"", 87, 88]]]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Python version:  3.8.10
- HanLP version:  latest(v2.1.0.a)

**Other info / logs**
nothing.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
有的模型在windows无法使用，因为windows不允许创建名为con的目录,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
有的模型在windows无法使用，因为windows不允许创建名为con的目录。

这是windows无法使用的目录名列表：
CON, PRN, AUX, NUL, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, and LPT9.

由于 NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA 模型会试图创建一个 con 目录，因此会出现python错误：
NotADirectoryError: [WinError 267] 目录名称无效。:

**Code to reproduce the issue**
在windows执行下列命令，在下载完毕后，就会看到上述错误。
```python
HanLP = hanlp.load(hanlp.pretrained.mtl.NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA)
```

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
代码合并,最新代码合并
DoubleArrayTrie里的LongestSearcher的next方法需要进行强化，当传入的treemap的value为null时…,
DoubleArrayTrie里的LongestSearcher的next方法，当传入的treemap的value为null时，会引发bug,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**

DoubleArrayTrie里的LongestSearcher的next方法，当传入的treemap的value为null时，会引发bug
![image](https://user-images.githubusercontent.com/13550295/130592417-32ae4155-0ea3-4c7a-ad17-9fd2c67e92a5.png)


**Code to reproduce the issue**

```java
 public void testLongestSearcherWithNullValue() {
        TreeMap<String, String> buildFrom = new TreeMap<String, String>();
        TreeMap<String, String> buildFromValueNull = new TreeMap<String, String>();
        String[] keys = new String[]{""he"", ""her"", ""his""};
        for (String key : keys) {
            buildFrom.put(key, key);
            buildFromValueNull.put(key, null);
        }
        DoubleArrayTrie<String> trie = new DoubleArrayTrie<String>(buildFrom);
        DoubleArrayTrie<String> trieValueNull = new DoubleArrayTrie<String>(buildFromValueNull);

        String text = ""her3he6his-hers! "";

        DoubleArrayTrie<String>.LongestSearcher searcher = trie.getLongestSearcher(text.toCharArray(), 0);
        DoubleArrayTrie<String>.LongestSearcher searcherValueNull = trieValueNull.getLongestSearcher(text.toCharArray(), 0);

        while (true) {
            boolean next = searcher.next();
            boolean nextValueNull = searcherValueNull.next();

            if (next && nextValueNull) {
                assertTrue(searcher.begin == searcherValueNull.begin && searcher.length == searcherValueNull.length);
            } else if (next || nextValueNull) {
                assert false;
                break;
            } else {
                break;
            }
        }
    }
```

**Describe the current behavior**
由于map传入的value为null，即使存在，也查不到

**Expected behavior**
A clear and concise description of what you expected to happen.
根据length或者index进行判断
修复参见pull request (https://github.com/hankcs/HanLP/pull/1674)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: xxx
- HanLP version: 1.8.2

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
import hanlp bug on hanlp-2.1.0a55,"import hanlp的时候，出现AttributeError 
---------------------------------------------------------------------------
AttributeError       Traceback (most recent call last)
/var/folders/31/7mbjndl966d92drq1zvt9t3c0000gn/T/ipykernel_30433/3724283222.py in <module>
----> 1 import hanlp #Successfully installed hanlp-2.1.0a55

~/anaconda3/lib/python3.7/site-packages/hanlp/__init__.py in <module>
      8 from hanlp.version import __version__
      9 
---> 10 hanlp.utils.ls_resource_in_module(hanlp.pretrained)
     11 
     12 
AttributeError: module 'hanlp' has no attribute 'utils' 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS montery 
- Python version: Python 3.7.3
- HanLP version:hanlp-2.1.0a55

**Other info / logs** 

* [x] I've completed this form and searched the web for solutions. "
pos/ctb词性识别差异较大,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
【整箱】金龙鱼五常稻花香原香稻大米2.5kg*4/箱新米

上面这条语句中2.5kg pos/ctb词性标注时，被标注成了url，感觉差异有点大，并且通过 *海量级native API* 的python语言进行识别，仍然被识别成了URL
![图片](https://user-images.githubusercontent.com/56599784/129319120-4df90514-9747-4f02-9beb-a0ff7ac1bdee.png)
![图片](https://user-images.githubusercontent.com/56599784/129319786-9ba92cb4-e99f-4f28-a63a-6e225e19d661.png)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
test = ['【整箱】金龙鱼五常稻花香原香稻大米2.5kg*4/箱新米']
HanLP: MultiTaskLearning = hanlp.load(
    hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)
lp = HanLP(test)
print(lp)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: 3.8.10
- HanLP version: test = ['【整箱】金龙鱼五常稻花香原香稻大米2.5kg*4/箱新米']

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
关于多音字“莎”的拼音解析优化建议,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
**莎**这个多音字一般常见于人名，比如：`莎士比亚`，`丽莎`等，当前发现在`莎草`这个词或者一些古文中读作`suo`。
目前的拼音词典里将`suo`排在了`sha`前面，导致所有人名、地名的拼音转换全都不正确，是否将`sha`作为词典的第一顺位解析比较合适呢？

**Will this change the current api? How?**
当前的API不需要调整，只需要调整词典即可，当然用户可以自己调整拼音词典，但是这个多音字确实是`sha1`用的比较频繁，应用更广泛，因此才恳请在项目中调整。
修改词典文件[data/dictionary/pinyin/pinyin.txt](https://github.com/hankcs/HanLP/blob/1.x/data/dictionary/pinyin/pinyin.txt#L21325)，将`莎=suo1,sha1`调整成`莎=sha1,suo1`，同时添加词典`踏莎行=ta4,suo1,xing2`。

**Who will benefit with this feature?**
基本所有人都会受益于这次改动，毕竟`莎草`也已经定义到词典中了，剩余情况基本都是解释成`sha`。

**Are you willing to contribute it (Yes/No):**
如果有必要的话，我可以提交PR。当然作者直接调整会更快些。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: Java 8
- HanLP version:
```xml
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.8.2</version>
</dependency>
```

**Any other info**
代码段：
```java
System.out.println(HanLP.convertToPinyinString(""《罗密欧与朱丽叶》是莎士比亚创作的。"", ""_"", false));
```
输出结果：
```text
《_luo_mi_ou_yu_zhu_li_ye_》_shi_suo_shi_bi_ya_chuang_zuo_de_。
```

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
pip install hanlp[full] 的时候有依赖冲突,"
**Describe the bug**
进行full的hanlp安装时，有2.1和2.0两个版本同时安装而且依赖的tf版本不同，无法安装。

WARNING: hanlp 2.0.0a5 does not provide the extra 'full'
  Using cached hanlp-2.0.0a4.tar.gz (872 kB)
WARNING: hanlp 2.0.0a4 does not provide the extra 'full'
  Using cached hanlp-2.0.0a3.tar.gz (867 kB)
WARNING: hanlp 2.0.0a3 does not provide the extra 'full'
ERROR: Cannot install hanlp[full]==2.0.0a10, hanlp[full]==2.0.0a11, hanlp[full]==2.0.0a12, hanlp[full]==2.0.0a13, hanlp[full]==2.0.0a14, hanlp[full]==2.0.0a15, hanlp[full]==2.0.0a16, h
anlp[full]==2.0.0a17, hanlp[full]==2.0.0a18, hanlp[full]==2.0.0a19, hanlp[full]==2.0.0a20, hanlp[full]==2.0.0a21, hanlp[full]==2.0.0a22, hanlp[full]==2.0.0a24, hanlp[full]==2.0.0a25, h
anlp[full]==2.0.0a26, hanlp[full]==2.0.0a27, hanlp[full]==2.0.0a28, hanlp[full]==2.0.0a29, hanlp[full]==2.0.0a3, hanlp[full]==2.0.0a30, hanlp[full]==2.0.0a31, hanlp[full]==2.0.0a32, ha
nlp[full]==2.0.0a33, hanlp[full]==2.0.0a34, hanlp[full]==2.0.0a35, hanlp[full]==2.0.0a36, hanlp[full]==2.0.0a37, hanlp[full]==2.0.0a38, hanlp[full]==2.0.0a39, hanlp[full]==2.0.0a4, han
lp[full]==2.0.0a40, hanlp[full]==2.0.0a41, hanlp[full]==2.0.0a42, hanlp[full]==2.0.0a43, hanlp[full]==2.0.0a44, hanlp[full]==2.0.0a45, hanlp[full]==2.0.0a46, hanlp[full]==2.0.0a47, han
lp[full]==2.0.0a48, hanlp[full]==2.0.0a49, hanlp[full]==2.0.0a5, hanlp[full]==2.0.0a50, hanlp[full]==2.0.0a51, hanlp[full]==2.0.0a52, hanlp[full]==2.0.0a53, hanlp[full]==2.0.0a54, hanl
p[full]==2.0.0a55, hanlp[full]==2.0.0a56, hanlp[full]==2.0.0a57, hanlp[full]==2.0.0a58, hanlp[full]==2.0.0a59, hanlp[full]==2.0.0a6, hanlp[full]==2.0.0a60, hanlp[full]==2.0.0a61, hanlp
[full]==2.0.0a62, hanlp[full]==2.0.0a63, hanlp[full]==2.0.0a64, hanlp[full]==2.0.0a65, hanlp[full]==2.0.0a66, hanlp[full]==2.0.0a67, hanlp[full]==2.0.0a68, hanlp[full]==2.0.0a69, hanlp
[full]==2.0.0a8, hanlp[full]==2.0.0a9, hanlp[full]==2.1.0a0, hanlp[full]==2.1.0a1, hanlp[full]==2.1.0a10, hanlp[full]==2.1.0a11, hanlp[full]==2.1.0a12, hanlp[full]==2.1.0a13, hanlp[ful
l]==2.1.0a14, hanlp[full]==2.1.0a15, hanlp[full]==2.1.0a16, hanlp[full]==2.1.0a17, hanlp[full]==2.1.0a18, hanlp[full]==2.1.0a19, hanlp[full]==2.1.0a2, hanlp[full]==2.1.0a20, hanlp[full
]==2.1.0a21, hanlp[full]==2.1.0a22, hanlp[full]==2.1.0a23, hanlp[full]==2.1.0a24, hanlp[full]==2.1.0a25, hanlp[full]==2.1.0a26, hanlp[full]==2.1.0a27, hanlp[full]==2.1.0a28, hanlp[full
]==2.1.0a29, hanlp[full]==2.1.0a3, hanlp[full]==2.1.0a30, hanlp[full]==2.1.0a31, hanlp[full]==2.1.0a32, hanlp[full]==2.1.0a33, hanlp[full]==2.1.0a34, hanlp[full]==2.1.0a35, hanlp[full]
==2.1.0a36, hanlp[full]==2.1.0a37, hanlp[full]==2.1.0a38, hanlp[full]==2.1.0a4, hanlp[full]==2.1.0a41, hanlp[full]==2.1.0a42, hanlp[full]==2.1.0a43, hanlp[full]==2.1.0a44, hanlp[full]=
=2.1.0a45, hanlp[full]==2.1.0a46, hanlp[full]==2.1.0a47, hanlp[full]==2.1.0a48, hanlp[full]==2.1.0a5, hanlp[full]==2.1.0a50, hanlp[full]==2.1.0a51, hanlp[full]==2.1.0a52, hanlp[full]==
2.1.0a53, hanlp[full]==2.1.0a6, hanlp[full]==2.1.0a7, hanlp[full]==2.1.0a8 and hanlp[full]==2.1.0a9 because these package versions have conflicting dependencies.

The conflict is caused by:
    hanlp[full] 2.1.0a53 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a52 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a51 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a50 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a48 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a47 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a46 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a45 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a44 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a43 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a42 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a41 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a38 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a37 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a36 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a35 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a34 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a33 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a32 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a31 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a30 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a29 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a28 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a27 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a26 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a25 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a24 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a23 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a22 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a21 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a20 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a19 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a18 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a17 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a16 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a15 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a14 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a13 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a12 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a11 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a10 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a9 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a8 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a7 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a6 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a5 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a4 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a3 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a2 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a1 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a0 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a69 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a68 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a67 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a66 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a65 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a64 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a63 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a62 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a61 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a60 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a59 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a58 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a57 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a56 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a55 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a54 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a53 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a52 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a51 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a50 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a49 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a48 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a47 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a46 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a45 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a44 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a43 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a42 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a41 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a40 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a39 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a38 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a37 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a36 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a35 depends on tensorflow==2.1.0
    ...


**Code to reproduce the issue**

```python
pip install hanlp[full] 
```

**Describe the current behavior**
pip提示冲突，无法安装

**Expected behavior**
正常安装
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Python version: 3.9.5
- HanLP version: full

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
ImportError: cannot import name 'TFAutoModel',"**Describe the bug**
ImportError: cannot import name 'TFAutoModel'

**Code to reproduce the issue**
from transformers import BertTokenizer, BertConfig, PretrainedConfig, TFAutoModel, \

**Describe the current behavior**
ImportError: cannot import name 'TFAutoModel'

**Expected behavior**
ImportError: cannot import name 'TFAutoModel'

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Darwin-19.6.0-x86_64-i386-64bit
- Python version:3.6.9
- HanLP version:2.1.0-alpha.53

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/tok/pku98_6m_conv_ngram_20200110_134736.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp_common/reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp_common/reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/components/tok_tf.py"", line 12, in <module>
    from hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger_tf.py"", line 11, in <module>
    from hanlp.layers.transformers.loader_tf import build_transformer
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/layers/transformers/loader_tf.py"", line 12, in <module>
    from hanlp.layers.transformers.tf_imports import zh_albert_models_google, bert_models_google
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/layers/transformers/tf_imports.py"", line 5, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, TFAutoModel, \
ImportError: cannot import name 'TFAutoModel'
=================================ERROR LOG ENDS=================================
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above and the system info below.
OS: Darwin-19.6.0-x86_64-i386-64bit
Python: 3.6.9
PyTorch: 1.9.0
HanLP: 2.1.0-alpha.53

* [x] I've completed this form and searched the web for solutions."
cannot import name 'albert_models_tfhub' from 'bert' ,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
bert包里没有albert_models_tfhub
  File ""C:\ProgramData\Anaconda3\envs\nlp\lib\site-packages\hanlp\layers\transformers\loader_tf.py"", line 9, in <module>
    from bert import albert_models_tfhub, fetch_tfhub_albert_model, load_stock_weights
ImportError: cannot import name 'albert_models_tfhub' from 'bert' (C:\ProgramData\Anaconda3\envs\nlp\lib\site-packages\bert\__init__.py)

**Code to reproduce the issue**
import hanlp
import bert
from hanlp_restful import HanLPClient

test1 = '葛鹤军先生:博士学位。2008年至2011年就职于中诚信证券评估有限公司、中诚信国际信用评级有限公司。2011年11月加盟银华基金管理有限公司,历任研究员、基金经理助理。现任投资管理三部基金经理。自2014年10月8日起担任“银华中证中票50指数债券型证券投资基金(LOF)”“银华信用债券型证券投资基金(LOF)”的基金经理;自2014年10月8日起至2016年4月25日担任“银华中证成长股债恒定组合30/70指数证券投资基金”基金经理;自2015年4月24日起担任“银华泰利灵活配置混合型证券投资基金”基金经理;自2015年5月6日起担任“银华恒利灵活配置混合型证券投资基金”基金经理;自2015年6月17日起担任“银华信用双利债券型证券投资基金”基金经理;自2016年8月5日起担任“银华通利灵活配置混合型证券投资基金”基金经理;自2016年12月5日开始担任“银华上证10年期国债指数证券投资基金”基金经理;自2016年12月5日开始担任“银华上证5年期国债指数证券投资基金”基金经理;自2017年4月17日开始担任“银华中债-10年期国债期货期限匹配金融债指数证券投资基金”基金经理;自2017年4月17日开始担任“银华中债-5年期国债期货期限匹配金融债指数证券投资基金”基金经理。2018年7月加入长盛基金管理有限公司,现任固定收益部执行总监,自2018年12月6日起任长盛盛琪一年期定期开放债券型证券投资基金基金经理。拟任长盛安鑫中短债债券型证券投资基金基金经理。2020年9月29日起担任长盛盛裕纯债债券型证券投资基金基金经理。'

HanLP = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)
HanLP(test1)

```python
```

**Describe the current behavior**
启动不了

**Expected behavior**
正常启动

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
centos 加载依存句法报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
其他功能OK，但是加载依存句法的时候出错了

**Code to reproduce the issue**
报错内容
dependency_info = HanLP.parseDependency(sentence)
jpype._jclass.ExceptionInInitializerError: None
```python
```

**Describe the current behavior**
直接报错退出

**Expected behavior**
win系统都好好的，部署到linux就跪了，希望能运行起来吧

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  centos7
- Python version: 3.6.8
- HanLP version: 1.7.5
- JDK version：1.8.0_252

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Failed to load model,"
**Describe the bug**
Failed to load model

**Code to reproduce the issue**
HanLP = hanlp.load(hanlp.pretrained.classifiers.CHNSENTICORP_BERT_BASE_ZH)

**Describe the current behavior**
Failed to load model

**Expected behavior**
Failed to load model

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows-10-10.0.19041-SP0
- Python version:3.8.3
- HanLP version:2.1.0-alpha.50

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/classification/chnsenticorp_bert_base_20200104_164655.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\utils\component_util.py"", line 81, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\keras_component.py"", line 214, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\keras_component.py"", line 223, in build
    self.transform.build_config()
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\transform_tf.py"", line 54, in build_config
    self.output_types, self.output_shapes, self.padding_values = self.create_types_shapes_values()
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\components\classifiers\transformer_classifier_tf.py"", line 78, in create_types_shapes_values
    shapes = ([max_length], [max_length], [max_length]), [None, ] if self.config.multi_label else []
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp_common\structure.py"", line 95, in __getattr__
    return self.__getitem__(key)
KeyError: 'multi_label'
=================================ERROR LOG ENDS=================================
Please upgrade hanlp with:
pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above and the system info below.
OS: Windows-10-10.0.19041-SP0
Python: 3.8.3
PyTorch: 1.8.1+cu102
HanLP: 2.1.0-alpha.50
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

* [x] I've completed this form and searched the web for solutions."
增加自定义词典bin是否主动根据文件变更时间更新的配置，默认为true,增加配置CustomDictionaryBinUpdate，用于判断是否主动根据文件时间变革来更新CustomDictionary.txt.bin，默认为true
能否将自定义词典bin更新加载改为可配置,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
因为在开发elasticsearch相关hanlp插件时，elasticsearch在7.11.0之后限制了插件的权限，只有读取文件权限，但HanLP在1.8.0之后增加了词典热更新，在读取custom dictionary时，会判断bin文件是否是最新，从而开启热加载（删除并重新构建），但elasticsearch插件安装时，词典文件的最后更新时间不定，从而导致热加载，因权限问题热加载失败

**Will this change the current api? How?**
能否将词典热加载改为可配置，增加开关

**Who will benefit with this feature?**
elasticsearch关于hanlp的分词插件

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version: >= 1.8.0

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
執行分詞會產生 RuntimeError: cannot perform reduction function argmax on a tensor with no elements because the operation does not have an identity,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
某個字串使用執行分詞會產生 exception。

**Code to reproduce the issue**
```python
import hanlp
import re


HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 這個不會有 exception
#HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH) # 這個會有 exception


def segmenter_for_report(sentence):
    import re

    regex_patterns = {}
    regex_patterns['patter1'] = re.compile('\s+')
    regex_patterns['patter2'] = re.compile('[^\s]+')

    tokens = []

    s = sentence
    cur_pos = 0
    token_start_pos_base = cur_pos
    while True:
        max_matched_len = 0
        next_token = None
        token_type = None

        # greedy search for the longest pattern from the beginning of 's'
        print('Segmenting sentence: %s' % s)
        for k, p in regex_patterns.items():
            # return matchobject if matches
            m = p.match(s)
            if m:
                print('【%s】 matched pattern: %s, start: %d, end: %d' %(s, k, m.start(0), m.end(0)))
                cur_matched_len = m.end(0) - m.start(0)
                if max_matched_len < cur_matched_len:
                    max_matched_len = cur_matched_len
                    next_token = m.group(0)
                    token_type = k

        if token_type:
            print('longest matched pattern type: %s, token: %s' % (token_type, next_token))
        else:
            print('Cannot find the matched pattern. sentence: %s' % s)

        # split off the longest token we found.
        if next_token:
            string_left = s[max_matched_len:]
            string_left_trimmed = s[max_matched_len:].lstrip()

            # process the token we found
            token_start_pos = token_start_pos_base
            token_end_pos = token_start_pos_base + max_matched_len
            tokens.append(next_token)

            s = string_left_trimmed
            token_start_pos_base = token_end_pos + (
                        len(string_left) - len(string_left_trimmed))  # the next base
                
        else:
            if len(s.strip()) > 0:
                print('Unprocessed substring: %s' % s)
                break

        if len(s) == 0:
            break
    return tokens


test_cases = [
    '( ͡° ͜ʖ ͡ °)',
]    

for c in test_cases:
    seg_phrases = segmenter_for_report(c)
    for seg_text in seg_phrases:
        tok_results = HanLP(seg_text, tasks=['tok/fine', 'pos/ctb'])
        print('result: %s' % tok_results)
```

**Describe the current behavior**
而且以上狀況只有使用 hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH 才會發生
使用 hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH 不會發生

**Expected behavior**
至少正常返回。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Python version: Python 3.7.3
- HanLP version: 2.1.0a38

**Other info / logs**
```
Segmenting sentence: ( ͡° ͜ʖ ͡ °)
【( ͡° ͜ʖ ͡ °)】 matched pattern: patter2, start: 0, end: 1
longest matched pattern type: patter2, token: (
Segmenting sentence: ͡° ͜ʖ ͡ °)
【͡° ͜ʖ ͡ °)】 matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: ͡°
Segmenting sentence: ͜ʖ ͡ °)
【͜ʖ ͡ °)】 matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: ͜ʖ
Segmenting sentence: ͡ °)
【͡ °)】 matched pattern: patter2, start: 0, end: 1
longest matched pattern type: patter2, token: ͡
Segmenting sentence: °)
【°)】 matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: °)
result: {
  ""tok/fine"": [
    ""(""
  ],
  ""pos/ctb"": [
    ""PU""
  ]
}
result: {
  ""tok/fine"": [
    ""°""
  ],
  ""pos/ctb"": [
    ""PU""
  ]
}
result: {
  ""tok/fine"": [
    ""ʖ""
  ],
  ""pos/ctb"": [
    ""NN""
  ]
}
Traceback (most recent call last):
  File ""tests\report_hanlp.py"", line 75, in <module>
    tok_results = HanLP(seg_text, tasks=['tok/fine', 'pos/ctb'])
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\torch\autograd\grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\common\torch_component.py"", line 633, in __call__
    **kwargs))
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\common\component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 505, in predict
    cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 592, in predict_task
    self.decode_output(output_dict, batch, output_key)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 732, in decode_output
    self.model.decoders[task_name])
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\tasks\tok\tag_tok.py"", line 123, in decode_output
    return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\tokenizers\transformer.py"", line 97, in decode_output
    output = super().decode_output(logits, mask, batch, model)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\taggers\tagger.py"", line 64, in decode_output
    return logits.argmax(-1)
RuntimeError: cannot perform reduction function argmax on a tensor with no elements because the operation does not have an identity
```

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
有些中文字會誤判為標點符號（ctb: PU),"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
有些中文字會誤判為標點符號.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)

test_cases = [
        '嗯..啊..我們沒有收到檢舉啊',
        '普丁就大量的來汙衊希拉蕊',
        '去洗那個鹼 去洗那個鹽',
        '福島那個是爆炸的問題 核電廠那個氚洗不掉的',
        '像它的模式可以改成15分鐘或者是永..永久',
        '禿溜窩 哈批溜 歐普搜',
    ]   

for c in test_cases:
    HanLP(c, tasks=['tok/fine', 'pos/ctb'])

```

**Describe the current behavior**
這幾個字會誤判為 標點符號 (PU）: 

嗯..（PU）：嗯..啊..我們沒有收到檢舉啊
啊..（PU）：嗯..啊..我們沒有收到檢舉啊
衊（PU）：普丁就大量的來汙衊希拉蕊
鹼（PU）： 去洗那個鹼
氚（PU）： 福島那個是爆炸的問題 核電廠那個氚洗不掉的
永（PU）：像它的模式可以改成15分鐘或者是永..永久
溜（PU）：禿溜窩 哈批溜 歐普搜

**Expected behavior**
應該至少要知道是中文。不論詞性正確與否

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- Python version: Python 3.7.3 
- HanLP version:  2.1.0a38

**Other info / logs**
```
2021-05-14 09:42:45,673 - unittest - INFO - {
  ""tok/fine"": [
    ""嗯.."",
    ""啊.."",
    ""我們"",
    ""沒有"",
    ""收到"",
    ""檢舉"",
    ""啊""
  ],
  ""pos/ctb"": [
    ""PU"",
    ""PU"",
    ""PN"",
    ""VV"",
    ""VV"",
    ""NN"",
    ""SP""
  ]
}

2021-05-14 09:42:45,694 - unittest - INFO - {
  ""tok/fine"": [
    ""普丁"",
    ""就"",
    ""大量"",
    ""的"",
    ""來汙"",
    ""衊"",
    ""希拉蕊""
  ],
  ""pos/ctb"": [
    ""NR"",
    ""AD"",
    ""CD"",
    ""DEV"",
    ""VV"",
    ""PU"",
    ""NR""
  ]
}

2021-05-14 09:42:45,712 - unittest - INFO - {
  ""tok/fine"": [
    ""去"",
    ""洗"",
    ""那"",
    ""個"",
    ""鹼""
  ],
  ""pos/ctb"": [
    ""VV"",
    ""VV"",
    ""DT"",
    ""M"",
    ""PU""
  ]
}

2021-05-14 09:42:45,772 - unittest - INFO - {
  ""tok/fine"": [
    ""核電廠"",
    ""那"",
    ""個"",
    ""氚"",
    ""洗"",
    ""不"",
    ""掉"",
    ""的""
  ],
  ""pos/ctb"": [
    ""NN"",
    ""DT"",
    ""M"",
    ""PU"",
    ""VV"",
    ""AD"",
    ""VV"",
    ""SP""
  ]
}

2021-05-14 09:42:45,792 - unittest - INFO - {
  ""tok/fine"": [
    ""像"",
    ""它"",
    ""的"",
    ""模式"",
    ""可以"",
    ""改成"",
    ""15"",
    ""分鐘"",
    ""或者"",
    ""是"",
    ""永.."",
    ""永久""
  ],
  ""pos/ctb"": [
    ""P"",
    ""PN"",
    ""DEG"",
    ""NN"",
    ""VV"",
    ""VV"",
    ""CD"",
    ""M"",
    ""CC"",
    ""VC"",
    ""PU"",
    ""VA""
  ]
}


2021-05-14 09:42:45,830 - unittest - INFO - {
  ""tok/fine"": [
    ""哈批"",
    ""溜""
  ],
  ""pos/ctb"": [
    ""IJ"",
    ""PU""
  ]
}

2021-05-14 09:42:45,830 - unittest - INFO - {
  ""tok/fine"": [
    ""哈批"",
    ""溜""
  ],
  ""pos/ctb"": [
    ""IJ"",
    ""PU""
  ]
}
```


* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
ValueError: invalid literal for int() with base 8: 'uild_ten',"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
Run failed with simple code after hanlp pip installed.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库
HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'])
```

**Describe the current behavior**
Run failed with exceptions.

**Expected behavior**
Run success with ouputs as readme.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Python version: python 3.6.9
- HanLP version: 2.1.0a38 (pip install)

**Other info / logs**
```
Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip
100% 114.3 MiB   2.7 MiB/s ETA:  0 s [=============================================================]
Decompressing /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to /home/zhangguanqun/.hanlp/mtl
Downloading: 100%|██████████| 627/627 [00:00<00:00, 409kB/s]
Downloading: 100%|██████████| 110k/110k [00:00<00:00, 151kB/s]  
Downloading: 100%|██████████| 269k/269k [00:00<00:00, 277kB/s] 
Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.53kB/s]
Downloading: 100%|██████████| 112/112 [00:00<00:00, 81.4kB/s]
Downloading: 100%|██████████| 19.0/19.0 [00:00<00:00, 15.1kB/s]
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 189, in nti
    n = int(s.strip() or ""0"", 8)
ValueError: invalid literal for int() with base 8: 'uild_ten'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 2299, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File ""/usr/lib/python3.6/tarfile.py"", line 1093, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File ""/usr/lib/python3.6/tarfile.py"", line 1035, in frombuf
    chksum = nti(buf[148:156])
  File ""/usr/lib/python3.6/tarfile.py"", line 191, in nti
    raise InvalidHeaderError(""invalid header"")
tarfile.InvalidHeaderError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 555, in _load
    return legacy_load(f)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 466, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File ""/usr/lib/python3.6/tarfile.py"", line 1591, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1621, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1484, in __init__
    self.firstmember = self.next()
  File ""/usr/lib/python3.6/tarfile.py"", line 2311, in next
    raise ReadError(str(e))
tarfile.ReadError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 81, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 182, in load
    self.load_weights(save_dir, **kwargs)
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 100, in load_weights
    self.model_.load_state_dict(torch.load(filename, map_location='cpu'), strict=False)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 559, in _load
    raise RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name))
RuntimeError: /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/model.pt is a zip archive (did you mean to use torch.jit.load()?)
=================================ERROR LOG ENDS=================================
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```


* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
pyhanlp多进程问题,"**Describe the bug**
pyhanlp多进程异常. 不能充分利用cpu,而且感觉  代码停止/""卡住""

**Code to reproduce the issue**
```
!pip3 install pyhanlp
from multiprocessing import Pool
from tqdm import tqdm
from pyhanlp import HanLP
print(HanLP.segment('hello'))

def test_process(tmp: int):
    for i in range(10000):
        HanLP.segment(""商品和服务"")

pool_ = Pool(2)
result = pool_.map(test_process, tqdm(range(10)))
pool_.close()
pool_.join()
# print(result)
print('END')
```

**Describe the current behavior**
同样的多进程代码,就单纯的分词代码改成其它分词工具是没有问题的
```HanLP.segment``` -> ```jieba.cut```
但是hanlp运行的时候cpu使用率在130%左右(机器是2颗  E5-2620 v4,每颗是8核16线程,内存剩余30G)
我不知道是真的卡住还是,速度慢.



**Expected behavior**
我希望能够多进程,高速运行hanlp分词


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core) 
- Python version:3.6.5
- HanLP version:
hanlp                       2.1.0a36
hanlp-common                0.0.6
hanlp-downloader            0.0.20
hanlp-trie                  0.0.2
pyhanlp                     0.1.77

**Other info / logs**
这是使用
https://play.hanlp.ml/run/hanlp-zh
运行的结果

cpu使用率依旧是0,等了好久,一直在转.
图片链接
https://sm.ms/image/acSxlnBwpG49ehJ

代码改自**https://github.com/hankcs/HanLP/issues/1625**

在网上找到的可能相关的问题
https://bbs.hankcs.com/t/topic/2128

* [x] I've completed this form and searched the web for solutions.
"
Pinyin.java 中 部分韵母错误,"**Describe the bug**
Hello, hanlp developers, I found some bugs in the file Pinyin.java

in commit 6b60684f447d4c9f4ad68016fd1b443ef50e9bb4

lve3 's yunmu is ve ， but lve4 's yunmu is ue .

Source Code: 
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L702
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L698
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L841

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
from pyhanlp import *
from multiprocessing import Pool, cpu_count
Pinyin = JClass(""com.hankcs.hanlp.dictionary.py.Pinyin"")
# hanlp 测试
text = ""驴子，略带，疟疾""       
pinyin_list = HanLP.convertToPinyinList(text)
for p in pinyin_list:
    print(p.__str__(), end="" "")
print()
for pinyin in pinyin_list:
    print(""%s,"" % pinyin.getYunmu(), end="" "")
```

**Describe the current behavior**
```text
lv2 zi5 none5 lve4 dai4 none5 nve4 ji2 
u, i, none, ue, ai, none, ue, i, 
```

**Expected behavior**
```text
lv2 zi5 none5 lve4 dai4 none5 nve4 ji2 
ve, i, none, ve, ai, none, ve, i, 
```

**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04
- Python version: 3.6.12
- HanLP version: 1.8.1

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
"
golang support,hanlp restful golang support
调整golang 说明 相对位置,其中java中跟快速上手感觉是联合起来的。所以调整了一下相对位置
golang support,golang restful support
golang support,golang restful support
调用CustomDictionary.reload()返回false，无法更新词典,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
from hanlp import CustomDictionary 后，调用CustomDictionary.reload()返回false，无法更新词典，通过打印部分日志后仍然没有解决问题
![image](https://user-images.githubusercontent.com/70429720/112116759-92bc6280-8bf5-11eb-9518-2d554eccc8ae.png)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

>>> from pyhanlp import CustomDictionary, HanLP
>>> print(HanLP.Config.CustomDictionaryPath)
('/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/CustomDictionary.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/现代汉语补充词库.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/全国地名大全.txt ns', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/人名词典.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/机构名词典.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/上海地名.txt ns', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/person/nrf.txt nrf')
>>> print(CustomDictionary.DEFAULT.path)
()
>>> print(CustomDictionary.reload())
False


**Describe the current behavior**
A clear and concise description of what happened.
reload永远返回false，暂时无法定位具体原因

**Expected behavior**
A clear and concise description of what you expected to happen.
期望reload得到True的返回结果，成功更新

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- Python version: 3.6.8
- HanLP version: 0.1.63


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
见上边的截图中

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
convertToPinyinList方法，返回了所有同音字的拼音。,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
convertToPinyinList方法，返回了所有同音字的拼音。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Java
System.out.println(HanLP.convertToPinyinString(""你好"", "" "", false));
// ni hao hao
```

**Describe the current behavior**
convertToPinyinList方法返回了多音字所有的拼音，导致返回的拼音字符串有多余的多音字，同时当需要转拼音字符串有中英混杂时，会出现报错IndexOutOfBoundaryException。1.7.8无此问题。

**Expected behavior**
“你好”返回“ni hao”而不是“ni hao hao”。

**System information**
- Linux、Windows
- Python version:
- HanLP version:1.8.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
FileNotFoundError: The identifier cc.en.300.bin resolves to a non-exist meta file config.json,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
FileNotFoundError when try to load word2vec pre-trainned model

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
fst = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN)
```

**Describe the current behavior**
load FASTTEXT_CC_300_EN failed with FileNotFoundError

**Expected behavior**
load FASTTEXT_CC_300_EN model should success 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Python version: Python 3.8.8
- HanLP version: Version: 2.1.0a36

**Other info / logs**
```
(hanlp) ACTIONTRUSTNAME:~ pe.li$ python
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
>>> fst = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/pe.li/miniconda3/envs/hanlp/lib/python3.8/site-packages/hanlp/__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""/Users/pe.li/miniconda3/envs/hanlp/lib/python3.8/site-packages/hanlp/utils/component_util.py"", line 53, in load_from_meta_file
    raise FileNotFoundError(f'The identifier {save_dir} resolves to a non-exist meta file {metapath}. {tips}')
FileNotFoundError: The identifier /Users/pe.li/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin resolves to a non-exist meta file /Users/pe.li/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin/config.json.
```

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
demo训练数据低下,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
运行给出来的 训练demo，gpu 利用率很低，

**Code to reproduce the issue**
https://github.com/hankcs/HanLP/blob/master/plugins/hanlp_demo/hanlp_demo/zh/demo_custom_dict.py

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
![cuda](https://user-images.githubusercontent.com/22882285/109949146-00235480-7d16-11eb-89bd-32da15eff0f9.jpg)
![hanlp](https://user-images.githubusercontent.com/22882285/109949170-05809f00-7d16-11eb-8709-6b1aed23e42f.jpg)
![hanlp1](https://user-images.githubusercontent.com/22882285/109949193-0a455300-7d16-11eb-9f18-6653c562014c.jpg)


**System information**
- OS Platform and Distribution (e.g., win 10):
- Python version: 3.6
- HanLP version: 2.1
- torch version:1.7.0
- torchvision: 0.8.1

**Other info / logs**
请确人是不是你们那边没有对win作优化，导致 在win 10下更不上 linux 的训练速度

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
hjj ,"
![hanlp1](https://user-images.githubusercontent.com/22882285/109948619-796e7780-7d15-11eb-8e43-6dac7856ab63.jpg)
<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
demo 训练缓慢，gpu使用率几乎为零

**Code to reproduce the issue**
import hanlp
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from tests import cdroot

cdroot()
HanLP: MultiTaskLearning = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
tok: TaggingTokenization = HanLP['tok/fine']

# tok.dict_force = tok.dict_combine = None
# print(f'不挂词典:\n{HanLP(""商品和服务行业"")[""tok/fine""]}')
#
# tok.dict_force = {'和服', '服务行业'}
# print(f'强制模式:\n{HanLP(""商品和服务行业"")[""tok/fine""]}')  # 慎用，详见《自然语言处理入门》第二章
#
# tok.dict_force = {'和服务': ['和', '服务']}
# print(f'强制校正:\n{HanLP(""正向匹配商品和服务、任何和服务必按上述切分"")[""tok/fine""]}')

tok.dict_force = None
tok.dict_combine = {'和服', '服务行业'}
print(f'合并模式:\n{HanLP(""商品和服务行业"")[""tok/fine""]}')

# 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php
# See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
![cuda](https://user-images.githubusercontent.com/22882285/109948589-72476980-7d15-11eb-93b5-73c71bc857d7.jpg)
![hanlp](https://user-images.githubusercontent.com/22882285/109948605-770c1d80-7d15-11eb-8cee-a51abec776e9.jpg)
![hanlp1](https://user-images.githubusercontent.com/22882285/109948680-8723fd00-7d15-11eb-9010-17ac0c7988ce.jpg)


**System information**
- OS Platform and Distribution (e.g., win 10):
- Python version:3.6
- HanLP version:2.1

**Other info / logs**
 在加载完成模型之后，没有看到gpu负载指标，这个是不正常的

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
pyhanlp多进程问题,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
使用pyhanlp在多线程下测试无问题，在多进程下会出现异常。


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```from multiprocessing import Pool
from tqdm import tqdm
from bert4keras.snippets import parallel_apply
import argparse
import pandas as pd
from pyhanlp import HanLP, SafeJClass
from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM

def test_process(tmp: int):
    print(str(tmp) + '\n')
    print(HanLP.segment(""商品和服务""))
    return str(tmp)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('-mode', '--mode', type=str, default='pool', help='')
    # 获取相关参数
    args = parser.parse_args()
    mode = args.mode
    if mode == 'apply':
        result = parallel_apply(func=test_process, iterable=tqdm(range(20)),
                                workers=20, max_queue_size=1500, callback=None, )
        print(result)
        print('END')
    elif mode == 'pool':
        pool_ = Pool(20)
        result = pool_.map(test_process, tqdm(range(10)))
        pool_.close()
        pool_.join()
        print(result)
        print('END')
```



**Describe the current behavior**
A clear and concise description of what happened.

HanLP segment ->: 
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f994b134449, pid=79026, tid=0x00007f9960d80700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29
#
# Core dump written. Default location: /data1/jiangxl/bert4keras-master/pretraining/core or core.79026
#
# Can not save log file, dump to screen..
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f994b134449, pid=79026, tid=0x00007f9960d80700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29
#
# Core dump written. Default location: /data1/jiangxl/bert4keras-master/pretraining/core or core.79026
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread is native thread

siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000000

Registers:
RAX=0x0000000001f26eb0, RBX=0x0000000000000000, RCX=0x0000000000000000, RDX=0x00000000008ddd00
RSP=0x00007f9960d7dc60, RBP=0x0000000001f26eb0, RSI=0x0000000000000000, RDI=0x00000000008ddd00
R8 =0x0000000000000000, R9 =0x000000000000000c, R10=0x0000000000000002, R11=0x0000000000a16e00
R12=0x00007f981c001910, R13=0x0000000000000005, R14=0x00007f9960d7dde0, R15=0x00007f9960d7dd60
RIP=0x00007f994b134449, EFLAGS=0x0000000000010202, CSGSFS=0x0000000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f9960d7dc60)
0x00007f9960d7dc60:   00007f9960d7dca0 00007f981c001910
0x00007f9960d7dc70:   0000000000000005 00007f9960d7dca0
0x00007f9960d7dc80:   0000000001de9df8 00007f994b142f2f
0x00007f9960d7dc90:   00007f9960d7dcbf 00007f990c0d80f8
0x00007f9960d7dca0:   00007f994b3805d0 00007f981c0018f0
0x00007f9960d7dcb0:   0000000000000005 0000000000512e89
0x00007f9960d7dcc0:   0000000000000000 0000000000000001
0x00007f9960d7dcd0:   0000000000000001 00007f994b13f6d8
0x00007f9960d7dce0:   0000000000000000 0000000000000001
0x00007f9960d7dcf0:   0000000000000008 0000000000000000
0x00007f9960d7dd00:   00007f9960d7dde0 0000000000000001
0x00007f9960d7dd10:   0000000001de9df8 00007f9960d7dd5f
0x00007f9960d7dd20:   00007f9960d7dde0 0000000000000001
0x00007f9960d7dd30:   00007f990c0cce10 00007f994b140b03
0x00007f9960d7dd40:   00007f9960d7de00 00007f9960d7dde0
0x00007f9960d7dd50:   0000000000000000 00007f9960d7dde0
0x00007f9960d7dd60:   00007f981c001670 00007f981c0018d0
0x00007f9960d7dd70:   00007f9960d7de00 00007f99bdff9ad8
0x00007f9960d7dd80:   0000000000000001 00007f981c0018d0
0x00007f9960d7dd90:   00007f9960d7de00 00007f994b1674a2
0x00007f9960d7dda0:   00007f9960d7ddd0 00007f9960d7dde0
0x00007f9960d7ddb0:   00007f9960d7ddcf 00007f994b0e8f00
0x00007f9960d7ddc0:   00007f994b0e3188 0000000000474a23
0x00007f9960d7ddd0:   00007f981c0018d0 00007f99c53dd630
0x00007f9960d7dde0:   00007f981c0015b0 00007f981c0015b8
0x00007f9960d7ddf0:   00007f981c0015b8 00007f9960d7de40
0x00007f9960d7de00:   00007f994b380590 00007f981c001600
0x00007f9960d7de10:   00007f981c001608 00007f981c001608
0x00007f9960d7de20:   0000000000000004 00007f994b0e8f00
0x00007f9960d7de30:   00007f994b167380 0000000001f26eb0
0x00007f9960d7de40:   00007f994b0e8f00 0000000000000000
0x00007f9960d7de50:   00007f990c0cce10 0000000000437e60 

Instructions: (pc=0x00007f994b134449)
0x00007f994b134429:   f4 55 53 48 83 ec 08 e8 4b d3 fe ff 48 89 c3 e8
0x00007f994b134439:   93 c9 fe ff 48 8b 10 48 89 c7 ff 52 30 48 89 c5
0x00007f994b134449:   48 8b 03 44 89 ea 48 89 df 4c 89 e6 ff 90 18 05
0x00007f994b134459:   00 00 48 89 c3 e8 6d c9 fe ff 48 89 c7 48 8b 00 

Register to memory mapping:

RAX=0x0000000001f26eb0 is an unknown value
RBX=0x0000000000000000 is an unknown value
RCX=0x0000000000000000 is an unknown value
RDX=0x00000000008ddd00: <offset 0x4ddd00> in python3 at 0x0000000000400000
RSP=0x00007f9960d7dc60 is an unknown value
RBP=0x0000000001f26eb0 is an unknown value
RSI=0x0000000000000000 is an unknown value
RDI=0x00000000008ddd00: <offset 0x4ddd00> in python3 at 0x0000000000400000
R8 =0x0000000000000000 is an unknown value
R9 =0x000000000000000c is an unknown value
R10=0x0000000000000002 is an unknown value
R11=0x0000000000a16e00 is an unknown value
R12=0x00007f981c001910 is an unknown value
R13=0x0000000000000005 is an unknown value
R14=0x00007f9960d7dde0 is an unknown value
R15=0x00007f9960d7dd60 is an unknown value


Stack: [0x00007f9960581000,0x00007f9960d81000],  sp=0x00007f9960d7dc60,  free space=8179k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29


---------------  P R O C E S S  ---------------

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap:
 PSYoungGen      total 305664K, used 124003K [0x00000000d5580000, 0x00000000eaa80000, 0x0000000100000000)
  eden space 262144K, 47% used [0x00000000d5580000,0x00000000dce98e28,0x00000000e5580000)
  from space 43520K, 0% used [0x00000000e8000000,0x00000000e8000000,0x00000000eaa80000)
  to   space 43520K, 0% used [0x00000000e5580000,0x00000000e5580000,0x00000000e8000000)
 ParOldGen       total 699392K, used 0K [0x0000000080000000, 0x00000000aab00000, 0x00000000d5580000)
  object space 699392K, 0% used [0x0000000080000000,0x0000000080000000,0x00000000aab00000)
 Metaspace       used 4217K, capacity 5120K, committed 5376K, reserved 1056768K
  class space    used 466K, capacity 496K, committed 512K, reserved 1048576K

Card table byte_map: [0x00007f9939e9c000,0x00007f993a29d000] byte_map_base: 0x00007f9939a9c000

Marking Bits: (ParMarkBitMap*) 0x00007f994b091c80
 Begin Bits: [0x00007f9914000000, 0x00007f9916000000)
 End Bits:   [0x00007f9916000000, 0x00007f9918000000)

Polling page: 0x00007f99c5444000

CodeCache: size=245760Kb used=4071Kb max_used=4086Kb free=241688Kb
 bounds [0x00007f993a65d000, 0x00007f993aa6d000, 0x00007f994965d000]
 total_blobs=565 nmethods=309 adapters=166
 compilation: enabled

Compilation events (10 events):
Event: 0.787 Thread 0x0000000001d60800  305       4       com.hankcs.hanlp.model.perceptron.model.LinearModel::load (165 bytes)
Event: 0.790 Thread 0x0000000001d69000  306       1       java.nio.Buffer::position (5 bytes)
Event: 0.790 Thread 0x0000000001d69000 nmethod 306 0x00007f993aa368d0 code [0x00007f993aa36a20, 0x00007f993aa36b30]
Event: 0.820 Thread 0x0000000001d6b800  307       1       java.nio.channels.spi.AbstractInterruptibleChannel::isOpen (5 bytes)
Event: 0.820 Thread 0x0000000001d6b800 nmethod 307 0x00007f993aa25590 code [0x00007f993aa256e0, 0x00007f993aa257f0]
Event: 0.825 Thread 0x0000000001d60800 nmethod 305 0x00007f993aa5b190 code [0x00007f993aa5b4c0, 0x00007f993aa5cba8]
Event: 0.872 Thread 0x0000000001d67000  308       3       java.lang.ThreadLocal::getMap (5 bytes)
Event: 0.872 Thread 0x0000000001d67000 nmethod 308 0x00007f993aa3e410 code [0x00007f993aa3e580, 0x00007f993aa3e6f0]
Event: 0.883 Thread 0x0000000001d6d800  309       3       java.lang.ThreadLocal::get (38 bytes)
Event: 0.883 Thread 0x0000000001d6d800 nmethod 309 0x00007f993aa5aa50 code [0x00007f993aa5abe0, 0x00007f993aa5af98]

GC Heap History (0 events):
No events

Deoptimization events (10 events):
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.641 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa0d7a0 method=java.util.TreeMap.getEntry(Ljava/lang/Object;)Ljava/util/TreeMap$Entry; @ 36
Event: 0.653 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa2cc90 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.663 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa37664 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.692 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa40950 method=com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(Lcom/hankcs/hanlp/corpus/io/ByteArray;)Z @ 31
Event: 0.702 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa20240 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.745 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa3f010 method=com.hankcs.hanlp.model.perceptron.model.LinearModel.load(Lcom/hankcs/hanlp/corpus/io/ByteArray;)Z @ 106
Event: 0.750 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa14e28 method=java.util.ArrayList.grow(I)V @ 15

Internal exceptions (7 events):
Event: 0.061 Thread 0x0000000001780000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.defineClass(Ljava/lang/String;[BII)Ljava/lang/Class; name or signature does not match> (0x00000000d5587ca8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/sh
Event: 0.061 Thread 0x0000000001780000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.prefetchRead(Ljava/lang/Object;J)V name or signature does not match> (0x00000000d5587f90) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jni.c
Event: 0.209 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56adce0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.209 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56adef0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.210 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56b01c8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.210 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56b03d8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.255 Thread 0x0000000001780000 Exception <a 'java/lang/NoClassDefFoundError': Ljava/lang;> (0x00000000d576d2c0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 199]




**Expected behavior**
A clear and concise description of what you expected to happen.
正常分词输出

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

python3.5
pyhanlp 0.1.44
JPype1 0.6.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
"IndexError: The shape of the mask [86, 39, 39] at index 1 does not match the shape of the indexed tensor [86, 41, 41, 19] at index 1","<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
Error when parsing 33082 english sentences.

**Code to reproduce the issue**
data here:  
[qtitles.zip](https://github.com/hankcs/HanLP/files/6056254/qtitles.zip)



```python
import hanlp
han = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MT5_SMALL) # 'mt5 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD and OntoNotes5 corpus.'

with open('/home1/qtitles','r')as ifile:
    corpus = ifile.read().split('\n')

ps=han(corpus)
```

**Describe the current behavior**
Error & Exit with the following output:

```
Traceback (most recent call last):
  File ""parse2.py"", line 14, in <module>
    ps=han(corpus)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/torch_component.py"", line 629, in __call__
    return super().__call__(data, **merge_dict(self.config, overwrite=True,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 512, in predict
    output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 590, in predict_task
    output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 682, in feed_batch
    'output': task.feed_batch(h,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 182, in feed_batch
    return decoder(h, batch=batch, mask=mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/ner/biaffine_ner/biaffine_ner_model.py"", line 82, in forward
    return self.decode(contextualized_embeddings, gold_starts, gold_ends, gold_labels, mask,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/ner/biaffine_ner/biaffine_ner_model.py"", line 116, in decode
    candidate_ner_scores = candidate_ner_scores[candidate_scores_mask]
IndexError: The shape of the mask [86, 39, 39] at index 1 does not match the shape of the indexed tensor [86, 41, 41, 19] at index 1

```

**Expected behavior**
no output

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.6.1810 (Core) 
- Python version: 3.8.3
- HanLP version: '2.1.0-alpha.25'


**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Get the index of a token or a ner for example in the input text,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
I've been looking into hanlp source code and documentation to find a way to get the index of a token or a ner in the original input text. I was not able to find a solution to this problem (i.e I only get the index of a word in the tokens list).  
Here is an example:  
```
import hanlp
model = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE, devices=0)
ner = model(""My name is John Smith. I am 19 and a student in college."")
print(ner)
```

The output is:  
`{'tok': ['My', 'name', 'is', 'John', 'Smith', '.', 'I', 'am', '19', 'and', 'a', 'student', 'in', 'college', '.'], 'ner': [('John Smith', 'PERSON', 3, 5), ('19', 'DATE', 8, 9)], 'srl': [[('My name', 'ARG1', 0, 2), ('is', 'PRED', 2, 3), ('John Smith', 'ARG2', 3, 5)], [('I', 'ARG1', 6, 7), ('am', 'PRED', 7, 8), ('19 and a student in college', 'ARG2', 8, 14)]], 'sdp/dm': [[], [(1, 'poss'), (3, 'ARG1')], [(1, 'orphan')], [(1, 'orphan')], [(3, 'ARG2'), (4, 'compound')], [(1, 'orphan')], [(8, 'ARG1')], [], [(8, 'ARG2')], [(1, 'orphan')], [(1, 'orphan')], [(9, '_and_c'), (11, 'BV'), (13, 'ARG1')], [(1, 'orphan')], [(13, 'ARG2')], [(1, 'orphan')]], 'sdp/pas': [[], [(1, 'det_ARG1'), (3, 'verb_ARG1')], [(1, 'orphan')], [(1, 'orphan')], [(3, 'verb_ARG2'), (4, 'noun_ARG1')], [(1, 'orphan')], [(8, 'verb_ARG1')], [(6, 'conj_ARG2')], [(10, 'coord_ARG1')], [(8, 'verb_ARG2')], [(1, 'orphan')], [(10, 'coord_ARG2'), (11, 'det_ARG1'), (13, 'prep_ARG1')], [(1, 'orphan')], [(13, 'prep_ARG2')], [(1, 'orphan')]], 'sdp/psd': [[(2, 'APP')], [(3, 'ACT-arg')], [(6, 'CONJ.member')], [(5, 'NE')], [(3, 'PAT-arg')], [], [(8, 'ACT-arg')], [(6, 'CONJ.member'), (10, 'CONJ.member')], [(8, 'PAT-arg')], [(6, 'CONJ.member')], [(6, 'orphan')], [(8, 'PAT-arg'), (10, 'CONJ.member')], [(6, 'orphan')], [(12, 'LOC')], [(6, 'orphan')]], 'con': ['TOP', [['S', [['S', [['NP', [['PRON', ['My']], ['NOUN', ['name']]]], ['VP', [['AUX', ['is']], ['NP', [['PROPN', ['John']], ['PROPN', ['Smith']]]]]]]], ['PUNCT', ['.']], ['S', [['NP', [['PRON', ['I']]]], ['VP', [['AUX', ['am']], ['NP', [['NP', [['NUM', ['19']]]], ['CCONJ', ['and']], ['NP', [['NP', [['DET', ['a']], ['NOUN', ['student']]]], ['PP', [['ADP', ['in']], ['NP', [['NOUN', ['college']]]]]]]]]]]]]], ['PUNCT', ['.']]]]]], 'lem': ['my', 'name', 'be', 'John', 'Smith', '.', 'I', 'be', '19', 'and', 'a', 'student', 'in', 'college', '.'], 'pos': ['PRON', 'NOUN', 'AUX', 'PROPN', 'PROPN', 'PUNCT', 'PRON', 'AUX', 'NUM', 'CCONJ', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT'], 'fea': ['Number=Sing|Person=1|Poss=Yes|PronType=Prs', 'Number=Sing', 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin', 'Number=Sing', 'Number=Sing', '_', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', 'Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin', 'NumType=Card', '_', 'Definite=Ind|PronType=Art', 'Number=Sing', '_', 'Number=Sing', '_'], 'dep': [(2, 'nmod:poss'), (4, 'nsubj'), (5, 'cop'), (0, 'root'), (4, 'flat'), (4, 'punct'), (9, 'nsubj'), (9, 'cop'), (4, 'parataxis'), (12, 'cc'), (12, 'det'), (9, 'conj'), (14, 'case'), (12, 'nmod'), (9, 'punct')]}`

For example instead/in addition of getting `('John Smith', 'PERSON', 3, 5)` is it possible to get `('John Smith', 'PERSON', 11, 21)` where **11** and **21** are the start and end indexes of 'John Smith' in the original input text.
**Will this change the current api? How?**
It is not necessary to change the current API, it is possible to add it as an option.
**Who will benefit with this feature?**
Everyone who uses hanlp
**Are you willing to contribute it (Yes/No):**
No.
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Python version: 3.6
- HanLP version: `hanlp==2.1.0a12`

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
RecursionError: maximum recursion depth exceeded in comparison,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
使用readme.md示例的代码去解析20万行自然语言句子报此错误。每个句子的length限制在40以内。


**Code to reproduce the issue**
数据见 
[sg_phone40.zip](https://github.com/hankcs/HanLP/files/6042006/sg_phone40.zip)


```python
import hanlp
from tqdm import tqdm
han = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库
#p=han(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'])
#print(type(p))#<class 'hanlp_common.document.Document'>'
#print(p)


with open('sg_phone40','r')as ifile:
    corpus = ifile.read().split('\n')     #直接输入corpus，不用下面的k做batching也会报同样的错，但是要跑到大约7万多行的时候才出错，要等很久，不方便debug

err=73072
corpus=corpus[err-2000 : err+2000]

n=len(corpus)
k=1000
ps=[]

batch=[]
for s in tqdm(corpus):
    if len(batch)<k:
        batch.append(s)
    else:
        ptmp=han(batch)
        ps.append(ptmp)
        batch=[]

if len(batch)>0:
    ptmp=han(batch)
    ps.append(ptmp)

import pickle

pickle.dump( ps, open( ""phone_hanlp_pipeline20210225_2.pkl"", ""wb"" ) )

```


**Describe the current behavior**
报错退出
```
(base) [cc@localhost sg_phone]$ python parse2.py 
 50%|██████████████████████████████████████████████▌                                              | 2001/4000 [00:17<00:17, 111.90it/s]
Traceback (most recent call last):
  File ""parse2.py"", line 24, in <module>
    ptmp=han(batch)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/torch_component.py"", line 629, in __call__
    return super().__call__(data, **merge_dict(self.config, overwrite=True,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 512, in predict
    output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 592, in predict_task
    self.decode_output(output_dict, batch, output_key)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 728, in decode_output
    output_per_task['prediction'] = self.tasks[task_name].decode_output(
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/tasks/constituency.py"", line 135, in decode_output
    return CRFConstituencyParser.decode_output(self, out, mask, batch, output.get('span_probs', None),
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/constituency/crf_constituency_parser.py"", line 127, in decode_output
    chart_preds = decoder.decode(s_span, s_label, mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/constituency/crf_constituency_model.py"", line 189, in decode
    span_preds = cky(s_span, mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 313, in cky
    trees = [backtrack(p[i], 0, length) for i, length in enumerate(lens.tolist())]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 313, in <listcomp>
    trees = [backtrack(p[i], 0, length) for i, length in enumerate(lens.tolist())]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  [Previous line repeated 982 more times]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 305, in backtrack
    if j == i + 1:
RecursionError: maximum recursion depth exceeded in comparison
```

**Expected behavior**
正常跑完


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Python version: 3.8.3
- HanLP version: 2.1.0-alpha.23

**Other info / logs**
复现错误的代码不设置`k`做batching，直接跑也会出错的。

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
依存树结构错误，某些head index根本不存在（同issue#1499）,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
复现bug代码，测试数据见https://github.com/hankcs/HanLP/issues/1499#
当时是解决了，我安装了最新的''2.1.0-alpha.23'版本又出现同样的错误：某些head index大于max possible token index

**Code to reproduce the issue**
复现bug代码，测试数据同https://github.com/hankcs/HanLP/issues/1499#

```python
import hanlp

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL16_NEWS_BIAFFINE_ZH)
print(semantic_parser([('蜡烛', 'NN'), ('两', 'CD'), ('头', 'NN'), ('烧', 'VV')]))

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='part_of_speech_tags') \
    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies') \
    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')


with open('hanlp_phones','r')as ifile:
    corpus = ifile.read()

print(corpus[:200])

print('begin pipeline...')
p1=pipeline(corpus)
type(p1)  # hanlp.common.document.Document

print('pipeline done...')

import pickle

pickle.dump( p1, open( ""hanlp_pipeline0702.pkl"", ""wb"" ) )
```

检查head id过大的错误：
```python
print('checking pickle')
import pickle

p2 = pickle.load(open(""hanlp_pipeline0702.pkl"", ""rb""))


def check2(s,n):
    maxv=0
    for w in s:
        vid=w['id']
        maxv=max(maxv,vid)
    for w in s:
        head_id=w['head']
        if head_id>maxv:
            print(f'{n}:{p2[""sentences""][n]}\n')
            print(str(s)+'\n\n')
            break

n=0
for sentence in p2[""syntactic_dependencies""]:
    check2(sentence, n)
    n+=1
```

得到错误输出：
```
20:neken尼凯恩手机en8c怎样开机

1       neken尼凯恩     _       NR      _       _       3       nn      _       _
2       手机    _       NN      _       _       3       nn      _       _
3       en8c    _       NN      _       _       6       assmod  _       _
4       怎样    _       AD      _       _       3       assm    _       _
5       开机    _       VV      _       _       6       nn      _       _


21:neken/尼凯恩 en3三防手机 怎么样

1       neken/尼凯恩    _       NR      _       _       2       nn      _       _
2               _       CD      _       _       8       nsubj   _       _
3       en3     _       NR      _       _       8       advmod  _       _
4       三防    _       JJ      _       _       8       ba      _       _
5       手机    _       NN      _       _       7       assmod  _       _
6               _       CD      _       _       5       assm    _       _
7       怎么样  _       AD      _       _       8       nsubj   _       _


24:gigaset手机死机开不了机?

1       gigaset _       NR      _       _       4       nn      _       _
2       手机    _       NN      _       _       4       nn      _       _
3       死机    _       NN      _       _       4       nn      _       _
4       开      _       VV      _       _       5       top     _       _
5       不      _       AD      _       _       0       root    _       _
6       了      _       VV      _       _       5       dobj    _       _
7       机      _       VV      _       _       9       cop     _       _
8       ?       _       PU      _       _       9       advmod  _       _
```

**Describe the current behavior**
When doing dependency parsing,  some head id > max possible head id. 
**Expected behavior**
When doing dependency parsing,  all head id <= max possible head id. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)  
- Python version: 3.8.3 
- HanLP version: 2.1.0-alpha.23

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
HanLP2.1分词报新的错：Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.

您好，感谢作者及时的回复上一个bug！

但我刚刚重新pip install hanlp -U 之后，并同时尝试本地git pull，再运行同样的代码，会报如下新的错误，说是无法load模型，但我之前已经下载过这个模型到我本地了：

(base) river@riverdeMacBook-Pro mtl % ll
total 0
drwxr-xr-x 3 river staff 96 2 23 01:21 ./
drwxr-xr-x 4 river staff 128 2 22 21:36 ../
drwxr-xr-x 13 river staff 416 2 22 22:10 close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


完整代码如下


```python

import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库
tok = HanLP[‘tok/coarse’]

# 读取自定义词表
beauty_vocab = set()
with open(’…/data/userdict.txt’, ‘r’, encoding=‘utf-8’) as fin:
    for line in fin:
        beauty_vocab.add(line.strip())
tok.dict_combine = beauty_vocab

text = ‘我最心水的那些 红：对口红这种东西真的是毫无抵抗力啊 每次入了新色就在蓝朋友手臂上试 他总会说 女生到底需要有多少口红啊 身为一个直男可能真的不懂 红有一百种 哈哈哈哈 言归正传 阿玛尼红管501 503 504 阿玛尼红管真的是我最爱的唇釉了 入的第一支是504 第一次用就被惊艳到了 竟然有这么丝滑的唇釉 完全是奶油慕斯质地 还呈现很高级的哑光感 真的是超爱 爱不释手 后来又相继收了501和503 以后一定还会继续收 喜爱程度 ysl方管17 13 17是我春夏用的最多次的一支 几乎每天都在用 不知道今天擦什么的时候就用它 准没错 一个夏天过去就下去了一大截 喜爱程度 13是后期入的 相比17利用率就没有那么高 怎么说呢 13还是有些挑皮的 素颜涂肯定是土爆了 喜爱程度 mac子弹头 see sheer也是我的心头爱 刚买的时候每天都放包里 随时补 超级滋润 颜色炒鸡日常 上班涂完全不会显得突兀 超爱 喜爱程度 chill小辣椒 也是超爱的一支 怎么涂都好看 毫不夸张 秋冬不知道涂啥时 擦它总不会出错 喜爱程度 雅诗兰黛love系列 300橘红色 310梅子色 都是我爱的颜色 很显白 喜爱程度 阿玛尼小胖丁504 大名鼎鼎的奶油橘 各种被夸 扛不住风种草的 但是！真的没有那么好 好吗 无论后涂薄涂感觉都不是很满意 我通常是叠加哑光其他口红上面用 喜爱程度 tf16 风老大的番茄色 忍不住入的 但是使用率很低 不知道为啥 可能不够日常 也可能是不舍得用 偷笑R 喜爱程度 香奈儿58 也是跟风入的 据说各种适合秋冬 适合黄皮 毫不犹豫拿下 but有点斯望 并不是很好驾驭啊 而且感觉不显白！ 喜爱程度 ysl黑管402 这只是盲选的 适合春夏 感觉一般 中规中矩 喜爱程度 ysl镜面唇釉09 水红色 颜色不错 但是但是 这个唇釉的质地我真的是爱不起来 不能抿嘴 粘粘的 很不好上均匀 喜爱程度 3ce 南瓜色116 干！巨干！炒鸡干！根本不能用 买之前被颜色吸引 很多人都说它很干 我心想 再干能干成啥样 前一天晚上做唇膜 早晨先用唇部打底再涂 上班不到两小时 嘴就干到起皮 南瓜色口红很多 千万别买这一支 喜爱程度 最后说一句 本人黄皮 唇色浅 干起皮唇纹重 借蓝票的胳膊试色 仅供参考 第一次写这么长的笔记 手指断了 好了就酱 叹气R’

cut_res = HanLP(text, tasks=‘tok/coarse’)
print(cut_res)
```

**Describe the current behavior**
A clear and concise description of what happened.

我刚刚重新pip install hanlp -U 之后，并同时尝试本地git pull，再运行同样的代码，会报如下新的错误，说是无法load模型，但我之前已经下载过这个模型到我本地了：

(base) river@riverdeMacBook-Pro mtl % ll
total 0
drwxr-xr-x 3 river staff 96 2 23 01:21 ./
drwxr-xr-x 4 river staff 128 2 22 21:36 ../
drwxr-xr-x 13 river staff 416 2 22 22:10 close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/



**Expected behavior**
A clear and concise description of what you expected to happen.


1. 还请教原作者，如何解决这个问题。。万分感谢！
2. 另外还有个问题：是否修复之后、[自定义词表]可以用于open模型以及close模型？这两种模型的主要区别是什么？ 因为之前试文本分词的时候有些样本碰到tok/fine会报错，有些碰到tok/coarse会报错。。。open模型只有'tok'这个key，而close模型有'tok/fine'和'tok/coarse'两种，是否自定义词表也都可以用于这2种模式？我的业务场景需要粒度比较细的、语义信息更丰富的(分割成更长的)短语，是不是close模型的'tok/coarse'的效果会最好？

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac OS 10.15.7

- Python version:
python 3.9.2

- HanLP version:
hanlp==2.1.0a20
hanlp-common==0.0.6
hanlp-trie==0.0.2
torch==1.7.1



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

完整报错信息如下：

Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/component_util.py"", line 81, in load_from_meta_file
obj.load(save_dir, verbose=verbose, **kwargs)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py"", line 183, in load
self.to(devices)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py"", line 519, in to
devices = cuda_devices(devices)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/torch_util.py"", line 73, in cuda_devices
query = gpus_available()
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/torch_util.py"", line 46, in gpus_available
nvmlShutdown()
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pynvml/nvml.py"", line 772, in nvmlShutdown
fn = get_func_pointer(""nvmlShutdown"")
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pynvml/nvml.py"", line 387, in get_func_pointer
raise NVMLError(NVML_ERROR_UNINITIALIZED)
pynvml.nvml.NVMLError_Uninitialized: Uninitialized
=================================ERROR LOG ENDS=================================
Please upgrade hanlp with:
pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.

* [x] I've completed this form and searched the web for solutions.
"
HanLP2.1分词时大量文本会报错：RuntimeError: expected scalar type Float but found Long,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
我在用HanLP2.1分词时会遇到大量文本报错：RuntimeError: expected scalar type Float but found Long
很多长文本分词时会报这个错。。
谷歌搜了半天，都说是pytorch里的代码问题，不确定是不是源码的问题，还是我这边的原因导致。。

**Code to reproduce the issue**
完整代码如下

```python
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库
tok = HanLP[‘tok/coarse’]

# 读取自定义词表
beauty_vocab = set()
with open(’…/data/userdict.txt’, ‘r’, encoding=‘utf-8’) as fin:
    for line in fin:
        beauty_vocab.add(line.strip())
tok.dict_combine = beauty_vocab

text = ‘我最心水的那些 红：对口红这种东西真的是毫无抵抗力啊 每次入了新色就在蓝朋友手臂上试 他总会说 女生到底需要有多少口红啊 身为一个直男可能真的不懂 红有一百种 哈哈哈哈 言归正传 阿玛尼红管501 503 504 阿玛尼红管真的是我最爱的唇釉了 入的第一支是504 第一次用就被惊艳到了 竟然有这么丝滑的唇釉 完全是奶油慕斯质地 还呈现很高级的哑光感 真的是超爱 爱不释手 后来又相继收了501和503 以后一定还会继续收 喜爱程度 ysl方管17 13 17是我春夏用的最多次的一支 几乎每天都在用 不知道今天擦什么的时候就用它 准没错 一个夏天过去就下去了一大截 喜爱程度 13是后期入的 相比17利用率就没有那么高 怎么说呢 13还是有些挑皮的 素颜涂肯定是土爆了 喜爱程度 mac子弹头 see sheer也是我的心头爱 刚买的时候每天都放包里 随时补 超级滋润 颜色炒鸡日常 上班涂完全不会显得突兀 超爱 喜爱程度 chill小辣椒 也是超爱的一支 怎么涂都好看 毫不夸张 秋冬不知道涂啥时 擦它总不会出错 喜爱程度 雅诗兰黛love系列 300橘红色 310梅子色 都是我爱的颜色 很显白 喜爱程度 阿玛尼小胖丁504 大名鼎鼎的奶油橘 各种被夸 扛不住风种草的 但是！真的没有那么好 好吗 无论后涂薄涂感觉都不是很满意 我通常是叠加哑光其他口红上面用 喜爱程度 tf16 风老大的番茄色 忍不住入的 但是使用率很低 不知道为啥 可能不够日常 也可能是不舍得用 偷笑R 喜爱程度 香奈儿58 也是跟风入的 据说各种适合秋冬 适合黄皮 毫不犹豫拿下 but有点斯望 并不是很好驾驭啊 而且感觉不显白！ 喜爱程度 ysl黑管402 这只是盲选的 适合春夏 感觉一般 中规中矩 喜爱程度 ysl镜面唇釉09 水红色 颜色不错 但是但是 这个唇釉的质地我真的是爱不起来 不能抿嘴 粘粘的 很不好上均匀 喜爱程度 3ce 南瓜色116 干！巨干！炒鸡干！根本不能用 买之前被颜色吸引 很多人都说它很干 我心想 再干能干成啥样 前一天晚上做唇膜 早晨先用唇部打底再涂 上班不到两小时 嘴就干到起皮 南瓜色口红很多 千万别买这一支 喜爱程度 最后说一句 本人黄皮 唇色浅 干起皮唇纹重 借蓝票的胳膊试色 仅供参考 第一次写这么长的笔记 手指断了 好了就酱 叹气R’

cut_res = HanLP([text])[‘tok/coarse’][0]
print(cut_res)
```

**Describe the current behavior**
代码如上，碰到非常多如上的text文本，分词就会报错

**Expected behavior**
还请教原作者，如何解决这个问题。。万分感谢！

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac OS 10.15.7

- Python version:
python 3.9.2

- HanLP version:
hanlp==2.1.0a20
hanlp-common==0.0.6
hanlp-trie==0.0.2
torch==1.7.1

**Other info / logs**
完整的报错信息如下：

Traceback (most recent call last):
File “/Users/river/Desktop/1-tag-recall/note-tag/optimization/src/preprocess.py”, line 27, in
cut_res = HanLP([text])
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py”, line 768, in call
return super().call(data, batch_size, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py”, line 26, in decorate_context
return func(*args, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py”, line 629, in call
return super().call(data, **merge_dict(self.config, overwrite=True,
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/component.py”, line 36, in call
return self.predict(data, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py”, line 512, in predict
output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py”, line 590, in predict_task
output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py”, line 682, in feed_batch
‘output’: task.feed_batch(h,
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/init.py”, line 182, in feed_batch
return decoder(h, batch=batch, mask=mask)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py”, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py”, line 35, in forward
contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py”, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py”, line 310, in forward
x = layer(x, mask)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py”, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py”, line 264, in forward
x = self.self_attn(x, mask)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py”, line 727, in call_impl
result = self.forward(*input, **kwargs)
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py”, line 150, in forward
D = torch.einsum(‘nd,ld->nl’, self.r_w_bias, pos_embed)[None, :, None] # head x 2max_len, 每个head对位置的bias
File “/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/functional.py”, line 344, in einsum
return _VF.einsum(equation, operands) # type: ignore
RuntimeError: expected scalar type Float but found Long

* [x] I've completed this form and searched the web for solutions.
"
CharTabel 归一化部分字符存在错误,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
1. 有个issue关于调用CharTabel，把“幺”改为“么”不合理portable修复了，但是下载1.7.5 zip包有问题，后发现CharTable.txt.bin md5不一致
2. 以下字符有问题：其中第一列是原始字符，第二列是归一化后字符，括号表示 建议可以考虑括号内字符替换原有归一化内容
猛 勐
蜺 霓
脊 嵴
骼 胳
拾 十
劈 噼
溜 熘
呱 哌
怵 憷
糸 纟（丝）
乾 干
艸 艹（草）
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
public void testCharTable() {
        Map<String, String> normalizationBadCase = new HashMap<>();
        normalizationBadCase.put(""猛"", ""猛"");
        normalizationBadCase.put(""蜺"", ""蜺"");
        normalizationBadCase.put(""脊"", ""脊"");
        normalizationBadCase.put(""骼"", ""骼"");
        normalizationBadCase.put(""拾"", ""拾"");
        normalizationBadCase.put(""劈"", ""劈"");
        normalizationBadCase.put(""溜"", ""溜"");
        normalizationBadCase.put(""呱"", ""呱"");
        normalizationBadCase.put(""怵"", ""怵"");
        normalizationBadCase.put(""糸"", ""丝"");
        normalizationBadCase.put(""乾"", ""乾"");
        normalizationBadCase.put(""艸"", ""草"");
        for (Map.Entry<String, String> entry : normalizationBadCase.entrySet()) {
            assert CharTable.convert(entry.getKey()).equals(entry.getValue());
        }
    }
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version:
- HanLP version: 1.8.0

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Translate documents to Chinese,
自定义词典对KBeamArcEagerDependencyParser无效,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
添加新词进入自定义词典后，KBeamArcEagerDependencyParser仍然无法对该词进行正确分词。

**Code to reproduce the issue**
添加""申请笔 n 100""进入自定义词典
```scala
val parser = new KBeamArcEagerDependencyParser()
parser.parse(""又有新的申请笔可以拿了"")
```
```
1      又      又      A       AD      _       2       advmod  _       _
2       有      有      V       VE      _       0       ROOT    _       _
3       新      新      V       VA      _       6       rcmod   _       _
4       的      的      D       DEC     _       3       cpm     _       _
5       申请    申请    N       NN      _       6       nn      _       _
6       笔      笔      N       NN      _       8       nsubj   _       _
7       可以    可以    V       VV      _       8       mmod    _       _
8       拿      拿      V       VV      _       2       dep     _       _
9       了      了      S       SP      _       2       dep     _       _
```

**Describe the current behavior**
即使添加了新词，仍然无法对新词进行正确分词，而另一个接口HanLP.parseDependency可以根据新加入的词进行正确分词

**Expected behavior**
期待能通过加入自定义词典，KBeamArcEagerDependencyParser能够先正确分词，并在此基础上返回正确结果
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux
- Scala version:2.12.2
- HanLP version:1.7.8

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
CoreStopWordDictionary.dictionary.clear()空指针,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.

调用`CoreStopWordDictionary.reload()`后调用`CoreStopWordDictionary.dictionary.clear()`会空指针。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
CoreStopWordDictionary.reload()
CoreStopWordDictionary.dictionary.clear()
```

**Describe the current behavior**
A clear and concise description of what happened.

执行`CoreStopWordDictionary.dictionary.clear()`时`equivalenceClassMDAGNodeHashMap`空指针异常。

**Expected behavior**
A clear and concise description of what you expected to happen.

正确清空停用词。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Big Sur 11.0.1 20B29 x86_64.
- Python version:非python版本
- HanLP version:1.7.8

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

分析原因为`StopWordDictionary.reload()`时执行了`MDAG.simplify()`，其中有`equivalenceClassMDAGNodeHashMap = null;`

`MDAGSet.clear()`方法中`equivalenceClassMDAGNodeHashMap.clear();`没有判空。

相关：[https://github.com/hankcs/HanLP/commit/a57645c739b403b5fe8ae7cba0fb44d0dd992317](https://github.com/hankcs/HanLP/commit/a57645c739b403b5fe8ae7cba0fb44d0dd992317)

* [x] I've completed this form and searched the web for solutions.
"
RuntimeError: Already borrowed using mtl model in Hanlp2.1.x  with multiple threads,"**Describe the bug**
The multiple task learning model in Hanlp2.1.x can not support multiple threads, a ```RuntimeError``` with ```Already borrowed``` is raised and can reproduce stably. Seem as this is a bug in transformers >3.5.0 with a fast tokenizer.

**Code to reproduce the issue**
```
from multiprocessing.dummy import Pool as ThreadPool
import hanlp

class Hanlp(object):
    def __init__(self):
        self.model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    def get_batch(self, sentences):
        return self.model(sentences)

model = Hanlp()
def tokenizer_test(text):
    print(model.get_batch(text)['tok/fine'])

pool = ThreadPool(10)  
data_list = ['测试这是测试'] * 100
pool.map(tokenizer_test, data_list)
pool.close()
pool.join()
```

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""mul_test.py"", line 17, in <module>
    pool.map(tokenizer_test, data_list)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 290, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 683, in get
    raise self._value
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""mul_test.py"", line 12, in tokenizer_test
    print(model.get_batch(text)['tok/fine'])
  File ""mul_test.py"", line 8, in get_batch
    return self.model(sentences)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 748, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/torch_component.py"", line 631, in __call__
    **kwargs))
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 483, in predict
    for batch in dataloader:
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 429, in __iter__
    for raw_batch in super(PadSequenceDataLoader, self).__iter__():
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 435, in __next__
    data = self._next_data()
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 206, in __getitem__
    sample = self.transform_sample(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 105, in transform_sample
    sample = self.transform(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/transform.py"", line 319, in __call__
    sample = t(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 309, in __call__
    input_tokens, input_ids, subtoken_offsets = tokenize_str(input_tokens, add_special_tokens=True)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 248, in tokenize_str
    add_special_tokens=add_special_tokens).encodings[0]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 2462, in encode_plus
    **kwargs,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 465, in _encode_plus
    **kwargs,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 372, in _batch_encode_plus
    pad_to_multiple_of=pad_to_multiple_of,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 325, in set_truncation_and_padding
    self._tokenizer.no_truncation()
RuntimeError: Already borrowed
```

After search with Google, it seems as this is a bug in transformers >3.5.0 with a fast tokenizer.
https://github.com/huggingface/tokenizers/issues/537
I tried init an instance of the model for each thread as followed, the error is also occured, so confused. Whether there is bug in hanlp?
```
from multiprocessing.dummy import Pool as ThreadPool
import hanlp
from numpy import array

class Hanlp(object):
    def __init__(self):
        self.model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    def get_batch(self, sentences):
        return self.model(sentences)

def tokenizer_test(model, text):
    print(model.get_batch(text)['tok/fine'])

pool = ThreadPool(10)
data_list = ['测试这是测试'] * 100
models = list(array([Hanlp() for i in range(10)]).repeat(10))
print(models)
data = zip(models, data_list)
pool.starmap(tokenizer_test, data)
pool.close()
pool.join()
```
If move model init in single thread as follows，there is no error...but this solution is not effective because each data will init a model...
```
def tokenizer_test(text):
    model = Hanlp()
    print(model.get_batch(text)['tok/fine'])
```

**Expected behavior**
no error

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Python version: 3.7.1
- HanLP version: 2.1.0
- transformer version: 3.5.1

**Other info / logs**
* [x] I've completed this form and searched the web for solutions."
"install hanlp bug,demo run error","

**Describe the bug**
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
dony222:test dony$ python3 testHanLP.py
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_zh_20201222_130611.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp_common/reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp_common/reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 23, in <module>
    from hanlp.components.mtl.tasks import Task
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 22, in <module>
    from hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 9, in <module>
    from hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, PretrainedConfig, AutoTokenizer
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/layers/transformers/pt_imports.py"", line 10, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, \
ImportError: cannot import name 'BertTokenizerFast' from 'transformers' (/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/__init__.py)
=================================ERROR LOG ENDS=================================

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。']).pretty_print()
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.7
- HanLP version:2.1.0a5 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
TableTransform中，调用read_cells时所用的参数名，和函数定义中的参数名不一致,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
read_cells函数：在TableTransform中，调用read_cells时所用的参数名，和函数定义中的参数名不一致

**Code to reproduce the issue**
调用的位置：
class TableTransform
def file_to_inputs
for cells in read_cells(filepath, **skip_header**=self.config.skip_header, delimiter=self.config.delimiter)

read_cells函数在io_utils.py中的定义为：
def read_cells(filepath: str, delimiter='auto', strip=True, **skip_first_line**=False)

**Describe the current behavior**
无法调用read_cells

**Expected behavior**
None

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): not related
- Python version: not related
- HanLP version: 2.1.0a0

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
1.x,更新代码
Fix evaluation and other minor issues to adapt to multi-label classification,
HanLP 1.x 加载 bin 的错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
连续加载两次模型时，第二次没有加载而是直接使用第一次加载的模型

**Code to reproduce the issue**

```java
    public static void main(String[] args) {
        // 第一次执行会报出警告，然后会转换 txt 文件为 bin 文件，后面就不再报错
        // 连续执行两次时，第二次使用的是第一次载入的 bin 文件
        // 交换以下现场代码的顺序就可以看到区别
        show_subtitle(""my_cws_model"");
        trainBigram(MY_CWS_CORPUS_PATH, MY_MODEL_PATH);
        loadBigram(MY_MODEL_PATH);
        show_subtitle(""msr_ngram"");
        trainBigram(MSR_TRAIN_PATH, MSR_MODEL_PATH);
        loadBigram(MSR_MODEL_PATH);
    }
```

详情可以参考：https://github.com/zhuyuanxiang/Hanlp-Books-Examples/blob/main/src/main/java/ch03/sec03/DemoNgramSegment.java

**Describe the current behavior**
--------------->my_cws_model<---------------
「商品」的词频：2
「商品@和」的频次：1
[商品, 和, 服务]
[货币, 和, 服务]
--------------->msr_ngram<---------------
「商品」的词频：2
「商品@和」的频次：1
[商品, 和, 服务]
[货币, 和, 服务]

**Expected behavior**
--------------->my_cws_model<---------------
「商品」的词频：2
「商品@和」的频次：1
[商品, 和, 服务]
[货币, 和, 服务]
--------------->msr_ngram<---------------
「商品」的词频：1
「商品@和」的频次：0
[商品, 和, 服务]
[货币, 和, 服务]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version:
- Java version: 8.0
- HanLP version: 1.7.8

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
added support for multi-label classification,
unable to use mixed precision,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
Unable to use automatic mixed precision

**Code to reproduce the issue**

```
if self.config.use_amp:
     policy = mixed_precision.Policy('mixed_float16')
     mixed_precision.set_policy(policy)
```

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/__main__.py"", line 45, in <module>
    cli.main()
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py"", line 430, in main
    run()
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py"", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str(""__main__""))
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/lei/Documents/Projects/NLP/新华社NLP/分类/hanlp_classification.py"", line 42, in <module>
    history = classifier.fit(TRANIN_SET, DEV_SET, save_dir, **kwargs)
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/components/classifiers/transformer_classifier.py"", line 119, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/common/component.py"", line 341, in fit
    model, optimizer, loss, metrics = self.build(**merge_dict(self.config, logger=logger, training=True))
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/common/component.py"", line 255, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/components/classifiers/transformer_classifier.py"", line 174, in build_model
    model, self.transform.tokenizer = build_transformer(transformer, max_length, len(self.transform.label_vocab),
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/layers/transformers/loader.py"", line 66, in build_transformer
    output = l_bert([l_input_ids, l_token_type_ids], mask=l_mask_ids)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 258, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /usr/local/lib/python3.8/site-packages/bert/model.py:79 call  *
        embedding_output = self.embeddings_layer(inputs, mask=mask, training=training)
    /usr/local/lib/python3.8/site-packages/bert/embeddings.py:226 call  *
        embedding_output += tf.reshape(pos_embeddings, broadcast_shape)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1141 binary_op_wrapper
        raise e
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1125 binary_op_wrapper
        return func(x, y, name=name)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1447 _add_dispatch
        return gen_math_ops.add_v2(x, y, name=name)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:495 add_v2
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /usr/local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:503 _apply_op_helper
        raise TypeError(

    TypeError: Input 'y' of 'AddV2' Op has type float16 that does not match type float32 of argument 'x'.
```

**Expected behavior**
amp mode enabled

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Python version: 3.8
- HanLP version: LATEST

**Other info / logs**


* [x] I've completed this form and searched the web for solutions.
"
Added support for multi-label classification,
tokenizer.predict() 半小时 左右开始报错,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
```python
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
t= tokenizer.predict(content,batch_size=900)
```

**Describe the current behavior**

tokenizer.predict 半个小时左右开始报错

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Python version:3.8.5
- HanLP version:2.0.0a67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```python

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13499' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13500' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13501' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13502' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13503' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13504' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13505' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13506' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13507' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13508' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13509' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13510' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13511' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed





```

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
TypeError: build_callbacks() got multiple values for argument 'logger',"

**Describe the bug**
TypeError: build_callbacks() got multiple values for argument 'logger'

**Code to reproduce the issue**
```
import hanlp
from hanlp.datasets.classification.sentiment import CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_TEST, CHNSENTICORP_ERNIE_VALID

classifier = hanlp.load('LARGE_ALBERT_BASE')

# Train classifer
classifier.fit(CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_VALID, save_dir, transformer='albert_large_zh')

```

**Describe the current behavior**
TypeError: build_callbacks() got multiple values for argument 'logger'

**Expected behavior**
Model got trained

**System information**

**Other info / logs**
In `components.py`L326, it is trying to create a redundant logger, which caused this problem. Please take a look 

* [x] I've completed this form and searched the web for solutions."
module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects',"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.


报错 ： module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
```

**Describe the current behavior**
安装了graphviz 之后 再运行 代码开始报错，卸载掉graphviz 重新安装 tensorflow对应版本依然报错。

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 18.4
- Python version:3.6
- HanLP version:2.0.0a67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
调用了分词的接口，发现我的程序无法写文件了,"
**Describe the bug**
调用了分词的接口，发现我的程序无法写文件了

**Code to reproduce the issue**

token = load('LARGE_ALBERT_BASE')
output = token('商品和服务')
fd = open('output.txt','w')

for item in output:
fd.write(item+'\n')

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version:3.6.8
- HanLP version:2.0.0-alpha.67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
"KBeamArcEagerDependencyParser 加载模型时路径含""data""会导致异常","**Describe the bug**
KBeamArcEagerDependencyParser 加载模型时路径含""data""会导致异常

**Code to reproduce the issue**

```python
PerceptronPOSModelPath='G:/pythonProject/wikidata_django/env/Lib/site-packages/pyhanlp/static/data/model/perceptron/pku1998/pos.bin'
# 调用
parser = KBeamArcEagerDependencyParser()
```

**Describe the current behavior**
载入模型时直接将`PerceptronPOSModelPath`的路径从idata_django开始替换为`G:/pythonProject/wikidata/model/perceptron/ctb/pos.bin`

**Expected behavior**
从`pyhanlp/static/data`开始替换

**System information**
- OS Platform and Distribution: Windows 10 20H2
- Python version: 3.8.6
- HanLP version: 1.7.8

**Other info / logs**
```java
public KBeamArcEagerDependencyParser(String modelPath) throws IOException, ClassNotFoundException
{
    this(new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                                       HanLP.Config.PerceptronPOSModelPath.replaceFirst(""data.*?.bin"", ""data/model/perceptron/ctb/pos.bin"")
    ).enableCustomDictionary(false), new KBeamArcEagerParser(modelPath));
}
```
`com.hankcs.hanlp.dependency.perceptron.parser.KBeamArcEagerDependencyParser` 59行 replaceFirst导致
* [x] I've completed this form and searched the web for solutions."
请求增加主动初始化功能,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
第一次调用分词：NLPTokenizer.segment(word)，耗时5s左右
**Will this change the current api? How?**
no
**Who will benefit with this feature?**
quoter
**Are you willing to contribute it (Yes/No):**
no

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- Python version:
- HanLP version: 1.7.5

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
Fork进程中加载模型失败,"
**Describe the bug**
A clear and concise description of what the bug is.
```bash
[2020-11-07 20:23:18,313: WARNING/ForkPoolWorker-2] Failed to load https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip. See traceback below:
[2020-11-07 20:23:18,313: WARNING/ForkPoolWorker-2] ================================ERROR LOG BEGINS================================
[2020-11-07 20:23:18,316: WARNING/ForkPoolWorker-2] Traceback (most recent call last):
[2020-11-07 20:23:18,316: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 49, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 75, in build_transformer
    with stdout_redirected(to=os.devnull):
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 559, in stdout_redirected
    stdout_fd = fileno(stdout)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 540, in fileno
    raise ValueError(""Expected a file (`.fileno()`) or a file descriptor"")
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] ValueError: Expected a file (`.fileno()`) or a file descriptor
[2020-11-07 20:23:18,318: WARNING/ForkPoolWorker-2] =================================ERROR LOG ENDS=================================
[2020-11-07 20:23:18,811: WARNING/ForkPoolWorker-2] If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
def attr_extract(txt):
    map_path = os.path.join(os.path.dirname(__file__), ""map.txt"")
    regulation = load_map(map_path)
    sentenceStr = txt
    tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
    wordList = tokenizer(sentenceStr)
    tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN)
    labelList = tagger(wordList)

```

**Describe the current behavior**
A clear and concise description of what happened.
```
celery后台任务处理分词时加载模型失败
```
**Expected behavior**
A clear and concise description of what you expected to happen.
```
期望正常运行
```
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS
- Python version: Python3.6.12
- HanLP version: hanlp2.0.0a66

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
感知机词性标注出现java.lang.NullPointerException异常,"**Describe the bug**
按照https://github.com/hankcs/HanLP/wiki/%E7%BB%93%E6%9E%84%E5%8C%96%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A0%87%E6%B3%A8%E6%A1%86%E6%9E%B6
训练，第二步词性标注，出现空指针异常

**Code to reproduce the issue**

```java
String posModelFile = ""D:\\Develop\\hanlp\\icwb2-data\\training\\model\\perceptron\\msr_training.utf8\\pos.bin"";
        PerceptronTrainer trainer = new POSTrainer();
        trainer.train(
                ""D:\\Develop\\hanlp\\icwb2-data\\training\\msr_training.utf8"",
                posModelFile
        );
```

**Describe the current behavior**
训练的预料是：msr_training.utf8，感知机词性标注训练出现空指针异常

**Expected behavior**
正常运行

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version:
- HanLP version: portable-1.7.5

**Other info / logs**
`
开始加载训练集...

java.lang.NullPointerException
	at java.util.TreeMap.getEntry(TreeMap.java:347)
	at java.util.TreeMap.get(TreeMap.java:278)
	at com.hankcs.hanlp.model.perceptron.tagset.TagSet.add(TagSet.java:44)
	at com.hankcs.hanlp.model.perceptron.instance.POSInstance.<init>(POSInstance.java:43)
	at com.hankcs.hanlp.model.perceptron.instance.POSInstance.create(POSInstance.java:262)
	at com.hankcs.hanlp.model.perceptron.POSTrainer.createInstance(POSTrainer.java:36)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer$1.process(PerceptronTrainer.java:288)
	at com.hankcs.hanlp.model.perceptron.utility.IOUtility.loadInstance(IOUtility.java:81)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.loadTrainInstances(PerceptronTrainer.java:282)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.train(PerceptronTrainer.java:122)
	at com.hankcs.hanlp.model.perceptron.POSTrainer.train(POSTrainer.java:43)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.train(PerceptronTrainer.java:320)
`

* [x] I've completed this form and searched the web for solutions.
"
麻烦给CRFSegmenter添加流的读取方式，方便直接从jar包里直接读取bin文件,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
1.x,增加流的读取方式
spark中使用,"<!--
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！
提问请上论坛，不要发这里！

以下必填，否则恕不受理。
-->

**Describe the bug**
spark中只能本地使用hanlp的依存句法功能，在udf或分区中无法使用。

**Code to reproduce the issue**
报错信息：`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`

**Describe the current behavior**
我试着在spark中加载portable包’hanlp-portable-1.7.8.jar’，并且将data放到hdfs上，配置了hanlp.properties和ioadapter。
本地进行依存句法是可以执行的。但是运用udf或分布式执行就会报错：`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`

**Expected behavior**
希望不要报错，udf顺利执行

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Spark version: 2.4.5
- HanLP version:1.7.8

* [x] I've completed this form and searched the web for solutions.
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->
<!-- 发表前先搜索，此处一定要勾选！ -->"
加载模型hanlp.pretrained.cws.LARGE_ALBERT_BASE时出错,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
加载cws模型albert时出错
```bash
Downloading https://file.hankcs.com/hanlp/embeddings/albert_base_zh.tar.gz to /home/yihuazhou/.hanlp/embeddings/albert_base_zh.tar.gz

97.69%, 37.0 MB/37.9 MB, 92 KB/s, ETA 10 s      Failed to load https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 201, in download
    urlretrieve(url, tmp_path, reporthook)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/urllib/request.py"", line 286, in urlretrieve
    raise ContentTooShortError(
urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 38807364 out of 39731739 bytes>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 254, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 34, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/layers/transformers/loader.py"", line 39, in build_transformer
    bert_dir = get_resource(model_url)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 340, in get_resource
    path = download(url=path, save_path=realpath)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 214, in download
    installed_version, latest_version, latest_version_str = check_outdated()
ValueError: not enough values to unpack (expected 3, got 2)
=================================ERROR LOG ENDS=================================
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
```

**Describe the current behavior**
A clear and concise description of what happened.
目前下载完albert模型后加载出错

**Expected behavior**
A clear and concise description of what you expected to happen.
希望能正常加载ALbert模型

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Python version: 3.8.3
- HanLP version: '2.0.0-alpha.61'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Which word segmentation is faster in python or java  when using spark,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
I want to try HanLP in spark (in scala, i can directly use HanLP Java version 1.x) or pyspark(HanLP 2.x). The datasets are billion level, so i want to know which one is better
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
No
**Are you willing to contribute it (Yes/No):**
No
**System information**
- OS Platform and Distribution :Linux Ubuntu 16.04
- Python version: 3.7
- HanLP version: 2.x in Python, 1.x in Java
- Scala version: 2.11.x
- Java version: 1.8
**Any other info**
thank you for your support, i tried to find the post button in its bbs but couldn't find it. BTW, hava a happy national day!

* [x] I've carefully completed this form.
"
Cannot process Supplementary Character in Java / 无法在Java中处理Supplementary字符,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.

When handling supplementary character, HanLP(I tested with pinyin and word segmentation) couldn't handle the supplementary character properly. For short. Java represent a unicode character > 0xFFFF as two sepeprate char, thus HanLP treat them as two seperate Chinese character when getting Pinyin on it. However, those chars not assigned to any validate charset, so the pinyin result would be two 'none', rather than one 'none'.

Word segmentation cannot recognize it, but would always keep them as a word.

处理[Supplementary字符](https://www.oracle.com/technical-resources/articles/javase/supplementary.html)时，HanLP（我测试了拼音标注和分词）似乎没法恰当的处理Supplementary字符。简单地说，Java将0xFFFF以上的Unicode字符表示为两个char，因此HanLP在标注拼音的时候会将其视为两个独立的汉字。然而这些char的值特意的没有指定给任意有效的字符集，因此拼音标注的结果是两个'none'，而不是一个'none'

分词也并不能识别这种字符，但是分词总会确保这些字符是一个词，结果中不会产生破碎的char。

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```kotlin
println(HanLP.convertToPinyinList(""鼖"").map { it.pinyinWithToneMark })
```

**Describe the current behavior**
A clear and concise description of what happened.

Get: `[none, none]`
`鼖` would be represent as `\uD87E\uDE1B` in Java, none of them is validate Chinese character. So got 2 `none`.
`鼖`在Java中被表示为 `\uD87E\uDE1B`，每一个单独的char都不是有效的中文字符，因此得到了两个`none`。

**Expected behavior**
A clear and concise description of what you expected to happen.

Should get: `[fén]`, or at least a `[none]`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004
- Python version: Not available
- HanLP version: 1.7.8
- Java Version: Java 11

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
词性标注载入模型时发生错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
载入词性标注模型ctb5_pos_rnn_fasttext_20191230_202639.zip 时发生错误，提示版本不匹配。
前几个月使用一直正常，之前的hanlp版本是2.0.0a44。今天使用的时候突然报错，提示更新后再试。
但是更新到2.0.0a60后依然报错。

**Code to reproduce the issue**
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)

**Describe the current behavior**
载入时报错：MemoryError: bad allocation，
提示：https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running a lower version: 2.0.0-alpha.60. Please upgrade hanlp with:
pip install --upgrade hanlp
upgrade之后仍然存在该错误，已是最新版本。

**Expected behavior**
载入模型成功，不报错

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 专业版 18363.1016
- Python version:3.6.5
- HanLP version:2.0.0a60

**Other info / logs**
报错信息：
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\components\taggers\rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\layers\embeddings\__init__.py"", line 34, in build_embedding
    custom_objects=tf.keras.utils.get_custom_objects())
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 360, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 697, in from_config
    return cls(**config)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\layers\embeddings\fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\fasttext\FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\fasttext\FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
MemoryError: bad allocation
=================================ERROR LOG ENDS=================================
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running a lower version: 2.0.0-alpha.60. 
Please upgrade hanlp with:
pip install --upgrade hanlp
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
When reporting an issue, make sure to paste the FULL ERROR LOG above.

* [x] I've completed this form and searched the web for solutions.
"
Translate documents to Chinese,
"HanLP 2.0词性标注集（中英文）何时可以公布文档？想知道DEG, PU, CD等标注分别对应什么词性","<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
求助，HanLP 2.0词性标注集（中英文）哪里可以找到？
2.0的词性标注都换成了大写字母: CC, VV, DEG, PU, CD等等，我想知道每一个对应什么词性

找了一大圈没有找到2.0对应的标注集。。只搜到了1.x的：http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8

万分感谢！

**Will this change the current api? How?**
不会影响api，但是为了便于使用的话，还是很需要这个标注集。。

**Who will benefit with this feature?**
所有使用方

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.7
- HanLP version:2.0

**Any other info**
感谢作者的共享！
* [x] I've carefully completed this form.
"
Albert_tokenination.py 缺失一个import ,缺失 import sentencepiece as spm
"在运行""HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py""代码时报错。","<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
A clear and concise description of what the bug is.
在运行""HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py""代码时报错。
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
运行这个HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py脚本。
```python
```

**Describe the current behavior**
A clear and concise description of what happened.
报错信息TypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'
**Expected behavior**
A clear and concise description of what you expected to happen.
正常训练
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:python3.7
- HanLP version:HanLP 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:/Users/Administrator/Desktop/HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py"", line 14, in <module>
    metrics='f1')
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\ner.py"", line 83, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\taggers\transformers\transformer_tagger.py"", line 57, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\common\component.py"", line 352, in fit
    metrics=metrics, overwrite=True))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\taggers\transformers\transformer_tagger.py"", line 93, in train_loop
    validation_steps=dev_steps,
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 806, in train_function
    return step_function(self, iterator)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 796, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1211, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 2585, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 2945, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\autograph\impl\api.py"", line 275, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 789, in run_step
    outputs = model.train_step(data)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 757, in train_step
    self.trainable_variables)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 2745, in _minimize
    experimental_aggregate_gradients=False)
TypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'
* [x] I've completed this form and searched the web for solutions.
"
hanlp需要增加hdfs路径读取功能,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
hanlp需要增加hdfs路径读取功能
**Will this change the current api? How?**
 增加hdfs IOAdapter 
**Who will benefit with this feature?**
增强功能的适用性，不然每次修改词典，还需要重新打包；在公司大数据环境下里是不合适的
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   linux 、 Distribution
- Python version: none
- HanLP version:  portable1.7.8

**Any other info**
hanlp java版本，需要增加hdfs 的操作。如果只是把字典，或者模型放在 jar里太大了。
希望可以授予分支权限，我已经写完代码了。让我提交下分支吧。谢谢。🙏
* [x] I've carefully completed this form.
"
请求添加判断字符是否为汉字的 API,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
之前在用 TinyPinyin 库进行拼音转换，但是发现了该库的功能更为完善强大，遂使用该库重构之前的代码，但是其中使用了判断字符是否为汉字的 API，而在该库中没有找到类似的公开 API，不能完全消除对 TinyPinyin 库的依赖，所以想请作者添加类似 `HanLP.isHan('汉')` 的 API。

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Everyone. 该库专注于汉语领域相关的功能，所以我觉得此功能的受众也挺广的。

**Are you willing to contribute it (Yes/No):**
No.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows [版本 10.0.17763.1039]
- Python version: None
- HanLP version: Java com.hankcs:hanlp:portable-1.7.8

**Any other info**
TinyPinyin 库判断字符是否为汉字的 API `Pinyin.isChinese('汉')`: https://github.com/promeG/TinyPinyin/blob/master/lib/src/main/java/com/github/promeg/pinyinhelper/Pinyin.java#L146

* [x] I've carefully completed this form.
"
Update setup.py,Update sentencepiece version to 0.1.86
URLTokenizer 不能识别中文顶级域名,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则恕不受理。
-->

**Describe the bug**
运行测试 Demo https://github.com/hankcs/HanLP/blob/1.x/src/test/java/com/hankcs/demo/DemoURLRecognition.java ，发现不能识别出带中文顶级域名的网址。

**Code to reproduce the issue**

```python
        String text =
                ""HanLP的项目地址是https://github.com/hankcs/HanLP，"" +
                        ""发布地址是https://github.com/hankcs/HanLP/releases，"" +
                        ""我有时候会在www.hankcs.com上面发布一些消息，"" +
                        ""我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。"" +
                        ""听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……"";
        List<Term> termList = URLTokenizer.segment(text);
        System.out.println(termList);
        for (Term term : termList)
        {
            if (term.nature == Nature.xu)
                System.out.println(term.word);
        }
```
输出：
```
[HanLP/nx, 的/uj, 项目/n, 地址/n, 是/v, https://github.com/hankcs/HanLP/xu, ，/w, 发布/v, 地址/n, 是/v, https://github.com/hankcs/HanLP/releases/xu, ，/w, 我/r, 有时候/d, 会/v, 在/p, www/nx, ./w, hankcs/nrf, ./w, com/nx, 上面/f, 发布/v, 一些/m, 消息/n, ，/w, 我/r, 的/uj, 微博/n, 是/v, http://weibo.com/hankcs/xu, //w, ，/w, 会/v, 同步/vd, 推送/nz, hankcs/nrf, ./w, com/nx, 的/uj, 新闻/n, 。/w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ul, ,/w, 但/c, 我/r, 并/c, 没有/v, 申请/v, hankcs/nrf, ./w, 中国/ns, ,/w, 因为/c, 穷/a, ……/w]
https://github.com/hankcs/HanLP
https://github.com/hankcs/HanLP/releases
http://weibo.com/hankcs
```

**Describe the current behavior**
不能识别出带中文顶级域名的网址。

**Expected behavior**
能识别出带中文顶级域名的网址。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Update setup.py,Update params-flow==0.8.2
模型优化问题,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
我想问一下是否可以考虑使用tensorflow-lite或者openvino作为推理后端，这样的话可以避免依赖整个tensorflow或pytorch库，也能提高加载速度，对于gpu资源不是很丰富的用户也更加友好，能够提升桌面cpu端的推理速度。
如果使用transformer的话，是否可以使用模型蒸馏减少堆叠层数（例如, DistilBert），同样能提升推理速度

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
everyone who use this

**Are you willing to contribute it (Yes/No):**
可以做，但是可能没有足够的资源做

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version: v2.0.0-alpha.0      

**Any other info**
no

* [x] I've carefully completed this form.
"
ImportError: cannot import name 'BertModelLayer',"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
hanlp2.0安装使用错误
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import hanlp
```python
```

**Describe the current behavior**
A clear and concise description of what happened.
ImportError: cannot import name 'BertModelLayer'
**Expected behavior**
A clear and concise description of what you expected to happen.
正常导入
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version:3.6.9
- HanLP version:hanlp-2.0.0a46

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/__init__.py"", line 17, in <module>
    import hanlp.components
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/__init__.py"", line 5, in <module>
    from . import tok
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/tok.py"", line 12, in <module>
    from hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 13, in <module>
    from hanlp.layers.transformers.loader import build_transformer
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 9, in <module>
    from bert import BertModelLayer, albert_models_tfhub, fetch_tfhub_albert_model
* [x] I've completed this form and searched the web for solutions.
yes"
HMM-FirstOrderHiddenMarkovModelTest文件bug错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
在我跟踪使用hmm对玩具医疗模型分析时，得出prob并非0.015，分析问题发现bug如下
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
 for(int cur_s = 0; cur_s < max_s; ++cur_s) {
            score[cur_s] = this.start_probability[cur_s] + this.emission_probability[cur_s][observation[0]];
        }
如这里最后observation[0]在t=2时刻时是体寒状态，observation[0]值为2 ， 然而.emission_probability[cur_s][2]中却是第三个状态的值，问题原因在于0下标  导致2下标指向了第三个状态值 。

/**
     * 显状态
     */
    enum Feel
    {
        normal,
        dizzy,
        cold,
    }
我在Feel声明时调换了 dizzy和cold顺序 使其与emission_probability状态矩阵对应，得出prob概率值为0.015(之前概率结果为0.0097199995）
```

**Describe the current behavior**
A clear and concise description of what happened.
一个小小的BUG
**Expected behavior**
A clear and concise description of what you expected to happen.
提交
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows
- Python version:3.6
- HanLP version:1.78

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
testPredict
* [x] I've completed this form and searched the web for solutions.
"
手动安装的pyhanlp无tests.test_utility模组,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
自动安装失败，使用手动安装，from pyhanlp import *可以成功调用hanlp
**Will this change the current api? How?**
但是后续学习遇到了这个代码——from tests.test_utility import ensure_data
**Who will benefit with this feature?**
提示不存在这个模组，请问如何解决？
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
输入一个词语，要求返回与这个词相关的词语。除了词向量聚类还有别的方法吗,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
求一种能够处理标题所说任务的方法，只需要思路就可以。
**Will this change the current api? How?**
如果我可以完成这个功能，会开源代码并贡献给这个项目
**Who will benefit with this feature?**
We
**Are you willing to contribute it (Yes/No):**
Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
hanlp2.0源码小错误,"最近看hanlp2.0的源码，发现一个不影响hanlp功能的小手误（可能）。
代码位置：hanlp/common/transform中264行。
函数名称为input_to_inputs。
小错误为该函数的返回值类型Tuple[bool, Any]
结合该函数实际返回值类型与其他地方调用该函数返回值使用情况，返回值类型应该为Tuple[Any, bool]。
不影响使用，就是编辑器pycharm会报warning。"
分词之后词和原文不一致，分词之后有的汉字和原文中不一致,"**Describe the bug**

分词之后词和原文不一致，分词之后有的汉字和原文中不一致示例源码如下。

**Code to reproduce the issue**

```java
String paragraph =""客观来讲，这一价格与福特猛禽相比有着不小优势，但当疫情刚刚过去，国人会花20万元买一辆皮卡吗？"";
Segment  segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableIndexMode(true).enableMultithreading(true);
  List<Term> termList = segment.seg(paragraph);
```

**Describe the current behavior**

 分词之后termList 中 ‘“猛禽”  变成了  “勐禽”

**Expected behavior**

希望分词之后和原句中的词保持一致
即termList 依然是 ‘“猛禽” 两个字

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7
- java version: 1.8 
- HanLP version: 1.7.3

**Other info / logs**
最后的分词结果，在下面的截图中，可以看到。
![分词](https://user-images.githubusercontent.com/30866940/87658023-35326500-c78e-11ea-9379-f84a10beaab2.png)

* [x] I've completed this form and searched the web for solutions.
"
添加文本标点,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
    在一段无标点文本中，添加标点断句
**Will this change the current api? How?**
  不会
**Who will benefit with this feature?**
    可用于语音识别所生成的文本，将无标点的文本断句
**Are you willing to contribute it (Yes/No):**
   Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
  No
* [x] I've carefully completed this form.
   
"
依存树结构错误，某些head index根本不存在,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
依存树结构错误，某些head index根本不存在

**Code to reproduce the issue**

```python
>>> syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
>>> print(syntactic_parser([(尼凯恩, 'NR'), ('手机', 'NN'), ('怎么样', 'AD'), ('？', 'PU')])) #这是hanlp官方demo pipeline输出的分词结果
1       尼凯恩  _       NR      _       _       2       nn      _       _
2       手机    _       NN      _       _       4       assmod  _       _
3       怎么样  _       AD      _       _       2       assm    _       _
4       ？      _       PU      _       _       7       nsubj   _       _
```

**Describe the current behavior**
上述样例中，‘？’的head节点7完全不存在

**Expected behavior**
head index <=  mac vertex index


**System information**
- OS Platform and Distribution :centos 7
- Python version: 3.7.6
- HanLP version: hanlp-2.0.0a44

**Other info / logs**
好多都有结构错误，详见  [bug_syn.txt](https://github.com/hankcs/HanLP/files/4857936/bug_syn.txt)
```
尼凯恩手机怎么样？
1       尼凯恩  _       NR      _       _       2       nn      _       _
2       手机    _       NN      _       _       4       assmod  _       _
3       怎么样  _       AD      _       _       2       assm    _       _
4       ？      _       PU      _       _       7       nsubj   _       _

尼凯恩老人机手机好用吗
1       尼凯恩  _       NR      _       _       3       nn      _       _
2       老人    _       NN      _       _       3       nummod  _       _
3       机手机  _       NN      _       _       5       nn      _       _
4       好      _       AD      _       _       5       nn      _       _
5       用      _       VV      _       _       7       nsubj   _       _
6       吗      _       SP      _       _       7       mmod    _       _

天语手机到底怎么样？
1       天语    _       NN      _       _       4       dep     _       _
2       手机    _       NN      _       _       4       punct   _       _
3       到底    _       AD      _       _       4       nummod  _       _
4       怎么样  _       AD      _       _       6       nsubj   _       _
5       ？      _       PU      _       _       6       nsubj   _       _

天语智能手机怎么样?
1       天语    _       NN      _       _       2       nn      _       _
2       智能    _       NN      _       _       6       nn      _       _
3       手机    _       NN      _       _       6       nn      _       _
4       怎么样  _       AD      _       _       5       nn      _       _
5       ?       _       PU      _       _       6       nn      _       _


```


* [x] I've completed this form and searched the web for solutions.
"
fix errors for combineNER,fix errors when a compound word consists of two words and appears at the end of a sentence
jvm.dll文件在对应路径下存在但startJvm找不到,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**

pyhanlp和JPype1都安装好了对应的环境，并且JAVA环境也配置好了，cmd运行java也是ok的。
但是运行代码时，jvm.dll文件在对应路径下存在但startJvm找不到

**Code to reproduce the issue**

import pyhanlp
# 机构名识别,标注为nt
def organization_recognize(sentence):
    segment = HanLP.newSegment().enableOrganizationRecognize(True)
    return segment.seg(sentence)
print(organization_recognize('dawdaw'))

```python
```

**Describe the current behavior**
程宇运行时报错无法找到jvm.dll文件，但该文件在对应路径下存在

**Expected behavior**

正常执行

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- Python version: python3.8
- HanLP version: jpype1-0.7.0 pyhanlp-0.1.66

**Other info / logs**
具体信息如图：

![1](https://user-images.githubusercontent.com/65661595/85613616-a51c6680-b68c-11ea-8a74-b6f92f0f42c5.png)
![2](https://user-images.githubusercontent.com/65661595/85613620-a64d9380-b68c-11ea-9561-2402145f5bde.png)
![3](https://user-images.githubusercontent.com/65661595/85613630-a8afed80-b68c-11ea-8a35-91441ead366e.png)



* [ ] I've completed this form and searched the web for solutions.
并未找到解决方案"
Java Memory Leak NLP分词加载模型占用的内存不释放,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
Java项目启动后加载执行如下代码：
NLPTokenizer.segment(""当年末个人存款余额"")
代码后，根据监控工具提示加载模型占用500多M的内存，好像长时间不释放，CMS垃圾回收器一直在回收但是回收不掉，有什么方法可以释放内存吗，使用MAT工具提示内存可能泄漏，或者减少模型占用的内存大小

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
Class Name                                                                                           | Shallow Heap | Retained Heap | Percentage
-------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                                     |              |               |           
org.apache.catalina.loader.WebappClassLoader @ 0x769990000                                           |          208 |   546,208,656 |     72.14%
|- class com.hankcs.hanlp.tokenizer.NLPTokenizer @ 0x76acbe470                                       |            8 |   489,747,520 |     64.68%
|  '- com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer @ 0x76a791710                      |           32 |   489,747,512 |     64.68%
|     |- com.hankcs.hanlp.model.perceptron.PerceptronSegmenter @ 0x76fed02f8                         |           24 |   278,039,544 |     36.72%
|     |  |- com.hankcs.hanlp.model.perceptron.model.StructuredPerceptron @ 0x76ff1bec8               |           24 |   278,038,960 |     36.72%
|     |  |  |- com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap @ 0x76c1575d8       |           24 |   149,699,096 |     19.77%
|     |  |  |  '- com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger @ 0x7702d8b30|           32 |   149,699,072 |     19.77%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.IntArrayList @ 0x76c7c8ce8              |           40 |    74,849,512 |      9.89%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.IntArrayList @ 0x76c7c8d10              |           40 |    74,849,512 |      9.89%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.Utf8CharacterMapping @ 0x77031ead8      |           16 |            16 |      0.00%
|     |  |  |     '- Total: 3 entries                                                                |              |               |           
|     |  |  |- float[32084956] @ 0x7770ecf40                                                         |  128,339,840 |   128,339,840 |     16.95%
|     |  |  '- Total: 2 entries                                                                      |              |               |           
|     |  |- com.hankcs.hanlp.model.perceptron.tagset.CWSTagSet @ 0x770326b38                         |           48 |           560 |      0.00%
|     |  '- Total: 2 entries                                                                         |              |               |           
|     |- com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger @ 0x76ff1bee0                         |           16 |   164,834,720 |     21.77%
|     |- com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer @ 0x76ff1e5f8                      |           24 |    46,873,184 |      6.19%
|     |- com.hankcs.hanlp.seg.Config @ 0x76b798570                                                   |           32 |            32 |      0.00%
|     '- Total: 4 entries                                                                            |              |               |           
-------------------------------------------------------------------------------------------------------------------------------------------------

```

**Describe the current behavior**

发现不太释放内存，内存占用过多，是不是使用默认的模型库文件太大，加载了过多的缓存

**Expected behavior**

期待可以定期释放或有办法处理降低占用内存大小，或者内置的模型库如何减小

**System information**
- Mac OS10.14
- Java1.8
- HanLP1.7.5

**Other info / logs**
暂无

* [x] I've completed this form and searched the web for solutions."
java portable-1.7.7 data/dictionary/place缺少文件ns.txt.bin,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
java portable-1.7.7版本， data/dictionary/place缺少文件ns.txt.bin

**Code to reproduce the issue**
1. 引入maven依赖
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.7.7</version>
</dependency>

2. 运行“地址识别”官方demo
```java
String[] testCase = new String[]{
        ""武胜县新学乡政府大楼门前锣鼓喧天"",
        ""蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机"",
};
Segment segment = HanLP.newSegment().enablePlaceRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}
```


**Describe the current behavior**
报错，提示data/dictionary/place缺少文件ns.txt.bin。
查看依赖包中，缺少此文件


**Expected behavior**
正常运行

**System information**
- Ubuntu 16
- Java version: 11
- HanLP version: portable-1.7.7

* [ ] I've completed this form and searched the web for solutions.
"
segment.seg wrong,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
当我使用以下这段代码时：
segment = HanLP.newSegment().enableNameRecognize(True)
result = segment.seg(""某一段话"")

此时我在我的Ubuntu系统下并不会报错，并会返回结果
但当我在Windows系统下却报错：
TypeError: Ambiguous overloads found for com.hankcs.hanlp.seg.Segment.seg(str) between:
	public java.util.List com.hankcs.hanlp.seg.Segment.seg(char[])
	public java.util.List com.hankcs.hanlp.seg.Segment.seg(java.lang.String)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
Ubuntu下正常返回结果，Windows下没有

**Expected behavior**
应该正常返回结果

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04 and Windows10
- Python version: Python3.8
- HanLP version:  HanLp1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [ ] I've completed this form and searched the web for solutions.
"
自定义IOAdapter时com.hankcs.hanlp.dictionary.other.CharType 存在BUG,"

**Describe the bug**
A clear and concise description of what the bug is.
在使用hanlp 作为 阿里的elasicsearch云分词插件时，阿里云限制了插件写文件权限，只能远程访问hanlp字典文件，因此自定义 IOAdapter，但CharType 类保存 data/dictionary/other/CharType.bin 文件时，未使用IOUtil类，直接使用了FileOutputStream 类保存，导致无法写入文件异常
**Code to reproduce the issue**
`CharType.java`
```java
 DataOutputStream out = new DataOutputStream(new FileOutputStream(HanLP.Config.CharTypePath));
```

"
www.hanlp.com登不上去了？有人能看一下吗？,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [ ] I've completed this form and searched the web for solutions.
"
下载Data失败,"---
name: 🐛Bug report
about: Create a report to help us improve
title: 'Import时不能下载data'
labels: bug
assignees: hankcs

---
**Describe the bug**
当我使用jupyter notebook运行import pyhanlp会下载data-1.7.zip，一直会出现timeout，而当我手工下载data-1.7.zip并将复制到/opt/anaconda3/lib/python3.7/site-packages/pyhanlp/static/，并将其重命名为data-1.7.7.zip时，下次运行import pyhanlp还是会自动下载该文件。

而当我使用IPython命令行运行Import pyhanlp时，又会自动下载data-1.7.5.zip文件，虽然文件夹也已经有该文件（我手工下载后放入）。请问是何回事呢？

**Code to reproduce the issue**

```import pyhanlp
```

**Describe the current behavior**
不能完成下载pyhanlp所需的data文件，也无法识别手工下载的data文件。

**Expected behavior**
完成下载

**System information**
- MacOS 10.15
- Anaconda python3.7
- HanLP version:latest

**Other info / logs**
screenshot included

* [ ] I've completed this form and searched the web for solutions.


![hanlp](https://user-images.githubusercontent.com/62363087/82623297-6705d000-9c12-11ea-90d6-515ecd1a012b.png)
![hanlp2](https://user-images.githubusercontent.com/62363087/82623310-708f3800-9c12-11ea-87af-c22cc5546346.png)

"
能否增加多语言支持？比如韩文 日文 中文,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the feature and the current behavior/state.**
支持多语言分词,或分词多语言化
**Will this change the current api? How?**
可能需要更新
**Who will benefit with this feature?**
有国际业务的或跨国,想对于日新月异的中国 很是需要
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
官网被阻断了,官网被阻断了，去备案修复下吧。
semantic parser结果无法使用pickle进行持久化,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
鉴于深度学习模型速度问题以及已知的tf2内存泄露问题， 一个解决办法是使用离线任务处理数据， 然后持久化数据后再接入下游任务。 然而， 在测试持久化过程中， 发现似乎由于`SerializableDict`类没有implement `__getstate__` method, 导致semantic analysis 结果无法被pickle持久化

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ""-1""
import hanlp
import pickle
import traceback
tokenizer = hanlp.load(hanlp.pretrained.cws.PKU_NAME_MERGED_SIX_MONTHS_CONVSEG)
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL16_NEWS_BIAFFINE_ZH)
text = """"""HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。
HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。""""""

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='part_of_speech_tags') \
    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')
res = pipeline(text)

x = res['semantic_dependencies'][0]

print(""direct dumping result...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(res, f)
except Exception as e:
    traceback.print_exc()

print(""dumping dict(result) with semantic...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(dict(res), f)
    print(""dump succeeded"")
except Exception as e:
    traceback.print_exc()

print(""dumping semantic result...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(x[0], f)
except Exception as e:
    traceback.print_exc()

print(""direct dumping result without semantic...."")
del res['semantic_dependencies']
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(res, f)
except Exception as e:
    traceback.print_exc()

print(""dumping dict(result) without semantic...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(dict(res), f)
    print(""dump succeeded"")
except Exception as e:
    traceback.print_exc()
```

**Describe the current behavior**
5个实验：
- 直接pickle, 失败
- 给pipeline result 前面封装一层dict， 失败
- 只pickle semantic parser result， 失败
- 删掉result的semantic parser部分， 直接dump， 失败
- 给删掉semantic parser部分的result封装一层dict， 成功


**Expected behavior**
给result 封装一层dict后应该都可以用pickle进行序列化并dump?


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7
- HanLP version: 2.0.0-alpha.43


**Other info / logs**
```
direct dumping result....
Traceback (most recent call last):
  File ""<input>"", line 6, in <module>
TypeError: 'list' object is not callable
dumping dict(result) with semantic....
Traceback (most recent call last):
  File ""<input>"", line 13, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf2_gpu\lib\site-packages\hanlp\common\structure.py"", line 92, in __getattr__
    return self.__getitem__(key)
KeyError: '__getstate__'
dumping semantic result....
Traceback (most recent call last):
  File ""<input>"", line 21, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf2_gpu\lib\site-packages\hanlp\common\structure.py"", line 92, in __getattr__
    return self.__getitem__(key)
KeyError: '__getstate__'
direct dumping result without semantic....
Traceback (most recent call last):
  File ""<input>"", line 29, in <module>
TypeError: 'list' object is not callable
dumping dict(result) without semantic....
dump succeeded
```

* [x] I've completed this form and searched the web for solutions.
"
hanlp-2.0.0a43加载ner模型报错，hanlp与模型版本不匹配,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
hanlp版本和ner模型版本不匹配


**Code to reproduce the issue**
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform: Linux Ubuntu 16.04
- Python version: Python 3.6
- HanLP version:hanlp-2.0.0a43

**Other info / logs**
错误提示：
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.43. Try to upgrade hanlp with
pip install --upgrade hanlp
但是安装了hanlp-2.0.0-alpha.5版本的提示
Failed to download https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20191230_205748.zip due to timeout('The read operation timed out',). Please download it to /home/ubuntu/.hanlp/ner/ner_bert_base_msra_20191230_205748.zip by yourself. Or consider upgrading pip install -U hanlp

"
繁体转简体有一些错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->
- Java Code: `String simplified=HanLP.convertToSimplifiedChinese(tradition);`
- HanLP version: 1.7.7

比如“陷阱” 被 转换成 “猫腻”
“猛烈”被转换成“勐烈""
”顺口溜“被转换成”顺口熘""
""脊梁""被转换成“嵴梁”
“通道”被转换成“信道”
这些转换都没有必要，转换前后并不是简体与繁体的关系。"
limie_eval参数值太小时，报空指针,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
limie_eval参数值太小时，报空指针.
不同的测试字符串情况下，有的是在0.2的时候报这个错，有的0.1的时候报这个错
ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:242)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```scala
object HanLPTest {
  def main(args: Array[String]): Unit = {
    val analyzer: ClusterAnalyzer[String] = new ClusterAnalyzer[String]()
    val strs: List[(String, String)] = List[(String, String)](
      (""1"",""中国辽宁沈阳大东区望花北街【望花中街111-8号万城6号楼2单元20-4号】 (鑫淼海鲜馆附近)""),
      (""2"",""中国辽宁沈阳大东区【万城6号楼2单元20-4号】""),
      (""3"",""辽宁沈阳市大东区三环内望花中街111-8号万城6号楼2单元20-4号""))
    for ((id, str) <- strs) {
      analyzer.addDocument(id, str)
    }
    println(analyzer.repeatedBisection(0.01))
  }
}
```

**Describe the current behavior**
A clear and concise description of what happened.

调整到较高的参数后不报错，如0.3、0.5之类的，就不报错

**Expected behavior**
A clear and concise description of what you expected to happen.

是否可以增加空指针验证

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10 非分布
- Python version: scala2.11
- HanLP version: 1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:242)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:206)
	at HanLPTest$.main(HanLPTest.scala:31)
	at HanLPTest.main(HanLPTest.scala)

* [ ] I've completed this form and searched the web for solutions.
"
"pyhanlp 和 Hanlp 分词结果不一致, 词性不一致","<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
pyhanlp 和 java 版的Hanlp分词结果不一致
词性也不一致


**Code to reproduce the issue**

pyhanlp,  0.1.63
```python
from pyhanlp import *
print(HanLP.segment('你说的这句话没有根据,根据以往的经验,我是对的.'))
[你/rr, 说/v, 的/ude1, 这/rzv, 句话/q, 没有/v, 根据/p, ,/w, 根据/p, 以往/t, 的/ude1, 经验/n, ,/w, 我/rr, 是/vshi, 对/p, 的/ude1, ./w]
```
com.hankcs:hanlp:portable-1.7.5
```java
String text = ""你说的这句话没有根据,根据以往的经验,我是对的."";
System.out.println(HanLP.segment(text));
[你/r, 说/v, 的/uj, 这/r, 句话/q, 没/d, 有/v, 根据/p, ,/w, 根据/p, 以往/t, 的/uj, 经验/n, ,/w, 我/r, 是/v, 对/p, 的/uj, ./w]
```

**Describe the current behavior**
pyhanlp 的分词 没有/v,  词性: 你/rr
Hanlp 分词: 没/d, 有/v,  词性: 你/r

**Expected behavior**
表现一致

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.62
- HanLP version: portable-1.7.5

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
"
HanLP 2.0 中的 pipeline 中如何在最后一步进行根据自定义词典对模型分词结果合并,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
想在 pipeline 中使用自定义词典后于模型生效，用于对模型分词结果的后合并，如 HanLP 1.x 版本那样
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from hanlp.common.trie import Trie

import hanlp

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
text = '我国新能源汽车产量突破2亿台产量，是不是聪明人？NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。'
print(tokenizer(text))

trie = Trie()
trie.update({'自定义': 'custom', '词典': 'dict', '聪明人': 'smart', '国新能源': 'stock'})


def split_sents(text: str, trie: Trie):
    words = trie.parse_longest(text)
    sents = []
    pre_start = 0
    offsets = []
    for word, value, start, end in words:
        if pre_start != start:
            sents.append(text[pre_start: start])
            offsets.append(pre_start)
        pre_start = end
    if pre_start != len(text):
        sents.append(text[pre_start:])
        offsets.append(pre_start)
    return sents, offsets, words


print(split_sents(text, trie))


def merge_parts(parts, offsets, words):
    items = [(i, p) for (i, p) in zip(offsets, parts)]
    #items += [(start, [word]) for (word, value, start, end) in words]
    # In case you need the tag, use the following line instead
    items += [(start, [(word, value)]) for (word, value, start, end) in words]
    return [each for x in sorted(items) for each in x[1]]


tokenizer = hanlp.pipeline() \
    .append(split_sents, output_key=('parts', 'offsets', 'words'), trie=trie) \
    .append(tokenizer, input_key='parts', output_key='tokens') \
    .append(merge_parts, input_key=('tokens', 'offsets', 'words'), output_key='merged')

print(tokenizer(text))
```

**Describe the current behavior**
A clear and concise description of what happened.
""merged"": [
    ""我"",
    [""国新能源"", ""stock""],
    ""汽车"",
    ""产量"",
    ""突破"",
    ""2亿"",
    ""台"",
    ""产量"",
    ""，"",
    ""是"",
    ""不"",
    ""是"",
    [""聪明人"", ""smart""],
**Expected behavior**
A clear and concise description of what you expected to happen.
“国新能源” 这个词应该不生效
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Python version: 3.6
- HanLP version: 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
hanlp+jupyter的docker镜像,"**Describe the feature and the current behavior/state.**
目前官方文档里没有提供更快上手HanLP 的方式，所以我做了一个HanLP + Jupyter 的Docker镜像，可以帮助感兴趣的人更快上手体验。
walterinsh/hanlp:2.0.0a41-jupyter

[https://github.com/WalterInSH/hanlp-jupyter-docker](https://github.com/WalterInSH/hanlp-jupyter-docker)

如果满足你们的期望，可以加在文档里。

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
会使用docker，期望快速尝试的人

**Are you willing to contribute it (Yes/No):**
yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian
- Python version: 3.6
- HanLP version: 2.0.0a41

**Any other info**

* [x] I've carefully completed this form.
"
NER模型加载失败,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
加载 NER 模型失败，以下是报错信息：

Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip. See stack trace below
Traceback (most recent call last):
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/layers/transformers/loader.py"", line 115, in build_transformer
    l_bert = bert.BertModelLayer.from_params(bert_params, name='albert' if albert else ""bert"")
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/params/with_params.py"", line 67, in from_params
    instance = cls(*args, **kwargs)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/params/with_params.py"", line 47, in __init__
    self._construct(*args, **other_args)
TypeError: _construct() got an unexpected keyword argument 'name'
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.40. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
```

**Describe the current behavior**
几天前调用模型是正常的，模型和调用方式都没有改变，今天调模型突然失败了，按照提示将 hanlp 升级到 a5 版本也会挂。是不是内部 API 改了什么地方导致调用失败的？

**Expected behavior**
模型加载成功。

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.7
- HanLP version: 2.0.0-alpha.40. (试了 2.0.0-alpha.5 也不行)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
基于bert自定义分词训练出错,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
tests/train/zh/cws/train_msr_cws_bert.py 运行出错
TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ((tf.int32, tf.int32, tf.int32), tf.int32), but the yielded element was (['""', '人', '们', '常', '说', '生', '活', '是', '一', '部', '教', '科', '书', ',', '而', '血', '与', '火', '的', '战', '争', '更', '是', '不', '可', '多', '得', '的', '教', '科', '书', ',', '她', '确', '实', '是', '名', '副', '其', '实', '的', '‘', '我', '的', '大', '学', '’', '。'], ['S', 'B', 'E', 'S', 'S', 'B', 'E', 'S', 'S', 'S', 'B', 'M', 'E', 'S', 'S', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S', 'B', 'M', 'M', 'E', 'S', 'B', 'M', 'E', 'S', 'S', 'B', 'E', 'S', 'B', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S']).
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2019-12-21 15:39
from hanlp.components.tok import TransformerTokenizer
from hanlp.datasets.cws.sighan2005.msr import SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, SIGHAN2005_MSR_TEST
from tests import cdroot
from bert.loader import bert_models_google
bert_models_google['bert-base-chinese']='/usr/cws/bert-base-chinese'
cdroot()
tokenizer = TransformerTokenizer()
save_dir = 'data/model/cws_bert_base_msra'
tokenizer.fit(SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, save_dir, transformer='bert-base-chinese',
              metrics='f1')
# tagger.load(save_dir)
print(tokenizer.predict(['中央民族乐团离开北京前往维也纳', '商品和服务']))
tokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)
print(f'Model saved in {save_dir}')

```

**Describe the current behavior**
模型能够加载的，但是dataset里都是中文，应该是中文的词转成inputids有错误，所以报了`generator` yielded an element that did not match the expected structure. 定位到的是hanlp/common/transform.py 里的samples = self.inputs_to_samples(inputs, gold)，inputs_to_samples 没有调用类似convert_examples_to_features这种方法，所以全是中文。
我是通过pip install hanlp 安装的
**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.6.8
- HanLP version: hanlp-2.0.0a39

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
"was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38.","Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip. See stack trace below
Traceback (most recent call last):
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 70, in get_tensor
    self, compat.as_bytes(tensor_str))
RuntimeError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\components\taggers\transformers\transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\layers\transformers\loader.py"", line 141, in build_transformer
    skipped_weight_value_tuples = bert.load_bert_weights(l_bert, ckpt)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\bert_for_tf2-0.12.7-py3.6.egg\bert\loader.py"", line 216, in load_stock_weights
    ckpt_value = ckpt_reader.get_tensor(stock_name)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 74, in get_tensor
    error_translator(e)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 48, in error_translator
    raise errors_impl.OpError(None, None, error_message, errors_impl.UNKNOWN)
tensorflow.python.framework.errors_impl.OpError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
* [ ] I've completed this form and searched the web for solutions.
"
windows10安装后导入失败,"Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
Traceback (most recent call last):
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
依存句法分析加载模型出错,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
从`hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH`加载依存分析模型出错

**Code to reproduce the issue**
```python
import hanlp
hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
```

**Describe the current behavior**
从`hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH`加载依存分析模型出错

**Expected behavior**
成功加载

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: Linux xxx 4.15.0-72-generic #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
- HanLP version: 2.0.0a39

**Other info / logs**
```bash
Failed to load https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip. See stack trace below
Traceback (most recent call last):
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/common/component.py"", line 269, in build
    self.model(sample_inputs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/components/parsers/biaffine/model.py"", line 92, in call
    x = self.lstm(embed, mask=mask)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 281, in call
    outputs = layer(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 543, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 657, in call
    initial_state=forward_state, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 644, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1144, in call
    **cudnn_lstm_kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1362, in cudnn_lstm
    time_major=time_major)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py"", line 1906, in cudnn_rnnv3
    ctx=_ctx)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py"", line 2002, in cudnn_rnnv3_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: Fail to find the dnn implementation. [Op:CudnnRNNV3]
https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.39. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
```

* [x] I've completed this form and searched the web for solutions.
"
Pyhanlp与java Hanlp分词结果不一致,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
我用Python下载了pyhanlp, version update 到了1.7.7，同样，在java maven project我用了hanlp-portable-1.7.7, 但是我发现分词结果竟然不一致。
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
test case: “内河运输”
```python
```
Hanlp.segment(“内河运输”)

**Describe the current behavior**
A clear and concise description of what happened.
pyhanlp中Hanlp.segment结果：“内河运输”
Java中Hanlp.segment结果：“内河 运输”
**Expected behavior**
A clear and concise description of what you expected to happen.
pyhanlp与java中Hanlp.segment结果应该一致才对。
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.4
- Python version:3.6.2
- HanLP version:1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
non
* [x] I've completed this form and searched the web for solutions.
"
Typo Fix,"### 注意事项

- 这次修改没有引入第三方类库。
- 也没有修改JDK版本号
- 所有文本都是UTF-8编码
- 代码风格一致
- [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

### 解决了什么问题？带来了什么好处？

fix日志打印错误问题"
Typo fix,Typo fix
pipeline添加自定义function后， 循环使用内存溢出,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
我需要在tokenizer和tagger后， 直接取到一个""token/tag""拼接的词。故此添加了一个`hanlp_get_tokens`的function append在pipline里。 详情见下方代码。

实际任务需要预测一万篇文章的分词和tag； 然而在gpu环境下跑大概1/10的loop时oop；
disable了GPU， 用`tracemalloc`查看发现`transform\txt.py`内存溢出。


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import pandas as pd
from tqdm import tqdm
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
import hanlp
import tensorflow as tf
# Set CPU as available physical device
# tf.config.set_visible_devices([], 'GPU')
tf.config.list_physical_devices()
tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
def hanlp_get_tokens(tokens, tags, target_tag_list=[]):
    res = []
    for tokens, post in zip(tokens, tags):
        for i, j in zip(tokens, post):
            if i.strip():
                if not target_tag_list:
                    res.append(i + '/' + j)
                else:
                    if j in target_tag_list:
                        res.append(i + '/' + j)
    return res

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='tags') \
    .append(hanlp_get_tokens, input_key=('tokens', 'tags'), output_key='target_token')

import tracemalloc
import linecache

def display_top(snapshot, key_type='lineno', limit=3):
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))

# 获取数据
text = ""今天我回家买了个大西瓜;""*100
# data_df = data_df.loc[data_df['doc_id'] >= 5150]
tracemalloc.start()
for index, i in enumerate(tqdm([text]*20000)):
    tokenized = {}
    # title_counters = Counter(title_token)
    res = pipeline(i)['target_token']
    # content_counters = Counter(content_token)
    snapshot = tracemalloc.take_snapshot()
    display_top(snapshot)
```

**Describe the current behavior**
可以看到的是， 随着iteration数目增加， `word += c`这里的内存也不断增加。。。但是我翻来覆去看那段代码完全没看出问题， 故此只能提出issue， 期待何老师看看。具体log已经附上。

**Expected behavior**
loop prediction不会增加内存使用

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7
- HanLP version: 2.0.0-alpha.39

**Other info / logs**
[log gist](https://gist.github.com/luoy2/ee8256e6f90289e068695dcf661a14a6)


* [x] I've completed this form and searched the web for solutions.
"
batch为32时报错，其他数字不报错,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
用的是python,cpu跑的代码,用hanlp.pipeline()进行分析，输入batch为32时报错，其他数字不报错

下面是报出的错误信息：

UnknownError                              Traceback (most recent call last)
<ipython-input-88-3f7e6c8a5769> in <module>
      9 for i in tqdm(rand_loader):
     10     print(i)
---> 11     doc = pipeline(i)
     12     #print(doc)
     13 end=time.time()

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\pipeline.py in __call__(self, doc, **kwargs)
     95     def __call__(self, doc: Document, **kwargs) -> Document:
     96         for component in self:
---> 97             doc = component(doc)
     98         return doc
     99 

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in __call__(self, data, **kwargs)
     49 
     50     def __call__(self, data, **kwargs):
---> 51         return self.predict(data, **kwargs)
     52 
     53     @staticmethod

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\pipeline.py in predict(self, doc, **kwargs)
     48         if unpack:
     49             kwargs['_hanlp_unpack'] = True
---> 50         output = self.component(input, **kwargs)
     51         if isinstance(output, types.GeneratorType):
     52             output = list(output)

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in __call__(self, data, **kwargs)
     49 
     50     def __call__(self, data, **kwargs):
---> 51         return self.predict(data, **kwargs)
     52 
     53     @staticmethod

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in predict(self, data, batch_size, **kwargs)
    453         num_samples = 0
    454         data_is_list = isinstance(data, list)
--> 455         for idx, batch in enumerate(dataset):
    456             samples_in_batch = tf.shape(batch[-1] if isinstance(batch[-1], tf.Tensor) else batch[-1][0])[0]
    457             if data_is_list:

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in __next__(self)
    628 
    629   def __next__(self):  # For Python 3 compatibility
--> 630     return self.next()
    631 
    632   def _next_internal(self):

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in next(self)
    672     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    673     try:
--> 674       return self._next_internal()
    675     except errors.OutOfRangeError:
    676       raise StopIteration

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in _next_internal(self)
    663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:
--> 665         return structure.from_compatible_tensor_list(self._element_spec, ret)
    666 
    667   @property

D:\Anaconda3\envs\pytorch\lib\contextlib.py in __exit__(self, type, value, traceback)
    128                 value = type()
    129             try:
--> 130                 self.gen.throw(type, value, traceback)
    131             except StopIteration as exc:
    132                 # Suppress StopIteration *unless* it's the same exception that

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\eager\context.py in execution_mode(mode)
   1898   finally:
   1899     ctx.executor = executor_old
-> 1900     executor_new.wait()
   1901 
   1902 

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\eager\executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

UnknownError: AssertionError: unable to assign 22 datapoints to 32 clusters
Traceback (most recent call last):

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\ops\script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\parsers\conll.py"", line 239, in generator
    buckets = dict(zip(*kmeans(lengths, n_buckets)))

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\parsers\alg.py"", line 52, in kmeans
    assert len(d) >= k, f""unable to assign {len(d)} datapoints to {k} clusters""

AssertionError: unable to assign 22 datapoints to 32 clusters


	 [[{{node PyFunc}}]]


"
2.0.0a38的类好像有个方法不对,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**

我在看hanlp\common\structure.py里面的Serializable类代码,发现load方法中无论文件是不是json,都使用load_json去读取文件 ,
(在save时会保存为json或pickle文件,)
load时对于json外的格式按理说应该使用load_pickle而不是load_json
(类里面有定义load_pickle函数,但是没有用到)

疑似有问题的代码,见下面的注释

**Code to reproduce the issue**
下面是Serializable类的部分代码:

```python
class Serializable(object):
    """"""
    A super class for save/load operations.
    """"""

    def save(self, path, fmt=None):
        if not fmt:
            if filename_is_json(path):
                self.save_json(path)
            else:
                self.save_pickle(path)
        elif fmt in ['json', 'jsonl']:
            self.save_json(path)
        else:
            self.save_pickle(path)

    def load(self, path, fmt=None):
        if not fmt:
            if filename_is_json(path):
                self.load_json(path)
            else:
                self.load_json(path)   ######这一行我觉得有问题,感觉应该是load_pickle
        elif fmt in ['json', 'jsonl']:
            self.load_json(path)
        else:
            self.load(path)
```

* [x] I've completed this form and searched the web for solutions.
"
support getting all tags,
调用CharTabel，把“幺”改为“么”不合理,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.

调用CharTabel，把“幺”改为“么”不合理，虽然 么 也有1的意思，且也有发音为yao的，但是么通常不代表幺的意思，且幺字已经是正则化的，没有必要进一步改变，需要去掉CharTable.txt里面幺=么

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
 System.out.println(CharTable.convert(""幺妹的手机号码是幺三二开头的""));
```

**Describe the current behavior**
A clear and concise description of what happened.

么妹的手机号码是么三二开头的

**Expected behavior**
A clear and concise description of what you expected to happen.

幺妹的手机号码是幺三二开头的

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: java
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
1.x,同步更新
AbstractClassifier中的enableProbability方法似乎有BUG,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
用户调用enableProbability(boolean enable)后，不管传入的值是true还是false，都不会影响到configProbabilityEnabled的值，从而导致 if (configProbabilityEnabled) MathUtility.normalizeExp(predictionScores); 永远执行。

建议将
````java
    @Override
    public IClassifier enableProbability(boolean enable)
    {
        return this;
    }
````
修改为
````java
    @Override
    public IClassifier enableProbability(boolean enable)
    {
        configProbabilityEnabled = enable;
        return this;
    }
````

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
    public Map<String, Double> test() throws IOException {
        IClassifier iClassifier = new NaiveBayesClassifier(trainOrLoadModel()).enableProbability(false);;
        return iClassifier.predict(""test java demo""); 
}
```

**Describe the current behavior**
enableProbability方法失效。

**Expected behavior**
enableProbability方法生效。

**System information**
- Windows 0
- Java 1.8
- HanLP 1.7.5

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
词网保证从起点出发的“所有”路径都会连通到终点的问题,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
java 1.7.6版本，构建词网，无法保证从起点出发的所有路径都会连通到终点

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# -- coding: utf-8 --
from jpype import JString
from pyhanlp import *
from tests.book.ch03.msr import msr_model
from tests.test_utility import test_data_path


NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')
CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')
WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')
Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')
CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
Nature = JClass('com.hankcs.hanlp.corpus.tag.Nature')


def my_cws_corpus():
    data_root = test_data_path()
    corpus_path = os.path.join(data_root, 'my_cws_corpus1.txt')
    if not os.path.isfile(corpus_path):
        with open(corpus_path, 'w', encoding='utf-8') as out:
            out.write('''商品 和服 务
商品 和 货币''')
    return corpus_path



def train_bigram(corpus_path, model_path):
    sents = CorpusLoader.convert2SentenceList(corpus_path)
    for sent in sents:
        for word in sent:
            if word.label is None:
                word.setLabel(""n"")
    maker = NatureDictionaryMaker()
    maker.compute(sents)
    maker.saveTxtTo(model_path)  # tests/data/my_cws_model.txt


def load_bigram(model_path, verbose=True):
    HanLP.Config.CoreDictionaryPath = model_path + "".txt""  # unigram
    HanLP.Config.BiGramDictionaryPath = model_path + "".ngram.txt""  # bigram
    # 以下部分为兼容新标注集，不感兴趣可以跳过
    HanLP.Config.CoreDictionaryTransformMatrixDictionaryPath = model_path + "".tr.txt""  # 词性转移矩阵，分词时可忽略
    if model_path != msr_model:
        with open(HanLP.Config.CoreDictionaryTransformMatrixDictionaryPath) as src:
            for tag in src.readline().strip().split(',')[1:]:
                Nature.create(tag)
    CoreBiGramTableDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary')
    CoreDictionary.getTermFrequency(""商品"")
    # 兼容代码结束
    if verbose:
        sent = '商品和服务'
        wordnet = generate_wordnet(sent, CoreDictionary.trie)
        print(wordnet)
    return 0


def generate_wordnet(sent, trie):
    """"""
    生成词网
    :param sent: 句子
    :param trie: 词典（unigram）
    :return: 词网
    """"""
    searcher = trie.getSearcher(JString(sent), 0)
    wordnet = WordNet(sent)
    while searcher.next():
        wordnet.add(searcher.begin + 1,
                    Vertex(sent[searcher.begin:searcher.begin + searcher.length], searcher.value, searcher.index))
    # 原子分词，保证图连通
    vertexes = wordnet.getVertexes()
    i = 0
    while i < len(vertexes):
        if len(vertexes[i]) == 0:  # 空白行
            j = i + 1
            for j in range(i + 1, len(vertexes) - 1):  # 寻找第一个非空行 j
                if len(vertexes[j]):
                    break
            wordnet.add(i, Vertex.newPunctuationInstance(sent[i - 1: j - 1]))  # 填充[i, j)之间的空白行
            i = j
        else:
            # print(vertexes[i][-1].realWord)
            i += len(vertexes[i][-1].realWord)

    return wordnet



if __name__ == '__main__':
    corpus_path = my_cws_corpus()
    model_path = os.path.join(test_data_path(), 'my_cws_model1')
    train_bigram(corpus_path, model_path)
    load_bigram(model_path)


```

**Describe the current behavior**
A clear and concise description of what happened.
假如词典中有[""商品”，“和服”，“和”，“务”]这些词，那么""商品和服务“这句话的词网输出为
0:[ ]
1:[商品]
2:[]
3:[和, 和服]
4:[]
5:[务]
6:[ ]
**Expected behavior**
A clear and concise description of what you expected to happen.
0:[ ]
1:[商品]
2:[]
3:[和, 和服]
4:[服]
5:[务]
6:[ ]
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Python version: 3.7
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
HanLP.segment关于与“一”有关的分词错误,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
java 1.7.6版本，调用HanLP.segment后，“十一介绍”，分词为[十/m, 一介/nz, 绍/nz]，“十一中国放假吗”分词为[十/m, 一中/j, 国/n, 放假/vi, 吗/y]，类似这种与“一”相关的分词存在错误
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
HanLP.segment(""十一中国放假吗"")
```

**Describe the current behavior**
A clear and concise description of what happened.
“十一介绍”，分词为[十/m, 一介/nz, 绍/nz]
**Expected behavior**
A clear and concise description of what you expected to happen.
[十一, 介绍]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- Python version: java
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

所以很多 ""一A@*""这种模式，其中A代表一个字，*代表任意字，然后我把以A开头的词全部拿出来，并以频率排序，发现有很多这样的问题，原因应该是  一中、一发、一通等进行转移不太合适，上传文件，不知有无兴趣强化
[hanLP关于一的分词错误文件.txt](https://github.com/hankcs/HanLP/files/4196279/hanLP.txt)


中国=39573  例子：十一中国放假吗
发展=20444  例子：十一发展计划
时间=17679
通过=16921  例子：十一通过山海关吗
发现=16201  
经济=14634
会议=13344
同时=13137
生活=12721
对于=10684
发生=10588
中央=10431
介绍=9492
价格=9121
一直=8928
行为=8549
中心=8440
一起=8280
环境=7994
大家=7830
期间=7734
世界=7554
代表=7412
大学=7294
发布=7279
时候=7149
领导=7055
手机=6656


* [x] I've completed this form and searched the web for solutions.
"
module 'hanlp' has no attribute 'load',"
**Describe the bug**
AttributeError: module 'hanlp' has no attribute 'load'

**System information**
- OS Platform and Distribution (e.g., Centos7):
- Python version:3.7
- HanLP version:2.0
"
load数据中的error,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
以下必填，否则直接关闭。
-->

**Describe the bug**
过程是这样的：
通过hanlp.pretrained.ALL  取得了load在变量后进行批量load，发现出错很多：


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.
hanlp.load('GLOVE_6B_ROOT')
hanlp.load('GLOVE_6B_50D')
hanlp.load('GLOVE_6B_100D')
hanlp.load('GLOVE_6B_200D')
hanlp.load('GLOVE_6B_300D')
hanlp.load('GLOVE_840B_300D')
hanlp.load('PTB_POS_RNN_FASTTEXT_EN')
hanlp.load('FLAIR_LM_FW_WMT11_EN')
hanlp.load('FLAIR_LM_BW_WMT11_EN')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_PKU')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_MSR')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_CHAR')
hanlp.load('SEMEVAL16_EMBEDDINGS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_NEWS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_TEXT_CN')
hanlp.load('CTB5_FASTTEXT_300_CN')
hanlp.load('TENCENT_AI_LAB_EMBEDDING')
hanlp.load('RADICAL_CHAR_EMBEDDING_100')

hanlp.load()   时发生错误。


**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 prof  X64
- Python version:  Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32
- HanLP version: hanlp==2.0.0a36


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
详细的信息如下：
******第一:******
hanlp.load('GLOVE_6B_ROOT')
hanlp.load('GLOVE_6B_50D')
hanlp.load('GLOVE_6B_100D')
hanlp.load('GLOVE_6B_200D')
hanlp.load('GLOVE_6B_300D')
hanlp.load('GLOVE_840B_300D')
hanlp.load('PTB_POS_RNN_FASTTEXT_EN')
hanlp.load('FLAIR_LM_FW_WMT11_EN')
hanlp.load('FLAIR_LM_BW_WMT11_EN')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_PKU')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_MSR')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_CHAR')
hanlp.load('SEMEVAL16_EMBEDDINGS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_NEWS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_TEXT_CN')
hanlp.load('CTB5_FASTTEXT_300_CN')
hanlp.load('TENCENT_AI_LAB_EMBEDDING')
hanlp.load('RADICAL_CHAR_EMBEDDING_100')

这几个下载报类似这样在错误：
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python36\lib\site-packages\hanlp\__init__.py"", line 51, in load
    return load_from_meta_file(save_dir, meta_filename, transform_only=transform_only, load_kwargs=load_kwargs, **kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 34, in load_from_meta_file
    raise FileNotFoundError(f'The identifier {save_dir} resolves to a non-exist meta file {metapath}. {tips}')
FileNotFoundError: The identifier C:\Users\Roottan\AppData\Roaming\hanlp\hanlp\lm\flair_lm_wmt11_en_20191229_033714\flair_lm_bw_wmt11_en resolves to a non-exist meta file C:\Users\****\AppData\Roaming\hanlp\hanlp\lm\flair_lm_wmt11_en_20191229_033714\flair_lm_bw_wmt11_en\meta.json.

就是报：meta.json 文件找不到。

******第二:********
hanlp.load('MSRA_NER_BERT_BASE_ZH')
hanlp.load('MSRA_NER_ALBERT_BASE_ZH')
hanlp.load('CONLL03_NER_BERT_BASE_UNCASED_EN')
hanlp.load('CHNSENTICORP_BERT_BASE_ZH')
报类似以下的错：
Done loading 197 BERT weights from: C:\Users\****\AppData\Roaming\hanlp\thirdparty\storage.googleapis.com\bert_models\2018_11_03\chinese_L-12_H-768_A-12\bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000020C43D25828> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]
Unused weights from checkpoint:
        bert/pooler/dense/bias
        bert/pooler/dense/kernel
        cls/predictions/output_bias
        cls/predictions/transform/LayerNorm/beta
        cls/predictions/transform/LayerNorm/gamma
        cls/predictions/transform/dense/bias
        cls/predictions/transform/dense/kernel
        cls/seq_relationship/output_bias
        cls/seq_relationship/output_weights

就是报：Unused weights from checkpoint:


******第三:********
hanlp.load('SST2_BERT_BASE_EN')
Failed to load https://file.hankcs.com/hanlp/classification/sst2_bert_base_uncased_en_20200104_175422.zip. See stack trace below
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 39, in load_from_meta_file
    obj: Component = object_from_class_path(cls, **kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\utils\reflection.py"", line 24, in object_from_class_path
    class_path = str_to_type(class_path)
  File ""C:\Python36\lib\site-packages\hanlp\utils\reflection.py"", line 37, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'hanlp.components.classifiers.bert_text_classifier'
https://file.hankcs.com/hanlp/classification/sst2_bert_base_uncased_en_20200104_175422.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.36. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

说明：pip install --upgrade hanlp   不能解决问题。


******第四:********
hanlp.load('SST2_ALBERT_BASE_EN')
>>> hanlp.load('SST2_ALBERT_BASE_EN')
Fetching ALBERT model: albert_base version: 2
albert_base.tar.gz: 0.00B [00:00, ?B/s]
Failed to load https://file.hankcs.com/hanlp/classification/sst2_albert_base_20200122_205915.zip. See stack trace below
Traceback (most recent call last):

hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_BASE_EN')
Fetching ALBERT model: albert_base version: 2
albert_base.tar.gz: 0.00B [00:00, ?B/s]
Failed to load https://file.hankcs.com/hanlp/classification/empathetic_dialogues_situation_albert_base_20200122_212250.zip. See stack trace below
Traceback (most recent call last):
  File ""C:\Python36\lib\urllib\request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""C:\Python36\lib\http\client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1026, in _send_output
    self.send(msg)
  File ""C:\Python36\lib\http\client.py"", line 964, in send
    self.connect()
  File ""C:\Python36\lib\http\client.py"", line 1392, in connect
    super().connect()
  File ""C:\Python36\lib\http\client.py"", line 936, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""C:\Python36\lib\socket.py"", line 724, in create_connection
    raise err
  File ""C:\Python36\lib\socket.py"", line 713, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""C:\Python36\lib\site-packages\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""C:\Python36\lib\site-packages\hanlp\components\classifiers\transformer_classifier.py"", line 153, in build_model
    tagging=False)
  File ""C:\Python36\lib\site-packages\hanlp\layers\transformers\loader.py"", line 84, in build_transformer
    transformer))
  File ""C:\Python36\lib\site-packages\bert\loader_albert.py"", line 179, in fetch_tfhub_albert_model
    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir, local_file_name=local_file_name)
  File ""C:\Python36\lib\site-packages\params_flow\utils\fetch_unpack.py"", line 48, in fetch_url
    urllib.request.urlretrieve(url, local_path, report_hook, data=None)
  File ""C:\Python36\lib\urllib\request.py"", line 248, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""C:\Python36\lib\urllib\request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python36\lib\urllib\request.py"", line 526, in open
    response = self._open(req, data)
  File ""C:\Python36\lib\urllib\request.py"", line 544, in _open
    '_open', req)
  File ""C:\Python36\lib\urllib\request.py"", line 504, in _call_chain
    result = func(*args)
  File ""C:\Python36\lib\urllib\request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""C:\Python36\lib\urllib\request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试 失败。>
https://file.hankcs.com/hanlp/classification/empathetic_dialogues_situation_albert_base_20200122_212250.zip was created with hanlp-2.0.0-alpha.27, while you are running 2.0.0-alpha.36. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_LARGE_EN')

这几个经检查是https://tfhub.dev 上不去的原因，用代理后解决，在此不得不吐槽一下，国内作程序研究，double hit：
albert_models_tfhub = {
    ""albert_base"":    ""https://tfhub.dev/google/albert_base/{version}?tf-hub-format=compressed"",
    ""albert_large"":   ""https://tfhub.dev/google/albert_large/{version}?tf-hub-format=compressed"",
    ""albert_xlarge"":  ""https://tfhub.dev/google/albert_xlarge/{version}?tf-hub-format=compressed"",
    ""albert_xxlarge"": ""https://tfhub.dev/google/albert_xxlarge/{version}?tf-hub-format=compressed"",
}

******第五:********
tokenizer = hanlp.load('SST2_ALBERT_BASE_EN')
>>> tokenizer = hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_LARGE_EN')
Fetching ALBERT model: albert_large version: 2
Already  fetched:  albert_large.tar.gz
already unpacked at: C:\Users\Roottan\AppData\Roaming\hanlp\thirdparty\tfhub.dev\google\albert_large\albert_large
Done loading 23 BERT weights from: C:\Users\****\AppData\Roaming\hanlp\thirdparty\tfhub.dev\google\albert_large\albert_large into <bert.model.BertModelLayer object at 0x000001DCDF95B828> (prefix:albert_2). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]
Unused weights from saved model:
        bert/pooler/dense/bias
        bert/pooler/dense/kernel
        cls/predictions/output_bias
        cls/predictions/transform/LayerNorm/beta
        cls/predictions/transform/LayerNorm/gamma
        cls/predictions/transform/dense/bias
        cls/predictions/transform/dense/kernel

不知道是个案还是大家会遇到的通病




* [x] I've completed this form and searched the web for solutions.
"
pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN don't work.,"**Describe the bug**
version: hanlp 2.0.0-alpha.34
because of the lack of downloading, I downloaded ""ner_conll03_bert_base_uncased_en_20200104_194352.zip"" manually, place it into /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip

**Code to reproduce the issue**
```python
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)
```

**Describe the current behavior**
if don't unzip the .zip file by myself, got msg as below:
FileNotFoundError: The identifier /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip resolves to a non-exist meta file /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip/meta.json.

Then, I unzip it and retry.

**Expected behavior**
https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.34. Try to upgrade hanlp with
pip install --upgrade hanlp

how to get alpha.5? which source/image mirror?

**System information**
-CentOS Linux release 7.7.1908 (Core)
- Python version: Python 3.7.4
- HanLP version: 2.0.0-alpha.34

"
hanlp 2.0.0a33 pipeline 自定义 output_key 后，json 序列化结果残留sentences 、 tokens、part_of_speech_tags 等字段,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
hanlp 2.0.0a33 pipeline 自定义 output_key 后，json 序列化结果残留**sentences 、 tokens、part_of_speech_tags、named_entities、syntactic_dependencies、semantic_dependencies** 等字段

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

print('正在加载')

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')
semantic_parser = hanlp.load('SEMEVAL16_TEXT_BIAFFINE_ZH')

# 下面自定义了 ouput_key
pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='s') \
    .append(tokenizer, output_key='t') \
    .append(tagger, output_key='p') \
    .append(syntactic_parser, input_key=('t', 'p'), output_key='syn', conll=False) \
    .append(semantic_parser, input_key=('t', 'p'), output_key='sem', conll=False)
print(pipeline)

text = '''HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。
HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。
'''

doc = pipeline(text)
print(doc)  # 打印结果正常，没有残留字段

with open('output.json',""w"") as f:
    json.dump(doc, f, indent=4, ensure_ascii=False)    # 序列化的结果有残留字段
```

**Describe the current behavior**
json 序列化后的结果，存在下面一些“**未定义字段**”
{
    ""sentences"": [],
    ""tokens"": [],
    ""part_of_speech_tags"": [],
    ""named_entities"": [],
    ""syntactic_dependencies"": [],
    ""semantic_dependencies"": [],
    ""s"": [
        ""HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。"",
        ""HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。"",
        ""内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。""
    ],
...

**Expected behavior**
**未定义字段不出现**

**System information**
- OS Platform and Distribution:18.04
- Python version:3.6
- HanLP version:hanlp 2.0.0a33

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
pip install hanlp会自动下载tensorflow2.1,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
pip install hanlp会自动下载tensorflow2.1，并覆盖已有的tensorflow-gpu2.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```bash
pip install hanlp
```

**Describe the current behavior**
```bash
Collecting hanlp
  Using cached hanlp-2.0.0a33.tar.gz (130 kB)
Collecting tensorflow==2.1.0
  Using cached tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)
```


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04.1-Ubuntu
- Python version: 3.6
- HanLP version: hanlp-2.0.0a33

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0a31 CoNLLWord JSON 序列化结果 与 文档说明不一致,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
CoNLLWord JSON 序列化结果 与 文档说明不一致

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')

text = '内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。'

pipeline = hanlp.pipeline() \
            .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
            .append(tokenizer, output_key='tokens') \
            .append(tagger, output_key='part_of_speech_tags')
            .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies')

output = pipeline(text)

print(output)  

with open('output.json',""w"") as f:
    json.dump(output, f, indent=4, ensure_ascii=False)
```

**Describe the current behavior**
# 当前输出结果如下
```
...
""syntactic_dependencies"": [
    [{""id"": 1, ""form"": ""内部"", ""cpos"": ""NN"", ""pos"": null, ""head"": 2, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 2, ""form"": ""算法"", ""cpos"": ""NN"", ""pos"": null, ""head"": 18, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 3, ""form"": ""经过"", ""cpos"": ""P"", ""pos"": null, ""head"": 18, ""deprel"": ""prep"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 4, ""form"": ""工业界"", ""cpos"": ""NN"", ""pos"": null, ""head"": 6, ""deprel"": ""conj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 5, ""form"": ""和"", ""cpos"": ""CC"", ""pos"": null, ""head"": 6, ""deprel"": ""cc"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 6, ""form"": ""学术界"", ""cpos"": ""NN"", ""pos"": null, ""head"": 7, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 7, ""form"": ""考验"", ""cpos"": ""NN"", ""pos"": null, ""head"": 3, ""deprel"": ""pobj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 8, ""form"": ""，"", ""cpos"": ""PU"", ""pos"": null, ""head"": 18, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 9, ""form"": ""配套"", ""cpos"": ""NN"", ""pos"": null, ""head"": 10, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 10, ""form"": ""书籍"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 11, ""form"": ""《"", ""cpos"": ""PU"", ""pos"": null, ""head"": 14, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 12, ""form"": ""自然"", ""cpos"": ""NN"", ""pos"": null, ""head"": 13, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 13, ""form"": ""语言"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 14, ""form"": ""处理"", ""cpos"": ""VV"", ""pos"": null, ""head"": 18, ""deprel"": ""dep"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 15, ""form"": ""入门"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""dobj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 16, ""form"": ""》"", ""cpos"": ""PU"", ""pos"": null, ""head"": 14, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 17, ""form"": ""已经"", ""cpos"": ""AD"", ""pos"": null, ""head"": 18, ""deprel"": ""advmod"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 18, ""form"": ""出版"", ""cpos"": ""VV"", ""pos"": null, ""head"": 0, ""deprel"": ""root"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 19, ""form"": ""。"", ""cpos"": ""PU"", ""pos"": null, ""head"": 18, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}]
  ],
...
```

**Expected behavior**
```
...
""syntactic_dependencies"": [
[[2, ""nn""], [18, ""nsubj""], [18, ""prep""], [6, ""conj""], [6, ""cc""], [7, ""nn""], [3, ""pobj""], [18, ""punct""], [10, ""rcmod""], [15, ""nn""], [15, ""punct""], [15, ""nn""], [15, ""nn""], [15, ""nn""], [18, ""nsubj""], [15, ""punct""], [18, ""advmod""], [0, ""root""], [18, ""punct""]]
]
...
```

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Python version: 3.6
- HanLP version: hanlp 2.0.0a31

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
pipeline Object of type 'CoNLLWord' is not JSON serializable,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
pipeline  syntactic_parser  输出的结果 通过 json dump 失败    .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')

text = '测试一下这个方法能否使用。'

pipeline = hanlp.pipeline() \
            .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
            .append(tokenizer, output_key='tokens') \
            .append(tagger, output_key='part_of_speech_tags')
            .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies')

output = pipeline(text)

print(output)    # **结果能正常打印**

with open('output.json',""w"") as f:
    json.dump(output, f, indent=4, ensure_ascii=False)
```

**Describe the current behavior**
json.dump 失败

**Expected behavior**
能正常 JSON serializable

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Python version: 3.6
- HanLP version: hanlp 2.0.0a30

**Other info / logs**
Traceback (most recent call last):
  File ""down.py"", line 33, in <module>
    json.dump(output, f, indent=4, ensure_ascii=False)
  File ""/usr/lib/python3.6/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/usr/lib/python3.6/json/encoder.py"", line 430, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 437, in _iterencode
    o = _default(o)
  File ""/usr/lib/python3.6/json/encoder.py"", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'CoNLLWord' is not JSON serializable

* [x] I've completed this form and searched the web for solutions.
"
拼音转换 “看着” 应该是 kanzhe,如题
怎么查看GPU是否成功使用,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
pip安装之后，可以成功运行NER，但是增加batch后GPU的显存始终停留在100多MB，怎么查看GPU是否成功使用？




* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0a26 使用 CTB5_POS_RNN 报错,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
hanlp 2.0.0a26 使用 CTB5_POS_RNN 报错 .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load('CTB5_POS_RNN')
(tokenizer('商品和服务')
```

**Describe the current behavior**
运行上述代码出错

**Expected behavior**
能够运行测试代码

**System information**
- Ubuntu 18.04
- Python version: 3.6
- HanLP version: 2.0.0a26

**Other info / logs**
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mode
    yield
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterator_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 825, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""load-sem.py"", line 5, in <module>
    tokenizer('商品和服务')
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 51, in __call__
    return self.predict(data, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 47, in predict
    return super().predict(sents, batch_size)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 454, in predict
    for idx, batch in enumerate(dataset):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __next__
    return self.next()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mode
    executor_new.wait()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 825, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.


	 [[{{node PyFunc}}]]
```

* [x] I've completed this form and searched the web for solutions.
"
2.0.0-alpha.25 加载 CTB5_POS_RNN 出错,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
2.0.0-alpha.25 加载 CTB5_POS_RNN 出错

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load('CTB5_POS_RNN')
print(tokenizer('商品和服务'))
```

**Describe the current behavior**
加载 CTB5_POS_RNN 出错

试了几次，此处出错
Downloading **https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt** to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
**100.00%, 4 KB/4 KB, 14.1 MB/s, ETA 0 s**     
......
**FileNotFoundError**: [Errno 2] No such file or directory: '/root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'

**Expected behavior**
能够运行测试代码

**System information**
- Ubuntu 18.04
- Python version: 3.6
- HanLP version: 2.0.0-alpha.25

**Other info / logs**
Downloading https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip to /root/.hanlp/pos/ctb5_pos_rnn_20191229_015325.zip
100.00%, 52.4 MB/52.4 MB, 4.2 MB/s, ETA 0 s        
Extracting /root/.hanlp/pos/ctb5_pos_rnn_20191229_015325.zip to /root/.hanlp/pos
Downloading https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
**100.00%, 4 KB/4 KB, 14.1 MB/s, ETA 0 s**      
Extracting /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/__init__.py"", line 53, in build_embedding
    trainable=config.get('embedding_trainable', False))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/word2vec.py"", line 22, in __init__
    self.vocab, self.array_np = self._load(path, vocab, normalize)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/word2vec.py"", line 50, in _load
    word2vec, dim = load_word2vec(path)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 420, in load_word2vec
    with open(realpath, encoding='utf-8', errors='ignore') as f:
**FileNotFoundError**: [Errno 2] No such file or directory: '/root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.25. Try to upgrade hanlp with
pip install --upgrade hanlp

* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0-alpha.25 加载 hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH 出错,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
hanlp 2.0.0-alpha.25 加载 hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH 出错

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
tagger(['我', '的', '希望', '是', '希望', '和平'])
```

**Describe the current behavior**
加载 hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH 报错

**Expected behavior**
能够运行测试代码
**不会是内存不够吧？**

**System information**
- Ubuntu 18.04 
- Python version: 3.6
- HanLP version: 2.0.0-alpha.25

**Other info / logs**
Downloading https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip to /root/.hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip
100.00%, 1.4 MB/1.4 MB, 677 KB/s, ETA 0 s      
Extracting /root/.hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip to /root/.hanlp/pos
Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin to /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip
1.68%, 53.6 MB/3.1 GB, 9.6 MB/s, ETA 5 m 27 s   
100.00%, 3.1 GB/3.1 GB, 8.0 MB/s, ETA 0 s      
Extracting /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip to /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
**MemoryError: std::bad_alloc**
**https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip** was created with **hanlp-2.0.0, while you are running 2.0.0-alpha.25.** Try to upgrade hanlp with
pip install --upgrade hanlp

* [x] I've completed this form and searched the web for solutions.
"
根据地址手动下载模型后，加载报错,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
A clear and concise description of what the bug is.
执行代码：
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)

因为下载太慢

Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin to /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip

Please download it to /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip

我手动下载后，到 /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip 后，运行代码报错

Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See stack trace below
Traceback (most recent call last):
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/layers/embeddings/fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/fasttext/FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/fasttext/FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
RuntimeError: Caught an unknown exception!
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.24. Try to upgrade hanlp with
pip install --upgrade hanlp



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os
- Python version: 3.69
- HanLP version: 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
"Error in component_util.py file. Should be sys.exit(), but instead it is exit(). Also need to import sys ","<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
I am trying to run NER with hanlp, but it is giving me an error. I am running the following comman in Google Colab with GPU

recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)

**Code to reproduce the issue**
Following were my Cell values in Colab
!pip install --hanlp
import hanlp
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)


```python
```

**Describe the current behavior**
When I run the said code, I get the follwoing error and point to this file ""component_util.py""
NameError: name 'exit' is not defined on line 56

**Expected behavior**
The command should have ran successfully allowing me to Find Named Entity in  sentence

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab with GPU
- Python version: 3.6
- HanLP version: 2.0.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Named Entity Recognition Doesnt work,"I am running this project in Google Colab with GPU, which I run this command
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)

It gives following error,
Downloading https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip to /root/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip
92.72%, 356.6 MB/384.6 MB, 7.3 MB/s, ETA 4 s      Executing op Range in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Downloading https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
100.00%, 388.8 MB/388.8 MB, 7.9 MB/s, ETA 0 s      
Extracting /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18
Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op TruncatedNormal in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0

Failed to load https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 35, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 108, in build_transformer
    with stdout_redirected(to=os.devnull):
  File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 520, in stdout_redirected
    stdout_fd = fileno(stdout)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 501, in fileno
    fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()
io.UnsupportedOperation: fileno
https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.20. Try to upgrade hanlp with
pip install --upgrade hanlp

---------------------------------------------------------------------------

UnsupportedOperation                      Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
     42                     load_kwargs = {}
---> 43                 obj.load(save_dir, **load_kwargs)
     44             obj.meta['load_path'] = load_path

9 frames

UnsupportedOperation: fileno


During handling of the above exception, another exception occurred:

NameError                                 Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
     54                 f'Try to upgrade hanlp with\n'
     55                 f'pip install --upgrade hanlp')
---> 56         exit(1)
     57 
     58 

NameError: name 'exit' is not defined

Any help will be appreciated!
Thank in advance"
聚类算法-传入簇参数大于文档个数时报空指针,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
传入簇参数大于文档个数时报空指针

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
public static void main(String[] args) {
		String a = ""select * from table;"";
		String b = ""select * from table;"";
		String c = ""select * from table where id = 100;"";
		String d = ""select * from table;"";
		String e = ""delete from table;"";
		String f = ""update table set age = 1 where id = 9"";
		String g = ""update table set age = 1 where id = 88"";
		String h = ""update table set age = 1 where id = 10"";

		Set<Integer> set = new HashSet<>();
		List<String> list = new ArrayList<>();
		list.add(a);
		list.add(b);
		list.add(c);
		list.add(d);
		list.add(e);
		list.add(f);
		list.add(g);
		list.add(h);
		String[] array = list.toArray(new String[0]);
		set.add(0);
		set.add(1);
		set.add(3);
		set.add(5);
		set.add(6);
		set.add(7);
		System.out.println(""================聚类算法=============="");
		ClusterAnalyzer<String> analyzer = new ClusterAnalyzer<>();
		for (Integer s: set) {
			analyzer.addDocument(String.valueOf(s), array[s]);
		}
		// k大于set集合大小
		int k = 10;
		System.out.println(analyzer.kmeans(k));
		System.out.println();

		System.out.println(analyzer.repeatedBisection(k));
		System.out.println(analyzer.repeatedBisection(1.0));

	}
```

**Describe the current behavior**
```
kmeans:
================聚类算法==============
Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.refine_clusters(ClusterAnalyzer.java:263)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.kmeans(ClusterAnalyzer.java:147)
```
```
repeatedBisection:
================聚类算法==============

Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:222)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:180)
```
**Expected behavior**
簇类参数传的值越界时，程序应当友好处理为文档个数
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Python version:
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
ModuleNotFoundError: No module named 'regex',"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**

**ModuleNotFoundError: No module named 'regex'**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
Python 3.7.5 (default, Jan  6 2020, 17:18:04)
[Clang 11.0.0 (clang-1100.0.33.16)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/__init__.py"", line 6, in <module>
    import hanlp.common
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/__init__.py"", line 4, in <module>
    from . import component
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/component.py"", line 17, in <module>
    from hanlp.common.structure import SerializableDict
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/structure.py"", line 6, in <module>
    from hanlp.utils.io_util import save_json, save_pickle, load_pickle, load_json, filename_is_json
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/__init__.py"", line 5, in <module>
    from . import rules
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/rules.py"", line 3, in <module>
    from hanlp.utils.english_tokenizer import tokenize_english
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/english_tokenizer.py"", line 12, in <module>
    from regex import compile, DOTALL, UNICODE, VERBOSE
ModuleNotFoundError: No module named 'regex'
```

**Describe the current behavior**
ModuleNotFoundError: No module named 'regex'

**Expected behavior**
No error, no warning.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin MacBook 19.2.0
- Python version: 3.7.5
- HanLP version: 2.0.0a10

**Other info / logs**

**Solved**

```
pip install regex
```"
下载句法解析时出错,"

**Describe the bug**
下载句法解析时出错，提示https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.12. T

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
```

**Describe the current behavior**
```
Downloading https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip to /Users/wuhaixu/.hanlp/dep/biaffine_ctb7_20191229_130325.zip
100.00%, 65.8 MB/65.8 MB, 219 KB/s, ETA 0 s          
Extracting /Users/wuhaixu/.hanlp/dep/biaffine_ctb7_20191229_130325.zip to /Users/wuhaixu/.hanlp/dep
Downloading https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt to /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
100.00%, 0 KB/0 KB, 0 KB/s, ETA 0 s      
Extracting /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip to /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive
Failed to load https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip. See stack trace below
Traceback (most recent call last):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 36, in load_from_meta_file
    obj.load(save_dir)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 231, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 242, in build
    loss=kwargs.get('loss', None)))
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/parsers/biaffine_parser.py"", line 35, in build_model
    self.transform) if pretrained_embed else None
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/layers/embeddings/word2vec.py"", line 119, in __init__
    word2vec, _output_dim = load_word2vec(filepath)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/utils/io_util.py"", line 410, in load_word2vec
    with open(realpath, encoding='utf-8', errors='ignore') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'
https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.12. Try to upgrade hanlp with
pip install --upgrade hanlp
```


**System information**
- Mac OS
- Python 3.6.9
- HanLP version: 2.0.0-alpha.12



* [x] I've completed this form and searched the web for solutions.
"
实体识别AttributeError: 'FullTokenizer' object has no attribute 'unk_token',"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
以下必填，否则直接关闭。
-->

**Describe the bug**
实体识别输入以下文本时，会出现AttributeError: 'FullTokenizer' object has no attribute 'unk_token'

**Code to reproduce the issue**
```python
import hanlp

recognizer([list('孽债 （上海话）')])
recognizer([list('关于爱的四篇小小说《鱼头》儿子从小就记得，但凡家里吃鱼，妈妈总是把鱼头夹自己碗里，因为鱼头肉少。后来每次爸爸都把鱼头夹给她。直到妈妈老了，依旧如此。后来，妈妈病重，一天，家里又吃鱼了，妈妈跟他和爸爸说：我吃了一辈子鱼头，我要死了，让我也吃一次鱼肉吧。爸爸痛哭：“我这辈子最喜欢的就是鱼头就酒，我以为你真的爱吃，才每 次忍着口水把鱼头让给你。”《下雨》一场车祸，夺走了女孩很多东西，女孩变得又聋又瞎，男朋友也离她而去，只有妈妈一直陪在她身边。女孩从小就喜欢下雨天，她喜欢在细雨中漫步的感 觉。妈妈知道她喜欢雨，就常扶女孩出去，让她感受雨水拍打双手。所幸的是今年雨特别多。一天，又下雨了，妈妈扶女儿出去。“妈，我想回去了。”""好。”回去时女儿的耳朵突然有了剧烈的疼痛和反应，模糊中隐约听到：“看，又是那两个人，大晴天的雇人洒水，真可笑，又不是拍戏......”此刻，她强忍泪水，不让妈妈看见。 《老伴》退休在家后，老伴最爱从早到晚数落我又 老又胖好吃懒做。今早起床我突然咳嗽并吐出一口鲜血，他看到后整个上午没有说出一句话，闷闷的抽烟。中午拉我去了医院，最后得知那是我牙龈发炎口腔出的血，他立马就站在医院怒骂我：”你这个没用的胖老太婆……”只是没骂完，他的眼眶已满是泪水......《存折》老人得肺癌住院，家里几乎掏空了，最后实在没办法，家人只能眼睁睁看着老人受着病痛的折磨。老人临走前的一晚，将孙子叫到床前，小心地从怀中掏出一本存折，小声对孙子说：这是我留的一点私房钱，就知道那傻老婆子会把自己的养老钱拿出来给我看病，这钱是我偷偷藏的，等我走后就留给她养老吧……小声对孙子说：这是我留的一点私房钱，就知道那傻老婆子会把自己的养老钱拿出来给我看病，这钱是我偷偷藏的，等我走后就留给她养老吧……')])
```

**Describe the current behavior**
Traceback (most recent call last):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mode
    yield
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterator_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FullTokenizer' object has no attribute 'unk_token'
Traceback (most recent call last):

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/transform.py"", line 155, in generator
    yield from samples

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_transform.py"", line 85, in inputs_to_samples
    pad_token_label_id=pad_label_idx)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/utils.py"", line 43, in convert_examples_to_features
    word_tokens = [x] * len(word)

AttributeError: 'FullTokenizer' object has no attribute 'unk_token'


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 51, in __call__
    return self.predict(data, **kwargs)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 441, in predict
    for idx, batch in enumerate(dataset):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __next__
    return self.next()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mode
    executor_new.wait()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FullTokenizer' object has no attribute 'unk_token'
Traceback (most recent call last):

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/transform.py"", line 155, in generator
    yield from samples

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_transform.py"", line 85, in inputs_to_samples
    pad_token_label_id=pad_label_idx)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/utils.py"", line 43, in convert_examples_to_features
    word_tokens = [tokenizer.unk_token] * len(word)

AttributeError: 'FullTokenizer' object has no attribute 'unk_token'


	 [[{{node PyFunc}}]]


**Expected behavior**
期望顺利返回实体识别结果

**System information**
- MacOS 10.14.6
- python3.6.9
- 2.0.0-alpha.10



* [x] I've completed this form and searched the web for solutions.
"
Export models for serving. Explore the scalable way to serve efficiently,tensorflow serving consumes memory greedily. Will find a way to fit all the pipelines into one GPU.
Train a LM on wiki + weibo + qa + news + danmaku + reviews + ...,"What makes HanLP different than the majority of OSS projects?
One of the most important factors would be the large scale professional corpora, and the correct way to make use of them.
To have some unique pretrained LM before releasing the beta version would be a cool idea. Don't you think so?"
Unittest and CI integration,"Shall we use CI? I think so, as the project grows fast, CI makes it easier to releases stable codes. The downside would be, it requires lots of effort to write unit tests.  "
Deploy on the production server. Invite users for beta test,"Only a handful models are able to serve. Those subclass models are not likely to fit with tensorflow serving. Need more time to investigate.

Will need an authentication service to identify users for test purpose and rate limiting. I don't have much computation resource for all the users. Might invite 10 lucky users. "
RESTful API in Python,"Implement a RESTful API in Python, then release it to pypi."
RESTful API in Java,Planning to implement a RESTful API in Java. Then release it to maven.
Documentation,Planning to fully document all the codes and set up a documentation service.
文件加载报错,"https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.10. Try to upgrade hanlp with"
命令行输入Hanlp后出错,"Traceback (most recent call last):
  File ""e:\coding\annoconda\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""e:\coding\annoconda\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""E:\coding\annoconda\Scripts\hanlp.exe\__main__.py"", line 5, in <module>
  File ""e:\coding\annoconda\lib\site-packages\pyhanlp\__init__.py"", line 145, in <module>
    _start_jvm_for_hanlp()
  File ""e:\coding\annoconda\lib\site-packages\pyhanlp\__init__.py"", line 133, in _start_jvm_for_hanlp
    HANLP_JVM_XMX, convertStrings=True)
  File ""e:\coding\annoconda\lib\site-packages\jpype\_core.py"", line 219, in startJVM
    _jpype.startup(jvmpath, tuple(args), ignoreUnrecognized, convertStrings)
jpype._jclass.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0"
下载数据文件出错。貌似是pythons3的对urls格式的链接的读取安全问题,"![U9L7}ZG)Y2{ %V3D29$UD7B](https://user-images.githubusercontent.com/44012390/71877670-c03dfd00-3164-11ea-8637-b658cfe518c1.png)
"
hanlp如何导入到luke中呢？,"https://github.com/DmitryKey/luke
两个项目的issue里我都没搜到解决办法，有大佬知道吗？"
预训练模型下载太慢，尝试了很多次都失败了,"recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

Downloading:   0%|          | 939k/478M [07:50<2643:19:23, 50.2B/s]

请问通过其它工具下载完成，放在哪个目录下？我在日志里看到从./cache/*** 里加载
"
词性分析的过程中报错,"python 版本:3.7.4
hanlp  版本:2.0.0a5
报错信息:
`
Traceback (most recent call last):
  File ""/home/cy/liunx_work_26/hanlp_eg.py"", line 15, in <module>
    print(tagger.predict(['我', '的', '希望', '是', '希望', '和平']))
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/components/taggers/rnn_tagger.py"", line 47, in predict
    return super().predict(sents, batch_size)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/common/component.py"", line 445, in predict
    for output in self.predict_batch(batch, inputs=inputs):
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/common/component.py"", line 455, in predict_batch
    Y = self.model.predict_on_batch(X)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1240, in predict_on_batch
    return training_v2_utils.predict_on_batch(self, x, standalone=True)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 556, in predict_on_batch
    return predict_on_batch_fn(inputs)  # pylint: disable=not-callable
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 267, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 717, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 891, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 543, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 657, in call
    initial_state=forward_state, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 644, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1147, in call
    **normal_lstm_kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1281, in standard_lstm
    if sequence_lengths is not None else timesteps)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 3897, in rnn
    if mask.dtype != dtypes_module.bool:
TypeError: data type not understood
`"
加载MSRA NER 预训练模型的时候报错,"环境 Ubuntu 18.04, Python 3.6
```
In [2]: recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_CN)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-736be980464f> in <module>
----> 1 recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_CN)

~/.local/lib/python3.6/site-packages/hanlp/__init__.py in load(save_dir, meta_filename, **kwargs)
     40     save_dir = hanlp.pretrained.ALL.get(save_dir, save_dir)
     41     from hanlp.utils.component_util import load_from_meta_file
---> 42     return load_from_meta_file(save_dir, meta_filename, **kwargs)
     43
     44

~/.local/lib/python3.6/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, **kwargs)
     30     obj: Component = object_from_class_path(cls, **kwargs)
     31     if hasattr(obj, 'load') and os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 32         obj.load(save_dir)
     33     obj.meta['load_path'] = load_path
     34     return obj

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in load(self, save_dir, logger, **kwargs)
    225         self.load_config(save_dir)
    226         self.load_vocabs(save_dir)
--> 227         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    228         self.load_weights(save_dir)
    229         self.load_meta(save_dir)

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in build(self, logger, **kwargs)
    236         self.transform.build_config()
    237         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
--> 238                                                    loss=kwargs.get('loss', None)))
    239         self.transform.lock_vocabs()
    240         optimizer = self.build_optimizer(**self.config)

~/.local/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py in build_model(self, transformer, max_seq_length, **kwargs)
     37         tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(transformer)
     38         self.transform.tokenizer = tokenizer
---> 39         transformer: TFPreTrainedModel = TFAutoModel.from_pretrained(transformer, name=os.path.basename(transformer))
     40         self.transform.transformer_config = transformer.config
     41

~/.local/lib/python3.6/site-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    219             return TFRobertaModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
    220         elif 'bert' in pretrained_model_name_or_path:
--> 221             return TFBertModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
    222         elif 'openai-gpt' in pretrained_model_name_or_path:
    223             return TFOpenAIGPTModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)

~/.local/lib/python3.6/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    311         ret = model(model.dummy_inputs, training=False)  # build the network with dummy inputs
    312
--> 313         assert os.path.isfile(resolved_archive_file), ""Error retrieving file {}"".format(resolved_archive_file)
    314         # 'by_name' allow us to do transfer learning by skipping/adding layers
    315         # see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357

AssertionError: Error retrieving file /home/smtech/.cache/torch/transformers/987cd265ea1aa9cd7e884caf8dd86c2e764e5114ee9a14a67686c1fe05f7a26c.h5
```"
hanlp支持恶意评论检测吗？,
这个版本没有zip包啊，程序里下载报错,"![image](https://user-images.githubusercontent.com/11661959/71723130-71dbe600-2e66-11ea-9a8d-a34ae0beb1e6.png)
"
java 版不同 module 下使用不同的配置,作者你好， 我在两个不同的 module 下配置不同的 hanlp.properties ， 在第三个 module  下调用两个 module ， 发现用的都是同一份配置， 有什么办法让 不同的 module 用不同的   hanlp.properties 配置吗？ 谢谢
java代码以后维护吗？,你好作者，我刚刚买了你们的书，发现今天源码都换成py了，请问以后java部分还会同步更新吗？
pyhanlp和hanlp,"你好，
请问pyhanlp和hanlp是什么关系呢？然后pyhanlp的依存句法分析，支持输入分好词的句子嘛？谢谢回复 ~
新年快乐"
pyhanlp配置问题：HANLP_JAR_PATH已配置，报错不是jar文件，已使用 '/',"请确认下列注意事项：

我已仔细阅读下列文档，都没有找到答案：
首页文档
wiki
常见问题
我已经通过Google和issue区检索功能搜索了我的问题，也没有找到答案。
我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
 我在此括号内输入x打钩，代表上述事项确认完毕
版本号
当前最新版本号是：1.7.6
我使用的版本是：1.7.6

我的问题

HANLP_JAR_PATH和HANLP_ROOT_PATH已配置为static
运行from pyhanlp import *，报错ValueError: 配置错误: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static 不是jar文件
我已经将环境变量HANLP_JAR_PATH和HANLP_ROOT_PATH的路径改为D:/anaconda/Lib/site-packages/pyhanlp/static，properties文件的root路径也修改了"
congrats！ hanlp2.0.0 release！base on pure py!,as title
pyhanlp配置问题：HANLP_JAR_PATH已配置，报错不是jar文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.6
我使用的版本是：1.7.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp已通过pip install pyhanlp安装
已从官网上下载了data文件和hanlp-1.7.6-release.zip文件，并解压至了static文件夹下
HANLP_JAR_PATH和HANLP_ROOT_PATH已配置为static
运行from pyhanlp import *，报错ValueError: 配置错误: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static 不是jar文件"
pyhanlp配置问题：HANLP_JAR_PATH已配置，报错不是jar文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [×] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

pyhanlp已通过pip install pyhanlp安装
已从官网上下载了data文件和hanlp-1.7.6-release.zip文件，并解压至了static文件夹下
HANLP_JAR_PATH和HANLP_ROOT_PATH已配置为static

运行from pyhanlp import *，报错ValueError: 配置错误: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static 不是jar文件"
pyhanlp的配置问题：HANLP_JAR_PATH，不是jar文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.6
我使用的版本是：1.7.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp已通过pip install pyhanlp安装
已从官网上下载了data文件和hanlp-1.7.6-release.zip文件，并解压至了static文件夹下
HANLP_JAR_PATH和HANLP_ROOT_PATH已配置为static

运行from pyhanlp import *，报错ValueError: 配置错误: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static 不是jar文件
 

"
短语提取：当左熵或右熵都是0时，score为NaN,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
对某篇文章进行关键短语提取时，发现短语的score都是NaN，跟进发现是词语的左熵或右熵都是0导致的

### 触发代码
从 MutualInformationEntropyPhraseExtractor.extractPhrase(text, size)  ->  occurrence.compute()
```
package com.hankcs.hanlp.corpus.occurrence;
public class Occurrence
{
...
    /**
     * 输入数据完毕，执行计算
     */
    public void compute()
    {
        entrySetPair = triePair.entrySet();
        double total_mi = 0;
        double total_le = 0;
        double total_re = 0;
        for (Map.Entry<String, PairFrequency> entry : entrySetPair)
        {
            PairFrequency value = entry.getValue();
            value.mi = computeMutualInformation(value);
            value.le = computeLeftEntropy(value);
            value.re = computeRightEntropy(value);
            total_mi += value.mi;
            total_le += value.le;
            total_re += value.re;
        }

        for (Map.Entry<String, PairFrequency> entry : entrySetPair)
        {
            PairFrequency value = entry.getValue();
            // 问题出在下面这句，当total_le或total_re为0时，score为NaN
            // 因对左右信息熵不太了解，不确定下面的处理方式是否可行：
            // 给分母加一个足够小的数，例如：value.score = value.mi / total_mi + value.le / (total_le+0.0001)+ value.re / (total_re+0.0001);
            value.score = value.mi / total_mi + value.le / total_le+ value.re / total_re;   // 归一化
            value.score *= entrySetPair.size();
        }
    }
}
```
"
Nature is not concurrent safe. Change TreeMap to ConcurrentHashMap,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
Nature 这个类不是线程安全的，会在并发调用时产生服务Halt 的问题。
将Nature 中的 TreeMap 改成了 ConcurrentHashMap ，解决掉这个问题

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
Which kind of model is better for keyword-set classification?,"There exists a similar task that is named text classification.

But I want to find a kind of model that the inputs are keyword set. And the keyword set is not from a sentence.

For example:
```
input [""apple"", ""pear"", ""water melon""] --> target class ""fruit""
input [""tomato"", ""potato""] --> target class ""vegetable""
```

Another example:
```
input [""apple"", ""Peking"", ""in summer""]  -->  target class ""Chinese fruit""
input [""tomato"", ""New York"", ""in winter""]  -->  target class ""American vegetable""
input [""apple"", ""Peking"", ""in winter""]  -->  target class ""Chinese fruit""
input [""tomato"", ""Peking"", ""in winter""]  -->  target class ""Chinese vegetable""
```
Thank you."
按照《自然语言处理入门》随书代码复现时，发现自定义词典不能完全生效。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我按照《自然语言处理入门》的随书代码进行3.4节内容的复现，在3.4.1的“预测”一节中，使用范例代码调用先前生成的语言模型，并生成对应的词频时，仅1-gram模型的词频统计有效，ngram（实际上是2gram）模型的词频统计无效。

## 复现问题

### 步骤

1. 删除各词典缓存bin文件
2. 把随书对应章节的范例代码写入Jupyter Notebook，期间未出现报错。
3. 代码具体内容为：先加载自定义的简易语料库，然后创建NatureDictionaryMaker对象，训练得到并保存1gram与2gram模型为词典形式。然后重新调用这些模型时，输出却与实际不符，也不同于原书中的应有输出。
      
### 触发代码

```
import zipfile
import os
from pyhanlp import *
from pyhanlp.static import download, remove_file, HANLP_DATA_PATH


def test_data_path():
    """"""
    获取测试数据路径，位于$root/data/test，根目录由配置文件指定。
    :return:
    """"""
    data_path = os.path.join(HANLP_DATA_PATH, 'test')
    if not os.path.isdir(data_path):
        os.mkdir(data_path)
    return data_path


def ensure_data(data_name, data_url):
    root_path = test_data_path()
    dest_path = os.path.join(root_path, data_name)
    if os.path.exists(dest_path):
        return dest_path
    if data_url.endswith('.zip'):
        dest_path += '.zip'
    download(data_url, dest_path)
    if data_url.endswith('.zip'):
        with zipfile.ZipFile(dest_path, ""r"") as archive:
            archive.extractall(root_path)
        remove_file(dest_path)
        dest_path = dest_path[:-len('.zip')]
    return dest_path

def my_cws_corpus():
    data_root = test_data_path()
    corpus_path = os.path.join(data_root, 'my_cws_corpus.txt')
    if not os.path.isfile(corpus_path):
        with open(corpus_path, 'w', encoding='utf8') as out:
            out.write('''商品\t和\t服务
商品\t和服\t物美价廉
服务\t和\t货币''')
    return corpus_path

#CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')

def train_bigram(corpus_path, model_path):
    sents = CorpusLoader.convert2SentenceList(corpus_path) #加载语料库
    for sent in sents:
        for word in sent:
            word.setLabel('n') #标注
    maker = NatureDictionaryMaker()
    maker.compute(sents)
    maker.saveTxtTo(model_path)
    
def load_bigram(model_path):
    HanLP.Config.CoreDictionaryPath = model_path + '.txt' #1gram模型
    HanLP.Config.BiGramDictionaryPath = model_path + '.ngram.txt' #ngram模型
    CoreDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
    CoreBiGramTableDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary')
    print(CoreDictionary.getTermFrequency(""商品"")) # 获取对应词语的词频
    print(CoreBiGramTableDictionary.getBiFrequency(""商品"", ""和"")) #获取二元语法的频次

NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')
CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')
WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')
Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')
ViterbiSegment = JClass('com.hankcs.hanlp.seg.Viterbi.ViterbiSegment')
DijkstraSegment = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')
CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
Nature = JClass('com.hankcs.hanlp.corpus.tag.Nature')

corpus_path = my_cws_corpus()
model_path = os.path.join(test_data_path(), 'my_cws_model')
train_bigram(corpus_path, model_path)
load_bigram(model_path)
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
2
1
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
2
0
```

第二行的输出频数与实际不符，因为对应的词典my_cws_model.ngram.txt中的确记述了：
商品@和 1

因此输出的频数统计结果应当为1而不是0，这两个词也均在1gram模型词典中有出现，应当不是这方面的原因。

## 其他信息

使用的自定义词典都是由maker.saveTxtTo(model_path)得到的，未做任何改动。
如果ngram模型并不调用这个自定义词典，而是使用自带的词典，则词频统计并无问题
如果ngram词典中出现了1gram不曾出现的词，则其词频统计也会是0，但是我遇到的情况不属于此类。
我的系统是Windows10，语言环境为Python 3.7.4 anaconda环境
ngram模型生成的缓存文件：my_cws_model.ngram.txt.table.bin大小仅有1kb，我不清楚这是否正常

"
hanlp elasticsearch插件,"希望可以开源一个ealsticsearch插件，elasticsearch的分词插件ik,无法支持分布式的动态修改词库，希望可以有一套基于hanlp的elasticsearch插件，并且支持分布式。"
加载CoreDictionaryPath显示SSL和URL证书验证失败,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
当前最新版本号是：1.7.5
我使用的版本是：1.7.5

## 我的问题
使用最新版书籍P35页代码加载HanLP的核心词典库，出现SSL，及URL证书验证失败。代码来自ch02的utitil.py.
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)>

试着使用全局不检测SSL及URl未能解决

## 复现问题
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
添加代码以无视SSL证书检测，无效



### 代码复现

```
  from pyhanlp import *
# 全局取消证书验证

import ssl
ssl._create_default_https_context = ssl._create_unverified_context



def load_dictionary():
    '''
    加载HanLp中的mini词库
    :return: 一个set形式的词库
    '''
    IOUtil=JClass('com.hankcs.hanlp.corpus.io.IOUtil')
    path = HanLP.Config.CoreDictionaryPath.replace('.txt', '.mini.txt')
    dic = IOUtil.loadDictionary([path])
    return set(dic.keySet())

if __name__ == '__main__':
    dic = load_dictionary()
    print(len(dic))
    print(list(dic)[0])
```
### 期望输出

返回一个set的形式的词库



## 其他信息

MAC Python3.7, Pycharm

"
tfidf，idf的数据可以通过加载idf文件得到,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
tfidf，idf的数据可以通过加载idf文件得到。

```
    /**
     * 加载自定义idf文件
     *
     * @param idfPath
     */
    public void loadIdfFile(String idfPath){
        String line = null;
        boolean first = true;
        try
        {
            idf  = new HashMap<String, Double>();
            BufferedReader bw = new BufferedReader(new InputStreamReader(IOUtil.newInputStream(idfPath), ""UTF-8""));
            while ((line = bw.readLine()) != null)
            {
                if (first)
                {
                    first = false;
                    if (!line.isEmpty() && line.charAt(0) == '\uFEFF')
                        line = line.substring(1);
                }
                String lineValue[] = line.split("" "");
                idf.put(lineValue[0],Double.valueOf( lineValue[1]));
            }
            bw.close();
        }
        catch (Exception e)
        {
            logger.warning(""加载"" + idfPath + ""失败，"" + e);
            throw new RuntimeException(""载入反文档词频文件"" + idfPath + ""失败"");
        }

    }
```


"
欢迎参加2019 HanLP技术交流会,"基于深度学习的HanLP2.0将在第二届问道崂山·人工智能与大数据高峰论坛上正式发布，HanLP2.0将有多项革命性的突破，诚邀各位开发者莅临论坛交流心得与体会。

论坛将于:2019年12月27日上午9:00，在青岛崂山区海天大剧院酒店召开。

详细地址:青岛市崂山区云岭路5号，海天大剧院酒店。

需要在会上演讲和安排住宿的人员请与论坛组委会联系

联系人:薛挣(电话:18605325520)

钟佩佩(150-6688-0317)

详细介绍：http://www.hdb-cdn4.cn/party/7pnca.html​"
pyhanlp/tests/book/ch02/dat.py的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

python: 3.7.3
jpype: 0.7.0
运行pyhanlp/tests/book/ch02/dat.py出现错误。

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/22760903/71138277-e8150180-2245-11ea-9ac3-9f5dda41fc87.png)

"
test中demo的例子在pycharm报错 ,"我使用的版本是：hanlp-1.7.5.jar   基于python3.7的anaconda
12月17日 通过pip install pyhanlp包
如截图所示，在copy学习 pyhanlp demo 代码上，pycharm报错
![image](https://user-images.githubusercontent.com/17291290/70996821-8f8f1880-210e-11ea-8cc3-bd014d172aad.png)

运行的是demo中的例行代码https://github.com/hankcs/pyhanlp/blob/master/tests/demos/demo_sentiment_analysis.py
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：hanlp-1.7.5.jar
我使用的版本是：hanlp-1.7.5.jar  

## 我的问题
如截图所示，在copy学习 pyhanlp demo 代码上，pycharm报错

## 复现问题

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
CoreStopWordDictionary.add() 如何持久化,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

CoreStopWordDictionary.add() 如何将停用词写入stopwords.txt？
还是说只能手动修改stopword.txt或者用文件流的方式。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  public class DemoStopWord
{
    public static void main(String[] args) throws Exception {
        System.out.println(CoreStopWordDictionary.contains(""一起""));
        stopwords();
        add();
        stopwords();
    }

    public static void add() {
        boolean add = CoreStopWordDictionary.add(""一起"");
        CoreStopWordDictionary.reload();
        System.out.println(""add = "" + add);
    }
    public static void stopwords() throws Exception {
        String con = ""我们一起去逛超市，我们先去城西银泰，我们再去城南银泰。然后我们再一起回家"";
        List<String> strings = HanLP.extractKeyword(con, 5);
        System.out.println(""strings = "" + strings);
    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
我期望是调用CoreStopWordDictionary.add(""一起"");后，将停用词""一起""加入停用词词典。然后调用CoreStopWordDictionary.reload();重新加载字典和缓存达到动态修改停用词词典的功能。
```
false
strings = [一起, 银泰, 城, 再, 西]
add = true
strings = [城, 西, 银泰, 先, 再]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
CoreStopWordDictionary.add(""一起"");并没有将“一起”写入停用词词典，只是在内存中。调用CoreStopWordDictionary.reload();以后停用词字典里没有“一起”
```
false
strings = [一起, 银泰, 城, 再, 西]
add = true
strings = [一起, 银泰, 城, 再, 西]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
所以说现在还是要手动修改stopword.txt或者是使用文件流的形式来修改对么？那CoreStopWordDictionary.add()和CoreStopWordDictionary.remove()这两个方法应该怎么用？
"
hanlp segment,"安装成功后，运行 hanlp segment 

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f649eb21b6e, pid=40945, tid=140070366816064
#
# JRE version: OpenJDK Runtime Environment (7.0_91) (build 1.7.0_91-mockbuild_2015_11_20_16_53-b00)
# Java VM: OpenJDK 64-Bit Server VM (24.91-b01 mixed mode linux-amd64 compressed oops)
# Derivative: IcedTea 2.6.2
# Distribution: CentOS Linux release 7.1.1503 (Core) , package rhel-2.6.2.3.el7-x86_64 u91-b00
# Problematic frame:
# V  [libjvm.so+0x616b6e]
#
# Core dump written. Default location: /root/UPA-Python-1.0.0-1/core or core.40945
#
# An error report file with more information is saved as:
# /tmp/jvm-40945/hs_error.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
#"
排重库,你好，这边考虑过排重库的需求嘛，类似于毕业论文的那种相似度的计算，建议个一个库，然后把相似度高的列出来，或者java实现有什么好的实现方案嘛？
关于语义依存咨询,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
关于新书咨询
<!--以上属于必填项，以下可自由发挥-->

## 我的问题

大佬你好，刚入手了你的新书，那个思维导图特别棒，思维导图中对语义依存里介绍的算法在哪儿有介绍吗，是否有开源的可以拜读学习？
"
 raise ValueError( ValueError: 配置错误: 数据包 C:/Users/15761/Anaconda3/envs/python38/Lib/site-packages/pyhanlp/static\data 不存在，请修改配置文件中的root,"## 我的问题
报错信息：
(python38) C:\Users\15761>hanlp
Traceback (most recent call last):
  File ""c:\users\15761\anaconda3\envs\python38\lib\runpy.py"", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\15761\anaconda3\envs\python38\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\15761\Anaconda3\envs\python38\Scripts\hanlp.exe\__main__.py"", line 4, in <module>
  File ""c:\users\15761\anaconda3\envs\python38\lib\site-packages\pyhanlp\__init__.py"", line 145, in <module>
    _start_jvm_for_hanlp()
  File ""c:\users\15761\anaconda3\envs\python38\lib\site-packages\pyhanlp\__init__.py"", line 75, in _start_jvm_for_hanlp
    raise ValueError(
ValueError: 配置错误: 数据包 C:/Users/15761/Anaconda3/envs/python38/Lib/site-packages/pyhanlp/static\data 不存在，请修改配置文件中的root

我已经按下面链接里的做了,把`data`
https://github.com/hankcs/pyhanlp/wiki/%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AE
![image](https://user-images.githubusercontent.com/39977054/70859711-b7d21800-1f52-11ea-88f3-062572949e2c.png)

但是还是不行
![image](https://user-images.githubusercontent.com/39977054/70859720-d6381380-1f52-11ea-8fd5-6d8243eaebde.png)



"
繁体转简体字典t2s.txt中 芝士=乾酪是不是错的？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
对于自训练模型CRFNERecognizer的recognize 方法不能识别出正确的nertags，但是PerceptronNERecognizer却可以,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
首先非常感谢您这个项目，我在用CRF的方式训练模型的时候，当调用CRFNERecognizer的recognize 方法不能识别出复合词正确的nertags，识别出来的都是O，实际上我已经按照人民日报的格式用中括号标记出来了，可当我用感知机的方式，即PerceptronNERecognizer 这个对象  是可以识别出我的复合词的正确nertags的，稍后我把CRF的代码列出来，请您看下是否有问题。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先 看下我的标记语料 如下：
[潘婷/nr 氨基酸/n 洗/v 护/v 套装/n 乳液/n 修护/v 洗发/v 水/n]/baj 500/m ml/q 搭配/v [3/m 分钟/q 奇迹/n 护发素/nz]/baj 70/m ml 新旧/q 包装/n 随机/d 发货/v ，/w 价格/n ￥/v 59.90/m ，/w 适合/v 头皮/n ：/w 干性/n ，/w 功效/n ：/w 水/n 润/v 。/w
这是训练词性模型和ner模型的语料，分词模型的语料 是去掉 词性标记和中括号之后的 单词
2. 然后 用hanlp提供（CRFSegmenter，CRFPOSTagger，CRFNERecognizer三个接口对象）的CRF接口分别训练分词模型，词性模型以及NER模型
3. 接着 用CRFNERecognizer 的recognize  方法 期望获取每个单词对应的nertag

### 触发代码

```
    str1 = '潘婷氨基酸洗护套装乳液修护洗发水500ml搭配3分钟奇迹护发素70ml 新旧包装随机发货，价格￥59.90，适合头皮：干性，功效：水润。'
    seg = CRFSegmenter(cws_path)
    wordList = seg.segment(str1)
    tagger = CRFPOSTagger(pos_path)
    pos_tags = tagger.tag(wordList)
    customNerTags = ['baj']
    ner = CRFNERecognizer(train_model+"".txt"",customNerTags)
    arrNer = ner.recognize(list(wordList),list(pos_tags))
    ner_tags = list([p for p in list(arrNer)])
    print(ner_tags)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
['B-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'E-baj', 'O', 'O', 'O', 'B-baj', 'M-baj', 'M-baj', 'E-baj', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

以上期望输出的结果是我用感知机的相关对象（PerceptronSegmenter，PerceptronPOSTagger，PerceptronNERecognizer）输出的"
1.6.2版本的尝试下载很多次了，就是下载不了，请问有下载过的吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
有bert 相关的类库么,有把bert 整合到hanlp里面的计划么(*_*)
jsp中调用分词器提示 HanLP cannot be resolved,"v1.7.5
在jsp中无法调用分词器，会提示 ：org.apache.jasper.JasperException: Unable to compile class for JSP: 
...
HanLP cannot be resolved
<%@ page contentType=""text/html;charset=UTF-8"" language=""java"" %>
<%@ page import=""ne.aaa"" %>
<%@ page import=""java.util.List"" %>
<%@ page import=""java.util.ArrayList"" %>
<%@ page import=""com.hankcs.hanlp.HanLP"" %>
<%
    List list = new ArrayList();  
   list = HanLP.segment(""***"");

%>
<html>
<head>
    <title>Title</title>
</head>
<body>
</body>
</html>


在调用HanLP.segment(""***"");的时候直接报错"
修复：加载自定义停用词文件无效,"- 修复：加载自定义停用词文件无效

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
打包后编译后字典怎么加载？路径怎么设置？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
我使用的版本是：<version>portable-1.7.5</version>

## 我的问题

      打包后编译后字典怎么加载？路径怎么设置？
      因为在本地idea测试的时候，可以将root设置成对应的路径，而且是可以运行成功的，但是打包编译后怎么进行路径的设置呢？




"
在线演示与代码句法分析结果存在差异。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->
hanlp-1.7.5，master均存在此问题
## 我的问题
在线演示“http://www.hanlp.com/?sentence=打开空调调高温度”
与源码结果不同
<!-- 请详细描述问题，越详细越可能得到解决 -->
对比在线演示和master，hanlp-1.7.5中的DemoDependencyParser的依存分析结果，解析句子“打开空调调高温度”，得到的结果是不同的，是使用了不同模型吗
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改任何
master与jar包结果。
1	打开	打开	v	v	_	0	核心关系	_	_
2	空调	空调	n	n	_	1	动宾关系	_	_
3	调高	调高	v	v	_	4	定中关系	_	_
4	温度	温度	n	n	_	1	动宾关系	_	_
在线演示结果
![图片20191207150048](https://user-images.githubusercontent.com/27552782/70370454-7ca46900-1902-11ea-8551-723f55489536.png)
"
import pyhanlp时报错（ImportError: numpy.core.multiarray failed to import）,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：0.1.57
我使用的版本是：0.1.57

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在根据书上步骤安装好之后，import pyhanlp时报错
C:\Users\ZzT>python
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyhanlp
D:\Anaconda3\lib\site-packages\numpy\core\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:
D:\Anaconda3\lib\site-packages\numpy\.libs\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll
D:\Anaconda3\lib\site-packages\numpy\.libs\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll
  stacklevel=1)
ImportError: numpy.core.multiarray failed to import
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 12, in <module>
    from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM, java, \
  File ""D:\Anaconda3\lib\site-packages\jpype\__init__.py"", line 17, in <module>
    from ._jpackage import *
  File ""D:\Anaconda3\lib\site-packages\jpype\_jpackage.py"", line 19, in <module>
    import _jpype
ImportError: numpy.core.multiarray failed to import


经查询百度，初步判定是numpy版本不匹配造成的，但是尝试了numpy1.16.0,1.16.5,1.17.4 之后，均没有解决，求晗哥解答，非常感谢





 

"
java环境已经配置好，运行hanlp时就是找不到java,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
java环境应该是配置好了， JDK 13
C:\Users\星回十九>java -version
java version ""13.0.1"" 2019-10-15
Java(TM) SE Runtime Environment (build 13.0.1+9)
Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)

在使用pyhanlp时就出现了下面的问题
>>> from pyhanlp import *
找不到Java，请安装JDK8：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
是否前往 https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html ？(y/n)

这是什么原因啊？


"
添加分析字典时，词频输入项应如何理解并实际应用,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
首页演示代码：
8. 用户自定义词典 
中的演示语句：
  System.out.println(CustomDictionary.add(""单身狗"", ""nz 1024 n 1""));
其中 1024 这个词频，应该如何理解及使用？

假设实际中有大文本段 txt，因为业务领域问题，我目前有一个专有名词 word，需要添加入动态字典。那我当然可以人为的定义一个它的词性，但词频我如何能知道呢？而这个字典，我是拿去分析txt，包括以后可能出现的 txt 的。如果按照词频的汉语意思，就是词出现的次数。实际中我连文本都在业务系统一直动态增加着，我显然不可能知道它会出现多少次吧。

故而，有如上问题。


## 其他信息

不知道是否能联系到 “蝴蝶效应” 站点的管理员，吐槽一下那个站点。
1，不明原因的个人无法注册，提示 ：ip地址已经注册，联系管理员。中国有独立IP的个人有几个啊，我们公司对外都没有独立IP吧，这个判定限制，作为个人，我都不知道怎样处理。是联系bbs管理员呢，还是联系持有这个IP的管理员，还是我们这栋大厦的物业团队。
2，github 似乎可以授权登录，但也很奇怪，我已经弹出窗口授权成功了，但没有直接登录上去，弹出一个登录/注册的层，我也不知道点哪个，反正点登录，登录不了，点注册，又回到吐槽1描述的情况。哎，难办。

"
"四川发文取缔全部不合规p2p, 不能分开""合规""和p2p","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        String words = ""四川发文取缔全部不合规P2P"";
        System.out.println(NLPTokenizer.segment(words));
```
### 期望输出
```
四川 发文 取缔 全部 不 合规 P2P
```

### 实际输出
```
[四川/n, 发文/v, 取缔/v, 全部/n, 不/d, 合规p2p/a]
```

## 其他信息
CoreNatureDictionary.ngram.txt中加入了二元句法：
合规@p2p 200
合规@P2P 200
仍不能被分开，不知道是不是因为训练语料没有这样的数据导致(搜索了data/test/下的文件没搜到＂p2p"")．

想知道有什么办法解决．


"
HanLP多实例魔改中,"由于早期设计局限，目前HanLP的`CustomDictionary`、`CoreDictionary`、`CoreBiGramTableDictionary`等都是静态资源类。而一些应用场景要求加载不同的词典，比如同一个JVM中不同用户实例，或者不同领域下加载不同的bigram模型。由于个人时间有限，这个功能让大家久等了。

现在，所有静态资源类正在逐步改造中。目前的进度如下：

- [x] `CustomDictionary`重构完毕
  - 如果你不需要多实例，无需任何改动，1.x保持前向兼容
  - 如果你需要多实例，可以为分词器`segment`或`analyzer`创建一个新的`DynamicCustomDictionary`实例，并且调用该实例的`insert`方法。
    - 即`segment.customDictionary = new DynamicCustomDictionary(""词典1.txt"", ""词典2.txt"")`
    - 然后`segment.customDictionary.insert`
    - 参考[demo](https://github.com/hankcs/HanLP/blob/74e6d7457b02ab872aa24c8476bf0b4449d8650e/src/test/java/com/hankcs/demo/DemoCustomDictionary.java#L70)

- [ ] `CoreDictionary`重构中
- [ ] `CoreBiGramTableDictionary`重构中

"
修改：加载自定义停用词文件无效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
加载自定义停用词文件无效
```
CoreStopWordDictionary.load(""data/dictionary/custom/CustomStopWords.txt"", false);
```

### 触发代码

```
    /**
     * 加载另一部停用词词典
     * @param coreStopWordDictionaryPath 词典路径
     * @param loadCacheIfPossible 是否优先加载缓存（速度更快）
     */
    public static void load(String coreStopWordDictionaryPath, boolean loadCacheIfPossible)
    {
        ByteArray byteArray = loadCacheIfPossible ? ByteArray.createByteArray(coreStopWordDictionaryPath + Predefine.BIN_EXT) : null;
        if (byteArray == null)
        {
            try
            {
                dictionary = new StopWordDictionary(HanLP.Config.CoreStopWordDictionaryPath);
                DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(HanLP.Config.CoreStopWordDictionaryPath + Predefine.BIN_EXT)));
                dictionary.save(out);
                out.close();
            }
            catch (Exception e)
            {
                logger.severe(""载入停用词词典"" + HanLP.Config.CoreStopWordDictionaryPath + ""失败""  + TextUtility.exceptionToString(e));
                throw new RuntimeException(""载入停用词词典"" + HanLP.Config.CoreStopWordDictionaryPath + ""失败"");
            }
        }
        else
        {
            dictionary = new StopWordDictionary();
            dictionary.load(byteArray);
        }
    }

```
### 修改后的代码


```
    /**
     * 加载另一部停用词词典
     * @param coreStopWordDictionaryPath 词典路径
     * @param loadCacheIfPossible 是否优先加载缓存（速度更快）
     */
    public static void load(String coreStopWordDictionaryPath, boolean loadCacheIfPossible)
    {
        ByteArray byteArray = loadCacheIfPossible ? ByteArray.createByteArray(coreStopWordDictionaryPath + Predefine.BIN_EXT) : null;
        if (byteArray == null)
        {
            try
            {
                // HanLP.Config.CoreStopWordDictionaryPath 改为 coreStopWordDictionaryPath
                dictionary = new StopWordDictionary(coreStopWordDictionaryPath);
                DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(coreStopWordDictionaryPath + Predefine.BIN_EXT)));
                dictionary.save(out);
                out.close();
            }
            catch (Exception e)
            {
                logger.severe(""载入停用词词典"" + coreStopWordDictionaryPath + ""失败""  + TextUtility.exceptionToString(e));
                throw new RuntimeException(""载入停用词词典"" + coreStopWordDictionaryPath + ""失败"");
            }
        }
        else
        {
            dictionary = new StopWordDictionary();
            dictionary.load(byteArray);
        }
    }

```
"
您好，我想请教一下关于从网络上公开的98年人民日报语料库到HanLP自带词典的处理过程,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1、在网络上有公开的98年人民日报语料库，有已经标注的和未标注的，我查看之前的lssues,有人说过网上98年的人民日报语料是由北大人工联合标注的，请问：最原始的新闻语料是文章，整个标注过程是怎样的？将所有的新闻文章收集起来，然后进行人工分词并且标注词性，然后按照规范整理后发布，就变成我们看到的网上的带有标注的公开语料库了？
2、如果第一个问题是人工分词并标注的，那现在我们已经有了被人工分词并标注好了的新的98年人民日报语料库（**下面第一张图**）
![image](https://user-images.githubusercontent.com/43544445/70107064-71420b00-1680-11ea-95f6-51168be49915.png)
经过怎样的处理得到HanLP中所用到的txt词典，例如mini.txt（**下面第二张图**）
![image](https://user-images.githubusercontent.com/43544445/70107614-f4b02c00-1681-11ea-80f9-c06f8627534f.png)
就是由被人工分词并标注好了的新的98年人民日报语料库  到  HanLP中所用到的txt词典，例如mini.txt或HanLP中其他的词典     之间经过了怎样的处理过程，是脚本程序吗？如果是脚本程序的话，在HanLP包中有源码吗？若没有的话可以给我讲一下思路吗？ HanLP中所用到的txt词典中的词频是怎样来的，脚本程序在最开始怎样统计每个单词的词频的？在最最开始就是用TF-IDF算法进行统计吗？
换句话说，我想问，从最原始的没有经过任何加工的生语料（也就是原文章）经过怎样的处理才可以变成可以被HanLP使用的词典文件，或者我想要自己制作专用语料库，从最开始的专用领域文章文本经过人工分词标注后**再**经过怎**样的处理**可以得到能够被HanLP使用的词典文件，并且可以将我自己的专用语料库对应的词典替换掉HanLP自带词典。
请求大神解答！十分感谢
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在线演示和本地运行分词词性不一致,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
当前最新版本号是：1.7.4
我使用的版本是：1.7.4

## 我的问题
本地分词的词性（＂需要＂这个词）和在线演示结果不一样:
本地分成了n,线上分成了vn(第一个""需要""),v（第二个""需要""）

## 复现问题
### 步骤

### 触发代码
        String words = ""当前阶段的配置需要立足防御，同时需要谨防今年强势板块在补跌风险"";
        System.out.println(NLPTokenizer.segment(words));
### 期望输出
[当前/n, 阶段/n, 的/u, 配置/n, 需要/v, 立足/n, 防御/n, ，/w, 同时/n, 需要/v, 谨防/v, 今年/t, 强势/n, 板块/n, 在/p, 补/v, 跌/v, 风险/n]

### 实际输出
[当前/n, 阶段/n, 的/u, 配置/n, 需要/n, 立足/n, 防御/n, ，/w, 同时/n, 需要/n, 谨防/v, 今年/t, 强势/n, 板块/n, 在/p, 补/v, 跌/v, 风险/n]

## 线上演示输出：
![深度截图_选择区域_20191203095146](https://user-images.githubusercontent.com/22389800/70013524-f101a400-15b2-11ea-8957-8bce0238f73c.png)




"
字节跳动被分开了，哈哈哈,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
import com.hankcs.hanlp.corpus.MSR;,1.7.5 代码里面没有定义相关的java文件了 - compile demo和book example 有相同错误，有碰到同样问题的吗？
HanLP可以使用OpenNLP训练得模型库吗?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"doc2vec计算文本相似度时,怎么才能返回全部，默认返回10个","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
git bash下命令无效,在cmd命令行下可以正常调用并使用功能hanlp segment，但在git bash下，输入句子并回车后没有任何反应
词性标注不准确，‘明确提出给的标注是名词’,"<img width=""855"" alt=""企业微信截图_15745009854716"" src=""https://user-images.githubusercontent.com/49904623/69476574-7e583200-0e16-11ea-9fe3-ec9d0c7f39a1.png"">
"
import pyhanlp网络报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于使用NaiveBayesClassifier进行文本情感分类,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Linux环境引用data数据包时报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.5
我使用的版本是：portable-1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在linux部署，在引用data数据包的时候发生下载错误，遇到两个问题：
1、我把data数据包是放置于/eyas/templete/ 目录下（/eyas为硬盘分区），而且hanlp.properties中的设置是 root=/eyas/templete/，但是却引用不到数据包。
2、基于问题1，程序触发重新下载操作，却下载不了数据包，报错内容：java.io.IOException: No file to download. Server replied HTTP code: 302

<i>本人是小白，如果问的问题太low，请见谅、稍微耐心指导下哈。谢谢！</i>

"
分句问题,"想要把句子
 ”然后跟您讲解一下车况和当地的优惠政策先生您看尾号5920是您本人的手机号吧“
分成下面俩段句子有什么办法吗
 然后跟您讲解一下车况和当地的优惠政策
 先生您看尾号5920是您本人的手机号吧
"
NLP分词对存在空格的句子进行分词时存在问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
NLP分词对存在空格的句子进行分词时存在问题，会将空格后的所有词分成一个词。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import *
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""郭德纲 刘德华 guodegang liudehua""
term_list = NLPTokenizer.segment(text)
print(term_list)    
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[郭德纲/nr, 刘德华/nr, guodegang/nx, liudehua/nx]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[郭德纲 刘德华 guodegang liudehua/nr]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
代码格式化,@hankcs  代码可以格式化下吗
所有的分词器运行一段时间后，将数字识别成nz,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.5
我使用的版本是：portable-1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

PerceptronLexicalAnalyzer   ViterbiSegment 等分词器在运行一段时间后，将数字识别成nz

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先 以ViterbiSegment分词器为例

### 触发代码

```
    private static Segment segment = HanLP.newSegment().enableOffset(true);

    public List<EntityResult> resolve(String sentence) {
        List<EntityResult> entities = new ArrayList<>();
        List<Term> termList = segment.seg(sentence);
        log.info(""QuantityResolver termList={}"",termList);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
termList=[1/m, 加/v, 2/m]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
termList=[1/nz, 加/v, 2/nz]
```

## 其他信息

此resolve(String sentence)接口1s执行1-5次，1个小时后就会识别错误

"
Infinite recursivon ?,https://github.com/hankcs/HanLP/blob/56058d68290578931267fd31cdfad88c31d75fa2/src/main/java/com/hankcs/hanlp/mining/cluster/Cluster.java#L140-L150
报错java.lang.NoClassDefFoundError: com/hankcs/hanlp/HanLP,如题，我安装了pyhanlp后，运行HanLP = JClass('com.hankcs.hanlp.HanLP')就报题目的错误，请问这个错是什么原因，要怎么解决呢？小白一枚，请大家棒棒忙解答一下
"Ch03的ngram_segment.py方法print(CoreBiGramTableDictionary.getBiFrequency(""商品"", ""和""))返回频次不对","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.5
我使用的版本是：hanlp-1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

直接git clone到本地的pyhanlp，运行Ch03的ngram_segment.py，返回的1-gram频次为2，2-gram频次为0，Java版的输出是正确的，返回【商品】的词频：2，【商品@和】的频次：1

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
未对代码做修改

### 步骤

1. 直接在Pycharm中运行的ngram_segment.py

### 触发代码

```
   print(CoreDictionary.getTermFrequency(""商品""))
   print(CoreBiGramTableDictionary.getBiFrequency(""商品"", ""和""))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出1-gram频次为2，2-gram频次为1
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出1-gram频次为2，2-gram频次为0
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![微信截图_20191108212057](https://user-images.githubusercontent.com/54297119/68480206-37dbd280-026f-11ea-92f3-10fb5df66af3.png)
![微信截图_20191108212350](https://user-images.githubusercontent.com/54297119/68480210-39a59600-026f-11ea-89e6-5ccc0ab56201.png)

"
安装过程出现的问题，以及最后安装成功的版本号，汇报一下,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp 0.1.50
我使用的版本是：pyhanlp 0.1.47

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
环境参数：
* win10
* jdk 1.8
* python 3.7

失败安装过程：
* 第一次：直接`pip install pyhanlp` （版本0.1.50），自动安装依赖jpype1（版本0.7.0），下载data文件执行`hanlp`报错：`startJVM() got an unexpected keyword argument 'convertStrings'`
* 第二次：`pip install pyhanlp=0.1.47`，`pip install jpype1=0.6.2`，执行报错：`AttributeError: module '_jpype' has no attribute 'setResource'`

成功过程：
* `pip install pyhanlp=0.1.47`
* `pip install jpype1=0.6.3`
"
HanLP加载自定义字典的Bug。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 在存在hanlp.properties配置文件的情况下,设置多个字典并用;分割.
例如:CustomDictionaryPath=a.txt;b.txt;c.txt;
    hanlp会启动绝对路径搜索的方式加载自定义字典.
2. 由于hanlp会尝试从第一个mainPath(寻找a.txt.bin),并在加载成功的情况下,立刻返回.
    导致b.txt;c.txt不会被加载.

### 触发代码

```
在CustomDictionary::public static boolean loadMainDictionary(String mainPath, String path[], DoubleArrayTrie<CoreDictionary.Attribute> dat, boolean isCache)方法中.

logger.info(""自定义词典开始加载:"" + mainPath);
        if (loadDat(mainPath, dat)) return true;
```

"
运行本书ch02 evaluate_cws 会报错,"运行后报错 错误为：
jpype._jclass.NoClassDefFoundError: java.lang.NoClassDefFoundError: com/hankcs/hanlp/HanLP"
没有找到python版本的代码，没有找到在书33页中提及的aipf_law.py文件,"<!--
1.7.5版本的，没有找到书中提及的py代码
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
     1.7.5版本的，没有找到书中提及的py代码
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
anaconda环境下安装pyhanlp的错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：python3.7
我使用的版本是：python3.7

<!--以上属于必填项，以下可自由发挥-->

## 出现了如下所示的问题
![image](https://user-images.githubusercontent.com/53120545/68183221-b36f2280-ffd6-11e9-91d0-8b9824b05c2c.png)

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在python中有提供将doc2vec保存并加载的方法吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在python中，通过word2vec加载预先训练好的词向量模型，然后加载待查询的文档后，生成文档向量doc2vec，该对象有提供保存和加载的方法吗？若可以，在之后的使用中，可以不加载word2vec，只加载doc2vec，就可以进行文本相似度计算吗？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import *
WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = 'E:/data/word2vec.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)
docs = [""doc1"",  ""doc2"", ""doc3"",  ""doc4"", ""doc5""]
for idx, doc in enumerate(docs):
    doc2vec.addDocument(idx, doc)

运行上述代码后，有提供doc2vec对象保存的方法吗？，若可以，在之后的使用中，可以不加载word2vec，只加载doc2vec，就可以进行文本相似度计算吗？
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
有提供doc2vec对象保存和加载的方法吗？
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Create arrayList,"import java.util.Scanner;

public class RuangKelas {
    public static void main(String[] args) {

        // Membuat Array dan Scanner
        String[][] meja = new String[2][3];
        Scanner scan = new Scanner(System.in);

        // mengisi setiap meja
        for(int bar = 0; bar < meja.length; bar++){
            for(int kol = 0; kol < meja[bar].length; kol++){
                System.out.format(""Siapa yang akan duduk di meja (%d,%d): "", bar, kol);
                meja[bar][kol] = scan.nextLine();
            }
        }

        // menampilkan isi Array
        System.out.println(""-------------------------"");
        for(int bar = 0; bar < meja.length; bar++){
            for(int kol = 0; kol < meja[bar].length; kol++){
                System.out.format(""| %s | \t"", meja[bar][kol]);
            }
            System.out.println("""");
        }
        System.out.println(""-------------------------"");
    }
}"
Prevent compile fail after JDK8,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

使用 `buffer` 替换了代码中的 `_`，否则 `JDK8` 之后的 `JDK` 会编译失败，删除了一个没有用到且会引起编译失败的文件。

"
请问同义词字典在lucene中如何使用？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
http://www.hankcs.com/program/java/lucene-synonymfilterfactory.html
在网上看到这篇文章，但是太老了，而且现在的同义词字典格式也不一样了。麻烦介绍一下hanLp中的同义词如何与lucene结合使用？谢谢。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
jpype._jclass.IllegalArgumentException: java.lang.IllegalArgumentException: 错误的模型类型: 传入的不是分词模型，而是 POS 模型,"问题：
   pyhanlp 按照教程 导入自己训练的cws报错，是什么原因呢？
代码如下：
CRFLexicalAnalyzer = JClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
analyzer = CRFLexicalAnalyzer(r""C:\Users\gy\Desktop\pku98\cws.txt"")
print(analyzer.analyze(""17kbav995%""))"
python 如何导入词典训练自定义crf模型。,大神们好，弱弱问下，python 如何导入词典训练自定义crf模型？训练完之后如何导入呢？求demo谢谢~
想问下后续会有错字纠错能力,想问下后续会有错字纠错能力
CRFLexicalAnalyzer分词索引模式和非索引模式结果一样,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.5
我使用的版本是：1.7.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
CRFLexicalAnalyzer分词索引模式下分词结果和非索引模式结果一致
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
将IndexTokenizer类中
public static final Segment SEGMENT = HanLP.newSegment().enableIndexMode(true);
修改为
public static final Segment SEGMENT = HanLP.newSegment(""crf"").enableIndexMode(true);
2. 然后……
执行DemoIndexSegment的main方法
3. 接着……

### 触发代码

```
   List<Term> termList = IndexTokenizer.segment(""主副食品"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

        System.out.println(""\n最细颗粒度切分："");
        IndexTokenizer.SEGMENT.enableIndexMode(1);
        termList = IndexTokenizer.segment(""主副食品"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
主副食品/n [0:4]
主副食/j [0:3]
副食品/n [1:4]
副食/n [1:3]
食品/n [2:4]

最细颗粒度切分：
主副食品/n [0:4]
主副食/j [0:3]
主/ag [0:1]
副食品/n [1:4]
副食/n [1:3]
副/b [1:2]
食品/n [2:4]
食/v [2:3]
品/ng [3:4]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
主副食品/n [0:4]

最细颗粒度切分：
主副食品/n [0:4]

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
 分类训练的目录结构能否支持一个文件里头包含很多个文章的?," /**
     * 用UTF-8编码的语料训练模型
     *
     * @param folderPath  用UTF-8编码的分类语料的根目录.目录必须满足如下结构:<br>
     *                    根目录<br>
     *                    ├── 分类A<br>
     *                    │   └── 1.txt<br>
     *                    │   └── 2.txt<br>
     *                    │   └── 3.txt<br>
     *                    ├── 分类B<br>
     *                    │   └── 1.txt<br>
     *                    │   └── ...<br>
     *                    └── ...<br>
     *                    文件不一定需要用数字命名,也不需要以txt作为后缀名,但一定需要是文本文件.


原支持格式如上

但是假设文本有几十万个的话,文件系统压力很大. 

所以能不能支持一下
一个目录有几个文件  文件名代表分类,  文件里头每一行就是一个文章.

经济.1
经济.2

体育.1
体育.2

军事.1
军事.2
军事.3

类似以上

万分感谢
"
java.lang.OutOfMemoryError: Java heap space,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：0.1.50
我使用的版本是：0.1.50*[pip安装]* (hanlp-1.7.5.jar,data-for-1.7.5)

## 我的问题

使用Python版本的hanlp分析红楼梦文本时（提取摘要操作）提示错误：
`Traceback (most recent call last):
  File ""D:\Files\python\MachineLearning\自然语言处理\hanlp分析红楼梦.py"", line 22, in <module>
    print(HanLP.extractSummary(text, 5))
jpype._jclass.java.lang.OutOfMemoryError: java.lang.OutOfMemoryError: Java heap space`

所用文本文件在https://github.com/ouening/MLPractice/blob/master/Red_Mansions_Anasoft.txt

## 其他信息
OS: win10 64bit
python: 3.7.2

"
word2vec 模型有1个G，无论设置多少内存给模型，都感觉不是一般的慢，无法使用。,我的word2vec 模型有1个G，发现运行docVectorModel.nearest 方法是在太慢，我应该如何提高模型训练好的，预测速度？现在大于 20秒以上，真痛心，压根没法用呢
hanlp.properties.in作用,"修改了hanlp.properties后，hanlp.properties.in的相对应字段要改吗？
这个.in文件作用是啥？是否是备份文件？"
分词时每个词性的含义是什么,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

##例如：小白很简单的一个问题，例如分次结果： [张亮/nr, 开/v, 了/ule, 一个/mq, 麻辣烫/nf, 店/n]，里面的nr、ule、mq代表什么词性含义，有没有一个统一的对应关系的词典。在网上没有找到统一的词性表示表可参考。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
中文文件名的自定义词库加载失败,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
hanlp-1.7.4-release

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

**中文文件名**的自定义词库加载失败（**详情见截图**）
**英文名**的词库可以加载成功，比如：CustomDictionary.txt可以加载成功。
然后我将**现代汉语补充词库.txt**改成**hdhybcck.txt**就可以加载成功。

系统：centos6.10
系统的语言使用的是 zh_CN.UTF-8
编程语言使用的是python：pyhanlp
java：openjdk version ""1.8.0_222""

hanlp.properties
root=/usr/local/python3/lib/python3.7/site-packages/pyhanlp/static/
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf;

## 复现问题



## 其他信息
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->


======================
摸索了半天问题已经解决了。
**原因是：我在本地解压了data-for-1.7.4.zip再上传到服务器导致的~**


"
依存句法模块是否有方法改进,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好，我发现依存句法分析模块如下两个问题。针对第一个问题，是否我可以自行做修改或训练或改进？针对第二个问题，想请问在线版本的配置是怎么样的。

### 1. 在线和离线都包含错误
```
hanlp parse <<< ""李克强主持召开经济形势座谈会""
1	李克强	李克强	nh	nr	_	2	主谓关系	_	_
2	主持	主持	v	v	_	0	核心关系	_	_
3	召开	召开	v	v	_	2	并列关系	_	_
4	经济	经济	n	n	_	5	定中关系	_	_
5	形势	形势	n	n	_	3	动宾关系	_	_
6	座谈会	座谈会	n	n	_	3	动宾关系	_	_
```
第5个“形势”的父节点应该是6而不是3，关系应该是定中

![case1.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypnuk99bj208v029wek.jpg)

查看了ltp的结果是正确的：
![case2_2.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypyc36m5j20cj02pglo.jpg)

### 2. 在线演示正确，离线版本有错误

```
hanlp parse <<< ""曹国请辞韩国法务部长官""
1	曹国	曹国	nh	nr	_	4	定中关系	_	_
2	请辞	请辞	v	v	_	4	定中关系	_	_
3	韩国法务部	韩国法务部	ni	nt	_	4	定中关系	_	_
4	长官	长官	n	n	_	0	核心关系	_	_
```
第一个""曹国""显示是定中关系(错误)，在线版本如下图，显示是主谓关系(正确)
![case2.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypuq7zp3j207u02awei.jpg)

感谢~









"
错误分词“伞护”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

sentence= `扮演“大伞护小伞”角色的县委书记被逮捕`

![badcase.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7yo7qainsj20d603caag.jpg)

说明
1. 在线演示和本地版本都存在这个badcase
2. 检查了下挂载的各个自定义词典，没有出现这个词
"
感知机分词器的结果和在线演示不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
目前在测试HanLP的感知机分词，发现默认配置下，一些结果与在线演示的结果不一致，而且效果不如在线演示的结果，请问是否在线演示用的分词模型有优化或者更新，是否会在近期的新版本中发布。


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
1. 测试句：能不能进前十
在线演示结果：
![B8C02F9288DA606A512811A552D1C861](https://user-images.githubusercontent.com/980318/66748351-8b017600-eeb9-11e9-953a-9bdb16a951ab.jpg)

1.7.4测试结果
`能不能/i 进前/v 十/m`

2.测试句：广场上有很多人在跳舞
在线演示结果：
![843B0807FDE9AEBBD8A725B04B84C7B3](https://user-images.githubusercontent.com/980318/66748564-0cf19f00-eeba-11e9-838b-76f38f864ddc.jpg)

1.7.4测试结果
`广场/n 上有/v 很多/m 人/n 在/p 跳舞/v`



## 其他信息
如果不会有新版模型发布，有没有什么办法修正分词。
已经尝试在线学习，但发现并不一定能影响分词的结果。
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"请问能否配置多个用户词典, 分词优先级顺序，用户词典1>核心词典>用户词典2","您好,感谢分享
      请问能否配置多个用户词典,分词优先级顺序，用户词典1>核心词典>用户词典2 ?
      如果配置不能实现, 代码层面有可能实现吗? 如果有可能实现，那我想自己试着改代码。如果能得到些许指导，十分感谢~"
您好，请问有语义角色标注功能么，SRL,
性能测试 DoubleArrayTrie 内存使用 > AhoCorasickDoubleArrayTrie >  Bintrie,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.7.2

## 我的问题
按照原理 
   对DAT 、 ACDat 、Bintrie 进行性能测试

测试文本 : 

""最近正在做一个自己的NLP库，刚起步的第一个问题就是字典的储存与查询。毫无疑问，最佳的数据结构是Trie树，同时为了平衡效率和空间，决定使用双数组Trie树。现在的问题是，双数组Trie树是一个压缩的Trie树"";

测试词典: 
CoreNatureDictionary.txt

JVM 堆设置: -Xms2g Xmx2g 

性能分析工具: JConsole  

测试结果 : 
按照原理 DAT 的内存使用应该是三者最小的
但目前测到的结果 DAT > ACDAT > Bintrie 
反而 Bintrie 堆内存使用最小

性能方面 : DAT 性能约等于 ACDAT > Bintrie 

### 触发代码

```
        String text = ""最近正在做一个自己的NLP库，刚起步的第一个问题就是字典的储存与查询。毫无疑问，最佳的数据结构是Trie树，同时为了平衡效率和空间，决定使用双数组Trie树。现在的问题是，双数组Trie树是一个压缩的Trie树"";

public void testACDAT() throws InterruptedException {
        Thread.sleep(10000);
        Runtime.getRuntime().gc();
        TreeMap<String, CoreDictionary.Attribute> map = new TreeMap<String, CoreDictionary.Attribute>();
        IOUtil.LineIterator iterator = new IOUtil.LineIterator(""data/dictionary/CoreNatureDictionary.txt"");
        while (iterator.hasNext())
        {
            String line = iterator.next().split(""\\s"")[0];
            map.put(line, new CoreDictionary.Attribute(Nature.n));
        }
        Runtime.getRuntime().gc();
        System.out.println("" Map 构建成功 !!!!!"");
        Thread.sleep(10000);

        //AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute> act = new AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute>();
        //DoubleArrayTrie<CoreDictionary.Attribute> act = new DoubleArrayTrie<CoreDictionary.Attribute>();
        BinTrie<CoreDictionary.Attribute> act = new BinTrie<CoreDictionary.Attribute>();
        long timeMillis = System.currentTimeMillis();
        act.build(map);
        System.out.println(""构建耗时："" + (System.currentTimeMillis() - timeMillis) + "" ms"");
        timeMillis = System.currentTimeMillis();
        for(int i = 0 ; i < 30000000; i ++){
            act.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>() {
                @Override
                public void hit(int begin, int end, CoreDictionary.Attribute value) {

                }
            });
        }
        System.out.println(""分词耗时："" + (System.currentTimeMillis() - timeMillis) + "" ms"");
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
hanlp和pyhanlp的dependency parse结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
当前最新版本号是：
我使用的版本是：

jar  1.7.4: lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/hanlp-1.7.4.jar
data 1.7.4: lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/data
config    : lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/hanlp.properties


<!--以上属于必填项，以下可自由发挥-->

## 我的问题

```
sent = ""中国11家主流快递公司宣布开启备战双11""
from pyhanlp import HanLP, JClass
sentence = HanLP.parseDependency(sent)
print([w.LEMMA for w in sentence.iterator()])
```
得到的结果是: ['中国', '11', '家主', '流', '快递公司', '宣布', '开启', '备战', '双', '11']

但是，直接在命令行中使用`hanlp segment <<< ""中国11家主流快递公司宣布开启备战双11""`(或hanlp parse)得到的结果是:
中国/ns 11/m 家/q 主流/n 快递公司/nis 宣布/v 开启/v 备战/vi 双/q 11/m

明显第二个是对的，也就是pyhanlp的结果为什么和hanlp不一致呢？


"
自建词典无法生成.txt.bin文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
将词典转换为csv文件后，无法正常分词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

因为我要对英文进行分词，所以我将词典转变为csv格式。但是转换完之后，无法分词。转换的原文件是""198901.txt"" ,该文件转换前，进行训练结果正常

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先将txt文件改为csv文件

修改后，每行数据如下
```
怀/Ng,揣/v,这/r,如泣如诉/i,的/u,呵护/vn,，/w
```
2. 然后按照下列代码训练，结果就不对


### 触发代码

```
public class DemoTrainCWS{

    public static void main(String[] args) throws IOException {

        PerceptronTrainer trainer = new CWSTrainer();
        PerceptronTrainer.Result result = trainer.train(
                ""data/ordinaryData/addSpu/1008-origin.csv"",
                ""data/myDemo/cws.bin""
        );

        System.out.printf(""准确率F1:%.2f\n"", result.getAccuracy());
        PerceptronSegmenter segment = new PerceptronSegmenter(result.getModel());
        System.out.println(segment.segment(""商品与服务""));
    }


}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
['商品','和','服务']
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
['商品和服务']
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
jpype调用错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
jpype调用报错：jpype._jclass.ArrayIndexOutOfBoundsException: 1208
整体报错：
Traceback (most recent call last):
  File ""entity_2.py"", line 123, in <module>
    main()
  File ""entity_2.py"", line 48, in main
    sentence = HanLP.parseDependency(line)
jpype._jclass.ArrayIndexOutOfBoundsException: 1208


## 复现问题
没有额外的操作，只是调用了句法分析这个接口。


### 触发代码
sentence = HanLP.parseDependency(line)


"
如何避免 阿莫西林 被分为 ”阿莫“ ”西林“？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：<version>portable-1.7.4</version>

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   HanLP.segment(sentence);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
  药品 阿莫西林
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
 实际代码输出： 药品 阿莫 西林 
  在线hanlp.com可以按期望分词，但是应用程序输出将 阿莫西林 分开了，  http://www.hanlp.com/?sentence=%E8%8D%AF%E5%93%81%E9%98%BF%E8%8E%AB%E8%A5%BF%E6%9E%97

 请问应用程序中 如何设置？

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
我可以用自己的训练集训练一个依存句法分析模型吗,
分词badcase“十一推送内容”，不知如何纠正,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
下载了1.7.4的数据包  HanLP.segment来分词“十一推送内容”，出现badcase，不知如何纠正

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

HanLP.segment(""十一推送内容"")

### 触发代码


### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
十一/推送/内容
```
### 实际输出


<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
十/一推/送/内容
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
线上demo的结果和本地结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：
hanlp-1.7.4.jar
python 3.6.9
mac osx 10.14

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

测试语句: `上海农商银行1600万股股权将被拍卖`
从[在线demo](http://hanlp.com/)得到的结果如下图:
![demo.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7e589ip3vj20j30lfgn1.jpg)

但是在本地得到的结果如下:
![local.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7e58nwuyij20gg05mjs6.jpg)

如何得到在线的这个结果呢？"
do refactor in segment package,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
Hi.
I do refactor in the ```com.hankcs.hanlp.seg``` package. 

- rename four abstract class with word ```Abstract``` ahead, and put them into the ```com.hankcs.hanlp.seg.base``` package;

- remove ```com.hankcs.hanlp.seg.common.Config``` class into ``com.hankcs.hanlp.seg.common``` package, package permission of this class has also been modified;

- some package permission of ```com.hankcs.hanlp.seg.base.AbstractSegment```'s method has been modified;

- mainly, I write a ```com.hankcs.hanlp.seg.Segment``` class with a nested class ```Builder``` , and a ```com.hankcs.hanlp.seg.common.SegmentTypeEnum``` class, to provide a easier way to know what seg method can be used.

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
同一个词在不同的句子中分词结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：`<version>portable-1.7.4</version>`


<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
同一个词在不同的句子中分词结果不一致（下面以""就是""这个词为例)

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

三个测试句子：""我就是我"", ""就是我"", ""你好就是了""
输出结果: ""我 就 是 我"", ""就 是 我"", ""你好 就是 了""
"
当地名第二位是方位词时 分词不准确,"* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：hanlp-1.7.4
我使用的版本是：hanlp-1.5.0

## 我的问题
在用户自定义词典文本中配置了地名“淮南 ns 100“ 100是较大的词频 但当输入”6月28日淮南“时，不能得到”淮南“
分词结果为[6/m, 月/qt, 28/m, 日淮南/ns]

## 复现问题
Segment SEGMENT = HanLP.newSegment()
					.enableCustomDictionaryForcing(true)
					.enableNameRecognize(true)
					.enableTranslatedNameRecognize(true)
					.enableJapaneseNameRecognize(true)
					.enablePlaceRecognize(true)
					.enableOrganizationRecognize(true)
					.enablePartOfSpeechTagging(true);

### 步骤
1. 首先在\data\dictionary\custom下添加了新文本文件 使用utf-8编码 crlf换行
2. 然后在配置文件中添加""淮南 ns 100""

### 期望输出
期望分词结果为[6,月,28,日,淮南]

### 实际输出
实际分词结果为[6,月,28,日淮南]
没有分出地名淮南

## 其他信息
“肥东”、“肥西”等 第二个字是方位词的 不著名的 地名 都会分成“日肥东”，“日肥西”"
如何更细致的分割英文字符和空字符 以及汉字和数字组成的数量词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.4
我使用的版本是：portable-1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
运行如下程序后，文本中的 “3.一”会识别成一个词，英文字符的右括号和换行符\r\n识别成一个词，如何才能把他们分开呢。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  public static void main(String[] args) {
		
	String text = ""3.一位项目经理应该做下列哪一项？(C)\r\n"" ;
	    
	List<Term> term = HanLP.newSegment().enableOffset(true).enableIndexMode(true).enableIndexMode(1).seg(text);
		
	   System.out.println(term.toString());
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[3/m,./w,一/m, 位/q, 项目/n, 经理/n, 应该/v, 做/v, 下列/b, 哪/r, 一/m, 项/q, ？/w, (/w, C/nx, )/w, /w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[3.一/m, 位/q, 项目/n, 经理/n, 应该/v, 做/v, 下列/b, 哪/r, 一/m, 项/q, ？/w, (/w, C/nx, )
/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Spark使用HDFS词库分词，报java.lang.StackOverflowError,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->






## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.4
我使用的版本是：portable-1.7.4

## 我的问题

词库data目录已上传到hdfs上；

配置文件如下：
![image](https://user-images.githubusercontent.com/10821279/64748199-d97ec600-d543-11e9-8004-f8d410a365a2.png)

已经实现了IO适配器代码如下：
![image](https://user-images.githubusercontent.com/10821279/64747652-38434000-d542-11e9-9a7e-55a7889448b2.png)

## 复现问题

spark集群上一句简单的分词出现错误，代码如下：

![image](https://user-images.githubusercontent.com/10821279/64747583-fc0fdf80-d541-11e9-9f41-059c8c3583d7.png)

错误日志如下：

![image](https://user-images.githubusercontent.com/10821279/64747625-206bbc00-d542-11e9-9cd2-0cb391493602.png)

如何修改？

"
CharType 是不是有问题？字母都被标识为CT_SINGLE,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
建议把demo放在外面 ，而不是test里面，方便用户查找,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
使用gevent多进程启动flask报错Error occurred during initialization of VM Could not reserve enough space for object heap,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

你好，在使用gevent多进程启动flask服务时，在服务中有一个模块使用到hanlp中的文本相似度分析，调用方法如下述代码，启动服务时报错Error occurred during initialization of VM; Could not reserve enough space for object heap，若是将该模块从服务中移除，重新启动服务，服务可以正常启动。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import SafeJClass

@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))

class start_model_server:
    def __init__(self, port):
        self.port = port
    
    def start_new_server(self): 
        server = WSGIServer(('0.0.0.0', self.port), app)
        server.init_socket()
        server.start()
        server.start_accepting()
        server._stop_event.wait()
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    doc2vec = DocVectorModel(word2vec._proxy)
    from multiprocessing import Process
    ports = [6000,6001,6002,6003]
    for port in ports:
        s = start_model_server(port)
        p = Process(target=s.start_new_server)
        p.start()
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
服务正常启动
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Error occurred during initialization of VM
Could not reserve enough space for object heap
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
同义词多次出现会被覆盖,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
如果同一个词多次出现，会被替换，例如：Aa01A01=北京 北京人   Aa01A02=北京 北京天安门

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    for (Synonym synonym : synonymList)
                {
                    treeMap.put(synonym.realWord, new SynonymItem(synonym, synonymList, type));
                    // 这里稍微做个test
                    //assert synonym.getIdString().startsWith(line.split("" "")[0].substring(0, line.split("" "")[0].length() - 1)) : ""词典有问题"" + line + synonym.toString();
                }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
这里可以判断是否存在，存在则追加，反之新增
```
北京 北京人 北京天安门
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
北京 北京天安门
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
一是这是自己的追求,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
python版：
那可是夏皇东伯帝君的家族
[那/rzv, 可是/c, 夏皇/nr, 东/f, 伯/ng, 帝君/n, 的/ude1, 家族/n]
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
 segment = HanLP.newSegment().enableNameRecognize(True)
term_list = segment.seg(sentence)
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于词典编码的疑问,"同义词编码表 = # @ 分别表示什么意思
例如：
Aa01C05@ 众学生
Aa01C06# 妇孺 父老兄弟 男女老少 男女老幼
Aa01C07# 党群 干群 军民 工农兵 劳资 主仆 宾主 僧俗 师徒 师生 师生员工 教职员工 群体 爱国志士 党外人士 民主人士 爱国人士 政群 党政群 非党人士 业内人士 工农分子 军警民 党政军民
Aa01D01@ 角色
Aa02A01= 我 咱 俺 余 吾 予 侬 咱家 本人 身 个人 人家 斯人
词性后面的数字是什么意思
例如：
龙岗 A 14 B 6 X 1
一万遍	nz	9
一不压众	i	3
一不怕苦	l	10"
Python版安装失败,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
Python 3.7
pyhanlp v0.1.22
hanlp 1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

安装后初次运行，提示了以下错误：

`java.lang.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0
`

在Terminal查看hanlp状态，提示：
`
Traceback (most recent call last):
  File ""/usr/local/bin/hanlp"", line 6, in <module>
    from pyhanlp.main import main
  File ""/usr/local/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 122, in <module>
    _start_jvm_for_hanlp()
  File ""/usr/local/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 119, in _start_jvm_for_hanlp
    HANLP_JVM_XMX, convertStrings=True)
  File ""/usr/local/lib/python3.7/site-packages/jpype/_core.py"", line 219, in startJVM
    _jpype.startup(jvmpath, tuple(args), ignoreUnrecognized, convertStrings)
jpype._jclass.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0
`

我安装了最新的JDK，但仍无法运行。请问是什么原因？是和 #1244 同一个问题吗？"
自定义词性和语法分析器解析结果中的词性不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。



当前最新版本号是：1.7.4 protable.jar
我使用的版本是： 1.6.8 protable.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先: 在customerdictionary.txt 添加新词和词性，刷新缓存后，调用hanlp.seg（）方法
分词效果合理
2. 然后，调用dependancyPraser进行语法分析，发现pos结果和之前不一致
3. 接着……

### 触发代码

public static void main(String[] args)
    {

        CoNLLSentence sentence = HanLP.parseDependency(""免费房赠送积分吗"");
        System.out.println(HanLP.segment(""免费房赠送积分吗""));
        System.out.println(sentence);
}

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
[免费房/n, 赠送/v, 积分/n, 吗/y]
1	免费房	免费房	**n	n**	_	2	状中结构	_	_
2	赠送	赠送	v	v	_	0	核心关系	_	_
3	积分	积分	n	n	_	2	动宾关系	_	_
4	吗	吗	e	y	_	2	右附加关系	_	_

### 实际输出
[免费房/n, 赠送/v, 积分/n, 吗/y]
1	免费房	免费房	v	vd	_	2	状中结构	_	_
2	赠送	赠送	v	v	_	0	核心关系	_	_
3	积分	积分	n	n	_	2	动宾关系	_	_
4	吗	吗	e	y	_	2	右附加关系	_	_




## 其他信息
txt内容
：免费房 n 1
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
文本聚类ClusterAnalyzer报错：Null指针,"repeatedBisection参数调节过程中，com.hankcs.hanlp.mining.cluster.ClusterAnalyzer  207行会报空指针错误。
![image](https://user-images.githubusercontent.com/18278523/63823557-db168e80-c986-11e9-967f-8232c42cd3ab.png)
"
有JavaScript 版本吗？,希望出一个 JavaScript 版本
倒装句分析,"
“吃饭了吗你” 处理成 ”你吃饭了吗“

这种倒装句处理有什么建议吗"
关于自定义词典，在线学习 复合词及自定义词性标注的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

总的需求是自定义词性和复合词能够结合起来
例如下面一段话：
String text = ""腰酸、乏力、口干、目涩、手足心热、夜尿多、脉细或弱"";

使用的方法默认的分词结果为：
腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w 手足/n 心热/n 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

我希望的效果：
腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w  [手足心/n 热/a] /clinic 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

我尝试了在线学习和自定义词典两种方法，发现都没办法得到预想的效果。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

1、自定义词典：
词典：手足心热 clinic 1000

String text = ""腰酸、乏力、口干、目涩、手足心热、夜尿多、脉细或弱"";

        PerceptronLexicalAnalyzer analyzer = new 
                   PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);

        System.out.println(analyzer.analyze(text));

### 期望输出

腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w  [手足心/n 热/a] /clinic 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

### 实际输出

腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w 手足心热/clinic 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

这种方式分词和词性标注是对的，但却不是复合词

2、在线学习的方式：
String text = ""腰酸、乏力、口干、目涩、手足心热、夜尿多、脉细或弱"";

        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);

        analyzer.getNERTagSet().nerLabels.add(""clinic"");
        analyzer.learn(""[手足心/n  热/a]/clinic"");
        System.out.println(analyzer.analyze(text));

### 期望输出

腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w  [手足心/n 热/a] /clinic 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

### 实际输出
腰酸/n 、/w 乏力/a 、/w 口干/n 、/w 目涩/Ng 、/w 手足心热/n 、/w 夜尿多/j 、/w 脉/Ng 细/a 或/c 弱/a

这种方式分词是对的，但是词性标注是错的，也不是复合词
"
NLPTokenizer 对于 句子中 “和” 和 “与” 的解析不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在使用NLPTokenizer 进行分成的过程中 对于“和” 和“与的解释不一样”
代码：
System.out.println(NLPTokenizer.segment(""主席与特朗普通电话""));
System.out.println(NLPTokenizer.segment(""主席和特朗普通电话""));

输出的结果：
[主席/n, 与/c, 特朗普/nrf, 通/v, 电话/n]
[主席/n, 和/c, 特朗/nr, 普通/a, 电话/n]

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    System.out.println(NLPTokenizer.segment(""主席与特朗普通电话""));
     System.out.println(NLPTokenizer.segment(""主席和特朗普通电话""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
[主席/n, 与/c, 特朗普/nrf, 通/v, 电话/n]
[主席/n, 与/c, 特朗普/nrf, 通/v, 电话/n]

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
[主席/n, 与/c, 特朗普/nrf, 通/v, 电话/n]
[主席/n, 和/c, 特朗/nr, 普通/a, 电话/n]
```
实际输出
``
在官网上测试也是同样的效果。
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
命令hanlp找不到，需要如何配置?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

根据 https://github.com/hankcs/pyhanlp 提到 “使用命令hanlp segment进入交互分词模式”，然而：
[root@ec-n1-1d68-0005 ~]# hanlp
-bash: hanlp: command not found

是不是还差什么配置没有做好？
"
java.io.IOException: No FileSystem for scheme: D,"* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.3
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
警告: 字符正规化表缓存加载失败，原因如下：java.io.IOException: No FileSystem for scheme: D
配置文件
root=D:/A/B/C/hanlp/
然后报错类似
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
八月 20, 2019 11:57:38 上午 com.hankcs.hanlp.dictionary.other.CharTable loadBin
警告: 字符正规化表缓存加载失败，原因如下：java.io.IOException: No FileSystem for scheme: D

Exception in thread ""main"" java.lBang.ExceptionInInitializerError
	警告: 读取D:/A/B/C/hanlp/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.IOException: No FileSystem for scheme: D
<!-- 请详细描述问题，越详细越可能得到解决 -->



### 触发代码

```
    val a  = hanlp.HanLP.extractSummary(""这是一个测试"",5)
```


"
分词结果是否支持不进行词性标注呢？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
```java
System.out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！""));
```
的输出结果为
```
[你好/l, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/nz, 处理/v, 包/v, ！/w]
```
我想知道，有没有参数或者有没有其他接口能够支持不进行分词后的词性标注，例如
```
[你好, ，, 欢迎, 使用, HanLP, 汉语, 处理, 包, ！]
```
"
jpype._jclass.OutOfMemoryError: Java heap space,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在进行多进程聚类算法repeateBisection调用时候，出现卡住，进行不下去。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp 0.1.45
我使用的版本是：pyhanlp 0.1.45

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
def myfunc(qaid):
        analyzer1 = ClusterAnalyzer()#停止在这一句，进行不下去。
results.append(pool.apply_async(myfunc, (msg, )))
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于自定义词典和核心词典中的频次问题,"hankcs  你好! 可以讲一下自定义词典和核心词典中的频次的作用吗.我看了相关的Issues并没有得到解惑,谢谢."
java.lang.NullPointerException,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内v输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp 0.1.45
我使用的版本是：pyhanlp 0.1.45

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
res=analyzertxt.repeatedBisection(0.1)
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息
Traceback (most recent call last):
  File ""test_cluster.py"", line 65, in <module>
    res=cluster(filename)
  File ""test_cluster.py"", line 60, in cluster
    res=analyzertxt.repeatedBisection(0.3)
jpype._jexception.NullPointerExceptionPyRaisable: java.lang.NullPointerException


<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Change method name 'convert' to 'createSynonymList',"This class is used to represent  CoreSynonymDictionary.  This method named 'convert' is to create a synonym list. Thus, the method name 'createSynonymList' is more intuitive than 'convert'."
doc2vec计算文本相似度时，一些字母组成的特殊字符不起作用,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

doc2vec计算文本相似度时，一些字母组成的特殊字符不起作用，不参与相似度计算

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = './data/word2vec.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)

docs = [""dd的工作职责"", ""dm的工作职责"", ""pmo的工作职责""]

for idx, doc in enumerate(docs):
       doc2vec.addDocument(idx, doc)

for res in doc2vec.nearest('dd的工作职责'):
    print('%s = %.2f' % (docs[res.getKey().intValue()], res.getValue().floatValue()))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
dd的工作职责 = 1.00

dm的工作职责，pmo的工作职责的得分小于1，并且排在dd的工作职责之后
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
dm的工作职责 = 1.00
pmo的工作职责 = 1.00
dd的工作职责 = 1.00
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
使用SafeJClass调用停用词典时，自定义停用词过滤器不生效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用SafeJClass调用停用词典时，自定义停用词过滤器不生效，而使用JClass进行调用时，自定义停用词过滤器生效。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import *

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
MyFilter = JClass('MyFilter')  # 自定义停用词过滤器
CoreStopWordDictionary.FILTER = MyFilter()
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""怎么办理工作证""
term_list = NLPTokenizer.segment(text)
print(term_list)
输出结果：[怎么, 办理, 工作证]
CoreStopWordDictionary.apply(term_list)
print(term_list)
输出结果：[怎么, 办理, 工作证]

当使用SafeJClass时：
CoreStopWordDictionary = SafeJClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
MyFilter = SafeJClass('MyFilter')  # 自定义停用词过滤器
CoreStopWordDictionary.FILTER = MyFilter()
NLPTokenizer = SafeJClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""怎么办理工作证""
term_list = NLPTokenizer.segment(text)
print(term_list)
输出结果：[怎么, 办理, 工作证]
CoreStopWordDictionary.apply(term_list)
print(term_list)
输出结果：[办理, 工作证]
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[怎么, 办理, 工作证]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[办理, 工作证]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
对日期分词，奇怪的结果,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。


## 版本号 1.72


当前最新版本号是：
我使用的版本是：1.72

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
对日期进行分词，不同日期会得到不同的结果。比如：
2018年2月11日  分词结果为 2018年2月11日/t
2018年3月11日  分词结果为 2018年/t 3月/t 11日/t

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
使用NLPTokenizer.ANALYZER可以复现，其他方式未测试。

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""2018年2月11日""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
2018年/t 2月/t 11日/t
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
2018年2月11日/t
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
【建议】是否作者分派任务，多人贡献，作者审核,"   首先非常感觉提供HanLP这么棒的开源项目，据我所知，现在HanLP还是您一个人维护的，
是否可以由您，或者您组建一个大牛团队，比如HanLP委员会QQ群来把关，然后将HanLP的任务分解为容易实现的明细项，然后由热心网友领取任务开发，然后提交汇总。
   这样，作者就可以专注核心部分，不用干那么多体力活了。
当然，这只是一个建议，不一定可行，真正落地还需要考虑很多方面的细则"
删除stopwords.txt文件内容后重启，自带停用词任然生效？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
 非常感谢，用了您这边提供的补丁 #1253 ，有以下两个问题：
1、分词器只能选择基类要是seg.segment的吗？我选择最短路径分词就可以进行语义查询，但是选择NLPTokenizer 就会提示报错，
com.hankcs.hanlp.mining.word2vec.DocVectorModel(com.hankcs.hanlp.mining.word2vec.WordVectorModel,com.hankcs.hanlp.seg.Segment,boolean)；
2、我的目的是删除hanlp里面自带的停用词，使自带的停用词在我的程序中不生效，然后添加自己的停用词，但是最后的测试结果是，自带的停用词删除无效，不管是用那个分词器，然后使用CoreStopWordDictionary.apply(term_list)进行删除停用词时，自带的停用词后还是会起作用。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
text = ‘员工怎么办理工作证’
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
term_list = NLPTokenizer.segment(text)
print(term_list)

输出结果：
[员工/n,  怎么/r, 办理/v, 工作证/n]

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
CoreStopWordDictionary.apply(term_list)
print(term_list)

输出结果：
[员工/n,  办理/v, 工作证/n]

接着，删除在data/dictionary目录下，删除stopwords.txt.bin文件，并将stopwords.txt文件删除为空，重启后，重新运行程序。

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
CoreStopWordDictionary.apply(term_list)
print(term_list)

输出结果：
[员工/n,  办理/v, 工作证/n]

问题：为什么删除了停用词典，hanlp自带的原有停用词任然能器作用？

```
### 期望输出


```
[员工/n,  怎么/r, 办理/v, 工作证/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[员工/n,  办理/v, 工作证/n]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
删除stopwords.txt文件内容后重启，自带停用词任然生效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在data/dictionary目录下，删除stopwords.txt.bin文件，并将stopwords.txt文件删除为空，重启后，重新运行程序，发现原停用词表中的停用词任然生效，即删除无效。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
text = '员工怎么办理工作证?'
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
print(NotionalTokenizer.segment(text)) 

输出结果：
[员工/n, 办理/v, 工作证/n]

接着在data/dictionary目录下，删除stopwords.txt.bin文件，并将stopwords.txt文件删除为空，并在stopwords.txt中添加停用词“员工”，重启后，重新运行程序

text = '员工怎么办理工作证?'
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
print(NotionalTokenizer.segment(text)) 

输出结果：
[办理/v, 工作证/n]

自己添加的停用词能生效，但是hanlp自带的停用词任然生效，删除不起作用。
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出结果：
[怎么/r, 办理/v, 工作证/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[办理/v, 工作证/n]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Add unit tests for com.hankcs.hanlp.utility.ByteUtil,"I've analysed your codebase and noticed that `com.hankcs.hanlp.utility.ByteUtil` is not fully tested.
I've written some tests for the methods in this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests will help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other classes that you consider important in a subsequent PR."
为什么执行此脚本后直接报ssl错误,"![image](https://user-images.githubusercontent.com/41661610/61841888-da429680-aec8-11e9-8aa5-fae1d38f803e.png)
"
android跑依存句法问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我是使用的android板跑的依存句法分析，内存4G，只有一个应用程序，在加载依存句法模型的时候总是报.OutOfMemoryError: Failed to allocate a 12 byte allocation with 3856 free bytes and 3KB until OOM
异常，后来我将应用的内存调整为不受限制，依然会报该错误，之前看android的demo里面讲解的可以支持句法分析的，所以想要求证是我的配置出现问题了吗？我除了依存句法有问题之外，其他的分词，拼音转换都是可以正常使用的，谢谢。

### 步骤

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CoNLLSentence sentence = HanLP.parseDependency(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"");
        // 也可以用基于ArcEager转移系统的依存句法分析器
//        IDependencyParser parser = new KBeamArcEagerDependencyParser();
//        CoNLLSentence sentence = parser.parse(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"");
        Log.i(tag,""start=""+sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence)
        {
            Log.i(tag,""1、lemma=""+word.LEMMA+"";deprel=""+ word.DEPREL+"";HEAD=""+word.HEAD.LEMMA);
        }
        // 也可以直接拿到数组，任意顺序或逆序遍历
        CoNLLWord[] wordArray = sentence.getWordArray();
        for (int i = wordArray.length - 1; i >= 0; i--)
        {
            CoNLLWord word = wordArray[i];
            Log.i(tag,""2、lemma=""+word.LEMMA+"";deprel=""+ word.DEPREL+"";HEAD=""+word.HEAD.LEMMA);
        }
        // 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根
        CoNLLWord head = wordArray[12];
        while ((head = head.HEAD) != null)
        {
            if (head == CoNLLWord.ROOT) {
                Log.i(tag,""head.LEMMA=""+head.LEMMA);
            } else {
                Log.i(tag,""%s --(%s)--> ""+head.LEMMA+"";""+ head.DEPREL);
            }
        }
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在使用Suggester时，返回列表为空的问题,"```
from pyhanlp import *


def demo_suggester():
    """""" 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)
    >>> demo_suggester()
    [威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]
    [英报告说空气污染带来“公共健康危机”]
    [《时代》年度人物最终入围名单出炉 普京马云入选]
    [魅惑天后许佳慧不爱“预谋” 独唱《许某某》]
    """"""
    Suggester = JClass(""com.hankcs.hanlp.suggest.Suggester"")
    suggester = Suggester()
    title_array = [
        ""威廉王子发表演说 呼吁保护野生动物"",
        ""魅惑天后许佳慧不爱“预谋” 独唱《许某某》"",
        ""《时代》年度人物最终入围名单出炉 普京马云入选"",
        ""“黑格比”横扫菲：菲吸取“海燕”经验及早疏散"",
        ""日本保密法将正式生效 日媒指其损害国民知情权"",
        ""英报告说空气污染带来“公共健康危机”""
    ]
    for title in title_array:
        suggester.addSentence(title)

    print(suggester.suggest(""陈述"", 2))      # 语义
    print(suggester.suggest(""危机公关"", 1))  # 字符
    print(suggester.suggest(""mayun"", 1))   # 拼音
    print(suggester.suggest(""徐家汇"", 1)) # 拼音


if __name__ == ""__main__"":
    import doctest
    doctest.testmod(verbose=True)
```


### **我在执行以上代码时，失败了，data和jar都安装了，没有问题的，同事在windows上执行没有问题，我的环境是ubuntu 18.0，java环境是""1.8.0_212""，python3.6**
### **不清楚原因**
### **在几个服务器平台，和多人都测试的情况下，结果是一样的。我觉得这是一个bug,,只有suggester会返回空列表，其他的功能都能正常使用**
![image](https://user-images.githubusercontent.com/36363279/61576187-2d070180-ab09-11e9-9278-be663fd9672c.png)

"
寻求解决方案：人名、电话、省市区识别问题中的：人名识别问题,"当前最新版本号是：portable-1.7.4
我使用的版本是：portable-1.7.3


## 我的问题

需求：从一串文本中提取电话、省市区（单独提取），人名
实现逻辑：建议对应的省、市、区词库（使用的是CustomDictionary.insert强行插入） 、人名单独的俩个词典


## 复现问题
![image](https://user-images.githubusercontent.com/9693917/61531356-2faa1e00-aa59-11e9-874d-048e126c79a0.png)

打印词性如下：
一串文本中：黄英男 12345328978 湖南娄底双峰A街道华洪水大厦A座102

这种情况会把：华洪水和黄英男都视为人名，我的大概思路为：多个/nr情况以最远距离的为准，但不知道是否有其他实现思路，或者可行api

@hankcs 这么多人的问题，打扰了~~^V^
"
Pyhanlp 的User Guide,"https://github.com/FontTian/pyhanlp_user_guide

去年自己写了个Pyhanlp 的user guide。忘记和你说一下了。
这是CSDN博客列表：https://blog.csdn.net/fontthrone/article/category/8073727"
word2vec文本相似度计算和词语顺序无关吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我下载的hanlp的模型，然后测试语句:“我爱中国“和”我爱中国”相似度1.0000001192092896；”我爱中国”和”中国爱我“相似度也是1.0000001192092896

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
String word2VecPath = HanlpPropeties.getWord2VecPath();
        WordVectorModel wordVectorModel = null;
        try {
            wordVectorModel = new WordVectorModel(word2VecPath);
        } catch (IOException e) {
            e.printStackTrace();
        }
        DocVectorModel docVectorModel=new DocVectorModel(wordVectorModel);
        double ruleProb1=docVectorModel.similarity(caseInfo,ruleInfo);
```
### 期望输出

我想知道相似度比较词的顺序有关系没？张三打了李四；和李四打了张三相似度是不是1，还是我哪里写错了


```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
AhoCorasickDoubleArrayTrieSegment 使用自定义词典，分词结果词性为空,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
portable-1.7.4

当前最新版本号是：v1.7.4
我使用的版本是：portable-1.7.4



## 我的问题

AhoCorasickDoubleArrayTrieSegment 使用自定义词典，分词结果词性为空

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先创建词性TreeMap
2. 然后初始化AhoCorasickDoubleArrayTrieSegment并使用TreeMap的关键词
3. 接着进行AhoCorasickDoubleArrayTrieSegment分词

### 触发代码

```
  public static void main(String[] args) {
		TreeMap<String, CoreDictionary.Attribute> dictionary = new TreeMap<>();
		dictionary.put(""高冰种"", CoreDictionary.Attribute.create(""fcz 1""));
		AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment(dictionary);
		List<Term> termList = segment.seg(""高冰种正阳辣绿萤火虫手镯"");
		termList.forEach(System.out::println);
	}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
高冰种/fcz
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
高冰种/null
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
无法安装python版本,"具体操作和报错如下：
Last login: Mon Jul 15 20:16:21 on ttys001
MacBook-Pro-de-Chen:~ noah$ pip install pyhanlp
Collecting pyhanlp
Collecting jpype1>=0.7.0 (from pyhanlp)
  Using cached https://files.pythonhosted.org/packages/28/63/784834e8a24ec2e1ad7f703c3dc6c6fb372a77cc68a2fdff916e18a4449e/JPype1-0.7.0.tar.gz
Building wheels for collected packages: jpype1
  Building wheel for jpype1 (setup.py) ... error
  ERROR: Complete output from command /Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-wheel-1dve7lyv --python-tag cp37:
  ERROR: /Users/noah/anaconda3/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'use_scm_version'
    warnings.warn(msg)
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.7
  creating build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcollection.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcomparable.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_classpath.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jtypes.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_pykeywords.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jproxy.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_gui.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_darwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/nio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jstring.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_cygwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jboxed.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/types.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/beans.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jvmfinder.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/imports.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcustomizer.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_core.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jinit.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_linux.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jarray.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jobject.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jclass.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_windows.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jexception.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/reflect.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jpackage.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  running build_ext
  running build_java
  Using Jar cache
  creating build/lib
  creating build/lib/org
  creating build/lib/org/jpype
  creating build/lib/org/jpype/classloader
  copying native/jars/org/jpype/classloader/JPypeClassLoader.class -> build/lib/org/jpype/classloader
  copying native/jars/org.jpype.jar -> build/lib
  running build_thunk
  Building thunks
    including thunk build/lib/org/jpype/classloader/JPypeClassLoader.class
    including thunk build/lib/org.jpype.jar
  /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setupext/build_ext.py:85: FeatureNotice: Turned ON Numpy support for fast Java array access
    FeatureNotice)
  building '_jpype' extension
  creating build/temp.macosx-10.7-x86_64-3.7
  creating build/temp.macosx-10.7-x86_64-3.7/build
  creating build/temp.macosx-10.7-x86_64-3.7/build/src
  creating build/temp.macosx-10.7-x86_64-3.7/native
  creating build/temp.macosx-10.7-x86_64-3.7/native/python
  creating build/temp.macosx-10.7-x86_64-3.7/native/common
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/noah/anaconda3/include -arch x86_64 -I/Users/noah/anaconda3/include -arch x86_64 -DMACOSX=1 -DHAVE_NUMPY=1 -Inative/common/include -Inative/python/include -Ibuild/src -Inative/jni_include -I/Users/noah/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/noah/anaconda3/include/python3.7m -c build/src/jp_thunk.cpp -o build/temp.macosx-10.7-x86_64-3.7/build/src/jp_thunk.o -ggdb
  warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  In file included from build/src/jp_thunk.cpp:1:
  In file included from build/src/jp_thunk.h:3:
  native/common/include/jpype.h:82:10: fatal error: 'map' file not found
  #include <map>
           ^~~~~
  1 warning and 1 error generated.
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for jpype1
  Running setup.py clean for jpype1
Failed to build jpype1
Installing collected packages: jpype1, pyhanlp
  Running setup.py install for jpype1 ... error
    ERROR: Complete output from command /Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-record-l51xr0fq/install-record.txt --single-version-externally-managed --compile:
    ERROR: /Users/noah/anaconda3/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'use_scm_version'
      warnings.warn(msg)
    running install
    running build
    running build_py
    creating build/lib.macosx-10.7-x86_64-3.7
    creating build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcollection.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcomparable.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_classpath.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jtypes.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_pykeywords.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jproxy.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_gui.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_darwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/nio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jstring.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_cygwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jboxed.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/types.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/beans.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jvmfinder.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/imports.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcustomizer.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_core.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jinit.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_linux.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jarray.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jobject.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jclass.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_windows.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jexception.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/reflect.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jpackage.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    running build_ext
    running build_java
    Using Jar cache
    copying native/jars/org/jpype/classloader/JPypeClassLoader.class -> build/lib/org/jpype/classloader
    copying native/jars/org.jpype.jar -> build/lib
    running build_thunk
    Building thunks
      including thunk build/lib/org/jpype/classloader/JPypeClassLoader.class
      including thunk build/lib/org.jpype.jar
    /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setupext/build_ext.py:85: FeatureNotice: Turned ON Numpy support for fast Java array access
      FeatureNotice)
    building '_jpype' extension
    creating build/temp.macosx-10.7-x86_64-3.7
    creating build/temp.macosx-10.7-x86_64-3.7/build
    creating build/temp.macosx-10.7-x86_64-3.7/build/src
    creating build/temp.macosx-10.7-x86_64-3.7/native
    creating build/temp.macosx-10.7-x86_64-3.7/native/python
    creating build/temp.macosx-10.7-x86_64-3.7/native/common
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/noah/anaconda3/include -arch x86_64 -I/Users/noah/anaconda3/include -arch x86_64 -DMACOSX=1 -DHAVE_NUMPY=1 -Inative/common/include -Inative/python/include -Ibuild/src -Inative/jni_include -I/Users/noah/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/noah/anaconda3/include/python3.7m -c build/src/jp_thunk.cpp -o build/temp.macosx-10.7-x86_64-3.7/build/src/jp_thunk.o -ggdb
    warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
    In file included from build/src/jp_thunk.cpp:1:
    In file included from build/src/jp_thunk.h:3:
    native/common/include/jpype.h:82:10: fatal error: 'map' file not found
    #include <map>
             ^~~~~
    1 warning and 1 error generated.
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command ""/Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-record-l51xr0fq/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/
MacBook-Pro-de-Chen:~ noah$ 

求问如何解决"
分词词性标注感觉欠妥【三九】,"当前最新版本号是：portable-1.7.4
我使用的版本是：portable-1.7.4

## 我的问题： 我所在医药相关行业，处理【华润三九医药股份有限公司】时，【三九】被标注为时间词， 我认为【三九】一词可以是数量词，可以是名词。但是无法理解是一个时间词，说【三九天】是一个时间词我认同。

### 步骤
使用标准分词器分词【华润三九医药股份有限公司】

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(HanLP.segment(“华润三九医药股份有限公司”));
    }
```
### 期望输出

```
期望【三九】是数量词
```

### 实际输出

```
【三九】是时间词
```

"
如何按不同角色去识别其对应的相关信息,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.3

您好，我新手接触nlp，接到一个需求去在文本中识别不同角色的账号，我的思路如下，不知道这样对不对，请指教：

我的文本示例：
演员周星驰（电话号码：12345678966）在1998年出演过《大话西游》，导演是王晶（电话号码：34567889922），一同参演这部电影的还有演员吴孟达（QQ号：12342314），同时王晶导演又请了周帆导演（微信号：asfd1234234）来一同指导。
邓超，电话：12343453425，去年参演了电影《影》，这部电影的导演是王二麻子，他的手机号是：324523523534，王二麻子觉得自己导演的不行，又请了张艺谋（微信号：234532kjhk）一同指导。

我使用了crf分词去标注这段文本：
演员周星驰/ACTOR（/W 电话号码/TEL ：/W 12345678966/ACT_TEL ）W 在1998年出演过/AA 《/W 大话西游/MOVIE 》/W ，/W 导演是王晶/DIR （/W 电话号码/TEL ：/W 34567889922/DIR_TEL ）/W ，/W 一同参演这部电影的还有/AA 演员吴孟达/ACTOR （/W QQ号/QQ ：/W 12342314/ACT_QQ ）/W ，/W 同时/AA 王晶导演/DIR 又请了/AA 周帆导演/DIR （/W 微信号/WX ：/W asfd1234234/DIR_WX ）/W 来一同指导/AA 。/W
邓超/ACTOR ，/W 电话/TEL ：/W 12343453425/ACT_TEL ，/W 去年参演了电影/AA 《/W 影/MOVIE 》/W ，/W 这部电影的/AA 导演是王二麻子/DIR ，/W 他的手机号是/TEL ：/W 324523523534/DIR_TEL ，/W 王二麻子/DIR 觉得自己导演的不行/AA ，/W 又请了张艺谋/DIR （/W 微信号/WX ：/W 234532kjhk/DIR_TEL ）/W 一同指导/AA 。W

语料标记涉及到的词性：
/AA 是无关信息，/W是标点符号；
/ACTOR,/ACT_TEL,/ACT_QQ,/ACT_WX (演员及其各种号码)；
/DIR,/DIR_TEL,/DIR_QQ,/DIR_WX (导演及其各种号码)。

我有1亿多条这种数据，要在其中识别出演员的各种账号：/ACT_TEL,/ACT_QQ,/ACT_WX,和导演的各种账号：/DIR_TEL,/DIR_QQ,/DIR_TEL,

标记了大概500条这种数据，
然后用 CRFSegmenter 和 CRFPOSTagger 去分别训练cws.bin 和 pos.bin,
然后 CRFLexicalAnalyzer crfLexicalAnalyzer = new CRFLexicalAnalyzer(“cws.bin”，“pos.bin”);
用 crfLexicalAnalyzer.analyze(""我的文本"")，去获取相关词性的号码，这样是否正确，
我在训练了几次后发现，各种号码的识别正确率还行，但是角色的正确率却不高，比如 12345678966 应该识别为 /ACT_TEL(演员手机号码)，但却识别成了 /DIR_TEL(导演手机号码)；
这种情况下是要加大训练量还是要通过别的什么办法来提高正确率，请指教，谢谢！！！"
句法依赖错误问题,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
句法分析的结果有误

## 复现问题
在命令行，直接输入hanlp parse <<<  '老广的味道Ⅲ:山海'，即可看到‘Ⅲ'被识别为“核心关系”，这个罗马数字应该不算的吧？

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
CRFNERecognizer训练加载模型问题。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.7.4
我使用的版本是：v1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用`com.hankcs.hanlp.model.crf.CRFNERecognizer`类的train方法训练NER模型后，模型加载失败。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先：
// 训练模型
CRFNERecognizer nerecognizer = new CRFNERecognizer(null);
nerecognizer.train(""D:\\project\\jupyter-notebook\\data\\训练.txt"", ""D:/hanlp_data/data/model/crf/pku199801/ner.bin"");
// 训练完成，生成两个文件：ner.bin, ner.bin.txt
2. 然后
// 加载模型
CRFNERecognizer(""D:/hanlp_data/data/model/crf/pku199801/ner.bin.txt"");
3. 接着，就报错了
```
Exception in thread ""main"" java.lang.IllegalArgumentException: 错误的模型类型: 传入的不是命名实体识别模型，而是 POS 模型
	at com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer.<init>(PerceptronNERecognizer.java:43)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:67)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:49)
	at com.andglf.main.main.main(main.java:21)
```
### 触发代码

```
// 训练模型
CRFNERecognizer nerecognizer = new CRFNERecognizer(null);
nerecognizer.train(""D:\\project\\jupyter-notebook\\data\\训练.txt"", ""D:/hanlp_data/data/model/crf/pku199801/ner.bin"");
// 训练完成，生成两个文件：ner.bin, ner.bin.txt
// 注意，程序到这里是正常执行的。
// 加载模型
new CRFNERecognizer(""D:/hanlp_data/data/model/crf/pku199801/ner.bin.txt"");
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
正常运行不报错
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: 错误的模型类型: 传入的不是命名实体识别模型，而是 POS 模型
	at com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer.<init>(PerceptronNERecognizer.java:43)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:67)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:49)
	at com.andglf.main.main.main(main.java:21)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
英文文本分类速度好慢，怎么办,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我从谷歌上下载了文本分类的语料库，一共有8000篇10多个种类的文章，我已经训练成了模型，但是运行分类的时候速度特别慢，大概要两分钟才识别出来，中文的大概两秒钟，为什么英文和中文的差距那么大呢，哪里出的问题呢，我大概看了在获取模型的时候时间很长NaiveBayesModel model = (NaiveBayesModel) IOUtil.readObjectFrom(MODEL_PATH);为什么会这样呢，请大神帮忙分析一下。

我的语料库大概长这样
From: admiral@jhunix.hcf.jhu.edu (Steve C Liu)
Subject: Re: Bring on the O's
Organization: Homewood Academic Computing, Johns Hopkins University, Baltimore, Md, USA
Lines: 39
Distribution: world
Expires: 5/9/95
NNTP-Posting-Host: jhunix.hcf.jhu.edu
Summary: Root, root, root for the Orioles...

I heard that Eli is selling the team to a group in Cinninati. This would
help so that the O's could make some real free agent signings in the 
offseason. Training Camp reports that everything is pretty positive right
now. The backup catcher postion will be a showdown between Tackett and Parent
although I would prefer Parent. #1 Draft Pick Jeff Hammonds may be coming
up faster in the O's hierarchy of the minors faster than expected. Mike
Flanagan is trying for another comeback. Big Ben is being defended by
coaches saying that while the homers given up were an awful lot, most came
in the beginning of the season and he really improved the second half. This
may be Ben's year. 
	I feel that while this may not be Mussina's Cy Young year, he will
be able to pitch the entire season without periods of fatigue like last year
around August. I really hope Baines can provide the RF support the O's need.
Orsulak was decent but I had hoped that Chito Martinez could learn defense
better and play like he did in '91. The O's right now don't have many
left-handed hitters. Anderson proving last year was no fluke and Cal's return
to his averages would be big plusses in a drive for the pennant. The 
rotation should be Sutcliffe, Mussina, McDonald, Rhodes, ?????. Olson is an
interesting case. Will he strike out the side or load the bases and then get

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

NaiveBayesModel model = (NaiveBayesModel) IOUtil.readObjectFrom(MODEL_PATH);
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
执行from pyhanlp import *  报错”A fatal error has been detected by the Java Runtime Environment:“,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.4
我使用的版本是：hanlp-1.7.4

<!--以上属于必填项，以下可自由发挥-->
 执行 ：from pyhanlp import *  “ 导入pyhanlp时报错
## 我的问题
   Python 3.6.3 |Anaconda, Inc.| (default, Oct  6 2017, 12:04:38) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from pyhanlp import *


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先 在mac的终端Terminater输入”python3“ 
2. 然后执行”from pyhanlp import *“

### 触发代码

```
  from phhanlp import *
```
### 期望输出



```
程序正常执行，无错误提示
```

### 实际输出
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000001124355c0, pid=20972, tid=775
#
# JRE version: Java(TM) SE Runtime Environment (7.0_79-b15) (build 1.7.0_79-b15)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.79-b02 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.dylib+0x30f5c0]  jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x1b
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /Users/www/hs_err_pid20972.log

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/12984460/60889481-ab52e080-a28b-11e9-91db-42422d65c4aa.png)

"
情感分析没有看到有训练出来的模型文件呢？在哪里,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息
情感分析训练数据会训练处模型文件吗，我看代码没有呢

"
perceptronLexicalAnalyzer的enableOrganizationRecognize设置为false，仍能识别出nt,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
根据自身使用情况，想关闭机构识别，perceptronLexicalAnalyzer的enableOrganizationRecognize设置为false，仍能识别出nt。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
analyze(""北京教育局举办知识竞赛"")；
关闭后仍会识别出[[北京/city 教育局/n]/nt, 举办/v, 知识/n, 竞赛/vn]
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
文本分类英文语料库,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

做文本分类支持英文吗，是不是需要英文的语料库，找不到英文的预料库呢？你们大家谁有呢，分享一个

"
hanlp源码集成到springBoot中，windwo下一切正常，但是到linux上报错，字典路径找不到,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤


1：首页我把hanlp源码复制到springboot项目里，没有用官网提供的两种方式，而是直接引入源码。
2：在window下测试一切正常
3：但在linux上部署后，报错，提示找不到字典路径
4：希望作者尽快看到回复，急用！！感激不尽！！！！

### 触发代码

```
   首先把hanlp源码复制到springBoot项目里，在window下运行正常，一部署到linux上提示找不到字典路径
		//调用分词组件生成 产品名称关键词
		List<ProductKeywordDO> keyList=new ArrayList<>();
		List<Term> gjzList=NotionalTokenizer.segment(product.getProdName());
		if(gjzList!=null&&gjzList.size()>0){
			for (int i = 0; i <gjzList.size() ; i++) {
				ProductKeywordDO pkd=new ProductKeywordDO();
				pkd.preInsert();
				pkd.setProductId(product.getId());
				pkd.setName(gjzList.get(i).word);
				pkd.setIsFc(""2"");
				keyList.add(pkd);
			}
		}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望能正确分词，然而现在报错，提示找不到字典路径
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
现在报错，提示找不到字典路径，data目录应该放在linux上的什么位置才能被正确找到路径
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/CustomDictionary.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/CustomDictionary.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/CustomDictionary.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/现代汉语补充词库.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/现代汉语补充词库.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/现代汉语补充词库.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/全国地名大全.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/全国地名大全.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/全国地名大全.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/人名词典.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/人名词典.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/人名词典.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/机构名词典.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/机构名词典.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/机构名词典.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/上海地名.txt读取错误！java.io.FileNotFoundException: data/dictionary/custom/上海地名.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/custom/上海地名.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/person/nrf.txt读取错误！java.io.FileNotFoundException: data/dictionary/person/nrf.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 失败：data/dictionary/person/nrf.txt
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 没有加载到任何词条
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - 自定义词典data/dictionary/custom/CustomDictionary.txt不存在！java.io.FileNotFoundException: data/dictionary/custom/CustomDictionary.txt.bin (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 自定义词典[data/dictionary/custom/CustomDictionary.txt, data/dictionary/custom/现代汉语补充词库.txt, data/dictionary/custom/全国地名大全.txt ns, data/dictionary/custom/人名词典.txt, data/dictionary/custom/机构名词典.txt, data/dictionary/custom/上海地名.txt ns, data/dictionary/person/nrf.txt nrf]加载失败
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 读取data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - 核心词典data/dictionary/CoreNatureDictionary.txt不存在！java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt (No such file or directory)

```

## 其他信息

希望作者尽快回复"
doc2vec进行语义查询时，是否已经内置了去停用词的操作？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

doc2vec进行语义查询时，是否已经内置了去停用词的操作？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
fix a small mistake in writing,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
IndexTokenizer索引分词器没有拆分出CustomDictionary.add()动态添加的拆分词条,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.4
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用CustomDictionary动态添加了自定义词条，CustomDictionary.add();，
但是使用索引分词IndexTokenizer的时候，去不能拆分出上面自定义添加的词语。List<Term> segment = IndexTokenizer.segment(buffer);
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
``
```
    //添加自定义词条
				List<CustomWord> customList = wordFrequencyService.queryCustomList();
				List<Term> segment = IndexTokenizer.segment(buffer);
				Segment newSegment = HanLP.newSegment(""viterbi"");
				Segment segment3 = newSegment.enableIndexMode(2);
				
				segment3.enableCustomDictionary(true);
				
				for (CustomWord customWord : customList) {
					boolean add = CustomDictionary.add(customWord.getWord());
					System.out.println(add);
				}
				final char[] charArray = buffer.toCharArray();
				CustomDictionary.parseText(buffer, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
		        {
		            @Override
		            public void hit(int begin, int end, CoreDictionary.Attribute value)
		            {
		            	  System.out.printf(""[%d:%d]=%s %s\n"", begin, end, new String(charArray, begin, end - begin), value);
		            }
		        });
				System.out.println(newSegment.seg(buffer));
				System.out.println(segment3.seg(buffer));
				// 自定义词典在所有分词器中都有效
		        //List<Term> segment = HanLP.segment(buffer);
				/*
				 * IndexTokenizer tokenizer = new IndexTokenizer(); List<Term> segment = tokenizer.segment(buffer);
				 * 
				 * System.out.println( tokenizer.segment(buffer)); System.out.println(tokenizer.seg2sentence(buffer));
				 */
				
				System.out.println(segment);
		        CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
		        Segment segment2 = analyzer.enableIndexMode(1);
		        List<Term> seg = segment2.seg(buffer);
		        System.out.println(seg);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
分词问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.7.4
我使用的版本是：1.7.4


## 我的问题

`长大`分词问题

## 复现问题
对长大进行分词，某些情况下会分成`长、大`
### 步骤

1. 准备句子`希望长大`
2. 使用分词
3. 查看结果

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        HanLP.segment(""希望长大"").forEach(term -> System.out.print(term.word+"",""));
    }
```
### 期望输出

分词成 `希望`+`长大`

```
希望,长大,
```

### 实际输出

分词成 `希望`+`长`+`大`
```
希望,长,大,
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
使用`长大以后`进行分词就正常，分词结果是
```
长大,以后,
```
"
pyhanlp中，Analyzer类好像不能在enableCustomDictionaryForcing(True)后使用seg()，,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.4.jar
我使用的版本是：hanlp-1.7.4.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
1. 我想要在分词前强制启动自定义词典，分词结果使用CoreStopWordDictionary类过滤停用词。Tokenizer类没有enableCustomDictionaryForcing方法，所以只能使用Analyzer类分词。Analyzer类实例化后有两个分词方法，一个seg()，一个segment()。segment()方法产生的结果是LinkedList类，对它调用CoreStopWordDictionaryapply()会报错。seg()方法产生的结果是ArrayList类，可以正常过滤停用词。但是对分词器使用#enableCustomDictionaryForcing后，使用seg()方法会直接报java.util.NoSuchElementException。请问这个有办法解决吗？
如果上述问题无法解决，有什么方案能在pyhanlp里，同时使用enableCustomDictionaryForcing和CoreStopWordDictionary吗？

2. 对默认分词器的分词结果执行CoreStopWordDictionary.apply()返回的结果为None，1.7.3版本没有这个问题。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
![image](https://user-images.githubusercontent.com/20453470/60585884-5b73a580-9dc3-11e9-90a5-bad816737bf0.png)

![image](https://user-images.githubusercontent.com/20453470/60634009-d11d5700-9e3f-11e9-8728-94c9e24b6516.png)





"
"请问这个""data/test/crf/cws-template.txt""模板在哪里",
hanlp segment分词失败,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.4.jar    
我使用的版本是：hanlp-1.7.4.jar    

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
windows系统cmd命令行模式下，输入hanlp segment分词报错，输入hanlp serve在线正常
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
将data文件夹放入D:\python35\Lib\site-packages\pyhanlp\static文件夹下，同时修改了hanlp.properties中的root路径为D:\python35\Lib\site-packages\pyhanlp\static
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
Traceback (most recent call last):
  File ""d:\python35\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\python35\Scripts\hanlp.exe\__main__.py"", line 9, in <module>
  File ""d:\python35\lib\site-packages\pyhanlp\main.py"", line 99, in main
    print(' '.join(term.toString() for term in segmenter.seg(any2utf8(line))))
TypeError: sequence item 0: expected str instance, java.lang.String found
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
正确的分词结果
```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Traceback (most recent call last):
  File ""d:\python35\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\python35\Scripts\hanlp.exe\__main__.py"", line 9, in <module>
  File ""d:\python35\lib\site-packages\pyhanlp\main.py"", line 99, in main
    print(' '.join(term.toString() for term in segmenter.seg(any2utf8(line))))
TypeError: sequence item 0: expected str instance, java.lang.String found
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
crf++效率问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v 1.6.8
我使用的版本是：v 1.6.8

<!--以上属于必填项，以下可自由发挥-->

 当ner训练语料为600M以上,crf++的迭代会很慢，参数f 2 ，c 3.0 请问有什么优化方法吗？或者其他高效的像这么大的训练语料的ner训练方式

"
感知机模型人名识别错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：pyhanlp 0.1.45

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
对句子 “这时我女儿凤霞推门进来，又摇摇晃晃地把门关上。凤霞尖声细气地对我说：”分词，
会得到“。凤霞”也是个人名这种匪夷所思的结果。把句号改成逗号，分词结果就会变正常。
我已将“凤霞”加入词典，结果是相同的

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```

txt = ""这时我女儿凤霞推门进来，又摇摇晃晃地把门关上，凤霞尖声细气地对我说：""
data_path = ""/home/dream/miniconda3/envs/py37/lib/python3.7/site-packages/pyhanlp/static/data/model/perceptron/large/cws.bin""
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer(data_path,
                                    HanLP.Config.PerceptronPOSModelPath,
                                    HanLP.Config.PerceptronNERModelPath)
print(analyzer.seg(txt))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[这时/r, 我/r, 女儿/n, 凤霞/nr, 推门/v, 进来/v, ，/w, 又/d, 摇摇晃晃/v, 地/u, 把/p, 门关/n, 上/f, 。/w, 凤霞/nr, 尖声/nz, 细气/a, 地/u, 对/p, 我/r, 说/v, ：/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[这时/r, 我/r, 女儿/n, 凤霞/nr, 推门/v, 进来/v, ，/w, 又/d, 摇摇晃晃/v, 地/u, 把/p, 门关/n, 上/f, 。 凤霞/nr, 尖声/nz, 细气/a, 地/u, 对/p, 我/r, 说/v, ：/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
doc2vec语义查询高并发时，服务停止,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

基于flask，把pyhanlp中调用文档向量模型进行语义查询做成一个服务，高并发请求结果时，服务停止。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']
    
    if not jpype.isThreadAttachedToJVM():
          jpype.attachThreadToJVM()

    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    doc2vec = DocVectorModel(word2vec._proxy)

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    init_log()
    start_model_server()

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
基于HanLP的Elasticsearch分词插件,"基于HanLP的Elasticsearch分词插件，做了挺久的了，2.x版本也有，但是基本上从6.3.1版本之后和ES同步更新，有时可能更新不及时，但是如果有时间，一直做下去，如果有有兴趣的同学可以一起开发维护

项目地址：[https://github.com/KennFalcon/elasticsearch-analysis-hanlp](https://github.com/KennFalcon/elasticsearch-analysis-hanlp)

- 支持了HanLP大部分的分词方式

- 支持词典热更新

- 支持远程词典功能

@hankcs 希望收录一下，谢谢"
hanlp有提供solr的python api？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

hanlp有提供solr的python api吗？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
CoNLLWord类的注释中疑似出现拼写错误？,"`CoNLLWord`类的第 24行 对变量`LEMMA`注释中写道

> 当前词语（或标点）的原型或词干，在中文中，此列与FORM相同

但是该文件（以及整个项目）中并没有对`FORM`变量的定义，所以这里是不是出现了拼写错误，应该是`NAME`变量。还是我理解的偏差？

注：同样的问题还出现在该文件的第 61、76 行。"
Python调用接口得到的词法分析结果与官网显示结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.7.3
我使用的版本是：v1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
Python调用接口得到的词法分析结果与官网显示结果不一致。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
from pyhanlp import *
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(""佳奔汽车贸易有限公司大岗汽车用品分公司""))

得到输出：
![image](https://user-images.githubusercontent.com/15197043/59669211-65ef4600-91ec-11e9-9997-d1c6737170ab.png)

官网词法分析输出：

![image](https://user-images.githubusercontent.com/15197043/59669295-89b28c00-91ec-11e9-983a-9b9bf60751c7.png)
"
基于flutter、HanLP的NLP学习App,使用flutter基于HanLP发布了一个App，项目地址https://github.com/sppsun/nlp_starter ，欢迎拍砖～～
merge,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
"在1.7.3/pyhanlp里面,PerceptronLexicalAnalyzer ,没有seg方法了","在1.7.3/pyhanlp里面,PerceptronLexicalAnalyzer ,没有seg方法了,调用的时候,出现异常:jpype._jexception.NoSuchElementExceptionPyRaisable: java.util.NoSuchElementException
而调用segment方法就没问题.(而此方法也不是所有的分词器都有)
使用HanLP.newSegment('crf')也一样,没有seg方法了,出异常."
基于flask，把pyhanlp做成一个服务，调用出错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

基于flask，把pyhanlp做成一个服务，调用文档向量模型进行语义查询，遇到以下问题，以第一种方式代码调用服务，系统没有报错，可加载模型后服务自动死掉；以第二种代码调用服务，就能够正常输出结果，我想了解的是第一种调用方式的报错原因是什么？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

第一种：
```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    doc2vec = DocVectorModel(word2vec._proxy)

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    init_log()
    start_model_server()

```

第二种：
```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    doc2vec = DocVectorModel(word2vec._proxy)
    init_log()
    start_model_server()

```

### 期望输出

```
第一种能够正常输出结果
```

### 实际输出

```
服务自动死掉
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
python接口的句法解析器parseDependency可否解析分词结果,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3 pyhanlp

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
准备用句法解析器去解析已经经过分词处理后的结果。
>['我','来自', '火星', '。']

因为特殊领域的分词词表未知，准备用另外的分词工具来预处理。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
    from pyhanlp import *
    dep = HanLP.parseDependency(['我','来自', '火星', '。'])

### 触发代码

```
from pyhanlp import *
dep = HanLP.parseDependency(['我','来自', '火星', '。'])
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
1	我	我	r	r	_	2	主谓关系	_	_
2	来自	来自	v	v	_	0	核心关系	_	_
3	火星	火星	n	n	_	2	动宾关系	_	_
4	。	。	wp	w	_	2	标点符号	_	_
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-34-e4d45bcc3795> in <module>
----> 1 dep = HanLP.parseDependency(['我','来自', '火星', '。'])

RuntimeError: No matching overloads found for parseDependency in find. at native/common/jp_method.cpp:127
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Add unit tests for com.hankcs.hanlp.utility.MathUtilityTest,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

Hi,

I've analysed your code base and noticed that `com.hankcs.hanlp.utility.MathUtility` in the `hanlp` module is not fully tested.

I've written some tests that cover this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests should help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other particular classes that you consider important.
"
word2vec计算短文本相似度支持英文吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

word2vec计算短文本相似度支持英文吗？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import *

WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')


word2vec = WordVectorModel(model_path)
doc2vec = DocVectorModel(word2vec)

docs = []

for idx, doc in enumerate(docs):
    doc2vec.addDocument(idx, doc)

for res in doc2vec.nearest(text):
    print('%s = %.2f' % (docs[res.getKey().intValue()], res.getValue().floatValue()))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词典根据不同用户使用不同词典,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：portable-1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
自定义词典CustomDictionary能不能，“选择不同的词典“进行分词？
就是根据不同的用户，使用不同的自定义词典进行分词。

或者是能否给CustomDictionary添加一个类型，根据不同类型进行 添加“词汇”，以及分词，不同类型互不影响


"
每个用户使用单独的词典,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Portable,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
Add unit tests for com.hankcs.hanlp.algorithm.EditDistance,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

Hi,

I've analysed your code base and noticed that `com.hankcs.hanlp.algorithm.EditDistance` in the `hanlp` module is not fully tested.

I've written some tests that cover this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests should help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other particular classes that you consider important."
NER对地名的识别受高频词（如：市长）的干扰,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用NER识别人名、地名、组织名时，经常会被’市长‘这个词所干扰。如：织造有限公司公司位于江阴市长泾镇 的NER提取结果为 （只关注 江阴市长泾镇 这几个字）

### 实际输出

江阴/ns 市长/nnt 泾镇/ns

### 期望输出

江阴市/ns 长泾镇/ns

请问如何修改词典可以获得期望结果？

我尝试修改customer dictionary。对于网上的例子，“攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰”这句话来说，加上在字典里定义攻城狮确实可以改变分词结果，但对我上面的例子没有效果 
####= 备注
为了打开地名和组织名的提取，我对segment加上以下两个开关：enablePlaceRecognize(True).enableOrganizationRecognize(True)
"
分词不准确，不能按照自定义词典分词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
自定义词典分词不准确，原因是因为英文逗号引起的

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args) {
        String str = ""c,我们都喜欢c语言，color,c++,c#,"";
        CustomDictionary.add(""c"",""zdy"");
        CustomDictionary.add(""c++"",""zdy"");
        System.out.println(HanLP.segment(str));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[c/zdy, ,/w, 我们/r, 都/d, 喜欢/v, c/zdy, 语言/n, ，/w, color/nx, ,/w, c++/zdy,  /w, ,/w, c#/nx, ,/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[c/zdy, ,/w, 我们/r, 都/d, 喜欢/v, c/zdy, 语言/n, ，/w, color/nx, ,/w, c/zdy, ++,/w, c#/nx, ,/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Python 3.6 ModuleNotFoundError: No module named 'pyhanlp.static',"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v5.0.0
我使用的版本是：v5.0.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<pip install hanlp 后执行python 3.6报error ModuleNotFoundError: No module named 'pyhanlp.static' >

## 复现问题
### 步骤

1. 首先 pip install hanlp
2. 然后执行以下
# -*- coding: utf8 -*-

import hanlp
from pyhanlp import *

print(hanlp.segment('商品和服务'))

3. 接着出现error 
ModuleNotFoundError: No module named 'pyhanlp.static'

### 触发代码

```
Traceback (most recent call last):
  File ""D:/python/FunNLP/FunNLP/pyhanlp/test1.py"", line 5, in <module>
    import hanlp
  File ""D:\Program Files\Anaconda3\lib\site-packages\hanlp\__init__.py"", line 6, in <module>
    from pyhanlp.static import STATIC_ROOT
ModuleNotFoundError: No module named 'pyhanlp.static'

```
### 期望输出

```
期望输出
```能够成功执行。

### 实际输出

```
实际输出
``` error如上。实际上在D:\Program Files\Anaconda3\Lib\site-packages 里面是有对应的static。

## 其他信息

无。

"
CRFNERecognizer使用CRF++导出的模型和CRF++结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp中如何关闭中文依存自动转换,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp v0.1.22
我使用的版本是：pyhanlp v0.1.22



<!--以上属于必填项，以下可自由发挥-->

## 我的问题

pyhanlp中如何关闭中文依存自动转换. 输出的时候句法依存是中文。如何输出英文呢？

![image](https://user-images.githubusercontent.com/10768193/58684668-ecfe8a80-83b3-11e9-8494-30be8b7b4b05.png)


我看到了博主用java时的设置是可以关闭的，http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html。


![image](https://user-images.githubusercontent.com/10768193/58684736-38b13400-83b4-11e9-905c-b23369b08401.png)

使用pyhanlp的话可以直接关闭吗？ 

"
URLTokenizer.segment(text);会导致程序卡住,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.3
我使用的版本是：portable-1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用URLTokenizer的时候会遇到程序卡住的情况，dubug了一下，会一直卡在matcher.find()
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 见下面代码，必定触发

### 触发代码

```
    public void testIssue() throws Exception
    {
        String text = ""随便写点啥吧？abNfxbGRIAUQfGGgvesskbrhEfvCdOHyxfWBq"";
        List< Term > terms = URLTokenizer.segment(text);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望能够正确执行完
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际程序会一直卡在matcher.find()
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
textrank如何获取关键词的权重？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

阅读了文档，关于textrank的介绍里只有方法extractKeyword(String content, int size)，即输入文档与要提取的关键词数量，返回相应数量的关键词。但是对于不同关键词的权重，是否有接口可以获取呢？

### 触发代码

```
String content = ""程序员(英文Programmer)是从事程序开发、维护的专业人员。一般将程序员分为程序设计人员和程序编码人员，但两者的界限并不非常清楚，特别是在中国。软件从业人员分为初级程序员、高级程序员、系统分析员和项目经理四大类。"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
能否有接口可以获取到不同关键词的权重呢？

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP微服务版,"我们利用当前流行的微服务框架，封装了HanLP大部分方法，通过Restfull API的方式提供微服务调用，项目地址https://github.com/sppsun/sca-best-practice ，欢迎拍砖～

"
同义词词典的引入是否有助于文档余弦相似度计算,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。


当前最新版本号是：1.7.3
我使用的版本是：1.7.3


## 我的问题
我是刚接触HanLP，有个问题想问一下，同义词词典的引入是否有助于文档余弦相似度计算，如果可以，要如何配置？
比如： System.out.println(wordVectorModel.similarity(""买卖人"",""商人""));
""买卖人""和""商人""在同义词词典中是同义词，但是通过WordVectorModel求相似度是-1。
"
繁转简错误比较多,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
对于国际化业务，query里面会有简体、繁体、英文、日语等情况，繁转简错误比较多。

## 测试方法
拿了约2g的百科语料，对比了opencc和hanlp的繁转简结果，第一列为语料，由于语料比较长，窃取了以diff为中心的前后10个字，第二列为opencc结果，第三列为hanlp结果。


原始句子(【】内部为diff内容) |	opencc |	hanlp
 |  ------------| -- | -- | 
，斟酒的人翻过大金斗【猛】击代君，一下就砸死 |	猛 |   勐
校及科研单位挂钩，并【建】立了长期的协作关系 |	建 |	创
寇夫人 他自拣一搭金【堦】死。”亦省作“ ⁤|	堦|	階
综合兼容性 　　二、【大】众娱乐性 　　三、|	大|	福
合兼容性 　　二、大【众】娱乐性 　　三、互|	众|	斯
进行有效的传播控制和【整】合管理。2007年|	整|	集
行有效的传播控制和整【合】管理。2007年，|	合|	成
有物饮碧水，高林挂青【蜺】。”"",""ts"":|	蜺|	霓
西安市莲湖城内，共计【房】屋231户。"",""|	房|	住
；行程万里的“世界屋【脊】汽车挑战赛”等成功|	脊|	嵴
成“全国性”、“全程【式】”的技术创新公共服|	式|	序

更多diff参见文件 [**diff.txt**](https://github.com/hankcs/HanLP/files/3223791/diff.txt)，以tab键分隔，很多由于原始query与繁转简后句子长度不一致，可能会出现【】在非转换字的上。

"
请求出一个server版的，以便于其他开发语言可以通过接口调用,请求出一个server版的，以便于其他开发语言可以直接通过API的形式调用
“直面人生”这个词转成繁体，对于“面”的转换出现错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.3
我使用的版本是：portable-1.7.3

<!--以上属于必填项，以下可自由发挥-->



### 触发代码

```
    public void testIssue1234() throws Exception
    {
        String content = ""直面现实,直面人生"";
		System.out.println(HanLP.s2hk(content));
		System.out.println(HanLP.s2tw(content));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
直面現實,直面人生
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
直面現實,直麪人生
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
为什么我提取地址的时候和Demo上的不一样呢,"对于同一个地址，DEMO和我的程序分出来是不一样的，我使用的是1.7.3
上海上海市浦东新区金桥镇金高路2216弄
DEMO分出来是：上海 上海市 浦东新区 金桥镇 金高 路 2216 弄
我的程序分出来是：[上海/ns, 上海市浦东新区/ns, 金桥镇/ns, 金高路/ns, 2216/m, 弄/v]
把上海市浦东新区没有分开成上海市  浦东新区
代码：
       Segment segment = HanLP.newSegment().enablePlaceRecognize(true);
        List<Term> termList = segment.seg(""上海上海市浦东新区金桥镇金高路2216弄"");
        System.out.println(termList);"
使用自己的语料训练分词模型时，怎么加入效果比较好的人名、机构名的命名识别,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.3
我使用的版本是：hanlp-1.7.3 ; master分支

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先我使用https://github.com/hankcs/HanLP/wiki/CRF%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90 的crf命名实体识别文档的方法，对自己的专业领域语料进行了训练
2. 然后我发现对人名机构名的识别效果有点糟糕，我在想能否将自己所得的专业领域分词器和默认的人名识别，机构名识别效果好的结合使用


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
修改 Java 运行环境版本（Hanlp1.7.3）,"运行代码或者命令行`hanlp segment <<< ...`时，提示我使用使用Java6，我已经有最新版本了（通过homebrew 安装），可不可以使用这个版本？

[~] java --version
openjdk 12.0.1 2019-04-16
OpenJDK Runtime Environment (build 12.0.1+12)
OpenJDK 64-Bit Server VM (build 12.0.1+12, mixed mode, sharing)"
如果语料集中有中括号开头的词，训练模型时会抛出异常。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

如果语料集中有 **中括号** 开头的词，例如： **[2003]00072/nz**  ，训练模型时会抛出异常。
java.lang.IllegalArgumentException: /opt/corpus/1.txt读取失败

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 用自己训练的感知机模型分词、标注词性。

1.txt 文本内容如下：
```
工程项目免税确认书（编号：[2003]00072）
```

分词标注结果：
```
工程/n 项目/n 免税/n 确认书/n （/w 编号/nz ：/w [2003]00072/nz ）/w
```

2. 提取语料生成词典或者训练模型

### 触发代码

```
    public void testLoadCoupus() {
        final DictionaryMaker dictionaryMaker = new DictionaryMaker();
        CorpusLoader.walk(""/opt/corpus/1.txt"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(com.hankcs.hanlp.corpus.document.Document document)
            {
                    for (IWord word : document.getWordList()) {
                        if (word.getLabel().equals(""n"")) {
                                System.out.println(word.getValue());
                                dictionaryMaker.add(word);
                        }
                   }
            }
        });
        dictionaryMaker.saveTxtTo(""/opt/n"");
    }
```
### 期望输出

期望可以正常生成 n 文件。

### 实际输出

/opt/corpus/1.txt2019-05-22 10:42:02.503  WARN 2618 --- [           main] HanLP                                    : 使用 2003创建单个单词失败
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : 使用参数2003构造单词时发生错误
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : 在用 [2003]00072/nz 构造单词时失败，句子构造参数为 工程/n 项目/n 免税/n 确认书/n （/w 编号/nz ：/w [2003]00072/nz ）/w
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : 使用 工程/n 项目/n 免税/n 确认书/n （/w 编号/nz ：/w [2003]00072/nz ）/w 创建句子失败

java.lang.IllegalArgumentException: /opt/corpus/1.txt读取失败

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
python 封装的hanlp-1.7.3 运行报错,"下面是报错信息。
Python3.7， macOS
hanlp-1.7.3

> 
> Using local /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/hanlp-1.7.3-release.zip, ignore https://github.com/hankcs/HanLP/releases/download/v1.7.3/hanlp-1.7.3-release.zip
> Traceback (most recent call last):
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/bin/hanlp"", line 6, in <module>
>     from pyhanlp.main import main
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 122, in <module>
>     _start_jvm_for_hanlp()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 39, in _start_jvm_for_hanlp
>     from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/__init__.py"", line 309, in <module>
>     install_hanlp_jar()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/__init__.py"", line 200, in install_hanlp_jar
>     with zipfile.ZipFile(jar_zip, ""r"") as archive:
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py"", line 1222, in __init__
>     self._RealGetContents()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py"", line 1289, in _RealGetContents
>     raise BadZipFile(""File is not a zip file"")
> zipfile.BadZipFile: File is not a zip file"
TextRank迭代算法疑问----提问（已解决）,"<!--
TextRank迭代算法疑问----提问
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题（已解决）

 
作者在提取关键字能力TextRankKeyword.java中，迭代公式用的是原始的PageRank吗？貌似没有用TextRank的迭代公式，还是我理解的不对，望指正！

----上述理解错了，另外一个有权重的公式，是用来提取摘要用的


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        for (int i = 0; i < max_iter; ++i)
        {
            Map<String, Float> m = new HashMap<String, Float>();
            float max_diff = 0;
            for (Map.Entry<String, Set<String>> entry : words.entrySet())
            {
                String key = entry.getKey();
                Set<String> value = entry.getValue();
                m.put(key, 1 - d);
                for (String element : value)
                {
                    int size = words.get(element).size();
                    if (key.equals(element) || size == 0) continue;
                    // ---------这边的迭代公式-----------
                    m.put(key, m.get(key) + d / size * (score.get(element) == null ? 0 : score.get(element)));

                }
                max_diff = Math.max(max_diff, Math.abs(m.get(key) - (score.get(key) == null ? 0 : score.get(key))));
            }
            score = m;
            if (max_diff <= min_diff) break;
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
数据源下载w,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：pyhanlp  1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 当前的直接下载速度很慢能不能将数据源上传一份到云盘 -->

"
关于结构化感知机中文命名实体识别器无法输出正确的词语所属的命名实体成分,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
利用结构化感知机标注框架中的命名实体识别器对新的语句进行识别，但未能输出正确的结果：
原页面示例：
recognize(""吴忠市 乳制品 公司 谭利华 来到 布达拉宫 广场"".split("" ""), ""ns n n nr p ns n"".split("" ""))));
输出：
[B-nt, M-nt, E-nt, S, O, S, O]
以上示例在自己的复现代码中没有问题

但是 在调用感知机分词对其他语句获取分词和词性并输入 如下：

## 复现问题

但是 在调用感知机分词对其他语句获取分词和词性并输入 如下：

### 步骤

1. 首先利用NLPTokenizer 获取分词以及标注结果
2. 然后两个数组结果传入recognize(String[],String[])


### 触发代码
    recognizer.recognize(""美国纽约 和 中国 上海 都 很 好玩"".split("" ""), ""ns c ns ns d d a"".split("" ""));

### 期望输出
<!-- 你希望输出什么样的正确结果？-->
```
[B-nt, O,M-nt, E-nt, O, O, O, O]//大概猜测的结果，但至少会有B-nt/M-nt/E-nt这样的标注项
```

### 实际输出
[O ,O, O, O, O, O, O] 

## 其他信息

是否与输入的分词结果和词性标注有关，
NLPTokenizer的标注结果不能有效识别？
"
NLP无法识别日期？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

从百度还有stanfod coreNLP过来的，但是HanLP连日期也无法识别？
![图片](https://user-images.githubusercontent.com/13548273/57267480-e5004300-70b2-11e9-8bc2-9ab1eba2b8f2.png)
"
自定义词典存在长词，自定义词性 以m或x 开头，会被修改词性,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
自定义词典存在长词，自定义词性 以m或x 开头，会被修改词性
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 添加带有自定义词性的自定义分词词典，自定义.txt

我的额度 xyz 1000
提高额度 my 1000

2. 修改hanlp.properties，在CustomDictionaryPath添加自定义词典(自定义.txt)
3. 调用HanLP.segment(""我的额度不够，需要提高额度"")

### 触发代码

```

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[我的额度/xyz, 不够/a, ，/w, 需要/v, 提高额度/my]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[我的额度/x, 不够/a, ，/w, 需要/v, 提高额度/mq]
```
### 原因分析
自定义分词的时候，在拼接词的时候，ViterbiSegment的combineWords方法调用Vertex的compileRealWord方法，在词性以x和m开头的时候，导致词性修改

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词 添加到 CustomDictionary 里面就可以被识别，自己加载词典就不被识别,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
FAQ第一条,"![图片](https://user-images.githubusercontent.com/13548273/57191377-6c38a400-6f57-11e9-91f9-928b71247edd.png)
文中提到无论怎么分词，商品、和服、服务都不可能同时出现，这是不严谨的。
![图片](https://user-images.githubusercontent.com/13548273/57191407-c76a9680-6f57-11e9-8567-cf0979b8f234.png)
"
当多线程做句法依存分析，当用enableDeprelTranslator(False)时，会报地址边界错误。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我修改了pyhanlp中test/test_multithread.py 文件，测试多线程句法依存分析。发现当我使用英文依存关系，即: enableDeprelTranslator(False)时，会报Address boundary error错误。但如果直接使用中文的依存关系，程序运行正常。

### 触发代码

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (c) 2017 <> All Rights Reserved
#
#
# File: /Users/hain/ai/pyhanlp/tests/test_multithread.py
# Author: Hai Liang Wang
# Date: 2018-03-23:17:18:30
#
# ===============================================================================
from __future__ import print_function
from __future__ import division

__copyright__ = ""Copyright (c) 2017 . All Rights Reserved""
__author__ = ""Hai Liang Wang""
__date__ = ""2018-03-23:17:18:30""

import os
import sys

curdir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(curdir, os.path.pardir))

if sys.version_info[0] < 3:
    reload(sys)
    sys.setdefaultencoding(""utf-8"")
    # raise ""Must be using Python 3""

import threading
import time
from pyhanlp import HanLP, SafeJClass

# 在线程体外部用SafeJClass线程安全地引入类名
parser_class = SafeJClass(""com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser"")


class MyThread(threading.Thread):
    def __init__(self, name, counter, analyzer):
        threading.Thread.__init__(self)
        self.thread_name = name
        self.counter = counter
        self.analyzer = analyzer

    def run(self):
        print(""Starting "" + self.thread_name)
        while self.counter:
            time.sleep(1)
            t = self.analyzer.parse(""他们白发苍苍他们精神矍铄,"")
            for word in t.iterator():  # 通过dir()可以查看sentence的方法
                print(""%d: %s --(%s)--> %d: %s (%s, %s)"" % (word.ID, word.LEMMA, word.DEPREL, word.HEAD.ID, word.HEAD.LEMMA, word.POSTAG, word.CPOSTAG))
            self.counter -= 1



def test():
    analyzer = parser_class().enableDeprelTranslator(False)

    thread1 = MyThread(""Thread-1"", 1, analyzer)
    thread2 = MyThread(""Thread-2"", 2, analyzer)

    thread1.start()
    thread2.start()

    print('waiting to finish the thread')

    thread1.join()
    thread2.join()

    print(""Exiting Main Thread"")


if __name__ == '__main__':
#     FLAGS([__file__, '--verbosity', '1'])  # DEBUG 1; INFO 0; WARNING -1
    test()

```
### 期望输出

```
解析这句话三遍
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Starting Thread-1
Starting Thread-2
waiting to finish the thread
'python test_threading.py' terminated by signal SIGSEGV (Address boundary error)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
可以把自定义的分词结果用于依存分析吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp再进行封装客户端时报错，应该怎么办？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.03
我使用的版本是：1.7.02

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
python程序没有封装成客户端时，可以正常调用，封装后就出现各种路径问题，应该怎么解决？
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于corpus.dependency.CoNll.CoNLLWord类中POSTAG和CPOSTAG的一些歧义性问题。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是 1.7.3
我使用的版本是：1.7.3
pyhanlp 0.1.45

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好，我有个关于句法依存解析的问题。看文档发现com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord 类有CPOSTAG和POSTAG两个属性，其中CPOSTAG是粗粒度词性，POSTAG是细粒度词性。然而我在测试demo中那个例子的时候发现似乎不太清楚。例子是：上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观。
其中 谭旭光 和 胡花蕊 的POSTAG是nr， CPOSTAG是nh。 这两个好像类别差别好像非常大。
所以想请问关于这两个属性的具体解释。

## 复现问题
直接调用以下触发代码

### 触发代码

```
t = HanLP.parseDependency(""上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观"")
for word in t.iterator():  
    print(""%d: %s --(%s)--> %s (%s, %s)"" % (word.ID, word.LEMMA, word.DEPREL, word.HEAD.LEMMA, word.POSTAG, word.CPOSTAG))
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
1: 上海华安工业（集团）公司 --(定中关系)--> 董事长 (nt, ni)
2: 董事长 --(定中关系)--> 谭旭光 (n, n)
3: 谭旭光 --(主谓关系)--> 来到 (nr, nh)
4: 和 --(左附加关系)--> 胡花蕊 (c, c)
5: 秘书 --(定中关系)--> 胡花蕊 (n, n)
6: 胡花蕊 --(并列关系)--> 谭旭光 (nr, nh)
7: 来到 --(核心关系)--> ##核心## (v, v)
8: 美国纽约现代艺术博物馆 --(动宾关系)--> 来到 (ns, ns)
9: 参观 --(并列关系)--> 来到 (v, v)
```


"
python调用聚类， 显示“pytho 已停止工作”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.7.3
我使用的版本是：1.7.2
pyhanlp 0.1.45

## 我的问题
执行ClusterAnalyzer = SafeJClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')这一句
弹出 “pytho 已停止工作”，捕捉不了错误

## 复现问题
直接执行 pip install pyhanlp==0.1.45 安装成功
执行tests中的例子：test_multithread.py成功
执行 IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil') 这一句成功

### 步骤

1. python ClusterService.py

### 触发代码

import os,math,time,datetime
from pyhanlp import *
from pyhanlp import SafeJClass

from jpype import JavaException
import pandas as pd
import re,json,uuid

class ClusterService:
    def __init__(self):
        try:           
            print(""1"")
            self.IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil')    
            print(""2"")
            self.ClusterAnalyzer = SafeJClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
            print(""3"")   
        except JavaException as e:
            print('聚类初始化出错：',e)
        except Exception as  ex:         
            print('聚类初始化出错：',ex)

 if __name__ == ""__main__"":  
        CS = ClusterService()

### 期望输出

""1""
""2""
""3""
```
期望输出
```

### 实际输出

""1""
""2""

```
实际输出
```

## 其他信息

弹出 “pytho 已停止工作”，捕捉不了错误

"
为什么repo里的data文件夹里面没有CharType.bin,"为什么repo里的data/dictionary/other/文件夹里面没有CharType.bin
而，库需要

java.lang.IllegalArgumentException: 字符类型对应表 /data/dictionary/other/CharType.bin 加载失败： java.io.FileNotFoundException:dictionary/other/CharType.bin (No such file or directory)"
最新的jar(1.7.3)包在pyhanlp下有兼容性问题,"将pyhanlp库目录中的hanlp-1.7.2.jar换成了hanlp-1.7.3.jar,data目录也更新为最新版后,发现之前跑的好好的程序,现在出现各种缺失元素方法的异常.
维特比分词器好使,感知器分词器不好使."
关于简繁转换的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.5

## 我的问题

System.out.println(HanLP.convertToSimplifiedChinese(""「以後等妳當上皇后，就能買士多啤梨慶祝了」""));
输出 ： “以后等你当上皇后，就能买草莓庆祝了”
System.out.println(HanLP.convertToTraditionalChinese(""“以后等你当上皇后，就能买草莓庆祝了”""));
输出 ： “以後等你當上皇后，就能買草莓慶祝了”
期望能输出 “以後等妳當上皇后，就能買士多啤梨慶祝了”，是有什么地方需要配置吗？

"
码农厂的图片无法显示了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
感知机分词时 类似“很好”，“高端”一类的词被识别成人名,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：1.7.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
感知机分词时 类似“很好”，“高端”，“较高”，“单选”等一类的常用词被识别成人名
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {   
       // SentenceAnalyzer为简单封装的感知机分词 
        SentenceAnalyzer sentenceAnalyzer = new SentenceAnalyzer();
        System.out.println(sentenceAnalyzer.analyze(""高端""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
高端 /n
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
高端 /nr
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
CollectionUtility.sortMapByValue方法在存在bug,"v1.72版本存在bug，最新版master中依然存在
com.hankcs.hanlp.classification.utilities.CollectionUtility中
public static <K, V extends Comparable<V>> Map<K, V> sortMapByValue(Map<K, V> input, final boolean desc)方法存在bug
ArrayList<Map.Entry<K, V>> entryList = new ArrayList<Map.Entry<K, V>>(input.size());
中的input.size()应该改为input.entrySet()"
重新加载停用词（热更新）,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.3
我使用的版本是：hanlp-1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在变更自定义停用词典`stopwords.txt`文件后，在不重新启动程序的情况下，无法自动加载更新，亦无手动更新的操作。
即使手动删除`stopwords.txt.bin`缓存文件，
在之后的操作中（如：提取关键词`HanLP.extractKeyword( )`），也不会触发stopwords.txt的重新载入。

从 [issue:1136](https://github.com/hankcs/HanLP/issues/1136) 中了解到 自定义词典 有`reload()`的方法 可以手动 重新载入 自定义词典



## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

（不适用此issue）

### 触发代码

（不适用此issue）

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

被追加到`stopwords.txt`文件中的词，不再出现在“提取关键词”的结果中

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

与 “变更`stopwords.txt`文件”之前的结果一样

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

目前为了实现“手动重新载入stopword”之目的，
查看了`CoreStopWordDictionary`类的静态代码块，仿照着

在需要 reload 停用词典 的地方，如下操作：
```
try
{
    // 反射：
    // 1. 获取到CoreStopWordDictionary中非public的dictionary成员变量
    // 2. 再变更其 可请问权限
    Field dictionaryField = CoreStopWordDictionary.class.getDeclaredField(""dictionary"");
    dictionaryFiled.setAccessible(true);

    // 3. 重新创建一个StopWordDictionary对象，调用其 save()方法，并替换旧的StopWordDictionary对象
    StopWordDictionary dictionary = new StopWordDictionary(HanLP.Config.CoreStopWordDictionaryPath);
    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(HanLP.Config.CoreStopWordDictionaryPath + Predefine.BIN_EXT)));
    dictionary.save(out);

    dictionaryFiled.set(dictionaryField, dictionary);
    out.close();
}
catch (Exception e)
{
    // 
}
```

显然，这样做十分不优雅。希望能在官方的版本能加入“手动重新载入 停用词典”的功能。

相关 issue: [#28](https://github.com/hankcs/HanLP/issues/28) 、[#32](https://github.com/hankcs/HanLP/issues/32)

动画演示：https://i.loli.net/2019/04/23/5cbf2741654a3.gif
说明：如图的Demo，循环地提取一段文本的关键词。（来源：百度百科的github词条）

初始：提取到“代码”一词；

将其添加到`stopword.txt`中，Ctrl+S保存，重启程序，未生效。
删除`stopword.txt.bin`之后，重启程序，已生效。（结论①：如同`Readme.md`所言，“如果你修改了任何词典，只有删除缓存才能生效”）

再 添加“概念”一词，不重启程序，输出仍有此词，
删除了 3个`*.txt.bin`文件（未重启程序），输出仍有此词。（结论②：stopword 不能热更新 加载）

[![StopWord.gif](https://i.loli.net/2019/04/23/5cbf2741654a3.gif)](https://i.loli.net/2019/04/23/5cbf2741654a3.gif)"
博客的图全挂了,博客的图都挂了，能修复一下吗，文章写的挺好的
为什么NLPTokenizer.ANALYZER下segment和analyze结果不同？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
不好意思 我是用python运行的
```
a=JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
sentence=""阶段注采比1.04""
seg_result1=a.ANALYZER.seg(sentence)
print(seg_result1)
seg_result1=a.ANALYZER.analyze(sentence)
print(seg_result1)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[阶段/n, 注采比/n, 1.04/m]
阶段/n 注采比/n 1.04/m
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[阶段/n, 注采比/v, 1.04/m]
阶段/n 注采比/n 1.04/m
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

注采比 n 是自定义词典中的"
动态自定义词典持久化问题,"* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。

## 版本号

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

## 我的问题

请问：我现在使用:CustomDictionary.insert(""hualiang"", ""YangL 1024"");

自定义词典，但是无法“持久化”，当程序关闭之后添加的词便会消失掉。

请问各位有没有什么方法，添加词的同时，将 “词” 存入词典中。

万分感谢！



"
怎样评价文本自动聚类效果？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.7.2
我使用的版本是：v1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

```
ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
tag_analyzer = ClusterAnalyzer()
tag_hanlp_cluster = []

for index, row in zip(df.loc[:, ""LabelTwoTag""].index.tolist(), df['LabelTwoTag']):
    tag_analyzer.addDocument(index, row.toString()[1:-1])

for beta in TAG_BETA_HANLP:
    tag_hanlp_cluster = tag_analyzer.repeatedBisection(beta)

print(""聚类数量:"", len(tag_hanlp_cluster))
```
请问有接口可以查看类似无监督聚类之后的轮廓系数么？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

使用pyhanlp进行文本自动聚类

### 触发代码

```
ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
tag_analyzer = ClusterAnalyzer()
tag_hanlp_cluster = []

for index, row in zip(df.loc[:, ""LabelTwoTag""].index.tolist(), df['LabelTwoTag']):
    tag_analyzer.addDocument(index, row.toString()[1:-1])

for beta in TAG_BETA_HANLP:
    tag_hanlp_cluster = tag_analyzer.repeatedBisection(beta)

print(""聚类数量:"", len(tag_hanlp_cluster))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
能够通过analyzer提供轮廓系数来简单评估自动聚类的效果。

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
没能找到提供轮廓系数的接口
```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

在pyhanlp的github没法提issue，所以就跑到这里来提了，本质是关于hanlp的issue。"
感知机训练模型提问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
请问在训练感知机的分词和词性模型时，如果想添加一些专业领域的词汇，但是不想添加词典特征，因为缺少标注，能不能把一堆专业领域词汇做成人民日报预料库的格式加入训练？
 `<领域词汇>/<领域词性>    <领域词汇>/<领域词性>`
不知道这种理论上可行不？
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP.parseDependency 1024问题,"您好，sentence = HanLP.parseDependency(doc)会报如下错误：
jpype._jexception.ArrayIndexOutOfBoundsExceptionPyRaisable: java.lang.ArrayIndexOutOfBoundsException: Index 1032 out of bounds for length 1024

请问能有办法扩展长度么？"
感知机不同接口出来的标点符号词性不一样,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用Wiki中的示例，感知机词法分析器和感知机词性标注，不同接口对同一句话，标点符号词性标注不一致，请问在不依赖词典的前提下怎么才能做到一致？
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        // 这里均是默认模型，分词模型是large目录下的，pos模型是98年目录下的
        PerceptronSegmenter segment = new PerceptronSegmenter();
        PerceptronPOSTagger pos = new PerceptronPOSTagger();
        PerceptronLexicalAnalyzer pla = new PerceptronLexicalAnalyzer();
        String text = ""此外，李彦宏、陆奇、DuerOS的景鲲，AI技术平台的王海峰，还邀请了上百位人工智能领域的专家以及相关的合作企业。"";
        List<String> tokens = segment.segment(text);
        String[] tags = pos.tag(tokens);
        for (int i = 0; i < tags.length; ++i) {
            System.out.print(tokens.get(i) + '/' + tags[i] + "" "");
        }
        System.out.println();
        System.out.println(pla.analyze(text));
        System.out.println(pla.seg(text));
```
### 期望输出
期望标点符号的词性输出一致，为 `w`
<!-- 你希望输出什么样的正确结果？-->

```
标点符号均为 w
```

### 实际输出
```
此外/c ，/v 李彦宏/nr 、/v 陆奇/nr 、/v DuerOS/v 的/u 景鲲/n ，/n AI/vn 技术/n 平台/n 的/u 王海峰/nr ，/n 还/d 邀请/v 了/u 上百/m 位/q 人工智能/n 领域/n 的/u 专家/n 以及/c 相关/v 的/u 合作/vn 企业/n 。/w 
此外/c ，/w 李彦宏/nr 、/w 陆奇/nr 、/w DuerOS/nx 的/u 景鲲/n ，/w AI/nx 技术/n 平台/n 的/u 王海峰/nr ，/w 还/d 邀请/v 了/u 上百/j 位/q 人工智能/n 领域/n 的/u 专家/n 以及/c 相关/v 的/u 合作/vn 企业/n 。/w
[此外/c, ，/w, 李彦宏/nr, 、/w, 陆奇/nr, 、/w, DuerOS/nx, 的/u, 景鲲/n, ，/w, AI/nx, 技术/n, 平台/n, 的/u, 王海峰/nr, ，/w, 还/d, 邀请/v, 了/u, 上百/m, 位/q, 人工智能/n, 领域/n, 的/u, 专家/n, 以及/c, 相关/v, 的/u, 合作/vn, 企业/n, 。/w]
```
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
敏感信息过滤,你好什么时候上线敏感信息识别功能？
[Mac OS 10.14.4]Python3.6 下pip install pyhanlp 失败,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

安装环境：
Mac OS 10.14.4 
Python 3.6.8 :: Anaconda, Inc.
gcc-8 (Homebrew GCC 8.3.0) 8.3.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

安装命令：
pip intall pyhanlp 

提示：error: command 'gcc' failed with exit status 1

log如下：

[log.txt](https://github.com/hankcs/HanLP/files/3067924/log.txt)

"
语料pku199801链接http://hanlp.linrunsoft.com/release/corpus/pku98.zip无法打开,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
能否输出文档向量模型，也就是多维度的向量表示一个文档的txt文本格式,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
能否输出类似词向量模型的文档向量模型，也就是多维度向量表示文档的txt文本格式

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤


### 触发代码

### 期望输出

<!-- 你希望输出什么样的正确结果？-->





### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
 想知道opennlp训练maxEnt模型的训练参数是什么样子的 初学者 希望不吝赐教,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何计算两个字符串或文本的相似度？,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.7.2 <br/>
我使用的版本是：1.7.1

## 我的问题

如何计算两个字符串或文本的相似度？

## 复现问题


### 步骤



### 触发代码


### 期望输出

计算两个字符串或文本的相似度



### 实际输出



## 其他信息



"
求推荐HanLP for NodeJS 还在活跃的版本,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

如题，求大神推荐，谢谢"
1.7.2 版本中 CustomDictionary.insert 对 NLPTokenizer 无效？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
<!-- 请详细描述问题，越详细越可能得到解决 -->

　　非常感谢这个项目，对NLP的理解深入了很多，我之前用的版本是1.6.4，基本只用了我认为最核心稳定的以统计模型（StandardTokenizer「ViterbiSegment」）为主，规则（CustomDictionary）为辅的中文分词服务。
　　这样虽然高效、稳定，但随着遇到问题复杂性的提高，我逐渐发现已不能满足我nlp任务上一些需求，比如对未 insert 到 CustomDictionary 的「机构名」、「人名」识别较差，再比如没能好好利用到 parseDependency。
　　在看到HanLP 公开了[在线演示](http://hanlp.com)的1亿级语料训练的分词模型后，非常兴奋想要好好的利用起来。虽然我已经看过好几遍首页及wiki、FAQ、以及相关 issues，但可能由于基础较差，对 HanLP 的诸多「特性」理解不深，不知道该如何用好，总感觉各个功能间总是“鱼与熊掌不可兼得”。
　　于是想在这里集中整理一下我的问题：

1. 版本1.7.2 与 版本1.6.4，它们的默认 HanLP.segment（StandardTokenizer） 效果是一样的吗？它们都是基于98年人民日报标注语料的统计模型分词吗？
2. HanLP.segment 分词结果中的“词性”（包括人名和地名）是怎么来？都是根据 CoreDictionary 和 CustomDictionary 中确定来的吗？所以这里并不涉及到 HMM、CRF 对吗？所以虽然一个词可能会有多个词性，但在 HanLP.segment 结果中每个词一定是一个固定的词性？
3. 版本1.7.2 利用亿级语料训练的分词模型只是应用到了 NLPTokenizer 上吗？parseDependency 也是基于这个语料吗？还是说 parseDependency 是基于 NLPTokenizer 的结果？
4. “自定义词典在所有分词器中都有效”，但我发现 1.7.2 版本中利用 CustomDictionary.insert 后并未生效，但修改 `dictionary/custom/机构名词典.txt `后确实会生效，相关代码在下面。
5. 相同的句子在[在线演示](http://hanlp.com/?sentence=支援臺灣正體香港繁體：微软公司於1975年3月1日由比爾·蓋茲和保羅·艾倫創立)上与1.7.2版本的 NLPTokenizer 结果不同，相关代码在下面。



## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        # 问题 4
        NLPTokenizer.ANALYZER.enableCustomDictionary(true);
        CustomDictionary.insert(""钟正"", ""nr 1"");
        CustomDictionary.insert(""财新智库莫尼塔研究"", ""ntc 1"");
        System.out.println(HanLP.segment(""财新智库莫尼塔研究董事长、首席经济学家钟正生表示""));
        System.out.println(NLPTokenizer.analyze(""财新智库莫尼塔研究董事长、首席经济学家钟正生表示""));

        # 问题 5
        NLPTokenizer.ANALYZER.enableCustomDictionary(false);
        // 注意观察下面两个“希望”的词性、两个“晚霞”的词性
        System.out.println(NLPTokenizer.analyze(""支援臺灣正體香港繁體：微软公司於1975年3月1日由比爾·蓋茲和保羅·艾倫創立。""));

    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
# 问题 4
# 此部分我理解 “钟/n, 正生/v” 的切分方式
[财新智库莫尼塔研究/ntc, 董事长/nnt, 、/w, 首席/n, 经济学家/nnt, 钟/n, 正生/v, 表示/v]
# 此部分应该是“财新智库莫尼塔研究/ntc” 和 “[钟/n, 正生/v]/nr”
财新智库莫尼塔研究/ntc 董事长/nnt 、/w 首席/n 经济学家/n [钟/n, 正生/v]/nr 表示/v

# 问题 5
# “比爾·蓋茲” 应该是 nr
支援/v [臺灣/ns 正體/n 香港/ns 繁體/n]/nt ：/w [微软/nt 公司/n]/nt 於/p 1975年/t 3月/t 1日/t 由/p 比爾·蓋茲/nr 和/c 保羅·艾倫/nr 創立/v 。/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
# 问题 4
# 此部分没问题
[财新智库莫尼塔研究/ntc, 董事长/nnt, 、/w, 首席/n, 经济学家/nnt, 钟/n, 正生/v, 表示/v]
# 此部分 CustomDictionary.insert 后对 NLPTokenizer 未生效， 而且为什么“董事长”词性是 n 而不是 nnt？而且为什么不是“[钟/n, 正生/v]/nr”
[财新/j 智库/n 莫尼塔/n 研究/vn 董事长/n]/nt 、/w 首席/n 经济学家/n 钟正生/nr 表示/v

# 问题 5
# 注意 “比爾·蓋茲/nz”
支援/v [臺灣/ns 正體/n 香港/ns 繁體/n]/nt ：/w [微软/nt 公司/n]/nt 於/p 1975年/t 3月/t 1日/t 由/p 比爾·蓋茲/nz 和/c 保羅·艾倫/nr 創立/v 。/w

而在线演示中是正确的 nr
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
用户自定义词在NLPTokenizer.segment中不起作用,"from pyhanlp import *

def add_dictionary():
CustomDictionary = JClass(""com.hankcs.hanlp.dictionary.CustomDictionary"")
CustomDictionary.add(""攻城狮"")

def keyword_extract():
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"") # NLP标注
print(NLPTokenizer.segment(""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰""))

if name == ""main"":
add_dictionary()
keyword_extract()

结果：[攻城/ns, 狮/Ng, 逆袭/v, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/nr]

在python版本里不起作用呢"
pyhanlp自动安装失败，手动下载release和data文件后，手动配置仍然缺少jar文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!--pyhanlp自动安装失败，手动下载release和data文件后，手动配置仍然缺少jar文件，给的下载链接失效，求助，感谢 -->

Downloading https://github.com/hankcs/HanLP/releases/download/v1.7.2/hanlp-1.7.2
-release.zip to E:\Anaconda3\lib\site-packages\pyhanlp\static\hanlp-1.7.2-releas
e.zip
Failed to download https://github.com/hankcs/HanLP/releases/download/v1.7.2/hanl
p-1.7.2-release.zip
Please refer to https://github.com/hankcs/pyhanlp for manually installation.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 122, in <modul
e>
    _start_jvm_for_hanlp()
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 39, in _start_
jvm_for_hanlp
    from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_
DATA_PATH
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\static\__init__.py"", line 309, in
 <module>
    install_hanlp_jar()
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\static\__init__.py"", line 200, in
 install_hanlp_jar
    with zipfile.ZipFile(jar_zip, ""r"") as archive:
  File ""E:\Anaconda3\lib\zipfile.py"", line 1090, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'E:\\Anaconda3\\lib\\sit
e-packages\\pyhanlp\\static\\hanlp-1.7.2-release.zip'"
"java maven项目,已将hanlp.properties放入resources下,data应放哪?hanlp.properties中的root应该怎么写,好像试了各种都不行 ","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.72

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

JavaWeb maven项目, 想把data包放在项目里使用.
我的操作: 把hanlp.properties和data都放在resources下,
hanlp.properties中的配置root=./    (就是当前目录)
无法读取到核心词典.
所以想问hanlp.properties和data在maven中应放哪, hanlp.properties中的root如果配置

"
CRF分词复用感知机维特比的疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
<!-- 请详细描述问题，越详细越可能得到解决 -->
您好，阅读你的感知机词法分析的时候，看到您说`新版条件随机场复用了感知机的维特比算法，所以速度持平`，我想问一下，条件随机场在预测的时候也是用的维特比，这个和感知机的维特比有什么不一样的吗？因为对条件随机场不是特别了解，代码还没阅读完，想请您给详细说说，然后对照代码看

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
reload热更新问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
需要实现载入内存后字典的更新
想达到效果如修改txt词典增加词语后，jar能重新载入
发现提供了CustomDictionary.reload，故使用此api过程中产生了与预期不一致的分词情况

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先在dictionary下新增自定义词典menu.txt,其中有词盒饭 日料，运行如下代码
2. 然后在线程sleep时往词典中新增 真好吃

### 触发代码

```
    public void testIssue1234() throws Exception
    {
         String text = ""盒饭日料真好吃""；
         System.out.println(StandardTokenizer.segment(text ));
          Thread.sleep(60000); //此时在新增的词典menu.txt中加入 <真好吃>
         CustomDictionary.reload();
        System.out.println(StandardTokenizer.segment(text));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

期望第一次输出：<盒饭> <日料> <真> <好吃>
期望第二次输出：<盒饭> <日料> <真好吃>
期望输出
```

### 实际输出

第一次输出：<盒饭> <日料> <真> <好吃>
第二次输出：<盒饭> <日料> <真> <好吃>

reload并没有更新各个分词器中dat原有数据内容 ，仅更新了 CustomDictionary中自有的dat
```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
依存关系拆分与官网不一致,"注意事项
请确认下列注意事项：

我已仔细阅读下列文档，都没有找到答案：
首页文档
wiki
常见问题
我已经通过Google和issue区检索功能搜索了我的问题，也没有找到答案。
我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
 我在此括号内输入x打钩，代表上述事项确认完毕。
**版本号**
当前最新版本号是：1.7.2
我使用的版本是：1.7.2

**我的问题**
依存关系拆分最新版与官网预览不一致

复现问题
步骤
触发代码
   String  chineseSentence = “你爸叫什么?”;
   CoNLLSentence sentence = HanLP.parseDependency(chineseSentence);
        System.out.println(sentence);
        List<WordRelate> wordRelateList = new ArrayList<WordRelate>();
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
            WordRelate wordRelate = new WordRelate();
            wordRelate.setCont(word.LEMMA);
            wordRelate.setId(word.ID);
            wordRelate.setParent(word.HEAD.ID);
            wordRelate.setRelate(RelateEnum.getEnumByKey(word.DEPREL).getRelateTag());
            wordRelate.setPos(word.POSTAG);
            wordRelateList.add(wordRelate);
        }

期望输出
1	你	你	r	r	_	2	定中关系	_	_
2	爸	爸	n	n	_	0	主谓关系	_	_
3	叫	叫	v	v	_	2	核心关系	_	_
4	什么	什么	r	r	_	3	动宾关系	_	_
5	?	?	wp	w	_	3	标点符号	_	_

make
实际输出

1	你	你	r	r	_	2	主谓关系	_	_
2	爸	爸	v	v	_	0	核心关系	_	_
3	叫	叫	v	v	_	2	并列关系	_	_
4	什么	什么	r	r	_	3	动宾关系	_	_
5	?	?	wp	w	_	3	标点符号	_	_"
新词提取英文拆词了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在新词提取中，英文单词被拆分了


## 复现问题

### 步骤


### 触发代码

```
     BufferedReader bufferedReader = new BufferedReader(new FileReader(fileName));

                List<WordInfo> keywordList = HanLP.extractWords(bufferedReader, 100,true);


                for (WordInfo info : keywordList) {

                    System.out.println(info.text+""_""+info.frequency+""_""+info.entropy+""_""+info.aggregation);

                    insertData(info.text,info.frequency);

                }

```
### 期望输出

```
make
```

### 实际输出

```
ma
mak
make
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/31303643/54976835-7f5e6880-4fd6-11e9-9295-c772918cc7a0.png)

"
如何在分词的时候增加特殊符号,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

如何在分词的时候增加特殊符号

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[<中国科学院计算技术研究所>/n, 的/u, 宗/n, 成庆/nr, 教授/n, 正在/d, 教授/n, 自然语言处理/v, 课程/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[</v, 中国科学院计算技术研究所/n, >/v, 的/u, 宗/n, 成庆/nr, 教授/n, 正在/d, 教授/n, 自然语言处理/v, 课程/n]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何在分词的时候增加特殊符号,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题 
如何在分词的时候增加特殊符号

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
print(NLPTokenizer.segment('<中国科学院计算技术研究所>的宗成庆教授正在教授自然语言处理课程'))
```
### 期望输出

[<中国科学院计算技术研究所>/n, 的/u, 宗/n, 成庆/nr, 教授/n, 正在/d, 教授/n, 自然语言处理/v, 课程/n]

```
期望输出
```

### 实际输出
[</v, 中国科学院计算技术研究所/n, >/v, 的/u, 宗/n, 成庆/nr, 教授/n, 正在/d, 教授/n, 自然语言处理/v, 课程/n]

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
训练感知机模型时怎么加入词典？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，我阅读Wiki中感知机词法分析中，看到您描述的：训练时`只使用了7种状态特征，未使用词典`，我想知道在训练时可以加入分词词典吗？该怎么加入？在做预测时也可以考虑词典的作用吗？
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
/**
     * 训练
     *
     * @param trainingFile  训练集
     * @param developFile   开发集
     * @param modelFile     模型保存路径
     * @param compressRatio 压缩比
     * @param maxIteration  最大迭代次数
     * @param threadNum     线程数
     * @return 一个包含模型和精度的结构
     * @throws IOException
     */
    public Result train(String trainingFile, String developFile,
                        String modelFile, final double compressRatio,
                        final int maxIteration, final int threadNum) throws IOException
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
WordVectorModel API加载模型出现空指针错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.2
我使用的版本是：portable-1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用
```
java  -cp ~/.m2/repository/com/hankcs/hanlp/portable-1.7.2/hanlp-portable-1.7.2.jar: com.hankcs.hanlp.mining.word2vec.Train  -input ./data/hanlp-wiki-vec-zh/hanlp-wiki-vec-zh.txt -output data/hanlp-wiki-vec-zh.model
```
训练得到模型文件之后，使用

```
DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(MODEL_FILE_NAME));
```
加载报错如下：
```
java.lang.NullPointerException
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:40)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
```

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(MODEL_FILE_NAME));
float sim = docVectorModel.similarity(doc1,doc2);
...

```
### 期望输出

正常计算得到sim

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

出现空指针错误。

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
读取失败，问题发生在java.lang.NegativeArraySizeException,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在windows 10下运行相同的代码，没有问题，将项目部署在ubuntu18.04上时，则会出现BUG。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    Stream<Term> ab= HanLP.segment(“TEST”).stream();
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
无报错
### 实际输出
2019-03-21 22:46:39 WARN [HanLP]:368 - 读取失败，问题发生在java.lang.NegativeArraySizeException
        at com.hankcs.hanlp.dictionary.CoreDictionary$Attribute.<init>(CoreDictionary.java:232)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:356)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:320)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:70)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.<init>(ViterbiSegment.java:47)
        at com.hankcs.hanlp.HanLP.newSegment(HanLP.java:648)
        at com.hankcs.hanlp.tokenizer.StandardTokenizer.<clinit>(StandardTokenizer.java:31)
        at com.hankcs.hanlp.HanLP.segment(HanLP.java:636)


2019-03-21 22:46:39 WARN [HanLP]:161 - 读取失败，问题发生在java.lang.ArrayIndexOutOfBoundsException: 239
2019-03-21 22:46:40 WARN [HanLP]:204 - 尝试载入缓存文件/var/lib/tomcat8/webapps/survey-assessment/data/dictionary/CoreNatureDictionary.ngram.mini.txt.table.bin发生异常[java.io.StreamCorruptedException: invalid stream header: EFBFBDEF]，下面将载入源文件并自动缓存……
2019-03-21 22:46:41 DEBUG [org.springframework.web.multipart.commons.CommonsMultipartResolver]:282 - Cleaning up multipart file [file] with original filename [src_1.jpg], stored at [/var/lib/tomcat8/work/Catalina/localhost/survey-assessment/upload_01ae546b_0ad4_4dea_b1b7_fde6177867f5_00000000.tmp]
2019-03-21 22:46:41 DEBUG [org.springframework.web.servlet.DispatcherServlet]:984 - Could not complete request
org.springframework.web.util.NestedServletException: Handler processing failed; nested exception is java.lang.ExceptionInInitializerError

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
python 调用（LazyLoadingJClass） 文档向量DocVectorModel报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp  0.1.45
我使用的版本是：pyhanlp  0.1.45


<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用JClass的线程安全版SafeJClass或惰性加载LazyLoadingJClass，加载完词向量后，继续加载文档向量报错。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先：
WordVectorModel =LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
word2vec = WordVectorModel(save_model_path)
2. 然后：
DocVectorModel = LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
doc2vec = DocVectorModel(word2vec)
3. 接着：
运行之后就报错No matching overloads found for [init in find. at native\common\jp_method.cpp:127

### 触发代码

```
from pyhanlp import *
WordVectorModel =  LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel =  LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
word2vec = WordVectorModel(save_model_path)
doc2vec = DocVectorModel(word2vec)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
正常运行
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
RuntimeError: No matching overloads found for [init in find. at native\common\jp_method.cpp:127
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
dat分词大部分带“不”字的都分词错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
@hankcs 使用双数组trie树分词时，发现一个比较影响业务结果的问题：
如果待分词语句中带有“不……”的格式，“……”表示形容词或动词，大部分语句会分词错误，如“不成功”、“不失败”、“不好看”等等，会分词为“不成”+“功”、“不失”+“败”等等，在CoreNatureDictionary.txt词典和用户词典CustomDictionary.txt及现代汉语补充词库.txt等等词典中删除相关词如“不失”、“不成”才能分词成功。
这里的dat分词器，似乎并没有用到CoreNatureDictionary.ngram.txt及ngram.mini等模型（和它们缓存bin文件），利用[#384](https://github.com/hankcs/HanLP/issues/384)的方式修正无效。
使用dat分词器的话，有什么方式能修复这类问题吗？
<!-- 请详细描述问题，越详细越可能得到解决 -->


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        Segment segment = HanLP.newSegment(""dat"");
        List<Term> termList1 = segment .seg(""老是显示不正确"");
        List<Term> termList2 = segment .seg(""东西不好看，分词不失败是大概率事件"");
        //遍历list输出word
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
老是    显示    不    正确
东西    不    好看   ，    分词    不    失败     是    大   概率    事件
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
老是    显示    不正    确
东西    不好    看   分词    不失    败     是    大概    率    事件
```
（这里的“大概率”，分词成了“大概    率”也错误了）
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
使用viterbi和crf分词结果是正确的，但是为什么选用dat极速分词，因为我发现在做相似度匹配时，使用dat分词然后使用文档向量相似匹配，效果是最好的，猜测原因可能是大多数word2vec词向量模型的分词采用的是这种极速分词的方式。
"
直接在githun上下载hanlp-1.7.2-release.zip ，到200k左右后就失败，需要帮助谢谢。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Where I can find the CI service?,"Is this project using continuous integration services (e.g., Travis-CI or Jenkings)? Or has it used at any point of its lifetime?"
关于data-for-1.7.2.zip下载慢的问题，提供一个私人云盘链接供大家下载,https://drive.google.com/open?id=1NEN5KoPkgw0o_hpENfUyo0sitZtiIORN
老白干酒汉字转拼音问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

针对中文""老白干酒""的拼音转换问题，正确读音应该为“laobaiganjiu”""lbgj""，使用HanLP.convertToPinyinList()方法得到的是“laobaiqianjiu”。
虽然我通过在拼音字典中添加词项的方式可以解决，想知道这个问题产生的原因？

此外还碰到了“阿=a1,e1”，需要获取某个多音字的所有读音该调用什么方法实现？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        String text = ""老白干酒"";
        List<Pinyin> pinyinList = HanLP.convertToPinyinList(text);
        System.out.println(pinyinList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出 laobaiganjiu
```

### 实际输出 

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出 laobaiqianjiu  干字单独正确，但在老白干酒中被当做了千
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
未来有加入ERNIE模型的的计划吗,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
 

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
 最近看到BERT、GPT2.0等模型，而且百度这几天发布了ERNIE，据说效果比BERT还好，不知道HanLP是否有加入ERNIE的计划呢？
https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE
 https://www.jiqizhixin.com/articles/2019-03-16-3?from=synced&keyword=ERNIE
   
"
spark使用hanlp，提供一种简单的方案,"spark使用hanlp，提供一种简单的方案。我只用分词和自定义词典的功能，搜索了其他类似的问题和答案感觉对我来说太麻烦了，提供一个简单的方案。
一定是在各种Partitions函数里用，我是mapPartitions，自定义词典，我采用下面的代码
     def addStopWord(){
      CustomDictionary.dat.synchronized{
         brdicStopDic.value.foreach(x=> {
          CoreStopWordDictionary.add(x)})
      }
    }
简单就是添加停用词的函数里加锁，因为大数据环境下运行，词典添加多次就多次没有关系，不在乎这点性能，如果不加锁会报ArrayIndexOutOfBoundsException异常，原因很简单，并发调用。上面的函数是个例子，找了个静态对象 CustomDictionary.dat加锁，只要能保证线程安全，用其他对象也一样。"
关键词提取时取不同个数关键字所得到的结果不同？,"```
String content = ""【双11狂欢价】DHS/红双喜乒乓球拍五星级全能型5星乒乓球成品拍单拍旗舰店官网"";
for (int i = 1; i <= 10; i++) {
    System.out.println(HanLP.extractKeyword(content, i));
}
```
### 结果
```
[全能型]
[乒乓球, 全能型]
[全能型, 乒乓球, 乒乓球拍]
[全能型, 乒乓球, 成品, 乒乓球拍]
[乒乓球, 全能型, 乒乓球拍, 成品, 单拍]
[全能型, 乒乓球, 成品, 乒乓球拍, 红双喜, 单拍]
[全能型, 乒乓球, 乒乓球拍, 成品, 红双喜, 单拍, 旗舰店]
[全能型, 乒乓球, 成品, 乒乓球拍, 红双喜, 单拍, DHS, 旗舰店]
[全能型, 乒乓球, 成品, 乒乓球拍, 红双喜, 单拍, 旗舰店, DHS, 官网]
[全能型, 乒乓球, 成品, 乒乓球拍, 红双喜, 单拍, 旗舰店, DHS, 狂欢, 官网]
```
-----

这会导致我想取出唯一的关键字时出错，出现这种情况是因为输入句子的问题么？？"
使用NLPTokenizer报：data\dictionary\other\CharType.bin (系统找不到指定的文件。),"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2.

<!--以上属于必填项，以下可自由发挥-->

## data\dictionary\other\CharType.bin (系统找不到指定的文件。)

<!-- 请详细描述问题，越详细越可能得到解决 -->

##只是一个简单的demo，直接调用NLPTonizer.segment(""我的文档内容，篇幅大概有300字""）
执行报错
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
System.out.println(NLPTokenizer.segment(""部分文档：截止目前，申通、中通、圆通、汇通（现在叫百世）阿里都参了股，还在还剩韵达，不过我估计年内也会入股。""
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
读取data/dictionary/other/CharType.bin时发生异常java.io.FileNotFoundException: data\dictionary\other\CharType.bin (系统找不到指定的文件。)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
感知机命名实体识别结果不正确,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.7.2

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我用下面的句子
```
葛优,杨浦区国和路76544号74202室,7b01444445@126.com
```
的识别结果中,杨浦区国被识别为人名,想请问该如何修正结果


## 复现问题


### 步骤

去hanlp.com的官网测试

### 触发代码


### 期望输出


```
葛优/nr ,/w 杨浦区/ns 国和路/ns .......
```

### 实际输出


```
葛优/nr ,/w 杨浦区国/nr 和/c  路/n.......
```

## 其他信息

![image](https://user-images.githubusercontent.com/10104473/54171691-3bd40c80-44b6-11e9-9b5f-dc8e85aa8135.png)

"
KBeamArcEagerDependencyParser输出的词的DEPREL值，如 rcmod，nsubj这些如何理解,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.7.2
我使用的版本是：hanlp-1.7.2

请问， 最新提供的KBeamArcEagerDependencyParser输出的词的DEPREL值，如 rcmod，nsubj这些如何理解，哪里有文档么

"
问下，把一段话分成几个句子应该用哪个类？？？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：未使用过

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
想把一大段话断句，分隔成几个小句子，希望能结合一些算法之类的，不要纯句号处分隔之类的
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```java
    // 无
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```text
一个数组包含各个小分句
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何ByteArray.createByteArray载入jar包内部的bin文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

我使用ByteArray.createByteArray载入jar包内部的bin文件，使用了AhoCorasickDoubleArrayTrie这个结构
想把这个词典bin放到jar包里面，但是直接传入路径报错，我的词典路径是：
com/data/core/element/searchname_element.csv
```
    /**
     * 载入词典
     */
    public static boolean loadDat(String path, AhoCorasickDoubleArrayTrie<String> trie) {
        ByteArray byteArray = ByteArray.createByteArray(path + Predefine.BIN_EXT);
        if (byteArray == null) return false;
        int size = byteArray.nextInt();
        String[] valueArray = new String[size];
        for (int i = 0; i < valueArray.length; ++i)
        {
            valueArray[i] = byteArray.nextString();
        }
        trie.load(byteArray, valueArray);
        return true;
    }
```
调用方法是：
```
        String actPath=dictPath.substring(0, dictPath.lastIndexOf('.'));//去掉文件扩展名
        AhoCorasickDoubleArrayTrie<String> act =new AhoCorasickDoubleArrayTrie<String>();
        try{
            loadDat(actPath, act);//尝试载入
        }catch (Exception ex){
            ex.printStackTrace();
        }
```

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

复现问题，把该代码放到main函数下即可执行

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
```
我想该代码可以正常载入自己产生的缓存bin词典，通过loadDat载入
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实实际报出异常，是因为只能载入外部路径吗?不支持jar包里面的路径？
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词典reload问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
调用 CustomDictionary.reload() 接口，会报 java.lang.ArrayIndexOutOfBoundsException 异常

## 复现问题
这个问题应该是我添加自定义词典，修改CustomDictionaryPath后会出现
![image](https://user-images.githubusercontent.com/32827610/53787524-20b74900-3f5a-11e9-913d-9e339ac922a1.png)
异常是 reload() 时，走到这一步报的 allocSize 大于 base 的大小
![image](https://user-images.githubusercontent.com/32827610/53787581-53614180-3f5a-11e9-8515-2f6f5cce80ac.png)

### 步骤

1. 首先……
创建一个词典然后修改CustomDictionaryPath，方式：
![image](https://user-images.githubusercontent.com/32827610/53787524-20b74900-3f5a-11e9-913d-9e339ac922a1.png)
2. 然后……
自定义词典加载成功，调用 CustomDictionary.reload() 热更新
3. 接着……
发生异常
![image](https://user-images.githubusercontent.com/32827610/53787820-fdd96480-3f5a-11e9-8aa5-6f027ebbcb10.png)

### 触发代码

### 期望输出

<!-- 你希望输出什么样的正确结果？-->
是否是我自定义词典路径覆盖错误？ 
如果我如下处理，没有错误但是自定义的词典并不生效；我的分词器设置了enableCustomDictionaryForcing(true)

HanLP.Config.CustomDictionaryPath = new String[]{path，""zeus-rest-crawler/zeus-crawler-core/src/main/data/dictionary/customize/customDictionary.txt""};
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词库无效问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.2
我使用的版本是：1.7.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
![image](https://user-images.githubusercontent.com/24597473/53779150-12593500-3f3a-11e9-8e69-f8983733e830.png)

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args) {
    {
       // 动态增加
        CustomDictionary.add(""攻城狮"");
        CustomDictionary.add(""娶白富美"");
        // 强行插入
        CustomDictionary.insert(""娶白富美"", ""nz 1024"");
        // 删除词语（注释掉试试）
        CustomDictionary.remove(""迎娶"");
        System.out.println(CustomDictionary.add(""单身狗"", ""nz 1024 n 1""));
        System.out.println(CustomDictionary.get(""单身狗""));

        String text = ""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰"";  // 怎么可能噗哈哈！

        // AhoCorasickDoubleArrayTrie自动机扫描文本中出现的自定义词语
        final char[] charArray = text.toCharArray();
        CustomDictionary.parseText(charArray, new 
 AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
        {
            @Override
            public void hit(int begin, int end, CoreDictionary.Attribute value)
            {
                System.out.printf(""[%d:%d]=%s %s\n"", begin, end, new String(charArray, begin, end - begin), value);
            }
        });

        // 自定义词典在所有分词器中都有效
        System.out.println(HanLP.segment(text));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出  
true
nz 1024 n 1 
[0:3]=攻城狮 nz 1 
[5:8]=单身狗 nz 1024 n 1 
[10:14]=娶白富美 nz 1024 
[11:14]=白富美 nr 33 
[攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎/v, 娶白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出  
true
nz 1024 n 1 
[0:3]=攻城狮 nz 1 
[5:8]=单身狗 nz 1024 n 1 
[10:14]=娶白富美 nz 1024 
[11:14]=白富美 nr 33 
[攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
文本分类不支持增量吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.72
我使用的版本是：1.72

你好作者，各位朋友
每次新增文本分类时都要重新训练，如何解决这个问题。
我试过先加载模型在训练，但这不行，还是覆盖了原有的模型。"
在CRFNERecgonizer中添加自定义NERTags功能,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

在com.hankcs.hanlp.model.crf.CRFNERecognizer中, construct一个空模型的时候会自动添加三个默认label, nerTagSet作为一个private attribute, 并没有很好的支持自训练ner时添加新的nerTag的需求. 修改后, 在保留原有功能的情况下, 可以直接通过new CRFNERecognizer(null, customTagSet)


## 相关issue
暂无"
感知机标签分数计算部分提问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
最近在阅读主体识别感知机部分的源码。在com.hankcs.hanlp.model.perceptron.model.LinearModel.class 类内的标签分数计算部分有如下公式。
```
index = index * this.featureMap.tagSet.size() + currentTag;
score += (double)this.parameter[index];
```
而不是直接将特征乘以权重获得相应得分，请问这是出于什么考虑的呢？

"
通过train函数对中文语料训练CRF模型时出错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在通过train函数对中文语料训练CRF模型，得到模型文件CWS_MODEL_FILE 时，一直出错，并且如果CRFSegmenter停用，CRFLexicalAnalyzer接口没有相关的训练函数，该怎么对语料进行训练CRF模型？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

	public static void main(String[] args) throws Exception {
		//CRFTrain();
		testCWS();
	}
	//通过train函数对中文语料训练CRF模型，得到模型文件CWS_MODEL_FILE
	public static void CRFTrain() throws Exception
    {        
        CRFSegmenter segmenter = new CRFSegmenter(null);
        segmenter.train(CORPUSPATH,CWS_MODEL_FILE);
    }
	//通过CRFSegmenter函数和CRF模型文件CWS_MODEL_FILE，对测试语句进行分词
	public static void testCWS() throws Exception
	{
		//CRFSegmenter segmenter = new CRFSegmenter(CWS_MODEL_FILE);
        CRFSegmenter segmenter = new CRFSegmenter();
        List<String> wordList = segmenter.segment(SENTENCE);
        System.out.println(wordList);
    }
}
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
测试上述代码， 在得到CRF模型语句segmenter.train(CORPUSPATH,CWS_MODEL_FILE);时出错

```


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
reload加载错误ArrayIndexOutOfBoundsException,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在词库进行reload操作时出现数组溢出，在1.7.0版本中没有该问题。
是否和1.7.1修改reload生成自定义词典bin文件在重新载入时抛出异常ArrayIndexOutOfBoundsException #1028该问题有关

## 复现问题

### 步骤
`在CustomDictionary类的loadMainDictionary方法中dat.build(map);时发生异常。
经过跟踪发现在DoubleArrayTrie的

```
   private int resize(int newSize)
    {
        int[] base2 = new int[newSize];
        int[] check2 = new int[newSize];
        if (allocSize > 0)
        {
            System.arraycopy(base, 0, base2, 0, allocSize);
            System.arraycopy(check, 0, check2, 0, allocSize);
        }

        base = base2;
        check = check2;

        return allocSize = newSize;
    }
```
在System.arraycopy(base, 0, base2, 0, allocSize);中base做为拷贝源数据长度为1426181
base2的长度为2097152，而allocSize复制长度也为2097152，导致数据溢出
### 触发代码

```
   HanLP.segment(""自然语言处理"");
   CustomDictionary.reload();
```
### 期望输出

正常重载


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
警告: 自定义词典./work/data/dictionary/custom/CustomDictionary.txt缓存失败！
java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 2097152 out of bounds for int[1426181]
	at java.base/java.lang.System.arraycopy(Native Method)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.resize(DoubleArrayTrie.java:94)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:403)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:338)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:365)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:378)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:107)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
	at com.hankcs.hanlp.dictionary.CustomDictionary.reload(CustomDictionary.java:657)
	at com.gildata.removal.Test_Hanlp.main(Test_Hanlp.java:21)
```

## 其他信息
希望能够指教一下
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请教词频获取问题。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在对广东省进行主体识别时，获取form：始##始 to：广，的词频是78。
但在CoreNatureDictionary.ngram.txt内并未找到，始##始@广  的记录。
请问这个词频是从何获取的?小白提问提前谢过。
![image](http://thyrsi.com/t6/672/1550717998x2890202402.jpg)

"
可以提供隐马维特比训练命名实体的语料格式吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
hanks你好，在你的帮助下我这边已经熟悉感知机的操作，所以现在想试试用隐马和维特比来训练新实体。
我参照wiki中的https://github.com/hankcs/HanLP/wiki/%E8%A7%92%E8%89%B2%E6%A0%87%E6%B3%A8%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93  进行了仿写，
但是在TestNRDctionaryMaker.java中的语料文件【""data/dictionary/2014_dictionary.txt""】和【D:\JavaProjects\CorpusToolBox\data\2014\】并没有在任何地方找到，希望可以提供该处语料所需的格式，以准备新实体的语料。
我已在relese中下载了最新的data的压缩包并没有找到对应文件，https://github.com/hankcs/HanLP/issues/311  这个issue中所提供的页面也已经找不到了。
感谢您的时间！
"
pyhanlp有rasa的组件吗，类似tokenizer_jieba这样的,"最近在看RASA, 觉得分词组件还是HanLP好，请问pyhanlp有rasa的组件吗，类似tokenizer_jieba这样的？
tokenizer_jieba在下面的里面有介绍
https://blog.csdn.net/u010505246/article/details/82997100
https://www.jianshu.com/u/4b912e917c2e
https://blog.csdn.net/u010505246/article/details/83276354
"
KBeamArcEagerDependencyParser 缺少data文件,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

从github上拿的最新代码和data. 使用KBeamArcEagerDependencyParser做句法分析
运行缺少 data/model/dependency/perceptron.bin
和 data/model/perceptron/ctb/pos.bin

<!-- 请详细描述问题，越详细越可能得到解决 -->
"
关于复合词的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP 1.7.1
我使用的版本是：HanLP 1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
非常感谢作者提供功能强大的工具。
1.我使用data-for-1.7.1的模型进行分词和词性标记，假如我要处理的句子为：“香港特别行政区的张朝阳说商品和服务是三原县鲁桥食品厂的主营业务。”，按照结构化感知机标注框架中的说明，“香港特别行政区”和“三原县鲁桥食品厂”应该作为复合词出现在结果中，但是我利用感知机模型进行分词时并没有得到说明中的结果。
2.分词结果：""[香港/ns, 特别/a, 行政区/n]/ns""表明香港特别行政区是作为一个词进行分割的，和结果：“香港特别行政区/ns”的区别是什么？我的理解是，对于前者，当“香港特别行政区”这几个字出现在一起的时候，作为一个词分割，只出现“香港”的时候，也可以把香港分割出来，比如“香港是购物天堂”会被分成“香港/ns 是/v 购物/v 天堂/n”，而对于后者，如果训练数据中只有标注的“香港特别行政区/ns”，而没有“香港/ns”的标注，关于香港的分割及标记可能会出错。


## 复现问题

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    String sentence = ""香港特别行政区的张朝阳说商品和服务是三原县鲁桥食品厂的主营业务。"";
PerceptronLexicalAnalyzer segmenter = new PerceptronLexicalAnalyzer(
                ""data-for-1.7.1\\data\\model\\perceptron\\pku199801\\cws.bin"",
                ""data-for-1.7.1\\data\\model\\perceptron\\pku199801\\pos.bin"");
        Sentence sentence = segmenter.analyze(sentences);
        System.out.println(sentence);
```
### 期望输出
[香港/ns 特别/a 行政区/n]/ns 的/n 张朝阳/nr 说/v 商品/n 和/c 服务/vn 是/v [三原县/ns 鲁桥/nz 食品厂/n]/nt 的/z 主营/vn 业务/n


### 实际输出

香港/ns 特别行政区/nz 的/u 张朝阳/nr 说/v 商品/n 和/c 服务/vn 是/v 三原县/ns 鲁桥/nz 食品厂/n 的/u 主营/vn 业务/n 。/w
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
“着”的拼音不正确,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

使用转拼音功能时，发现汉字“着”在任何时候都输出了“zhuó”，但是其实大多数应该输出“zhe”的。

比如：盼望着盼望着，春天来了；功名浑是错，更莫思量着；悄没声的河沿上，满铺着寂寞和黑暗；我从山中来，带着兰花草。。等等这些句子里的“着”都输出了拼音“zhuó”，但是明显应该是“zhe”

（大部分其他字的拼音结果都是正确的）

### 触发代码

使用gradle自动配置方式，然后使用以下代码：
List<Pinyin> pinyinList = HanLP.convertToPinyinList(text);
for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getPinyinWithToneMark());
        }
"
DoubleArrayTrie的2个问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
*[x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.1
我使用的版本是：hanlpVersion='portable-1.7.1'

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1. 每次刷新自定义词典时出现下面的错误，经过分析，是因为扩展数组时出错，
     DoubleArrayTrie.java的private int resize(int newSize)方法的如下2行代码： 数据下标越界异常
  每次都是allocSize比base2的实际size大一个。
      System.arraycopy(base, 0, base2, 0, allocSize);
      System.arraycopy(check, 0, check2, 0, allocSize);

---错误堆栈信息如下
[2019-02-12 17:43:31,193] WARN  [http-nio-0.0.0.0-8083-exec-3] com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:150) 自定义词典D:/myDev/hanlpData/data/dictionary/custom/CustomDictionary.txt缓存失败！
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.resize(DoubleArrayTrie.java:94)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:403)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:338)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:365)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:378)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:107)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
	at com.hankcs.hanlp.dictionary.CustomDictionary.reload(CustomDictionary.java:658)
   
2.     DoubleArrayTrie.java的 private BitSet used; 这个字段曾经多次出现过空指针异常。
        当时debug，发现虽然构造函数里面有new, 但是clear和build等诸多方法都有设置为null,
        设置为null之后，下一次运行，肯定空指针。 
         debug中途意外重启，之后又ok了。就没有深入分析了，目前没有再次重现。
"
句子的句法分析结果不是以关系图的结果显示了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.0


## 我的问题

句子的句法分析结果不是以关系图的结果显示了


"
pyhanlp如何调用java版本的聚类方法,"注意事项
请确认下列注意事项：
我已仔细阅读下列文档，都没有找到答案： 
首页文档
wiki
常见问题
我已经通过Google和issue区检索功能搜索了我的问题，也没有找到答案。
我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
 我在此括号内输入x打钩，代表上述事项确认完毕。
版本号
当前最新版本号是：
我使用的版本是：hanlp-1.7.0
我的问题：
在 com.hankcs.hanlp.mining.cluster.ClusterAnalyzer 中，ClusterAnalyzer的初始化是用泛型参数：
public class ClusterAnalyzer<K>
{...}
但是jpype中没找到传泛型参数的方法，请问如何用python调用？"
衍生项目之 elasticsearch-hanlp 分词插件包,"项目地址：https://github.com/AnyListen/elasticsearch-analysis-hanlp
兼容范围：ES5.X、ES6.X

基于 HanLP 的 Elasticsearch 中文分词插件，核心功能：
- 内置词典，无需额外配置即可使用；
- 支持用户自定义词典；
- 支持远程词典热更新（待开发）；
- 内置多种分词模式，适合不同场景；
- 拼音过滤器（待开发）；
- 简繁体转换过滤器（待开发）。"
这个分词分成这样，为啥呢？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
分词错误
<!-- 请详细描述问题，越详细越可能得到解决 -->
""努力造就一支忠诚干净担当的高素质干部队伍""分词错误
## 复现问题
### 触发代码

```
List<Term> testA = HanLP.segment(""努力造就一支忠诚干净担当的高素质干部队伍"");
```
### 期望输出

```
期望输出:一支/  忠诚/ 干净
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
一/支忠诚/ 干净
```

## 其他信息
none
"
关于感知机在线学习，中文符号无法正确识别词性问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我使用的是python版本的，但我想应该不会影响下面的问题；在训练的时候始终无法训练到想要的结果。那个中文的 '：'始终是n，不知道什么原因，请教一下

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```

from pyhanlp import *
from jpype import *
jvmPath = getDefaultJVMPath()
if not isJVMStarted():
    startJVM(jvmPath)
PerceptronLexicalAnalyzer = JPackage(""com"").hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer
Config = JPackage(""com"").hankcs.hanlp.model.perceptron.Config
LinearModel = JPackage(""com"").hankcs.hanlp.model.perceptron.model.LinearModel
CoreStopWordDictionary = JPackage(""com"").hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary
NotionalTokenizer = JPackage(""com"").hankcs.hanlp.tokenizer.NotionalTokenizer

cws_model = LinearModel(HanLP.Config.PerceptronCWSModelPath)
ner_model = LinearModel(HanLP.Config.PerceptronNERModelPath)
pos_model = LinearModel(HanLP.Config.PerceptronPOSModelPath)

analyzer = PerceptronLexicalAnalyzer(cws_model, pos_model, ner_model)

analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
print(analyzer.analyze(""版式设计：沈亦伶《人民日报》（2018年11月09日06版）""))

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
版式/n 设计/vn ：/n 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日06/m 版/n ）/w

// 多训练几遍
版式/n 设计/vn ：/n 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于感知机在线学习，中文符号无法正确识别词性问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.0

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

我使用的是python版本的，但我想应该不会影响下面的问题；在训练的时候始终无法训练到想要的结果。那个中文的 '：'始终是n，不知道什么原因，请教一下

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……


### 触发代码

```
from pyhanlp import *
from jpype import *
jvmPath = getDefaultJVMPath()
if not isJVMStarted():
    startJVM(jvmPath)
PerceptronLexicalAnalyzer = JPackage(""com"").hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer
Config = JPackage(""com"").hankcs.hanlp.model.perceptron.Config
LinearModel = JPackage(""com"").hankcs.hanlp.model.perceptron.model.LinearModel
CoreStopWordDictionary = JPackage(""com"").hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary
NotionalTokenizer = JPackage(""com"").hankcs.hanlp.tokenizer.NotionalTokenizer

cws_model = LinearModel(HanLP.Config.PerceptronCWSModelPath)
ner_model = LinearModel(HanLP.Config.PerceptronNERModelPath)
pos_model = LinearModel(HanLP.Config.PerceptronPOSModelPath)

analyzer = PerceptronLexicalAnalyzer(cws_model, pos_model, ner_model)

analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
#analyzer.learn(""版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w"")
print(analyzer.analyze(""版式设计：沈亦伶《人民日报》（2018年11月09日06版）""))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
版式/n 设计/vn ：/w 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
版式/n 设计/vn ：/n 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日06/m 版/n ）/w

// 多训练几遍
版式/n 设计/vn ：/n 沈亦伶/nr 《/w 人民日报/nz 》/w （/w 2018年11月09日/t 06版/m ）/w
```
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
请教几个关于特殊人名实体识别的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1、我搜索TranslatedPersonRecognition和JapenesePersonRecognition发现这两个java类只被维特比、最短路径、n最短路径分词所用到，CRF和感知机分词没有用到，那么如果我想让CRF和感知机分词能识别日本人名和翻译人名是否只能依靠训练新的相关语料？
2、如果我想识别新的人名类型（如少数民族人名），字典文件是否只需要准备一份带有人名关键字的nrx.txt呢？trie.dat和value.dat是自动生成的吗？
再次感谢您的时间和耐心！
<!-- 请详细描述问题，越详细越可能得到解决 -->

### 触发代码

```
   public class DemoJapaneseNameRecognition
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""北川景子参演了林诣彬导演的《速度与激情3》"",
                ""林志玲亮相网友:确定不是波多野结衣？"",
                ""龟山千广和近藤公园在龟山公园里喝酒赏花"",
        };
        Segment segment = HanLP.newSegment(""crf"").enableJapaneseNameRecognize(true);
        for (String sentence : testCase)
        {
            List<Term> termList = segment.seg(sentence);
            System.out.println(termList);
        }

    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]
[林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w]
[龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[北川/ns, 景子/n, 参演/v, 了/u, 林诣彬/nr, 导演/n, 的/u, 《/w, 速度/n, 与/c, 激情/n, 3/m, 》/w]
[林志玲/nr, 亮相/v, 网友/n, :/w, 确定/v, 不/d, 是/v, 波多野/n, 结衣/n, ？/w]
[龟/v, 山/n, 千/m, 广/q, 和/c, 近藤/a, 公园/n, 在/p, 龟山公园/ns, 里/f, 喝/v, 酒/n, 赏/v, 花/n]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
有方法能判断一个字是不是多音字吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
加载新训练好的CRF模型时出错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：hanlp 1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

训练好CRF模型后，在使用CRFLexicalAnalyzer或者CRFSegmenter分词时，加载时出错，
具体错误是
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""1@?@@?@""
	at java.lang.NumberFormatException.forInputString(Unknown Source)
	at java.lang.Integer.parseInt(Unknown Source)
	at java.lang.Integer.parseInt(Unknown Source)
	at com.hankcs.hanlp.model.crf.LogLinearModel.convert(LogLinearModel.java:124)
	at com.hankcs.hanlp.model.crf.LogLinearModel.<init>(LogLinearModel.java:101)
	at com.hankcs.hanlp.model.crf.CRFTagger.<init>(CRFTagger.java:41)
	at com.hankcs.hanlp.model.crf.CRFSegmenter.<init>(CRFSegmenter.java:47)
	at com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer.<init>(CRFLexicalAnalyzer.java:71)
	at fenci.CRFCWSTrain.main(CRFCWSTrain.java:59)

想问一下，出现上述错误可能是由哪些原因导致的，在调用HanLP和CRF++模型的时候都会出现上述问题。感谢。"
如何在训练自定义词典、CRF模型、感知机模型的过程中添加新的词性,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：hanlp-1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

请问一下，如何在训练自定义词典、CRF模型、感知机模型的过程中添加新的词性？

"
"chisquareScore = stats.n * Math.pow(N11 * N00 - N10 * N01, 2) / ((N11 + N01) * (N11 + N10) * (N10 + N00) * (N01 + N00))计算值溢出","ChiSquareFeatureExtractor中的这行代码N11等变量均为int型， 计算时会引发数值溢出， 虽然不会报错， 但是计算结果是错误的
2000L * 82 * 2001 * 3919 = 1286074716000
但是实际程序中：
2000 * 82 * 2001 * 3919 = 1879494496，这个值是错误的， 导致卡方检测运算结果其实是错误的"
您好，为什么hanlp-1.7.0-release.zip文件下载不了？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
NLPTokenizer分词可能存在bug，分词时不能按用户自定义词典切分,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.1
我使用的版本是：portable-1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

NLPTokenizer分词可能存在bug，分词时不能按用户自定义词典切分


### 触发代码

```
    public static void main(String[] args) throws IOException
    {
      		CustomDictionary.insert(""晚霞"", ""wx 1"");
		System.out.println(NLPTokenizer.segment(""我的希望是希望张晚霞的背影被晚霞映红""));
        // 注意观察下面两个“希望”的词性、两个“晚霞”的词性
		System.out.println(NLPTokenizer.analyze(""我的希望是希望张晚霞的背影被晚霞映红"").translateLabels());
    }
```
### 期望输出



```
[我/r, 的/u, 希望/n, 是/v, 希望/v, 张晚霞/nr, 的/u, 背影/n, 被/p, 晚霞/wx, 映红/nr]
我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/wx 映红/人名
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[我/r, 的/u, 希望/n, 是/v, 希望/v, 张晚霞/nr, 的/u, 背影/n, 被/p, 晚霞/n, 映红/nr]
我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/wx 映红/人名
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
add DeprelTranslator support for MaxEntDependencyParser,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
MaxEntDependencyParser 支持英文标签输出


"
求问demo（hanlp.com）里面用的分词器和依存句法分析器,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
请问demo（http://hanlp.com）使用了什么分词器和依存分析器？

我在demo上测了一些句子，发现效果比较好，然而在本地使用1.7.1代码测试。发现和demo有很大差距

1. 首先是分词器。
NeuralNetworkDependencyParser.compute(String sentence)使用的是CharacterBasedSegment，无法使用自定义词典，所以效果不太好。我给替换成了ViterbiSegment分词器，效果还可以，但是和demo还是有些出入，想问下demo中使用的分词器是哪个？

2. 依存句法分析器
测试句子：我送她一束花

demo的结果：
![21_05_33__01_02_2019](https://user-images.githubusercontent.com/11264714/50593202-56946e80-0ed2-11e9-9947-df12a1505ac3.jpg)

本地测试代码：
NeuralNetworkDependencyParser dependencyParser = new NeuralNetworkDependencyParser();
dependencyParser.setSegment(new ViterbiSegment());
System.out.println(dependencyParser.parse(""我送她一束花。""));
本地测试结果：
![21_07_51__01_02_2019](https://user-images.githubusercontent.com/11264714/50593297-c0ad1380-0ed2-11e9-833f-b64f84158655.jpg)


试了一些句子，并没有分析出间宾关系，然而demo中可以正常分析出。所以想请问下demo中使用的依存句法分析器有什么不同，谢谢！


"
DemoWord2Vec实际运行结果和wiki文档中描述的结果结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP 1.7.1
我使用的版本是： HanLP 1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我将word2vecDemo原封不动的copy下来，数据集也是文档中提到的搜狗数据，按照wiki文档中的每一种应用场景(比如计算山东-江苏的语义距离)运行了一遍。但发现实际的运行结果与文档中的结果不一致。请解释一下这是为什么？另外相似度计算的结果为何出现了负值？难道不应该是0-1之间的值吗？

文档传送门：https://github.com/hankcs/HanLP/wiki/word2vec

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先Copy文档中的Demo程序，下载搜狗语料，训练模型
2. 然后添加了两行文档中的代码
3. 接着运行结果

### 触发代码

```
  System.out.println(wordVectorModel.similarity(""山东"", ""江苏""));
  System.out.println(wordVectorModel.similarity(""山东"", ""上班""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
0.81871825
0.25067142
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
0.39530003
-0.004012134
```


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于分词问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
最近下载最新的版本1.7.1，运行 DemoCRFLexicalAnalyzer 和 DemoPerceptronLexicalAnalyzer 测试分词，发现 微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。对于人名的识别不对，以前版本我记得是没问题的。现在分词的结果是：
微软公司/ntc 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v ，/w 18年/t 啟動/v 以/p 智慧/n 雲端/n 、/w 前端/f 為/v 導向/n 的/u 大/a 改組/vn 。/w

## 复现问题
运行 DemoCRFLexicalAnalyzer 类中的方法

### 步骤
运行 DemoCRFLexicalAnalyzer 类中的方法

### 触发代码
public static void main(String[] args) throws IOException
    {
        CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
        analyzer.enableAllNamedEntityRecognize(true)
        .enableCustomDictionary(true)
        .enableOrganizationRecognize(true)
        .enablePartOfSpeechTagging(true)
        .enableTranslatedNameRecognize(true)
        .enableNameRecognize(true);
        
        String[] tests = new String[]{
            ""商品和服务"",
            ""上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观"",
            ""微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。"", 
            ""微软公司於1975年由比尔·盖茨和保罗·艾伦创立，18年启动以智慧云端、前端为导向的大改组。"" 
        };
        for (String sentence : tests)
        {
            System.out.println(analyzer.analyze(sentence));
//            System.out.println(analyzer.seg(sentence));
        }
    }

期望输出
微软公司/ntc 於/p 1975年/t 由/p 比爾·蓋茲/nrf 和/c 保羅·艾倫/nrf 創立/v ，/w 18年/t 啟動/v 以/p 智慧/n 雲端/n 、/w 前端/n 為/v 導向/n 的/u 大/a 改組/vn 。/w

实际输出
微软公司/ntc 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/v 創立/v ，/w 18年/t 啟動/v 以/p 智慧/n 雲端/n 、/w 前端/n 為/v 導向/n 的/u 大/a 改組/vn 。/w
存在问题的地方：比爾·蓋茲/n 和/c 保羅·艾倫/v
这两个人名识别不对。
"
在CRF模型的基础上训练自己的模型,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP v1.2.8
我使用的版本是：hanlp-1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

感知机分词/词性识别、CRF分词，能否通过我们的自己的训练模型作为对程序本身提供的模型的补充，不仅限于在线学习，该怎么实现对模型的修改，或者是不是可以通过训练出模型文件是否放到相关目录下即可启用？"
使用hanlp分词造成内存泄漏,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 使用hanlp分词做为elasticsearch的分词器

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用hanlp 1.7.1做为elasticsearch的分词器,启动之后内存消耗上升,之后出现java.lang.OutOfMemoryError: Java heap space Java的内存分配为2g
jvm 内存分配参数 
-Xms2g
-Xmx2g

## 复现问题
暂无

### 步骤
暂无
### 触发代码
    @Override
    public boolean load(ByteArray byteArray)
    {
        if (byteArray == null)
            return false;
        featureMap = new ImmutableFeatureMDatMap();
        featureMap.load(byteArray);
        int size = featureMap.size();
        TagSet tagSet = featureMap.tagSet;
        if (tagSet.type == TaskType.CLASSIFICATION)
        {
            parameter = new float[size];
            for (int i = 0; i < size; i++)
            {
                parameter[i] = byteArray.nextFloat();
            }
        }
        else
        {
            parameter = new float[size * tagSet.size()];
            for (int i = 0; i < size; i++)
            {
                for (int j = 0; j < tagSet.size(); ++j)
                {
                    parameter[i * tagSet.size() + j] = byteArray.nextFloat();
                }
            }
        }
//        assert !byteArray.hasMore();
//        byteArray.close();
        if (!byteArray.hasMore())
            byteArray.close();
        return true;
    }
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息
错误日志信息
[2018-12-25T20:20:51,407][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [chechi] fatal error in thread [elasticsearch[chechi][masterService#updateTask][T#1]], exiting
java.lang.OutOfMemoryError: Java heap space
	at com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(IntArrayList.java:180) ~[?:?]
	at com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger.load(MutableDoubleArrayTrieInteger.java:1185) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:93) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:95) ~[?:?]
	at com.hankcs.hanlp.HanLP.newSegment(HanLP.java:685) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper.lambda$getSegment$0(ConfigHelper.java:149) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper$$Lambda$2916/0x0000000800aed840.run(Unknown Source) ~[?:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper.getSegment(ConfigHelper.java:132) ~[?:?]
	at org.elasticsearch.plugin.hanlp.analysis.HanLPAnalyzerProvider.<init>(HanLPAnalyzerProvider.java:35) ~[?:?]
	at org.elasticsearch.plugin.hanlp.analysis.HanLPAnalyzerProvider.getPerceptronAnalyzerProvider(HanLPAnalyzerProvider.java:51) ~[?:?]
	at org.elasticsearch.plugin.hanlp.AnalysisHanLPPlugin$$Lambda$778/0x0000000800512840.get(Unknown Source) ~[?:?]
	at org.elasticsearch.index.analysis.AnalysisRegistry.buildMapping(AnalysisRegistry.java:367) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.analysis.AnalysisRegistry.buildAnalyzerFactories(AnalysisRegistry.java:191) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.analysis.AnalysisRegistry.build(AnalysisRegistry.java:160) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.IndexService.<init>(IndexService.java:165) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.IndexModule.newIndexService(IndexModule.java:397) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.indices.IndicesService.createIndexService(IndicesService.java:503) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:457) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService.validateAndAddTemplate(MetaDataIndexTemplateService.java:233) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService.access$300(MetaDataIndexTemplateService.java:64) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService$2.execute(MetaDataIndexTemplateService.java:174) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:639) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:268) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:198) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:133) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) ~[elasticsearch-6.5.4.jar:6.5.4]"
中文API乱码，eclipse maven 导入后方法文档是乱码,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.7.1
我使用的版本是：portable-1.7.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

api文档是乱码
类似于这样：
杞寲涓烘嫾闊筹紙棣栧瓧姣嶏級Parameters:text 鏂囨湰separator 鍒嗛殧绗�remainNone 鏈変簺瀛楁病鏈夋嫾闊筹紙濡傛爣鐐癸級锛屾槸鍚︿繚鐣欏畠浠紙鐢╪one琛ㄧず锛�Returns:涓�涓瓧绗︿覆锛岀敱[棣栧瓧姣峕[鍒嗛殧绗[棣栧瓧姣峕鏋勬垚
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp支持word2vec模型训练吗？, pyhanlp支持word2vec模型训练吗？谢谢！
使用繁體分詞後，髮變發,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.1
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用 TraditionalChineseTokenizer.segment 來分詞 ""飛利浦整髮造型吹風梳""

### 期望输出

```
[飛利浦/ntc, 整髮/v, 造型/n, 吹風/vn, 梳/v]
```

### 实际输出

```
[飛利浦/ntc, 整發/v, 造型/n, 吹風/vn, 梳/v]
```

### 其他信息
用NLPTokenizer.analyze來分詞則是期望的輸出

"
旧版的data文件404了,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.7.1
我使用的版本是：1.6.4

## 我的问题

从releases页面中，找到v1.6.4常规维护版, 里面的 data-for-1.6.4.zip 链接失效了, 404
"
繁简转换出错,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7
我使用的版本是：1.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

1.7master版本，使用繁简转换，“21克拉”，转换结果为“21克拉布”。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1.7master版本，使用繁简转换，“21克拉”，转换结果为“21克拉布”。

### 触发代码

```
    System.out.println(HanLP.convertToSimplifiedChinese(""21克拉""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
21克拉
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
21克拉布
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
最大熵模型的依存句法分析没有训练接口吗？代码中只看到直接加载训练好的模型,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何设置自定义词典最高优先级级 不管其他词分词效果,"请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

当前最新版本号是：HanLP v1.2.8
我使用的版本是：portable-1.7.0

## 我的问题

自定义了一批词，并使词具有统一的词性，但是部分词未按自定义词典分词；我不需要考虑其它分词的准确性，只要求自定义的词被全部识别。谢谢

    public void testIssue1234() throws Exception
    {
        CustomDictionary.insert(""中关村科技"",""company  10"");
        CustomDictionary.insert(""人民网"",""company  10"");

       HanLP.newSegment().enableCustomDictionaryForcing(true);
        System.out.println(HanLP.segment(""人民网，中关村科技园区管理委员会副巡视员刘航介绍了""));
    }

 期望输出
[人民网/company, ，/w, 中关村科技/company，园区/ns, 管理/vn, 委员会/n, 副/b, 巡视员/n, 刘航/nr, 介绍/v, 了/ul]

实际输出
[人民网/company, ，/w, 中关村/ns, 科技园区/ns, 管理/vn, 委员会/n, 副/b, 巡视员/n, 刘航/nr, 介绍/v, 了/ul]"
请问hanlp有没有类似jieba分词的全模式,"比如：
“我来到北京清华大学”
用jieba分词：
【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
【精确模式】: 我/ 来到/ 北京/ 清华大学
全模式把特殊名词之类的切分出来之后，还可以继续切分下去，hanlp有类似的功能吗

"
HMM的机构名识别的自动转换程序生成的规则串缺失,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
当前最新版本号是：1.7.0
我使用的版本是：1.7.0（master）

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我目前需要识别是1千万的公司名，使用一下代码提取之后：
`       
 
        EasyDictionary dictionary = new EasyDictionary();
        final NTDictionaryMaker ntDictionaryMaker = new NTDictionaryMaker(dictionary);
        CorpusLoader.walk(""/home/gildata/cheny/company/"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(Document document)
            {
                ntDictionaryMaker.compute(document.getComplexSentenceList());
            }
        });
        ntDictionaryMaker.saveTxtTo(""/home/gildata/cheny/nt"");

`
发现训练生成的nt.pattern.txt共19w左右的，但是经过测试还缺少模式串，导致1000w数据中有123w的公司名没有识别出来。

### 期望输出

训练生成的nt.pattern.txt能够包含所有的规则模式串

谢谢各位大大

"
能否查找某一个词语的义项数？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP1.7.0
我使用的版本是：HanLP 1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
  我想计算文章的一词多义比率，需要获得两个值。一个是词语的所有义项数之和，另一个是词语数。后者可以通过分词轻松获得，但前者不知HanLP能否实现？也就是说，我想输入一个词语，获得这个词语的所有意思个数。请问HanLP有这个功能吗？

如果没有集成这个功能，请问如何实现这个功能呢？找了很久，也没有找到存有义项数的词典。
谢谢！

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
无

### 步骤

无

### 触发代码

```
   无
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```输入一个词语，获得这个词语的所有意思个数

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出无
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
getAccuracy的返回值个数如何控制？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
请教一下模型训练，也就是PerceptronTrainer里面的getAccuracy函数何时会返回一个准确度p，何时又会返回三个值p、r、f。这个返回是和语料有关吗？请问有明确的标准使得训练结果能得到一个p值或得到3个值吗？谢谢！
<!-- 请详细描述问题，越详细越可能得到解决 -->

"
句子中标点前如有空格 seg2sentence 会分割为一句话,"请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
当前最新版本号是：HanLP 1.7.0 pyhanlp 0.1.44
我使用的版本是：HanLP 1.7.0 pyhanlp 0.1.44

## 我的问题

如题 如果句子中的标点（无论中英文）前有空格时 seg2sentence 会将其分割为一句话 数字也是
另外如果句子中有英文等非中文 且单词和汉字间有空格的话 也会被分割为一句话
虽然中文标点和汉字之间按标准的排版来说肯定是不能加空格的 但毕竟不是所有文本都会遵守
而且如果中文句子里含有英文 有人排版上是习惯在英文单词和汉字的衔接处加一个汉字的 另外如果中文句子里含有连续的两个或两个以上的英文（或其他印欧语系的语种）单词 那么其中必定包含空格。

另外我看[这个 Commit](https://github.com/hankcs/HanLP/commit/1094563c0f13fb1438cc1f9f155a88351e38ac8c) 说修复了我提的 #1019 但我试了下
`for s in standard_tokenizer.seg2sentence('这是一个句子，中间有个逗号。', shortest = False): print(s)`

结果还是不对 实在是不会 Java 这几个重载的函数头都晕了

还有用分词器的 seg2sentence 函数会进一步把句子再切分为 token 的列表 这样我想要句子的字符串还要手动拼回去
但是上面我说的问题 中文里含有英文时 就算切割正确 英文和数字都会被切成独立的 token 这样我拼回去还得再判断一下 很麻烦
com.hankcs.hanlp.utility.SentencesUtil.toSentenceList 直接就是返回每句句子的字符串 这个接口能直接句子级别的分割吗？（不在逗号断开）

最后强迫症表示 pyhanlp 项目的命名是否应该规范为 PyHanLP 比较好 和 HanLP 对应。。。（我指 ReadMe 和 PyPI 里的标题 Package 名倒是无所谓）

### 触发代码

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')

for s in standard_tokenizer.SEGMENT.seg2sentence('我用不来 Java，只会 Python。', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('今天天气真好 （宜出行 * 左括号前有空格）', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('pyhanlp 需要 Java 1.8+ 才能运行。', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('你知道gangnam style吗。', False):
	print(s)
```
### 期望输出

```
[我/rr, 用/p, 不来/v, Java/nx, ，/w, 只/d, 会/v, Python/nx, 。/w]
[今天天气/nz, 真好/d, （/w, 宜/ag, 出行/vi, */w, 左/f, 括号/n, 前/f, 有/vyou, 空格/n, ）/w]
[pyhanlp/nx, 需要/v, Java/nx, 1.8/m, +/w, 才能/n, 运行/vn, 。/w]
[你/rr, 知道/v, gangnam/nx, style/nx, 吗/y, 。/w]
```

### 实际输出

```
[我/rr, 用/p, 不来/v]
[Java/nx, ，/w, 只/d, 会/v]
[Python/nx, 。/w]

[今天天气/nz, 真好/d]
[（/w, 宜/ag, 出行/vi]
[*/w]
[左/f, 括号/n, 前/f, 有/vyou, 空格/n, ）/w]

[pyhanlp/nx]
[需要/v]
[Java/nx]
[1.8/m, +/w]
[才能/n, 运行/vn, 。/w]

[你/rr, 知道/v, gangnam/nx]
[style/nx, 吗/y, 。/w]
```
"
如果识别所有的商品品牌，可以用什么方式实现,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0（master）

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

目前需要识别所有的品牌名称，有近亿条，这些品牌名称数据都有，因为数据量太多，所有不可能加载到自定义词库中，而他又不像人名和公司名那样是有一定的规则的，所以想请教一下，有什么其他可以实现的路径吗


"
繁体转简体错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用HanLP由繁体转换为简体时，“一克拉”为转换为“一克拉布”
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤


### 触发代码

```
    import com.hankcs.hanlp.HanLP;

/**
 * 将简繁转换做到极致
 *
 * @author hankcs
 */
public class DemoTraditionalChinese2SimplifiedChinese
{
    public static void main(String[] args)
    {
        System.out.println(HanLP.convertToSimplifiedChinese(""一克拉""));
    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
“一克拉”
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
“一克拉布”
## 其他信息
所有的版本都存在这个问题。
另外“乾隆”被转换为“干隆”，明星“张钧甯”亦被转换为“张钧宁”
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
使用pyhanlp进行分词时意外停止报错Process finished with exit code -1073740940 (0xC0000374),"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

语料库是wiki繁体最新版本
在两次操作的时候都会报错
1.繁体转简体
2.使用感知机分词
都是运行到第57行报Process finished with exit code -1073740940 (0xC0000374)

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

# coding=utf8
from opencc import OpenCC
import codecs,sys
from pyhanlp import *
'''
使用感知机进行分词，需要调用
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
seg=analyzer.seg(sentence)
'''
def cut_words(sentence):
    #print sentence
    # seg=HanLP.segment(sentence)
    PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
    analyzer = PerceptronLexicalAnalyzer()
    seg=analyzer.seg(sentence)
    print(seg)
    print(type(seg))
    list=[]
    for s in seg:
        list.append(s.word)
    condition = lambda t: t != "" ""
    new_list = filter(condition, list)
    after_cut = "" "".join(new_list)
    return after_cut

filepath=r'C:\Users\01\Desktop\【】NLP\[01]learning-file\wiki.zh.text'
f=codecs.open('wiki.zh.jian2.txt','r',encoding=""utf8"")
target = codecs.open(""wiki.zh.seg.txt"", 'w',encoding=""utf8"")
line_num=1
line = f.readline()

while line:
    print('---- processing ', line_num, ' article----------------')
    line_seg=cut_words(line)
    target.writelines(line_seg)
    print(line_seg)
    line_num = line_num + 1
    line = f.readline()
f.close()
target.close()
exit()




"
Merge pull request #1 from hankcs/master,"第一次从源升级

"
classpath文件夹带中文怎么处理？,"重写了 从classpath中获取路径 
public class SimpleClassPathIIOAdapter implements IIOAdapter {
	/**
	 * 打开一个文件以供读取
	 *
	 * @param path
	 *            文件路径
	 * @return 一个输入流
	 * @throws IOException
	 *             任何可能的IO异常
	 */
	@Override
	public InputStream open(String path) throws IOException {
		return new FileInputStream(this.getClass().getClassLoader().getResource(path).getFile());
	}

	/**
	 * 创建一个新文件以供输出
	 *
	 * @param path
	 *            文件路径
	 * @return 一个输出流
	 * @throws IOException
	 *             任何可能的IO异常
	 */
	@Override
	public OutputStream create(String path) throws IOException {
		return new FileOutputStream(this.getClass().getClassLoader().getResource(path).getFile());
	}
}
WARN  HanLP - 读取hanlp/data/dictionary/other/CharType.bin时发生异常java.io.FileNotFoundException: E:\%e5%85%ac%e5%8f%b8\%e9%87%8d%e6%9e%84\%e5%90%8e%e5%8f%b0%e6%ba%90%e7%a0%81\common\target\classes\hanlp\data\dictionary\other\CharType.bin (系统找不到指定的路径。)

用的springboot 文件夹带中文怎么处理？
"
新增可自定义用户词典的维特比分词器,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

通过扩展的维特比分词器，可以完全自主设置用户自定义词典；
如果用户不设置，则默认使用全局用户词典；
如果用户指定了词典路径，则按照用户设置的路径加载自定义词典，不同实例可以使用不同词典；
词典默认进行缓存，且词典修改后会自动重新构建。"
import pyhanlp后jupyter notebook无法print,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：最新版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
jupyter notebook中import pyhanlp后无法print出任何内容


"
感知机中可变DAT的entrySet方法有bug,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：最新版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

感知机中使用可变DAT加载特征集合，加载large/cws.bin，在调试过程中发现
```
 System.out.println(featureMap.size());
 输出值为8021239
```
但是通过
```
 Set<String> set = new HashSet<String>();

Set<Map.Entry<String, Integer>> entries = featureMap.entrySet();

for (Map.Entry<String, Integer> m : entries) {
                    set.add(m.getKey());
 }
```
排除重复后set.size() == 8021206 少掉了33个key。
比如 ""\u0001/\u00014"" 这个key，
通过featureMap.idOf(""\u0001/\u00014"") 可以返回id是8
但是featureMap.entrySet()中并包括这个id=8的项。
也就是id=8并没有被恢复出来。


"
能否自定义模板分词？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我想要自定义模板来分词。例如
句子“2018年9月1日前”
如果用hanlp标准分词，则 2018年/9月/1/日前
如果用crf分词,则 2018年/9月/1日/前
虽然crf分词正确了，但是在我的任务中，标准分词的效果更有帮助。

**所以我想问能不能 在标准分词时，自定义正则模板，针对 ‘日前‘ 来做特殊分词？**
当然我说的情况，不是仅仅针对 ‘日前’这么一个词汇，是指这类功能。
谢谢

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp如何为分词模型添加语料训练 以及 是否可以基于韵律进行分词？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP v1.6.6
我使用的版本是：HanLP v1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
目前想对句子在韵律层级的分词，比如一层韵律词，二层韵律短语，三层语调短语，四层句末。
问题有三：
其一，请问基于python的pyhanlp如何为分词模型添加额外的词典？
其二，请问如何在pyhanlp下添加额外语料进行训练？
其三，请问对于韵律层级的分词，您有何建议或者指点吗？

<!-- 请详细描述问题，越详细越可能得到解决 -->



"
如何通过训练自己的预料提高文本聚类准确度,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->如何通过训练自己的预料，或者调节什么参数，可以提高文本聚类的准确度？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 文本聚类
2. 统计准确度

### 触发代码

```
   
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
欢迎参加HanLP线下技术交流会,"感谢大家对HanLP的支持，欢迎参与12月7号于青岛举办的HanLP主题论坛。这次论坛由大快搜索举办，在美丽的海滨城市崂山举行。主题包括Hadoop解决方案DKH、FreeRCH2和HanLP。会后有崂山爬山、游园和农家宴等活动。宾至如归，公司负责会议期间的住宿和餐食。

HanLP主题论坛将介绍HanLP1.7的技术干货，并且会发布HanLP2.0的消息。现在**征集两位开发者**（个人或公司）分享自己的开发案例，欢迎大家踊跃报名。详细的介绍请参考[
会议介绍](https://github.com/hankcs/HanLP/files/2617987/2018.docx)和[邀请函](https://github.com/hankcs/HanLP/files/2617988/2018.docx)。

感兴趣的朋友可以直接留下联系方式，也可以通过如下方式咨询主办方：

联 系 人：赵金鹏
联系电话：15554211163
尊席诚待，特此盛邀！"
DemoPerceptronLexicalAnalyzer 加载不到cws.bin 文件 文件路径正确,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-protable-1.7.0
我使用的版本是：hanlp-protable-1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

maven构建hanlp项目

### 触发代码

```
      PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(""data/model/perceptron/pku199801/cws.bin"",
                                                                           HanLP.Config.PerceptronPOSModelPath,
                                                                           HanLP.Config.PerceptronNERModelPath);
        System.out.println(analyzer.analyze(""上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观""));
        System.out.println(analyzer.analyze(""微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观 的分词结果
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

十一月 27, 2018 9:54:53 上午 com.hankcs.hanlp.corpus.io.ByteArrayStream createByteArrayStream
警告: 打开失败：data/model/perceptron/pku199801/cws.bin
Exception in thread ""main"" java.io.IOException: data/model/perceptron/pku199801/cws.bin 加载失败
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:390)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70)
	at com.hankcs.demo.DemoPerceptronLexicalAnalyzer.main(DemoPerceptronLexicalAnalyzer.java:34)


## 其他信息

"
已经成词的词不能通过learn拆分,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
北京百度网讯科技有限公司，用感知机分词永远都是一个词，用learn手动分开既然没用
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
   analyzer.learn(""北京 百度 网讯 科技 有限公司"")
   analyzer.analyze(""北京百度网讯科技有限公司"");
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[北京/ns 百度/nz 网讯/n 科技/n 有限公司/n]/nt
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
北京百度网讯科技有限公司/nt
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
感知机报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1.语料采用2014年的标注标准，用感知机训练词性标注模型，之后测试“整车可动性差""报错
日志信息如下：
Exception in thread ""main"" java.lang.IllegalArgumentException: 在打分时传入了非法的下标
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.score(LinearModel.java:366)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.viterbiDecode(LinearModel.java:296)
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.tag(PerceptronPOSTagger.java:72)
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.tag(PerceptronPOSTagger.java:65)
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.tag(AbstractLexicalAnalyzer.java:154)
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.analyze(AbstractLexicalAnalyzer.java:193)


2.采用更多语料训练词性标注，加载模型报错
日志信息如下：
Exception in thread ""main"" java.lang.IllegalArgumentException
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:334)
	at com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(ByteArrayFileStream.java:83)
	at com.hankcs.hanlp.corpus.io.ByteArrayStream.nextInt(ByteArrayStream.java:56)
	at com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(IntArrayList.java:182)
	at com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger.load(MutableDoubleArrayTrieInteger.java:1185)
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:93)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:75)
"
感知机分词识别的一些疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0。

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
非常感谢作者开放自己的亿级训练结果，在使用中有些疑问希望能给予帮助：
1.使用感知机分词时对一些比较明显不是人名的词或符号识别为人名，如“否”，“归档”这些字或词单独出现时 识别为nr   ""普卡客户""中的“普卡”会被识别为地名等
2.在线演示的模型和1.6.8发布的模型是一个么，我看到之前有人问到这个，差异主要在现代汉语词典，不过使用的时候一般我在本地识别不如预期的时候，拿到在线演示上面测试时效果更好一些，有点不清楚这之间具体有哪些差异导致。
3.现在的使用上包含的人名地名较多，  常常会将地名识别成人名，我现在的解决办法只有加词典，请问下还有更好的方式解决么。 
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
词典和模型使用的1.7.0版本，加了些地名的词典 但已确认出现问题的词的不在新加的词典中出现

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.analyze(""归档""));
        System.out.println(analyzer.analyze(""普卡客户""));
        System.out.println(analyzer.analyze(""谢师傅帮忙处理""))；
    }
```
### 期望输出
<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
归档/vn
普卡客户/n
谢/v 师傅/n 帮忙/v 处理/v
### 实际输出
归档/nr
普卡/ns 客户/n
谢/nr 师傅/n 帮忙/v 处理/v
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![qq 20181122140354](https://user-images.githubusercontent.com/18030444/48884619-2cc24380-ee60-11e8-8d9d-da0f8374c31f.png)
![qq 20181122140502](https://user-images.githubusercontent.com/18030444/48884620-2cc24380-ee60-11e8-96a7-d37ae4f49b3d.png)
![qq 20181122140536](https://user-images.githubusercontent.com/18030444/48884622-2d5ada00-ee60-11e8-89fb-acc6be19706c.png)
"
分词问题,"对一篇含有“迈凯伦法拉利恩佐兰博基尼”、“迈凯伦塞纳迈凯伦卡宾”的文本采用标准分词时，分词的结果中包含这些长词。如果将这上述两个词连在一起，分词结果是两个词的结合。对于音译名的分词，连在一起的音译词会被当作一个词。是要用索引分词吗？
代码如下：
List termList1 = HanLp.segment(""迈凯伦法拉利恩佐兰博基尼, 迈凯伦塞纳迈凯伦卡宾"");
List termList2 = HanLp.segment(""迈凯伦法拉利恩佐兰博基尼迈凯伦塞纳迈凯伦卡宾"");
System.out.println(termList1);
System.out.println(termList2);
结果如下：
[迈凯伦法拉利恩佐兰博基尼/nrf, ，/w, 迈凯伦塞纳迈凯伦卡宾/nrf]
[迈凯伦法拉利恩佐兰博基尼迈凯伦塞纳迈凯伦卡宾/nrf]"
reload生成自定义词典bin文件在重新载入时抛出异常ArrayIndexOutOfBoundsException,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
当新增自定义词典文件时, 通过调用CustomDictionary中的reload方法生成新的CustomDictionary.txt.bin文件生成并加载, 从而实现对新加自定义词典中的词的切分功能. 



## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
目前发现的问题是 : 通过reload方法生成的新bin文件在第二次程序启动过程中不能够被正确识别, 会抛出
```python
十一月 21, 2018 2:41:38 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadDat
警告: 读取失败，问题发生在java.lang.ArrayIndexOutOfBoundsException: 149
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:327)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:64)
	at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
```
且通过reload生成的bin文件大小和初始化HanLP生成的bin文件大小不一致.
我的例子中, 通过reload生成的bin文件大小为28788656, 通过HanLP初始化生成的bin文件大小为28788888. 
已经确定不是因为进程提前关闭导致文件没有写完这种情况.

### 步骤
 

### 触发代码

```
from jpype import JClass
from pyhanlp import HanLP, SafeJClass
from datetime import datetime
custom_dictionary = SafeJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
custom_dictionary.reload()
HanLP.segment('今天天气不错')
```
在运行过上述代码后, 会生成一个bin文件, 为了便于区分简称为reload.bin
```
from jpype import JClass
from pyhanlp import HanLP, SafeJClass
from datetime import datetime
#custom_dictionary = SafeJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
#custom_dictionary.reload()
HanLP.segment('今天天气不错')
```
在运行上述代码过程中, 会抛出问题中遇到的异常, 而且会生成新的bin文件, 称为init.bin

reload.bin文件大小为28788656
init.bin文件大小为28788888

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
2018-11-21 17:00:55
推测可能是loadDat里面有些逻辑是程序初始化中会走, 但是reload不会走的. 所以导致写入文件的内容不一致.

2018-11-22 10:33:15
重新复现了一遍过程, 查看了其他的issue. 过往的issue都是因为老版本的反射机制导致自定义词性报错. 但是目前用的是1.6.8版本, 应该不存在这个问题. 同时采取了不同的策略进行测试

1) 策略:  本地有init.bin  结果 : 正常载入 不报错 正常分词
2) 策略: 本地无init.bin 结果 : 正常生成init.bin 正常载入 不报错 正常分词.
3) 策略: 本地有init.bin 第一次和第二次分词之间进行reload 结果 : 正常生成reload.bin 两次分词均正常 任务结束后在其他任务中进行分词, 无法正常载入reload.bin, 重新生成init.bin.

reload中也是通过删除本地bin文件后重新调用loadMainDictionary方法进行新bin文件生成, 和init过程中的逻辑是一样的, 但是生成的文件大小不一致, 且不能正常读取. 怀疑是在reload过程中是否没有初始化一些全局变量的值导致reaload生成的bin文件缺少一些内容.

"
分词算法问题,我想把 “机器人” 分词成 “机器” “机器人” “机” “器” “人” 这五个词应该选择哪个分词算法
android开发文件无法写入到data/dictionary/person/nrf.txt.trie.dat,"当前最新版本号是：
我使用的版本是：1.7.0

## 我的问题
在android机上面做动态分词，经常会抛出
11-20 19:26:18.068 1366-2392/com.mtrobot.studio.mtrobotstudio_product E/CrashHandler: java.lang.IllegalAccessError: 不支持写入data/dictionary/person/nrf.txt.trie.dat！请在编译前将需要的数据放入app/src/main/assets/data

该异常直接导致应用程序崩溃，并且我们的系统是root权限的，不会发生读写失败的问题，查看系统日志发现nrf.txt.trie.dat从未写入成功过，请大牛帮忙解决,谢谢


"
"未进行修改, 自定义词典缓存失败","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
使用python3.7.1 版本, 直接pip 安装得到
Windows 10 家庭中文版1803 17134.376
当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在我使用pip 安装后, 在命令行运行`hanlp segment` 下载过jar和data后, 输入测试任意用句, 会出现报错信息.
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
在我清空以前的hanlp文件后, 重新pip 安装, 没有对data 中的任何数据修改, 直接使用hanlp segment即出现问题.

### 触发代码

```
$ hanlp segment
测试用句

```

<!-- 你希望输出什么样的正确结果？-->

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadDat
警告: 读取失败，问题发生在java.lang.ArrayIndexOutOfBoundsException: 7
        at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToInt(ByteUtil.java:239)
        at com.hankcs.hanlp.corpus.io.ByteArray.nextInt(ByteArray.java:68)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:325)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:64)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)

十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典c:/program读取错误！java.io.FileNotFoundException: c:\program (系统找不到指定的文件。)
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：c:/program
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 没有加载到任何词条
十一月 17, 2018 3:09:57 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 自定义词典c:/program files/python37/lib/site-packages/pyhanlp/static/data/dictionary/custom/CustomDictionary.txt缓存失败！
java.lang.NullPointerException
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:116)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)

测试/vn 用/p 句/q
```

谢谢您的解答!

"
构造PerceptronLexicalAnalyzer报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
构造词法分析器，构造函数出错
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤



### 触发代码

```
		PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(""D:/myeclipse-workplace/NLP/data-for-1.6.8/data/model/perceptron/pku199801/cws.bin"",
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: 错误的模型类型: 传入的不是词性标注模型，而是 CWS 模型
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.<init>(PerceptronPOSTagger.java:37)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:50)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70)
	at com.NLP.DemoPerceptronLexicalAnalyzer.main(DemoPerceptronLexicalAnalyzer.java:24)

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')能否加载.bin模型,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

python调用hanlp时，WordVectorModel只能加载后缀为txt的词向量模型，能否加载后缀为bin为模型，如何加载

<!-- 请详细描述问题，越详细越可能得到解决 -->

### 触发代码

```
from pyhanlp import *
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = 'E:/data/baike_26g_news_13g_novel_229g.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Normalization词典CharTable.txt中①映射为数字1是否更合适？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->
分词前进行Normalization 操作，会根据 `data/dictionary/other/CharTable.txt`对每一个 字符 进行一一映射。我发现带圆圈的数字被映射为对应的汉字，是否修改为阿拉伯数字更合适？

我遇到的场景是 数字域名、号码等，阿拉伯数字是更合适的，不过是否有些场景更适合用汉字？

https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/CharTable.txt#L40"
您好，请问在线演示中使用的是什么分词器呢，我试了很多没有发现相同的分词器,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，请问在线演示中使用的是什么分词器呢，我试了很多没有发现相同的分词器
我使用的是pyhanlp

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 我对两句话进行分词
s1 = '而这些伟人用行动阐述了实现理想的真谛。'
s2 = '为了攻克“哥德巴赫猜想”，他整天进行演算，遇到了许多挫折，但他都坚持下来了'
2. 在线演示的分词结果
![image](https://user-images.githubusercontent.com/32705832/48398994-593dd780-e75d-11e8-8c22-6db8ce44b405.png)
![image](https://user-images.githubusercontent.com/32705832/48399018-6955b700-e75d-11e8-94e4-3d2b960a30a1.png)





3. 我试的分词结果

为了/p, 攻克/v, “/w, 哥/n, 德/b, 巴赫/nrf, 猜想/v, ”/w, ，/w, 他/rr, 整天/d, 进行/vn, 演算/vn, ，/w, 遇到/v, 了/ule, 许多/m, 挫折/n, ，/w, 但/c, 他/rr, 都/d, 坚持/v, 下来/vf, 了/ule

而/c 这些/r 伟人/n 用/p 行动/vn 阐述/v 了/u 实现理想/n 的/u 真谛/n 。/w

### 触发代码

```
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(s))
print(analyzer.analyze(s1))
```
### 期望输出
像在线演示的分词结果一样
<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出
如上我的分词结果
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
命名实体提取，时间词识别有误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周四天气怎么样"");
```

输出结果：
![image](https://user-images.githubusercontent.com/3538629/48359900-f276da80-e6d8-11e8-9bd4-fff6e366d647.png)

该结果符合预期。

```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周三天气怎么样"");
            System.out.println(wordList);
```

输出结果

![image](https://user-images.githubusercontent.com/3538629/48359938-03275080-e6d9-11e8-8676-3cb230f9570b.png)

该结果不符合预期，应该将*下周三*识别为时间词。

请问怎么能提高时间词的识别效果？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

### 触发代码

```
    public void testDateNer() {
        try {
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周三天气怎么样"");
            System.out.println(wordList);
        } catch (IOException e) {
            e.printStackTrace();
        }

    }
```


"
1.7.0 指定 shortest 为 False seg2sentence() 还是按逗号切分,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

## 我的问题

如题 无论 shortest 为 True 还是 False seg2sentence 都是按逗号和分号切分的
直接调用 com.hankcs.hanlp.utility.SentencesUtil.toSentenceList() 情况也一样

### 触发代码

不会 Java 用的 pyhanlp (更新过 jar 包了)

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')
sentence_util = jpype.JClass('com.hankcs.hanlp.utility.SentencesUtil')

for s in standard_tokenizer.seg2sentence('这是一个句子，中间有个逗号。', shortest = False): print(s)
for s in sentence_util.toSentenceList('这是一个句子，中间有个逗号。', shortest = False): print(s)

```
### 期望输出

```
[这/rzv, 是/vshi, 一个/mq, 句子/n, ，/w, 中间/f, 有/vyou, 个/q, 逗号/n, 。/w]
这是一个句子，中间有个逗号。
```

### 实际输出

```
[这/rzv, 是/vshi, 一个/mq, 句子/n, ，/w]
[中间/f, 有/vyou, 个/q, 逗号/n, 。/w]
这是一个句子，
中间有个逗号。
```
"
seg2sentence() 把逗号和分号也分割为一句话,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.6.8
我使用的版本是：1.6.8

## 我的问题
这个问题 [Issue 876](https://github.com/hankcs/HanLP/issues/876) 里提到过 但是被关闭了
目前 StandardTokenizer, BasicTokenizer, NLPTokenizer, SpeedTokenizer 提供的 seg2sentence 分句接口（其他分词器比如 TraditionalChineseTokenizer, URLTokenizer, CRFLexicalAnalyzer, PerceptronLexicalAnalyzer, DijkstraSegment, NShortSegment, ViterbiSegment 我大致看了下没找到分句的接口 不知道是不是一样的表现）会把逗号也切分为一句话 不知这个功能是否是为了后期其他文本处理的考虑才这样切分的？

如果是用于生产环境的分句的话 感觉逗号肯定不能切成一句 按照最新的国家标准：[ 中华人民共和国国家标准GB/T15834-2011标点符号用法](http://yywz.sumhs.edu.cn/f4/ad/c4705a193709/page.htm) 3.1.1 一节中所述 只有句号 问号 叹号 这三个标点符号才算一句话 所以严格来说 分号也不能算

### 触发代码
因为我不会 Java 只会 Python 所以用的 pyhanlp 就用 StandardTokenizer 举个例子 其他分词器的结果类似

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')

for sentence in standard_tokenizer.seg2sentence('这是一句用来测试的句子，中间有个逗号。'): 
    print(sentence)
```
### 期望输出

```
[这/rzv, 是/vshi, 一/m, 句/q, 用来/v, 测试/vn, 的/ude1, 句子/n, ，/w, 中间/f, 有/vyou, 个/q, 逗号/n, 。/w]
```

### 实际输出

```
[这/rzv, 是/vshi, 一/m, 句/q, 用来/v, 测试/vn, 的/ude1, 句子/n, ，/w]
[中间/f, 有/vyou, 个/q, 逗号/n, 。/w]
```
"
请问下这个里面是否可以用深度学习的算法来训练,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
大幅度,"<!-大幅度-
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
大幅度
当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
人名分词问题,"例句：“拨张三的电话“  分词后  “ 拨/v, 张/q, 三/m, 的/uj, 电话/n“
例句：""拨打张三的电话"" 分词后  “拨打/v, 张三/nr, 的/uj, 电话/n”
为什么人名分词不一"
ngram词典修改无效,"## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我在ngram词典里把“很@漂亮”删除了，调用com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary#reload重新加载词典，bin文件也删除重建了，为什么分词还是把“很漂亮”这个词合并在一起了？我这边需要把ngram里的很多副词和形容词分开，有没有什么方法把ngram词典禁用掉？

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""车子很漂亮。""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
[车子/n, 很/d, 漂亮/a, 。/w]


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
[车子/n, 很漂亮/n, 。/w]


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
test_word2vec .py中加载词向量模型WordVectorModel(model_file)报内存不足,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.8
我使用的版本是：hanlp-1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
```
from pyhanlp import *
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
model_file = 'user/data/baidubaike.txt'
word2vec = WordVectorModel(model_file)
```
其中baidubaike.txt文件大小4G，运行出现如下报错：
```
File ""D:\Anaconda3\lib\site-packages\jpype\_jclass.py"", line 111, in _javaInit*args)
java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded
```
修改jvm内存也没有解决问题，请问当文件较大时，如何解决这个问题？
<!-- 请详细描述问题，越详细越可能得到解决 -->


"
Translating docs and codebase to English,Are there any forks of this repository that translate it into English ?
繁简转换 ：佘字转错 ：畲,"
System.out.println(HanLP.convertToSimplifiedChinese(""佘诗曼""));
会误将 佘字转错 ：畲"
doc2vec的实现原理能否说一下？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
想使用doc2vec的接口计算句子的相似度，想仔细了解一下这个模块的原理？


<!-- 请详细描述问题，越详细越可能得到解决 -->
"
pyhanlp 在python3环境下data-for-1.6.8.zip 解压缩乱码,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp 在第一次 import 的时候会下载并解压数据文件，实测python3 环境下，data-for-1.6.8使用`zipfile`直接解压会出现中文乱码（custom目录的几个文件）。python2 没有这个问题，参考后面两个链接。

## 复现问题
手工执行`static.__init__.py` 里面的代码
https://github.com/hankcs/pyhanlp/blob/master/pyhanlp/static/__init__.py#L241

### 触发代码

```python
with zipfile.ZipFile(""data-for-1.6.8.zip"", ""r"") as f:
  for fn in f.namelist():
    print(fn)
```
### 期望输出

正确的中文文件名

### 实际输出

乱码

## 其他信息
参见这两个 StackOverflow 问题：
https://stackoverflow.com/questions/41019624/python-zipfile-module-cant-extract-filenames-with-chinese-characters
https://stackoverflow.com/questions/37723505/namelist-from-zipfile-returns-strings-with-an-invalid-encoding

话说我解压了，再mac用系统自带的zip再打包，结果还是乱码：`zip -r data data`，使用7z压缩后就没问题了。`7z a -tzip data.zip data`。另外7z压缩的文件更小，推荐。

"
能否在依存句法分析的过程中使用指定分词器再做后续处理？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在体验过感知机分词后，确实感受到语料库的重要性，同时可以支持在线增量训练以后方便了许多。
目前我这边项目中需要使用依存句法分析后再做规则处理，目前考虑是使用nnparser，看了下之前的代码。个人理解是先分词后再进行句法分析处理，请问是否能够指定nnparser所使用的句法分析器，例如替换成感知机分词？（因为在感知机分词的模型增量训练了自己的专有词语，希望统一维护。）

我看到nnparser在26天前提交了构造函数支持传入segment。是否就是意图支持nnparser指定不同的分词器？
如果这个改动是这个意图的话，想请问一下大概何时会把这个feature给release一个版本出来？

如果短期不会放出这个release的话，是否我只需要根据这次提交所修改的nnparser的代码和abstractLexicalAnalyzer进行修改就能支持？（暂时看到这次提交只有这2个改动，不知是否还有其他需要修改的地方）
"
感知机训练对硬件的要求,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v 1.6.8
我使用的版本是：v 1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 问题
使用结构化感知机标注框架训练自己的词性标注模型时内存溢出。
1.新闻语料文件夹大小1.7g左右
2.使用命令：java -Xmx30720m -cp hanlp-portable-1.6.8.jar:src/main/resources com.hankcs.hanlp.model.perceptron.Main -task POS -train true -model pos.bin -reference test
3.系统性能如下：
![image](https://user-images.githubusercontent.com/18524483/47700860-e3147d80-dc52-11e8-957a-65fc6c061bb1.png)
4.运行结果如下：
![a5f85674429ccc04acaf8366e186c03](https://user-images.githubusercontent.com/18524483/47700984-50281300-dc53-11e8-9e68-a6ab27f0e81c.png)
请问：
1.词性标注模型训练对内存的要求，另外特征总数达到六亿多这正常吗？
2.您当时训练分词模型large.bin的时候的硬件环境
期望尽快得到您的赐教，谢谢！





"
关于elasticsearch衍生项目的问题,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.8

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

## 我的问题

[衍生项目的wiki](https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE) 里说elasticsearch 的两个插件都跑不起来

1.  hanlp-ext @hualongdata
2. https://github.com/shikeio/elasticsearch-analysis-hanlp

反而是这个没列出 https://github.com/KennFalcon/elasticsearch-analysis-hanlp 能跑起来。
第二个问题和这个衍生项目有关也给它开了issue ，它目前支持1.6.6， 我如果直接替换成1.6.8的data不会有问题吧？"
能否基于Word2VecTrainer做word2vec的增量训练？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1、我在尝试用HanLP的Word2Vec做文档相似性分析，想问下通过Word2VecTrainer可否做word2vec向量词的增量训练，我做了尝试，发现会覆盖已有文件，请问是否有其他方法，谢谢！

"
在线学习并序列化后的新模型无效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!---->
在线学习后的新模型无效
## 复现问题
<!--  -->
 您好，感谢您的开源！我正在尝试感知机的在线学习方法。用learn方法在线学习后，用getPerceptronSegmenter().getModel().save()进行了序列化并保存到模型，但是注释掉序列化和learn之后再次调用DemoPerceptronLexicalAnalyzer.java并没有达到在线学习的效果 

### 步骤

1. 首先把DemoPerceptronLexicalAnalyzer.java源代码中序列化的注释去掉，然后运行，得到正确的在线学习结果，并序列化覆盖原来的CWS.bin模型
2. 然后注释掉序列化语句和在线学习learn方法
3. 接着再次运行DemoPerceptronLexicalAnalyzer.java

### 触发代码

```
 public class DemoPerceptronLexicalAnalyzer extends TestUtility
{
    public static void main(String[] args) throws IOException
    {
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                                                                           HanLP.Config.PerceptronPOSModelPath,
                                                                           HanLP.Config.PerceptronNERModelPath);

        // 任何模型总会有失误，特别是98年这种陈旧的语料库
        System.out.println(analyzer.analyze(""总统普京与特朗普通电话讨论太空探索技术公司""));
        // 支持在线学习
        //analyzer.learn(""与/c 特朗普/nr 通/v 电话/n 讨论/v [太空/s 探索/vn 技术/n 公司/n]/nt"");

        // 学习到新知识
        System.out.println(analyzer.analyze(""总统普京与特朗普通电话讨论太空探索技术公司""));
        // 还可以举一反三
        System.out.println(analyzer.analyze(""主席和特朗普通电话""));

        // 知识的泛化不是死板的规则，而是比较灵活的统计信息
        System.out.println(analyzer.analyze(""我在浙江金华出生""));
        //analyzer.learn(""在/p 浙江/ns 金华/ns 出生/v"");

        System.out.println(analyzer.analyze(""我在四川金华出生，我的名字叫金华""));

        // 在线学习后的模型支持序列化，以分词模型为例：
        //analyzer.getPerceptronSegmenter().getModel().save(HanLP.Config.PerceptronCWSModelPath);

        // 请用户按需执行对空格制表符等的预处理，只有你最清楚自己的文本中都有些什么奇怪的东西
        System.out.println(analyzer.analyze(""空格 \t\n\r\f&nbsp;统统都不要""
                                                .replaceAll(""\\s+"", """")    // 去除所有空白符
                                                .replaceAll(""&nbsp;"", """")  // 如果一些文本中含有html控制符
        ));
    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
总统/n 普京/nr 与/p 特朗普/nr 通/v 电话/n 讨论/v [太空/s 探索/vn 技术/n 公司/n]/nt
总统/n 普京/nr 与/p 特朗普/nr 通/v 电话/n 讨论/v [太空/s 探索/vn 技术/n 公司/n]/nt
主席/n 和/c 特朗普/nr 通/v 电话/n
我/r 在/p 浙江/ns 金华/ns 出生/v
我/r 在/p 四川/ns 金华/ns 出生/v ，/w 我/r 的/u 名字/n 叫/v 金华/nr
空格/n 统统/d 都/d 不要/d
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
总统/n 普京/nr 与/p 特朗普/b 通/vn 电话/n 讨论/v 太空/s 探索/vn 技术/n 公司/n
总统/n 普京/nr 与/p 特朗普/b 通/vn 电话/n 讨论/v 太空/s 探索/vn 技术/n 公司/n
主席/n 和/c 特朗普/b 通/vn 电话/n
我/r 在/p [浙江/ns 金华/nz]/nt 出生/v
我/r 在/p 四川/ns 金华/nr 出生/v ，/w 我/r 的/u 名字/n 叫/v 金华/nr
空格/n 统统/d 都/d 不要/d
```

## 其他信息

<!---->
CWS.bin是用pku98训练的；
我能确定序列化已经生效，有一次save到config.POS里面去了，再运行的时候会报错模型输入有误，重新训练了POS才能继续运行；
感谢您的时间！

做了了两天实验又发现了新问题，如果单独为某一类实体特地训练了一个NER模型，这个模型是覆盖在基础语料模型上，还是另外储存？如果是覆盖的话，有模型序列化到模型的方法吗？如果是另外储存，那下次调用analyze方法的时候需要一次把两个模型都读取才行？
谢谢！

"
带表情符的CRF分词出现编码错误 UnicodeEncodeError,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
hanlp-1.6.3.jar
当前最新版本号是：
我使用的版本是：
pyhanlp
环境是python 3.6
<!--以上属于必填项，以下可自由发挥-->

## 我的问题
text = '主动提出拿宝宝椅，👍。十块钱😄'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)

出现以下错误：UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed

而用HanLP.segment(text)则是正常的。去掉👍和😄，CRF也是正常的。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
 text = '主动提出拿宝宝椅，👍。十块钱😄'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
使用HanLP.segment()分词导致python停止工作,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用HanLP.segment()对字符串进行分词时导致python停止工作。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
except_list = []
for i,item in enumerate(train_content):
    try:
        #item = text_seg(item,seg_tool='hanlp')
        item=HanLP.segment(item) # 触发问题语句
        item=[term.word for term in item]
        train_content[i] = item
        print('sent: ', i, ' ,', item)
        if i % 1000 == 0:
            print(str(i)+' lines had been segmented')
    except:
        except_list.append(i)
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->



![image](https://user-images.githubusercontent.com/44291990/47215523-46d9b380-d3d4-11e8-83ae-225f264c669c.png)
![image](https://user-images.githubusercontent.com/44291990/47215609-a041e280-d3d4-11e8-9a33-d0b4898eb8c6.png)




## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
新词发现在300mb的语料上报内存不够,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp-0.1.44
我使用的版本是：pyhanlp-0.1.44

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
用pyhanlp的接口在300mb的语料上做新词发现，服务器内存为128g，报了out of memory，有什么优化或者解决的办法吗。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
新词
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
jpype._jexception.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
hanlp的分词在sighan2005公共数据集上的表现,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

请问有没有对比hanlp的分词效果在sighan2005公共数据集上的表现？谢谢~


"
"""大家电""分词问题","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
""大家电""分词错误,切分为""大家""和""电"", 应该切分为""大""和""家电"",请问如何修改核心词典解决这个问题.
我通过删除""大家""这个词,可以解决这个问题,但是这样之后,""大家""这个词就切分不出来了.

## 复现问题
使用viterbi分词""大家电""即可复现此问题

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""大家电""));
    }
```
### 期望输出
```
""大""和""家电""
```
### 实际输出
```
""大家""和""电""
```


"
繁体转简体对币种转换错误的问题,"Hi,

调用HanLP.convertToSimplifiedChinese()做繁体到简体的转换，但是对于下面的这些币种，存在下面的问题，还烦劳看下怎么修改这个问题：
荷属安的列斯盾 ——> 荷属安的列斯斯盾
塔吉克卢布 ——> 塔吉克斯坦卢布
巴哈马币 ——> 巴赫马币
厄瓜多尔UCV ——> 厄瓜多尔尔UCV

左边是输入的Query，右边是转换后的结果，右边转换都是有问题的。"
加载CustomDictionary.txt.bin抛出java.lang.IllegalArgumentException异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
CustomDictionary添加了自定义词典(1.35GB), 并删除缓存重新生成了一个3GB+的CustomDictionary.txt.bin

当程序再次运行时... 加载CustomDictionary.txt.bin失败
readBytes中抛出java.lang.IllegalArgumentException异常

INFO: 自定义词典开始加载:../../myproject/data/dictionary/custom/CustomDictionary.txt
十月 16, 2018 3:12:39 下午 **com.hankcs.hanlp.corpus.io.IOUtil readBytes**
**WARNING: 读取../../myproject/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.lang.IllegalArgumentException**
十月 16, 2018 3:12:39 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
INFO: 以默认词性[n]加载自定义词典../../myproject/data/dictionary/custom/CustomDictionary.txt中……

尝试重新生成了一个小词典 是能正常运行的 是词典缓存过大导致?
<!-- 请详细描述问题，越详细越可能得到解决 -->

"
pyhanlp的新词发现没有涉及英文的新词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp-0.1.44
我使用的版本是：pyhanlp-0.1.44

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
用pyhanlp的新词发现接口不会出现涉及英文单词的新词，同样使用左右熵和互信息的另外一个版本实现会发现这个词，是hanlp对所有的英文都过滤掉了吗？可以有什么办法保留英文吗？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
K线图
A股账户
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
无涉及英文的新词
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
安装完pyhanlp和jpype之后，ModuleNotFoundError: No module named '_jpype',"import from pyhanlp import *
报错：+
Traceback (most recent call last):
  File ""/home/vip/wusaifei/project/preprocess.py"", line 2, in <module>
    from pyhanlp import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 11, in <module>
    from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/__init__.py"", line 17, in <module>
    from ._jpackage import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/_jpackage.py"", line 18, in <module>
    import _jpype
ModuleNotFoundError: No module named '_jpype'"
pyhanlp安装JDK之后报错,"报错信息
Traceback (most recent call last):
  File ""C:/zengxianfeng/lac/utils.py"", line 4, in <module>
    from pyhanlp import *
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\pyhanlp\__init__.py"", line 116, in <module>
    _start_jvm_for_hanlp()
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\pyhanlp\__init__.py"", line 108, in _start_jvm_for_hanlp
    getDefaultJVMPath(),
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\jpype\_core.py"", line 121, in get_default_jvm_path
    return finder.get_jvm_path()
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\jpype\_jvmfinder.py"", line 153, in get_jvm_path
    .format(self._libfile))
jpype._jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properl
环境变量里面已经添加了 JAVA_HOME C:\zengxianfeng\Anaconda\Lib\site-packages\jdk"
com.hankcs.hanlp.utility.TestUtility找不到,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

当我在运行DemoSentimentAnalysis时，import com.hankcs.hanlp.utility.TestUtility报错，发现在utility中不存在TestUtility

"
可以用来做输入法的联想词嘛,
感知机无法对英文进行分词吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

是否是英文训练的模型没有加入英文，所以无法对英文进行分词？

### 触发代码

```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
   String str = ""Apache Commons is an Apache project focused on all aspects of reusable Java components. "";
   System.out.println(analyzer.analyze(str));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[Apache/nx,  /w, Commons/nx,  /w, is/nx,  /w, an/nx,  /w, Apache/nx,  /w, project/nx,  /w, focused/nx,  /w, on/nx,  /w, all/nx,  /w, aspects/nx,  /w, of/nx,  /w, reusable/nx,  /w, Java/nx,  /w, components/nx, ./w,  /w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Apache Commons is an Apache project focused on all aspects of reusable Java components. /nr
```


"
如何改进获取摘要的结果,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
如何可以使得获取摘要的结果更为符合题意？
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先输入要获取摘要的内容，
       String answerString = ""一，辛亥革命给封建专制制度以致命的一击。二，辛亥革命推翻了“洋人的朝廷”, 沉重打击了帝国主义的侵略势力。三，辛亥革命为民族资本主义的发展创造了有利的条件。四，辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响，特别是对越南、印度尼西亚等国家的反对殖民主义的斗争起了推动作用。"";
2. 然后调用HanLP.extractSummary获取摘要
3. 接着对摘要结果进行分析。

### 触发代码

```
   List<String> summaryList = HanLP.extractSummary(answerString, 4, ""[，,。:：“”？?！!；;、]"");
		 for(String summary : summaryList){
			 System.out.println(summary);
		 }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
辛亥革命给封建专制制度以致命的一击。
辛亥革命推翻了“洋人的朝廷”, 沉重打击了帝国主义的侵略势力。
辛亥革命为民族资本主义的发展创造了有利的条件。
辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响。
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
辛亥革命推翻了
辛亥革命给封建专制制度以致命的一击
辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响
印度尼西亚等国家的反对殖民主义的斗争起了推动作用
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
语料分类训练准确率低，需要如何提高呢,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我使用语料库 进行分割训练集和测试集的测试，为什么准确率这么低呢，是语料库的原因，
还是需要如何训练的问题呢。我该如何提高准确率
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
耗时 7557 ms 加载了 9 个类目,共 1791 篇文档
     P       R      F1       A        
 34.82   66.83   45.78   82.41  互联网
 77.19   44.22   56.23   92.35   体育
 51.61   24.12   32.88   89.06   健康
 61.84   47.24   53.56   90.90   军事
 64.02   76.88   69.86   92.63   招聘
 59.52   12.56   20.75   89.34   教育
 26.04   69.35   37.86   74.71   文化
 47.06   12.06   19.20   88.72   旅游
 53.72   50.75   52.20   89.67   财经
 52.87   44.89   48.55   44.89  avg.
data size = 1791, speed = 36551.02 doc/s


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        //CORPUS_FOLDER 是网上找的sogou分类库，不是mini版的
        DataSet trainingCorpus = new FileDataSet().                          // FileDataSet省内存，可加载大规模数据集
            setTokenizer(new HanLPTokenizer()).                               // 支持不同的ITokenizer，详见源码中的文档
            load(CORPUS_FOLDER, ""UTF-8"", 0.9);               // 前90%作为训练集
        IClassifier classifier = new NaiveBayesClassifier();
        classifier.train(trainingCorpus);
        IDataSet testingCorpus = new MemoryDataSet(classifier.getModel()).
            load(CORPUS_FOLDER, ""UTF-8"", -0.1);        // 后10%作为测试集
        // 计算准确率
        FMeasure result = Evaluator.evaluate(classifier, testingCorpus);
        System.out.println(result);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
动态更新完词典怎样写入文件CustomDictionary.txt.bin,"我是想动态更新词典用于现有的搜索，同时也写入文件，并重建CustomDictionary.txt.bin
这样就不用重启solr服务器，并重启solr服务器以后，也不会出问题。
找了里面的代码，但是没有找到， 是不是有其他办法可以实现此功能？"
词典不能用空格,"删除CustomDictionary.txt.bin以后，会重新生成CustomDictionary.txt.bin
但在生成CustomDictionary.txt.bin的时侯，出现如下错误：
`data/dictionary/custom/words-my.dic读取错误！java.lang.NumberFormatException: For input string: ""Jill""`
因为有一行记录为：
`Jack N' Jill`
不知道能不能用空格分隔自定义词典？或者有其他办法吗？"
希望能推出go版本,"希望能推出go版本
"
pyhanlp使用SafeJClass调用crf分词器意图多线程分词，报jvm错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp GitHub master
我使用的版本是：pyhanlp GitHub master


<!--以上属于必填项，以下可自由发挥-->

## 我的问题
      pyhanlp使用SafeJClass调用crf分词器意图多线程分词，报jvm错误


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

class Divider(threading.Thread):
    def __init__(self, threadID, data):
        threading.Thread.__init__(self)
        self.threadID=threadID
        self.data=data
        CRFLexicalAnalyzer = SafeJClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
        #CustomDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
        #HanLP = SafeJClass('com.hankcs.hanlp.HanLP')
        extend_dic = format_dic(external_dic_path)
        for word_e in extend_dic:
            CustomDictionary.add(word_e)  # 添加外部军事词典
        # 打开词性标注
        s = CRFLexicalAnalyzer().enableCustomDictionary(True).enablePartOfSpeechTagging(True)
        self.segment=s

    def run(self):
        divide_word(self.data,self.threadID,self.segment)


def divide_word(data,pid,segment,Debug=False):
    all_words=[]

    for index,article in enumerate(data):
        article_contont = article[""article_content""]
        article_contont = norm_article(article_contont)
        terms=segment.seg(article_contont)
        words=[term.word for term in terms]
        if Debug:
            print(words)
        all_words.append(words)
        if index%500==0:
            print(""Thread:"",pid,""divide...line:"",index)
    return all_words


    data=load_json(data_path)
    mid=int(len(data)*0.5)
    data1=data[:mid]
    data2=data[mid:]
    thread1 = Divider( ""Thread-1"", data1)
    thread2 = Divider( ""Thread-2"", data2)

    # 开启线程
    thread1.start()
    thread2.start()
    thread1.join()
    thread2.join()

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7c4d07fbb0, pid=17521, tid=0x00007f7bc90ce700
#
# JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13)
# Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-36m-x86_64-linux-gnu.so+0x3cbb0]  JPJavaEnv::FindClass(char const*)+0x20
#
# Core dump written. Default location: /home/ygwang/workspace/hannlp/handler/core or core.17521
#
# An error report file with more information is saved as:
# /home/ygwang/workspace/hannlp/handler/hs_err_pid17521.log
[thread 140169611896576 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
Aborted (core dumped)



```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于DoubleArrayTrie的默认初始化大小,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：portable-1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
![image](https://user-images.githubusercontent.com/26922926/46203542-b2d47900-c34c-11e8-9b0d-efccb5a6ce0e.png)
请问初始化时，初期大小必须是 65536*32这么大吗，如果要匹配的词不是很多（几个到几百个不等），也必须设定这么大吗。（程序中需要用到很多这种小字典，而且字典会频繁的变化）
我改成65536后，有时候用Searcher匹配的时候报ArrayIndexOutOfBoundsException的异常。

## 复现问题
1.用【潘家园街道】构建一个词典
2.用构建的词典匹配附件的这篇文章
3.用dart.getSearcher(text, 0)匹配，会在这个地方异常
![image](https://user-images.githubusercontent.com/26922926/46203923-e8c62d00-c34d-11e8-8a2b-c4c3db18942d.png)

[doc.txt](https://github.com/hankcs/HanLP/files/2427850/doc.txt)

"
hanlp打包问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

打包后找不到hanlp.properties

### 步骤

1. 首先:  下载hanlp源码(master分支),在com.hankcs.hanlp.HanLP类中加了一个main方法用于测试：
```
    public static void main(String[] args) {
        System.out.println(HanLP.segment(""商品和服务""));
    }
```
2. 然后:  打包，执行打包命令 `mvn clean package -Dmaven.test.skip=true`

3. 接着:  修改hanlp.properties中的root路径为data的父文件夹并放到target目录下，和hanlp-1.6.8-sources.jar、hanlp-1.6.8.jar在同一目录下，命令运行com.hankcs.hanlp.HanLP类的main方法:
```
java -cp hanlp-1.6.8.jar com.hankcs.hanlp.HanLP start
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[商品/n, 和/cc, 服务/vn]
```

期望和hanlp-1.6.8-release.zip里面打包出来的效果一样，两个jar包一个配置文件。

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
sep 28, 2018 1:47:03 PM com.hankcs.hanlp.HanLP$Config <clinit>
SEVERE: 没有找到hanlp.properties，可能会导致找不到data
========Tips========
请将hanlp.properties放在下列目录：
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data
现在HanLP将尝试从/Users/pan/github/HanLP-master/target读取data……
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: 读取data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: 核心词典data/dictionary/CoreNatureDictionary.txt不存在！java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt (No such file or directory)
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at com.hankcs.hanlp.HanLP.main(HanLP.java:860)
Caused by: java.lang.IllegalArgumentException: 核心词典data/dictionary/CoreNatureDictionary.txt加载失败
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 7 more
```


## 其他信息


"
【分词问题】钱己经打到卡上了=>钱己经 打 到 卡 上 了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->"
关于标准分词的详细原理,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我现在想了解标准词性标注的原理，我知道，是用隐马尔科夫和比特算法实现，但我想知道的更具体些。
还有，转移矩阵是怎么得到的？是共现吗？
期望尽快能给答复，非常感谢了
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
是否能提供1.6.8的原始语料库,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

非常感谢作者无私的贡献了1.6.8版本，提供了1亿语料库训练的模型。目前项目组正在使用1.6.8版本提供的模型，对于常规的分词和词性识别效果基本达到预期，但是对于行业内的分词效果并不理想，请问是否能提供1.6.8的语料库，想基于这个语料库上增加行业内的语料，使其能满足行业内的分词需求。

"
请问1.6.8中大规模语料训练的感知机模型的命名实体识别ner.bin会提供嘛?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：**1.6.8**
我使用的版本是：**1.6.8**


## 我的问题

我发现**data-for-1.6.8**数据包中data/model/perceptron/large目录下只有分词模型cws.bin,并未提供ner.bin,请问方便提供嘛?


"
多线程下内存溢出,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
pyhanlp 0.1.44
我使用的版本是：
pyhanlp   0.1.41

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
多线程下调用句法依存会出现内存溢出
get_relations_pos_head(sentence)中的
word_array = HanLP.parseDependency(sentence).getWordArray()
报错：jpype._jexception.java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded

并且程序一开始运行时，物理内存占1.9G ，虚拟内存占5G, 运行一段时间后虚拟内存就到9G




"
stopwords能不能像自定义字典一样在配置里自定义新文件,"## 注意事项
请确认下列注意事项：
* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [👍 ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
pyhanlp
当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
stopwords 能不能自定义文件
print(HanLP.Config.CoreStopWordDictionaryPath)
/home/q/pyhanlp/data/dictionary/stopwords.txt; stop_dict.txt;
能不能这样配置
这样配置，下面这段代码会报错，也就是不能这样配置吗？

--------------------------------

from pyhanlp import *
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
text = ""小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝""
print(NotionalTokenizer.segment(text))

-----------------------------------

java.lang.NullPointerExceptionPyRaisable  Traceback (most recent call last)
<ipython-input-1-bfb7f4ece71a> in <module>()
      4 
      5 text = ""小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝""
      6 print(NotionalTokenizer.segment(text))

java.lang.NullPointerExceptionPyRaisable: java.lang.NullPointerException

----------------------------------------

## 我的问题2
win10也同样配置了hanlp.properties
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; Custom.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns; my.txt; feature_dict.txt; data/dictionary/person/nrf.txt nrf;
同样删除了缓存文件，不知为什么，生成出来的地址始终没有feature_dict.txt， Custom.txt; 

-----------------------------------

print(HanLP.Config.CustomDictionaryPath)
('d:/applications/pyhanlp/data/dictionary/custom/CustomDictionary.txt', 'd:/applications/pyhanlp/data/dictionary/custom/现代汉语补充词库.txt', 'd:/applications/pyhanlp/data/dictionary/custom/全国地名大全.txt ns', 'd:/applications/pyhanlp/data/dictionary/custom/人名词典.txt', 'd:/applications/pyhanlp/data/dictionary/custom/机构名词典.txt', 'd:/applications/pyhanlp/data/dictionary/custom/上海地名.txt ns', 'd:/applications/pyhanlp/data/dictionary/person/nrf.txt nrf')

-----------------------------------

上面就原始的七个，有什么检查方法吗，java也不是特别熟。"
n-gram关系由来,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
GitHub仓库版 master分支

当前最新版本号是： GitHub仓库版 master分支 
我使用的版本是： GitHub仓库版 master分支 

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我想咨询下hanlp的二元关系是怎么得到的？例如‘一@公益 1’，‘一@一对一 5’。我理解的二元关系是左元+右元能够是一个搭配之类的，但是这样的二元关系看起来有点怪，是不是我理解错了hanlp的二元关系？另外对于n-gram的搭配的‘出场顺序’也存在疑问，为什么整体上二元关系出现的频率都比较小，是跟语料有关系吗？烦请能够指点。非常感谢！
"
英文括号无法拆分,"目前存在问题，伪代码如下：

   ```
       String url = ""(((?)))"";
        CharType.type['('] = CharType.CT_DELIMITER;
       CustomDictionary.add(""("");
       调用 HanLP.segment(url);
   ```
结果如下：
(((?)))
目前版本是 portable-1.6.8
请问如何把字符拆开？


   "
urllib.error.URLError: <urlopen error unknown url type: https>,"python 3.6
pyhanlp  0.1.44
在运行如下代码时，报错

from pyhanlp import *
text = """"
print(HanLP.segment(text))

error：File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 116, in <module>
    _start_jvm_for_hanlp()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 38, in _start_jvm_for_hanlp
    from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 305, in <module>
    install_hanlp_jar()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 191, in install_hanlp_jar
    version = hanlp_latest_version()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 68, in hanlp_latest_version
    return hanlp_releases()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 76, in hanlp_releases
    content = urllib.urlopen(""https://api.github.com/repos/hankcs/HanLP/releases"").read()
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 549, in _open
    'unknown_open', req)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 1388, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: https>

"
基于信息凝固度与外部信息熵的新词识别的Java实现,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

Hi，Hankcs.很早以前就一直在关注和学习你的Blog。我的问题如下：
最近在做一个无监督提取领域词实验。看到您的《基于信息熵和互信息的新词识别》一文结合了内部凝固度与外部左右熵的Java实现，请问这部分代码是否在Hanlp中集成？ 在评论下看到你的回复：“哦，HanLP里面是提取短语用的，这个是提取词语用的，不一样的”，请问一下HanLP中所开源的信息熵短语识别与《基于信息熵和互信息的新词识别》文中提到的有何异同？如有不同，可否参考一下您的该文的Java实现？非常感谢！

## 复现问题
暂无

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   NULL
```
### 期望输出

NULL

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

[《基于信息熵和互信息的新词识别》](http://www.hankcs.com/nlp/new-word-discovery.html)

"
hanlp 可以添加英文单词组合的作为新词放到词典里吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

在CustomDictionary.txt中添加如下英文单词组合的新词，英文都是空格隔开的了我把分割符换成\t也不行
air motor n 1

报错：严重: 自定义词典D:/An/HanLP/data/dictionary/custom/CustomDictionary.txt读取错误！java.lang.NumberFormatException: For input string: ""n""


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""air motor is very good""));
    }
```"
Hanlp分词遇到Unicode表情符号程序就崩溃,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
只要分句中带有unicode表情符号，断点都没有作用，Python就停止工作，Java也是如此

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

```python
from pyhanlp import *

if __name__ == '__main__':
    try:
        result=HanLP.segment(""🙏🙏🙏"")
    except Exception:
        result=""""
    print(result)

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
应该根据正常的Unicode表情符号分词
"
elasticsearch-analysis-hanlp 插件在分词(analyze)时不支持归一化,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP 1.6.8
我使用的版本是：elasticsearch-analysis-hanlp 6.3.2(ElasticSearch插件版本)

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我通过`./bin/elasticsearch-plugin install https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v6.3.2/elasticsearch-analysis-hanlp-6.3.2.zip`安装了 elasticsearch-analysis-hanlp 6.3.2 插件。在ElasticSearch-Kibana devTools上分析：
```html
GET index_ik_test/_analyze
{
  ""analyzer"": ""hanlp"",
  ""text"": [""④⑧⑦④22②8Q笳看""]
}
```
得到的分词结果如下：(用 斜杠 来分隔分词结果)。
>④⑧⑦④/22/②/8/Q/笳/看

由于有特殊的数字字符，这些数字并没有分在一起。而我的需求是：在分词之前，先将一些特殊数字字符进行归一化，这样分词之后数字就能分在一起了，在HanLP1.6.3中测试归一化如下：
```java
    @Test
    public void testNorm() {
        HanLP.Config.Normalization = true;
        System.out.println(HanLP.segment(""④⑧⑦④22②8Q笳看""));
    }
```
输出：
>[48742228/m, q/nx, 笳/g, 看/v]

符合我的要求。但是ElasticSearch中用 elasticsearch-analysis-hanlp插件(hanlp、hanlp_standard、hanlp_index等分词器)来分词时，没有归一化效果。

我的问题是： elasticsearch-analysis-hanlp插件 是否考虑在分词时通过配置方式来支持归一化？

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
我看了下插件源码中： [com.hankcs.cfg.Configuration.java](https://github.com/KennFalcon/elasticsearch-analysis-hanlp/blob/master/src/main/java/com/hankcs/cfg/Configuration.java)里面定义了一些 关于分词的配置信息，但是没有关于是否开启归一化的配置 `HanLP.Config.Normalization = true`。
"
获取关键词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
hancks,您好！我想咨询一下，就是获得关键词的时候，第一步是要进行分词，源码中看到分词是用的DefaultSegment，这个分词是运用什么进行分词的？如果想用其他分词器，可以实现么？感谢大神献出这么好的一个资源，希望得到大神的指点，哈哈！

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
pyhanlp可以配置直接使用hanlp-portable-1.6.8.jar，而不需要hanlp.properties文件吗？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

    我将pyhanlp安装后，把hanlp-1.6.8.jar用hanlp-portable-1.6.8.jar替换
  
    但是依然需要hanlp.properties文件，可以直接和java一样使用快速版本吗？
    
    然后用户通过加入词典直接更新hanlp-portable-1.6.8.jar里面的txt.bin文件即可？

    恕我冒昧哈，提出这样的一个请求，里面pyhanlp代码好复杂，没怎么看懂

"
hanlp-portable-1.6.8.jar包中的CustomDictionary.txt.bin只有3802KB，但是自己根据默认配置产生有16583KB,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题


我想添加一点自己的词到\data\dictionary\custom目录下，然后产生bin文件

然后放到hanlp-portable-1.6.8.jar中覆盖CustomDictionary.txt.bin

但是为了保证jar包大小不要增加太多

所以想基于hanlp-portable-1.6.8.jar默认选的几个词典文件,但是怎么尝试都得不到3802KB

所以请教hanlp-portable-1.6.8.jar产生的CustomDictionary.txt.bin是基于哪几个txt文件吗？

"
DoubleArrayTrie的LongestSearcher有时候匹配遗漏,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：portable-1.6.6

## 我的问题
在使用DoubleArrayTrie类的LongestSearcher方法时，有些词匹配不到。

## 复现问题
### 步骤
1. 构建一个DoubleArrayTrie，包含以下关键词：
001乡道, 北京, 北京市通信公司, 来广营乡, 通州区
2. 用上面构建的DART，使用getLongestSearcher()，匹配下面的句子：
北京市通州区001乡道发生了一件有意思的事情，来广营乡歌舞队正在跳舞

### 触发代码
		DoubleArrayTrie<String>.LongestSearcher searcher = dart.getLongestSearcher(str, 0);
		while (searcher.next())
		{
                     ...
		}
### 期望输出
期望能按顺序匹配到下面的词：
北京 通州区 001乡道 来广营乡

### 实际输出
只匹配到了下面的词（通州区没有被匹配到）
北京 001乡道 来广营乡

## 其他信息
使用getSearcher()没有问题
"
java源码出现错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：hanlp-1.6.8
我使用的版本是：hanlp-1.6.8

## 我的问题

在hanlp-1.6.8中hmm2 的分词模型已经被废除，在data中实际不存在，但是在hanlp.java的
的源码中，仍然可以通过传入hmm2字符串来获得hmm2的分词器。

源码->hanlp.java
```
 /**
         * HMM分词模型
         *
         * @deprecated 已废弃，请使用{@link PerceptronLexicalAnalyzer}
         */
        public static String HMMSegmentModelPath = ""data/model/segment/HMMSegmentModel.bin"";


......
        else if (""hmm2"".equals(algorithm) || ""二阶隐马"".equals(algorithm))
            return new HMMSegment();

```

如果真的使用hmm2就会引起以下错误
错误
```
java.lang.IllegalArgumentExceptionPyRaisable: java.lang.IllegalArgumentException: 发生了异常：java.lang.IllegalArgumentException: HMM分词模型[ /home/font/anaconda3/lib/python3.6/site-packages/pyhanlp/static/data/model/segment/HMMSegmentModel.bin ]不存在
```"
pyhanlp在Debug断掉调试模型就会Python停止工作,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用pyhanlp过程中，如果是Debug模式，使用IDEA断点调试，就会出现Python停止工作
![1](https://user-images.githubusercontent.com/6327360/45194246-ced08780-b284-11e8-97c2-a519004e9fdd.png)


"
data中缺少segment文件夹?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
pyhanlp-1.6.8

当前最新版本号是：pyhanlp-1.6.8
我使用的版本是：pyhanlp-1.6.8



## 我的问题

使用自带demo中的CRF分词进行分词,出现模型加载错误,发现data-model中缺少相应segment文件夹.下载data后解压发现,还是没有该文件夹.

## 使用代码
在使用demo中的CRF分词时出现问题,代码如下
```
Config = JClass(""com.hankcs.hanlp.HanLP$Config"")
Config.ShowTermNature = False
CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
segment = CRFSegment().enableCustomDictionary(False)

for sentence in sentence_array:
    term_list = segment.seg(sentence)
    print(term_list)

```

## 出现错误
```
---------------------------------------------------------------------------
java.lang.IllegalArgumentExceptionPyRaisableTraceback (most recent call last)
<ipython-input-6-d745364c843d> in <module>()
     25 Config.ShowTermNature = False
     26 CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
---> 27 segment = CRFSegment().enableCustomDictionary(False)
     28 
     29 for sentence in sentence_array:

~/anaconda3/lib/python3.6/site-packages/jpype/_jclass.py in _javaInit(self, *args)
    109     else:
    110         self.__javaobject__ = self.__class__.__javaclass__.newClassInstance(
--> 111             *args)
    112 
    113 

java.lang.IllegalArgumentExceptionPyRaisable: java.lang.IllegalArgumentException: CRF分词模型加载 /home/font/anaconda3/lib/python3.6/site-packages/pyhanlp/static/data/model/segment/CRFSegmentModel.txt 失败，耗时 6 ms
```
"
I did not see the download address of 100 million corps,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

1亿字的中文语料，没有看到下载地址，想自己训练模型
"
[提问]自建某个行业的语料及遇到的分词不准问题 ,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题


我想咨询一下楼主，如果想自建某个行业的语料，我需要哪些操作步骤？
我现在的操作是：

1. 先对行业数据进行标准分词（标准分词后，会自动生成词性）
2. 对关键词进行自定义词性标注的检查和修正
3. 使用PerceptronLexicalAnalyzer（感知机词法分析器）进行分词和实体识别
4. 规则审核

但是在训练分好词和标注好词性的语料数据时（按照199801的语料分词标准准备的数据），有些分词和词性识别并不是很准还有些词性会被覆盖。
1. 问题1：这个时候是不是需要使用PerceptronLexicalAnalyzer.learn方法来进行学习修正，还是我上述的操作本身是有问题的？
2. 问题2：在进行语料标注时，标准词性和自定义复合词性对于上下文是否有要求？如果有要求，如何界定上下文的边界？
3. 问题3：语料训练时候，是需要将整片文章进行训练，还是只对需要提取内容的部分段落进行训练？
4. 问题4：实体识别时，如果一个实体存在不同词性时，是否需要根据上下文分词来判断当前实体的具体词性？如何设置上下文的窗口大小？


## 复现问题


### 步骤

1. 我先训练自己的语料(一份进行标准分词的合同正文)，在进行标准分词后，我对语料进行了词性修改（标注为自定义词性），然后将整篇文章进行训练
```
客户信息/khxx ：/w 
客户方/khfbq ：/w 致富银行股份有限公司记账中心/khfname
[发/v 票/n 内容/n]/fpxx
名称/fpxxmcbq ：/w 致富科技有限公司记账中心122号/fpxxmc
[发票/n 抬头/vi]/fptt ：/w 致富科技有限公司记账中心/fpttname
.......
```

2. 然后调用HanLP的API训练上面的语料
```
               //1、分词
		CWSTrainer();
		//2、词性标注
		POSTrainer();
		//3、实体识别
		NERTrainer();
```
3. 训练完成后，生成自定义的cws、pos、ner.bin和bin.txt文件，然后使用感知机分词进行生产数据的分词
```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(自定义_CWS_MODEL_FILE, 
   自定义_POS_MODEL_FILE,自定义_NER_MODEL_FILE);
   String testInfoTxt = FileUtils.readTxt(""D:\\testInfo.txt"");
   List<List<Term>> list = analyzer.seg2sentence(testInfoTxt );
   
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
--->[客户信息/khxx, ：/w]
--->[客户方/khfbq, ：/w, 致富科技有限公司记账中心/khfname]
--->[发票内容/fpxx]
--->[名称/fpxxmcbq, ：/w, 致富科技有限公司记账中心122号/fpxxmc]
--->[发票抬头：/fptt, 致富科技有限公司记账中心/fpttname]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
--->[客户信息/khxx, ：/w]
--->[客户/n]
--->[方/q, ：/w, 致富科技有限公司记账中心/khfname]
--->[发票内容/fpxx]
--->[名称/n, ：/w, 致富科技有限公司记账中心122号/khfname]
--->[发票抬头：/fptt, 致富科技有限公司记账中心/khfname]
```


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
感知机分词器不能正确处理文本中的空格,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
感知机分词器不能正确处理文本中的空格

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
待分词的文本：
```北京二手房成交涨五成 业内：贷款利率仍将上升```

感知机分词器（PerceptronLexicalAnalyzer）的结果：
```北京/ns 二手房/n 成交/v 涨五成 业内/s ：/vn 贷款/vn 利率/n 仍/d 将/d 上升/v```
这里“涨五成 业内”被分成了一个词。

对比默认分词器（ViterbiSegment）的结果：
```北京/ns 二手房/n 成交/v 涨/vi 五/m 成/v  /w 业内/n ：/w 贷款/n 利率/n 仍/d 将/d 上升/vi```
这里“涨五成”后的空格被正确切分出来。

## 其他信息
hanlp.properties等都是默认配置，1.6.8加载的感知机分词模型应该是large/cws.bin。

"
自定义词典更新时自动删除缓存文件,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
经常在 Issues 中看到因为没有删除缓存造成的自定义词典不生效的问题。

本更新使得用户在使用本地文件系统的情况下（`com.hankcs.hanlp.corpus.io.FileIOAdapter`），修改自定义词典后，缓存文件会自动删除，从而达到自定义词典及时生效的目的。

"
自定义词典，使用多个，只有第一个起作用,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
按照帖子CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;
方法添加自定义字典，但是只有一个起作用，我的词典.txt和CustomDictionary.txt同目录，即使我的词典用全路径data/dictionary/custom/我的词典.txt;也不可用。
测试分词的时候第二个字典里的词总是分不 出来，但是切换第1,2个字典位置，就能分词出来，但是原来的字典里的词就分不出来了。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
感知机学习语料报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
你好，我在使用最新版的感知机做在线学习分词的时候，发现语句`李琳/nr 部长/nnt 湘雅附二院/nth`在线学习的时候会报错。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
最开始我是在修改过的`large/cws.bin`发现的问题，起初怀疑是我的新模型有问题。为了验证，我重新下了`large/cws.bin`，再一次跑了`触发代码`，发现还是有同样的问题。同时，我做了对比，`1.6.6`版本也有这个问题。

### 触发代码

```
 HanLP.Config.enableDebug();
 PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
 System.out.println(analyzer.learn(""武汉/ns 水电工/n 吴师傅/nr""));
 System.out.println(analyzer.learn(""李琳/nr 部长/nnt 湘雅附二院/nth""));
 System.out.println(analyzer.analyze(""李琳部长湘雅附二院""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
李琳/nr 部长/nnt 湘雅附二院/nth
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
加载 /Users/fiveddd/hanlp_data/data/model/perceptron/large/cws.bin ... 耗时 1228 ms 加载完毕
加载 /Users/fiveddd/hanlp_data/data/model/perceptron/pku199801/pos.bin ... 耗时 403 ms 加载完毕
加载 /Users/fiveddd/hanlp_data/data/model/perceptron/pku199801/ner.bin ... 耗时 7 ms 加载完毕
八月 31, 2018 12:03:22 下午 com.hankcs.hanlp.dictionary.other.CharTable <clinit>
信息: 字符正规化表开始加载/Users/fiveddd/hanlp_data/data/dictionary/other/CharTable.txt
八月 31, 2018 12:03:22 下午 com.hankcs.hanlp.dictionary.other.CharTable <clinit>
信息: 字符正规化表加载成功：115 ms
true
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 13983494
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.score(LinearModel.java:371)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.viterbiDecode(LinearModel.java:296)
	at com.hankcs.hanlp.model.perceptron.model.StructuredPerceptron.update(StructuredPerceptron.java:68)
	at com.hankcs.hanlp.model.perceptron.PerceptronTagger.learn(PerceptronTagger.java:58)
	at com.hankcs.hanlp.model.perceptron.PerceptronTagger.learn(PerceptronTagger.java:70)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.learn(PerceptronLexicalAnalyzer.java:163)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.learn(PerceptronLexicalAnalyzer.java:150)
	at trainHanlpModel.main(trainHanlpModel.java:38)

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
List<Term> 怎么转换成List<String>,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8  master text 里面的com

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我想做两个短文本的相似性判断，仅仅根据关键词或者分词的相似性，相似的单词越多则两文本相似性更好。
因为HanLP很多分词函数返回是List<Term>，而通过DemoWordDistance文件判断两个单词相似性是String类型，所以我需要把Term转变成String.

因为代码报错，无法输出结果。后面详细问题并未填写。

Update Comment:感谢yaleimeng,问题得到解决，因为我测试用的是github的test里面下载的com大文件夹，而test里面的com/hankcs/hanlp/seg/common/下面没有Term.java，但是import 这个位置的Term没问题，好奇怪啊
/
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

对“龙子湖高校园区15号河南农业大学”分词时崩溃

## 复现问题
对“龙子湖高校园区15号河南农业大学”分词时崩溃

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args){
        Segment seg = new DijkstraSegment();
        seg.seg(""龙子湖高校园区河南农业大学"");
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
不崩溃
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
崩溃了
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Merge pull request #1 from hankcs/master,"merge

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
solr中使用hanlp自定义词典相关问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

solr中使用自定义同义词词库，多个同义词时返回不完整

## 复现问题

我的同义词词典synonyms_department.txt, 每一行的内容如下：

`肾内科,肾脏科,肾病内科,肾病科,肾脏内科,肾内`

定义一个hanlp分词的type:
```
     <fieldType name=""hanlp_syn_department"" class=""solr.TextField"" positionIncrementGap=""100"">
          <analyzer type=""index"">
               <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""true"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
               <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms_department.txt"" ignoreCase=""true"" expand=""false"" />
          </analyzer>
          <analyzer type=""query"">
              <!-- 切记不要在query中开启index模式 -->     
              <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""false"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
          </analyzer>              
     </fieldType>
```
使用hanlp_syn_department类型的字段做analysis：
![screen shot 2018-08-29 at 1 05 08 pm](https://user-images.githubusercontent.com/38783332/44766503-479c5900-ab8c-11e8-8d6c-bfc22b62e21a.png)


相同的同义词词典，在ik中的效果：
![screen shot 2018-08-29 at 12 25 31 pm](https://user-images.githubusercontent.com/38783332/44765325-c4c4cf80-ab86-11e8-853a-718b20f8bab1.png)


其他issues关于同义词的配置也翻了还是没有找到原因，请指教问题出在哪里. "
请教下NLPTokenizer切分短网址问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.8

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh 退订回T
短网址前后有空格，切分的词是：一起来 t.cn/rdhcuch 退订/d,。
这个切分是不是有问题？我把空格去掉后切分是这样的
一起/d, 来t/v, ./v, cn/rdhcuch/nx, 退订/v
来t和.号切分成动词，怎么会这样样切分呢？

有没有补救的办法

## 复现问题
System.out.println(NLPTokenizer
            .segment(""集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh 退订回T""));

System.out.println(NLPTokenizer
            .segment(""集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh退订回T""));

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请教一下关于分词的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

1.6.8

## 我的问题

我想取出如:""北京市人民医院"" 这样一个词,但是总是会分成 ""北京市"" + ""人民医院"" 两个词,请问有什么好的解决方式吗? (地名也需要取出来)

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
排除人名<周有>,"下周有雨 下周有雪 等不排除会误识别出人名 周有雨 周有雪

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
使用索引分词，遇到了奇怪的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
分词内容“公募基金20年绩优基金和绩优基金经理数据”，和预期的有差异。
1. 为什么“20年绩优”被分词为“20”和“年绩优”
2. 用在搜索引擎中，词语分词的时候不用进行组合吗？比如“基金经理”，分词出来的会有以下结果“基金”，“经理”，“基金经理”，“基金经理”会单独作为一个分词结果。

## 复现问题
先后使用程序自带的词典文件和下载的词典包 data.zip，输出的内容相同

### 触发代码

```
    public class DemoIndexSegment
{
    public static void main(String[] args)
    {
        List<Term> termList = IndexTokenizer.segment(""公募基金20年绩优基金和绩优基金经理数据"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

        System.out.println(""\n最细颗粒度切分："");
        IndexTokenizer.SEGMENT.enableIndexMode(1);
        termList = IndexTokenizer.segment(""公募基金20年绩优基金和绩优基金经理数据"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
    }
}
```
### 期望输出

```
公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
年绩优/nr [6:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]

最细颗粒度切分：
公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
20年...
年...
绩优/nr [6:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]
```

### 实际输出


```
公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
年绩优/nr [6:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]

最细颗粒度切分：
公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
年绩优/nr [6:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]
```

## 其他信息

DEBUG 模式输出
```
公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
年绩优/nr [6:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]

最细颗粒度切分：
粗分词网：
0:[ ]
1:[公, 公募]
2:[募]
3:[基, 基金]
4:[金]
5:[20]
6:[]
7:[年]
8:[绩]
9:[优]
10:[基, 基金]
11:[金]
12:[和]
13:[绩]
14:[优]
15:[基, 基金]
16:[金]
17:[经, 经理]
18:[理]
19:[数, 数据]
20:[据]
21:[ ]

粗分结果[公募/nz, 基金/n, 20/m, 年/qt, 绩/ng, 优/ag, 基金/n, 和/cc, 绩/ng, 优/ag, 基金/n, 经理/nnt, 数据/n]
人名角色观察：[  K 1 A 1 ][公募 A 20833310 ][基金 L 96 K 1 U 1 ][20 L 32 K 1 ][年 D 219 B 61 C 42 L 20 K 12 E 8 ][绩 D 8 E 6 C 1 ][优 E 110 C 8 D 6 K 1 ][基金 L 96 K 1 U 1 ][和 M 15401 L 2868 K 2281 D 538 E 34 C 1 ][绩 D 8 E 6 C 1 ][优 E 110 C 8 D 6 K 1 ][基金 L 96 K 1 U 1 ][经理 K 205 Z 42 L 5 ][数据 L 3 ][  K 1 A 1 ]
人名角色标注：[ /K ,公募/A ,基金/K ,20/L ,年/B ,绩/E ,优/E ,基金/L ,和/M ,绩/E ,优/E ,基金/L ,经理/Z ,数据/L , /A]
识别出人名：年绩 BE
识别出人名：绩优 EE
识别出人名：年绩优 BEE
识别出人名：绩优 EE
细分词网：
0:[ ]
1:[公募]
2:[]
3:[基金]
4:[]
5:[20]
6:[]
7:[年, 年绩, 年绩优]
8:[绩, 绩优]
9:[优]
10:[基金]
11:[]
12:[和]
13:[绩, 绩优]
14:[优]
15:[基金]
16:[]
17:[经理]
18:[]
19:[数据]
20:[]
21:[ ]

公募/nz [0:2]
基金/n [2:4]
20/m [4:6]
年绩优/nr [6:9]
年/qt [6:7]
绩/ng [7:8]
优/ag [8:9]
基金/n [9:11]
和/cc [11:12]
绩优/nr [12:14]
基金/n [14:16]
经理/nnt [16:18]
数据/n [18:20]
```
"
update,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
"全拼词典文件错误（体面=ti3,cao3）","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

体面转全拼，结果时ticao

## 复现问题

### 步骤


### 触发代码

String str = “体面”;
List<Pinyin> pinyinList = HanLP.convertToPinyinList(str);

### 期望输出
timian

### 实际输出
ticao

## 其他信息

经确认，pinyin.txt中配置的是：体面=ti3,cao3
应该修改为：体面=ti3,mian4
"
data-for-1.6.8.zip资源文件自定义词典中文乱码,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.8
我使用的版本是：portable-1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在windows下，解压缩资源包，文件名都显示正常；在ubuntu系统解压缩，data/dictionary/custom/目录下，中文名字的字典文件出现乱码。



<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
fail to set relative path in hanlp.properties,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.6.7
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

设置hanlp.properties中root为相对路径无效
参考 https://github.com/hankcs/HanLP/pull/254
尝试不同的relative path设置方式都失败了，以下为其中一种举例

## 复现问题

### hanlp.properties:
#本配置文件中的路径的根目录，根目录+其他路径=完整路径（支持相对路径，请参考：https://github.com/hankcs/HanLP/pull/254）
#Windows用户请注意，路径分隔符统一使用/
root=../HanLP/

### path
resources
\- HanLP
    \- data
        \- ...
   hanlp.properties

### report:
首次编译运行时，HanLP会自动构建词典缓存，请稍候……
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: 读取../HanLP/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: 核心词典../HanLP/data/dictionary/CoreNatureDictionary.txt不存在！java.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt (No such file or directory)

java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at HanLPMain.test(HanLPMain.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:124)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:580)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:716)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:988)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.privateRun(TestRunner.java:648)
	at org.testng.TestRunner.run(TestRunner.java:505)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:455)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:450)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:415)
	at org.testng.SuiteRunner.run(SuiteRunner.java:364)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:84)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1208)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1137)
	at org.testng.TestNG.runSuites(TestNG.java:1049)
	at org.testng.TestNG.run(TestNG.java:1017)
	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72)
	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123)
Caused by: java.lang.IllegalArgumentException: 核心词典../HanLP/data/dictionary/CoreNatureDictionary.txt加载失败
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 31 more"
命令行训练Word2Vec模型时，如何开启debug输出训练进度？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.7
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在训练Word2Vec模型时发现命令行训练的时候除了开始的thread和iter，没有其他输出，查看源码发现训练中是使用的logger输出进度信息。
通过代码训练可以注册callback获取训练进度，请问命令行方式执行如何打开debug输出logger以查看进度？
"
命名实体识别报错，不知原因,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.7
我使用的版本是：portable-1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

命名实体识别，个别报错java.util.NoSuchElementException

## 复现问题
正常测试

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
           String s1 = ""有限公司"";
        String s2 = ""工程有限公司"";
        String s3 = ""生态工程有限公司"";
//        String s4 = ""江川生态工程有限公司"";
//        String s4 = ""元亨生态工程有限公司"";
        String s4 = ""福哈生态工程有限公司"";

        Segment segment = HanLP.newSegment().enableOrganizationRecognize(true);
        List<Term> term1 =segment.seg(s1);
        for (Term t :term1) {
            System.out.print(t);
        }
        System.out.println("""");
        List<Term> term2 =segment.seg(s2);
        for (Term t :term2) {
            System.out.print(t);
        }
        System.out.println("""");

        List<Term> term3 =segment.seg(s3);
        for (Term t :term3) {
            System.out.print(t);
        }
        System.out.println("""");
        List<Term> term4 =segment.seg(s4);
        for (Term t :term4) {
            System.out.print(t);
        }
        System.out.println("""");
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
有限公司/nis
工程有限公司/nt
生态工程/nz有限公司/nis
福哈/nr生态工程/nz有限公司/nis
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
有限公司/nis
工程有限公司/nt
生态工程/nz有限公司/nis
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList$ListItr.next(LinkedList.java:890)
	at com.hankcs.hanlp.algorithm.Viterbi.computeEnum(Viterbi.java:188)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.viterbiCompute(OrganizationRecognition.java:124)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.recognition(OrganizationRecognition.java:53)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:100)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at com.cubr.slf.test.test.main(test.java:38)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/36945916/44455017-99466000-a62f-11e8-9b32-2e09756a080f.png)

"
批量繁体转简体时碰到的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.7
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
当我尝试将一批词语（简体词和繁体词都有）转换为简体时发现【战列舰】一词转换异常

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
    public static void main(String[] args) throws Exception
    {
               String str=""战列舰"";
		System.out.println(HanLP.t2s(str));
		System.out.println(HanLP.convertToSimplifiedChinese(str));
    }
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
战列舰
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
主力艦
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
portable 版提示【没有找到hanlp.properties，可能会导致找不到data】,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.7
我使用的版本是：portable-1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题


        <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.6.7</version>
        </dependency>

我使用maven 的portable版  在使用HanLp 时还说提示需要指定数据目录，portable 版不是内置了data吗？ 还是现在需要单独配置或者这个提示可以忽略


![qq 20180822090353](https://user-images.githubusercontent.com/16408873/44436934-4f875680-a5ea-11e8-9358-eb06a2c30072.png)


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
看到有个类似问题，但是他是打包，我这个直接使用maven 不需要打包吧

[https://github.com/hankcs/HanLP/issues/226](url)
"
pyhanlp CRF词法分析中词性显示不全？,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.7  
我使用的版本是：1.6.7   pyhanlp 0.1.44

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用CRF词法分析时，得到的词性标注信息不完整。但以前使用CRFSegment时词性标注正常。是不是两者的词性体系不一样？


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
from pyhanlp import *

sent = '''哪些项目可以享受优惠政策，如何申请？'''

# CRF词法分析
CRFAnalyzer =  JClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
seg = CRFAnalyzer().enableCustomDictionary(True)
out = seg.analyze(sent)
print(out)  

# CRF分词
Config = JClass(""com.hankcs.hanlp.HanLP$Config"")
Config.ShowTermNature = True
CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
seger = CRFSegment().enableCustomDictionary(True)
words =[str(term) for term in  seger.seg(sent)]
print(' '.join(words))
```
### 期望输出

```
每个词语标注完整的词性，比如 如何/ryv   
```

### 实际输出

```
部分词语的标注只有开头一两个字母。例如：
CRF词法分析结果：
- 哪些/r 项目/n 可以/v 享受/v 优惠政策/nz ，/w 如何/r 申请/v ？/w
- 每年/r 在/p 青海湖/ns 迁徙/v 停留/v 的/u 候鸟/n 有/v 92/m 种/q 。/w
老版本CRFSegment结果：
- 哪些/ry 项目/n 可以/v 享受/v 优惠政策/nz ，/w 如何/ryv 申请/v ？/w
- 每年/t 在/p 青海湖/ns 迁徙/v 停留/vi 的/ude1 候鸟/n 有/vyou 92/mq 种/q 。/w

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->"
CRFNERecognizer用pku1998年1-6月份数据训练，java.lang.OutOfMemoryError：GC overhead limit exceeded,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.6.7
我使用的版本是：v1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
CRFNERecognizer用pku1998年1-6月份数据(64.6M)训练内存溢出:java.lang.OutOfMemoryError：GC overhead limit exceeded
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args) throws IOException {
	String CORPUS = ""data/test/pku98/1998-2.txt"";
	String NER_MODEL_PATH = ""data/model/crf/pku1998-2/ner.txt"";
	CRFTagger tagger = new CRFNERecognizer(null);
        tagger.train(CORPUS, NER_MODEL_PATH);
	}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
同义词的词典是怎么训练的？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：V1.6.7
我使用的版本是：V1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
同义词的词典是怎么训练的？
dictionary\synonym\CoreSynonym.txt怎么添加新的行？

"
去除停用词，提取有用的关键词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.7.jar
我使用的版本是：hanlp-1.6.7.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我的问题在于提取的关键词会有一些不需要的词
比如：我根据一个电器公司的简介和经营产品，那么提取的关键词个人感觉应该都跟电器相关的
但是会有几个不重要的词会提取出来，请问怎么解决，有什么意见，请告诉我，谢谢大佬。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
![image](https://user-images.githubusercontent.com/32606897/44244108-d7ddb400-a204-11e8-8ebd-f6893840cd47.png)
```
  private static List getKeyWordRank(String path) throws IOException {
        String t1 = new String();
        BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), ""utf-8""));
        String str;
        while ((str = reader.readLine()) != null) {
            t1 += str;
        }
        List<String> list = HanLP.extractKeyword(t1, 10);
        for (String item:list) {
            System.out.println(item);
        }
        return list;
    }


```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出  [电视, 电脑, 电器, 空调, 电冰箱, 电子, 微波炉, 吸尘器, 消毒柜, 厨房]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出 [电视, 电脑, 电器, 空调, 海尔, 电子, 网点, 营业额, 白电, 厨房]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在自定义词典的情况下，索引分词全切分模式达不到预期的效果,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.6.7
我使用的版本是：hanlp-portable-1.6.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在使用HanLP分词时，自定义词典中已经添加了“南保村”、“保村”、“南保”词条，但是对“南保村民委员会”的分词结果中没有“南保村”和“保村”，分词结果不准确；对比使用IK分词时，使用相同内容的自定义词典，“南保村民委员会”的分词结果中含有“南保村”、“保村”、“南保”。

<!-- 请详细描述问题，越详细越可能得到解决 -->
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
1.HanLP分词结果如下
![hanlp](https://user-images.githubusercontent.com/42425179/44185683-8cb19b80-a147-11e8-8999-67728033cbdb.png)

2.IK使用同样的自定义词典内容，分词结果为： “南保村”，“南保”，“保村”，“村民委员会”，“村民”，“民委”，“委员会”
![ik](https://user-images.githubusercontent.com/42425179/44185677-815e7000-a147-11e8-984a-03b885cee745.png)


### 步骤

1. 首先设置自定义词典
2. 然后设置schema.xml中的fieldType节点
![image](https://user-images.githubusercontent.com/42425179/44185966-3a717a00-a149-11e8-8b91-ad9413f1da8b.png)
![image](https://user-images.githubusercontent.com/42425179/44185984-4f4e0d80-a149-11e8-9c9c-ad8f39b98541.png)

3. 接着对“南保村民委员会”进行分词

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""保村"");
        CustomDictionary.add(""南保村"");
        CustomDictionary.add(""南保"");
        System.out.println(IndexTokenizer.segment(""南保村民委员会""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出 “南保村”，“南保”，“保村”，“村民委员会”，“村民”，“民委”，“委员”，“委员会”
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出 “南保”，“村民委员会”，“村民”，“民委”，“委员”，“委员会”
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
贝叶斯模型训练文本分类模型时，选中特征数:0 / 380 = 0.00%,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.7.jar
我使用的版本是：hanlp-1.6.7.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我使用朴素贝叶斯模型训练进行文本分类时，我自己训练两个种类：商票、体育，每个种类训练5个句子，结果日志显示：
正在构造训练数据集...[商票]...50.00%...[体育]...100.00%...耗时 260 ms 加载完毕
原始数据集大小:10
使用卡方检测选择特征中...耗时 1 ms,选中特征数:0 / 380 = 0.00%
用训练得到的模型测试不同种类的文本时，每次都是体育，这是怎么回事呢？是训练的素材不够导致的吗？
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请问怎么训练依存句法模型,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：V1.6.7
我使用的版本是：V1.6.6

<!--以上属于必填项，以下可自由发挥-->

请问怎么训练依存句法模型？
有这样的示例吗？
"
portable-1.6.7版本中使用maven构建的情况下，执行NLPTokenizer.analyze分析时，报如下错误,"警告: 打开失败：data/model/perceptron/msra/cws.bin
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at HanLPTest.main(HanLPTest.java:18)
Caused by: java.lang.RuntimeException: java.io.IOException: data/model/perceptron/msra/cws.bin 加载失败
Maven构建的情况下，不是不需要额外手动导入data吗？"
 MaxEntDependencyParser在1.6.6版本抛出异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：hanlp-portable-1.6.6
我使用的版本是：hanlp-portable-1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
代码为： CoNLLSentence sent =  MaxEntDependencyParser.compute(text);
在1.6.4版本运行没问题
在1.6.6版本运行抛出异常：
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.dependency.MinimumSpanningTreeParser.parse(MinimumSpanningTreeParser.java:41)
	at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:56)
	at com.hankcs.hanlp.dependency.MaxEntDependencyParser.compute(MaxEntDependencyParser.java:86)
	at Test.main(Test.java:54)
Caused by: java.lang.NullPointerException
	at com.hankcs.hanlp.dependency.common.Node.<init>(Node.java:113)
	at com.hankcs.hanlp.dependency.common.Node.<clinit>(Node.java:28)
	... 4 more
"
solr分词与在线分词有差别的原因？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.5.3
我使用的版本是：hanlp-portable-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在solr配置了最新的jar包进行分词与http://hanlp.hankcs.com上的在线演示结果不同，主要原因是什么？
例如：夜圣乔治一级葡萄园
在线分词“夜圣乔治”，“一级”，“葡萄园”
solr分词“夜”，”圣”，”乔”，”治”，“一级”，“葡萄园”

"
按照词性优先级配置,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：master
我使用的版本是：master


## 我的问题
目前分词的词优先级好像是根据配置的顺序的
请问是否支持按照词性优先级输出分词结果？比如说我这边定义了两个词：
和服 n1
服装 n2
然后分词 日本和服装
但需要根据不同场景我可能需要分别输出和服或者服装，这时候我就需要配置一个优先级来控制输出（n1和n2的输出优先级）"
pyhanlp如何重写Filter过滤器,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp如何重写Filter过滤器

<!-- 请详细描述问题，越详细越可能得到解决 -->
正在使用pyhanlp，有一个需求是需要在使用corestopword后，分词结果保留数字。按照现在java版本的逻辑是需要重写Filter，自己不太知道如何在pyhanlp里面重写过滤器

"
提取新词的几点疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

1.在com.hankcs.hanlp.mining.word.WordInfo.computeProbabilityEntropy()计算左右熵的时候,如果该词位于首或者尾部时取最小值 这样取出的熵值为0,当设置默认熵值,开头结尾的词将无法取出
2.互信息
代码中给出的算法
![1](https://user-images.githubusercontent.com/36916036/43451100-c14b4b42-94e6-11e8-923a-912d6e534bd0.png)

![12379570-7c7355505813d673](https://user-images.githubusercontent.com/36916036/43620279-765a49ce-9704-11e8-80fa-7515e1a37ab4.png)
不是完全一致




"
索引分词错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

对问题`购物车里的产品无法删除`进行索引分词，没有将`购物车`分词出来

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

``` java
    public void testIssue1234() throws Exception
    {
         Segment segment = HanLP.newSegment().enableNameRecognize(false);
         segment.enableIndexMode(true);

         List<Term> termList = segment.seg(""购物车里的产品无法删除"");
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[购物车，购物, 车里, 的, 产品, 无法, 删除]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[购物, 车里, 的, 产品, 无法, 删除]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
 训练的word2vec 模型，放在hdfs上，用spark分布式的加载调用失败问题？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.6
我使用的版本是：portable-1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
 训练的word2vec 模型，放在hdfs上，用spark分布式的加载调用失败问题？
<!-- 请详细描述问题，越详细越可能得到解决 -->



### 触发代码

```
rdd.mapPartitions(iteratos => myFunctions(iteratos, words))
def myFunctions(iterator: Iterator[String], word: String): Iterator[mutable.HashMap[Integer, Float]] = {
    val wordVecModel = new WordVectorModel(""data/model.txt"")
    val docmentsModel = new DocVectorModel(wordVecModel)
    val sets = mutable.Set[mutable.HashMap[Integer, Float]]()
    for (iterm <- iterator) {
      val arrays=iterm.split(""\t"")
      val id=arrays(0).toInt
      val contents =iterm
      docmentsModel.addDocument(id, HanLP.convertToSimplifiedChinese(contents))
      val list = docmentsModel.nearest(word)
      import scala.collection.JavaConversions._
      for (id <- list) {
        val maps = mutable.HashMap[Integer, Float]()
        maps.put(id.getKey, id.getValue)
        sets.add(maps)
      }
    }
    sets.iterator
  }

```

### 错误输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
 空指针异常，在读取模型那行代码上
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
单机可以加载，放在集群上加载就不行
#默认的IO适配器如下，该适配器是基于普通文件系统的。
IOAdapter=com.npl.spark.HadoopFileIoAdapter 也已经重写了
"
维特比算法在分词之后关于词性标注的疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： portable-1.6.6
我使用的版本是： portable-1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我在使用`维特比`分词的时候，碰到一个很奇怪的现象：再碰到一句话，它能够在粗分词的时候正确识别词性，但是在最后的角色标注的时候，如果分词的结果可以组建为机构名（也就是可以组合为`FD`，`FCCD`这一类的组合），那么最后的结果会被识别为机构。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先声明一个`维特比分词`模型，强制开启自定义词典，人名识别，机构识别


### 触发代码

```
import com.hankcs.hanlp.HanLP
HanLP.Config.enableDebug()

val segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableNameRecognize(true).enableAllNamedEntityRecognize(true).
            enableJapaneseNameRecognize(true).enableOrganizationRecognize(true).
            enableTranslatedNameRecognize(true)

println(segment.seg(HanLP.convertToSimplifiedChinese(""贾经理全利集团"")))

println(segment.seg(""贺华平办公室""))
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[贾经理/nr, 全利集团/nt]
[贺华平/nr, 办公室/nis]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
粗分词网：
0:[ ]
1:[贾]
2:[经, 经理]
3:[理]
4:[全]
5:[利]
6:[集, 集团]
7:[团]
8:[ ]

粗分结果[贾/nz, 经理/nnt, 全/a, 利/n, 集团/nis]
人名角色观察：[  K 1 A 1 ][贾 B 1308 D 4 E 2 C 1 ][经理 K 205 Z 42 L 5 ][全 B 876 D 406 C 208 E 16 L 16 ][利 D 602 C 560 E 173 B 30 L 5 K 2 ][集团 L 119 K 26 ][  K 1 A 1 ]
人名角色标注：[ /K ,贾/B ,经理/Z ,全/L ,利/B ,集团/L , /A]
识别出人名：贾经理 BZ
地名角色观察：[  S 1163565 ][贾 C 8 ][经理 A 1 B 1 ][全 A 152 C 101 D 16 B 15 ][利 D 91 C 49 A 4 B 4 ][集团 B 38 A 1 ][  B 1322 ]
地名角色标注：[ /S ,贾/C ,经理/A ,全/C ,利/D ,集团/B , /S]
机构名角色观察：[  S 1169907 ][贾经理 F 6781 B 769 A 266 X 6 ][全 B 26 C 17 A 7 ][利 C 45 A 9 B 3 D 3 ][集团 K 1000 D 1000 ][  B 8423 ]
机构名角色标注：[ /S ,贾经理/F ,全/C ,利/C ,集团/D , /S]
识别出机构名：贾经理全利集团 FCCD
识别出机构名：利集团 CD
识别出机构名：全利集团 CCD
细分词网：
0:[ ]
1:[贾经理, 贾经理全利集团]
2:[经]
3:[理]
4:[全, 全利集团]
5:[利, 利集团]
6:[集团]
7:[]
8:[ ]

[贾经理全利集团/nt]


粗分词网：
0:[ ]
1:[贺]
2:[华]
3:[平]
4:[办, 办公, 办公室]
5:[公, 公室]
6:[室]
7:[ ]

粗分结果[贺/vg, 华/b, 平/v, 办/v, 公室/nz]
人名角色观察：[  K 1 A 1 ][贺 B 1011 E 78 C 70 D 25 ][华 D 4763 B 1296 C 966 E 595 L 1 ][平 D 6844 E 544 C 189 B 36 L 11 ][办 L 4 K 2 ][公室 A 20833310 ][  K 1 A 1 ]
人名角色标注：[ /K ,贺/B ,华/C ,平/D ,办/L ,公室/A , /A]
识别出人名：贺华 BC
识别出人名：贺华平 BCD
地名角色观察：[  S 1163565 ][贺 C 41 D 4 ][华 C 385 D 196 B 22 A 8 E 1 ][平 D 1039 C 241 A 16 B 13 ][办 B 23 A 3 ][公室 Z 20211628 ][  B 1322 ]
地名角色标注：[ /S ,贺/C ,华/D ,平/B ,办/A ,公室/Z , /S]
机构名角色观察：[  S 1169907 ][贺华平 F 6781 B 769 A 266 X 6 ][办 D 213 C 11 A 2 B 2 ][公室 Z 19892007 ][  B 8423 ]
机构名角色标注：[ /S ,贺华平/F ,办/D ,公室/Z , /S]
识别出机构名：贺华平办 FD
细分词网：
0:[ ]
1:[贺华平, 贺华平办]
2:[]
3:[]
4:[办]
5:[公室]
6:[室]
7:[ ]

[贺华平办/nt, 公室/nz]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
1. 我尝试修改过`CoreNatureDictionary.txt`和`CoreNatureDictionary.ngram.txt`（根据[issue813](https://github.com/hankcs/HanLP/issues/813#issuecomment-385966927)），比如针对`贺华平办公室`这一句话，我分别将`贺华平 nr 10000`和`贺华平@办公室 10000`加入到两个文件中，结果并没有改变，请问这种时候应该怎么修改？

2. 我同时使用了感知机去分词，不存在上面说的问题，但是由于目前很想搞明白为什么会出现上面的结果。

3. 非常期待并且感谢你的回复。
"
在一个工程里面同时使用多份自定义词典,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：4.0.0
我使用的版本是：4.0.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
能不能在一个工程里面同时使用多份自定义词典，例如我有两个业务场景，场景一需要自定义词典1，场景二需要自定义词典2

"
“苹果”拼音问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： portable-1.6.6
我使用的版本是： portable-1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

中文 `苹果` 转拼音，期望输出 `pingguo`，但是结果却是 `pinguo`

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
    System.out.println(HanLP.convertToPinyinString(""苹果"", "" "", true));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
pingguo

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
pinguo

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
试过很多版本都是这样

"
人名识别出错，会就近匹配并识别人名,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用 `HanLP.newSegment()` 新起分词之后，分词会优先匹配第一个人名，实际上它并不是人名

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
val segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableCustomDictionary(true).enableNameRecognize(true).enableAllNamedEntityRecognize(true).
                        enableJapaneseNameRecognize(true).enableOrganizationRecognize(true).
                        enableTranslatedNameRecognize(true)
val termList = segment.seg(""师专班蔡玲思"")
println(termList)

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[师专班/nis 蔡玲思/nr]
```

### 实际输出
[师专/nis, 班蔡玲/nr, 思/v]
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[师专/nis, 班蔡玲/nr, 思/v]
```

我猜测的原因是，分词的时候，分词器将`班蔡玲`有限匹配了，并识别为人名。

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
词共现统计异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.6

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
public static void main(String[] args)
    {
        Occurrence occurrence = new Occurrence();
        String co = ""共有约300个参展商的近2000辆经典老爷车参展。 ""+
""1）上海加速金融业开放，确定六大方面先行先试，比如支持外国银行在沪同时设立分行和子行等。 ""+
""2）社科院：预计今年房价平稳回落，一二线城市或探底。 ""+
""3）第三届丝博会文化产业与资本创新融合发展分论坛在西安举行。 ""+
""4）灵活智能数据驱动 “新零售”引物流行业深度变革。 ""+
""5）昨日收盘： ""+
""沪指3192.12/+0.57%  ""+
""深成指10747.99/+0.72%  ""+
""创业板1858.01/+1.48%  ""+
""恒指31152.03/-1.23% ""+
""1）学生减负获得感未明显提升，人民日报： 不能头痛医头脚痛医脚。 ""+
""2）孔子故里曲阜举行传统礼仪活动，传承孝德文化。 ""+
""3）全国首个养生马拉松在亳州体育场开跑，2万名选手参加。 ""+
""4）女排新赛北仑起航，郎平确认朱婷暂时不会参赛。 ""+
""1）北京PM2.5来源本地排放占2/3，区域传输占1/3，本地排放中移动源占比高达45%。北京大兴增44家注册资本过亿元企业。北京市继续实行招生资格学校名单公示制度，371所高中有招生资格。 ""+
""2）上海将重塑南京路淮海路，绝代风华华丽转身。上海出口退税申报“免填报”，为企业减少逾2亿项重复录入项目。 ""+
""3）深圳核发自动驾驶道路测试牌照，须有驾驶员和安全员。广东—黑龙江“寒来暑往、南来北往”旅游季日前在广州开幕。智慧交通威力初现，广东出行方式出现全方位变化。 ""+
""4）湖南：锻造干部硬作风，调研不打招呼不要陪同，扶贫走访直奔基层直插现场。天津市委严肃问责市社会组织管理局不作为慢作为问题，市社管局原局长张宝甫被免职。 ""+
""5）陕西搭建文化交流平台，推动“文化陕西”品牌走向世界，河南南阳：连环计骗老人——层层设套，骗老人购买“特效药”。 ""+
""6）5月15日20时至16日20时，黑龙江、辽宁东部、华北东部和南部、黄淮大部、江淮北部等地有中到大雨，黑龙江中部、山东西北部和西南部、河南东部、苏皖北部等地的部分地区有暴雨，局地大暴雨 (100～110毫米)，上述地区并伴有短时强降水、雷暴大风等强对流天气。""+ 
""1）皮肤在晚10~11点进入保养状态，长时间熬夜，人的内分泌和神经系统就会失调，使皮肤干燥、弹性差、晦暗无光，出现暗疮、粉刺、黑斑等问题。 ""+
""2）普洱茶多为压紧型，第二泡时茶叶往往没有完全展开，所以精华要从第三泡开始，一般三到五泡是普洱茶口感最好的"";
        
       // String co = ""在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法，今天是星期天我很开心，希望下个星期天我依然很开心！"";
        occurrence.addAll(co);//**bug出现处**
        occurrence.compute();

        Set<Map.Entry<String, TermFrequency>> uniGram = occurrence.getUniGram();
        for (Map.Entry<String, TermFrequency> entry : uniGram)
        {
            TermFrequency termFrequency = entry.getValue();
            System.out.println(termFrequency);
        }

       /* Set<Map.Entry<String, PairFrequency>> biGram = occurrence.getBiGram();
        for (Map.Entry<String, PairFrequency> entry : biGram)
        {
            PairFrequency pairFrequency = entry.getValue();
            if (pairFrequency.isRight())
                System.out.println(pairFrequency);
        }

        Set<Map.Entry<String, TriaFrequency>> triGram = occurrence.getTriGram();
        for (Map.Entry<String, TriaFrequency> entry : triGram)
        {
            TriaFrequency triaFrequency = entry.getValue();
            if (triaFrequency.isRight())
                System.out.println(triaFrequency);
        }*/
    }

![image](https://user-images.githubusercontent.com/22112317/42936929-25bf8680-8b80-11e8-90d0-dc939f62fb41.png)

## 复现问题
见代码
"
WordNet类的insert方法报错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在执行WordNet类的insert方法报Exception in thread ""main"" java.util.ConcurrentModificationException

## 复现问题
执行Test中的DemoSentimentAnalysis的main方法即可，里面会调用WordNet的insert方法的。

## 其他
其实问题已经解决了，但是修改了源码。错误原因大概就是你一边迭代一边往list中添加数据
"
短语提取为什么要去停用词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.6.6
我使用的版本是：1.6.6

短语提取为什么要去停用词，如果不去停用词

这是一篇文章：

欢迎投稿 文章来源 作者 对新手来说 学习机器学习和深度学习是比较困难的 各种深度学习库也是比较难理解 所以 我创建了这个机器学习和深度学习速查表 希望对多家有帮助 热门文章推荐 马化腾 马云和李彦宏都错了 场景比数据和技术都重要 最新 谷歌董事长 我可以直接告诉你 互联网很快消失 斯坦福 卷积神经网络视觉识别课程讲义 独家 面对人工智能 下个像柯洁一样哭泣的可能就是你 最新 扎克伯格 如何在被抛弃和被否定中成就自己 震惊 禁止中国人参加 的第二轮比赛 重磅 揭秘 版本的技术设计和棋艺水平 重磅 谁让英伟达一夜损失 亿还留下一道思考题

短语提取后：
[机器学习深度, 深度学习, 卷积神经网络, 学习速查表, 李彦宏错, 柯洁哭泣, 神经网络视觉, 马云李彦宏, 一道思考题, 英伟达损失]

我不知道短语提取用的分词时哪种方法，但是我用segement分词后结果为

[欢迎/v, 投稿/vi, /w, 文章/n, 来源/n, /w, 作者/nnt, /w, 对/p, 新手/n, 来说/uls, /w, 学习/v, 机器学习/gi, 和/cc, 深度/n, 学习/v, 是/vshi, 比较/d, 困难/an, 的/ude1, /w, 各种/rz, 深度/n, 学习/v, 库/n, 也/d, 是/vshi, 比较/d, 难/a, 理解/v, /w, 所以/c, /w, 我/rr, 创建/v, 了/ule, 这个/rz, 机器学习/gi, 和/cc, 深度/n, 学习/v, 速查表/n, /w, 希望/v, 对/p, 多/a, 家/q, 有/vyou, 帮助/v, /w, 热门/a, 文章/n, 推荐/v, /w, 马化腾/nr, /w, 马云/nr, 和/cc, 李彦宏/nr, 都/d, 错/v, 了/ule, /w, 场景/n, 比/p, 数据/n, 和/cc, 技术/n, 都/d, 重要/a, /w, 最新/a, /w, 谷歌/ntc, 董事长/nnt, /w, 我/rr, 可以/v, 直接/ad, 告诉/v, 你/rr, /w, 互联网/n, 很快/d, 消失/vi, /w, 斯坦福/nrf, /w, 卷积/gm, 神经网络/nz, 视觉/n, 识别/vn, 课程/n, 讲义/n, /w, 独家/d, /w, 面对/v, 人工智能/n, /w, 下/f, 个/q, 像/v, 柯洁/nr, 一样/uyy, 哭泣/vi, 的/ude1, 可能/v, 就是/v, 你/rr, /w, 最新/a, /w, 扎克伯格/nrf, /w, 如何/ryv, 在/p, 被/pbei, 抛弃/v, 和/cc, 被/pbei, 否定/v, 中/f, 成就/n, 自己/rr, /w, 震惊/v, /w, 禁止/v, 中国/ns, 人/n, 参加/v, /w, 的/ude1, 第二/mq, 轮/qv, 比赛/vn, /w, 重磅/n, /w, 揭秘/v, /w, 版本/n, 的/ude1, 技术/n, 设计/vn, 和/cc, 棋艺/n, 水平/n, /w, 重磅/n, /w, 谁/ry, 让/v, 英伟达/nz, 一/m, 夜/t, 损失/n, /w, 亿/m, 还/d, 留下/v, 一道/d, 思考题/n]

“英伟达一夜损失”，”一”是去停用词去掉了，“夜”不知道怎么去掉的(这不是重点)，然后短语就是英伟达损失，如果说不去停用词的话，假设短语提取后不会出现“英伟达一夜”或者“一夜损失”或者“英伟达一夜损失"",因为我是希望用提取出的短语当做文章的关键词，所以后三个短语正好也是我不想要的，（之后我也想用短语当做字典重新对文章分词，然后进行后面的实验）
"
短语提取为什么要去停用词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

版本号
当前最新版本号是：1.6.6
我使用的版本是：1.6.6

短语提取为什么要去停用词，如果不去停用词

这是一篇文章：

欢迎投稿 文章来源 作者 对新手来说 学习机器学习和深度学习是比较困难的 各种深度学习库也是比较难理解 所以 我创建了这个机器学习和深度学习速查表 希望对多家有帮助 热门文章推荐 马化腾 马云和李彦宏都错了 场景比数据和技术都重要 最新 谷歌董事长 我可以直接告诉你 互联网很快消失 斯坦福 卷积神经网络视觉识别课程讲义 独家 面对人工智能 下个像柯洁一样哭泣的可能就是你 最新 扎克伯格 如何在被抛弃和被否定中成就自己 震惊 禁止中国人参加 的第二轮比赛 重磅 揭秘 版本的技术设计和棋艺水平 重磅 谁让英伟达一夜损失 亿还留下一道思考题

短语提取后：
[机器学习深度, 深度学习, 卷积神经网络, 学习速查表, 李彦宏错, 柯洁哭泣, 神经网络视觉, 马云李彦宏, 一道思考题, 英伟达损失]

我不知道短语提取用的分词时哪种方法，但是我用segement分词后结果为

[欢迎/v, 投稿/vi, /w, 文章/n, 来源/n, /w, 作者/nnt, /w, 对/p, 新手/n, 来说/uls, /w, 学习/v, 机器学习/gi, 和/cc, 深度/n, 学习/v, 是/vshi, 比较/d, 困难/an, 的/ude1, /w, 各种/rz, 深度/n, 学习/v, 库/n, 也/d, 是/vshi, 比较/d, 难/a, 理解/v, /w, 所以/c, /w, 我/rr, 创建/v, 了/ule, 这个/rz, 机器学习/gi, 和/cc, 深度/n, 学习/v, 速查表/n, /w, 希望/v, 对/p, 多/a, 家/q, 有/vyou, 帮助/v, /w, 热门/a, 文章/n, 推荐/v, /w, 马化腾/nr, /w, 马云/nr, 和/cc, 李彦宏/nr, 都/d, 错/v, 了/ule, /w, 场景/n, 比/p, 数据/n, 和/cc, 技术/n, 都/d, 重要/a, /w, 最新/a, /w, 谷歌/ntc, 董事长/nnt, /w, 我/rr, 可以/v, 直接/ad, 告诉/v, 你/rr, /w, 互联网/n, 很快/d, 消失/vi, /w, 斯坦福/nrf, /w, 卷积/gm, 神经网络/nz, 视觉/n, 识别/vn, 课程/n, 讲义/n, /w, 独家/d, /w, 面对/v, 人工智能/n, /w, 下/f, 个/q, 像/v, 柯洁/nr, 一样/uyy, 哭泣/vi, 的/ude1, 可能/v, 就是/v, 你/rr, /w, 最新/a, /w, 扎克伯格/nrf, /w, 如何/ryv, 在/p, 被/pbei, 抛弃/v, 和/cc, 被/pbei, 否定/v, 中/f, 成就/n, 自己/rr, /w, 震惊/v, /w, 禁止/v, 中国/ns, 人/n, 参加/v, /w, 的/ude1, 第二/mq, 轮/qv, 比赛/vn, /w, 重磅/n, /w, 揭秘/v, /w, 版本/n, 的/ude1, 技术/n, 设计/vn, 和/cc, 棋艺/n, 水平/n, /w, 重磅/n, /w, 谁/ry, 让/v, 英伟达/nz, 一/m, 夜/t, 损失/n, /w, 亿/m, 还/d, 留下/v, 一道/d, 思考题/n]

“英伟达一夜损失”，”一”是去停用词去掉了，“夜”不知道怎么去掉的(这不是重点)，然后短语就是英伟达损失，如果说不去停用词的话，假设短语提取后不会出现“英伟达一夜”或者“一夜损失”或者“英伟达一夜损失"",因为我是希望用提取出的短语当做文章的关键词，所以后三个短语正好也是我不想要的，（之后我也想用短语当做字典重新对文章分词，然后进行后面的实验）

"
关于能否客观统计出文章中各词语的出现的次数,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ √] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.6
当前最新版本号是：1.6.6
我使用的版本是：1.6.6


## 我的问题
如何客观统计出文章中各词语的出现的次数。
例如：今天是星期天我狠开心，希望下个星期天我也依然开心。
以上：星期天：2次；开心：2次；今天：1次 等...

## 复现问题
无

### 步骤
无

### 触发代码
无
### 期望输出
无
### 实际输出

无

## 其他信息
无


"
在短语提取时，是否已经自动过滤停用词，如果是自动过滤，那我在stopwords.txt添加新词时却没有过滤； 如果是手动过滤，我应该怎么调用。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

在短语提取时，是否已经自动过滤停用词，如果是自动过滤，那我在stopwords.txt添加新词时却没有过滤；
如果是手动过滤，我应该怎么调用。


"
识别类似“王寒炮轰张冰冰”结构的句子时结果与预期不同。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在对“王寒炮轰张冰冰”的类似的句子分词时，nlp，感知机和crf都会出现不同程度的误差现象。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 分别对“王寒炮轰张冰冰”进行nlp,感知机分词和crf分词
2.查看三种方式的分词结果


### 触发代码

```
    public void testIssue1234() throws Exception
    {
       String testContent = ""王寒炮轰张冰冰"";
       //三种分词的方法为常规代码，暂省略
        nlpTokenizerTest(testContent);
        perceptronTest(testContent);
        crfTest(testContent);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
王寒/nr，炮轰/v，张冰冰/nr
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
nlp分词结果：[王寒炮轰/nr, 张冰冰/nr]
感知机分词结果：王寒炮轰张冰冰/nr
crf分词结果：[王寒炮/nr, 轰张冰冰/nr]
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![qq 20180711153707](https://user-images.githubusercontent.com/18030444/42557212-724bfa4e-8520-11e8-955d-8bd29747e7e1.png)
"
分号词性标注不准确，期待推出语义角色分析和指代消岐、初步实体和关系抽取等功能,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：发行版 1.6.6
我使用的版本是：发行版 1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

分号词性标注不准确，期待推出语义角色和指代消岐等功能

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

输入以下句子进行词性标注：
“而变革者则提出解释理解世界的新方法：凯恩斯在对斯密进行修补；弗洛伊德另辟蹊径；毕加索挑战马蒂斯；爱因斯坦修订牛顿为大自然的立法；德鲁克对组织的研究，和他提出的“知识工人”与受雇阶层。”

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
       CoNLLSentence sentence =  HanLP.parseDependency(sentence0);

        CoNLLWord[] wordArray = sentence.getWordArray();
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

正确的词性标注

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
1	而	而	c	c	_	4	状中结构	_	_
2	变革者	变革者	n	n	_	4	主谓关系	_	_
3	则	则	d	d	_	4	状中结构	_	_
4	提出	提出	v	v	_	0	核心关系	_	_
5	解释	解释	v	v	_	10	定中关系	_	_
6	理解	理解	v	v	_	10	定中关系	_	_
7	世界	世界	n	n	_	6	动宾关系	_	_
8	的	的	u	u	_	6	右附加关系	_	_
9	新	新	a	a	_	10	定中关系	_	_
10	方法	方法	n	n	_	11	定中关系	_	_
11	：凯	：凯	nh	nr	_	12	定中关系	_	_
12	恩斯	恩斯	nh	nr	_	21	主谓关系	_	_
13	在	在	p	p	_	21	状中结构	_	_
14	对	对	p	p	_	16	状中结构	_	_
15	斯密	斯密	j	j	_	14	介宾关系	_	_
16	进行	进行	v	v	_	18	定中关系	_	_
17	修补	修补	v	vn	_	16	动宾关系	_	_
18	；	；	n	n	_	19	定中关系	_	_
19	弗洛伊德	弗洛伊德	nh	nr	_	13	介宾关系	_	_
20	另	另	d	d	_	21	状中结构	_	_
21	辟	辟	v	v	_	23	定中关系	_	_
22	蹊径	蹊径	n	n	_	21	动宾关系	_	_
23	；	；	v	v	_	25	主谓关系	_	_
24	毕加索	毕加索	Vg	Vg	_	25	主谓关系	_	_
25	挑战	挑战	v	v	_	4	动宾关系	_	_
26	马蒂斯	马蒂斯	nh	nr	_	27	定中关系	_	_
27	；爱因斯坦	；爱因斯坦	ns	ns	_	28	主谓关系	_	_
28	修订	修订	v	v	_	25	动宾关系	_	_
29	牛顿	牛顿	nh	nr	_	30	主谓关系	_	_
30	为	为	v	v	_	34	定中关系	_	_
31	大自然	大自然	n	n	_	30	动宾关系	_	_
32	的	的	u	u	_	30	右附加关系	_	_
33	立法	立法	v	vn	_	34	定中关系	_	_
34	；德鲁克	；德鲁克	n	n	_	38	定中关系	_	_
35	对	对	p	p	_	38	定中关系	_	_
36	组织	组织	v	v	_	38	定中关系	_	_
37	的	的	u	u	_	36	右附加关系	_	_
38	研究	研究	v	vn	_	28	动宾关系	_	_
39	，	，	wp	w	_	28	标点符号	_	_
40	和	和	c	c	_	42	左附加关系	_	_
41	他	他	r	r	_	42	主谓关系	_	_
42	提出	提出	v	v	_	46	定中关系	_	_
43	的	的	u	u	_	42	右附加关系	_	_
44	“	“	wp	w	_	46	标点符号	_	_
45	知识	知识	n	n	_	46	定中关系	_	_
46	工人	工人	n	n	_	28	并列关系	_	_
47	”	”	wp	w	_	46	标点符号	_	_
48	与	与	c	c	_	49	左附加关系	_	_
49	受雇	受雇	v	v	_	46	并列关系	_	_
50	阶层	阶层	n	n	_	49	动宾关系	_	_
51	。	。	wp	w	_	4	标点符号	_	_

而 --(状中结构)--> 提出
变革者 --(主谓关系)--> 提出
则 --(状中结构)--> 提出
提出 --(核心关系)--> ##核心##
解释 --(定中关系)--> 方法
理解 --(定中关系)--> 方法
世界 --(动宾关系)--> 理解
的 --(右附加关系)--> 理解
新 --(定中关系)--> 方法
方法 --(定中关系)--> ：凯
：凯 --(定中关系)--> 恩斯
恩斯 --(主谓关系)--> 辟
在 --(状中结构)--> 辟
对 --(状中结构)--> 进行
斯密 --(介宾关系)--> 对
进行 --(定中关系)--> ；
修补 --(动宾关系)--> 进行
； --(定中关系)--> 弗洛伊德
弗洛伊德 --(介宾关系)--> 在
另 --(状中结构)--> 辟
辟 --(定中关系)--> ；
蹊径 --(动宾关系)--> 辟
； --(主谓关系)--> 挑战
毕加索 --(主谓关系)--> 挑战
挑战 --(动宾关系)--> 提出
马蒂斯 --(定中关系)--> ；爱因斯坦
；爱因斯坦 --(主谓关系)--> 修订
修订 --(动宾关系)--> 挑战
牛顿 --(主谓关系)--> 为
为 --(定中关系)--> ；德鲁克
大自然 --(动宾关系)--> 为
的 --(右附加关系)--> 为
立法 --(定中关系)--> ；德鲁克
；德鲁克 --(定中关系)--> 研究
对 --(定中关系)--> 研究
组织 --(定中关系)--> 研究
的 --(右附加关系)--> 组织
研究 --(动宾关系)--> 修订
， --(标点符号)--> 修订
和 --(左附加关系)--> 提出
他 --(主谓关系)--> 提出
提出 --(定中关系)--> 工人
的 --(右附加关系)--> 提出
“ --(标点符号)--> 工人
知识 --(定中关系)--> 工人
工人 --(并列关系)--> 修订
” --(标点符号)--> 工人
与 --(左附加关系)--> 受雇
受雇 --(并列关系)--> 工人
阶层 --(动宾关系)--> 受雇
。 --(标点符号)--> 提出

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

谢谢！！"
关于人名UV拆分优化建议（按格式重提）,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
例1：龚学平等表示会保证金云鹏的安全
例2：王中军代表蓝队发言
此两种情况，人名识别错误。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

### 触发代码

PersonDictionary的parsePattern函数，在进行拆分时，遗漏词语。
如例1 的语句，""保证金""，当识别出人名金云、金云鹏时，需要将“保证”添加到细分词图中。同理例2中的“军代表”也需要做相应处理。

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
例1：龚学平等表示会保证金云鹏的安全
[龚学平/nr, 等/udeng, 表示/v, 会/v, 保证/v, 金云鹏/nr, 的/ude1, 安全/an]

例2：王中军代表蓝队发言，分词结果为
[王中军/nr, 代表/nnt, 蓝队/n, 发言/vi]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[龚学平/nr, 等/udeng, 表示/v, 会/v, 保证金/n, 云/vg, 鹏/ng, 的/ude1, 安全/an]
[王中/nr, 军代表/nnt, 蓝队/n, 发言/vi]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词典的最长匹配,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
Hanlp的自定义词典功能，当自定义词典中的词较多时，可能会出现较长字符包含较短字符的情况，比如自定义词典中可能同时含有“增”以及“急增""两个词条，那么在对“盈利急增25亿元”进行分词时，会分出“盈利/vn，急/v，增/v，25/m，亿元/q”的结果，我想这可能和Trie树以及自动机的算法有关，现在我想实现自定义词条的最大匹配，我看到CustomDictionary类中有parseLongestText的方法，不过不知道其参数的含义和用法，不知道这个方法能否实现自定义词典最大匹配的功能。或者Hanlp的内部有没有实现类似功能的接口呢？如果没有的话能否指点下实现分词功能的相关代码的位置？谢谢
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
### 步骤

1. 首先在自定义词典中添加“增”和“急增”两个词条
2. 然后删除原来的bin文件，重新训练缓存文件
3. 接着利用N最短路分词，开启了自定义分词，地名识别，机构识别，并开启了自定义词典强制匹配
4.对“盈利急增25亿元”进行分词，会分出“盈利/vn，急/v，增/v，25/m，亿元/q”的结果

### 触发代码

```
    public void testIssue1234() throws Exception
    {
           String str = ""盈利急增25亿元"";          
           Segment nShortSegment = new 
           NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true).enableOffset(true);
           nShortSegment.enableCustomDictionaryForcing(true);
           List<Term> termList = nShortSegment.seg(str);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
盈利 急增 25 亿元
### 实际输出
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
盈利 急 增 25 亿元
没有实现自定义词典的最大匹配

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
冒号的识别貌似不太精准，另外凯恩斯也被识别为凯+恩斯了,"![b ab6_sl08h1mm kn9 vfz4](https://user-images.githubusercontent.com/32017215/42416259-6c3c8f82-829c-11e8-98e5-3076ddc4a0cd.png)
"
将特殊符号和标点符号作为停用词过滤,"不想要标点符号和特殊符号，好像没有这一配置？
如果没有，是否能增加这一feature？"
"HanLP.Config.Normalization = true 时，中文的“、”顿号与“，”逗号都切分成了“,”","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4 非portable版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
当 HanLP.Config.Normalization = true 时，默认分词将中文的“、”顿号与“，”逗号切分结果相同。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   HanLP.Config.Normalization = true;
   System.out.println(HanLP.segment(""为什么，号与、号结果一样？""));
   // 输出为：[为什么/ryv, ,/w, 号/q, 与/cc, ,/w, 号/q, 结果/n, 一样/uyy, ?/w]
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
关于实体识别方面的数据集,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4

我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

感谢分享！
请问您使用的实体识别和角色语义标注的数据集是哪些 个人研究者苦于没有数据可用。
希望能分享您使用的数据，如有后续开发需要可协助一起完成，万分感谢！

"
DemoNShortSegment数组越界异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.5
我使用的版本是：portable-1.6.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
直接checkout工程，运行test/java/com/hankcs/demo/DemoNShortSegment时，报ArrayIndexOutOfBoundsException。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  public class DemoNShortSegment
{
    public static void main(String[] args)
    {
        Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        Segment shortestSegment = new ViterbiSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        String[] testCase = new String[]{
                ""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。"",
                ""江西省监狱管理局与中国太平洋财产保险股份有限公司南昌中心支公司保险合同纠纷案"",
                ""新北商贸有限公司"",
        };
        for (String sentence : testCase)
        {
            System.out.println(""N-最短分词："" + nShortSegment.seg(sentence) + ""\n最短路分词："" + shortestSegment.seg(sentence));
        }
    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 40
	at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:216)
	at com.hankcs.hanlp.seg.common.WordNet.insert(WordNet.java:168)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3779)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3756)
	at com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie.parseText(AhoCorasickDoubleArrayTrie.java:115)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3755)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.recognition(OrganizationRecognition.java:71)
	at com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence(NShortSegment.java:79)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at com.hankcs.demo.DemoNShortSegment.main(DemoNShortSegment.java:36)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
希望線上演示的浮動說明能夠增加顯示該詞性的中文說明,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

http://hanlp.hankcs.com/

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

希望線上演示的浮動說明能夠增加顯示該詞性的中文說明(記性太差容易忘記英文詞性代表意思)

順便問下 線上演示的倉庫是哪一個 lol

似乎不在此倉庫內

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
""謎样""
n
名词
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

![2018-06-26-02-01-59-2](https://user-images.githubusercontent.com/167966/41867392-9b8e543e-78e5-11e8-99ad-dcc47b7edfb0.png)


"
Bug Hanlp.segment() 分词报错，java.lang.ArrayIndexOutOfBoundsException: 148,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：hanlp-1.6.4.jar、hanlp-1.6.4-sources.jar、hanlp.properties、data 版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
先说现象：
1. 在以代码的方式自定义词典加入一些词后，如`CustomDictionary.insert(""地产业或某个词"", ""Industry 1"");`后，单独运行测试`HanLP.segment(“任意内容”)`时，切分正常且都不会出现问题。
2. 但经过一系列的数据预处理以及连续的`CustomDictionary.insert(""某词"", ""Industry 1"");`处理后，再进行`HanLP.segment(“某些内容”)`时就会报错，错如：
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:106)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:90)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:189)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:325)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:227)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:601)
3. 报错后再 debug 时，加入的断点可以越过，也就是又不报错了，可以执行完，但不 debug 执行就会报错。
4. 反复执行后发现偶尔有时候可以运行成功，且不报错，但多数时候是会报错的。
5. 将`CustomDictionary.insert(""某词"", ""Industry 1"");`改为`CustomDictionary.insert(""某词"");`后，去掉词性和词频，就不会报错了。
==============以上是一类错误================
==============以下是一类错误================
1. 在利用 Spring boot 包装 HanLP 成为 service 的时候，加载不同的词库，且`CustomDictionary.insert(""各个词"", ""相应词性 1"");`后，切分效果不正确，尽管单独测试是正确的，但service中确实不正确，如果改为`CustomDictionary.insert(""各个词"");`后，切分效果正确。
==============综合上述两种情况的错误================
个人考虑为可能是同一个 BUG 引起的，于是代码只复现第一类错误，第二类也不好复现。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    package com.gemantic.Issues;

import com.google.common.collect.Lists;
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.dictionary.CustomDictionary;
import com.hankcs.hanlp.seg.common.Term;
import org.apache.commons.io.FileUtils;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Issue {

    public static void main(String[] args) throws IOException {
        File industryFile = new File(""data/issue.txt"");
        List<String> industries = FileUtils.readLines(industryFile, ""UTF-8"");

        List<String> allIndustryPrefixes = Lists.newArrayList();
        allIndustryPrefixes.add(""其他"");

        List<String> allIndustryPostfixes = Lists.newArrayList();
        allIndustryPostfixes.add(""行业"");

        // step1、仅仅查看了一下 industries 的分词效果，并没有做其他处理
        List<String> industriesLooked = afterLooked(industries);
        System.out.println(industries.size());
        System.out.println(industriesLooked.size());

        // step2、逐步对 industries 做了一些判断，且逐步的加入了 HanLP 的 Customdictionary 中
        List<String> smoothSegmentCustomDictionary = getSmoothSegmentCustomDictionary(industriesLooked, allIndustryPrefixes, allIndustryPostfixes);
        System.out.println(smoothSegmentCustomDictionary.size());
    }


    public static List<String> getSmoothSegmentCustomDictionary(List<String> allIndustries, List<String> prefixes, List<String> postfixes){

        // 细粒度分词词典 CustomDictionary
        List<String> smoothSegmentCustomDictionary = Lists.newArrayList();

        // 目的是分词能够将""前缀""与""后缀""分出来
        // 添加""前缀""
        for(String prefix : prefixes){
            if (!CoreDictionary.contains(prefix)) {
                CustomDictionary.insert(prefix);
            }
        }
        // 添加""后缀""
        for(String postfix : postfixes){
            if (!CoreDictionary.contains(postfix)) {
                CustomDictionary.insert(postfix);
            }
        }

        // 添加""特殊词""

        CustomDictionary.insert(""地产业"", ""Industry 1"");

        smoothSegmentCustomDictionary.add(""地产业"");


        // 避免重复处理 与 区分 第一轮的英文数字
        List<String> industryProcessed = new ArrayList<>();

        // 处理全英文及英文数字词
        // 中英文不需处理，同其他情况是同一种方式
        for (String industry : allIndustries){

            if (!industryProcessed.contains(industry)){
                Boolean x = industry.matches(""[0-9a-zA-Z]*"");
                if (x == Boolean.TRUE){
                    CustomDictionary.insert(industry);
                    smoothSegmentCustomDictionary.add(industry);
                    industryProcessed.add(industry);
                    System.out.println(industry);
                }
            }
        }

        // 处理1、2字词，认为1、2字词为最基本颗粒，不可再切分
        for (String industry : allIndustries){

            if (!industryProcessed.contains(industry)){
                if (industry.length() == 1 || industry.length() == 2){
                    if (!CoreDictionary.contains(industry)){
                        CustomDictionary.insert(industry);
                        smoothSegmentCustomDictionary.add(industry);
                        System.out.println(industry);
                    }
                }
            }
        }

        // 处理3字及以上词
        int max_length = 50;

        for (int length = 3 ;length < max_length; length ++){

            for (String industry : allIndustries){

                if (!industryProcessed.contains(industry)){

                    if (industry.length() == length){

                        List<Term> segmentsOfIndustry = HanLP.segment(industry);

                        if (segmentsOfIndustry.size() == 1){
                            if (!CoreDictionary.contains(industry)){
                                CustomDictionary.insert(industry);
                                smoothSegmentCustomDictionary.add(industry);
                                System.out.println(industry);
                            }
                        }

                        if (segmentsOfIndustry.size() > 1){
                            // 需要判断该词是否真的可拆分

                            // 获取该行业概念的分词列表
                            List<String> words = new ArrayList<>();
                            for (Term segment : segmentsOfIndustry){
                                words.add(segment.word);
                            }

                            // 可拆分条件
                            int condition = 0;
                            for (String industryPrefix : prefixes){
                                if (words.contains(industryPrefix)){
                                    condition = condition + 1;
                                }
                            }
                            for (String industryPostfix : postfixes){
                                if (words.contains(industryPostfix)){
                                    condition = condition + 1;
                                }
                            }
                            for (String word : words){
                                for (String industryTemp : allIndustries){
                                    if (industryTemp.equals(word)){
                                        condition = condition + 1;
                                    }
                                }
                            }

                            // 认为是不可拆分的词，应该作为整词加入 CustomDictionary
                            if (condition == 0 && !CoreDictionary.contains(industry)){
                                CustomDictionary.insert(industry);
                                smoothSegmentCustomDictionary.add(industry);
//                                if(industry.equals(""大军工"")){
//                                    System.out.println(condition);
//                                    System.out.println(industry);
//                                    System.out.println(words);
//                                }
                                System.out.println(industry);
                            }
                        }

                    }
                }
            }
        }
        // ======  细粒度分词处理完毕 =========
        return smoothSegmentCustomDictionary;
    }



    public static List<String> afterLooked(List<String> allIndustries) {

        // 只看了一眼分词后的新词典
        List<String> industriesLooked = Lists.newArrayList();

        for (String industry : allIndustries){

            // 只是分词看一下
            List<Term> segmentsOfIndustry = HanLP.segment(industry);
            List<String> segments = Lists.newArrayList();
            for(Term term : segmentsOfIndustry){
                segments.add(term.word);
            }
            System.out.println(segments);

            industriesLooked.add(industry);

        }

        return industriesLooked;
    }
}

```
```
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:106)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:90)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:189)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:325)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:227)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:601)
	at com.gemantic.Issues.Issue.getSmoothSegmentCustomDictionary(Issue.java:107)
	at com.gemantic.Issues.Issue.main(Issue.java:33)
```
```
data/issue.txt
云
奶
床
店
水
浆
炭
煤
车
酒
铬
铽
锂
锆
锌
锑
锗
锡
镁
镍
镝
镨
鸡
3c
4g
5g
ah
ai
ar
a股
b股
h股
ic
it
mr
s股
vr
三板
专利
专科
专网
丙烯
丙烷
两会
两融
中介
中药
临港
乙烯
乙烷
乙醇
乡镇
乳业
乳品
乳粉
二胎
二萜
云端
井台
交运
交通
京津
仓储
代工
代购
仪表
休闲
传媒
低位
低压
低碳
住宅
体彩
体育
供应
供暖
供电
保健
保荐
保险
信息
信托
信用
信贷
借贷
健身
储气
储能
元件
光伏
光学
光棒
光纤
光通
全屋
全息
公会
公安
公寓
公路
关节
兵器
养殖
养老
内酯
内镜
军事
军品
军工
军机
军民
农业
农化
农垦
农机
农村
农牧
农药
冰柜
冰箱
冷链
分销
创伤
创投
制剂
制糖
制药
券商
剧集
办公
动保
动态
动漫
动车
勘探
包装
化工
化纤
化肥
北斗
区域
医保
医改
医疗
医药
医院
单抗
单晶
博彩
卡车
卫星
卫浴
卫通
印染
即期
卷烟
厂库
压裂
原奶
原料
原油
原糖
原药
厨卫
厨柜
厨电
县乡
叉车
发电
古井
叶片
吊顶
同城
名表
名酒
吡啶
味精
售电
商旅
商贸
啤酒
器件
器具
圆环苗
地产业
```

### 期望输出
正常来说，不论 `CustomDictionary.insert(""任意词"", ""相应词性 1"");`后，HanLP.segment() 应该都会正常切分，最多不过是切分效果不理想而已，但现在为什么会报 HanLP.segment() 中 Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148 呢？
请多指教！不胜感谢！
<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
JDK9 支持,"在 `EnumBuster` 中用到了包 `sun.reflect`，在 JDK9 及 以后的 JDK 版本中，已经变成了 `jdk.internal.reflect`  。可以考虑另外实现，避免内部包的引入

<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

"
更改 CoreNatureDictionary.txt 且删除所有 bin 文件后，未能生效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：hanlp-1.6.4.jar、hanlp-1.6.4-sources.jar、hanlp.properties、data 版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1. 配置好 1.6.4 非 portable 版后，可以分词且效果正常，但在 CoreNatureDictionary.txt 中按着同等格式加入`大家电 n 1000`后，执行`CoreDictionary.contains(""大家电"")`结果为`false`。
2. data 文件夹中的 .bin .tr.txt 都删掉了，同样未发现有任何变化，且在`hanlp.properties`中发现，词典地址也没有用到这些如`CoreNatureDictionary.txt.bin`的文件，那么其存在的作用是什么呢？HanLP首页说是为了加速加载词典速度，但没有发现哪里用到这些 .bin 文件里呀？
3. 把 .bin 文件删掉后，重新运行分词程序，.bin 文件就不会再出现了是吗？我这 .bin 文件未再出现。 
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.dictionary.CustomDictionary;

public class TempTest {
    public static void main(String[] args) {
        System.out.println(CoreDictionary.contains(""大家电""));
    }
}

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
true
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
false
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
韩正副总理识别出错，总是识别为“正副”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ X] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

测试句子：认真落实李克强总理、韩正副总理重要批示指示精神，
总是识别为：韩 正副 总理

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    String text = ""认真落实李克强总理、韩正副总理重要批示指示精神，"";
    HanLP.newSegment().enableNameRecognize(true)
    CustomDictionary.insert(""韩正"", ""nr 9999"")
    System.out.println(HanLP.segment(text));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[认真/ad, 落实/v, 李克强/nr, 总理/nnt, 、/w, 韩正/nr, 副/b, 总理/nnt, 重要批示/n, 指示精神/nz, ，/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[认真/ad, 落实/v, 李克强/nr, 总理/nnt, 、/w, 韩/b, 正副/b, 总理/nnt, 重要批示/n, 指示精神/nz, ，/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Merge pull request #1 from hankcs/master,"再次同步

同步库代码

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
如何利用不同的 CustomDictionary 进行 HanLP.segment(),"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：hanlp-1.6.4.jar、hanlp-1.6.4-sources.jar、hanlp.properties、data 版

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
假设有两个用户自定义词典，CustomDictionary1.txt 与 CustomDictionary2.txt，如何在 HanLP.segment() 的基础上写一个分词函数，通过传入参数来控制分词的效果？
```
如：
Segment segment = HanLP.newSegment();
Segment segment1 = HanLP.newSegment();

HanLP.Config.Normalization = true;
segment.enableOffset(true);
segment1.enableOffset(true);

int availProcessors = Runtime.getRuntime().availableProcessors();
segment.enableMultithreading(availProcessors);
segment1.enableMultithreading(availProcessors);

组合 segment 与 segment1，并且他们的区别是 CustomDictionary 不同：
一个是 
CustomDictionary.insert(""雄安概念产业链"", ""n 1"");
另一个是
 CustomDictionary.insert(""雄安"", ""n 1"");
CustomDictionary.insert(""概念"", ""n 1"");
CustomDictionary.insert(""产业链"", ""n 1"");

但怎么把 CustomDictionary 当成参数传到不同的 segment 中呢？或是说怎么创建不同的 CustomDictionary 从而让 不同的 segment 依赖不同的 CustomDictionary 呢？
```
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出
```
雄安概念产业链 = CombineHanLP(parameter = true, normalization, offset, multithreading)
雄安/概念产/业链 = CombineHanLP(parameter = false, Normalization, offset, multithreading)
```

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
"
使用感知机识别机构实体，在线学习有的时候无效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机识别机构实体，在线学习有的时候无效，不知道什么原因，标注不对吗？

### 触发代码

```
        String s = ""就读于家乡江苏省镇江市省立师范学校初中二年级"";
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.learn(""[江苏省/ns 镇江市/ns 省立师范学校/n]/nt""));
        System.out.println(analyzer.seg(s));
```
### 期望输出
```
true
[就读/v, 于/p, 家乡/n, 江苏省镇江市省立师范学校/nt, 初中/n, 二/m, 年级/n]
```

### 实际输出
```
true
[就读/v, 于/p, 家乡/n, 江苏省/ns, 镇江市/ns, 省立/v, 师范学校/l, 初中/n, 二/m, 年级/n]
```

## 其他信息


"
感知机机构识别，如何对错误识别进行干预？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机进行机构识别，总体比较准确，但是很多识别到的机构是错误的，需要排除，如何利用在线学习进行人工干预呢？
"
一个代码的小问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在阅读 `DoubleArrayTrie` 源码的时候在第393行（`build` 方法)内发现一个很小的问题：

```
    if (_keySize > _key.size() || _key == null)
            return 0;
```
 这里的 `_key` 是先被使用然后再判断是否为空，当 `_key` 为空时，是没法执行到 `_key == null` 这一条件的吧


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤



### 触发代码


### 期望输出



### 实际输出



## 其他信息

"
词典优化,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

开启Normalization=true以后，
“薰衣草”分词结果会变成：“熏衣草”，这是不正确的
（代码的commit 信息中写错了是去掉 “薰=熏”不是“薰=香熏”）

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
CRF分词，数词和标点符号都被识别成nz,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4 
我使用的版本是：1.6.4 jar版本 master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
```
        String name1 = ""您好,帮我看一下电影院几点开门"";
        System.out.println(""标准分词：""+ HanLP.segment(name1));
        final Segment segment1 = new CRFSegment();
        List<Term> termList1 = segment1.seg(name1);
        System.out.println(""CRFSegment:""+termList1);
```
log:
标准分词：[您好/n, ,/w, 帮/v, 我/rr, 看/v, 一下/m, 电影院/nis, 几/d, 点/qt, 开门/vi]
CRFSegment:[您好/n, ,/nz, 帮/v, 我/rr, 看一下/nz, 电影院/nis, 几点/nz, 开门/vi]

-- “看一下”和“几点”，以及标点符号都被识别成nz，有办法解决这个问题吗？


"
使用自定义的标签为每个字进行标注，除了重新定义模型文件，还需要其他什么工作？,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

## 我的问题

使用自定义的标签为每个字进行标注，除了重新定义模型文件，还需要其他什么工作？比如我想得到如下的结果：
患    Bx
者    Ix
，	D
56	Ba
岁	Ia
，    D
血	Bi
压	Ii
不	Bis
稳	Iis
定	Iis

这个时候除要定义新的模型文件之外，代码部分还需要修改哪些地方？期待回复，谢谢！

"
使用python调用hanlp进行动态加载词典问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用python动态加载词典时，报出异常：RuntimeError: No matching overloads found for get in find. at native/common/jp_method.cpp:127

google查了下解决方法：https://stackoverflow.com/questions/7797406/jpype-and-java-util-properties
但依旧没有解决

也许不是hanlp的问题，但实在找不到解决方法了。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 步骤

1. 首先查看文档，通过python调用hanlp
2. 然后运行
3. 接着……报错了.....

### 触发代码

```
userwords=['java', 'java工程师', '数据挖掘工程师']
startJVM(getDefaultJVMPath(
), ""-Djava.class.path=/Users/alanlau/Workplace/MacInstallations/Hanlp/hanlp-1.5.3.jar:/Users/alanlau/Workplace/MacInstallations/Hanlp"",
         ""-Xms1g"", ""-Xmx1g"")
CustomDictionary = JClass('com.hankcs.hanlp.dictionary.CustomDictionary')
for userword in userwords:
    CustomDictionary.insert(userword, 'nski 10'))   #这一行报错
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
暂无期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Traceback (most recent call last):
  File ""transformat1.py"", line 8, in <module>
    from cleaner import slices_text
  File ""/Users/alanlau/Workplace/Bello/projlstm/transformat/cleaner.py"", line 2, in <module>
    from utils import segtools
  File ""/Users/alanlau/Workplace/Bello/projlstm/transformat/utils/segtools.py"", line 14, in <module>
    CustomDictionary.insert(userword, 'nski ' + str(len(userword) + 100))
RuntimeError: No matching overloads found for insert in find. at native/common/jp_method.cpp:127
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

操作系统：MacOS 10.13.4

java version ""10.0.1"" 2018-04-17
Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)

Python 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06) 
"
分词结果去除字符正则化变换,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

感知机分词器对分词结果的字符正则化问题

## 相关issue

https://github.com/hankcs/HanLP/issues/844


"
感知机分词器对分词结果的字符正则化问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
`AbstractLexicalAnalyzer.analyze()`方法，当没有加载ner.bin模型（`neRecognizer == null`）时，返回的分词结果是字符正则化后的形式，与输入原文不一致。

从代码中的行为来看，`AbstractLexicalAnalyzer.analyze()`中的分词过程会首先进行字符正则化，`segmenter`、`posTagger`和`neRecognizer`直接处理的都是经字符正则化的数据。

但是在输出分词结果的时候，只有执行了`neRecognizer`的逻辑分支里将未正则化的词写回分词结果数组：

```
// AbstractLexicalAnalyzer.java Line 190
// if (neRecognizer != null) 分支
String[] nerArray = neRecognizer.recognize(wordArray, posArray);
wordList.toArray(wordArray);
```

其他情况的返回值（Line 219 - Line 233）下都没有执行这个操作，返回的wordArray中的词是被字符正则化处理后的形式。


## 复现问题

### 触发代码

```
    public static void main(String[] args) throws Exception {
        // 加载 nerModel 的实例
        PerceptronLexicalAnalyzer analyzer1 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        // 未加载 nerModel 的实例
        PerceptronLexicalAnalyzer  analyzer2 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath);

        // 输出分词结果
        System.out.println(analyzer1.analyze(""上海自来水来自海上，黄山落叶松叶落山黄。""));
        System.out.println(analyzer2.analyze(""上海自来水来自海上，黄山落叶松叶落山黄。""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
```

### 实际输出

第二行的逗号是正则化后的，没有转换回来。

```
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
上海/ns 自来水/n 来自/v 海上/s ,/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
```
"
正则化增加处理各种类型的空格，特别是html中的空格,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
把各种类型的空格转为半角空格
## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
Merge pull request #1 from hankcs/master,"update fork

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
发布的正则化字典CharTable.txt和master分支不一致的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我发现master分支的CharTable.txt和下载的 data-for-1.6.2.zip里面的不一致，希望 data-for-1.6.2.zip能够升级一下，把最新的字典等修改同步一下。


"
"分词结果中名词性语素的词性标识“Ng”同Nature枚举类中的标识“ng""不一致","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机分词器（`PerceptronLexicalAnalyzer`）的分词结果中，名词性语素的词性标识输出是""Ng""，但是Nature枚举类中对应的是Nature.ng，对应字符串""ng""。这会输出一行警告：
```
五月 23, 2018 5:06:34 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
```

## 复现问题
```
String s = ""个人公积金如何办理？"";
Segment segment = new PerceptronLexicalAnalyzer();
segment.seg(s);
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
个人/n 公积/n 金/ng 如何/r 办理/v ？/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
个人/n 公积/n 金/Ng 如何/r 办理/v ？/w
```

## 其他信息

Nature.ng -> 名词性语素

"
词典优化：删除橙子和橘子的不合理的繁简转换,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

词典优化：删除“橙子”和“橘子”的不合理的繁简转换

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->
#835 

"
PersonRecognition的Recognition方法在1.6.4版本没有重构方法名，是不是遗漏了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
![image](https://user-images.githubusercontent.com/23119744/40400951-f3762d72-5e76-11e8-94fe-7249fbdc863a.png)



"
繁体简体转换词典问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
dictionary/other/CharTable.txt 中有一行
橙=橘

由此产生的问题就是如果开启了Normalization=true （我使用的场景中开启这个参数的主要目的是大小写统一）
“橙子”分词以后会产生的结果是“橘子”，但是这两种水果并不是相同的水果。

另外 dictionary/tc/t2s.txt 中有一个词
橙=橘子
不太确定这个词典的用途是不是繁体到简体的转换对应，如果是的话，感觉不太对，“橙”并不等于简体的“橘子”

## 复现问题


### 触发代码
配置文件中加入：Normalization=true

HanLP.segment(""橙子"")

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

橙子


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

橘子


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
word2vec 训练工具参数文档描述不准确,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

项目中提供的word2vec 训练工具，文档中关于-hs 和-cbow 的参数实际上必须要有数值参数（1或其它），否则运行时会报异常：
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
wiki 中关于训练工具的参数Examples上也有同样的问题。
看了一下代码AbstractTrainer.java  中的setConfig函数中解析参数，-cbow 后面必须跟上1才会使用 cbow 模型。
if ((i = argPos(""-cbow"", args)) >= 0) config.setUseContinuousBagOfWords(Integer.parseInt(args[i + 1]) == 1);


## 复现问题
java -cp  hanlp-1.6.3.jar com.hankcs.hanlp.mining.word2vec.Train -input input.txt -output output.txt  -cbow

### 期望输出
开始训练

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:50)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:40)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.setConfig(AbstractTrainer.java:62)
	at com.hankcs.hanlp.mining.word2vec.Train.execute(Train.java:24)
	at com.hankcs.hanlp.mining.word2vec.Train.main(Train.java:38)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
触发异常  java.lang.AssertionError: 构造空白节点会导致死循环！,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

切分以下句子报错
```
最大唐感女声双笙本人真正的照片 双笙子是谁双笙的真名叫什么
```


### 触发代码

```
   @Test
    public void errTest(){
        String eStr = ""最大唐感女声双笙本人真正的照片 双笙子是谁双笙的真名叫什么"";
        Segment seg = HanLP.newSegment().enableMultithreading(8);
        System.out.println(seg.seg(eStr));
    }
```


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
java.lang.AssertionError: 构造空白节点会导致死循环！

	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:214)
	at com.hankcs.hanlp.dictionary.nr.PersonDictionary.parsePattern(PersonDictionary.java:109)
	at com.hankcs.hanlp.recognition.nr.PersonRecognition.Recognition(PersonRecognition.java:67)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:78)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at hanlp.CutWord.errTest(CutWord.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
wiki 中word2vec章节提供的维基百科的预训练数据的训练参数,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

请问wiki 中word2vec章节提供的 维基百科的[预训练数据](https://pan.baidu.com/s/1qYFozrY) 的训练参数是什么？

"
自定义词典识别失败,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [钩] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
自定义词典读取失败

## 复现问题



### 步骤

1.首先 ：在custom文件夹下建立了一个新的my.txt 内容的编码格式是utf-8 ，内容是 攻城狮 nz 1024

2.然后：在hanlp.properties中 添加
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; my.txt;现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf

3.删除customdictionary.txt.bin     但是并没有生成新的

4.开启识别 读取失败

### 触发代码
HanLP.segment(""攻城狮是一个昌平北七家镇平西王府村出"");

期望输出
攻城狮应该是一个整体 ，因为我在自定义词典中添加了

实际输出：
以下是我的log
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CoreDictionary load
信息: 核心词典开始加载:/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.txt
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
信息: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.txt加载成功，153083个词条，耗时59ms
粗分词网：
0:[ ]
1:[攻, 攻城]
2:[城]
3:[狮]
4:[是]
5:[一个]
6:[个]
7:[昌, 昌平]
8:[平]
9:[北]
10:[七家镇]
11:[家]
12:[镇]
13:[平]
14:[西, 西王]
15:[王, 王府, 王府村]
16:[府]
17:[村]
18:[出]
19:[ ]

五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
信息: 开始加载二元词典/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.ngram.txt.table
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
信息: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.ngram.txt.table加载成功，耗时130ms
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
信息: 自定义词典开始加载:/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/custom/CustomDictionary.txt
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.CustomDictionary <clinit>
信息: 自定义词典加载成功:391944个词条，耗时384ms
粗分结果[攻城/vi, 狮/ng, 是/vshi, 一个/mq, 昌平/ns, 北七家镇/ns, 平西/nrf, 王府村/ns, 出/vf]
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
信息: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/person/nr.txt加载成功，耗时33ms
人名角色观察：[  K 1 A 1 ][攻城 L 1 ][狮 D 17 E 5 C 1 ][是 K 2507 L 2504 M 123 C 10 E 1 ][一个 K 90 L 84 ][昌平 Z 41 ][北七家镇 A 20833310 ][平西 A 20833310 ][王府村 A 20833310 ][出 K 168 L 68 C 1 ][  K 1 A 1 ]
人名角色标注：[ /K ,攻城/L ,狮/D ,是/L ,一个/K ,昌平/Z ,北七家镇/A ,平西/A ,王府村/A ,出/K , /K]
五月 16, 2018 5:02:42 下午 com.hankcs.hanlp.dictionary.nr.TranslatedPersonDictionary <clinit>
信息: 音译人名词典/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/person/nrf.txt加载成功，耗时26ms


以下打印结果为空
System.out.println(""我是""+LexiconUtility.getAttribute(""攻城狮""));
"
"word ""若果"" includes a unexpected space","<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
I find a small problem in stopwords.txt. I guess ""若果"" is right, and ""若果 "" may be error.

"
新词发现里空字符串的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

新词发现里这个[地方](https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/mining/word/NewWordDiscover.java#L64)附近有替换分割符为空串的步骤，在pyhanlp里得到的结果为 `哈哈\x00` 之类，这里是否应该是`""""`（空串，长度为0）而非`""\0""`（0号字符，长度为1）?

"
运行demo时发现提供的词典不完整,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP-1.6.3
我使用的版本是：HanLP-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在运行源码中的test类时发现有些词典是在data/test/目录下的，但是我下载下来的data包中不包含有这部分词典，请问在哪里可以获取到这些词典？



"
关于word2vec加载词向量的疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

加载词向量时发生数组越界。
```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at word2vec.TestWordVec.main(TestWordVec.java:8)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 300
	at com.hankcs.hanlp.mining.word2vec.VectorsReader.readVectorFile(VectorsReader.java:50)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:38)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
	at com.hrtps.qa.tool.WordVecAPI.<clinit>(WordVecAPI.java:23)
	... 1 more
```
## 复现问题
查看了源码的 VectorsReader类
发现加载文本词向量时对字符串进行了 trim 操作，是否存在将不可见词项处理掉的情况  导致向量无法正常读取呢？


### 触发代码

```java
WordVectorModel wordVectorModel = new WordVectorModel(""msr.txt"");
```


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于感知机在线学习的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我使用了感知机的在线学习功能，但是没有达到我要的效果。请看一下下面代码
### 触发代码

                  PerceptronSegmenter segmenter = new
		  PerceptronSegmenter(Config.CRFCWSModelPath);
		  segmenter.learn(""下雨天 地面 积水"");
		  System.out.println(segmenter.segment(""下雨天地面积水""));
                  这样可以正常输出：[下雨天, 地面, 积水]
                 但是如果这样：
                 PerceptronLexicalAnalyzer segmenter1 = new
		  PerceptronLexicalAnalyzer(Config.CRFCWSModelPath,
		  Config.CRFPOSModelPath, Config.CRFNERModelPath);
		  Sentence sentence = segmenter1.analyze(""下雨天地面积水"");
		  System.out.println(sentence);
                  输出就错了：[下雨，天,地，面,积，水]
上面代码不是已经在线学习过了，理论上应该可以正常输出了，为什么输出还是错的？我看到你回复过有序列化到模型？请问这个应该怎么做？

"
采用word2vec的api接口训练新模型完成时报java.lang.ArrayIndexOutOfBoundsException: 200错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 采用Word2VecTrainer训练自己的模型时，虽然能够完成训练，但最后会数组越界错误，我也翻阅了issues区的相关数组越界的话题，但还是没有找到解决办法；在引用新模型进行文本分类时也会报数组越界的错误，求指导~

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 我参考wiki上写了一个Word2VecTrain的类；
2. 里面注册了一个回调函数callbackLP，然后构建了一个训练trainerBuilder，只设置了上面的回调函数和并行线程数为4（在单机上测试，希望能够加快训练）
3. 具体代码如下：

### 触发代码

```
public class Word2VecTrain {

    public static void main(String[] args) {

        TrainingCallback callbackLP = null;
        final long timeStart = System.currentTimeMillis();
        if (callbackLP == null){
            callbackLP = new TrainingCallback() {
                public void corpusLoading(float percent) {
                    System.out.printf(""\r加载训练语料：%.2f%%"", percent);
                }

                public void corpusLoaded(int vocWords, int trainWords, int totalWords) {
                    System.out.println();
                    System.out.printf(""词表大小：%d\n"", vocWords);
                    System.out.printf(""训练词数：%d\n"", trainWords);
                    System.out.printf(""语料词数：%d\n"", totalWords);
                }

                public void training(float alpha, float progress) {
                    System.out.printf(""\r学习率：%.6f  进度：%.2f%%"", alpha, progress);
                    long timeNow = System.currentTimeMillis();
                    long costTime = timeNow - timeStart + 1;
                    progress /= 100;
                    String etd = Utility.humanTime((long) (costTime / progress * (1.f - progress)));
                    if (etd.length() > 0) System.out.printf(""  剩余时间：%s"", etd);
                    System.out.flush();
                }
            };
        }

//      构建训练方法
        Word2VecTrainer trainerBuilder = new Word2VecTrainer();

        trainerBuilder.setCallback(callbackLP);
        trainerBuilder.useNumThreads(4);

        WordVectorModel wordVectorModel = trainerBuilder.train
                (""D://DESKTOP//20180427//data-for-1.6.2//data//test//koubei_classify_seg.txt"",
                ""D://DESKTOP//20180427//data-for-1.6.2//data//test//msr_koubei_vectors_default.txt"");
    }
}

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
加载训练语料：100.00%
词表大小：24845
训练词数：2269300
语料词数：2368390
学习率：0.000163  进度：99.67%  剩余时间：01 s
训练结束，一共耗时：1 m 54 s 
正在保存模型到磁盘中……
模型已保存到：msr_koubei_vectors_default.txt
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
加载训练语料：100.00%
词表大小：176977
训练词数：32511717
语料词数：32511717

学习率：0.000005  进度：100.00%

训练结束，一共耗时：1 h 48 m 15 s 
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 200
	at com.hankcs.hanlp.mining.word2vec.VectorsReader.readVectorFile(VectorsReader.java:50)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:38)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
	at com.hankcs.hanlp.mining.word2vec.Word2VecTrainer.train(Word2VecTrainer.java:221)
	at com.autohome.Word2VecTrain.main(Word2VecTrain.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

实际输出入下图
```
http://attachbak.dataguru.cn/attachments/album/201805/11/163829w8bczfibvazzcbfu.png


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
a) 个人推测：是不是和缓存有关？因为我第一次运行训练程序时，设置了维度是200，但也报错，后来改为默认设置（默认维度应该是100），也是报数组越界200的异常，不知道是否和计算缓存有关？
b) 另外，在调用这个新模型时也会报数据越界的异常，没有另开issue，我想是模型本身没有训练成功的缘故，只是看本地文件貌似也正常，所以尝试着调用了一下新模型，但同样报数据越界异常。
c) 还有一个问题，就是wiki里写的训练模型保存文件格式是 .bin，但是 DemoWord2Vec里加载模型是用的文件格式是 .txt，所以不知道这两个之间有没有其他方法可以转换或者生成呢？"
数量,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
支持.csv,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

使AhoCorasickDoubleArrayTrieSegment支持讀取csv

## 相关issue
無

"
句法分析关键词提取占去内存很大,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [打钩 ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：0.1.5
我使用的版本是：0.1.41

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
执行get_relations_pos_head(sentence)会占用六百多兆的内存， 导致多线程出现内存溢出， java虚拟机设置的xms为1g,  xmx为1536m

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
句法分析提取关键词，采用多线程，会出现内存溢出,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [打钩 ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：0.1.5
我使用的版本是：0.1.41

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
采用pyhanlp
加载了自定义词典， 采用多线程，同时对对句话进行句法分析关键词提取时会出现内存溢出问题
word_array = HanLP.parseDependency(sentence).getWordArray()
jpype._jexception.java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: Java heap space


### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
relations, pos, head, words = get_relations_pos_head(sentence)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
依存句法分析：使用NeuralNetworkDependencyParser对相同词性结构句子得到不同依存句法分析结果,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：portable-1.6.3
我使用的版本是：portable-1.6.3（通过 maven 配置的版本）

## 我的问题
对于相同词性结构的句子<机构>的<修饰性名词><人名>，依存句法分析结果不一致。

## 复现问题
直接调用parser NeuralNetworkDependencyParser（并未进行任何修改），对以下句子进行依存句法分析：
1.上海华安工业公司的员工韩梅梅
2.上海华安工业公司的董事韩梅梅
得到的依存分析结果不一致。不知道是否有改进方法？

### 触发代码
```
    public void testIssue1234() throws Exception
    {
        IDependencyParser parser = new NeuralNetworkDependencyParser().enableDeprelTranslator(false);
        CoNLLSentence sentenceParsed1 = parser.parse(""上海华安工业公司的员工韩梅梅"");
        CoNLLWord[] arcs1 = sentenceParsed1.word;
        for (CoNLLWord word : arcs1) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
        System.out.println();
        CoNLLSentence sentenceParsed2 = parser.parse(""上海华安工业公司的董事韩梅梅"");
        CoNLLWord[] arcs2 = sentenceParsed2.word;
        for (CoNLLWord word : arcs2) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
    }
```
### 期望输出
```
上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 员工(6)
的(5) --(RAD)--> 公司(4)
员工(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)

上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 董事(6) //对应""员工""
的(5) --(RAD)--> 公司(4)
董事(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)
```

### 实际输出
但实际输出的依存分析，上海华安工业公司的ATT路径不一致，1.指向“员工”修饰性名词，2.指向“韩梅梅”实体。

```
上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 员工(6)
的(5) --(RAD)--> 公司(4)
员工(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)

上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 韩梅梅(7) //不对应""员工""
的(5) --(RAD)--> 公司(4)
董事(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)
```"
核心词库文件CoreNatureDictionary.txt，夏天 词性有问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

![image](https://user-images.githubusercontent.com/23119744/39564110-6f82598c-4ee5-11e8-94f1-cf832429612b.png)
夏天词性是ns，实际应该是t
春天，秋天，冬天都是词性都是t

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于 HanLP.segment() 的几个问题（主要围绕自定义词典、词性与词频）,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：hanlp-portable-1.6.3 （通过 maven 配置的版本）

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

1. 我已经了解用户自定义词典拥有以下特性：
（1）、加入自定义词典的词不一定会分出来，因为HanLP.segment会基于统计模型、语料与核心词典与词频等信息对句子进行切分，如将“国新能源”加入自定义词典后，分词并不会将“我国新能源行业”分成“我/国新能源/行业”，而是分成正确的“我国/新能源/行业”，但如“国新能源股票大涨”则会分为“国新能源/股票/大/涨”，这是非常理想的效果。
（2）、加入自定义词典的词、词性、词频，在分词后，其中词性会覆盖掉其原有的词性。如将“国家 Country 1”加入自定义词典中后，会将“国家/n”变为“国家/Country”。
（3）、自定义词典中的词频（具体数值）设置，并没能起到理想的作用（指调高词频后，会将其分出），如我将“国新能源 Stock 9999999” 加入自定义词典，并未能改变“我国/新能源/行业”的分词结果，与“国新能源 stock 1” 的效果是相同的，所以今后在自定义词典中设置词频时，可以选择 1 。
上述（1）、（2）、（3）点我的理解方向大致正确吗？（相关代码在下面）

2. 我在试验时，将“龙三  Conception 1”与“国元证券 Stock 1”加入用户自定义词典后，在不同句子中并不一定能将其分出，将其词频调高至9999999也不能将其分出，我了解到如果强制使用用户自定义词典而忽略掉核心词典的话，是可以达到将词出的效果，但正如 hanks 所说，它会在某个未知的位置引发其它未知（未发现）的错误。
如果我就想针对这个发现的不能分出的词作处理，并不想将所有的自定义词典的词都一定分出，也就是仍然需要核心词典信息等，该如何处理？（相关代码在下面）

3. 如何判断一个词是否在已知的词典中？只能去 CoreNatureDictionary.txt 中找吗？因为我有一个想法是，在添加自定义词典时，先判断这个词是否已在 HanLP.segment 的词典中，如果不在，我再添加并将词频设置为1，这么做的目的是不想更改原有的词性和词频，导致一些不必要的错误，因为我觉得，HanLP.segment 在绝大多数通常情况下分词是没有问题的。

4. 在添加自定义词典时，如果这个词已经在词典中，如何只更改此词的词性？这么做的目的是想保留原有词的词频。难道需要取到该词在原有词典中的词频，然后再同新词性一起加入到自定义词典中吗？这样，新词性就会覆盖掉原有词性了。

5. `CustomDictionary.insert(""国新能源"", ""hy 1"");`与`CustomDictionary.add(""国新能源"", ""hy 1"");`有什么区别？“强行加入”和“加入”？

6. `offset`属性只有在`IndexTokenizer.segment`中起作用吗？在`HanLP.segment`中分词后所有词的`offset`值都为0？（相关代码在下面）

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
在通过 maven 方式配置了 hanlp-portable-1.6.3 后，并未做任何修改，所有的自定义词典操作都是通过代码动态添加来进行试验的。
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码
```
    //问题1下的（1）
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""国新能源"", ""Stock 1"");
        System.out.println(HanLP.segment(""我国新能源行业""));
        System.out.println(HanLP.segment(""国新能源股票大涨""));
    }
    期望输出与实际输出一致：
        [我国/n, 新能源/nz, 行业/n]
        [国新能源/Stock, 股票/n, 大/a, 涨/v]
```
```
    //问题1下的（2）
    public static void main(String[] args) 
    {
        System.out.println(HanLP.segment(""家庭、私有制和国家的起源""));
        CustomDictionary.insert(""国家"", ""Country 1"");
        System.out.println(HanLP.segment(""家庭、私有制和国家的起源""));
    }
    期望输出与实际输出一致：
        [家庭/n, 、/w, 私有制/n, 和/c, 国家/n, 的/uj, 起源/n]
        [家庭/n, 、/w, 私有制/n, 和/c, 国家/Country, 的/uj, 起源/n]
```
```
    //问题1下的（3）
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""国新能源"", ""Stock 9999999"");
        System.out.println(HanLP.segment(""大赞我国新能源""));
    }
    实际输出：
        [大赞/nrf, 我国/n, 新能源/nz]
    期望输出：
        [大赞/nrf, 我/r, 国新能源/Stock]
```
```
    //问题2
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""龙三"", ""Conception 1"");
        CustomDictionary.insert(""国元证券"", ""Stock 1"");
        System.out.println(HanLP.segment(""龙三国元证券""));
        System.out.println(HanLP.segment(""龙三和国元证券""));
        System.out.println(HanLP.segment(""龙三与国元证券""));
        System.out.println(HanLP.segment(""龙三及国元证券""));

        System.out.println(""\n"");

        CustomDictionary.insert(""龙三"", ""Conception 9999999"");
        CustomDictionary.insert(""国元证券"", ""Stock 9999999"");
        System.out.println(HanLP.segment(""龙三国元证券""));
        System.out.println(HanLP.segment(""龙三和国元证券""));
        System.out.println(HanLP.segment(""龙三与国元证券""));
        System.out.println(HanLP.segment(""龙三及国元证券""));
    }
    实际输出：
        [龙三国/nr, 元/q, 证券/n]
        [龙/n, 三和/nz, 国元证券/Stock]
        [龙三/Conception, 与/p, 国元证券/Stock]
        [龙三/Conception, 及/c, 国元证券/Stock]
    期望输出：
        [龙三/Conception, 国元证券/Stock]
        [龙三/Conception, 和/c, 国元证券/Stock]
        [龙三/Conception, 与/p, 国元证券/Stock]
        [龙三/Conception, 及/c, 国元证券/Stock]
```
```
    //问题3、4为思路性问题，问题5 将 CustomDictionary.insert 换为 CustomDictionary.add 无变化
```
```
    //问题6
    public static void main(String[] args) 
    {
        List<Term> termList = HanLP.segment(""打工是不可能打工的"");
        System.out.println(termList);
        for (Term term : termList){
            System.out.println(""offset: "" + term.offset);
            System.out.println(""wordLength: "" + term.word.length());
        }
    }
    实际输出：
        [打工/v, 是/v, 不/d, 可能/v, 打工/v, 的/uj]
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 1
        offset: 0
        wordLength: 1
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 1
    期望输出：
        [打工/v, 是/v, 不/d, 可能/v, 打工/v, 的/uj]
        offset: 0
        wordLength: 2
        offset: 2
        wordLength: 1
        offset: 3
        wordLength: 1
        offset: 4
        wordLength: 2
        offset: 6
        wordLength: 2
        offset: 8
        wordLength: 1
```

## 其他信息
如何根据 HanLP.Config.enableDebug() 来分析分词结果？如：
```
    //当使用 HanLP.Config.enableDebug();
    public static void main(String[] args) 
    {
       CustomDictionary.insert(""龙三"", ""Conception 1"");
        CustomDictionary.insert(""国元证券"", ""Stock 1"");
        HanLP.Config.enableDebug();
        System.out.println(HanLP.segment(""龙三和国元证券""));
    }
    实际输出：

        粗分词网：
        0:[ ]
        1:[龙]
        2:[三和]
        3:[和]
        4:[国]
        5:[元]
        6:[证, 证券]
        7:[券]
        8:[ ]

        粗分结果[龙/n, 三和/nz, 国元证券/Stock]
        人名角色观察：[  K 1 A 1 ][龙 D 1350 E 924 B 498 C 325 L 17 M 8 K 1 ][三和 A 20833310 ][国元证券 A 20833310 ][  K 1 A 1 ]
        人名角色标注：[ /K ,龙/B ,三和/A ,国元证券/A , /A]
        [龙/n, 三和/nz, 国元证券/Stock]
```
其中，在粗分词网中，为什么没有发现加入自定义词典中的“龙三”和“国元证券”，还有“0:[ ]""、""8:[ ]""表示什么意思啊？
多谢指教！
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"Fixed #806,TextRank提取关键词提升算法速度","## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

提升TextRank算法的速度

## 相关issue

#806 

"
人名识别不准确,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
人名识别有误：
“他负责冲货（串货）的调查与处理，包括线上线下冲货” 
=> 
[他/r, 负责/v, 冲货/v, （/w, 串货/nr, ）/w, 的/u, 调查/vn, 与/p, 处理/vn, ，/w, 包括/v, 线/n, 上/f, 线/n, 下/f, 冲货/n]
<!-- 请详细描述问题，越详细越可能得到解决 -->
“串货”这个词误识别为人名
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
//这个问题是在运行以下代码时出现：
NLPTokenizer.ANALYZER
	.enableNameRecognize(true)
	.enableTranslatedNameRecognize(true)
	.enableJapaneseNameRecognize(true)
	.enablePlaceRecognize(true)
	.seg(""他负责冲货（串货）的调查与处理，包括线上线下冲货"");
```
"
NLPTokenizer.segment 线程安全问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
`NLPTokenizer.segment` 在多线程运行情况下，概率性地出现如下问题：

```
SEVERE: Exception while executing runnable io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@721b9773
java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at com.hankcs.hanlp.dictionary.TransformMatrixDictionary.extendSize(TransformMatrixDictionary.java:221)
        at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:62)
        at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
        at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.segSentence(AbstractLexicalAnalyzer.java:321)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
        at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:49)
        at com.company.xushen.Server$NLPImpl.segment(Server.java:71)
        at com.company.xushen.NLPGrpc$MethodHandlers.invoke(NLPGrpc.java:230)
        at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
        at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:698)
        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

我怀疑，虽然 `NLPTokenizer.segment` 声称是线程安全的，但是在调用栈的 `CustomNatureUtility.addNature` 并不是线程安全的，才导致这样的错误。
"
编译器警告：Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
从 JDK 8 开始编译器会警告代码中使用 '_' 作为标识符，在以后的 Java 版本中继续使用 '_' 作为标识符成为编译错误。建议把这个临时变量的名字改成别的。

## 复现问题
`mvn comiple`

### 期望输出
没有输出

### 实际输出

```
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,18] '_' 用作标识符
  (Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,25] '_' 用作标识符
  (Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,41] '_' 用作标识符
  (Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/model/trigram/CharacterBasedGenerativeModel.java:[210,24] '_' 用作标识符
  (Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/model/trigram/CharacterBasedGenerativeModel.java:[212,19] '_' 用作标识符
  (Java SE 8 之后的发行版中可能不支持使用 '_' 作为标识符)
```

## 其他信息
[Small Language Changes in Java SE 9](https://docs.oracle.com/javase/9/language/toc.htm#JSLAN-GUID-16A5183A-DC0D-4A96-B9D8-AAC9671222DD)

"
语义查询 添加类似“哈哈哈哈哈”的重复叠词文档时 无法返回正确结果,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 描述
在进行语义查询时, 插入的文档如果是类似“哈哈哈哈哈哈”这种叠词重复的组合，无法正确返回结果，有BUG。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现
使用doc2vector进行语义查询，调用addDocument接口添加“哈哈哈哈哈哈”这种文档
使用的词向量模型是官方提供的HANLP预训练向量。
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
无

### 触发代码
无
### 期望输出
无


### 实际输出
无

## 其他信息
无
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
TextRank算法的优化,"TextRank算法采用多轮迭代收敛来获取结果，目前的实现中score没有赋初值。
实际测试中依据已有信息赋初值能有效的降低迭代轮次，以作者
http://www.hankcs.com/nlp/textrank-algorithm-to-extract-the-keywords-java-implementation.html 
的例子，采用了两种不同的初值方法
1、初值采用sigmoid函数来赋值，实测示例计算从37轮迭代降低为9轮。我是用scala测试的，Java实现参考 
Map score = new HashMap(); 
for (Map.Entry<String, Set> entry : words.entrySet()){ score.put(entry.getKey(),sigmoid(entry.getValue().size()); 
}
2、把sigmoid函数更改为 1.0*entry.getValue().size()/words.size()，可以从37轮次降低为34轮次

建议采用sigmoid函数提升效率，sigmoid函数scala实现就是一行
def sigmoid(x:Double):Double ={1d/(1d+Math.exp(-x))}"
MathTools.java 代碼疑問,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： 1.6.4
我使用的版本是： 1.5.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，我想請教 MathTools.java 裡的 calculateWeight
   
` double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`    

這行代碼是參考什麼算法或公式所寫的?

<!-- 请详细描述问题，越详细越可能得到解决 -->

"
根据百度汉语和在线辞海修正拼音词典,"根据[百度汉语](http://hanyu.baidu.com)和[在线辞海](https://cihai.supfree.net/)对拼音词典进行了修正

修正规则：
1. 根据拼音词典从百度汉语逐一查询，进行对比，找到不一致的词条；
2. 将不一致的词条从在线辞海查询，如何查询结果和百度汉语一致，则进行替换。
3. 不一致的词条中包含“一”的，采用百度汉语的结果。"
v1.6.2感知机生效了，但是能自定义词性标签么？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
按照https://github.com/hankcs/HanLP/wiki/结构化感知机标注框架 训练自己的语料，没有达到预想效果。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 生成语料
   你好/vl	自治县/n	[你好/vl	自治县/n]/company
   上街/vi	北路/n	[上街/vi	北路/n]/company

2. 分别进行如下训练
   CWSTrainer PerceptronTrainer  PerceptronTrainer 

3. PerceptronLexicalAnalyzer 测试

### 触发代码

```
        Sentence sentence = segmenter.analyze(""上街北路"");
        System.out.println(sentence);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

上街北路/company

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

上街/vi	北路/n

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
v1.6.2 自定义词库不生效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
自定义词库不生效
1、新建了自定义词库：PlaceDictionary.txt place
    你好自治县
   上街北路

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
### 步骤
1、自定义词库 PlaceDictionary.txt place
     词库内容
     通道侗族自治县
    上地马连洼北路

2. 配置文件
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf, PlaceDictionary.txt place;

3. 删除bin，重新执行

### 触发代码

```
String[] testCase = new String[]{
                ""上街北路"",
                ""你好自治县""
        };
        for (String sentence : testCase)
        {
            Segment segment = HanLP.newSegment().enableCustomDictionary(true);
            List<Term> termList = segment.seg(sentence);
            String termString = join(termList,""\t"");
            System.out.println(termString);
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
上街北路/place
你好自治县/place

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

你好/vl	自治县/n	[你好/vl	自治县/n]/company
上街/vi	北路/n	[上街/vi	北路/n]/company

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP进行文本转汉语拼音时需要优化,"用HanLP的用户目前多为做语音识别相关，目前HanLP对汉字转汉语拼音的支持仅限汉字。
但是通常情况下，用户输入的文本并不能确定是中文还是英文，那么，在这种场景中应该支持的是：如果是中文，那么就转成汉语拼音，如果不是那么就保留。"
关于感知机与CRF对空格与标点的词性识别问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：portable-1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
综合比较其他几种分词器，感知机对词性的标注相对更准确，但是对于空格和标点符号(尤其是英文标点)的标注存在许多问题。

例如对以下这句话的标注：
`""你好， 我想知道： 风是从哪里来; 雷是从哪里来； 雨是从哪里来？""`

perceptron:
```
(你/r) (好/a) (，/w) ( /v) (我/r) (想/v) (知道/v) (：/w) ( 风/n) (是/v) (从/p) (哪里/r) (来/v) (; 雷/d) (是/v) (从/p) (哪里/r) (来/v) (；/w) ( 雨/n) (是/v) (从/p) (哪里/r) (来/v) (？/w) 
```
crf:
```
(你好/d) (，/v) ( /v) (我/r) (想/v) (知道/v) (：/w) ( 风/n) (是/v) (从/p) (哪里/r) (来;/v) ( 雷/n) (是/v) (从/p) (哪里/r) (来/v) (；/w) ( 雨/n) (是/v) (从/p) (哪里/r) (来/v) (？/v) 
```
viterbi:
```
(你好/l) (，/w) ( /w) (我/r) (想/v) (知道/v) (：/w) ( /w) (风/n) (是从/v) (哪里/r) (来/v) (;/w) ( /w) (雷/n) (是从/v) (哪里/r) (来/v) (；/w) ( /w) (雨/n) (是从/v) (哪里/r) (来/v) (？/w) 
```

重点关注空格与标点的分词，结果发现：感知机与CRF对有时空格与标点识别为其他词性，甚至会与前后的词成为组合，反而默认的viterbi对于标点的处理更好。

最近开始接触这方面，尚未仔细阅读源码，请问对于空格与标点是如何处理的，能否改进？请指导。

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->"
优化CRF分词效率的方法,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

[CRF分词](https://github.com/hankcs/HanLP#6-crf%E5%88%86%E8%AF%8D)和[感知机分词](https://github.com/hankcs/HanLP/wiki/%E7%BB%93%E6%9E%84%E5%8C%96%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A0%87%E6%B3%A8%E6%A1%86%E6%9E%B6)的流程相差不大(都是提取特征->查概率/权重->累加->Viterbi), 
但Wiki上面的测试结果差距却很大。而且HanLP早期的CRF模型特征模板数量少于当前感知机的七个模板。
因此查看了一下HanLP构造CRF模型的逻辑，我发现了一个问题：
CRF++生成的特征都是以“U[0-9]+:”开头的，而模型使用BinTrie索引特征概率，这就导致BinTrie加速的第一层只有一个字符“U”，，所有的特征都都走了二分查找，难怪速度会慢。

## 解决思路

需要解决的是如何把汉字索引到第一级同时又不影响效率，我觉得可以考虑拆解重组特征模板和特征Key，或者直接reverse字符串。
"
Merge pull request #1 from hankcs/master,"同步库代码

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
关于自定义Recognition和日期或者时间的分词上的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
请问在对""管理员于2017-10-12对该设备信息进行了修改操作""进行拆分时，“2017-10-12”这个日期会被单独拆开。有没有好的方式解决这点，找了找类似的issue,貌似没找到合适的方式。另外现在有提供Recognition自定义么。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->


### 触发代码

```
     public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer =  new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.seg(""管理员于2017-10-12对该设备信息进行了修改操作""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
管理员/n, 于/p, 2017-10-12, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
管理员/n, 于/p, 2017-/m, 10/m, -/q, 12/m, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v


```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
运行截图：
![38286722-5e5c1772-37f9-11e8-83b0-eb77566f9c82](https://user-images.githubusercontent.com/18030444/38715484-f8034542-3f0d-11e8-8b38-3fa6236a8e5b.png)
"
默认分词器开关设置异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.2
我使用的版本是：portable-1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在利用HanLP默认的维特比分词器分词时，系统抛出java.util.NoSuchElementException。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->


### 步骤

1. 下面的代码，enableNumberQuantifierRecognize打开时会抛出上述异常。
2. 把其他开关都去掉，只打开enableNumberQuantifierRecognize时，程序正常运行。


### 触发代码

```
 public static void main(String[] args) {
        Segment seg = HanLP.newSegment();
        seg.enablePartOfSpeechTagging(true);
        seg.enableMultithreading(8);
        seg.enableAllNamedEntityRecognize(true);
        seg.enableNameRecognize(true);
        seg.enableOrganizationRecognize(true);
        seg.enablePlaceRecognize(true);
        seg.enableNumberQuantifierRecognize(true);

        String raw=""余姚日报电子版余姚卫生计生事业单位公开招聘121名卫技人员2018年余姚市卫生计生事业单位公开招聘卫生技术人员简章根据有关规定和卫生计生事业发展需要，经研究决定，我市卫生计生事业单位公开招聘卫技人员121名。现将有关事项公告如下：一、招聘职位及指标招聘单位及招聘指标 职位编码 单位 职位 学历 数量 职位要求及其他 1 市人民医院 临床 研究生 1 肾内科方向 2 市中医医院 临床 研究生 1 心血管内科方向 3 市中医医院 临床 研究生 1 内分泌科方向 4 市二院 临床 研究生 1 儿科学专业 5 市三院 临床 研究生 1 临床医学、精神病与精神卫生学专业 6 市人民医院 临床 本科及以上 8 临床医学专业，第一批次入学 7 市人民医院 临床（从事麻醉） 本科及以上 1 临床医学、麻醉学专业，第一批次入学 8 市中医医院 临床 本科及以上 1 临床医学专业，第一批次入学 9 市二院 临床 本科及以上 1 临床医学专业，具有相应执业医师资格，执业注册范围为重症医学 10 市二院 临床（从事皮肤科） 本科及以上 1 临床医学专业，具有相应执业医师资格 11 市二院 临床（从事视光门诊） 本科及以上 1 临床医学、眼视光专业 12 市三院 临床（从事精神科） 本科及以上 2 临床医学、精神医学专业，男性 13 市三院 临床（从事精神科） 本科及以上 2 精神医学专业，具有相应执业医师资格，执业注册范围为精神卫生 14 市四院 临床（从事骨科） 本科及以上 1 临床医学专业，具有相应执业医师资格 15 市四院 临床（从事麻醉） 本科及以上 1 临床医学专业，具有相应执业医师资格 16 市急救站 临床 本科及以上 2 临床医学专业，男性 17 市卫校 临床（从事解剖教学） 本科及以上 1 临床医学专业，适合男性 18 市人民医院医共体成员单位 临床 本科及以上 3 临床医学专业 19 市四院黄家埠分院（黄家埠镇卫生院） 临床 本科及以上 1 临床医学专业 20 市四院（下属社区卫生服务站） 临床（从事儿保） 大专及以上 1 临床医学、医学影像学专业，具有相应执业（助理）医师资格 21 市四院（下属社区卫生服务站） 临床（从事B超） 大专及以上 1 临床医学、医学影像学专业，具有相应执业（助理）医师资格 22 市急救站 临床 大专及以上 4 临床医学专业，具有相应执业（助理）医师资格，适合男性 23 市人民医院梁弄分院（梁弄中心卫生院） 临床（从事妇儿保） 大专及以上 1 临床医学专业 24 市四院医共体成员单位 临床（从事医学影像诊断） 大专及以上 3 临床医学、医学影像学专业 25 市四院黄家埠分院（黄家埠镇卫生院） 临床 大专及以上 1 临床医学专业 26 市中医医院三七市分院（三七市镇卫生院） 临床（从事医学影像诊断） 大专及以上 1 临床医学、医学影像学专业 27 市中医医院大隐分院（大隐镇卫生院） 临床（从事儿保） 大专及以上 1 临床医学专业，具有相应执业（助理）医师资格，执业注册范围为全科医学或儿科 28 市二院大岚分院（大岚镇卫生院） 临床（从事医学影像诊断） 大专及以上 1 临床医学、医学影像学专业 29 市人民医院 临床（从事耳鼻咽喉科） 本科及以上 1 定向职位，临床医学专业，具有相应主治医师及以上资格 30 市四院 临床 本科及以上 1 定向职位，临床医学专业，具有相应主治医师及以上资格 31 市四院小曹娥分院（小曹娥镇卫生院） 临床 本科及以上 1 定向职位，临床医学专业 32 市人民医院马渚分院（马渚中心卫生院） 临床（从事妇保） 大专及以上 1 定向职位，临床医学专业，女性 33 市二院牟山分院（牟山镇卫生院） 临床（从事妇儿保） 大专及以上 1 定向职位，临床医学专业，女性 34 市四院小曹娥分院（小曹娥镇卫生院） 临床 大专及以上 1 定向职位，临床医学专业 35 市中医医院 中医 研究生 1 中医外科学专业 36 市中医医院 中医 研究生 1 中医骨伤科学专业 37 市中医医院 中医 研究生 1 中医妇科学专业 38 市中医医院 中医 研究生 1 针灸推拿学专业 39 市中医医院 中医 研究生 1 中医肿瘤学方向 40 市中医医院 中医 本科及以上 7 中医学专业，第一批次入学 41 市四院 中医 本科及以上 1 中医学专业 42 市人民医院阳明街道分院（阳明街道卫生院） 中医 本科及以上 1 中医学专业 43 市四院黄家埠分院（黄家埠镇卫生院） 中医 本科及以上 1 中医学专业 44 市四院 中医（从事骨伤） 本科及以上 1 定向职位，中医学专业 45 市人民医院 针灸推拿（从事推拿） 本科及以上 1 针灸推拿专业，适合男性 46 其他市级医院 针灸推拿 本科及以上 2 针灸推拿专业 47 市二院 口腔 研究生 1 口腔临床医学专业 48 市二院 口腔 本科及以上 1 口腔医学专业 49 市四院 口腔 本科及以上 1 口腔医学专业 50 市人民医院丈亭分院（丈亭中心卫生院） 口腔 本科及以上 1 口腔医学专业 51 市四院朗霞街道分院（朗霞街道卫生院） 口腔 本科及以上 1 口腔医学专业 52 市中医医院三七市分院（三七市镇卫生院） 口腔 大专及以上 1 口腔医学专业，具有相应执业（助理）医师资格 53 市二院鹿亭分院（鹿亭乡卫生院） 口腔 大专及以上 1 口腔医学专业，具有相应执业（助理）医师资格 54 市梨洲医院（康复医院） 口腔 本科及以上 1 定向职位，口腔医学专业 55 市二院大岚分院（大岚镇卫生院） 口腔 大专及以上 1 定向职位，口腔医学专业，年龄放宽至1978年1月1日及以后出生 56 市人民医院医共体成员单位 公共卫生 本科及以上 3 预防医学、临床医学专业 57 市四院黄家埠分院（黄家埠镇卫生院） 公共卫生 本科及以上 1 预防医学、临床医学专业 58 市二院四明山分院（四明山镇卫生院） 公共卫生 大专及以上 1 定向职位，预防医学、临床医学专业 59 市人民医院 医学影像诊断 本科及以上 1 医学影像学、临床医学专业 60 市人民医院 DSA介入医师 本科及以上 1 医学影像学、临床医学专业，适合男性 61 市中医医院 医学影像诊断 本科及以上 1 医学影像学、临床医学专业 62 市三院 医学影像诊断（从事B超） 本科及以上 1 医学影像学专业 63 市人民医院医共体成员单位 医学影像诊断 本科及以上 4 医学影像学、临床医学专业 64 市人民医院阳明街道分院（阳明街道卫生院） 医学影像诊断（从事妇科B超） 本科及以上 1 医学影像学、临床医学专业，女性 65 市四院 医学影像诊断（从事放射） 本科及以上 1 定向职位，医学影像学、临床医学专业 66 市人民医院陆埠分院（陆埠中心卫生院） 医学影像诊断（从事妇科B超） 大专及以上 1 定向职位，医学影像学、临床医学专业，女性 67 市人民医院 DSA介入技师 本科及以上 1 医学影像技术、医学影像学专业，适合男性 68 市人民医院 ECT技师 本科及以上 1 医学影像技术、医学影像学专业，适合男性 69 市人民医院 放疗技师 本科及以上 1 医学影像技术、医学影像学专业，适合男性 70 市中医医院 医学影像技术 大专及以上 1 医学影像技术专业 71 市人民医院丈亭分院（丈亭中心卫生院） 医学检验 本科及以上 1 医学检验、临床医学专业 72 市四院小曹娥分院（小曹娥镇卫生院） 医学检验 本科及以上 1 医学检验、临床医学专业 73 市人民医院 药剂 研究生 1 药剂学专业 74 市中医医院 药剂 本科及以上 1 临床药学专业 75 市人民医院梁弄分院（梁弄中心卫生院） 药剂 本科及以上 1 药学专业 76 市中医医院大隐分院（大隐镇卫生院） 药剂 本科及以上 1 药学专业 77 市二院 中药 本科及以上 1 中药学专业 78 市人民医院梁弄分院（梁弄中心卫生院） 中药 大专及以上 1 中药学专业 79 市人民医院 护理 本科及以上 1 护理学、助产士专业 80 其他市级医院 护理 本科及以上 2 护理学、助产士专业 81 市中医医院 护理 大专及以上 1 护理学、助产士专业 82 市二院医共体成员单位 护理 大专及以上 2 护理学、助产士专业；具有护士执业资格者，学历放宽至中专 83 市三院 护理（从事精神科） 大专及以上 2 护理学专业，男性 84 市二院 助产 大专及以上 1 助产士专业或具有全日制本科学历的护理学（助产方向）专业 85 市人民医院 放疗物理师 本科及以上 1 医学物理、核物理、物理学专业，第一批次入学，适合男性 86 市二院 生物医学工程 本科及以上 2 生物医学工程专业，高空作业，适合男性 87 市中医医院 康复医学 本科及以上 1 康复医学、临床医学专业 88 市梨洲医院（康复医院） 康复治疗 本科及以上 1 康复治疗学专业 合计 121 注：1.应聘者应具有与招聘职位对口或相应专业文凭，所学专业适合招聘职位工作需要，并与参加执业资格考试、专业技术资格考试的专业要求相匹配，不强调专业名称字面完全一致；2.临床职位根据单位需要从事临床类别各专业。二、招聘对象和条件(一)招聘对象1.全日制普通高校毕业的研究生，以及全日制普通高校医学影像学、麻醉学、医学物理、核物理、物理学专业本科学历的毕业生。2.浙江省生源或浙江省常住户口的全日制普通高校第一批次入学的临床医学专业本科学历的毕业生、全日制普通高校预防医学、医学影像技术专业本科学历的毕业生、具有临床或公共卫生类别执业医师资格的毕业生。3.宁波大市生源2018年全日制普通高校(含普通中等专业学校)应届毕业生或宁波大市常住户口的国民教育系列毕业生(我市定向培养为农村社区医生的毕业生限报除余姚市级医疗卫生单位以外的单位)。4.在我市各类医疗卫生计生机构连续工作满两年并现在岗(工作时间算至2018年4月4日，下同)、按规定签订劳动合同和缴纳养老保险，且具有执业(助理)医师、执业护士、初级(士)及以上资格的人员户籍不限。全日制普通高校毕业生包括2018年应届毕业生和历届毕业生。招聘职位另有规定的，从其规定；常住户口入户时间截止2018年4月4日。余姚市卫生计生系统在编和已被确定为拟聘用、录用人员不列入本次招聘对象范围。(二)招聘条件1.拥护党的路线、方针、政策，遵纪守法，品行端正，爱岗敬业。2.1988年1月1日及以后出生，身体健康。3.具有符合招聘职位要求的学历、专业〔报考者所持的学历证书须符合招聘职位参加相应的执业(助理)医师资格考试、执业护士资格考试、专业技术资格考试对文凭的相关要求〕。4.国外、港澳台留学回国(境)人员应聘时，须已取得国家教育部认定的学历(学位)证书，专业相近的以所学课程专业名称为准。5.对2016年8月31日及之前毕业的报考者应持有与招聘职位相对应的执业资格证书(专业技术资格证书)。6.对报考市级医疗卫生单位学历要求为本科及以上职位者(报考定向职位者除外)，要求为全日制普通高校毕业生，并具有相应专业的学士及以上学位和外语国家四级及以上成绩合格单。7.报考定向职位者，须具有相应的执业(助理)医师等资格，其中报考市级医疗单位临床、中医、口腔、医学影像诊断等定向职位者须具有相应的执业医师资格。8.对下列报考者年龄可放宽：具有硕士及以上学位的全日制普通高校毕业的研究生或具有相应的资格并报考定向职位者，年龄可放宽至1983年1月1日及以后出生；具有相应副主任医师(药师、技师、护师)及以上资格者，年龄可放宽至1973年1月1日及以后出生。招聘职位另有规定的，从其规定。三、招聘程序、办法招聘工作贯彻公开、平等、竞争、择优原则，采取公开报名、统一考试、体检、考察、择优聘用的办法进行。具体程序和办法：(一)报名时间：4月10日―4月11日(上午8：45 ～11：45，下午13：45～16：45)。地点：市卫生进修学校内(世南西路139号)。办法：持本人户口簿(或户籍证明)、身份证、毕业证书(应届毕业生需持学生证、高校核发的就业推荐表、就业协议书)和职位招聘条件所要求的相关证书(证明)原件及复印件，近期免冠1寸照片3张，并填写《余姚市招聘卫生技术人员报名登记表》。到报名现场报名并接受资格初审，初审合格者，缴纳考务费100元。仅符合招聘对象第4点即在我市各类医疗卫生计生机构连续工作满两年并现在岗、按规定签订劳动合同和缴纳养老保险，且具有执业(助理)医师、执业护士、初级(士)及以上资格的报名对象，还需提供工作证明、劳动合同和缴纳养老保险证明。每人限报一个职位，多报无效。准考证发放：报名者凭本人身份证到市卫生和计划生育局阳明东路办公区(阳明东路127号)组织人事科领取。准考证领取时间报名当天另行告知，逾期不领视作放弃考试。(二)考试(包括笔试和面试)1.笔试：笔试科目为卫生相关综合基础知识和相应职位专业知识两门，每门满分按100分计算。卫生相关综合基础知识内容包括卫生法律法规、医学伦理学等知识；相应职位专业知识内容为招聘职位所要求的相关专业知识。笔试采取闭卷形式。笔试具体时间、地点以准考证为准。应聘者若笔试成绩不足48分(折合分)，且低于同类职位所有实际参加笔试并取得有效分数人员平均成绩(折合分)的90%〔市急救站临床大专及以上(职位22)放宽至60%，保留小数点后两位〕，则淘汰，取消面试资格。2.面试：按照招聘职位先根据折算后的笔试成绩从高分到低分进行排名，再按以下比例确定排名在前的应聘者为面试对象：招聘职位需求人数为1人的按职位指标以1：3的比例确定面试对象，招聘职位需求人数为2至4人的按职位指标以1：2的比例确定面试对象，招聘职位需求人数在5人及以上的按职位指标以1：1.5的比例确定面试对象(按四舍五入保留整数，比例内最后一名分数并列者全部入闱面试)，不足规定比例的按实际人数确定。护理职位的面试以实践考试形式进行；其它职位的面试主要测评应聘者口头表达能力、现场应变能力、综合分析能力、回答问题准确性、知识面和举止仪表等。面试成绩满分为100分，不足60分者淘汰。面试时间、地点另行通知。考试成绩=卫生相关综合基础知识成绩×10%+相应职位专业知识成绩×70%+面试成绩×20%。(三)体检根据考试成绩，从高分到低分(同一招聘职位应聘者考试成绩并列的，依次按笔试成绩、相应职位专业知识笔试成绩高者排名在先。仍不能区分排名的，则另行增加测试内容，测试成绩高者排名在先，下同)按招聘职位指标以1：1的比例确定体检对象。同类职位实际参加笔试仅有1人的，若应聘者考试成绩未达到60分，取消体检资格。体检参照宁波市考录公务员的相关办法和标准执行(如有补充规定，则在体检前另行公布)。放弃体检或体检结果不合格者淘汰，体检时间、地点另行通知。(四)考察体检合格者为考察对象，主要考察被考察对象的德才表现和应聘资格条件等情况，考察结果为不宜聘用为事业人员者淘汰(详见附件2)。考察对象为2018年应届毕业的，须在2018年7月30日前将本人的毕业证书、学位证书、外语成绩合格单、入学批次证明等相应材料交至余姚市卫生和计划生育局进行验证，逾期视为放弃聘用资格。(五)聘用考察合格者为拟聘用对象，经公示7天无异议的，按权限批准后予以聘用(已参加工作的拟聘用对象，须按有关规定办理好与原单位终止人事关系的手续)，并办理报到聘用手续；有异议的，经核实如不宜聘用为事业人员的，取消聘用资格。体检合格人员因个人原因在规定时间内不能提供个人档案或拟聘用人员逾期不按规定报到的，取消聘用资格。本次招聘，因考生主动放弃体检出现的空缺按考试成绩从高分到低分依次递补，其它原因出现空缺不进行递补；如无合适对象，允许招聘不足或空缺。招聘工作相关通知、考试成绩、体检结果及拟聘用对象名单将在余姚人才网( www.yyrc.com )和余姚市卫生和计划生育局网站 ( www.yy.gov.cn/col/col73829/index.html) 上公布。新聘用的事业单位工作人员，按有关规定实行人事代理和试用期(见习期)，并按规定签订聘用合同。经试用期或见习期满并经考核合格后予以正式聘用，考核不合格者取消聘用资格。2016年9月1日及之后毕业未取得执业资格、专业技术资格的应聘者，按照正常的执业资格考试、专业技术资格考试的时间规定，聘用后在2019年12月31日前未能通过相应的执业资格考试、专业技术资格考试的，作待岗处理；在2020年12月31日前仍未能通过相应的执业资格考试、专业技术资格考试的，则解除聘用合同。定向职位录用人员必须在录用单位服务七年以上。工资福利待遇按有关规定执行。本简章中所称的市级医疗卫生单位是指市人民医院、市中医医院、市第二人民医院、市第三人民医院、市第四人民医院、市梨洲医院(康复医院)、市卫校、市急救站。其他市级医院是指市中医医院、市二院、市三院、市梨洲医院(康复医院)。本简章中所称的同类职位分为临床类职位、中医类职位、针灸推拿类职位、口腔类职位、公共卫生类职位、医学影像(DSA介入医师、DSA介入技师、ECT技师、放疗技师、医学影像技术)类职位、医学检验类职位、药剂类职位、中药类职位、护理(助产)类职位、生物医学工程(放疗物理师)类职位、康复医学类职位、康复治疗类职位。本简章及附件不再在报名现场发放，应聘者请在余姚人才网或余姚市卫生和计划生育局网站上自行下载。本简章由余姚市卫生和计划生育局负责解释。咨询电话(工作日)：62686886、62672605；监督电话：62682597。余姚市人力资源和社会保障局余姚市卫生和计划生育局二零一八年四月四日"";
        StringBuilder sb = new StringBuilder();
        for (Term term : seg.seg(raw)) {
            sb.append(term.word).append("" "");
        }
        System.out.println(sb.toString());
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
打印出正常的分词结果
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
抛出异常
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList.getFirst(LinkedList.java:244)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:148)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:507)

"
一个自定义词库生成bin文件的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.6.2
我使用的版本是：1.6.2




## 我的问题

自定义词库生成bin文件，通过配置hanlp.properties文件中CustomDictionaryPath，配置同一目录下多个自定义字典，Hanlp不能一次生成所有自定义字典的bin文件，每次只能生成CustomDictionaryPath配置项的第一个自定义字典的bin文件，导致剩下的自定义字典中的词不能识别拆分。


## 复现问题
Hadoop集群中添加自定义词典

### 步骤

1. 首先，在自定义目录custom下面添加自定义词典(比如，字典名为军事.txt; 动植物.txt; 历史.txt; 娱乐.txt; 旅行.txt)
2. 然后，在hanlp.properties中添加CustomDictionaryPath配置(比如，CustomDictionaryPath=data/dictionary/custom/军事.txt; 动植物.txt; 历史.txt; 娱乐.txt; 旅行.txt)
3. 接着，执行Hanlp拆分，执行完后，只能生成""军事.txt.bin""的bin文件，动植物、历史、娱乐、旅行对应的bin文件不能生成

### 触发代码


### 期望输出

通过配置文件可以生成所有bin文件



### 实际输出

只有第一个配置词典的bin文件，导致后续分词，除第一个以外的字典都不能用来自定义分词



<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Merge pull request #1 from hankcs/master,"同步库代码

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
textrank算法公式没有注释,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在阅读这个博文的时候，发现textrank算法的公式没注释说明，希望博主发一份公式说明，我最近在搞关键词提取，希望再textrank的基础上改进算法
博文url：
http://www.hankcs.com/nlp/textrank-algorithm-to-extract-the-keywords-java-implementation.html

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

![image](https://user-images.githubusercontent.com/23119744/38651352-5cafd126-3e33-11e8-8b29-f866538276cb.png)


"
分词出现问题，应该如何操作才可以修正？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
### Jar和词库
1.6.2 Release版Jar；
1.6.2的Release词库（data-for-1.6.2.zip）；
hanlp.properties 仅修改了root地址；

### 问题分词
以下语句使用标准分词（HanLP.segment）出现问题：
#### 语句
 学生从简短的诗歌中感受到朱德和战士们的深厚情谊。
#### 分词结果
学生/nnt 从/p 简短/a 的/ude1 诗歌/n 中/f 感受到/nz 朱德和/nr 战士/nnt 们/k 的/ude1 深厚/a 情谊/n 。/w

#### 提取关键字结果
[""感受到"",""朱德和"",""诗歌"",""战士"",""深厚"",""简短"",""情谊"",""学生""]
#### 提取短语结果
[""中感受到"",""学生简短"",""感受到朱德和"",""战士深厚"",""朱德和战士"",""深厚情谊"",""简短诗歌"",""诗歌中""]

### 期望结果
 想把朱德分出来，应该如何操作？
"
人名识别错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：portable-1.6.2
我使用的版本是：portable-1.3.4

## 我的问题
在自定义词典中添加了人名，分词过程中没有识别出来。

## 复现问题
### 步骤

1. 在自定义词典中添加了“苏苏 nr 10”，然后删除掉.bin的文件。
2. 重新运行分词代码，句子为“苏苏中级会计什么时候更新”，分词结果为“[苏苏中级, 会计, 什么, 时候, 更新]”。
3. 我在代码中添加
CustomDictionary.add(""苏苏"");
StandardTokenizer.SEGMENT.enableCustomDictionary(true);
执行的结果还是一样。
4. 我打开调试模式，HanLP.Config.enableDebug(); 输出分词过程，观察到“识别出人名：苏中级 XD
识别出人名：苏苏中级 BXD“。但是我在人名词典中并没有发现有“苏中级”或者“苏苏中级”这样的词。
5. 我关闭人名识别后，输出结果不是我想要的，在实际使用中还有很多句子包含人名，所以这种方式我就放弃了。
### 触发代码

```
    public void testIssue1234() throws Exception
    {
        String s = ""苏苏中级会计什么时候更新"";
        CustomDictionary.add(""苏苏"");
        StandardTokenizer.SEGMENT.enableCustomDictionary(true);
        HanLP.Config.enableDebug();
        System.out.println(HanLP.segment(s));
    }
```
### 期望输出

[苏苏, 中级, 会计, 什么, 时候, 更新]

### 实际输出

[苏苏中级, 会计, 什么, 时候, 更新]

## 其他信息

附上DEBUG日志：
人名角色观察：[  K 1 A 1 ][苏 B 2141 E 105 C 102 D 34 ][苏中 X 1 ][级 D 3 C 1 K 1 ][会计 K 12 L 1 ][什么 L 4 K 3 ][时候 K 21 ][更新 Z 96 L 11 ][  A 20843310 ]
人名角色标注：[ /K ,苏/B ,苏中/X ,级/D ,会计/L ,什么/L ,时候/K ,更新/Z , /A]
识别出人名：苏中级 XD
识别出人名：苏苏中级 BXD
细分词网：
0:[ ]
1:[苏, 苏苏中级]
2:[苏中, 苏中级]
3:[]
4:[级]
5:[会计]
6:[计]
7:[什么]
8:[]
9:[时候]
10:[]
11:[更新]
12:[]
13:[ ]

"
关于感知机分词的一点疑问,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
仿照文档中的示例可以正确识别“下雨天地面积水”，但是对“中国联通是公司”的学习不如预期，不知道是不是我在使用上有不当导致。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 触发代码

```
    public void testIssue1234() throws Exception
    {
        PerceptronSegmenter segmenter = new PerceptronSegmenter(HanLP.Config.PerceptronCWSModelPath);
            System.out.println(segmenter.segment(""下雨天地面积水""));
            segmenter.learn(""下雨天 地面 积水"");
            System.out.println(segmenter.segment(""下雨天地面积水""));
            System.out.println(segmenter.segment(""中国联通是公司""));
            segmenter.learn(""中国联通 是 公司"");
            System.out.println(segmenter.segment(""中国联通是公司""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
            [下雨, 天地, 面积, 水]
            [下雨天, 地面, 积水]
            [中国, 联通, 是, 公司]
            [中国联通, 是, 公司]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
            [下雨, 天地, 面积, 水]
            [下雨天, 地面, 积水]
            [中国, 联通, 是, 公司]
            [中国, 联通, 是, 公司]
```



"
 完善圆圈数字对应关系,"
## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

完善了圆圈数字对应关系，例如⑧=八；
此外修正了⑤的对应关系，之前⑤对应伍，现在改为五

"
对于类似时间或者日期分词拆分的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在对""管理员于2017-10-12对该设备信息进行了修改操作""进行拆分时，“2017-10-12”这个日期会被单独拆开。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 触发代码

```
    public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer =  new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.seg(""管理员于2017-10-12对该设备信息进行了修改操作""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
管理员/n, 于/p, 2017-10-12, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
管理员/n, 于/p, 2017-/m, 10/m, -/q, 12/m, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v
```

## 其他信息
操作结果：

![12](https://user-images.githubusercontent.com/18030444/38286722-5e5c1772-37f9-11e8-83b0-eb77566f9c82.png)


<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
com.hankcs.hanlp.summary.KeywordExtractor类的shouldInclude方法，不识别一个字的词,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.2.jar
我使用的版本是：hanlp-1.5.4.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

com.hankcs.hanlp.summary.KeywordExtractor类的shouldInclude方法，为什么判断一个字的词不在计算范围内，像  霾  这个字本身就是一个词，是不是太武断了

## 复现问题
### 触发代码

public boolean shouldInclude(Term term)
    {
        // 除掉停用词
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (_**term.word.trim().length() > 1**_ && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }


"
对部分人名中有数词（如张三丰），在拆分时会出现不如预期情况。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
存在某些人名中有数词出现的人名时，会出现拆分不如预期的问题，如“张三丰，黄三元，刘五郎”会出现问题，而“王三强，闻一多，李四光”等经测试没有问题，不知道是不是因为有问题的属于常用词或商标名等原因导致。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 触发代码

```
    public static void main(String[] args) {
        String content1 = ""张三丰，刘五郎，黄三元，张一楠，王三强，丁一楠，李四光，闻一多，赵一楠，李四"";
        System.out.println(NLPTokenizer.segment(content1));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出：张三丰，刘五郎，黄三元，张一楠，王三强，丁一楠，李四光，闻一多，赵一楠，李四
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出：张/q, 三丰/nz, ，/w, 刘/nr, 五郎/nz, ，/w, 黄/a, 三元/nz, ，/w, 张一楠/nr, ，/w, 王三强/nr, ，/w, 丁一楠/nr, ，/w, 李四光/nr, ，/w, 闻一多/nr, ，/w, 赵一楠/nr, ，/w, 李四/nr]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
测试结果
![jg](https://user-images.githubusercontent.com/18030444/38225667-8e0e25c2-3728-11e8-98cd-4d2070edddd1.png)
"
人名中有数词（如张三）在方式一portable与方式二自定义数据包的情况下拆分结果不同,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ×] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
对于句子“管理员张三”的拆分中，使用方式一portable整合版与方式二自定义数据包这两种方式获得的拆分结果不同
方式一：管理员/nr, 张三/nr
方式二：管理员/nnt, 张/q, 三/m
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 分别使用两种方式拆分“管理员张三”

### 触发代码

```
    public void testIssue1234() throws Exception
    {
       String content1 = ""管理员张三"";
        System.out.println(NLPTokenizer.segment(content1));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
```
期望输出：管理员，张三
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出：管理员，张，三
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
结果截图
方式一：
![qq 20180402162824](https://user-images.githubusercontent.com/18030444/38189476-f09255ba-3692-11e8-998a-21ccb11115ca.png)
方式二：
![qq 20180402162834](https://user-images.githubusercontent.com/18030444/38189477-f0d07fe8-3692-11e8-81e7-653578c53bc9.png)

"
命名实体识别后分词不理想,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是： master主分支

我新建了实体识别楼盘名，ViterbiSegment 分词 “地址是星海城三期”，
识别出楼名 
[星海城/nbd] 和 [星海城三期/nbd]，但分词结果却是
[地址/n, 是/vshi, 星海/n, 城/n, 三期/nbdp]

请问如何使结果为
[地址/n, 是/vshi, 星海城三期/nbd]"
依存分析设置使用感知机分词之后，标点符号的依存关系错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.1
我使用的版本是：maven  上的 portable 1.6.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

       依存句法分析可以设置分词器，在设置为感知机分词器之后，标点符号的依存关系不再是之前的 **标点符号** 关系，而是 **动宾关系** 之类。


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```java
        String test = ""什么是自然语言处理?"";
        IDependencyParser parser = new NeuralNetworkDependencyParser();
        PerceptronLexicalAnalyzer segmenter = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        parser.setSegment(segmenter);
        CoNLLSentence sentence = parser.parse(test);
        System.out.println(sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
        CoNLLWord[] wordArray = sentence.getWordArray();
        System.out.println(""-------0000----"");
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
1	什么	什么	r	ry	_	2	主谓关系	_	_
2	是	是	v	vshi	_	0	核心关系	_	_
3	自然语言处理	自然语言处理	nz	nz	_	2	动宾关系	_	_
4	?	?	wp	w	_	2	标点符号	_	_

什么 --(主谓关系)--> 是
是 --(核心关系)--> ##核心##
自然语言处理 --(动宾关系)--> 是
? --(标点符号)--> 是
-------0000----

```


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
1	什么	什么	r	r	_	2	主谓关系	_	_
2	是	是	v	v	_	0	核心关系	_	_
3	自然语言	自然语言	n	n	_	4	主谓关系	_	_
4	处理	处理	v	vn	_	2	动宾关系	_	_
5	?	?	n	n	_	4	动宾关系	_	_

什么 --(主谓关系)--> 是
是 --(核心关系)--> ##核心##
自然语言 --(主谓关系)--> 处理
处理 --(动宾关系)--> 是
? --(动宾关系)--> 处理
-------0000----
```
## 其他信息

依存句法分析的默认分词器对标点符号的依存分析是正确的。

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于感知机分词offset为0的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：1.6.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1.6.0后HanLP推出了感知机分词，非常感谢作者的辛苦付出。我的问题：
1、感知机分词没有对Term做偏移标记？Term中的offset现在都是0，是实现起来很困难吗？
2、如果感知机分词能实现offset，是否有计划在下一版增加这一功能，大致是什么时间？
3、有无可能通过HanLP.newSegment(int segType)提供一个工厂方法，来创建感知机分词？

### 触发代码

```
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
单个词的词性识别，是否可标注多个词性,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

关于词性标注，是否一个词可标注多个词性，例如：汪洋，可以是人名也可以是汪洋\n 大海\n

"
我自己也在做词典的命名实体，想知道哪里有命名实体的标注规则文档，还是说这些是根据自己的需求来定,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于“本周X”分词错误,"## 版本号

当前最新版本号是：
我使用的版本是：
1.6.0

### 触发代码

```
        System.out.println(HanLP.segment(""本周一我去上班的时候""));
        System.out.println(HanLP.segment(""本周二我去上班的时候""));
        System.out.println(HanLP.segment(""本周三我去上班的时候""));
        System.out.println(HanLP.segment(""本周四我去上班的时候""));
        System.out.println(HanLP.segment(""本周五我去上班的时候""));
        System.out.println(HanLP.segment(""本周六我去上班的时候""));
        System.out.println(HanLP.segment(""本周日我去上班的时候""));
```

### 期望输出
```
本/周一/我/去
```

### 实际输出
```
[本/r, 周一我/nr, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周二/t, 我/r, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周三我/nr, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周四我/nr, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周五/t, 我/r, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周六/t, 我/r, 去/v, 上班/v, 的/uj, 时候/n]
[本/r, 周日我/nr, 去/v, 上班/v, 的/uj, 时候/n]
```

"
关于hanlp分词中/++/.!=\\等符号以及符号组~@#$%^&的符号合成新词问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.0
我使用的版本是：1.5.2 / 1.6.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

利用hanlp分词的时候，英文半角符号组/,?,!,+,|,\,=,. 、符号组~,@,#,$,%,^&，都会自动拼接在一起，组成新词


### 触发代码

startJVM(getDefaultJVMPath(),""-Djava.class.path=C:/Users/reading-magic/Desktop/Work/Cut/hanlp-1.6.0.jar;\
C:/Users/reading-magic/Desktop/Work/Cut"",""-Xms1g"", ""-Xmx1g"")
HanLP = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
hanlp_words=HanLP.segment(""c#/.net,|?.*/\\=!,~@#$%^&"")
for i in hanlp_words:
    print(i)

### 期望输出
期望输出的符号能够分开
### 期望输出

c#    /nx
/    /w
.    /w
net    /w
,    /w
|    /w
.    /w
”*“   /w
?   /w
/    /w
\    /w
\    /w
=   /w
!    /w
,    /w
~     /nx
@   /nx
“#”   /nx
$   /nx
%   /nx
^   /nx
&   /nx

### 实际输出

实际上把对应词性为w和nx的词分别合成在一起，
```
实际输出
```
c#    /nx,
 /.    /w,
 net   /nx,
 ,|?.*/\=!,    /w
  ~@#$%^&     /nx
## 其他信息
”*“跟”#“会与html或者css冲突
利用了python调用了java 程序，同时在词典里找不到相应的信息
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/22814402/37819529-baf80648-2eb8-11e8-8e33-3519e5ac91f9.png)

![image](https://user-images.githubusercontent.com/22814402/37819522-b5b7bf20-2eb8-11e8-9969-4b8adf8b1043.png)
"
"分词错误：""为什么我扔出的瓶子没有人回复？""","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.0
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
对句子“为什么我扔出的瓶子没有人回复？”分词错误

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        List<Term> terms = HanLP.segment(""为什么我扔出的瓶子没有人回复？"");
        System.out.println(terms );
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[为什么, 我, 扔出, 的, 瓶子, 没有,人, 回复, ？]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[为什么, 我, 扔出, 的, 瓶子, 没,有人, 回复, ？]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
文本分类：贝叶斯统计中的特征选择可否可视化？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.0
我使用的版本是：hanlp-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
贝叶斯统计中使用卡方检测选择特征，控制台会输出类似“使用卡方检测选择特征中...耗时 80 ms,选中特征数:5857 / 15110 = 38.76%”的语句，但是我不能够看到它到底选择了哪些特征词汇。
举个例子，假如我利用情感分析对一些评论进行了分类，有正面和负面，但单单这样分类对我没有实际作用，我想知道这些正面、负面评论有哪些特点，这些负面评论又是在不满些什么东西，比如评论“早餐都凉了，我吃了肚子痛。”我就知道这个评论在不满早餐，进而对早餐做改进。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改代码，但我只能看到情感分析和贝叶斯统计的结果，不知道我可以看到其中的过程吗？
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

package com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier
....
....
protected BaseFeatureData selectFeatures(IDataSet dataSet)
    {
        ChiSquareFeatureExtractor chiSquareFeatureExtractor = new ChiSquareFeatureExtractor();

        logger.start(""使用卡方检测选择特征中..."");
        //FeatureStats对象包含文档中所有特征及其统计信息
        BaseFeatureData featureData = chiSquareFeatureExtractor.extractBasicFeatureData(dataSet); //执行统计

        //我们传入这些统计信息到特征选择算法中，得到特征与其分值
        Map<Integer, Double> selectedFeatures = chiSquareFeatureExtractor.chi_square(featureData);

        //从统计数据中删掉无用的特征并重建特征映射表
        int[][] featureCategoryJointCount = new int[selectedFeatures.size()][];
        featureData.wordIdTrie = new BinTrie<Integer>();
        String[] wordIdArray = dataSet.getLexicon().getWordIdArray();
        int p = -1;
        for (Integer feature : selectedFeatures.keySet())
        {
            featureCategoryJointCount[++p] = featureData.featureCategoryJointCount[feature];
            featureData.wordIdTrie.put(wordIdArray[feature], p);
        }
        logger.finish("",选中特征数:%d / %d = %.2f%%\n"", featureCategoryJointCount.length,
                      featureData.featureCategoryJointCount.length,
                      featureCategoryJointCount.length / (double)featureData.featureCategoryJointCount.length * 100.);
        featureData.featureCategoryJointCount = featureCategoryJointCount;

        return featureData;
    }
....
....


### 期望输出

<!-- 你希望输出什么样的正确结果？-->

希望看到正面评论之所以是正面评论的依据，比如特征与其分值？
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

正在从 data/test/ChnSentiCorp情感分析酒店评论 中加载分类语料...
正面 : 2000 个文档
负面 : 2000 个文档
开始训练...
正在构造训练数据集...[正面]...50.00%...[负面]...100.00%...耗时 6448 ms 加载完毕
原始数据集大小:4000
使用卡方检测选择特征中...耗时 80 ms,选中特征数:5857 / 15110 = 38.76%
贝叶斯统计结束
训练耗时：6689 ms

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
”扎心“多音字拼音,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.4
我使用的版本是：1.5.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
“扎心”得到的拼音是”za xin“
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码
```
System.out.println (HanLP.convertToPinyinString(""扎心"", "" "", true));

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
zha xin
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
za xin
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请问新词发现的代码是在哪个地方呢？,看到介绍里有新词发现功能，但是如何调用呢？是直接集成在了分词里面吗？可以单独输出吗？
python中使用hanlp，金华的词性，识别出来为nr，而不是ns,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.5.4
我使用的版本是：hanlp-portable-1.5.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我在python中使用hanlp来识别一段文字中出现的地区，而且大部分地区可以正常识别出来，但是金华识别出来的词性总是nr

### 触发代码

```
hanlp = JClass('com.hankcs.hanlp.HanLP')
segment = hanlp.newSegment().enablePlaceRecognize(True)
term_list = segment.seg(u'我在浙江金华出生')

for name in term_list:
    print(name.toString())
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
我/r
在/p
浙江/ns
金华/ns
出生/v
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
识别的金华词性为人名（nr）而不是地名（ns）
```
我/r
在/p
浙江/ns
金华/nr  
出生/v
```

## 其他信息
不知道是否是因为 HanLP/data/dictionary/CoreNatureDictionary.mini.txt
文件中，标识了金华的词性为nr引起的
![cosmos](https://user-images.githubusercontent.com/11661935/37083415-5ac2ccbe-21e7-11e8-8020-e6f022640af8.png)


"
索引模式下全切分不准确,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

IndexTokenizer索引模式全切分的问题，
如“我爱中华人民共和国”，
**实际输出**
“我”，“爱”，“中华人民共和国”，”中华人民 ”，”中华”，”华人” ，”人民共和国 ”，”人民”，”共和国 ”，”共和” 。
”人民政府”,”人民”,”政府”,”民政”
**期望输出**
“我”，“爱”，“中华人民共和国”，”中华人民 ”，”中华”，”人民共和国 ”，”人民”，”共和国 ”，”共和” 。
”人民政府”,”人民”,”政府”

viterbi取得最优解“中华人民共和国”后，然后**完全根据词库进行细切**的，故分出来“华人”，”民政”，等明显为bad case。
关于fix这个问题，您有什么建议吗？



"
运行DemoTextClassificationFMeasure. java，看不懂输出。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
master分支

当前最新版本号是：hanlp-1.5.4.jar
我使用的版本是：hanlp-1.5.3.jar

<!--以上属于必填项，以下可自由发挥-->

【问题】
运行开源项目中的DemoTextClassificationFMeasure. java（演示了分割训练集和测试集,进行更严谨的测试）
可是我看不懂输出，不知道这个测试和这些数据意味着什么，可以解释一下吗？

【实际输出】
模式:训练集
文本编码:UTF-8
根目录:data/test/ChnSentiCorp情感分析酒店评论
加载中...
[正面]...100.00% 1800 篇文档
[负面]...100.00% 1800 篇文档
耗时 21797 ms 加载了 2 个类目,共 3600 篇文档
原始数据集大小:3600
使用卡方检测选择特征中...耗时 2504 ms,选中特征数:5486 / 14103 = 38.90%
贝叶斯统计结束
模式:测试集
文本编码:UTF-8
根目录:data/test/ChnSentiCorp情感分析酒店评论
加载中...
[正面]...100.00% 200 篇文档
[负面]...100.00% 200 篇文档
耗时 1541 ms 加载了 2 个类目,共 400 篇文档
     P	          R	           F1	           A	      
 82.63	 88.00	 85.23	 84.75	正面
 87.17	 81.50	 84.24	 84.75	负面
 84.90	 84.75	 84.82	 84.75	avg.
data size = 400, speed = 44444.44 doc/s

请问下面这部分怎么理解，有什么意义？
     P------R-----F1----A	      
 82.63	 88.00	 85.23	 84.75	正面
 87.17	 81.50	 84.24	 84.75	负面
 84.90	 84.75	 84.82	 84.75	avg.
data size = 400, speed = 44444.44 doc/s
"
你好，请问HanLP如何分词并且统计词频,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
可否提供一个命令行可执行的jar包？,类似于[斯坦福NLP](https://stanfordnlp.github.io/CoreNLP/cmdline.html)的:-)
请问有没有办法做到语义上的相似度匹配,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [*] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.5.3
我使用的版本是：1.5.3

## 我的问题
在做短文本相似度匹配时，我希望优先匹配语义相近的。如：
**不喜欢** 匹配到 **讨厌**，而不是匹配到 **喜欢**。
请问有没有办法实现？

## 复现问题

### 步骤

### 触发代码

```
    DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(modelFileName));

		String[] documents = new String[]{
		        ""农民在江苏种水稻"",
		        ""山东苹果丰收"",
		        ""我很喜欢篮球"",
		        ""我很讨厌篮球"",
		        ""奥运会女排夺冠"",
		        ""世界锦标赛胜出""
		};

		for (int i = 0; i < documents.length; i++)
		{
		    docVectorModel.addDocument(i, documents[i]);
		}
	    System.out.println(docVectorModel.nearest(""我不喜欢篮球""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
不喜欢能先匹配到讨厌
```
[3=0.99999976, 2=0.87721485
```

### 实际输出
不喜欢先匹配到了 喜欢

```
[2=0.99999976, 3=0.87721485
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
常见词重叠，被误识别为机构的,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(false).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
        System.out.println(StandardTokenizer.segment(""辖区有中学和小学小学各1所。""));
    }
```
### 期望输出

[辖区/n, 有/vyou, 中学/nt, 和/cc, 小学/nt, 小学/nt, 各/rz, 1所/nt, 。/w]

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[辖区/n, 有中学/nt, 和/cc, 小学小学/nt, 各/rz, 1所/nt, 。/w]
```

## 其他信息

机构识别出差的问题，类似的很多，比如句子“战争下会无缘无故打起来”，“不”写错为“下”，把“战争下会”拼为nt了

"
新词发现,想询问一下目前HanLP有没有提供新词发现的功能
词网与词图的区别,"请教一下词网和词图这两种数据结构分别适用于怎样的算法。
在ViterbiSegment中使用了词网结构，每一行都是前缀词链。我觉得使用像DijkstraSegment中一样的词图，然后依次对节点选择最优解应该也可以实现Viterbi最短路切分。
那么，另外又定义的词网结构的意义是什么，它在效率上是更优吗？"
请问怎么自定义简繁转换的词典,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.5.3
我使用的版本是：portable-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
你好，请教个问题，因为我们公司的商标台陆通，台不转繁体，只有陆转繁体，所以想知道是否能通过自定义的方式解决这种特定情况下的简繁转换问题，默认情况下台也会被转成繁体，但这不是我们要的效果，该如何解决
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
你好，请问这可以做三元组抽取吗?,"你好，请问这可以做三元组抽取吗?
 比如：
张三在湖南长大，出生于1999年1月一日。
我想提出：张三出生于1999年1月一日

这个能抽取吗？或者作者可否指点一下，给一下思路，谢谢。"
最新版本（1.5.3）问题:同样调用HanLP.segment()方式一和方式二分词结果不一致,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
最新版本（1.5.3），按照说明分别配置，分词结果不一样
System.out.println(HanLP.segment(""把两张车票换成三张""));
方式一(maven)：[把/p, 两/m, 张/q, 车票/n, 换/v, 成三张/nr]
方式二(自定义)：[把/pba, 两/m, 张/q, 车票/n, 换成/v, 三/m, 张/q]

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
1.方式一：直接用maven 引，即：
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.5.3</version>
</dependency>
2.方式二：下载源码和data.zip数据配置
3.测试分词：System.out.println(HanLP.segment(""把两张车票换成三张""));

### 触发代码：System.out.println(HanLP.segment(""把两张车票换成三张""));

### 期望输出：[把/pba, 两/m, 张/q, 车票/n, 换成/v, 三/m, 张/q]

### 实际输出：[把/p, 两/m, 张/q, 车票/n, 换/v, 成三张/nr]
"
自定义词典 持久化问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1、自定义词典持久化好像官方没有给出最优做法，参考 问题  [#182#](https://github.com/hankcs/HanLP/issues/182)  需要维护 CustomDictionary.txt 文件，是需要自己通过IO操作文件 ； 

2、在配置文件中增加 CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; **# mydict.txt;** 自定义词典文件，没有效果，生成的缓存文件也没有，是否还需要有其他操作，或者需要单独生生成 

 
"
您好，请问有没有非全局的用户词典，即分词时可动态指定是用哪个用户词典,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.53
我使用的版本是：1.53

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好，请问有没有非全局的用户词典，即分词时可动态指定使用哪个用户词典，类似ANSJ分词时可指定
存储着自定义词集合的Forset参数
"
 HanLP对短文本（30字以内）分词效率低，如何解决？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.5.3
我使用的版本是：v1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
1. 使用HanLP对短文本（30字以内），分词效率在300ms-500ms之间。属于正常情况吗？
2. 如果我想提高分词效率在50ms左右（30字左右的短文本），但又不想用极速分词算法，有什么其他办法？是需要配置吗？
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我使用了Portable版本和600多M的data数据都得到了同样的结果，分词效率都在300ms-500ms之间。
### 步骤

如下代码所示：

### 触发代码

```
    public static void main(String[] args)
    {
        long current = System.currentTimeMillis();
        List<Term> termList = HanLP.segment(""帮我找找最近3天在三峡大坝拍的无人机照片"");
        long end = System.currentTimeMillis();
        System.out.println(end-current);
    }

   或者这样：
   public static void main(String[] args)
    {
        // TODO Auto-generated method stub
        Segment segment = HanLP.newSegment().enableAllNamedEntityRecognize(true).enableMultithreading(true).enableNumberQuantifierRecognize(true).enablePartOfSpeechTagging(true);
        long current = System.currentTimeMillis();
        List<Term> termList = segment.seg(""帮我找找最近3天在三峡大坝拍的无人机照片"");
        long end = System.currentTimeMillis();
        System.out.println(end-current);
    }

  
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
希望耗时在50ms以内。
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
300ms-500ms
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
“来张北京的车票“ 分词为 “张北” “京“,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [X ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
portable版
当前最新版本号是：v1.5.3
我使用的版本是：v1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

你好，这半年一直在用你开发的分词器做实验，感觉很好用。但是今天发现 “来张北京的车票”，无论用包中的几种分词器都分不出 “ 北京”  ，基本都分成  “”张北“  “京”  。能否赐教怎么解决这个问题？我已经向自定义辞典添加了“北京“  “来张“”，但是无效。谢谢！

## 复现问题
没有修改代码，直接调用这几个分词器

### 步骤

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
[来张,北京,的,车票]
(我忽略了词性)
### 实际输出

[来张北京/nr, 的/ude1, 车票/n]
[来张北京/nr, 的/ude1, 车票/n]
[来张北京/nr, 的/ude1, 车票/n]
[来/null, 张北/null, 京/null, 的/null, 车票/null]

```


"
同义词词典第一句开头包含\U+FEFF,"1. 我使用的mave依赖包版本号
        <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.5.3</version>
        </dependency>

2. 发现有个问题是同义词词典开头出现一个非占位空格\U+FEFF
![image](https://user-images.githubusercontent.com/5870269/35026499-59e0c7c4-fb86-11e7-90ef-d2862f224090.png)
【【 疑问 】】
UTF-8 with BOM这种格式的同义词词典在加载的时候，会将第一个字符当做args[0]来计算id值，那么这样计算出来的ID值是不是存在问题？
![image](https://user-images.githubusercontent.com/5870269/35042223-b80fc476-fbc2-11e7-8030-3dedacaf369e.png)

"
HMM-NGram分词模型等效词替换后如何保留的原词？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
master
当前最新版本号是：
1.5.3
我使用的版本是：
1.5.3
<!--以上属于必填项，以下可自由发挥-->

## 我的问题
二元接续词典地名人名等被替换成了“等效词”，这样在训练1998年语料的时候像“中国”，“北京”等地名都成了等效词。如下面：
`未##串	x	130296

未##人	nr	607718	nrf	113445

未##团	nt	112253	ntc	25517	nto	18894	ntu	5426	nth	2556	ntcb	1846	nts	677	ntch	568	ntcf	118

未##地	ns	595380	nsf	124178

未##它	xx	1000

未##数	mq	753456	m	733982

未##时	t	757118`
我训练后的结果也同上面一样，被替换后已有的词“中国”就没有了呀，但是你的二元接续词典里还有原词，比如：
`中国	ns	39573`

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   /**
 * 等效词编译器
 * @author hankcs
 */
public class PosTagCompiler
{
    /**
     * 编译，比如将词性为数词的转为##数##
     * @param tag 标签
     * @param name 原词
     * @return 编译后的等效词
     */
    public static String compile(String tag, String name)
    {
        if (tag.startsWith(""m"")) return Predefine.TAG_NUMBER;
        else if (tag.startsWith(""nr"")) return Predefine.TAG_PEOPLE;
        else if (tag.startsWith(""ns"")) return Predefine.TAG_PLACE;
        else if (tag.startsWith(""nt"")) return Predefine.TAG_GROUP;
        else if (tag.startsWith(""t"")) return Predefine.TAG_TIME;
        else if (tag.equals(""x"")) return Predefine.TAG_CLUSTER;
        else if (tag.equals(""nx"")) return Predefine.TAG_PROPER;
        else if (tag.equals(""xx"")) return Predefine.TAG_OTHER;
        return name;
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
期望既有等效词，也有地名等原词的条件转移词典，我看hankcs的CoreNatureDictionary.txt也是有的
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
CoreNatureDictionary.txt未输出：
`中国	ns	39573`
**想问下hankcs的训练的词典为什么既有等效词，也有地名等原来的词，是一个综合的结果呢?
我训练数据少了哪一步呢，跟你的结果不一样**
## 其他信息
我指的词典主要指的是：CoreNatureDictionary.txt，CoreNatureDictionary.ngram.txt，CoreNatureDictionary.tr.txt
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
使用CRFSegment进行分词时，出现java.lang.NullPointerException,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1. 下载data-for-1.5.3.zip，解压后上传data目录到hdfs上；
2. 继承IIOAdapter来读取hdfs上的文件；
3. hanlp.properties放在src/main/resources目录下，其中指定了CRFSegmentModelPath和各个词典的路径，以及IOAdapter；
4. 程序运行时，crf模型加载成功(不然会报找不到模型文件error)，但map算子中调用segment()进行分词时，日志中报空指针异常。

### 触发代码

```
    val segment = new CRFSegment()
          .enableNameRecognize(true)
          .enableTranslatedNameRecognize(true)
          .enableJapaneseNameRecognize(true)
          .enablePlaceRecognize(true)
          .enableOrganizationRecognize(true)
          .enablePartOfSpeechTagging(true)
          .enableCustomDictionary(true)
   segment.seg(text)
```

## 其他信息
Caused by: java.lang.NullPointerException
    at com.hankcs.hanlp.algorithm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.CharacterBasedGenerativeModelSegment.segSentence(CharacterBasedGenerativeModelSegment.java:81)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:507)

"
怎么将CRF分词模型存放在hdfs上，然后spark运行时进行加载呢？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1. 我这边在spark上运行时，出现CRF分词模型加载 data/model/segment/CRFSegmentModel.txt 失败，我知道是没有在hanlp.properties中配置模型文件路径。
2. 我想让spark集群程序在运行时加载hdfs上的crf分词模型文件，后续要怎么做？

"
如何使用CRF解码用CRF++训练生成的模型,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用CRF++解码用CRF++训练生成的模型进行实体识别时，如果开启多线程运行，则会报错，猜测是其c++底层有错误。
HanLP使用了CRF解码，但是只用于分词。
请问如果要用CRF解码用CRF++训练生成的模型进行实体识别，应该参考哪些部分，或者可以复用CRF解码用于分词的哪些思路吗？
<!-- 请详细描述问题，越详细越可能得到解决 -->"
依存方法分析中时间词的词性未能正确标注,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在对以下句子进行分析时：

我的太太小张今天感冒了

通过segment得到词性如下：
 ['我/rr', '的/ude1', '太太/n', '小张/n', '今天/t', '感冒/vi', '了/ule']
这里的今天被解析成为时间词，正确。

在依存分析中：
   1     我/rr 	 3	定中关系 
   2     的/ude1	 1	右附加关系
   3    太太/n  	 6	主谓关系 
   4    小张/n  	 6	主谓关系 
   5    今天/n  	 6	主谓关系 
   6    感冒/vi 	 0	核心关系 
   7     了/ule	 6	右附加关系

这里今天被解析成名词，虽未出错，但不够精细，导致两者不一致的原因是？

## 复现问题
我使用python，通过jpype来调用 Hanlp

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
依存方法分析的初始化问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我通过HanLP.parseDependency调用依存方法分析，经查看，这是一个静态方法，估计每次调用时都要加载模型。有没有办法可以将parser的实例保存起来以避免下次调用时再加载模型呢？

注：我通过Jpype (python)来使用HanLP

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Viterbi分词器模型异常,"
1、待分词文本：陶某、陶某某路经该处时
    public static void main(String args[]) {
        HanLP.Config.DEBUG = true;
        Segment segment = new ViterbiSegment();
        List<Term> termList = segment.seg(""陶某、陶某某路经该处时"");
        System.out.println(termList);
    }
经查看日志，陶某人名已经识别出，可识别结果依然为：
[陶/ag, 某/rz, 、/w, 陶/ag, 某/rz, 某/rz, 路经/v, 该/rz, 处/n, 时/qt]

程序运行日志为：
粗分结果[陶/ag, 某/rz, 、/w, 陶/ag, 某/rz, 某/rz, 路经/v, 该/rz, 处/n, 时/qt]
人名角色观察：[  K 1 A 1 ][陶 B 771 D 30 C 15 E 9 ][某 G 2055 C 1082 D 420 L 34 K 7 ][、 M 19857 L 5234 K 4094 ][陶 B 771 D 30 C 15 E 9 ][某 G 2055 C 1082 D 420 L 34 K 7 ][某 G 2055 C 1082 D 420 L 34 K 7 ][路经 A 20843310 ][该 K 18 L 13 ][处 L 52 K 10 D 9 ][时 L 228 C 137 D 134 K 106 B 88 ][  K 1 A 1 ]
人名角色标注：[ /K ,陶/B ,某/G ,、/M ,陶/B ,某/C ,某/L ,路经/A ,该/K ,处/D ,时/L , /K]
识别出人名：陶某 BG
识别出人名：陶某 BC

和预期结果不符合。


2、运用公式可能有误
经检查代码片段中，计算转移概率公式（见MathTools.calculateWeight方法）：
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

该公式经推敲，多乘了一个frequency ，由于是底层算法，不知理解是否有误。

我将公式修改为：
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

重新运行代码得到如下结果：
[陶某/nr, 、/w, 陶某某/nr, 路经/v, 该/rz, 处/n, 时/qt]
满足预期结果

3、由于是底层算法代码，不知是否修改有误，请指导！
"
机构识别角色标注是否不全。比如K、P没找到对应的意义,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息
![image](https://user-images.githubusercontent.com/30866940/34856575-ca7ea6ae-f780-11e7-9d4d-c9afbfaa567a.png

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于resize(65536 * 32)的数字大小问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我在看hanlp的源码时，看到在`src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java`中`resize(65536 * 32); `。 这段代码的目的是为了生成双数组而设置数组的初始大小。我想请问为什么要设置这么大呢。65536是根据char的最大值设定，而32是为了什么原因选中此数值呢？
"
分词问题：自定义词典有时会失效,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.5.3
我使用的版本是：hanlp-portable-1.3.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

自定义词典在某种情况下会失效，对“海南省海口市龙华区春来早市场”进行分词，结果正常，但是增加一个方位词内字之后会出现分词异常的情况

## 复现问题
我自定义的词典：
```
海南省 dictrict1 1000
海口市 dictrict2 1000
龙华区 dictrict3 1000
春来早市场 resrge 1000
```
```
   List<Term> termList = HanLP.segment(""海南省海口市龙华区春来早市场"");
    //分词结果：海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春来早市场/resrge
```
此时说明 “春来早市场”是正确加载的了

### 触发代码

```
   List<Term> termList = HanLP.segment(""海南省海口市龙华区春来早市场内"");
```
### 期望输出

```
海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春来早市场/resrge, 内/s
```

### 实际输出

```
海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春/tg, 来/v, 早市/n, 场内/s
```
增加了“内”字，重新分词，发现分词效果与预期的差别比较大，这种情况应该怎么处理呢





  
  "
"姓名前面有句号，导致姓名识别错误：""。杨瑞云告诉大家，慢慢的，""","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

对句子前面有句号，分词识别人名错误。删除句号，识别正确。句子如下：
。杨瑞云告诉大家，慢慢的，

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[。/w, 杨瑞云/nr, 告诉/v, 大家/rr, ，/w, 慢慢/d, 的/ude1, ，/w]
```

### 实际输出

```
[。/w, 杨瑞/nr, 云/vg, 告诉/v, 大家/rr, ，/w, 慢慢/d, 的/ude1, ，/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
句法依存分析错误一例,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.5.2

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
对下列句子进行依存关系分析：
他那样有身价的人能干出这样的事？
得到如下结果：
1	他	他	r	rr	_	3	主谓关系	_	_
2	那样	那样	r	rzv	_	3	状中结构	_	_
3	有	有	v	vyou	_	6	定中关系	_	_
4	身价	身价	n	n	_	3	动宾关系	_	_
5	的	的	u	ude1	_	3	右附加关系	_	_
6	人	人	n	n	_	11	定中关系	_	_
7	能干	能干	a	a	_	11	定中关系	_	_
8	出	出	v	vf	_	7	动补结构	_	_
9	这样	这样	r	rzv	_	11	定中关系	_	_
10	的	的	u	ude1	_	9	右附加关系	_	_
11	事	事	n	n	_	0	核心关系	_	_
12	？	？	wp	w	_	11	标点符号	_	_
这个结果应该是错的吧？核心关系应该是”干出“吧？


"
在Android上使用外挂字典遇到sun.reflect.ReflectionFactory类找不到的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.5.3
我使用的版本是：portable-1.3.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在Android使用hanlp.properties配置外挂字典时，遇到问题：
01-04 21:49:47.031 1196-2513/com.nio.nlu W/HanLP: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
                                                  如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
                                                  
                                                  --------- beginning of crash
01-04 21:49:47.033 1196-2513/com.nio.nlu E/AndroidRuntime: FATAL EXCEPTION: Thread-66
                                                           Process: com.nio.nlu, PID: 1196
                                                           Theme: themes:{}
                                                           java.lang.NoClassDefFoundError: Failed resolution of: Lsun/reflect/ReflectionFactory;
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34)
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43)
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50)
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202)
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505)
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561)
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105)
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java)
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95)
                                                               at java.lang.Thread.run(Thread.java:818)
                                                            Caused by: java.lang.ClassNotFoundException: Didn't find class ""sun.reflect.ReflectionFactory"" on path: DexPathList[[zip file ""/system/app/NluService/NluService.apk""],nativeLibraryDirectories=[/system/app/NluService/lib/arm, /system/app/NluService/NluService.apk!/lib/armeabi-v7a, /vendor/lib, /system/lib]]
                                                               at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:56)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:511)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:469)
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34) 
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43) 
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50) 
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202) 
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57) 
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505) 
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561) 
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105) 
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java) 
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95) 
                                                               at java.lang.Thread.run(Thread.java:818) 
                                                           	Suppressed: java.lang.ClassNotFoundException: sun.reflect.ReflectionFactory
                                                               at java.lang.Class.classForName(Native Method)
                                                               at java.lang.BootClassLoader.findClass(ClassLoader.java:781)
                                                               at java.lang.BootClassLoader.loadClass(ClassLoader.java:841)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:504)
                                                               		... 15 more
                                                            Caused by: java.lang.NoClassDefFoundError: Class not found using the boot class loader; no stack trace available



## 其他信息
是否是Android的java平台平台本身就不包含这个类，是否可以用java.lang.reflect替代sun.reflect？
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"建议核心词典添加""地 n""这个词条","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：mater
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

广州等地政府发行的债券大面积出现了问题。
""地""会识别为ude2词性，明显不对

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->


  "
常见姓名识别出错：赵红,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

原句：侦查员核实了信用卡持有人赵红的情况后，

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
侦查员/nnt, 核实/v, 了/ule, 信用卡/n, 持有人/nz, 赵红/nr, 的/ude1, 情况/n, 后/f, ，/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
侦查员/nnt, 核实/v, 了/ule, 信用卡/n, 持有人/nz, 赵/nz, 红/a, 的/ude1, 情况/n, 后/f, ，/w
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->


  "
常见短语误识别为机构：“公开政府”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

原句：应当及时、准确地公开政府信息。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
应当/v, 及时/ad, 、/w, 准确/ad, 地/ude2, 公开/v, 政府/n, 信息/n, 。/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
应当/v, 及时/ad, 、/w, 准确/a, 地/ude2, 公开政府/ntc, 信息/n, 。/w
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
依存分析是否线程安全,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ &radic;] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2 离线

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

问题很简单，文档说了HanLp的分词是线程安全的，那依存分析是不是线程安全的呢？使用在多线程中使用依存分析需要注意什么？



  "
关于商品型号（字母，分隔符，数据的组合）的分词效果探讨,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2 离线

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

采用标准分词，进行一些包含商品描述的语句进行切分。商品型号通常是任意的字母，分隔符，数据的组合。不是人名、地名、机构等。
进行分词时，可能最终结果粒度过细。而希望是一个整体。
由于型号在不断增加，比较笨的做法是时候添加词库，但那样工作量比较大。较为繁琐。


## 复现问题

Sample: SAMSUNG 三星 **RS542NCAEWW/SC** 545L 风冷变频对开门冰箱，还行还行，可以看看;
分词结果： [SAMSUNG][ ][三星][ ][RS][542][NCAEWW][/][SC][ ][545][L][ ][风冷][变频][对开门冰箱][\][n][还行][还行][，][可以][看看][;]

### 期望输出

[SAMSUNG][ ][三星][ ][RS542NCAEWW/]SC[ ][545][L][ ][风冷][变频][对开门冰箱][\][n][还行][还行][，][可以][看看][;]

即。希望是一个整体 “**RS542NCAEWW/]SC**”


## 其他信息

这种例子有很多，如 
WD 西部数据 **WD20EZRZ** 台式机硬盘 蓝盘 2TB

希望 “WD20EZRZ ” 整体切分

NORITZ 能率 **JSQ25-E4/GQ-13E4AFEX** 13升燃气热水器防冻型

希望 ‘’JSQ25-E4“ 整体切分

  "
调用依存语法分析接口CRFDependencyParser.compute(sentence)；出现返回结果死循环情况,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：hanlp-portable-1.5.2.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
调用依存语法分析接口CRFDependencyParser.compute(sentence)；出现返回结果死循环情况
返回结果中id为5的CoNLLWord的head为6，id为6的CoNLLWord的head为5。

## 复现问题
没有修改源码直接调用CRFDependencyParser.compute(sentence)接口；

### 触发代码

```
    public void testCompute() throws Exception
    {
        String sentence = ""男子从厦门游泳到金门"";
        CoNLLSentence cs = CRFDependencyParser.compute(sentence);
        System.out.println(cs.word[4].HEAD.ID);
        System.out.println(cs.word[5].HEAD.ID);
    }
```
### 期望输出

```
0
5
```

### 实际输出

```
6
5
```"
用于姓名识别改进的讨论Issue,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

之前由于工作上需要，研究了一下HanLP的实体识别存在的不足。主要是误召回比较高。也尝试了一些方法修改，能解决部分问题。准备在我的分支上面修改再PR。所以这里开一个Issue进行讨论。本Issue不太涉及具体识别错误的情况，主要用于收集反馈和讨论。

我在我的Fork新建了一个[Project](https://github.com/TylunasLi/HanLP/projects)：

### 触发代码

```
    public void testIssue729() throws Exception
    {
        String[] sentences = new String[]{
                ""贷款的钱打给谁"",
                ""今天钱扣了有逾期吗""
        };
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enablePlaceRecognize(true).enableOffset(true);
        for (String sentence : sentences)
                systme.out.println(segment.seg(sentence));
    }
```

### 期望输出


```
期望输出
```

### 实际输出


```
实际输出
```

## 其他信息


"
修复 Issue 691 以及调试时看到的一个Bug,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

1. 用调试器察看BinTrie结构时会发生报错，测试发现主要原因是BaseNode中的toString()没判空，所以这里修改一下；
2. Issue #691 N-最短路径分词中不会将独立的逗号词性变成 /m，但可以识别带千位分隔符的数字。
（修改了原子切分的逻辑。如果希望两个数字中间有逗号时分词分开，需要修改这段逻辑。数字识别最好做成状态机。）

另外很不好意思，我按照Github Help上的方法同步fork，但操作出了一些问题，导致您近期提交的Issue全被刷了一遍。看来以后还是老实用PR合并吧。

## 相关issue

issue #691
"
如何在索引分词中只使用自定义词典分词,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.5.2
我使用的版本是： 1.3.4

## 我的问题

您好，我希望在索引分词中只使用自定义的词典进行分词，关闭其他词典。请问有何方法吗？
因为索引分词中支持全切分，因此我希望以此来避免基于词典的分词方法只能匹配最长词的缺点。
期待你的回复谢谢。

## 复现问题
字典中为“夏洛特烦恼 n, 夏洛 nr”
List<Term> termList = IndexTokenizer.segment(""夏洛特烦恼"");

### 期望输出

```
夏洛特烦恼/n [0:5]
夏洛/nr [0:2]
```

### 实际输出

```
夏洛特烦恼/n [0:5]
夏洛/nr [0:2]
夏洛特/nrf [0:3]
烦恼/an [3:5]
```
"
人名识别出错：党员来到村民焦玉莲家中,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
人名识别出错：党员来到村民焦玉莲家中
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true).enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
党员/nnt, 来到/v, 村民/n, 焦玉莲/nr, 家中/s
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
党员/nnt, 来到/v, 村民/n, 焦/ng, 玉莲/nz, 家中/s
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何在python中识别日本人名的译名,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<1.5.2；master>

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<如何在python中实现对日本人名译名的分词>

本人使用了http://www.hankcs.com/nlp/python-calls-hanlp.html中在python中使用HanLP的方法，成功复现所有文中提到的功能，如何实现日本人名识别。
我目前在JClass中调用了com.hankcs.hanlp.recognition.nr.JapanesePersonRecognition包，但是不知道使用哪一个方法。

"
发现新词的方法，过滤的时候建议shiyonLinkedList,"   /**
     * 提取词语
     *
     * @param reader 大文本
     * @param size   需要提取词语的数量
     * @return 一个词语列表
     */
    public List<WordInfo> discover(BufferedReader reader, int size) throws IOException

在Line 83行，ArrayList替换为LinkedList，做移除时，会减少remove带来的list左移的复杂度，速度有很大提升
"
依存句法分析相关问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，我想根据自己的领域语料训练一个神经网络句法分析模型，但是没有看到相关资料，麻烦问下您，hanlp中有根据自己的语料生成神经网络句法分析模型的模块么？打扰了，谢谢！
主要是有两个问题不太明确：
（1）句法分析的语料应该如何标注呢，我利用gate可以进行普通的词性和命名实体标注，但是不晓得句法分析的标注方式；
（2）标注后的语料如何转换成hanlp所支持的神经网络句法分析模型。
万分感谢！
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
能关闭对于标点符号的词性标注吗？,"对于标点符号的词性标注不准确。
比如 ： (记者李鑫轶 整理)
分词结果为：(/w	记者/nnt	李鑫/nr	轶/ng	 /n	整理/v	)/w	

空格标注为n，不理解。

能把标点的词性标注关闭了吗？
"
你好，关于支持自定义词库英文空格分隔符,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2 非Portable版本

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

这个问题之前有同学提过，我在其他issue看到过。
就是一些，西方语言、日语等一些词是有空格作为分割符的。
如：
Mystery Ranch nz 1
Altec Lansing Technologies nz 1

## 复现问题
只要自定义词典包含这种词，就无法解析了

##解决办法
1. 动态insert ，但是感觉这样比较慢，希望统一管理
2. 等待新版本修复。 我看这个是今年3月份有计划，但是似乎现在还没有支持呢。
3. 本次的意图：想自行修改代码，自行用别的分隔符进行解析。

"
请问下hanlp模型自带的custom dictionary是根据什么来制定的？,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.5.2, data-for-1.3.3
我使用的版本是：v1.5.2, data-for-1.3.3

<!--以上属于必填项，以下可自由发挥-->
我在使用standardtokenizer 的时候有遇到一个问题， 像“大众”， “现代”， “光明“，不管任何状态下都会识别成ntc， 后来我们有进去看到custom dictionary有对这个调整一些frequency,想请教下这个frequency是怎么设定的。谢谢～～～



"
word2vector功能中，加载词向量模型128维度超出限制,"
## 版本号
当前最新版本号是：portable-1.5.2
我使用的版本是：portable-1.5.2



## 我的问题
加载一个128维的词向量模型，抛出异常
java.lang.ArrayIndexOutOfBoundsException: 128


"
训练最新中文wiki问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
训练最新的zh wiki抛出数组越界异常
## 复现问题

[hadoop@LOCAL-202-89 new]$ java -cp hanlp-portable-1.5.2.jar com.hankcs.hanlp.mining.word2vec.Train -input zhwiki-latest-pages-articles.xml.simplified -output zhwiki.txt

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -3
	at com.hankcs.hanlp.mining.word2vec.TextFileCorpus.reduceVocab(TextFileCorpus.java:69)
	at com.hankcs.hanlp.mining.word2vec.TextFileCorpus.learnVocab(TextFileCorpus.java:142)
	at com.hankcs.hanlp.mining.word2vec.Word2VecTraining.trainModel(Word2VecTraining.java:326)
	at com.hankcs.hanlp.mining.word2vec.Train.execute(Train.java:33)
	at com.hankcs.hanlp.mining.word2vec.Train.main(Train.java:38)"
custom文件夹下的词典问题？比如上海、全国地名等等。,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好！不好意思打扰了。我现在正在使用hanlp，想请问下custom文件夹下的词典都是怎么来的？比如上海、全国地名等等。谢谢~


"
基于CRF的依存句法分析报OutOfMemoryError异常,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我想自己训练一个依存句法分析器，看到你博客上面CRF提供提供了通过CRF++的训练方式，
我使用CRF++训练的一个150M左右的依存句法分析model，替换掉hanLP中的CRFDependencyModelMini.txt.bin，发现报如下错误：
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at com.hankcs.hanlp.model.crf.CRFModel.load(CRFModel.java:334)
	at com.hankcs.hanlp.dependency.CRFDependencyParser$CRFModelForDependency.load(CRFDependencyParser.java:234)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.loadDat(CRFDependencyParser.java:104)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.load(CRFDependencyParser.java:94)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:54)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:67)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:89)
	at dependency.HanLPDependency.main(HanLPDependency.java:10)
"
MutualInformationEntropyPhraseExtractor 实现建议,"我觉得短语提取增加训练语料更合适。如果仅凭输入的文档进行计算，互信息结果肯定不准（尤其是输入比较短的情况）。
实现起来也简单。先通过训练语料计算出有互信息的Occurrence对象。
在提取阶段使用训练好的Occurrence对象。如果没有训练，就使用原算法。"
时间识别不准确，CheckDateElements中的规则存在问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用NShortSegment分词器进行分词，时间识别不准确，存在以下问题：

1. 会将中英文逗号识别为/m类型
2. 如果中英文逗号后紧跟数字，分词时会将逗号和数字合并识别为/m类型
3. 九点这种可以识别为/t类型，但9点误识别为/m类型
4. 18:00这种格式的时间表述，会被误识别为/m类型

WordBasedGenerativeModelSegment中的CheckDateElements方法，相关规则建议修改
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        Segment segment = new NShortSegment();
        List<Term> list1 = segment.seg(""想出去玩，9点要上班，18:00下班,9成的人会加班。"");
        System.out.println(list1.toString());
        List<Term> list2 = segment.seg(""想出去玩但9点要上班，然后18:00下班,9成的人会加班。"");
        System.out.println(list2.toString());
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[想/v, 出去/vf, 玩/v, ，/w, 9点/t, 要/v, 上班/vi, ，/w, 18:00/t, 下班/vi, ,/w, 9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

[想/v, 出去/vf, 玩/v, 但/c, 9点/t, 要/v, 上班/vi, ，/w, 然后/c, 18:00/t, 下班/vi, ,/w, 9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[想/v, 出去/vf, 玩/v, ，9点/m, 要/v, 上班/vi, ，18:00/m, 下班/vi, ,9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

[想/v, 出去/vf, 玩/v, 但/c, 9点/m, 要/v, 上班/vi, ，/m, 然后/c, 18:00/m, 下班/vi, ,9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
1.51版本分词问题,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.51portable

当前最新版本号是：1.52
我使用的版本是：1.51-portable

## 我的问题
对于“无法”这个词，如果单独分词会分成“无”“法”，但是放到句子“当前订单无法派送”里面会分成“无法”，有没有方法让这两种情况得到同一个分词结果
"
项目打包data目录没打进去,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
1.mvn clean package -DskipTests 打包后data目录并没有被打进去，请问作者你们是怎么打包的？
2.我通过修改pom.xml文件的resources配置这种方式将data目录打进去了，但是在使用的时候依然报文件找不到的错误。

请指点下，非常感谢！


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先执行 mvn clean package -DskipTests 
2. 然后查看target下的jar包内容 没有将 data打进去

"
请教作者，依存句法分析，对复合句子解析不够合理，有什么方法改进？,"比如对这个句子： 如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。
整体解析感觉有问题，如下：

1	如果	如果	c	c	_	4	ADV	_	_
2	一个	一个	m	mq	_	3	ATT	_	_
3	算法	算法	n	n	_	4	SBV	_	_
4	有	有	v	vyou	_	0	HED	_	_
5	缺陷	缺陷	n	n	_	4	VOB	_	_
6	,	,	wp	w	_	4	WP	_	_
7	或	或	c	c	_	9	LAD	_	_
8	不	不	d	d	_	9	ADV	_	_
9	适合于	适合于	v	v	_	4	COO	_	_
10	某个	某个	r	rz	_	11	ATT	_	_
11	问题	问题	n	n	_	9	VOB	_	_
12	,	,	wp	w	_	4	WP	_	_
13	执行	执行	v	v	_	4	COO	_	_
14	这个	这个	r	rz	_	15	ATT	_	_
15	算法	算法	n	n	_	13	VOB	_	_
16	将	将	d	d	_	18	ADV	_	_
17	不会	不会	v	v	_	18	ADV	_	_
18	解决	解决	v	v	_	13	COO	_	_
19	这个	这个	r	rz	_	20	ATT	_	_
20	问题	问题	n	n	_	18	VOB	_	_
21	.	.	wp	w	_	18	WP	_	_


我期望的中心词是 “解决” 。 神经网络模型返回的是“有“，条件随机场模型返回的是“适合于”



"
CRF分词时，标点被分成nz,"版本1.3.3
比如这句话：
如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。
分成：

[如果/c, 一个/mq, 算法/n, 有缺陷/nz, ,/nz, 或/c, 不/d, 适合于/v, 某个/rz, 问题/n, ,/nz, 执行/v, 这个/rz, 算法/n, 将/d, 不会/v, 解决/v, 这个/rz, 问题/n, ./nz]"
nr.txt 中的数据是怎么来的,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
找不到 hanlp.properties 文件,release 中
"较常见姓名识别出错：江,成与","<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
public class DemoChineseNameRecognition
{
.......
       String sentence = ""山东省交通运输厅党组书记江成与大家进行交流"",
       Segment segment = 
        HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
        List<Term> termList = segment.seg(sentence);
        System.out.println(termList);
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[山东省/ns, 交通运输/nz, 厅/n, 党组书记/n, 江成/nr, 与/cc, 大家/rr, 进行/vn, 交流/vn]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[山东省/ns, 交通运输/nz, 厅/n, 党组书记/n, 江/n, 成与/nr, 大家/rr, 进行/vn, 交流/vn]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
拼音切词,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.5.2
我使用的版本是：1.5.2



## 我的问题

你好，HanLP支持自定义拼音词库吗？因为我想使用拼音来进行切词，我们是通过语音识别转文字，识别的时候回有同音词，所以想通过拼音来切词


   CustomDictionary.add(""dakai"");
		CustomDictionary.add(""yiloumenkou"");
		CustomDictionary.add(""deng"");
		List<Term> list = StandardTokenizer.segment(""dakaiyiloumenkoudeng"");
		System.out.println(list);
### 期望输出

dakai yiloumenkou  deng

### 实际输出 dakaiyiloumenkoudeng

"
lucene-core-7.0.1与hanlp1.52 hanlp-index存在分词兼容问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.52
我使用的版本是：1.52

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
通过hanlp-ext 编译整合进elasticsearch6.0 做文档类型的分词映射，映射的分词是hanlp-index
elasticsearch6.0 文档映射部分代码如下
```
PUT test_2017_11_12 
{
    ""mappings"":
    {
        ""meal"":
        {
            ""properties"":
            {
                ""city"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }
                },
                ""name"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""address"":
                {
                    ""type"": ""text"",
                    ""analyzer"": ""hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 512
                        }
                    }
                },
                ""tags"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""price"":
                {
                    ""type"": ""float""
                },
                ""phone"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }

                },
                ""images"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""recommendations"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index""
                },
                ""facilities"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 128
                        }
                    }
                },
                ""uriSign"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""uri"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""createTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""updateTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""syncTime"": 
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                }
            }
        }
    }
}
```
通过google搜索相关资料显示是hanlp-index 分词的方式与lucene-core-7.0.1存在兼容问题
DefaultIndexingChain.java:767  lucene 默认的索处理链文件
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
这是针对elasticsearch文档映射的字段

报错的Java错误堆栈信息如下
```
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
	at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:767) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:239) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:481) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1717) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1462) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
```
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先建立elasticsearch 文档映射
2. 然后通过elasticsearch存储文reset文档接口存储文档
3. 接着elasticsearch日志报错，错误堆栈已经写在文档上，见上文

### 触发代码

```
 PUT test_2017_11_12/meal/5a098a920f3f766cdebf4767
{ 
    ""city"" : ""长沙"", 
    ""name"" : ""盟重烧烤"", 
    ""address"" : ""冬瓜山裕南街85号"", 
    ""tags"" : [
        ""书院路"", 
        ""其它""
    ], 
    ""priceText"" : ""68/人"", 
    ""price"" : 68, 
    ""phone"" : ""15116157770"", 
    ""images"" : [
        ""http://test.com/host/2017/07/30/1501392633001415.jpg-ssq75""
    ], 
    ""recommendations"" : [
        ""星城最正宗的湘西味烤串"", 
        ""主打牛油由湘西直接供货"", 
        ""时尚潮人聚集的深夜食堂""
    ], 
    ""facilities"" : [
        ""Wi-Fi"", 
        ""适合小聚""
    ], 
    ""uriSign"" : ""cd5fb379260756ae50ac9ceb1c2e2331"", 
    ""uri"" : ""http://test/hotel/203381"", 
    ""syncTime"" : 1510669453830, 
    ""createTime"" :1510584054604, 
    ""updateTime"" : 1510584054604
}

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
语料有了，如何重新训练CrfSegment  model,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
想重新训练crf分词模型，相关wiki里提示用 
..\..\crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t
但是不会用，上面这个语句在哪里执行，什么环境？多谢！！

另外，看了一些issue也有同学提出比如将信息熵、互信息或tfidf加入到特征中，也有论文介绍将embedding的特征融入到crf模型中 ，如 Revisiting Embedding Features for Simple Semi-supervised Learning ，如果要加入这些特征，请问修改哪些类及方法可以实现，多谢指导 ！！
"
认为crf里的解码tag方法有误。我修改后的请看一下。," /**
     * 维特比后向算法标注
     *
     * @param table
     */
    public void tag(Table table)
    {
        int size = table.size();
        if (size == 0) return;
        int tagSize = id2tag.length;
        double[][] net = new double[size][tagSize];
        for (int i = 0; i < size; ++i)
        {
            LinkedList<double[]> scoreList = computeScoreList(table, i); //一个char对应的状态BMSE的概率。double[]，特征个数！
            for (int tag = 0; tag < tagSize; ++tag)//4个状态！
            {
                net[i][tag] = computeScore(scoreList, tag);
            }
        }

        if (size == 1)
        {
            double maxScore = -1e10;
            int bestTag = 0;
            for (int tag = 0; tag < net[0].length; ++tag)
            {
                if (net[0][tag] > maxScore)
                {
                    maxScore = net[0][tag];
                    bestTag = tag;
                }
            }
            table.setLast(0, id2tag[bestTag]);
            return;
        }

        //viterbi的核心步骤！
        //note: 改进了viterbi解码的算法。发现原来的net结构不够清晰。
        int[][] from = new int[size][tagSize];//path
        double pro[][] = new double[size][tagSize];
        for (int i = 1; i < size; ++i)
        {
            for (int now = 0; now < tagSize; ++now)
            {
                double maxScore = -1e10;
                for (int pre = 0; pre < tagSize; ++pre)
                {
                    double score = pro[i-1][pre] + matrix[pre][now] + net[i][now];//?net代替了发射概率，肯定是不准确的。
                    if (score > maxScore)
                    {
                        maxScore = score;
                        from[i][now] = pre;
                        pro[i][now] = maxScore; 
                    }
                }
            }
        }
    	   
    	
     
    	
      double maxScore = -1e10;
      int maxTag = 0;
      for (int tag = 0; tag < net[size - 1].length; ++tag)
      {
          if (pro[size - 1][tag] > maxScore)
          {
              maxScore = pro[size - 1][tag];
              maxTag = tag;
          }
      }
    	table.setLast(size-1, id2tag[maxTag]);
    	for (int i = size - 1; i >=1 ; --i)
    	{
          table.setLast(i-1, id2tag[from[i][maxTag]]);
          maxTag = from[i][maxTag];
    	 }
    	
        // 反向回溯最佳路径
//        double maxScore = -1e10;
//        int maxTag = 0;
//        for (int tag = 0; tag < net[size - 1].length; ++tag)
//        {
//            if (net[size - 1][tag] > maxScore)
//            {
//                maxScore = net[size - 1][tag];
//                maxTag = tag;
//            }
//        }
//
//        table.setLast(size - 1, id2tag[maxTag]);
//        maxTag = from[size - 1][maxTag];
//        for (int i = size - 2; i > 0; --i)
//        {
//            table.setLast(i, id2tag[maxTag]);
//            maxTag = from[i][maxTag];
//        }
//        table.setLast(0, id2tag[maxTag]);
    }"
基于神经网络的句法分析器问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：
最新版本
<!--以上属于必填项，以下可自由发挥-->

你好，nndepparser的代码是不是不包括训练部分的代码呢？我没找到相应的函数，谢谢了



"
TextClassifcation 的 temp.data 如何重複使用?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： master branch  https://github.com/hankcs/HanLP/commit/7d35473e4e26631e7362cb861d5a000594f71619
我使用的版本是：master branch  https://github.com/hankcs/HanLP/commit/7d35473e4e26631e7362cb861d5a000594f71619
<!--以上属于必填项，以下可自由发挥-->

## 我的问题

DemoTextClassifcationFMeasure.java
裡面
FileDataSet 是以 File.createTempFile 建立 TempFile to cache
如何重複使用 TempFile ?



### 触发代码

```
    public FileDataSet() throws IOException
    {
        this(File.createTempFile(String.valueOf(System.currentTimeMillis()), "".dat""));
    }
```
"
繁转简中出现的“陷阱”被翻译为“猫腻”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.5.2
我使用的版本是：hanlp-1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

繁体转简体出现简体“陷阱”被替换为“猫腻”。

System.out.println(HanLP.convertToSimplifiedChinese(""瓦爾．閃電陷阱""));
输出：瓦尔．闪电猫腻

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改代码，词典和模型

### 步骤



### 触发代码

```
  System.out.println(HanLP.convertToSimplifiedChinese(""瓦爾．閃電陷阱""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
瓦尔．闪电陷阱
```


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
瓦尔．闪电猫腻
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
word2vector准确率测试，貌似和C没有什么区别,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

1. hanlp中word2vector的参数配置问题
2. 对于C版本，准确率比你的测试结果低了10%
3. 我对word2vector的各方版本进行了测试，发现准确率差别并不大

* 对于1，源码中参数只要发现有cbow和hs，就直接设为true，无关0与1的值，所以当测试了hs=0的时候，其实hanlp使用hs，而c版本没有，在[《word2vec原理推导与代码分析》](http://www.hankcs.com/nlp/word2vec.html)中尽管参数一样，但实际训练过程不一样，不知道这是不是造成准确率差别比较大的原因。我分别测试了hanlp在hs=1和没有添加hs这个参数时的准确率。

* 对于2，对于c版本，采用的c进行训练，gensim计算accuracy，我看过源码和跑过c的accuracy，两个结果一致，没有问题，但是gensim的更快，log更清晰，就跑了gensim的。
这是测试结果：比[《Accuracy rate seems to be 10% lower than the original version》](https://github.com/kojisekig/word2vec-lucene/issues/21)中的c低了10%，不知道为什么？

./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 17:29:30,471 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,375 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,436 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 17:29:46,578 : INFO : capital-common-countries: 77.5% (392/506)
2017-11-28 17:30:15,301 : INFO : capital-world: 45.6% (1626/3564)
2017-11-28 17:30:20,082 : INFO : currency: 19.5% (116/596)
2017-11-28 17:30:38,799 : INFO : city-in-state: 41.2% (959/2330)
2017-11-28 17:30:42,157 : INFO : family: 61.7% (259/420)
2017-11-28 17:30:50,121 : INFO : gram1-adjective-to-adverb: 13.8% (137/992)
2017-11-28 17:30:56,214 : INFO : gram2-opposite: 13.1% (99/756)
2017-11-28 17:31:07,010 : INFO : gram3-comparative: 60.6% (807/1332)
2017-11-28 17:31:14,960 : INFO : gram4-superlative: 25.0% (248/992)
2017-11-28 17:31:23,447 : INFO : gram5-present-participle: 38.6% (408/1056)
2017-11-28 17:31:35,607 : INFO : gram6-nationality-adjective: 77.6% (1181/1521)
2017-11-28 17:31:48,147 : INFO : gram7-past-tense: 34.8% (543/1560)
2017-11-28 17:31:58,815 : INFO : gram8-plural: 49.5% (659/1332)
2017-11-28 17:32:05,812 : INFO : gram9-plural-verbs: 30.8% (268/870)
2017-11-28 17:32:05,812 : INFO : total: 43.2% (7702/17827)


* 对于3，分别测试了google的c版本，gensim，hanlp，deeplearning4j。 除了deeplearning4j，没有测试hs=0的情况，其他都测试了。统计发现使用hs的准确率更差一下，猜测是数据较少，太稀疏导致的。对于hs=0，各家大概43%，hs=1，各家大概35%。

:gensim
model = word2vec.Word2Vec(sentences, size=200, window=8, negative=25, hs=1, sample=0.0001, workers=8, iter=15)
2017-11-29 11:49:46,647 : INFO : loading projection weights from E:/data/word2vec/text8.gensim.word2vec.txt
2017-11-29 11:50:00,520 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.gensim.word2vec.txt
2017-11-29 11:50:00,599 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:50:04,786 : INFO : capital-common-countries: 76.5% (387/506)
2017-11-29 11:50:33,871 : INFO : capital-world: 37.9% (1349/3564)
2017-11-29 11:50:38,687 : INFO : currency: 7.0% (42/596)
2017-11-29 11:50:57,526 : INFO : city-in-state: 40.4% (942/2330)
2017-11-29 11:51:01,313 : INFO : family: 47.4% (199/420)
2017-11-29 11:51:09,776 : INFO : gram1-adjective-to-adverb: 10.8% (107/992)
2017-11-29 11:51:16,038 : INFO : gram2-opposite: 9.0% (68/756)
2017-11-29 11:51:26,976 : INFO : gram3-comparative: 51.4% (685/1332)
2017-11-29 11:51:34,859 : INFO : gram4-superlative: 19.8% (196/992)
2017-11-29 11:51:43,236 : INFO : gram5-present-participle: 25.5% (269/1056)
2017-11-29 11:51:55,519 : INFO : gram6-nationality-adjective: 73.0% (1111/1521)
2017-11-29 11:52:07,953 : INFO : gram7-past-tense: 35.5% (554/1560)
2017-11-29 11:52:18,648 : INFO : gram8-plural: 49.2% (655/1332)
2017-11-29 11:52:25,628 : INFO : gram9-plural-verbs: 21.8% (190/870)
2017-11-29 11:52:25,628 : INFO : total: 37.9% (6754/17827)

model = word2vec.Word2Vec(sentences, size=200, window=8, negative=25, hs=0, sample=0.0001, workers=8, iter=15)
2017-11-29 11:53:14,415 : INFO : loading projection weights from E:/data/word2vec/text8.gensim.word2vec.txt_1
2017-11-29 11:53:27,427 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.gensim.word2vec.txt_1
2017-11-29 11:53:27,505 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:53:31,894 : INFO : capital-common-countries: 72.9% (369/506)
2017-11-29 11:54:01,937 : INFO : capital-world: 51.1% (1822/3564)
2017-11-29 11:54:06,974 : INFO : currency: 18.0% (107/596)
2017-11-29 11:54:26,329 : INFO : city-in-state: 41.5% (966/2330)
2017-11-29 11:54:29,640 : INFO : family: 59.3% (249/420)
2017-11-29 11:54:37,565 : INFO : gram1-adjective-to-adverb: 14.3% (142/992)
2017-11-29 11:54:43,559 : INFO : gram2-opposite: 13.6% (103/756)
2017-11-29 11:54:54,144 : INFO : gram3-comparative: 64.3% (857/1332)
2017-11-29 11:55:02,068 : INFO : gram4-superlative: 23.1% (229/992)
2017-11-29 11:55:10,453 : INFO : gram5-present-participle: 36.0% (380/1056)
2017-11-29 11:55:22,509 : INFO : gram6-nationality-adjective: 73.7% (1121/1521)
2017-11-29 11:55:34,861 : INFO : gram7-past-tense: 34.3% (535/1560)
2017-11-29 11:55:45,290 : INFO : gram8-plural: 49.8% (664/1332)
2017-11-29 11:55:52,154 : INFO : gram9-plural-verbs: 31.5% (274/870)
2017-11-29 11:55:52,155 : INFO : total: 43.9% (7818/17827)


:hanlp
-input E:\data\word2vec\text8 -output E:\data\word2vec\text8.hanlp.word2vec.txt -size 200 -window 8 -negative 25 -hs 0 -cbow 1 -sample 1e-4 -threads 8 -binary 1 -iter 15
2017-11-28 16:53:03,293 : INFO : loading projection weights from E:/data/word2vec/text8.hanlp.word2vec.txt
2017-11-28 16:53:15,493 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.hanlp.word2vec.txt
2017-11-28 16:53:15,553 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:53:19,831 : INFO : capital-common-countries: 69.8% (353/506)
2017-11-28 16:53:49,194 : INFO : capital-world: 30.3% (1079/3564)
2017-11-28 16:53:54,053 : INFO : currency: 4.9% (29/596)
2017-11-28 16:54:12,895 : INFO : city-in-state: 35.7% (831/2330)
2017-11-28 16:54:16,322 : INFO : family: 31.9% (134/420)
2017-11-28 16:54:24,401 : INFO : gram1-adjective-to-adverb: 7.7% (76/992)
2017-11-28 16:54:30,487 : INFO : gram2-opposite: 9.9% (75/756)
2017-11-28 16:54:41,328 : INFO : gram3-comparative: 38.3% (510/1332)
2017-11-28 16:54:49,278 : INFO : gram4-superlative: 13.5% (134/992)
2017-11-28 16:54:58,219 : INFO : gram5-present-participle: 21.6% (228/1056)
2017-11-28 16:55:10,444 : INFO : gram6-nationality-adjective: 72.4% (1101/1521)
2017-11-28 16:55:22,950 : INFO : gram7-past-tense: 28.5% (445/1560)
2017-11-28 16:55:33,730 : INFO : gram8-plural: 45.9% (612/1332)
2017-11-28 16:55:40,694 : INFO : gram9-plural-verbs: 17.1% (149/870)
2017-11-28 16:55:40,696 : INFO : total: 32.3% (5756/17827)

-input E:\data\word2vec\text8 -output E:\data\word2vec\text8.hanlp.word2vec.txt_1 -size 200 -window 8 -negative 25 -cbow 1 -sample 1e-4 -threads 8 -binary 1 -iter 15
2017-11-29 11:15:27,628 : INFO : loading projection weights from E:/data/word2vec/text8.hanlp.word2vec.txt_1
2017-11-29 11:15:42,361 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.hanlp.word2vec.txt_1
2017-11-29 11:15:42,461 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:15:47,365 : INFO : capital-common-countries: 80.0% (405/506)
2017-11-29 11:16:20,013 : INFO : capital-world: 46.2% (1647/3564)
2017-11-29 11:16:25,338 : INFO : currency: 14.4% (86/596)
2017-11-29 11:16:46,128 : INFO : city-in-state: 46.4% (1081/2330)
2017-11-29 11:16:49,861 : INFO : family: 53.1% (223/420)
2017-11-29 11:16:58,723 : INFO : gram1-adjective-to-adverb: 15.7% (156/992)
2017-11-29 11:17:05,424 : INFO : gram2-opposite: 9.9% (75/756)
2017-11-29 11:17:17,216 : INFO : gram3-comparative: 51.1% (680/1332)
2017-11-29 11:17:26,082 : INFO : gram4-superlative: 20.0% (198/992)
2017-11-29 11:17:35,536 : INFO : gram5-present-participle: 29.9% (316/1056)
2017-11-29 11:17:49,177 : INFO : gram6-nationality-adjective: 82.4% (1254/1521)
2017-11-29 11:18:03,059 : INFO : gram7-past-tense: 32.5% (507/1560)
2017-11-29 11:18:15,029 : INFO : gram8-plural: 53.7% (715/1332)
2017-11-29 11:18:22,894 : INFO : gram9-plural-verbs: 26.7% (232/870)
2017-11-29 11:18:22,894 : INFO : total: 42.5% (7575/17827)


:deeplearning4j
Word2Vec vec = new Word2Vec.Builder().layerSize(200).windowSize(8).negativeSample(25).minWordFrequency(5).useHierarchicSoftmax(true).sampling(0.0001).workers(8).iterations(15).epochs(15).iterate(iter)
                .elementsLearningAlgorithm(""org.deeplearning4j.models.embeddings.learning.impl.elements.CBOW"")
                .tokenizerFactory(t)
                .build();
2017-11-28 16:46:26,894 : INFO : loading projection weights from E:/data/word2vec/text8.deeplearning4j.word2vec.txt
2017-11-28 16:46:39,391 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.deeplearning4j.word2vec.txt
2017-11-28 16:46:39,453 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:46:43,596 : INFO : capital-common-countries: 67.4% (341/506)
2017-11-28 16:47:12,592 : INFO : capital-world: 33.9% (1208/3564)
2017-11-28 16:47:17,515 : INFO : currency: 6.0% (36/596)
2017-11-28 16:47:36,332 : INFO : city-in-state: 36.6% (852/2330)
2017-11-28 16:47:39,834 : INFO : family: 38.3% (161/420)
2017-11-28 16:47:47,898 : INFO : gram1-adjective-to-adverb: 9.0% (89/992)
2017-11-28 16:47:53,953 : INFO : gram2-opposite: 7.0% (53/756)
2017-11-28 16:48:04,632 : INFO : gram3-comparative: 38.7% (515/1332)
2017-11-28 16:48:12,653 : INFO : gram4-superlative: 11.8% (117/992)
2017-11-28 16:48:21,220 : INFO : gram5-present-participle: 23.0% (243/1056)
2017-11-28 16:48:33,519 : INFO : gram6-nationality-adjective: 76.7% (1166/1521)
2017-11-28 16:48:46,165 : INFO : gram7-past-tense: 27.2% (424/1560)
2017-11-28 16:48:56,894 : INFO : gram8-plural: 48.2% (642/1332)
2017-11-28 16:49:03,973 : INFO : gram9-plural-verbs: 19.2% (167/870)
2017-11-28 16:49:03,974 : INFO : total: 33.7% (6014/17827)


:google_c
./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 1 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 16:49:29,132 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt
2017-11-28 16:49:41,848 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt
2017-11-28 16:49:41,914 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:49:46,154 : INFO : capital-common-countries: 75.7% (383/506)
2017-11-28 16:50:15,078 : INFO : capital-world: 33.2% (1184/3564)
2017-11-28 16:50:19,993 : INFO : currency: 6.0% (36/596)
2017-11-28 16:50:38,967 : INFO : city-in-state: 36.0% (838/2330)
2017-11-28 16:50:42,348 : INFO : family: 47.4% (199/420)
2017-11-28 16:50:50,315 : INFO : gram1-adjective-to-adverb: 10.6% (105/992)
2017-11-28 16:50:56,355 : INFO : gram2-opposite: 7.8% (59/756)
2017-11-28 16:51:07,065 : INFO : gram3-comparative: 48.3% (644/1332)
2017-11-28 16:51:14,905 : INFO : gram4-superlative: 18.0% (179/992)
2017-11-28 16:51:23,299 : INFO : gram5-present-participle: 29.0% (306/1056)
2017-11-28 16:51:35,345 : INFO : gram6-nationality-adjective: 70.1% (1066/1521)
2017-11-28 16:51:47,733 : INFO : gram7-past-tense: 31.9% (498/1560)
2017-11-28 16:51:58,316 : INFO : gram8-plural: 50.1% (667/1332)
2017-11-28 16:52:05,321 : INFO : gram9-plural-verbs: 20.0% (174/870)
2017-11-28 16:52:05,322 : INFO : total: 35.6% (6338/17827)

./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 17:29:30,471 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,375 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,436 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 17:29:46,578 : INFO : capital-common-countries: 77.5% (392/506)
2017-11-28 17:30:15,301 : INFO : capital-world: 45.6% (1626/3564)
2017-11-28 17:30:20,082 : INFO : currency: 19.5% (116/596)
2017-11-28 17:30:38,799 : INFO : city-in-state: 41.2% (959/2330)
2017-11-28 17:30:42,157 : INFO : family: 61.7% (259/420)
2017-11-28 17:30:50,121 : INFO : gram1-adjective-to-adverb: 13.8% (137/992)
2017-11-28 17:30:56,214 : INFO : gram2-opposite: 13.1% (99/756)
2017-11-28 17:31:07,010 : INFO : gram3-comparative: 60.6% (807/1332)
2017-11-28 17:31:14,960 : INFO : gram4-superlative: 25.0% (248/992)
2017-11-28 17:31:23,447 : INFO : gram5-present-participle: 38.6% (408/1056)
2017-11-28 17:31:35,607 : INFO : gram6-nationality-adjective: 77.6% (1181/1521)
2017-11-28 17:31:48,147 : INFO : gram7-past-tense: 34.8% (543/1560)
2017-11-28 17:31:58,815 : INFO : gram8-plural: 49.5% (659/1332)
2017-11-28 17:32:05,812 : INFO : gram9-plural-verbs: 30.8% (268/870)
2017-11-28 17:32:05,812 : INFO : total: 43.2% (7702/17827)
"
Python下测试NLP分词功能时出现异常“java.lang.ExceptionInInitializerError”,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在Python下测试NLP分词时出现异常“java.lang.ExceptionInInitializerError”，其他方式测试正常
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
代码如下：
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程'))




"
portable版本比完整版好 ,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用默认标准分词, 发现maven引用portable版本的分词效果比下载下jar并挂载了数据包的好

## 复现问题

在同样的语料上进行分词测试(sighan bakeoff05的msr和pku)一下就行

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

起码分词结果一致吧? 或者更新数据包

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
分词错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""修改稿件""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[修改/v, 稿件/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[修改稿/n, 件/q]
```

"
标点符号,"请问如何关闭 标点符号变更？
输入的是中文标点符号，变成了英文标点，而且有一些特殊的也变了，比如（变成了《，并且英文标点的词性都是m"
Word2Vec训练1G的语料出现java.lang.OutOfMemoryError: Java heap space,训练过程中没有出现问题，但训练到100%后会发生该错误，不晓得能不能优化一下，使得Word2Vec能支持更大的语料的训练（其实感觉1G应该不算很大）
人名识别导致分词错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

句子       ： 
```
商改后要买另外的保险保费怎么算
```

分词结果：
```
[商改后/nr, 要买/nz, 另外/c, 的/ude1, 保险/n, 保费/n, 怎么/ryv, 算/v]
```

其中 商改后 被识别为一个人名

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""商改后要买另外的保险保费怎么算""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[商改/v，后/,  要买/nz, 另外/c, 的/ude1, 保险/n, 保费/n, 怎么/ryv, 算/v]
```

### 实际输出

```
[商改后/nr, 要买/nz, 另外/c, 的/ude1, 保险/n, 保费/n, 怎么/ryv, 算/v]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
人名识别错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
人名识别错误

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
未修改任何源代码

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
//        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""韩国呢""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
韩国/nsf 呢/y
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
粗分词网：
0:[ ]
1:[韩, 韩国]
2:[国]
3:[呢]
4:[ ]

粗分结果[韩国/nsf, 呢/y]
人名角色观察：[  K 1 A 1 ][韩国 K 55 X 28 L 4 ][呢 L 29 D 1 ][  K 1 A 1 ]
人名角色标注：[ /K ,韩国/X ,呢/D , /A]
识别出人名：韩国呢 XD
细分词网：
0:[ ]
1:[韩国, 韩国呢]
2:[]
3:[呢]
4:[ ]

韩国呢/nr
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
N-最短路分词 分词结果词性标注错误,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：portable-1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
待分词的文本内容：
""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。""
分词结果：
[今天/t, ，/m, 刘志军/nr, 案/ng, 的/ude1, 关键人物/nz, ,/m, 山西/ns, 女/b, 商人/nnt, 丁书苗/nr, 在/p, 市二中/n, 院/n, 出庭/vi, 受审/vi, 。/w]
问题：
中文逗号和英文逗号的词性标注错误，应该为w词性

## 复现问题
未做其它改动，直接使用项目README.md中的N-最短路分词的demo测试


### 触发代码

```
	public static void main(String[] args) {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		System.out.println(nShortSegment.seg(""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。""));
	}
```


"
1.3.4版本运行大批量语料分词吃内存严重,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
首先感觉hanlp很靠谱，觉得是一个非常好的NLP开源项目，我拿来对语料库（大约8KW的数据集）做分词，发现非常吃内存，这块我自定义的词典里面加入了大约120W左右的用户自定义词典。随着程序的运行，内存使用量攀高，但是CPU使用率基本没有什么变化。而且随着时间的增长，内存使用量越来越大，逼近整个电脑的峰值，最后不得不终止。请问楼主在测试大语料分词的时候，是否有这种情况发生？谢谢

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先我把自己自定义词典加入到myDictionary.txt中，大约120W左右
2. 然后我写了一个thrift服务，服务端采用CRFSegment与Hanlp.newSegment()两种方式测试，都发现吃内存比较严重。随着时间的增长，内存使用越来越多。当程序处理了大约1000万的数据的时候，发现我的电脑扛不住了，我的电脑是8G的内存，发现内存基本上被占满了。

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
如何在程序中重新加载自定义的词典,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：
我使用的版本是：hanlp-1.3.2-portable.jar  、 hanlp-lucene-plugin-1.1.2.jar


## 实际场景
一个web容器中有两个webapp，一个是webappA，另一个是solr，solr使用hanlp为中文分词器，并配置了用户词典（即customDictionaryPath属性），webappA有一个在线编辑词典的功能，希望编辑完字典，solr能够看到效果而不需要重启tomcat容器。

## 解决思路
在hanlp solr插件中的HanLPTokenizerFactory开启一个守护线程，每隔一段时间去检查字典的校检码，如果发生变化就删掉.bin缓存文件，并重新加载字典。

## 我的问题
1、我目前在CustomDictionary添加了如下一个静态方法，但是这样会把所有的自定义词典重新加载一遍，有没有只加载某个文件的方法呢
```
  public static void reloadDic(){
    	trie = null;
    	dat = new DoubleArrayTrie<CoreDictionary.Attribute>();
    	loadMainDictionary(path[0]);
    }
```
2、执行CustomDictionary.insert()方法后，为什么新词典已经产生效果，但dat.size()没有发生变化
"
P2P和C2C这种词没有分出来，希望加到主词库,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我希望“P2P”和“C2C”这些词能够分成一个词，但是现在是分成了多个。

## 复现问题


### 步骤


### 触发代码

```
    public void testIssue1234() throws Exception
    {
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
        List<Term> terms=segment.seg(""P2P C2C"");
        System.out.println(terms);
    }
```
### 期望输出
P2P/n  C2C/n


### 实际输出
[P/nx, 2/m, P/nx,  /w, C/nx, 2/m, C/nx]

## 其他信息


"
HanLP的二元文法词典如何使用?,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我在data\dictionary\CoreNatureDictionary.ngram.txt文件中发现了""琐碎@事情 2"",  '琐碎'和'事情' 是合理的接续.我不知道运行之后是出现什么样的效果
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
       String rawText = ""琐碎的事情"";
	List<Term> termList = StandardTokenizer.segment(rawText);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

[琐碎事情]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

[琐碎, 的, 事情]

## 其他信息
不知道我这样的想法是不是错误的.还是我哪里操作有问题
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请问分词的时候能识别时间吗？如11月22日,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词性 + 自定义词条，与 portable 自带词典冲突,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号 
当前最新版本号是：portable-1.5.2
我使用的版本是：portable-1.5.2
备注：web项目，基于 springboot 1.5.8.RELEASE，docker 部署

## 代码复现
我想用 HanLP 匹配语句中的国家城市信息，国家城市名字是规定好的，在库里有一套，在 app 启动时，将库里的国家城市数据刷入 CustomDictionary 中，代码如下：
```java
/**
   * 将国家城市名添加到用户词典
   */
  @Override
  public void addCountryAndCityNameIntoDict() {
    //添加国家
    List<Country> countries = countryRepository.findAll();
    for (Country country : countries) {
      boolean countryInvalid = country.getId() == null || StringUtils.isEmpty(country.getCnName())
              || StringUtils.isEmpty(country.getEnName());
      if (countryInvalid) {
        continue;
      }
      CustomDictionary.insert(country.getCnName(), ""country 1000000"");
      CustomDictionary.insert(country.getEnName().toLowerCase(), ""country 1000000"");
      //根据国家，添加城市
      List<City> cities = cityRepository.findByCountryId(country.getId());
      for (City city : cities) {
        boolean cityInvalid = city.getId() == null || StringUtils.isEmpty(city.getCnName())
                || StringUtils.isEmpty(city.getEnName());
        if (cityInvalid) {
          continue;
        }
        CustomDictionary.insert(city.getCnName(), ""city 1000000"");
        CustomDictionary.insert(city.getEnName().toLowerCase(), ""city 1000000"");
      }
    }
  }
```
城市名称示例：
```
---------------------
  cn_name   en_name  
---------------------
  上海       Shanghai 
  北京       Beijing     
  杭州       Hangzhou 
  台北       Taibei    
  海南       Kainan    
  重庆       Zhongqing
---------------------
```
然后我试了几个分词器，分词结果都是以 portable 版本自带词典的词语为主：
```java
@GetMapping(""/test"")
  public Result testHanLP(@RequestParam String query) {
    //转换格式
    query = HanLP.convertToSimplifiedChinese(query.toLowerCase());

    Map<String, String> data = new LinkedHashMap<>(7);

    Segment standardTokenizer = StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""标准分词"", standardTokenizer.seg(query).toString());

    data.put(""NLP分词"", NLPTokenizer.segment(query).toString());

    Segment indexTokenizer = IndexTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""索引分词"", indexTokenizer.seg(query).toString());

    Segment nShortSegment = new NShortSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""N-最短路径分词"", nShortSegment.seg(query).toString());

    Segment shortSegment = new DijkstraSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""最短路径分词"", shortSegment.seg(query).toString());

    return Results.SUCC.data(data);
  }
```
测试语句：海南省北京市上海市重庆市台湾岛台湾省台北市北京是首都海南是旅游胜地
分词效果：
```json
{
  ""code"": 0,
  ""msg"": ""成功"",
  ""data"": {
    ""标准分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""NLP分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""索引分词"": ""[海南省/ns, 海南/ns, 北京市/ns, 北京/ns, 上海市/ns, 上海/ns, 重庆市/ns, 重庆/ns, 台湾岛/nz, 台湾/ns, 台湾省/ns, 台湾/ns, 台北市/ns, 台北/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""N-最短路径分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""最短路径分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]""
  }
}
```

### 期望输出
目前来看，索引分词还算凑合，我期望的是：
```
[海南省/ns, 海南/city, 北京市/ns, 北京/city, 上海市/ns, 上海/city, 重庆市/ns, 重庆/city, 台湾岛/nz, 台湾/city, 台湾省/ns, 台湾/city, 台北市/ns, 台北/city, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]
```

### 提问
- 我该如何在用官方 portable jar 包的前提下，得到预期的输出？毕竟自己打包，然后放在项目下，不太优雅（我现在没有自己的 maven 仓库，如果自己打包的话，只能放在项目下了）。
- 在使用官方 portable jar 包时，能否提供一个禁用其自带的某些词典的方法？感觉这是一种解决思路。
- 我有考虑借助 hanlp.properties 和 自定义 data 目录，但是这样对于一个 web 项目来说，部署时就不太方便。
"
常见名词和连词“和”被合并为一个名词了,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
HanLP.Config.enableDebug();
Segment segment = new DijkstraSegment();
List<Term> termList = segment.seg(""开展公共资源交易活动监督检查和举报投诉处理"");
System.out.println(termList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[开展/v, 公共资源/gi, 交易/vn, 活动/vn, 监督/vn, 检查/n,和/cc, 举报/vn, 投诉/vn, 处理/vn]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
粗分结果[开展/v, 公共资源/gi, 交易/vn, 活动/vn, 监督/vn, 检查和/n, 举报/vn, 投诉/vn, 处理/vn]
人名角色观察：[  K 1 A 1 ][开展 L 11 K 4 ][公共资源 A 20833310 ][交易 L 2 ][活动 L 13 K 3 ][监督 A 20833310 ][检查和 A 20833310 ][举报 K 100 L 33 M 22 ][投诉 K 9 L 3 ][处理 L 27 K 12 ][  K 1 A 1 ]
人名角色标注：[ /K ,开展/L ,公共资源/A ,交易/L ,活动/L ,监督/A ,检查和/A ,举报/K ,投诉/K ,处理/L , /A]

[开展/v, 公共资源/gi, 交易/vn, 活动/vn, 监督/vn, 检查和/n, 举报/vn, 投诉/vn, 处理/vn]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
debug部分输出：
to: 21, from: 19, weight:11.60, word:督@检查
to: 22, from: 20, weight:11.24, word:检@查
to: 23, from: 21, weight:03.15, word:检查@和
to: 23, from: 22, weight:04.03, word:查@和
to: 24, from: 23, weight:07.47, word:和@举
"
常见官职识别出错,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
常见官职如“外交部长”识别出错
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
List<Term> termList = segment.seg(""国防部长王毅向记者介绍此访情况。"");
System.out.println(termList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[国防部长/n, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
```
[国防部长/n, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[国防部/nt, 长/a, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
```
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
总体感觉master不够稳定
"
自定义词的优先级以及自定义词性的问题,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：portable-1.5.0
我使用的版本是：portable-1.5.0

## 我的问题
1. 自定义词典中，词性可以任意定吗。比如我新创造一种 ""ssr"" 的词性，特指我自己的词
2. 使用了自定义词典，并且打开了自定义词典优先的开关。但是仍然无法匹配出想要的结果。

## 复现问题

### 触发代码

```
		String rawText = ""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰"";
		// 原始分词效果： [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走/v,
		// 上/f, 人生/n, 巅峰/n]

		// 测试1:
		CustomDictionary.insert(""狮逆"", ""ssr 20000"");
		CustomDictionary.insert(""狗，迎"", ""ssr 2000"");
		CustomDictionary.insert(""走上"", ""ssr 2000"");
		StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true);
		System.out.println(HanLP.segment(rawText));

		// 效果：
		// [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗，迎/ssr, 娶/v, 白富美/nr, ，/w, 走上/ssr, 人生/n, 巅峰/n]
```
### 期望输出

[攻城/vi, 狮逆/ssr, 袭/vi, 单身/n, 狗，迎/ssr, 娶/v, 白富美/nr, ，/w, 走上/ssr, 人生/n, 巅峰/n]

可以看出， ""狗，迎"" 以及 ""走上"" 这种硬生生添加的词被识别出来了。
但是 “ 狮逆” 没有匹配出来。
请问是词性上有限制？ 还是说需要修改核心字典什么的？ 
https://github.com/hankcs/HanLP/issues/393 跟这个issue有关吗。portable版本不是很想单独修改文件
谢谢！



"
NShortSegment分词算法实现问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.5.2
我使用的版本是：v1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1） 为什么NShortSegment分词默认只选择第1条路径进行后续分析和处理，那它前面求出至少2条最短路径的意义何在？（文件NShortSegment.java第101行：List vertexList = coarseResult.get(0)）
2） 为什么NShortSegment分词定死nKind = 2？是为了什么考虑的？


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
例如：我使用NShortSegment进行分词“他说的确实在理。”

虽然中间得到两条分词结果：
coarseResult = [[ , 他, 说, 的, 确实, 在, 理, 。, ], [ , 他, 说, 的, 确实, 在理, 。, ]]

但是后面默认使用第一个结果进行后续分析（第101行：List vertexList = coarseResult.get(0)），导致实际输出就是第一个分词结果。

那么这样的话，NShortSegment分词出两种分词结果，有什么意义呢？第二个分词结果后面根本就没用？而且个人认为第二个分词结果稍微好一些（路径更短）。

另外，为什么NSegment.java中定死nKind=2，即变成2-最短路径。这个nKind定死为2是出于分词效率考虑吗？是不是可以让用户来确定nKind的具体取值？
### 步骤

程序运行如下。

### 触发代码

```
   	public void testSegment() {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		List<Term> term1 = nShortSegment.seg(""他说的确实在理。"");
		System.out.println(term1);
	}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
```
[ , 他, 说, 的, 确实, 在理, 。, ]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[ , 他, 说, 的, 确实, 在, 理, 。, ]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
1.3.2版本    “停牌自查”在处于句末时进行分词，该词会消失,"1.3.2版本中出现：
例句：武汉凡谷9日停牌自查
词语“停牌自查”在 segment.seg(sentence)操作后消失，仅出现‘武汉凡谷‘ 、’9日’；
除此之外还有些其它动词在句末时会显现此种情况。
望解决。

"
自定义词库优先级问题,"CustomDictionary.add(""政策"");
StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true);
System.out.println(HanLP.segment(""新政策""));

想要切出的是“政策”返回的是新政策"
非常奇怪的姓名识别错误,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
句子中姓名识别出错，按常理推论不应该的
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
   List<Term> termList = segment.seg(sentence);
   System.out.println(termList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
三亚市教育局/nt, 吴萍/nr, 局长/nnt
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
三亚市教育局/nt, 吴/tg, 萍/nz, 局长/nnt
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于分词的效率问题，标准分词和SpeedTokenizer都存在，请教！,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0 portable
我使用的版本是： 1.5.0 portable

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
HanLp的分词效率如何，只看到过关于急速分词的demo。但是那个demo是重复了1百万次相同的文本，怀疑内部有缓存等，看起来分词速度很高。而我自己进行了简单的5个文本的测试。效果不是很理想。

## 复现代码
```java
	@Test
	public void testCustomizeDict() {
		String[] sentences = new String[] { ""这是一个伸手不见五指的黑夜。我叫孙悟空，我爱北京，我爱Python和C++。"", ""我不喜欢日本和服。"",
				""雷猴回归人间。"", ""工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"", ""结果婚的和尚未结过婚的"" };
		// 进行自定义词性的配置
		CustomDictionary.insert(""敏感词"", ""ssr 1024"");
		CustomDictionary.insert(""白洁"", ""ssr 1024"");
		CustomDictionary.insert(""白小洁"", ""ssr 1024"");
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			HanLP.segment(sentence);
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
**// 大概消耗 300~400ms**
	}

	@Test
	public void testCustomizeSpeed() {
		String[] sentences = new String[] { ""这是一个伸手不见五指的黑夜。我叫孙悟空，我爱北京，我爱Python和C++。"", ""我不喜欢日本和服。"",
				""雷猴回归人间。"", ""工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"", ""结果婚的和尚未结过婚的"" };
		// 进行自定义词性的配置
		CustomDictionary.insert(""敏感词"", ""ssr 1024"");
		CustomDictionary.insert(""白洁"", ""ssr 1024"");
		CustomDictionary.insert(""白小洁"", ""ssr 1024"");
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			SpeedTokenizer.segment(sentence);
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
	}
**// 大概消耗80ms**
```
相比之下，确实急速分词快很多，但是还没有达到理想状态。急速分词根据您提供的demo来看，
分词速度：11068068.62字每秒 。 然而实际上多个不同的进行分词，感觉效率很低。
**请问是我的代码哪里写的不对吗？还是说环境上有哪些不正确的配置。**

另外，**HanLp是我首选的，但是看到jieba分词的java版。相同的测试用例，分词总计才消耗8ms。**
比Hanlp急速分词还快10倍，感觉这才是正常的水平。
环境都是本地开发机，在Intellij上进行运行，i5,8g内存。

```
	public static void main(String[] args) throws Exception {
		String[] sentences = new String[] { ""这是一个伸手不见五指的黑夜。我叫孙悟空，我爱北京，我爱Python和C++。"", ""我不喜欢日本和服。"",
				""雷猴回归人间。"", ""工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"", ""结果婚的和尚未结过婚的"" };
		JiebaSegmenter segmenter = new JiebaSegmenter();
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			System.out
					.println(segmenter.process(sentence, JiebaSegmenter.SegMode.INDEX).toString());
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
	}
**//大概消耗8ms**
```
"
Java1.9已经不开始支持动态增加枚举的部分方法,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
[
![qq 20171116103726](https://user-images.githubusercontent.com/13550295/32871119-3df4c0f8-caba-11e7-9018-9d3e3bac5196.png)
](url)
![package](https://user-images.githubusercontent.com/13550295/32871723-aa166f90-cabd-11e7-818c-10b494657c50.png)
JDK1.9重构了部分东西，且放到了不支持的包，参见上图
不知道有什么什么好方法可以解决"
姓名前面有空格（全角和半角都算），导致姓名无法识别,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：hanlp:portable-1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
姓名前面有空格（全角和半角都算），导致姓名无法识别
<!-- 请详细描述问题，越详细越可能得到解决 -->
hanlp的DemoChineseNameRecognition.java里，姓名前面加个空格，姓名就无法识别： 原句："" 王总和小丽结婚了"", 识别结果：[ /w, 王/n, 总/b, 和/cc, 小丽/nr, 结婚/vi, 了/ule]
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我没有修改代码，所有的hanlp版本都有这个问题
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    Segment segment = HanLP.newSegment().enableNameRecognize(true);
    List<Term> termList = segment.seg(“ 赵志辉同志任北京市顺义区质量技术监督局党组成员”);
    System.out.println(termList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
[ /w, 赵志辉/nr, 同志/n, 任/v, 北京市/ns, 顺义区/ns, 质量/n, 技术/n, 监督局/nis, 党组/nis, 成员/nnt]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
[ /w, 赵/nz, 志/n, 辉/ng, 同志/n, 任/v, 北京市/ns, 顺义区/ns, 质量/n, 技术/n, 监督局/nis, 党组/nis, 成员/nnt]
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
添加用户自定义词后报viterbi数组越界,"Hi：
我在分词的时候遇到了添加用户自定义词后报viterbi数组越界的问题，麻烦帮忙看下。
报错信息：
**java.lang.ArrayIndexOutOfBoundsException: 72
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:154)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:521)
	at com.hankcs.hanlp.tokenizer.NotionalTokenizer.segment(NotionalTokenizer.java:48)
	at com.hankcs.hanlp.tokenizer.NotionalTokenizer.segment(NotionalTokenizer.java:37)
	at com.mig.ml.mlplatform.segment$$anonfun$2.apply(segment.scala:398)
	at com.mig.ml.mlplatform.segment$$anonfun$2.apply(segment.scala:341)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)**

触发版本：master分支最新版**1.5.0**
触发代码：
```
 public static String readFile(String filePath) throws IOException {
        StringBuffer sb = new StringBuffer();
        fileUtils.readToBuffer(sb, filePath);
        return sb.toString();
    }
public static void readToBuffer(StringBuffer buffer, String filePath) throws IOException {
        InputStream is = new FileInputStream(filePath);
        String line; // 用来保存每行读取的内容
        BufferedReader reader = new BufferedReader(new InputStreamReader(is));
        line = reader.readLine(); // 读取第一行
        while (line != null) { // 如果 line 为空说明读完了
            buffer.append(line); // 将读到的内容添加到 buffer 中
            buffer.append(""\n""); // 添加换行符
            line = reader.readLine(); // 读取下一行
        }
        reader.close();
        is.close();
    }

     try {
            String dic_word = readFile(""d:\\dic_word"");
            String[] split_dic_word = dic_word.split(""\n"");
            for(String sp: split_dic_word){
                CustomDictionary.add(sp);
            }

        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println(NotionalTokenizer.segment(""【仁创】倒计时1天 武汉糖酒会在汉口武展明天盛大开幕，您准备好了吗？找产品、了解市场，高规格行业活动等你来，不容错过18171482672黄""));
```

词库dic_word文件如下：


"
创建同义词典未定义词后调用CommonSynonymDictionary.SynonymItem.toString方法空指针异常,"## 版本号
当前最新版本号是：portable-1.5.0
我使用的版本是：portable-1.5.0

## 我的问题
创建词典未定义词调用CommonSynonymDictionary.SynonymItem.toString方法空指针异常

### 触发代码

```
    SynonymItem item = CommonSynonymDictionary.SynonymItem.createUndefined(""测试版"");
    System.out.println(item.toString());
```

### 实际输出
空指针异常

## 其他信息
创建同义词典中未出现的词时 com.hankcs.hanlp.corpus.synonym.Synonym 的Type为null

"
Python 调用时如何 enable debug 信息？,"当前最新版本号是：1.5.0 
我使用的版本是： 1.5.0 portable

使用 Python 调用时希望打开调试信息查看词典是否加载成功，阅读了 jpype 的相关调用文档，发现由于 inner class 的问题并不能按常规方式调用
https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/HanLP.java#L53

`Config = JClass('com.hankcs.hanlp.HanLP$Config')`

请问应该如何打开调试信息呢？

PS:

```
Config = JClass('com.hankcs.hanlp.HanLP$Config')
Config.enableDebug()
```

更新到1.5.0之后问题已解决，但是可能并不是版本的问题，是自己某处调用没有写对。







"
NShortSegment分词算法实现问题,"原文件：HanLP/src/main/java/com/hankcs/hanlp/seg/NShort/NShortSegment.java

第101行： List<Vertex> vertexList = coarseResult.get(0);

问题：
1） 为什么默认只选择第1条路径进行后续分析和处理，那它前面求出至少2条最短路径的意义何在？
2） 为什么定死nKind = 2？是为了什么考虑的？

例如：我使用NShortSegment进行分词“他说的确实在理。”

中间得到两条分词结果：
coarseResult = [[ , 他, 说, 的, 确实, 在, 理, 。,  ], [ , 他, 说, 的, 确实, 在理, 。,  ]]

最后输出：
[他/rr, 说/v, 的/ude1, 确实/ad, 在/p, 理/n, 。/w]

为什么后面只选择了第1个分词结果进行分析？而忽略了第2个分词结果？
按理来说，第2个分词结果不应该更好吗？"
请问，为什么不支持分词结果最小颗粒度？,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.3.5
<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
请问，为什么不支持分词结果最小颗粒度。
在lucene场景中创建更丰富的索引，以便支持更丰富的检索场景。
我翻阅了部分代码和文档发现，以及使用作为
com.hankcs.lucene.HanLPTokenizerFactory
索引分词器。需要分词的短语为:“弹簧床”，得到的分词结果为：“弹簧”+“床”。
那么检索器无论怎样设置，是无法通过：“弹”，或者相关检索字词查询到结果？

刚接触搜索相关知识，如有不认识不周的，请包涵。


"
关于Vector类以及相关的问题,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
1. 对于Vector类里面的cosine方法表示不理解
为什么计算dot的结果再开平方，加载词向量的时候已经是单位向量，也就是说计算cosine直接返回结果即可。没明白为什么这么做？

2. 对于DocVectorModel的query方法的不理解（文档向量生成）
结果除以n的目的是为了什么，求期望？如果是求期望，是不是计算完成之后应该计算单位向量。以下贴一段我的测试代码：
<pre><code>  
    public static void main(String[] args) {
        List<String> one = new ArrayList<>();
        one.add(""我"");
        one.add(""是"");
        one.add(""程序员"");

        List<String> other = new ArrayList<>();
        other.add(""程序员"");
        other.add(""很"");
        other.add(""苦逼"");

        float[] senOne = new float[200];
        for (String s : one) {
            //通过ansj提供的源码加载的二进制词向量文件
            float[] floats = WordSimilarity.getWordMap().get(s);
            if (floats == null) {
                System.out.println(s + ""未找到词向量"");
                continue;
            }
            for (int i = 0; i < floats.length; i++) {
                senOne[i] += floats[i];
            }
        }
        float[] senTwo = new float[200];
        for (String s : other) {
            //通过ansj提供的源码加载的二进制词向量文件
            float[] floats = WordSimilarity.getWordMap().get(s);
            if (floats == null) {
                System.out.println(s + ""未找到词向量"");
                continue;
            }
            if (floats == null) {
                System.out.println(s + ""未找到词向量"");
                continue;
            }
            for (int i = 0; i < floats.length; i++) {
                senTwo[i] += floats[i];
            }
        }
        System.out.println(""词向量直接相加 + cosine ："" + cosine(senOne, senTwo));
        System.out.println(""DocVectorModel.query + Vector.cosine："" + query(one).cosine(query(other)));
        System.out.println(""DocVectorModel.query + cosine："" + cosine(query(one).elementArray, query(other).elementArray));
    }

    public static double cosine(float[] x, float[] y) {
        double xx = 0;
        double xy = 0;
        double yy = 0;
        for (int i = 0; i < x.length; i++) {
            xx += x[i] * x[i];
            xy += x[i] * y[i];
            yy += y[i] * y[i];
        }
        return xy / Math.sqrt(xx * yy);
    }


    /**
     * 直接copy的DocVectorModel的query方法
     * @param words 分词结果
     * @return 句向量
     */
    public static Vector query(List<String> words) {
        if (words == null || words.size() == 0) return null;
        Vector result = new Vector(200);
        int n = 0;
        for (String word : words) {
            float[] floats = WordSimilarity.getWordMap().get(word);

            if (floats == null) {
                continue;
            }
            result.addToSelf(new Vector(floats));
            ++n;
            if (n == 0) {
                return null;
            }
        }
        result.divideToSelf(n);
        return result;
    }
</code></pre>
## 测试结果
词向量直接相加 + cosine ：0.7249490543441774
DocVectorModel.query + Vector.cosine：0.5651151
DocVectorModel.query + cosine：0.7249490499784463

## 结论
个人感觉1.文档向量应该除以向量的模，2.Vector.cosine方法应该直接返回dot结果。
不知道我说的对不对。希望指正。"
Readme中所说的带空格的纯文本csv自定义词典报错,"`.txt`词典文件的分隔符为空格或制表符，所以不支持含有空格的词语。如果需要支持空格，请使用英文逗号`,`分割的**纯文本**`.csv`文件。在使用Excel等富文本编辑器时，则请注意保存为**纯文本**形式。<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
1.5.0 新词发现特性OutOfMemory，discovery函数读取语料字符串，是否可以提供流式管道作为参数的discovery函数,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用1.5.0版本新增加的特性：新词发现。输入语料文件大小90M，出现OutOfMemory

## 复现问题
		NewWordDiscover nd = new NewWordDiscover(4, 0.00005f, .6f, 0.2f, true);
		try {
			List<com.hankcs.hanlp.mining.word.WordInfo> newWords =nd.discovery(this.read(""/content/discuss_content.txt""), 100);
			
			System.out.println(newWords);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

### 步骤

1. 首先，构造NewWordDiscover对象，参数：4, 0.00005f, .6f, 0.2f, true
2. 然后，从文件中读取语料，语料文件约90M
```
	private String read(String fileName) throws IOException {
		StringBuffer result = new StringBuffer();
		BufferedReader  br = new BufferedReader(new FileReader(new File(fileName)));
		
		String s = null;
		while((s = br.readLine())!=null){
            result.append(System.lineSeparator()+s);
        }
		br.close(); 
		return result.toString();
	}
```
3. 接着，运行10分钟左右后报OutOfMemory

### 触发代码
```
    private static void increaseFrequency(char c, Map<Character, int[]> storage)
    {
        int[] freq = storage.get(c);
        if (freq == null)
        {
            freq = new int[]{1};
            storage.put(c, freq);  // <-- 触发OutOfMemory
        }
        else
        {
            ++freq[0];
        }
    }
```
### 期望输出

正常输出新词结果，不报错

### 实际输出
抛出OutOfMemory异常

## 其他信息
JVM内存大小设置：-Xmx4096M -Xms4096M

## 建议
期望在语料较大时，能通过流读取语料文件，防止一次性加载撑爆内存

"
解析txt格式的个人求职简历,"楼主好，有个问题想请教下，我现在有这么个需求：
需要**解析txt格式的个人求职简历**
首先我通过分词来获取求职者的姓名、性别、住址、邮箱等基本信息没有什么问题，
**但是现在需要提取出简历中的求职者的项目工作经验，这个好像很不好搞，没找到好的办法能够完整的提取出项目经验的信息来，楼主有啥办法帮忙指导下么？**
感谢！！！"
把代码中的System.exit替换成RuntimeException异常,"在使用过程中我发现，在遇到异常情况的时候代码基本都采用System.exit来强制退出JVM，这样给HanLP使用带来很多不方便的地方。

分词出错仅仅是HanLP库的错误，不能让整个应用退出，应用的退出应该交给应用程序的开发者来决定。

建议定义一个或多个RuntimeException的子类，然后在HanLP遇到致命错误的时候抛出这个异常，而不是直接退出JVM给应用开发造成不便。"
"繁体简体转换词典中""立體=三維""的一个bug","t2s.txt中
立體=三維

但是""三維""的“維”也是一个繁体字，应该是“三维”"
能否支持观点抽取,"@hankcs    你好   
我想了解一下HanLP能否支持观点抽取
比如:＂服务态度很好，环境也不错，就是点歌系统不太好用。＂
正向观点 : 服务态度很好 环境不错
负向观点 : 点歌系统落后
"
在spark中使用分词器有些问题,"当前最新版本号是：1.3.5
我使用的版本是：1.3.4


## 我的问题
在spark中使用分词器时，报找不到词典！
请问怎样让程序加载放在hdfs上的data目录下的文件，或者说您有没有分词器在分布式计算框架中的一些好的实践？谢谢
"
CQueue.java 两个double型数据直接比较大小？,"原代码：

第32行：
```java
 while (pCur != null && pCur.weight < newElement.weight)
{
            pPre = pCur;
            pCur = pCur.next;
}
```

下面这一句：
```
 pCur.weight < newElement.weight
```
是否应该改成：```Double.compare(pCur.weight, newElement.weight) < 0```

"
可以先分词，后翻译吗?,"## 版本号

当前最新版本号是：1.3.5
我使用的版本是：1.3.5

## 我的问题
HanLP 分词正确，可是繁簡转换结果错误，查找一下，发现 HanLP 好像其中用了 OpenCC 同样词库，所以两边有共同错误，不知是否可以以分词来对比词库?

比如转换 ""计算发现"" 到繁体:
https://github.com/BYVoid/OpenCC/issues/272

https://github.com/BYVoid/OpenCC/issues/224"
如何让书名号内的词语不进行切分？用CustomDictionary.insert() ，不一定能起作用，麻烦。有没有有一个方法，能强制性给予不切分该词？,如何让书名号内的词语不进行切分？用CustomDictionary.insert() ，不一定能起作用，麻烦。有没有有一个方法，能强制性给予不切分该词？求指教。
找不到配置文件？,"_![Uploading image.png…]()
示例配置文件:hanlp.properties 在GitHub的发布页中，hanlp.properties一般和jar打包在同一个zip包中。
配置文件的作用是告诉HanLP数据包的位置，只需修改第一行_
 并未找到配置文件，jar包下载不了"
对于提取不同的词，选取哪种分词方法回更好？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：jar包-1.3.4
我使用的版本是：jar包-1.3.4


## 我的问题

您好！
我现在需要对一篇300字左右的新闻进行关键词提取，关键词的分别为：
```
时间
社会地点（eg：医院、学校）
自然地点（eg：江苏、杭州）
社会地位（eg：省长、总书记）
身份（eg：孕妇、医生）
人名
行为动词（主要是v和vi，eg：报告、安慰）
```
经过尝试以后发现不同的分词方法对于相同的文本会有不同的分词结果，例如有些就会识别更精准的行政机构，而有些则相比较差，因为对于NLP接触时间并不是很长，所以对于各种分词原理了解并不是很清晰。
请问您能否对于这些不同的关键词提取，提供在HanLP现存分词方法中，较好的分词方式，即对于不同的词的提取，采取最好的分词方式？
谢谢！"
运行Demo中的DemoCustomNature.java出现NoSuchMethodError错误,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ √] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：jar包-1.3.4
我使用的版本是：jar包-1.3.4


## 我的问题
您好！
我在运行demo中的DemoCustomNature.java时，出现了在下面“实际输出”中的报错。
操作系统是archlinux，java版本1.9和1.8都尝试过，均出现报错。
参考过[#279](!https://github.com/hankcs/HanLP/issues/279#ref-issue-170997234)的问题，但是无法解决。
请问如何才能正常运行代码？谢谢！

## 复现问题
直接运行了demo

### 触发代码
demo代码中的DemoCustomNature.java

### 实际输出

```
n
null
10月 22, 2017 11:08:47 上午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
Exception in thread ""main"" java.lang.NoSuchMethodError: sun.reflect.ReflectionFactory.newConstructorAccessor(Ljava/lang/reflect/Constructor;)Lsun/reflect/ConstructorAccessor;
	at com.hankcs.hanlp.corpus.util.EnumBuster.findConstructorAccessor(EnumBuster.java:255)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:92)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
	at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
	at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
	at demo.DemoCustomNature.main(DemoCustomNature.java:41)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:131)

```"
像“挺好”，“没想到”等在字典文件CoreNatureDictionary.txt文件中都被标注成了nz,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

像“挺好”，“没想到”等在字典文件CoreNatureDictionary.txt文件中都被标注成了nz，但是实际上它们通常都不是专有名词。对词性分析时造成影响。

## 复现问题

例如，“真是没想到呀。”就会得出“没想到/nz”，“这个东西挺好”就会得出“挺好/nz”。

"
无论改变什么都分不开，包括修改语料本身,"![default](https://user-images.githubusercontent.com/17618881/31812053-555d8bfa-b5b4-11e7-982b-01aaa6531a3b.png)

上第一   ，这个词就是分不开，然后我看原代码发现，代码中将 m m结构给合并。我尝试去改变，上的词性为  上 f 64454 ， 但机器却标注为 m词性？ 为什么上字词性要给我标上 m 这个词性。 我压根都没有标
"
如何获取字符串中的所有可能分词,"我目前采用的是1.3.1版本，使用用户自定义字典，字典中包含：牛仔马甲、 牛仔、 马甲、牛仔马甲外套、马甲。
针对源字符串：“帅气无袖连帽牛仔马甲外套女短款2017秋装新款韩版复古牛仔马夹” 进行分词时，采用的是索引分词器，得到的结果是：“牛仔马甲外套，牛仔，马甲”，无法分词得出“牛仔马甲”。hanlp中是否有办法获取到所有可能存在的分词？"
ViterbiSegment 中粗切算法有些疑问,"ViterbiSegment.viterbi(WordNet wordNet) 
这个算法我大致看了， 但感觉不像维特比算法， 我看算法中只是从后向前取了每个词的到前面那个词的最距离最短的那个词。

比如  ""我爱汉语处理""  ，先取得处理， 接着从处理中取得
‘’汉语‘’ 而不是 ‘语’ ， 因为他们俩的距离最短。  接着再从 “汉语” 取到 '爱' 而不是“我爱” 因为他们俩的距离最短。 

但是维特比算法不是要考虑到所有前面的最短路径， 而我看这个算法只是取了当前词的到前面那个词最短路径所对应词。
"
可以有自定义停词库吗,最新版本可以使用自定义词库，不知道是否支持自定义停词库。试了一下直接在配置文件中停词path后面加路径不支持
CRFDependencyParser.compute出现bug，有两个词互为对方的HEAD,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.3.4
我使用的版本是：hanlp-1.2.8-sources


## 我的问题

在使用CRFDependencyParser.compute(sentence)时，依存关系出现了环的问题，比如“不少iPhone铁杆粉丝选择跳过iPhone8系列，等待iPhoneX的上市。”，结果如下（[word.ID, word.NAME, word.HEAD.ID, word.DEPREL]）：
1	未##数	2	数量
2	iphone	3	限定
3	铁杆	4	限定
4	粉丝	6	受事
5	选择	6	并列
6	跳过	7	限定
7	iphone8	8	限定
8	系列	10	受事
9	未##串	10	受事
10	等待	13	限定
11	iphonex	10	内容
12	的	11	“的”字依存
13	上市	10	内容
      结果中""等待""和""上市""互为HEAD，这应该是不正常的，这是咋回事儿捏？
    （我只是在自定义词库中加了iphone, iphone8, iphonex这些词）

"
请求进行一次常规性更新,距离上一次release发布已经有好几个月了，可否将近几个月的小幅修正整理发布一个新版本的release，同时更新一下Maven版本。
自定义词典读取失败,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.2.8


## 我的问题
我的配置文件里只改变了root路径，经测试无误
但是自定义分词时添加了自己的词典my.txt后总是出现警告读取失败不能显示分词结果，当在自定义词典路径里不加入my.txt时能显示出默认分词结果

## 复现问题
未改变任何已有词典和模型，只添加了自己的词典（UTF-8）编码
且每次都删掉了缓存文件
### 步骤
首先按照步骤把配置文件内容复制好，更改root路径，在自定义词典路径处添加了my.txt(已注意了前面的空格和后面的分号)

### 触发代码
import java.util.List;
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;

public class hanlp
{  public static void main (String[] args)
	{
       String  testline=""测试自定义分词词分义定自试测"";
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
       
       List<Term>termlist =segment.seg(testline);
       for(Term term:termlist)
       {
    	   System.out.println(term.toString());
       }
	}
}

### 期望输出


### 实际输出

无分词结果
 
提示：十月 11, 2017 5:28:24 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadDat
警告: 读取失败，问题发生在java.lang.ArrayIndexOutOfBoundsException: 1056291
	at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToInt(ByteUtil.java:239)
	at com.hankcs.hanlp.corpus.io.ByteArray.nextInt(ByteArray.java:68)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:323)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:66)
	at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:53)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:199)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)
	at hanlp.main(hanlp.java:12)


当删去自定义词典路径里my.txt 时输出
测试/vn
自定义/nz
分词/n
词/n
分/qt
义定/nr
自/p
试/v
测/v

自定义词典my.txt的内容：
试测"
#623尚未修改完全的部分：确保中文数字的词性为m,"
## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

确保中文数字的词性为m
字符类型统一使用CharType类中的常量，但TextUtility中的保留以便剥离使用

## 相关issue

#623"
你好，我将我自己训练好的文件放到替换了原有模型文件，我的模型在crf++中有97的正确率，但是在这里只有30，也是4tag，我不知道是模型加载出现了问题，还是解码过程中出现了问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请求和建议（违禁词、敏感词匹配）！,"您好！
感谢百忙中回复！
只是一个请求。
目前有类似的需求：
**将一段文本中的违禁词（和谐词）进行匹配和过滤**
目前结合HanLP，我觉得先对文本进行分词，将分词后的词和敏感词库进行对比（采用hash或者trie等）
能达到 n O(1) 的时间复杂度（比如直接使用hashmap）。
不知HanLP有没有更大的办法，能够在分词过程中直接匹配某个特殊子词库的功能（违禁词库）
谢谢！

"
咨询HanLP与LTP的关系，及License相关问题,"Hi，
  请问该项目和LTP是什么关系，哪些是移植，哪些是重新编写呢？如果商用，是否需要联系LTP取得授权呢？
  因为看到其他issue中提到依存句法分析模型训练是通过LTP训练出的。

THX

"
人民日报的标注预料库是人工的还是机器做的呢？,"
## 我的问题

人民日报的标注预料库是人工的还是机器做的呢？

"
python调用HanLP.parseDependency的时候报错,"
报错信息：jpype._jexception.LinkageErrorPyRaisable: java.lang.ExceptionInInitializerError，求助怎么解决
"
字标注分词模块更新：优化2阶HMM分词，CRF数字字母词标注,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

1. 为2阶HMM分词（基于字的生成式模型）增加了状态转移矩阵，用于搜索路径剪枝。速度提高近两倍。且效果得到较大提升。#566
2. 通过和CRF共用Vertex转换代码，HMM分词也支持词性标注和自定义词典合并。
3. CRF分词结果中的数字和字母不再标注为""nz'。分别标注为‘m’和‘nx’ ，这样在用于新词提取时可以仅仅拿到新词。 #196 

## 相关issue




"
java web项目中data文件夹和hanlp.properties应该放在哪里,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题
请问如何在Java web项目中放置data文件夹和hanlp.properties呢？应该放在哪里才能加载到
"
可以增加针对elasticsearch的支持吗,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

能针对elasticsearch 5.x做支持吗

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
“中国移动通信集团” 和 “中国移动” 2个词的分词词序不一样,"当前最新版本号是：c57895f14801df54312e8b656ea8ac2eeef72f99
我使用的版本是：c57895f14801df54312e8b656ea8ac2eeef72f99

## 我的问题
中国移动通信集团 和 中国移动 2个词的分词词序不一样，导致solr完全匹配搜索的时候搜不到数据

## 复现问题

### 触发代码

```
        List<Term> termList = IndexTokenizer.segment(""中国移动通信集团"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
        List<Term> termList2 = IndexTokenizer.segment(""中国移动"");
        for (Term term : termList2) {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

```
### 实际输出

```
中国移动通信集团/nt [0:8]
中国移动通信/nz [0:6]
中国移动/nz [0:4]
中国/ns [0:2]
移动通信/nz [2:6]
移动/vn [2:4]
通信/vn [4:6]
集团/nis [6:8]

中国移动/nz [0:4]
中国/ns [0:2]
移动/vn [2:4]
```

”中国移动通信集团“的分词结果中 “中国”和“移动”之间多了一个“移动通信”，导致和“中国移动”的分词结果顺序不匹配。这样在solr中使用text:""中国移动""这种全词匹配模式搜索的话，是搜不到数据的

之前因为词序的事情已经麻烦过你了，经过上次的修改，分词的词序已经趋于稳定，但还有少数有问题，这个就是一个特例，希望能够得到修正，再次感谢！
"
111,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
DoubleArrayTrie中的一个bug,"/**
     * 精确查询
     *
     * @param keyChars 键的char数组
     * @param pos      char数组的起始位置
     * @param len      键的长度
     * @param nodePos  开始查找的位置（本参数允许从非根节点查询）
     * @return 查到的节点代表的value ID，负数表示不存在
     */
    public int exactMatchSearch(char[] keyChars, int pos, int len, int nodePos)
    {
        int result = -1;

        int b = base[nodePos];
        int p;

#如果len是键的长度那么，循环应该是  for (int i = pos; i < pos+len; i++)
        for (int i = pos; i < len; i++)   

        {
            p = b + (int) (keyChars[i]) + 1;
            if (b == check[p])
                b = base[p];
            else
                return result;
        }"
添加用户自定义词典后没起作用,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：portable 1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先…… 添加了用户自定义词典，包括nova,p10等词，词性为pr
2. 然后……使用Hanlp进行分词，未得到理想效果
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        List<Term> termList = HanLP.segment(""novap10"");
        System.out.println(termList);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```nova/pr,p10/m 

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
   
```
实际输出
```novap/nx,10/m
   novap作为一个英文词输出，10作为一个数字词输出

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
Merge pull request #1 from hankcs/master,"update from origin

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
分词疑问：“钱管家中怎么绑定网银”,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
分词疑问，添加了自定义词，但是没有分词出来，而是人名识别依然起效果，但感觉人名识别也不对。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""钱管家"");
        System.out.println(HanLP.segment(""钱管家中怎么绑定网银""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[钱管家/n,  中/, 怎么/ryv, 绑定/gi, 网银/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[钱管/nr, 家中/s, 怎么/ryv, 绑定/gi, 网银/n]
```

## 其他信息

从分词结果看，`钱管`分词为一个人名了，添加自定义分词后，没有任何影响。
然后，我关闭人名识别功能，发现还是没有作用，必须要向CoreNatureDictionary.ngram.txt中
添加
```
钱管家@中 10
```
并且，将`钱管家`添加到CoreNatureDictionary.txt中才行。



"
是否考虑出个python版？或者提供python接口？,向python开放应该是个好的方向。
Hanlp使用,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用Hanlp分词的时候发现效果不错，所以想应用到某一个特定领域（例如新闻）。苦于对整个工程的原理和工程实现不是很清楚。作者能否出个专题（或者书籍）对广大READER介绍一下语料库的收集，使用，处理；模型的训练，调优测试；以及后续的维护等主题

@hankcs 
"
test,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
分词过程结果的不理解,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题
我在调试标准分词和N-short最短路分词的过程中遇到如下问题：
String testSentence = ""重庆邮电大学是一所工科大学，坐落在美丽的南山上。这里是数字通信发源地。"";
        //标准分词
        List<Term> standList = HanLP.segment(testSentence);
        System.out.println(""标准分词："" + standList);
输出结果是：
标准分词：[重庆邮电大学/ntu, 是/vshi, 一所/n, 工科大学/l, ，/w, 坐落/vi, 在/p, 美丽/a, 的/ude1, 南山/ns, 上/f, 。/w, 这里/rzs, 是/vshi, 数字通信/nz, 发源地/n, 。/w]
“重庆邮电大学”这个专有名词是在机构名词典里面的，标准分词并没有开启机构名词典这个模型，输出结果应该是“重庆”+“邮电大学”。当在机构名词典去掉“重庆邮电大学”这个词语后，分词结果和预想的一样，是“重庆”+“邮电大学”。说明调用标准分词的时候用到了机构名词典里面的数据，这和代码逻辑不一样，麻烦帮忙解答一下"
读取crf模型的时候得到的权重值不要人工去设定；具体位置在第一个字不可能M或者E,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

事例：U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-1,0]/%x[0,0]
U06:%x[0,0]/%x[1,0]
"
自定义词性的用户词典导入,"## 版本号

当前最新版本号是：1.3.4
我使用的版本是：portable-1.3.4


## 我的问题
根据[issue243](https://github.com/hankcs/HanLP/issues/243)

那可否在加载用户自定义词典文件前，先用CustomNatureUtility.addNature(“新词性”);然后再用ACDoubleArrayTrieSegment.loadDistionary来导入带有自定义词性的用户自定义词典文件？
我做了如下操作:
```
CustomNatureUtility.addNature(“新词性1”);
CustomNatureUtility.addNature(""新词性2"");
AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment()
                .loadDictionary(HanLP.Config.CustomDictionaryPath[1]);
```
发现报错：
java.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.新词性2

跟踪了一下代码发现上述代码添加词性之后，词性常量里面，只添加成功了新词性1，没有新词性2.

如果只添加一个新词性1并且词典文件中只有新词性1的话，就可以正确对分词结果标注用户定义的新词性1，添加两个新词性，就不可以，不知是否有人遇到类似问题？谢谢。



"
请问是否可以提取指定的关键字,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 我的问题

假如说我有一个关键字列表 (10000+)；我想提取一个文章的所有的可以匹配的关键字(没有匹配的全部剔除)；请问 HanLP 可以实现吗？

请问大家是否有相似的，高性能的开源扩展库，也可以推荐一下。主要用于提取指定关键词列表的关键词 (这个标关键词表有点大)。

比如：  
Shell是Linux运维中必不可少的脚本语言，CentOS是一款伟大的操作系统。  
我只需要：Shell，Linux，Linux运维，脚本语言，CentOS，操作系统  
这些关键字(标签)是我事先定义好的。  
"
为Term类添加equal方法,"
## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

为Term类添加了equal方法，主要用于解决以往判断Term是否相等，始终返回false的问题；
目前使用了词性和词语内容来校验是否相等。



"
fix chinese numbers link with roman numbers,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

目前的版本对中文数字和罗马数字没有分开，例如：“赵四158开头的号码”会把“四158”分到一起。原因是构建词图的时候就没有切开，通过改词典并不能解决问题。

新版本新增CT_CNUM类型表示中文数字类，经测试，构建词图时，中文数字和罗马数字会切开，其他逻辑不变。“赵四158开头的号码”会把“赵四”，“156”单独分开。

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
句子”仍有很长的路要走“分词错误,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 句子”仍有很长的路要走“分词错误

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
没有修改

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(HanLP.segment(""仍有很长的路要走""));
    }
```
### 期望输出

```
[仍/d, 有/vyou, 很长/d, 的/ude1, 路/n, 要/v, 走/v]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[仍/d, 有/vyou, 很长/d, 的/ude1, 路要/nr, 走/v]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

## 分析
从结果看，好像是人名识别影响了。
但这个句子，不应该有人名的，这里错误将 路要 分成了人名了。

"
时间能否作为一个整体识别出来呢？,"在分词的时候我发现时间都是分开了，比如说2017年9月8日，分词结果为：
2017 年 9 月 8 日 
能不能把他们作为一个整体识别出来呢？而不是分开的，可以在用户词典里面加入正则表达式来把时间作为一个整体进行分割吗？"
机构名识别出后添加到词网中， 未##团 词性读取错乱。,"<!-- 请详细描述问题，越详细越可能得到解决 -->
机构名识别，
已经识别出的机构名创建新的节点并 inset 到词网中，
根据 未##团 获得的 ATTRIBUTE读取错乱，
以下是 debug 截图，和我根据词频搜索出的唯一结果，

![31caf6d28e7006c994a7a6fc554953c](https://user-images.githubusercontent.com/26315728/30159424-19cfb804-93fb-11e7-9e55-aa2e3809785e.png)

可以看到，词频顺序是对的，词性错乱了。
删除缓存文件后问题得到解决。
这种情况偶尔发生。
当然，我修改了代码，而且有点多。
能想到的就是在读取核心词典的时候错乱了，
希望给出建议。
谢谢。

"
pku_training.bmes.txt这个文件在哪里？,"使用这个命令时，`crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t`
需要 pku_training.bmes.txt文件，但是项目里没有，是从哪里可以找到么"
算法人名识别有点奇怪？,"他叫小明却能切出小明。
![default](https://user-images.githubusercontent.com/17618881/30004018-4e074046-90fa-11e7-8a8e-993775e333c0.png)

小萍就不对，切成小 萍。
![default](https://user-images.githubusercontent.com/17618881/30004013-286dc738-90fa-11e7-8107-67c5aeb6b0e2.png)

小花却又错。

我如何自定义一下，但我不可能所有的名字有小 #什么的人名都定义的，我如何让所有叫@人## 就能匹配到小 后面的全部人名都切对？    

"
增加核心数据，到40多M后，大概是100万条词条后，出现 java.lang.OutOfMemoryError: Java heap space,"你好，本人来自北京航空航天大学一名学生，发现程序可能存在的一个BUG，想请教一下。为何增加核心数据，到40多M后，大概是100万条词条后，出现 java.lang.OutOfMemoryError: Java heap space
![default](https://user-images.githubusercontent.com/17618881/29923940-c1204f9c-8e8d-11e7-887f-0422be6bcfe3.png)

请问你是否那里写错？JAVA应该不会这么烂的，渴望能得到你的回答。谢谢
"
hanlp计算同义词相似度中，同一类别的词相似度为什么不高,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
我在计算同一类中出现的两个词之间的相似度时，本以为相似度会很高，而实际计算结果却不理想。
<!-- 请详细描述问题，越详细越可能得到解决 -->
比如说，我计算“孤”和“寡人”两个词的相似度时，发现相似度不够，所以我有点疑问，当一个词存在多个类别时，对于该类别选择的机理是什么?
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
String[] wordArray = new String[]
            {
                ""孤"", ""寡人""
            };
        System.out.printf(""%-5s\t%-5s\t%-10s\t%-5s\n"", ""词A"", ""词B"", ""语义距离"", ""语义相似度"");
        for (String a : wordArray) {
            for (String b : wordArray) {
                System.out.printf(""%-5s\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
            }
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
词A   	词B   	语义距离      	语义相似度
孤    	孤    	0              	1.0000000000
孤    	寡人   	0              	1.0000000000
寡人   	孤    	0              	1.0000000000
寡人   	寡人   	0              	1.0000000000
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
词A   	词B   	语义距离      	语义相似度
孤    	孤    	0              	1.0000000000
孤    	寡人   	28319444208    	0.6188445149
寡人   	孤    	28319444208    	0.6188445149
寡人   	寡人   	0              	1.0000000000
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
你好，请问这个项目有没有聊天机器人的子项目或者改为聊天机器人的计划和项目推荐吗？,你好，请问这个项目有没有聊天机器人的子项目或者改为聊天机器人的计划和项目推荐吗
能够将识别出来的机构名称进行提取吗？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 我的问题
您好！我在使用您的HanLP的时候发现机构命名识别率很高。
但是有什么好的方法将识别出来的机构名称提取出来？





"
solr中强制该次请求不使用分词,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

在solr查询的过程中，我想强制某查询字段该次请求不使用分词，一直没有找到应该修改哪里。

例如：
solr在查询时，会对传入的值分词， 中国人有可能会被分成“中国”，“国人”，“中国人”。
包含着三个短语的语句都会被搜到。强制要求solr不分词，直接匹配中国就好

"
核心词典CoreNatureDictionary.txt加载失败,"

在使用hanlp分词时，一直显示核心词典CoreNatureDictionary.txt加载失败，仔细确认过hanlp.properties中的路径，加载了相关jar文件，仍然无法执行程序。 
"
关于license问题，hankcs 你好，我有一个分词项目，使用了hanLP分词算法和代码,"hankcs 你好，我有一个分词项目，使用了hanLP分词算法和代码。

主要进行了大量的重构和结构的调整，修改面积过大没法提交pull 

如果也在github上开源，是否可以

如果可以，在license怎么声明，因为我看之前您的文档写明了版权和上海林源公司有关，是不是影响license选择"
智能推荐算法原理是什么？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.2.8
我使用的版本是：1.2.8


## 我的问题

1.作者大人！我看过HanLP的文档1.1.2，这里对智能推荐的原理没有介绍，能回复一下是用什么算法做的吗？或者原理名字或者链接什么都行
2.作者大人，我也来说个题外话，希望您能看见，之前在别的issue中看到过您说很多人问问题之前都没有好好看文档，其实我真的有认真看文档，并且把您附的链接都打开看了，但是我比较关注算法，希望能弄懂算法，然后可以根据自己的需求去修改程序。我现在看到智能推荐这里，真的没有写算法是什么。。。
3.我的废话完了，上天保佑，作者大大能看到我的留言


"
语义距离中的同义词词林怎么怎么修改,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.2.8
我使用的版本是：hanlp-1.2.8


## 我的问题

您好，我在使用HanLP中的语义距离时发现您是利用同义词词林的一维映射实现的，并且在以往的issue中看到您说，可以自己根据同义词词林的格式增加新词，那我想知道怎么添加？添加完还要按照您的映射方式进行映射对吗？能否像分词功能的customDictionary一样？


"
添加了类序列化接口，可支持spark集群调用,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
添加了类序列化接口，可以在Spark集群上并行化使用HanLP；
使用方法:
1. 在com.hankcs.hanlp.utility.Config中修改放置词典的hdfs路径
2. 重新打jar包
3. 调用方法
```
import com.hankcs.hanlp.Config;
import com.hankcs.hanlp.seg.Viterbi.ViterbiSegment;
import com.hankcs.hanlp.corpus.io.IIOAdapter
import java.io._;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.net.URI;
 
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }   
 
    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}

val a = sc.parallelize(Seq(""互联网搜索算法也成为当今的热门方向。"",""算法工程师逐渐往人工智能方向发展。""))
val test = a.map(e=> {
    Config.IOAdapter = new HadoopFileIoAdapter(); 
    val vs = new ViterbiSegment();
    vs.seg(e)
    })

test take 10
```


## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->
#588 "
如何只使用自定义生成的词典分词？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
Hancks，你好：
       因为业务需要，目前只想使用自己生成的词典进行分词，请问有关闭系统词典的方法吗？



"
简繁转换错误：姓名“梁鹏”转为繁体后变成了”樑鹏”,"版本：portable-1.3.4

"
怎样拿取匹配到的所有词性？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.3.4
我使用的版本是：v1.3.4


## 我的问题
我自定义了一个词性“深圳”为 shenzhen，而深圳的基础词库里的词性是 ns 地名词性，我代码里拿取深圳的词性只能拿到 shenzhen 词性了，有办法能拿到深圳这个词所匹配的所有词性吗？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
       System.out.println(HanLP.segment(""深圳""));
        CustomDictionary.insert(""深圳"",""shenzhen 1"");
        System.out.println(HanLP.segment(""深圳""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[深圳/ns,/shenzhen]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[深圳/ns]
[深圳/shenzhen]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
OrganizationRecognize,"请教一下，开启OrganizationRecognize后为什么要重新生成切分图？ 我使用自定义词典的词，在这次重新生成后就失效了。

"
root 写相对路径但是读取文件一直有异常,"我使用的版本是：hanlp-1.3.4    master

我的问题
我把写好的程序集成到web系统上，data放在了web/asset/HanLP-master下面，有看到一个issue里提到的，所以我写了System.out.println(new File(""."").getAbsolutePath());输出的是D:\jboss\jboss-4.2.2.GA - FJ\bin\.   我写成root=../assets/HanLP-master/是不行的，那该怎么写呢

 期望输出
读取data进行分词

实际输出
读取../assets/HanLP-master/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException:




"
关于唐诗人名的简繁、拼音体转换以及convertToPinyinString函数的建议,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

1. 唐诗诗人人名繁体转简体出现乱码
2. 某些人名似乎可不转
3.多音字人名不正确
4.convertToPinyinString函数建议


 

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 下载jar，加入工程，运行代码
2. 然后从 https://github.com/chinese-poetry/chinese-poetry 获得json
3. 接着转换json，打印诗人人名、简体、拼音

### 触发代码

```
     Log.d(TAG, HanLP.convertToSimplifiedChinese(""裴諴""));
    Log.d(TAG, HanLP.convertToSimplifiedChinese(""拾得""));
    List<Pinyin> pinyinList = HanLP.convertToPinyinList(""曾扈"") ;
    String pyname="""";
    for (Pinyin pinyin : pinyinList)
    {
      pyname+= pinyin.getPinyinWithToneMark()+"" "";
    }
    Log.d(TAG, pyname);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
裴諴 拾得 zeng hu
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
裴��   十得 céng hù 
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

另外不知道为什么convertToPinyinString(String text, String separator, boolean remainNone)这么设计，能不能加个convertToPinyinString(String text, int start,int len)? 谢谢。"
自定义词典没有起到作用？,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
 我是通过maven直接引入的
   <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.3.4</version>
   </dependency>

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

自定义词典不起作用

## 复现问题

自定义词典不起作用

### 步骤

```
我在配置文件hanlp.properties中加入了
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CustomDictionary/自定义词典.txt
```
自定义词典格式如下：

```
罗氏婴儿配方粉 n 1000
挂花大头菜 n 1000
黄毛籽 n 1000
青豆 n 1000
儿童营养饼干 n 1000
汤菜 n 1000
青萝卜 n 1000
```

分词结果：

### 触发代码

```
String str = ""罗氏婴儿配方粉是什么?"";
        CoNLLSentence sentence = new NeuralNetworkDependencyParser().enableDeprelTranslator(false).parse(str);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s--%s --(%s)--> %s\n"",word.ID, word.LEMMA, word.DEPREL, word.CPOSTAG);
        }
```
### 期望输出

```
1--罗氏婴儿配方粉 --(SBV)-->n
2--是 --(HED)--> v
3--什么 --(VOB)--> r
4--? --(WP)--> wp
```

### 实际输出

```
1--罗氏 --(ATT)--> nz
2--婴儿 --(ATT)--> n
3--配方 --(ATT)--> n
4--粉 --(SBV)--> a
5--是 --(HED)--> v
6--什么 --(VOB)--> r
7--? --(WP)--> wp
```



"
solr中特殊符号导致了高亮位置偏差,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.3.4
我使用的版本是：hanlp-portable-1.3.4
hanlp-solr-plugin-1.1.2
solr版本：6.3.0
tomcat版本：8.5.15

## 我的问题
部分特殊符号导致高亮位置有变差
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
配置好词典，solrcloud应用成功，查询分词，设置高亮，通过solr的http接口看到，高亮标记的位置不对
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

<!-- 你希望输出什么样的正确结果？-->

### 实际输出
正常情况应该是把故事高亮，但是一旦有？的存在，就会导致高亮位置变化，标记失败，如下highlighting字段的返回：id=4444444444444是正确的，其它都是因为特殊符号？而导致分词出现偏差。
目前测试发现？会导致这种情况，暂时没有发现其它字符导致这种情况




<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
{
  ""responseHeader"":{
    ""zkConnected"":true,
    ""status"":0,
    ""QTime"":2011,
    ""params"":{
      ""q"":""name:故事"",
      ""hl"":""on"",
      ""indent"":""on"",
      ""hl.fl"":""name"",
      ""wt"":""json"",
      ""_"":""1501643817193""}},
  ""response"":{""numFound"":3,""start"":0,""maxScore"":0.6395861,""docs"":[
      {
        ""id"":""22222222222"",
        ""name"":""魯豫有約: ？出你的故事"",
        ""_version_"":1574587208305737728},
      {
        ""id"":""33333333333"",
        ""name"":""魯豫有約:？出你的故事"",
        ""_version_"":1574587388861087744},
      {
        ""id"":""4444444444444"",
        ""name"":""魯豫有約:出你的故事"",
        ""_version_"":1574587451757821952}]
  },
  ""highlighting"":{
    ""22222222222"":{
      ""name"":[""魯豫有<em>約:</em> ？出你的故事""]},
    ""33333333333"":{
      ""name"":[""魯豫有<em>約:</em>？出你的故事""]},
    ""4444444444444"":{
      ""name"":[""魯豫有約:出你的<em>故事</em>""]}}}
```

## 其他信息
具体配置如下：
managed-schema中配置
```
<field name=""name"" type=""hanlp_cn"" indexed=""true"" stored=""true"" multiValued=""false"" />

   <fieldType name=""hanlp_cn"" class=""solr.TextField"">
      <analyzer type=""index"" enableCustomDictionary=""true"">
          <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" 
	             enableIndexMode=""true"" 
		     enableCustomDictionary=""true"" 
		     customDictionaryPath=""路径隐藏"" 
		     stopWordDictionaryPath=""路径隐藏""
		     enableTraditionalChineseMode=""true""/>
          <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
          <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
      <analyzer type=""query"">
          <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" 
		    enableIndexMode=""false""
		    enableCustomDictionary=""true""
                    customDictionaryPath=路径隐藏""
                    stopWordDictionaryPath=""路径隐藏""
                    enableTraditionalChineseMode=""true""/>
          <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
          <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
          <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
  </fieldType>

```
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
1.3.4版本自定义词典发现问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.2.8


## 我的问题
当我使用1.3.4版本的时候，在hanlp.properties中添加自定义词典的路径（只写一个CustomDictionaryPath配置，其他都不写），就会报出找不到核心词典的错误，当换到1.2.8版本之后，同样的配置文件使用起来没有任何问题。不知道是我配置的原因，还是本身项目的bug
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先
我的配置文件内容为：
root=F:/hanlp/
CustomDictionaryPath=data/dictionary/custom/xxx.txt;
2. 然后
运行程序会报出
警告: 读取F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt.bin时发生异常java.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt.bin (系统找不到指定的文件。)
八月 08, 2017 4:20:46 下午 com.hankcs.hanlp.dictionary.CoreDictionary load
警告: 核心词典F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt不存在！java.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt (系统找不到指定的文件。)
八月 08, 2017 4:20:46 下午 com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
严重: 核心词典F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt加载失败
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
能不能借阅一下你们生成CRFSegmentModel.txt.bin原文件CRFSegmentModel.txt,"请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ √] 我在此括号内输入x打钩，代表上述事项确认完毕。

 1、我现在试着想加入词性，重新构造特征模版，可不可以参考一下你们的
"
增加两个新的文本摘要抽取方法，在抽取文本摘要时指定句子分割符，改变默认的句子分割符,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
原来的文本摘要抽取方法无法指定句子之间的分割符，因为逗号是默认分割符之一，会造成抽取结果语义破碎。增加两个新的文本摘要抽取方法，在抽取文本摘要时指定句子分割符。原来的抽取方法仍然会采用默认的句子分割符。

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue
无。
测试方法为：src/test/java/com/hankcs/test/other/TestExtractSummary.java
<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
现在elasticsearch 如日中天，谁能提供一个 elasticsearch与 HanLP结合的分词插件？谢谢,
关键词提取可能会把自定义词性的词全部过滤掉,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
关键词提取时会去掉停用词，即通过 KeywordExtractor的shouldInclude()方法，
任何自定义词性如果首字母满足switch条件，都会被作为停用词去掉，无法提取任何自定义词性的词汇作为关键词。我在复现步骤提供一个简单的case。
shouldInclude 的代码如下:
```
public boolean shouldInclude(Term term)
    {
        // 除掉停用词
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)  
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (term.word.trim().length() > 1 && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }
```
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""我是自定义的词"", ""mZiDingYi 1111"");
        String msg = ""测试一下,我是自定义的词我是自定义的词我是自定义的词我是自定义的词,测试一下"";
        System.out.println(HanLP.segment(msg));
        System.out.println(TextRankKeyword.getKeywordList(msg, 100));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[测试/vn, 一下/m, ,/w, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, ,/w, 测试/vn, 一下/m]
[测试, 我是自定义的词]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[测试/vn, 一下/m, ,/w, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, ,/w, 测试/vn, 一下/m]
[测试]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
在提取关键词的过程中，根据词性过滤得出的关键词,"在提取关键词的过程中，我想根据词性过滤，只留下词性为名词的关键词。
我的想法是在分词的步骤中，就按照词性去过滤，只留下名词，但是在标准分词源码部分，并没有找到关于词性的代码，hankcs能否指点一下小弟呢？不胜感激
"
分词粒度是否可以配置,"hanlp的分词粒度可否配置？
比如""团购网站的本质是什么？""能否通过配置粒度，分词结果保存不同粒度的结果“团购网站/团购/团购网/网站/的/本质/是/什么” "
Add docker image links in README,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
快速集成HanLP服务，容器化，微服务，减少依赖和部署成本。




"
Wiki 项目介绍请帮忙修改一下,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

衍生项目中，之前我描述的不准确，**应该将 (HanLP 自然语言处理 for nodejs)  改成 (HanLP Docker Image: 自然语言处理)**。

https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE

![image](https://user-images.githubusercontent.com/3538629/28705638-52d03156-73a3-11e7-8111-83bdd253211a.png)

"
猜测最可能的词性方法数组越界,"com.hankcs.hanlp.seg.common.Vertex
 public Nature guessNature() {
        return attribute.nature[0];
    }


这个方法数组越界"
修复数词判断的错误,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

目前我们正在基于HanLP进行分词后数量表达式识别的二次开发，
在开发中发现了HanLP基本数字识别中存在不少问题，且问题在较为底层的Utility包中。
因此进行了修改并发起该PR。

## 相关issue

（无）
参照TestTextUtility.java测试"
在Spark单机使用正常，map到节点上使用出现错误,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4

@hankcs 您好，按照您说的方法，修改配置
root=hdfs://localhost:9000/hanlpdata/
IOAdapter=com.xxx.HDFSIOAdapter
而且在scala重写了接口：
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }    

    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}
HanLP.Config.IOAdapter = new HadoopFileIoAdapter();

也只能在单机spark下运行分词
scala> System.out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！""));
[你好/vl, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/gi, 处理/vn, 包/v, ！/w]


分发到节点的时候就出错了，请问你们在集群上使用的时候是否遇到过这种问题？需要注意哪些坑呢？
val a = sc.parallelize(Seq(""中国的神威太湖之光计算机被用于天气预报"",""制药研究和工业设计等领域。""))
val res = a.map(e=>{
        HanLP.segment(e).toString
    })
res take 2 foreach println

Exit code: 255
Stack trace: ExitCodeException exitCode=255:
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:578)
	at org.apache.hadoop.util.Shell.run(Shell.java:489)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:755)
	at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:297)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)"
代码质量太差,"这个项目是个好东西，可惜代码质量太差
特别是 IO读写的地方，很多inputStream，outputStream打开了就没关闭过，会造成资源泄露。

下面的代码，br.readLine()抛异常后，这个流就永远不会被关闭。。。
我尝试修改这个项目的代码，发现这样低质量代码实在太多根本改不过来
try
        {
            BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(path), ""UTF-8""));
            while ((line = br.readLine()) != null)
            {
                Map.Entry<String, Map.Entry<String, Integer>[]> args = EnumItem.create(line);
                EnumItem<NR> nrEnumItem = new EnumItem<NR>();
                for (Map.Entry<String, Integer> e : args.getValue())
                {
                    nrEnumItem.labelMap.put(NR.valueOf(e.getKey()), e.getValue());
                }
                valueList.add(nrEnumItem);
            }
            br.close();
        }
        catch (Exception e)
        {
            logger.severe(""读取"" + path + ""失败["" + e + ""]\n该词典这一行格式不对："" + line);
            return null;
        }"
"""大学城里""分词错误的问题","## 版本号

当前最新版本号是：v1.3.4
我使用的版本是：v1.3.4


## 我的问题

对以下句子进行分词""深圳大学城里各大国内外知名高校合办学校校牌在阳光下熠熠发光。""
发现“深圳大学城里”分词错误，可以在CoreNatureDictionary.ngram.txt中添加""大学城@里 10""解决该问题。

## 复现问题

### 触发代码

```
public class DemoSegment
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""深圳大学城里各大国内外知名高校合办学校校牌在阳光下熠熠发光。毗邻深圳大学城的是以创新产业为发展核心""
        };
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            System.out.println(termList);
        }
    }
}

```
### 期望输出

```
深圳大学/ntu, 城里/s, 各/rz, 大/a, 国内外/s, 知名/a, 高校/n
```

### 实际输出

```
深圳/ns, 大学城/nz, 里/f, 各/rz, 大/a, 国内外/s, 知名/a, 高校/n
```
"
“广州大学城”与“深圳大学城”分词结果不同的问题,"## 版本号

当前最新版本号是：v1.3.4
我使用的版本是：v1.3.4


## 我的问题

对包含“深圳大学城”和“广州大学城”的句子进行分词，发现分词结果不一致。

### 触发代码

```
public class DemoSegment
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""深圳大学城和广州大学城各大国内外知名高校合办学校校牌在阳光下熠熠发光。毗邻深圳大学城的是以创新产业为发展核心""
        };
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            System.out.println(termList);
        }
    }
}
```
### 期望输出

```
深圳/ns, 大学城/nz, 和/cc, 广州/ns, 大学城/nz
```

### 实际输出
```
深圳大学城/nz, 和/cc, 广州/ns, 大学城/nz
```
"
停止词中添加控制字符的问题,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。


当前最新版本号是：1.3.4
我使用的版本是：1.3.4

我希望在停用词中添加 ""\t""  这样的控制字符 ,因为切分 ""运动用途	用法""(中间是""\t"")这个词的时候结果是:
运动/vn
用途/n
	/nx
法/n

不知道为什么把制表符认为成了字母专名.
然后我再stopword.txt中添加""	"" 当读取停止词词典就报下边的错:

严重: 载入停用词词典D:/Idea_workplace/HanLP/data/dictionary/stopwords.txt失败java.lang.StringIndexOutOfBoundsException: String index out of range: 0
	at java.lang.String.charAt(String.java:658)
	at com.hankcs.hanlp.collection.MDAG.MDAG.replaceOrRegister(MDAG.java:541)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:220)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:170)
	at com.hankcs.hanlp.collection.MDAG.MDAGSet.<init>(MDAGSet.java:42)
	at com.hankcs.hanlp.dictionary.stopword.StopWordDictionary.<init>(StopWordDictionary.java:44)
	at com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary.<clinit>(CoreStopWordDictionary.java:42)
	at com.hankcs.demo.DemoCustomDictionary.main(DemoCustomDictionary.java:79)

不知道是不是我的用法有问题?

感谢大神的开源,方便我很多  :)"
"""仙剑奇侠传""多音字拼音（chuan）","<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：
```
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.3.4</version>
</dependency>	
```


## 我的问题
“仙剑奇侠传”得到的拼音是“xian,jian,qi,xia,chuan”
<!-- 请详细描述问题，越详细越可能得到解决 -->

### 触发代码

```java
@Test
public void testPinyin() {
    String word = ""仙剑奇侠传"";
    System.out.println(word + "": "" + HanLP.convertToPinyinString(word, "","", false));
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
xian,jian,qi,xia,zhuan
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
xian,jian,qi,xia,chuan
```


"
对“训练HMM-NGram分词模型”产生的问题和理解,"对你在 [训练分词模型](https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B) 中提到的HMM-NGram分词模型产生了几个问题

1、HMM是由输出量求输入量的模型，请问这里的分词输出量是什么，是指分词的序列吗？那输入量又是什么呢？

HanLP中使用了一阶隐马模型，在这个隐马尔可夫模型中，隐状态是词性，显状态是单词。见[词性标注](http://www.hankcs.com/nlp/part-of-speech-tagging.html)


2、训练后一共得出3个文件：

CoreNatureDictionary.txt：单词词性词典
CoreNatureDictionary.ngram.txt：二元接续词典
CoreNatureDictionary.tr.txt：词性转移矩阵

请问这几个字典的作用？你能简单地用最短分词结合上面的字典做一个梳理吗，即是如何分词，又是如何做词性标注的？

我的理解：

CoreNatureDictionary.txt和CoreNatureDictionary.ngram.txt是用来分词的，CoreNatureDictionary.tr.txt是用来做词性标注的。在S-segment中，先通过动态规划，即用字典CoreNatureDictionary.txt和CoreNatureDictionary.ngram.txt生成词图（动态规划路径图），然后选择最短的路径。这里只是用到了MM，并没有用到HMM，属于机械式规则+统计的分词方法。
而CoreNatureDictionary.tr.txt正如问题1的说的，根据输入的分词序列来判断词性的序列。

有资料中说到，分词方法的演进，可分为
1、机械式规则
2、规则+统计
3、MM+Viterbi
4、由字构词
5、神经网络

我认为3其实也是2，MM首先也要将句子进行动态规划进行分词（机械规则），也就是查字典，先将句子中所有可以查到的词（查核心字典），先划分出来，然后按时词到词出现的顺序（二元连续字典）生成路径的权重，也就是构造词图。"
英文数量词的识别问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.3


## 我的问题

在开启数量词识别后，英文的数量没有识别

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
Segment segment = HanLP.newSegment()
              .enablePlaceRecognize(true)
              .enableIndexMode(false)
              .enableOrganizationRecognize(true)
              .enableNumberQuantifierRecognize(true);
System.out.println(segment.seg(""18k金项链""));
System.out.println(segment.seg(""18k""));
System.out.println(segment.seg(""18kg""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[18k/mq, 金项链/nz]
[18k/mq]
[18kg/mq]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[18/m, k/nx, 金项链/nz]
[18/m, k/nx]
[18/m, kg/nx]
```

## 其他信息

我在CoreNatureDictionary.txt中添加了下面记录后倒是得到了想要的结果
```
k q 1000
kg q 1000
```

问题是为什么核心词典里没有这些词的配置，是有什么特殊原因吗？？？

"
ngram词典重新加载,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： 1.3.4
我使用的版本是： 1.3.3


## 我的问题

现在ngram词典很多标识的不是很准确，而且也有很多缺失的（当然，很大一部分的原因是因为我们业务需求的问题），所以现在打算人工修改词典，然后做一个ngram词典的热更新，问题是：

**如果要重新加载ngram词典，是否删除缓存然后调用com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary#load这个方法即可？**




"
http://ictclas.nlpir.org/nlpir/ 请问跟这个比，区别在哪,
修正全角年份识别中字符串长度错误,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

在MSR语料的分词测试中发现。年份判断中有一处错误，导致不能识别全角字符年份，

## 相关issue

（无）"
TextRank 里面 max_iter = 200 如果文本字数多，性能有影响,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

com.hankcs.hanlp.summary.TextRankKeyword#max_iter = 200 是 protect 的，我发现我在处理大量文本时候，出现性能问题，一次调用需要消耗 200-300ms ，可以把这个变量改成 public 的，方便我在包外修改

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
请问一下，您这里的自动摘要模块支持英文吗,请问一下，您这里的自动摘要模块支持英文吗
发现CustomDictionary字典，只有最后add()或insert()的词条，才会在分词中生效？！,"# HanLP版本号
环境1：字典 v1.3.3（full）+jar包 v1.3.4
环境2：字典 v1.2.8（mini）+jar包 v1.2.8
# 我的问题
实验发现，不论是修改.txt并刷新.bin字典文件，还是途经CustomDictionary.add()|insert()方法，都是只有最后附加上去的自定义词条，在分词时才生效？！
# 复现问题
原文：北京海淀区，西八里庄，裕友大厦3层；
字典：
裕友大厦 ns 100
西八里庄 ns 100
分词结果：北京/ns 海淀/ns 区/n ，/w 西八里庄/ns ，**/w 裕/vg 友/ng 大厦/n** 3层/mq ；/w
字典：（仅调换自定义词条的次序）
西八里庄 ns 100
裕友大厦 ns 100
分词结果：北京/ns 海淀/ns 区/n ，/w **西/f 八里庄/ns** ，/w 裕友大厦/ns 3层/mq ；/w
# 继续试验
附加2个无意义的词条（“西八里”，“裕友”），就好了！
字典：
**西八里** ns 100
西八里庄 ns 100
**裕友** ns 100
裕友大厦 ns 100
分词结果正常：
北京/ns 海淀区/ns ，/w 西八里庄/ns ，/w 裕友大厦/ns 3层/mq ；/w
# 临时解决方法：
将未能成功组合分词的词条，在自定义字典中，拷贝2次！
字典：
西八里庄 ns 1
**西八里庄** ns 1
裕友大厦 ns 1
**裕友大厦** ns 1
分词结果正常：
北京/ns 海淀区/ns ，/w **西八里庄/ns** ，/w **裕友大厦/ns** 3层/mq ；/w
# 请教
@hackers
1：有无正规的解决方法？
2：能否比较清晰地，将最常用的ViterbiSegment使用字典分词时的优先级理解，明确声明一下？
3：什么样的自定义分词可以通过修改自定义词条解决，什么样的自定义分词则必须修改其它（专属）字典？"
"单例模式,使用一段时间(1~2天左右)就存在内存溢出,内存配置了2G","请问下  我这样使用有没有什么问题
我截取部分代码

@Component
@Service(""aiServiceHit"")
public class AiServiceByHit implements IAIService {
        private Segment segment = new NShortSegment().enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).enablePartOfSpeechTagging(true);

	@Override
	public boolean answerService(RequestDto requestDto) {
                   这里部分代码这样使用 
                  
                        List<Term> seg = segment.seg(text);
			LOG.info(""分词算法:"" + JSONArray.toJSONString(seg));
			List<String> getNsV = new ArrayList<String>();//名词集合
			for(Term t : seg){
				switch (t.nature) {
				case n://名词
				case nx:
				case ns:
				case nz:
				case nf:
				case nt:
				case m:
				case mq:
				case Mg:
					getNsV.add(t.word.toLowerCase());
					break;
				default:
					break;
				}
			}
                    下面就是部分业务
        }
}


现在的异常提醒：
J 7395 C1 com.hankcs.hanlp.recognition.nr.TranslatedPersonRecognition.Recognition(Ljava/util/List;Lcom/hankcs/hanlp/seg/common/WordNet;Lcom/hankcs/hanlp/seg/common/WordNet;)V (238 bytes) @ 0x00007f3c6dc7fe44 [0x00007f3c6dc7f720+0x724]
J 7488 C1 com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence([C)Ljava/util/List; (420 bytes) @ 0x00007f3c6d46a9b4 [0x00007f3c6d469960+0x1054]
J 7618 C1 com.hankcs.hanlp.seg.Segment.seg(Ljava/lang/String;)Ljava/util/List; (458 bytes) @ 0x00007f3c6d42d12c [0x00007f3c6d42a9c0+0x276c]



是不是我的使用方法有误呢？"
Fix bug on println(CoNLLWord),"这次修改没有引入第三方类库
没有修改JDK版本号
所有文本都是UTF-8编码
代码风格一致
[x ] 我在此括号内输入x打钩，代表上述事项确认完毕

当分析依存关系时，需要考虑每个CoNLLWord的依存弧指向的CoNLLWord。当该CoNLLWord本身为root时，输出该片段的HEAD会
报错。

报错代码段如下：
CoNLLSentence sentence = HanLP.parseDependency(""坚决惩治贪污贿赂等经济犯罪"");
for (CoNLLWord word : sentence) {
System.out.println(word.HEAD);
}

分析原因为：
root的初始化代码为
public static final CoNLLWord ROOT = new CoNLLWord(0, ""##核心##"", ""ROOT"",
""root"");
其HEAD值为空，在toString方法中调用HEAD.ID时报错。

修改方法为：
在CoNLLWord的toString方法中判断是否为根节点(root)或空白节点(null)，若是则将HEAD.ID替换为下划线。"
JDK1.6 portable包下报错,"## 版本号

当前最新版本号是：hanlp-portable-1.3.4.jar
我使用的版本是：hanlp-portable-1.3.4.jar

## 我的问题

直接新建项目，加入引用hanlp-portable-1.3.4.jar，在jdk1.6环境下以下代码会报错
错误信息
```
2017-6-29 9:31:53 com.hankcs.hanlp.dictionary.TransformMatrixDictionary load
警告: 读取data/dictionary/CoreNatureDictionary.tr.txt失败java.lang.NullPointerException: Inflater has been closed
Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.algorithm.Viterbi.compute(Viterbi.java:121)
	at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:533)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:120)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)
	at Main.main(Main.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
```

### 触发代码

```
        Segment segment = new ViterbiSegment(){{
            enableIndexMode(false);
            enableOffset(true);
            enableNumberQuantifierRecognize(false);
            enableOrganizationRecognize(true);
            enableCustomDictionary(true);
            enablePlaceRecognize(true);
            enableNameRecognize(true);
            enableJapaneseNameRecognize(false);
            enableTranslatedNameRecognize(false);
            enablePartOfSpeechTagging(true);
        }};
        System.out.println(segment.seg(""看一下小王的日报""));
```
## 其他信息
1. 注释`enablePlaceRecognize(true);`可正常运行；
2. 使用portable源码调试无报错。


"
[Not a bug] HanLP API Docker Image,"各位

我封装了一个docker images，可以用来快速开始使用HanLP API，尤其是在跨语言和平台上，集成HanLP服务。

http://nlp.chatbot.io/public/index.html

快速开始: https://hub.docker.com/r/samurais/hanlp-api/"
关于Android的使用,"你好，很感谢你们能提供如此优秀的开源库，我想询问一下关于安卓如何使用这个库， 我用的IDE是android studio,例如HanLP.properties应该放哪儿，感谢解答"
根据主分支代码更新protable代码,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？
根据主版本的代码更新了protable分支的代码

## 相关issue



"
分词错误，问题：你好，请问你们有宝马car的价格吗？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.3.4
我使用的版本是：v1.2.9


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
分词问题：“你好，请问你们有宝马car的价格吗？”，分词结果：[你好/l, ，/w, 请问/v, `你/r, 们/k`, 有/v, 宝马/nz, car/nx, 的/uj, 价格/n, 吗/y, ？/w]，其中词 你们 被分拆开了，应该分为“你们”。

而直接分词：“你们有宝马car的价格吗？”，是没有问题的，只有“请问你们”，这种才会有问题。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改代码及模型。

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        List<Term> terms = HanLP.segment(""你好，请问你们有宝马car的价格吗？"");
        System.out.println(terms);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[你好/l, ，/w, 请问/v, 你们/r, 有/v, 宝马/nz, car/nx, 的/uj, 价格/n, 吗/y, ？/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[你好/l, ，/w, 请问/v, 你/r, 们/k, 有/v, 宝马/nz, car/nx, 的/uj, 价格/n, 吗/y, ？/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
"CoreDictionary中有一个""机收""的词，导致“手机收邮件”分词结果为“手 机收 邮件” ","<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.2


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在分词的时候发现对一个句子“手机收邮件的问题”进行分词，结果是“手 机收 邮件 的 问题”，即使将“手机”加到CustomDictionary中也还是这样子的结果。尝试了各个分词类：NotionalTokenizer,HanLP.segment(),HanLP.newSegment() 都出现这个问题
定位发现CoreDictionary中有一个""机收""的词，导致“手机收邮件”分词结果为“手 机收 邮件” 

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
static void testSeg(){
   Segment segment = HanLP.newSegment().enableCustomDictionary(true);
    String str = ""手机收邮件的问题"";
    List<Term> res = segment .seg(str);
    StringBuilder sb = new StringBuilder();
    for(Term term:res ){
      sb.append(term.word).append(""\t"");
    }
    System.out.println(sb.toString());
  }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
手机 收 邮件 的 问题
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
手 机收 邮件 的 问题
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
关于CharacterBasedGenerativeModel 原始语料无法找到的问题,"基于2阶HMM 分词器 HMMSegment中使用到的模型，但是无法找到与之相对应的原始语料。还望大神能够提供与之相对应的原始语料方便我们学习和进行debug
"
HanLP 汉字转换拼音时，多音字问题，“还款” 结果为：“hai kuan”，求更正。,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
HanLP 汉字转换拼音时，多音字问题，“还款” 结果为：“hai kuan”
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
 public static String convertToPinyinString(String text, String separator, boolean remainNone) {
        List pinyinList = PinyinDictionary.convertToPinyin(text, true);
        int length = pinyinList.size();
        StringBuilder sb = new StringBuilder(length * (5 + separator.length()));
        int i = 1;

        for(Iterator var7 = pinyinList.iterator(); var7.hasNext(); ++i) {
            Pinyin pinyin = (Pinyin)var7.next();
            if(pinyin == Pinyin.none5 && !remainNone) {
                sb.append(text.charAt(i - 1));
            } else {
                sb.append(pinyin.getPinyinWithoutTone());
            }

            if(i < length) {
                sb.append(separator);
            }
        }

        return sb.toString();
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
huan kuan

### 实际输出
hai kuan
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
hanlp支持从用户语料继续训练嘛？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

我从文档上看到hanlp可以帮助用户训练自己的语料。那么假设我有自己的分词语料，我需要怎么去训练呢？
"
hanlp 能否加入一个远程词典更新的功能,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

我再使用 [elasticsearch-ik](https://github.com/medcl/elasticsearch-analysis-ik)的时候，发现这个远程更新的功能非常不错，也很简单，就是通过远程的一个 txt 文件，判断 ETag 是否更新，然后重新 reload 词典，我自己在使用的项目里面已经参考 ik 的机制，写了一个自动更新的功能。

BTW: CustomDictionary 里面能不能加入一个 reload 的功能，我用远程更新的时候需要重新加载这个词典。我本地测试了一个直接把 CustomDictionary.BinTrie = null，运行过了一段时间，好像没有对象过多的内存泄露，但是没有详细测试过。

"
识别人名出错,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4
## 我的问题
 使用NLPTokenizer.segment（）方法调用 发现使用""刘飞在打电话"" 把其中的“刘飞在” 定性为nr

### 触发代码

```
public class DemoNLPSegment
{
    public static void main(String[] args)
    {
        HanLP.Config.enableDebug();
        Segment segment = HanLP.newSegment().enableOrganizationRecognize(false);
        List<Term> termList = NLPTokenizer.segment(""刘飞在打电话"");
        System.out.println(termList);
    }
}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
“刘飞/nr” ""在/p"" ""打电话/vi""
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[刘飞在/nr, 打电话/vi]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
神经网络句法分析的时候加载有问题，CoNLLWord类 toString 方法打印两次名称,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

采用NeuralNetworkDependencyParser.compute的进行句法分析的时候，默认会调用 jar 包的路径，会报错。需要先将 HanLP.Config.IOAdapter = null; 这样会从文件读取。而且CoNLLWord类的 toString 方法把词写了两次。

## 复现问题
### 步骤

### 触发代码

```
     CoNLLSentence sentence = NeuralNetworkDependencyParser.compute(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"");
        System.out.println(sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
1	徐先生	nh	nr	_	4	主谓关系	_	_
```

### 实际输出

```
1	徐先生	徐先生	nh	nr	_	4	主谓关系	_	_
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
consider empty emit for more stable result,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

修正確認關鍵字時及早退出的條件

## 相关issue

<!-- None -->


"
Early abandoning ac trie,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

確認文本中是否包含建立trie時使用的模式，一旦找到就可及早退出。
適用於只需確認任一模式存在，而不需要統計頻率的情境。

## 相关issue

<!-- None -->


"
关于solr中添加hanlp的分词插件中，配置hanlp.properties的root相对路径,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable.jar1.3.4和hanlp-solr-plugin.jar1.1.2
我使用的版本是：hanlp-portable.jar1.3.4和hanlp-solr-plugin.jar1.1.2


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我是想将hanlp的分词插件加载到solr中来，先已经把hanlp的配置文件hanlp.properties放进来了，data包放到solr项目工程的webContent下了，如果root写的绝对路径是可以正常加载的，如果写成项目的相对路径，就无法找到路径，想请教一下，root可以配置成相对路径吗？该如何配置？，还是说data包不能放到solr项目工程下的webContent下？我看了一下里面之前别人的提问 ，我看到说配置root路径里，是../的方式配置的，请大神解答，谢谢！
另，在solr中的schema文件里配置自定义词库时应该也可以写项目的相对路径吗？写法是不是和root配置是一致的？


## 触发代码
```
 直接启动solr项目
```


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先将hanlp.properties文件放到了solr项目工程的src路径下，通过编译可生成classpath下
2. 然后在WebContent下新建了一个名为hanlp的文件夹
3. 接着将data包放到solr项目工程的WebContent中的hanlp下，
4. 最后修改hanlp.properties里的root路径，我想修改成相对路径，root=../项目名称/hanlp/

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望可以通过相对路径找到data包
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出，无法载入data包
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
自定义词语无法被正确分出,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

自定义词语分词错误

## 复现问题


### 步骤

对“2016年6月大学英语六级考试真题”进行分词

### 触发代码

```
    public void testIssue1234() throws Exception
    {
		String[] testCase = new String[]{
                ""2016年6月大学英语六级考试真题"",
        };
		
		CustomDictionary.insert(""英语六级"", ""nz 99999"");
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            for(Term t : termList){
            	System.out.println(t.word + "":"" + t.getFrequency());
            }
        }
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

包含
……
英语六级：99999
……
的输出

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

2016:0
年:14340
6:0
月:7097
大:27275
学英语:16
六:0
级:3240
考试:3281
真题:12

## 其他信息
当CustomDictionary.insert(""大学英语六级"");时
却可以正确分出
……
大学英语六级：1
……
的结果


"
关于hanlp.properties 的root路径的相对路径配置问题,"我是想将hanlp的分词插件加载到solr中来，先已经把hanlp的配置文件hanlp.properties放进来了，data包放到solr项目工程的webContent下了，如果root写的绝对路径是可以正常加载的，如果写成项目的相对路径，就无法找到路径，想请教一下，root可以配置成相对路径吗？该如何配置？，还是说data包不能放到solr项目工程下的webContent下？我看了一下里面之前别人的提问 ，我看到说配置root路径里，是../的方式配置的，请大神解答，谢谢！
另，在solr中的schema文件里配置自定义词库时应该也可以写项目的相对路径吗？写法是不是和root配置是一致的？"
HanLP.properties 还是hanlp.properties,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

基于maven项目，配置HanLP.properties， 通过IDE调试，发现能正确加载指定目录的字典库。但是打包成jar，就发现，每次都是加载相对目录，即hanlp自带jar里的字典库。不管HanLP.properties 的配置如何调整，都无效。改成全部小写：hanlp.properties  即可。
虽然操作系统上 HanLP.properties 和hanlp.properties 都是指同一个文件，但源码里指明了是：hanlp.properties 。


## 复现问题
HanLP.properties 改成hanlp.properties 后问题解决。
如果改成HanLP.properties ，则又会读取相对目录。



"
系统加载的日志能打印到日志文件么？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题
最近几天再用hanlp分词，日志打印时，要么采用debug模式，但这样会打印很多日志。能否将系统加载时，加载的哪些文件作为核心日志打出呢？因为我分词文件几百兆，其实首先是期望配置加载是否正确，并不需要每个词的分词细节。但目前来看，还行只有一个选择：HanLP.Config.enableDebug();
另外，采用slfj，log4j 的配置，即使设定了日志级别为debug还是无效。是否也能考虑下升级、？
![image](https://cloud.githubusercontent.com/assets/4981629/26762138/47a4f694-496f-11e7-9cf8-fac1c237f822.png)



"
自定义的词性无法识别,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4

## 我的问题
在零配置的情况下，自定义字典的词性识别不正确

### 步骤
### 触发代码

```
    public static void main(String[] args) {
        CustomDictionary.insert(""别克"",""brand 1"");
        CustomDictionary.insert(""英朗"",""family 1"");

        String text1=""别克英朗"";
        String text2 = ""别克 英朗"";
        StandardTokenizer.SEGMENT.enableAllNamedEntityRecognize(false);

        List<Term> terms1 = StandardTokenizer.segment(text1);
        System.out.println(terms1);

        List<Term> terms2 = StandardTokenizer.segment(text2);
        System.out.println(terms2);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[别克/brand,  英朗/family]
[别克/brand,  /w, 英朗/family]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[别克英朗/nrf]
[别克/brand,  /w, 英朗/family]
```

## 其他信息



"
字典缓存*.bin删除后无法再次产生,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->


我使用的版本是：HanLP-1.3.2


## 我的问题
windows 环境，每次分词启动后会生成bin 缓存。如果更新字典或删除缓存，下次启动后会再次生成。
但在linux 上，首次运行生成了bin，手动删掉后，下次就没有bin 文件了，这是什么情况？’

"
依存文法运行效率问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
用scala做NLP的典型意见，期待效果如下：
![qq 20170531101801](https://cloud.githubusercontent.com/assets/10076707/26613211/3dd00be4-45ec-11e7-98f3-151ff9f90789.png)
我的解决方案：依存文法 + 提取核心、主谓、定中 + 形成意见
但是HanLP的依存文法运行 时间很长，spark上依存 500 条 记录需要40s，不能满足要求
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改代码，调用：
``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```
只实现了HdfsIOAdapter：
``` scala
import java.io.{InputStream, OutputStream}
import com.hankcs.hanlp.corpus.io.IIOAdapter
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
/**
  * @author zzzhy
  *         2017-05-18
  */
object HdfsIOAdapter {
  private val conf = new Configuration()
}
class HdfsIOAdapter extends IIOAdapter {

  import HdfsIOAdapter._
  private val fs = FileSystem.newInstance(conf)

  override def create(path: String): OutputStream = {
    val fsDataOutputStream = fs.create(new Path(path))
    fsDataOutputStream.getWrappedStream
  }

  override def open(path: String): InputStream = {
    val fsDataInputStream = fs.open(new Path(path.replace(""\\"", ""/"")))
    fsDataInputStream.getWrappedStream
  }
}
```

### 触发代码

``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```

有何使用不当或者有何改进方式？
"
你训练人名识别是用了一年的人民日报的语料吗？2014年的吗？总共有多少篇文章？,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
分词能否实现输出多个分词结果,"比如： 『购物就上天猫』  
这句话会用各种分词方法都是这样的结果：
[购物/vn, 就/d, 上天/vi, 猫/n]

天猫这个在自定义词典中，切词频设置很高也没有用。  
Hanlp能否实现把多种分词可能都输出， 比如上诉情况，返回两种结果：  
[[购物/vn, 就/d, 上天/vi, 猫/n],[购物/vn, 就/d, 上/v天猫/ntc]]"
TextRankKeyword 提取窗口相近词的强化,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
性能强化
## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->
[TextRankKeyword 提取窗口相近词的强化](https://github.com/hankcs/HanLP/issues/546)

"
Update CharTable.txt,"* [x] 这次修改没有引入第三方类库。
* [x] 也没有修改JDK版本号
* [x] 所有文本都是UTF-8编码
* [x] 代码风格一致

## 解决了什么问题？带来了什么好处？

添加了公式以及化学式上下标与数字的对应关系，使的上下标可以被正确识别

## 相关issue
[#543](https://github.com/hankcs/HanLP/issues/543)



"
TextRankKeyword 提取窗口相近词的强化,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.0，和1.3.4源码一样


## 我的问题

小小的功能强化，
源码：提取临近窗口词的复杂度为o(n^2)，n为窗口大小
建议：提取临近窗口词的复杂度为o(n)，n为窗口大小

想法：由于是文章，假设词的总数为C，所以时间又会被线性放大C倍，建议还是可以处理一下
地址：com.hankcs.hanlp.summary.TextRankKeyword.getRank(List<Term> termList) 方法
源码：
<pre><code>            que.offer(w);
            if (que.size() > 5) {
                que.poll();
            }

            for (String w1 : que)
            {
                for (String w2 : que)
                {
                    if (w1.equals(w2))
                    {
                        continue;
                    }

                    words.get(w1).add(w2);
                    words.get(w2).add(w1);
                }
            }
</code></pre>
建议：
<pre><code>     
            // 复杂度O(n-1)
            if (que.size() >= 5) {
                que.poll();
            }            
            for (String qWord : que) {
                if (w.equals(qWord)) {
                    continue;
                }
                //既然是邻居,那么关系是相互的,遍历一遍即可
                words.get(w).add(qWord);
                words.get(qWord).add(w);
            }
            que.offer(w);
</code></pre>
## 测试结果：
* 测试一：
  - 句子：
程序员是从事程序开发、维护的专业人员。一般将程序员分为程序设计人员和程序编码人员，但两者的界限并不非常清楚，特别是在中国。软件从业人员分为初级程序员、高级程序员、系统分析员和项目经理四大类。
  - 源码：
{专业=[人员, 从事, 分为, 开发, 程序, 程序员, 维护], 中国=[人员, 分为, 清楚, 特别, 界限, 程序员, 软件, 非常], 人员=[专业, 中国, 分为, 开发, 清楚, 特别, 界限, 程序, 程序员, 维护, 编码, 设计, 软件, 非常, 高级], 从事=[专业, 开发, 程序, 程序员, 维护], 分为=[专业, 中国, 人员, 特别, 程序, 程序员, 系统, 维护, 设计, 软件, 高级], 分析员=[程序员, 系统, 经理, 项目, 高级], 开发=[专业, 人员, 从事, 程序, 程序员, 维护], 清楚=[中国, 人员, 特别, 界限, 编码, 软件, 非常], 特别=[中国, 人员, 分为, 清楚, 界限, 软件, 非常], 界限=[中国, 人员, 清楚, 特别, 程序, 编码, 非常], 程序=[专业, 人员, 从事, 分为, 开发, 界限, 程序员, 维护, 编码, 设计, 非常], 程序员=[专业, 中国, 人员, 从事, 分为, 分析员, 开发, 程序, 系统, 经理, 维护, 设计, 软件, 项目, 高级], 系统=[分为, 分析员, 程序员, 经理, 项目, 高级], 经理=[分析员, 程序员, 系统, 项目], 维护=[专业, 人员, 从事, 分为, 开发, 程序, 程序员], 编码=[人员, 清楚, 界限, 程序, 设计, 非常], 设计=[人员, 分为, 程序, 程序员, 编码], 软件=[中国, 人员, 分为, 清楚, 特别, 程序员, 非常, 高级], 非常=[中国, 人员, 清楚, 特别, 界限, 程序, 编码, 软件], 项目=[分析员, 程序员, 系统, 经理, 高级], 高级=[人员, 分为, 分析员, 程序员, 系统, 软件, 项目]}
  - 建议：
{专业=[人员, 从事, 分为, 开发, 程序, 程序员, 维护], 中国=[人员, 分为, 清楚, 特别, 界限, 程序员, 软件, 非常], 人员=[专业, 中国, 分为, 开发, 清楚, 特别, 界限, 程序, 程序员, 维护, 编码, 设计, 软件, 非常, 高级], 从事=[专业, 开发, 程序, 程序员, 维护], 分为=[专业, 中国, 人员, 特别, 程序, 程序员, 系统, 维护, 设计, 软件, 高级], 分析员=[程序员, 系统, 经理, 项目, 高级], 开发=[专业, 人员, 从事, 程序, 程序员, 维护], 清楚=[中国, 人员, 特别, 界限, 编码, 软件, 非常], 特别=[中国, 人员, 分为, 清楚, 界限, 软件, 非常], 界限=[中国, 人员, 清楚, 特别, 程序, 编码, 非常], 程序=[专业, 人员, 从事, 分为, 开发, 界限, 程序员, 维护, 编码, 设计, 非常], 程序员=[专业, 中国, 人员, 从事, 分为, 分析员, 开发, 程序, 系统, 经理, 维护, 设计, 软件, 项目, 高级], 系统=[分为, 分析员, 程序员, 经理, 项目, 高级], 经理=[分析员, 程序员, 系统, 项目], 维护=[专业, 人员, 从事, 分为, 开发, 程序, 程序员], 编码=[人员, 清楚, 界限, 程序, 设计, 非常], 设计=[人员, 分为, 程序, 程序员, 编码], 软件=[中国, 人员, 分为, 清楚, 特别, 程序员, 非常, 高级], 非常=[中国, 人员, 清楚, 特别, 界限, 程序, 编码, 软件], 项目=[分析员, 程序员, 系统, 经理, 高级], 高级=[人员, 分为, 分析员, 程序员, 系统, 软件, 项目]}

* 测试二：
  - 句子：
算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。
算法可以宽泛的分为三类，
一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。
二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。
三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。

  - 源码：
{一定=[任务, 得出, 执行, 指定, 时间, 算法, 终止], 产生=[定义, 未能, 确定, 算法, 终止, 运行, 通常], 任务=[一定, 可能, 执行, 指定, 时间, 算法, 终止], 几何=[动态, 数论, 算法, 规划, 计算], 分为=[基本, 大致, 宽泛, 数据, 有限, 森林, 确定性, 算法, 结构], 分析=[加密, 动态, 排序, 数值, 算法, 规划], 加密=[分析, 动态, 排序, 数值, 检索, 算法, 规划], 动态=[几何, 分析, 加密, 数值, 算法, 规划, 计算], 取决于=[得出, 时间, 有限, 确定, 算法, 终止, 输入], 变形=[并行, 森林, 模型, 算法], 可能=[任务, 执行, 指定, 时间, 有限, 算法, 终止], 基本=[分为, 大致, 数据, 算法, 结构], 大致=[分为, 基本, 数据, 算法], 定义=[产生, 数据, 无法, 未能, 条件, 没有, 确定, 算法, 终止, 输入], 宽泛=[分为, 有限, 森林, 模型, 确定性, 算法], 并行=[变形, 检索, 森林, 模型, 算法], 得出=[一定, 取决于, 时间, 有限, 确定, 算法, 终止, 输入], 执行=[一定, 任务, 可能, 指定, 时间, 终止], 指定=[一定, 任务, 可能, 执行, 时间, 终止], 排序=[分析, 加密, 数值, 检索, 算法], 数值=[分析, 加密, 动态, 排序, 时间, 有限, 没有, 确定, 算法, 终止, 给定, 规划], 数据=[分为, 基本, 大致, 定义, 数论, 无法, 条件, 满足, 算法, 终止, 结构, 输入, 运行], 数论=[几何, 数据, 算法, 结构, 计算], 无法=[定义, 数据, 条件, 满足, 终止, 输入], 时间=[一定, 任务, 取决于, 可能, 得出, 执行, 指定, 数值, 有限, 确定, 确定性, 算法, 终止, 给定], 有限=[分为, 取决于, 可能, 宽泛, 得出, 数值, 时间, 确定, 确定性, 算法, 终止, 给定, 输入], 未能=[产生, 定义, 条件, 确定, 算法, 终止, 通常], 条件=[定义, 数据, 无法, 未能, 没有, 满足, 确定, 终止, 输入], 检索=[加密, 并行, 排序, 算法], 森林=[分为, 变形, 宽泛, 并行, 模型, 算法], 模型=[变形, 宽泛, 并行, 森林, 算法], 没有=[定义, 数值, 条件, 确定, 算法, 终止], 满足=[数据, 无法, 条件, 算法, 终止, 输入, 运行, 通常], 确定=[产生, 取决于, 定义, 得出, 数值, 时间, 有限, 未能, 条件, 没有, 算法, 终止, 给定, 输入, 通常], 确定性=[分为, 宽泛, 时间, 有限, 算法], 算法=[一定, 产生, 任务, 几何, 分为, 分析, 加密, 动态, 取决于, 变形, 可能, 基本, 大致, 定义, 宽泛, 并行, 得出, 排序, 数值, 数据, 数论, 时间, 有限, 未能, 检索, 森林, 模型, 没有, 满足, 确定, 确定性, 终止, 结构, 给定, 规划, 计算, 输入, 运行, 通常], 终止=[一定, 产生, 任务, 取决于, 可能, 定义, 得出, 执行, 指定, 数值, 数据, 无法, 时间, 有限, 未能, 条件, 没有, 满足, 确定, 算法, 给定, 输入, 运行, 通常], 结构=[分为, 基本, 数据, 数论, 算法, 计算], 给定=[数值, 时间, 有限, 确定, 算法, 终止], 规划=[几何, 分析, 加密, 动态, 数值, 算法], 计算=[几何, 动态, 数论, 算法, 结构], 输入=[取决于, 定义, 得出, 数据, 无法, 有限, 条件, 满足, 确定, 算法, 终止, 运行], 运行=[产生, 数据, 满足, 算法, 终止, 输入, 通常], 通常=[产生, 未能, 满足, 确定, 算法, 终止, 运行]}
  - 建议：
{一定=[任务, 得出, 执行, 指定, 时间, 算法, 终止], 产生=[定义, 未能, 确定, 算法, 终止, 运行, 通常], 任务=[一定, 可能, 执行, 指定, 时间, 算法, 终止], 几何=[动态, 数论, 算法, 规划, 计算], 分为=[基本, 大致, 宽泛, 数据, 有限, 森林, 确定性, 算法, 结构], 分析=[加密, 动态, 排序, 数值, 算法, 规划], 加密=[分析, 动态, 排序, 数值, 检索, 算法, 规划], 动态=[几何, 分析, 加密, 数值, 算法, 规划, 计算], 取决于=[得出, 时间, 有限, 确定, 算法, 终止, 输入], 变形=[并行, 森林, 模型, 算法], 可能=[任务, 执行, 指定, 时间, 有限, 算法, 终止], 基本=[分为, 大致, 数据, 算法, 结构], 大致=[分为, 基本, 数据, 算法], 定义=[产生, 数据, 无法, 未能, 条件, 没有, 确定, 算法, 终止, 输入], 宽泛=[分为, 有限, 森林, 模型, 确定性, 算法], 并行=[变形, 检索, 森林, 模型, 算法], 得出=[一定, 取决于, 时间, 有限, 确定, 算法, 终止, 输入], 执行=[一定, 任务, 可能, 指定, 时间, 终止], 指定=[一定, 任务, 可能, 执行, 时间, 终止], 排序=[分析, 加密, 数值, 检索, 算法], 数值=[分析, 加密, 动态, 排序, 时间, 有限, 没有, 确定, 算法, 终止, 给定, 规划], 数据=[分为, 基本, 大致, 定义, 数论, 无法, 条件, 满足, 算法, 终止, 结构, 输入, 运行], 数论=[几何, 数据, 算法, 结构, 计算], 无法=[定义, 数据, 条件, 满足, 终止, 输入], 时间=[一定, 任务, 取决于, 可能, 得出, 执行, 指定, 数值, 有限, 确定, 确定性, 算法, 终止, 给定], 有限=[分为, 取决于, 可能, 宽泛, 得出, 数值, 时间, 确定, 确定性, 算法, 终止, 给定, 输入], 未能=[产生, 定义, 条件, 确定, 算法, 终止, 通常], 条件=[定义, 数据, 无法, 未能, 没有, 满足, 确定, 终止, 输入], 检索=[加密, 并行, 排序, 算法], 森林=[分为, 变形, 宽泛, 并行, 模型, 算法], 模型=[变形, 宽泛, 并行, 森林, 算法], 没有=[定义, 数值, 条件, 确定, 算法, 终止], 满足=[数据, 无法, 条件, 算法, 终止, 输入, 运行, 通常], 确定=[产生, 取决于, 定义, 得出, 数值, 时间, 有限, 未能, 条件, 没有, 算法, 终止, 给定, 输入, 通常], 确定性=[分为, 宽泛, 时间, 有限, 算法], 算法=[一定, 产生, 任务, 几何, 分为, 分析, 加密, 动态, 取决于, 变形, 可能, 基本, 大致, 定义, 宽泛, 并行, 得出, 排序, 数值, 数据, 数论, 时间, 有限, 未能, 检索, 森林, 模型, 没有, 满足, 确定, 确定性, 终止, 结构, 给定, 规划, 计算, 输入, 运行, 通常], 终止=[一定, 产生, 任务, 取决于, 可能, 定义, 得出, 执行, 指定, 数值, 数据, 无法, 时间, 有限, 未能, 条件, 没有, 满足, 确定, 算法, 给定, 输入, 运行, 通常], 结构=[分为, 基本, 数据, 数论, 算法, 计算], 给定=[数值, 时间, 有限, 确定, 算法, 终止], 规划=[几何, 分析, 加密, 动态, 数值, 算法], 计算=[几何, 动态, 数论, 算法, 结构], 输入=[取决于, 定义, 得出, 数据, 无法, 有限, 条件, 满足, 确定, 算法, 终止, 运行], 运行=[产生, 数据, 满足, 算法, 终止, 输入, 通常], 通常=[产生, 未能, 满足, 确定, 算法, 终止, 运行]}
[测试结果.txt](https://github.com/hankcs/HanLP/files/1028723/default.txt)


"
核心字典增加自定义词性,"版本1.3.2
增加了一个称谓词性 ncw

        modified:   src/main/java/com/hankcs/hanlp/corpus/tag/Nature.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/common/Node.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/common/POSUtil.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/nnparser/util/PosTagUtil.java

在上面的文件加入了ncw


在测试“外公”（外公 ncw 10)时出错

五月 25, 2017 11:16:40 上午 com.hankcs.hanlp.dictionary.CoreDictionaryTransformMatrixDictionary <clinit>
信息: 加载核心词典词性转移矩阵/C:/code/github/segmentation/src/main/resources/hanlp/data/dictionary/core/CoreNatureDictionary.tr.txt成功，耗时：60 ms
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 151
	at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
	at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
	at com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence(NShortSegment.java:133)
	at com.hankcs.hanlp.seg.Segment.segAndCheckNature(Segment.java:588)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:566)
	at com.twsz.creative.seg.service.HanlpOriginalTest.main(HanlpOriginalTest.java:173)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

看着是转移矩阵出了错，CoreNatureDictionary.tr.txt没有ncw的相关信息，[训练分词模型](https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B) 中说到通过训练输出的这个文件，但我没有语料，无法训练。有什么别的办法可以处理吗？（不启用自定义字典条件下）

"
字符表加载失败导致程序退出,"## 版本号 1.3.4

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

字符表加载失败，导致主程序整个程序退出。

### 触发代码

com.hankcs.hanlp.dictionary.other.CharType
```
           try
            {
                byteArray = generate();
            }
            catch (IOException e)
            {
                e.printStackTrace();
                logger.severe(""字符类型对应表 "" + HanLP.Config.CharTypePath + "" 加载失败： "" + TextUtility.exceptionToString(e));
                System.exit(-1);
            }
```
### 期望结果

字符表初始化失败后抛出相应异常，从而用户可以做相应处理，而不应该导致整个程序退出。



"
关于字母数字组合词的建议,"1. 在数学表达式以及化学表达式里面有很多的上下标，这些符号在字符表里面没有定义，常见的对应关系如下：
```
º=0
¹=1
²=2
³=3
⁴=4
⁵=5
⁶=6
⁷=7
⁸=8
⁹=9
₀=0
₁=1
₂=2
₃=3
₄=4
₅=5
₆=6
₇=7
₈=8
₉=9
ⁿ=n
```
2. 对于字母数字的组合，目前系统是直接进行分割的，例如：化学表达式H2O，目前的分词结果为H/2/O；调试发现在生成词网的时候就已经被分开了，建议能够在生成词网的时候不分开，然后在索引模式的细切分模式中进行二次切分，效果会更好一些；例如qq2017的分词结果会变为：qq2017、qq、2017；H2O的结果为h20、h、2、o；当然可以在二次切分的时候判断一下子词的长度，如果小于等于1则进行忽略；"
ArrayIndexOutOfBoundsException异常,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：portable-1.3.4

## 我的问题
同时打开命名实体识别和数量词识别，在切分“一分钟就累了”时会导致异常：
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 8
        at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
        at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
同时打开命名实体识别和数量词识别，会导致异常。如下面代码：
        Segment seg = HanLP.newSegment();
        seg.enableAllNamedEntityRecognize(true);
        seg.enableNumberQuantifierRecognize(true);
        List<Term> list = seg.seg(""一分钟就累了"");
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
“糊”繁体转换后的结果为“煳“，bug,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
""糊""在进行简体转繁体结果是“糊”，从繁体转简体时结果是“煳”
互逆的操作，结果不同。应属于bug

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我没有修改任何代码和词典

### 步骤

1. 首先……
System.out.println(HanLP.convertToTraditionalChinese(""糊涂果""));
2. 然后……
System.out.println(HanLP.convertToSimplifiedChinese(""糊塗果""));
3. 接着……
出结果
### 触发代码
System.out.println(HanLP.convertToTraditionalChinese(""糊涂果""));
		System.out.println(HanLP.convertToSimplifiedChinese(""糊塗果""));
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
糊塗果
糊涂果

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
糊塗果
煳涂果
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
remove自定义添加词的时候遇到一些问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

在自定义添加用户字典之后，执行删除操作，发现有些词删不掉，比如“力帆”，“摩根”。
我做了几次测试结果是：
1.单独remove “摩根”  --成功。
2.remove “摩根”，“123（123表示任意一个字符，即我添加了2个词，现在要remove这两个词） 此时失败。
3.remove “摩根”，“力帆” 此时又是成功。

本人是新手菜鸟，感觉是不是某些词直接的关联关系导致的？不知道此现象是为何原因，又是如何去解决，望大神指点。

附上部分代码：
CustomDictionary.add(""123"");
		CustomDictionary.add(""摩根"");
		CustomDictionary.remove(""123"");
		CustomDictionary.remove(""摩根"");
此时最后一行 remove摩根报错。
		CustomDictionary.add(""力帆"");
		CustomDictionary.add(""摩根"");
		CustomDictionary.remove(""力帆"");
		CustomDictionary.remove(""摩根"");

此时程序执行正常。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP.properties如何引用Java系统参数,"问题产生的情景是：我有一个使用到hanlp的webapp项目工程，需要部署到windows和linux等不同的系统环境中，但是HanLP配置文件中的路径每次都需要手动修改，我想可不可以使用系统参数来代替呢
例如：修改之前HanLP.properties中的配置是这样的
```
    root=D:/dic/
```

修改之后，期望可以引用jvm参数（-Dhanlp.dic=D:/dic/），
```
root=${hanlp.dic}
```
如果hanlp没有此功能，我应该怎么扩展呢，需要修改哪些代码？




"
个别句子句法依存的核心词有误,"版本1.3.2
对于句法依存的结果，有种无从修改的感觉，是不是只能重新训练一个模型呢？

**""元旦下午8点03分提醒我去后海和老朋友聚会"",**
核心词为“聚会”，应该为“提醒”，聚会本来是动词的，我修改成了名词。

1	元旦	nt	t	_	2	定中关系	_	_
2	下午	nt	t	_	3	定中关系	_	_
3	8点	nt	t	_	4	定中关系	_	_
4	03分	nt	t	_	5	状中结构	_	_
5	提醒	v	v	_	8	定中关系	_	_
6	我	r	rr	_	5	兼语	_	_
7	去	v	vf	_	5	动宾关系	_	_
8	后	nd	f	_	11	定中关系	_	_
9	海和老	nh	nr	_	10	定中关系	_	_
10	朋友	n	n	_	11	定中关系	_	_
11	聚会	n	n	_	0	核心关系	_	_

元旦 --(定中关系)--> 下午
下午 --(定中关系)--> 8点
8点 --(定中关系)--> 03分
03分 --(状中结构)--> 提醒
提醒 --(定中关系)--> 后
我 --(兼语)--> 提醒
去 --(动宾关系)--> 提醒
后 --(定中关系)--> 聚会
海和老 --(定中关系)--> 朋友
朋友 --(定中关系)--> 聚会
聚会 --(核心关系)--> ##核心##

**“每星期日中午一点半提醒我出去玩的时间到了""**
核心词为“到”，应该为“提醒”

1	每星期日	nt	t	_	2	定中关系	_	_
2	中午	nt	t	_	4	定中关系	_	_
3	一点	nt	t	_	4	定中关系	_	_
4	半	m	mq	_	5	状中结构	_	_
5	提醒	v	v	_	10	定中关系	_	_
6	我	r	rr	_	5	兼语	_	_
7	出去	v	vf	_	5	动宾关系	_	_
8	玩	v	v	_	7	并列关系	_	_
9	的	u	ude1	_	7	右附加关系	_	_
10	时间	n	n	_	11	主谓关系	_	_
11	到	v	v	_	0	核心关系	_	_
12	了	u	ule	_	11	右附加关系	_	_

每星期日 --(定中关系)--> 中午
中午 --(定中关系)--> 半
一点 --(定中关系)--> 半
半 --(状中结构)--> 提醒
提醒 --(定中关系)--> 时间
我 --(兼语)--> 提醒
出去 --(动宾关系)--> 提醒
玩 --(并列关系)--> 出去
的 --(右附加关系)--> 出去
时间 --(主谓关系)--> 到
到 --(核心关系)--> ##核心##
了 --(右附加关系)--> 到"
生成CoreNatureDictionary.txt.bin的模块是哪个？,"   生成CoreNatureDictionary.txt.bin的模块是哪个？我用NatureDictionaryMaker来制作了自己的词典。但是用自己做的这个词典去进行分词是系统没有生成*.txt.bin 报错无法进行分词。
   我在做的不是中文，是小数民族文字。所以我想要是生成*.txt.bin 文件的模块有编码限制，对他进行修改。  
我在用的版本是1.2.8"
关于IIOAdapter的疑问,"IIOAdapter中有两个接口需要实现：open与create；
现在我想将词库放在HDFS上，读取很好实现，只是现在不大清楚create方法的具体作用；
现在需要明确的是：
1. create是否是生成bin文件使用的；
2. 在Linux上貌似是不生成bin文件的，那么是不是可以让create方法只接返回Null；
3. 如果在词库初始化的时候需要生成bin或者其他文件，那么在使用HDFS词库的情况下，多节点同时初始化的时候是不是会造成写入冲突问题




"
Merge pull request #1 from hankcs/master,"136da834f0a1a92987d733e67d0560c4a80a144e

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
Merge pull request #1 from hankcs/master,"update

<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->

## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->


"
"""烤羊排""进行分词为""烤""和""羊排""","问题
对词语""烤羊排""进行分词，
得到结果: 烤羊排 [nf]， 期望能把 烤 和 羊排 区分开

版本
程序版本号：hanlp-portable-1.2.7
数据版本号：data-for-1.3.2.zip

触发代码
`HanLP.segment(""烤羊排"");`

代码输出
`烤羊排 [nf]`

尝试
CoreNatureDictionary.txt中有 烤、羊排、烤羊排三个词语，我在词库CoreNatureDictionary.ngram.txt中增加 烤@羊排 2000，但是还是没有效果，麻烦帮忙看一下，非常感谢。"
整合LDA 主谓宾提取,"整合LDA 主谓宾提取 方便与其他分词一起使用
"
关于语义距离计算的问题,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.3.3-release
我使用的版本是：hanlp-1.3.3-release


## 我的问题
在调用[语义距离](https://github.com/hankcs/HanLP#20-语义距离)和语义相似度方法时，出现语义相似度为0。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先，我通过[关键词提取](https://github.com/hankcs/HanLP#14-关键词提取)获取了某个句子的关键词。例如，[程序员, 程序, 分为, 人员, 软件]。
2. 然后，我对获得的关键词进行语义距离和语义相似度的计算。计算结果中，与“程序员”一词相关的记录，语义距离为无穷，语义相似度为0。

新手，刚接触分词，以下是自己做出的一些尝试与猜想，如有描述不当，还请批评指正。
1.猜想需要添加“程序员”到自定义词典，尝试了动态增加词语“程序员”，结果无变化。
2.猜想在《同义词词林扩展版》中没有与“程序员”相关的记录，故无法计算语义距离和语义相似度。

请问，具体是哪一种问题导致一个词语对自己本身的语义相似度为0？对于相应的问题，我可以怎么解决，或参考什么样例去解决？

### 触发代码

```
String content = ""程序员(英文Programmer)是从事程序开发、维护的专业人员。一般将程序员分为程序设计人员和程序编码人员，但两者的界限并不非常清楚，特别是在中国。软件从业人员分为初级程序员、高级程序员、系统分析员和项目经理四大类。"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);
System.out.printf(""%-5s\t\t%-5s\t\t%-10s\t\t%-5s\n"", ""词A"", ""词B"", ""语义距离"", ""语义相似度"");
for (String a : keywordList)
{
	
    for (String b : keywordList)
    {
        System.out.printf(""%-5s\t\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
    }
}
```
### 期望输出
(输出结果末尾加""*""为方便提示关键行)
<!-- 你希望输出什么样的正确结果？-->

```
词A   		词B   	语义距离      	语义相似度
程序员  		程序员  	0              	1.0000000000*
程序员  		程序   	(非无穷)		(非0)*
程序员  		分为   	(非无穷)		(非0)*
程序员  		人员   	(非无穷)		(非0)*
程序员  		软件   	(非无穷)		(非0)*
程序   		程序员  	(非无穷)		(非0)*
程序   		程序   	0              	1.0000000000
程序   		分为   	28631246094    	0.6146479284
程序   		人员   	21054522402    	0.7166241456
程序   		软件   	14295982706    	0.8075883064
分为   		程序员  	(非无穷)		(非0)*
分为   		程序   	28631246094    	0.6146479284
分为   		分为   	0              	1.0000000000
分为   		人员   	49685768496    	0.3312720740
分为   		软件   	42927228800    	0.4222362347
人员   		程序员  	(非无穷)		(非0)*
人员   		程序   	21054522402    	0.7166241456
人员   		分为   	49685768496    	0.3312720740
人员   		人员   	0              	1.0000000000
人员   		软件   	6758539696     	0.9090358392
软件   		程序员  	(非无穷)		(非0)*
软件   		程序   	14295982706    	0.8075883064
软件   		分为   	42927228800    	0.4222362347
软件   		人员   	6758539696     	0.9090358392
软件   		软件   	0              	1.0000000000
```

### 实际输出
(输出结果末尾加""*""为方便提示关键行)
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
词A   		词B   		语义距离      	语义相似度
程序员  		程序员  	9223372036854775807	0.0000000000*
程序员  		程序   	9223372036854775807	0.0000000000*
程序员  		分为   	9223372036854775807	0.0000000000*
程序员  		人员   	9223372036854775807	0.0000000000*
程序员  		软件   	9223372036854775807	0.0000000000*
程序   		程序员  	9223372036854775807	0.0000000000*
程序   		程序   	0              		1.0000000000
程序   		分为   	28631246094    		0.6146479284
程序   		人员   	21054522402    		0.7166241456
程序   		软件   	14295982706    		0.8075883064
分为   		程序员  	9223372036854775807	0.0000000000*
分为   		程序   	28631246094    		0.6146479284
分为   		分为   	0              		1.0000000000
分为   		人员   	49685768496    		0.3312720740
分为   		软件   	42927228800    		0.4222362347
人员   		程序员  	9223372036854775807	0.0000000000*
人员   		程序   	21054522402    		0.7166241456
人员   		分为   	49685768496    		0.3312720740
人员   		人员   	0              		1.0000000000
人员   		软件   	6758539696     		0.9090358392
软件   		程序员  	9223372036854775807	0.0000000000*
软件   		程序   	14295982706    		0.8075883064
软件   		分为   	42927228800    		0.4222362347
软件   		人员   	6758539696     		0.9090358392
软件   		软件   	0              		1.0000000000
```

## 其他信息
补充一下自己同时结合调用两种方法（[语义距离](https://github.com/hankcs/HanLP#20-语义距离)和[关键词提取](https://github.com/hankcs/HanLP#14-关键词提取)）的初衷：
我期望通过输入关键词，计算关键词与句子的关联度（以下简称 关联度），以达到“关键词-句子-关联度”的结果。
而在计算关联度时，我计划：
先提取句子关键词，如，[程序员, 程序, 分为, 人员, 软件]；
再输入关键词，如，“程序员”，“编程”等，计算输入的关键词与句子关键词之间的相似度；
最后利用关键词的相似度，从一定的程度反映关联度。

以上是我目前期望的需求，如果作者有遇到或解决过类似问题，还请多多指教。
谢谢！
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
stopword词典加载问题,"我现在用的是hanlp 1.3.0版本. 在分析CoreStopWordDictionary.java发现以下词典加载语句：
dictionary = new StopWordDictionary(new File(HanLP.Config.CoreStopWordDictionaryPath));

之前的核心词典，用户自定义词典等均采用以下方式。以核心词典为例：CoreDictionary.java
br = new BufferedReader(new InputStreamReader(IOUtil.newInputStream(path), ""UTF-8""));
是采用IOUtil的统一接口。
而StopWordDictionary直接使用了File来做，造成了不统一。是否考虑对CoreStopWordDictionary建立统一性？
因为我自己定义的JarIOAdapter.java：
public class JarIOAdapter implements IIOAdapter
{
    @Override
    public InputStream open(String path) throws FileNotFoundException
    {
        /*
        采用第一行的方式加载资料会在分布式环境报错
        改用第二行的方式
         */
        //return ClassLoader.getSystemClassLoader().getResourceAsStream(path);
        return JarIOAdapter.class.getClassLoader().getResourceAsStream(path);
    }

    @Override
    public OutputStream create(String path) throws FileNotFoundException
    {
        return new FileOutputStream(path);
    }
}
这里是实现代码与词典数据的分离，单独把hanlp.properties与data目录做成一个jar。但由于CoreStopDictionary.java读文件接口不统一，导致读不到停用词典文件。
作者是否有意把代码与词典数据分成两个jar包，我这边已差不多完成，可以提交代码"
需要怎么处理不去加载内部的角色组和列表,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.3
我使用的版本是：portable-1.3.2


## 我的问题
 trie = new AhoCorasickDoubleArrayTrie<String>();
        TreeMap<String, String> patternMap = new TreeMap<String, String>();
        addKeyword(patternMap, ""CCCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCD"");
        addKeyword(patternMap, ""CCCCCCGD"");
        addKeyword(patternMap, ""CCCCCCICCCCD"");
        addKeyword(patternMap, ""CCCCCCPD"");
        addKeyword(patternMap, ""CCCCCD"");
        addKeyword(patternMap, ""CCCCCDD"");
需要怎么处理才能避开记载你们内部的这个角色标注列表，我有自己的角色标注列表。
机构名角色观察：[  S 1169909 ][沈阳 G 97524 ][硕 P 7 ][润 C 116 ][机械电子 F 1000 ][设备 C 273 B 2 ][有限公司 K 1000 D 1000 ][  B 8425 ]
机构名角色标注：[ /S ,沈阳/G ,硕/P ,润/C ,机械电子/F ,设备/C ,有限公司/D , /S]
识别出机构名：机械电子设备有限公司 FCD
识别出机构名：润机械电子设备有限公司 CFCD
识别出机构名：设备有限公司 CD
###期望输出
机械电子设备有限公司 FCD
输出这个结果的原因是我的列表中不存在C开头的角色标注组合
### 实际输出
[沈阳/ns, 硕/ag, 润机械电子设备有限公司/nt]   注意第二个  润机械电子设备有限公司 CFCD

每次调用都会默认记载内部的角色标注列表
"
com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes 中  int availableBytes = is.available();,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.3
我使用的版本是：portable-1.3.2


## 我的问题

在resin下部署HanLP 加载词典文件时, 出现空指针异常

	... 53 more
Caused by: java.lang.NullPointerException
	at com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes(ByteArrayOtherStream.java:72)
	at com.hankcs.hanlp.corpus.io.ByteArrayStream.nextInt(ByteArrayStream.java:56)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.load(DoubleArrayTrie.java:531)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.load(DoubleArrayTrie.java:516)
	at com.hankcs.hanlp.dictionary.common.CommonDictionary.loadDat(CommonDictionary.java:90)
	at com.hankcs.hanlp.dictionary.common.CommonDictionary.load(CommonDictionary.java:44)
	at com.hankcs.hanlp.dictionary.nr.PersonDictionary.<clinit>(PersonDictionary.java:57)

## 复现问题

每次都能复现

### 步骤

在resin环境下,  调用如下代码, 会产生异常
        List<com.hankcs.hanlp.seg.common.Term> segment = HanLP.segment(episodeName);

### 触发代码

com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes(ByteArrayOtherStream.java:72)

经过调试, 发现是由于resin对InputStream的实现is.available()每次最多读取8192个字节, 而不是将所有的数据都读取.  程序第一次在读取8192字节, 没有读取完整数据的情况下, 将InputStream流错误关闭, 第二次读取时报上述错误
                if (readBytes == availableBytes)
                {
                    is.close();
                    is = null;
                }

下面是InputStream available 的文档
 /**
     * Returns an estimate of the number of bytes that can be read (or
     * skipped over) from this input stream without blocking by the next
     * invocation of a method for this input stream. The next invocation
     * might be the same thread or another thread.  A single read or skip of this
     * many bytes will not block, but may read or skip fewer bytes.
     *
    * 这里说明了可能不会返回所有字节*
     *** <p> Note that while some implementations of {@code InputStream} will return
     * the total number of bytes in the stream, many will not.  It is
     * never correct to use the return value of this method to allocate
     * a buffer intended to hold all data in this stream.**
     *
     * <p> A subclass' implementation of this method may choose to throw an
     * {@link IOException} if this input stream has been closed by
     * invoking the {@link #close()} method.
     *
     * <p> The {@code available} method for class {@code InputStream} always
     * returns {@code 0}.
     *
     * <p> This method should be overridden by subclasses.
     *
     * @return     an estimate of the number of bytes that can be read (or skipped
     *             over) from this input stream without blocking or {@code 0} when
     *             it reaches the end of the input stream.
     * @exception  IOException if an I/O error occurs.
     */

建议代码修改:
              if(is.available()==0)
              {
                    is.close();
                    is = null;
                }

### 期望输出


### 实际输出


## 其他信息


"
"为什么""good*tvb""分成[good, *, tvb]，但是""good#tvb""分成整个""good#tvb""? 如何实现分成[good, #, tvb]?","<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
发现TextUtility.isAllSingleByte的bug，不知道分词过程会调用吗," while (i < len && b[i] < 128)
            {
                i++;
            }

----------------

b[i] < 128 应该改成 b[i] >0.

byte的取值范围是-128到127 和char不一样

还有我觉得应该没必要转换成gbk，直接用charAt(i)是否小于128即可判断。



"
关于添加自定义词典分词与词性标注问题,"
当前最新版本号是：
我使用的版本是：HanLP1.3.2
## 我的问题

在词性标注中，我们使用了用户自定义词典，发现自定义词典中的大部分词语能按用户自定义记号进行标注，而存在少部分词语无法按自定义标注进行词性标注。

## 复现问题

如：用户添加自定义词典userdict.txt 存在如下几个词语：
性别  sex
男 man
女 woman
故意伤害罪 anyou

其中，“性别”、“男”、“女”这三个词可以按自定义词典的标注进行标注，结果为 ：性别/sex、男/man、女/ woman，而“ 故意伤害罪”这个词语词性标注为“nt”,结果为：故意伤害罪/nt。

### 期望输出

我们的期望输出是：
性别 / sex
男 /man
女 /woman
故意伤害罪/ anyou



"
"修正了核心字典的”每xx""词性","
"
v1.3.3升级指南,":loudspeaker: v1.3.3依然保证了所有接口的兼容性。但做了如下两个微小改动：

## com.hankcs.hanlp.algorithm 包名拼写错误改正
这个包下面实现了一些算法，并没有在文档中公开。如果你调用了该包下面的算法，可能需要手动修正一下包名。

## portable版不再检查文件是否存在
未实现IOAdapter的用户可以忽略这条信息。

在v1.3.2之前，portable会检查配置项中指定的路径是否存在。如果不存在则该配置项不生效，形成一个“防错设计”。但自从引入了IOAdapter机制后，路径可以指向任何地方（远程）。所以不再可能为用户检查路径是否存在。

去掉该检查逻辑后，目前portable版的com.hankcs.hanlp.HanLP.Config#KeyPath与非portable版功能一致，都是KeyPath=root+KeyValue 。

所以如果你实现了自己的IOAdapter，open和create接收的path参数将会以root开头，敬请注意。此时可供参考的措施有：

- 将配置文件的root设为```""""```
- 或稍微调整一下自己的IOAdapter



## 结语
这些改动的目的是为了使项目更加规范，造成不便还望海涵。有任何问题欢迎在此处留言，谢谢！"
在自定义词库中增加一个词后，在某些特殊字符串分词时报错ArrayIndexOutOfBoundsException,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
使用的Maven仓库：
    <dependency>
      <groupId>com.hankcs</groupId>
      <artifactId>hanlp</artifactId>
      <version>portable-1.3.2</version>
    </dependency>

JDK1.8.0_20
当前最新版本号是：portable-1.3.2
我使用的版本是：portable-1.3.2

## 我的问题
增加自定义词库之后，并开启人名识别，在某些文本的分词时会报错ArrayIndexOutOfBoundsException。

## 复现问题
自定义词典中增加一个词：
CustomDictionary.add(""第一季"", ""n 1"")

并不是所有的词都会触发这个Exception，后面的代码中是找到的一个。

### 步骤
执行下面的触发代码即可。

### 触发代码

```

import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CustomDictionary;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;


import java.util.List;

public class Main {
    public static void main(String[] args){
        CustomDictionary.add(""第一季"", ""n 1"");

        Segment seg = HanLP.newSegment().enableIndexMode(true)
                .enableNameRecognize(true)
                .enableNumberQuantifierRecognize(false)
                .enableCustomDictionary(true)
                .enableTranslatedNameRecognize(false)
                .enableJapaneseNameRecognize(false)
                .enableOrganizationRecognize(false)
                .enablePlaceRecognize(false);

        HanLP.Config.enableDebug();
        List<Term> termList = seg.seg(""乘龙怪婿第一季"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
    }
}
```


### 实际输出
```
May 05, 2017 5:55:11 PM com.hankcs.hanlp.dictionary.CoreDictionary load
INFO: 核心词典开始加载:data/dictionary/CoreNatureDictionary.mini.txt
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
INFO: data/dictionary/CoreNatureDictionary.mini.txt加载成功，85585个词条，耗时232ms
粗分词网：
0:[ ]
1:[乘]
2:[龙]
3:[怪]
4:[婿]
5:[第, 第一]
6:[]
7:[季]
8:[ ]

May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
INFO: 开始加载二元词典data/dictionary/CoreNatureDictionary.ngram.mini.txt.table
粗分结果[乘/v, 龙/n, 怪/a, 婿/ng, 第一季/n]
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
INFO: data/dictionary/CoreNatureDictionary.ngram.mini.txt.table加载成功，耗时136ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.common.CommonDictionary load
INFO: 加载值data/dictionary/person/nr.txt.value.dat成功，耗时64ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.common.CommonDictionary load
INFO: 加载键data/dictionary/person/nr.txt.trie.dat成功，耗时78ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
INFO: data/dictionary/person/nr.txt加载成功，耗时274ms
人名角色观察：[  A 22202445 ][乘 K 15 C 7 L 5 E 1 M 1 ][龙 D 1350 E 924 B 498 C 325 L 17 M 8 K 1 ][怪 D 10 E 1 K 1 L 1 ][婿 E 2 K 1 ][第一季 K 1 U 1 ][  A 22202445 ]
人名角色标注：[ /A ,乘/K ,龙/B ,怪/E ,婿/E ,第一季/U , /A]
识别出人名：龙怪 BE
识别出人名：怪婿 EE
识别出人名：龙怪婿 BEE
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 9
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:142)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.douban.solr.Main.main(Main.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

```

"
关于层叠HMM中文实体识别的过程,"hankcs你好！
我在用python做中文实体识别的分词器，想请教一个问题，你的层叠HMM中文实体识别中，不同的实体识别之间是如何传递的。我的策略是先进行人名识别，将结果传递，再进行地名识别，再将结果传递，最后进行机构名识别。但是当多个实体识别同时开启时，会出现一点问题。
比如：
`s = '南翔向宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机'`

当只开启地名识别时，可以正确分词：
`['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑牛沟村', '捐赠', '了', '挖掘机']`

但是当我同时开启人名识别和地名识别时，分词结果就不太好了。
人名识别结果：
 `['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑', '牛沟村', '捐赠', '了', '挖掘机']`

将结果作为传递给地名识别，结果不能正确分出`'黑牛沟村'`
地名识别结果：
 `['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑', '牛沟村', '捐赠', '了', '挖掘机']`

我用HanLP试了下，发现，当只开启人名识别时，和我错误的结果一致：
`[南翔/ns, 向/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑/a, 牛沟村/nr, 捐赠/v, 了/ule, 挖掘机/n]`

但是同时开启人名和地名识别，则可以得到正确的结果：
`[南翔/ns, 向/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]`

我想请教下，当同时开启了人名和地名识别时，你是如何处理的。




"
发现个bug,"太差劲了!花了几百块钱什么毛病都没看出来！各种套路你付钱给狗狗看病结果屁都看不出来！想给狗狗看病的我劝你们千万不要去那家医院！妈的坑的一批！祝帮我家狗狗看病的那个戴眼镜的胖子全家爆炸.md



对上面这段文本进行分词看看，以md为结尾的会报错"
索引分词缺少词,"hi hankcs, 又遇到个问题想向你请教。
## 版本号
master分支
当前最新版本号是：221d2a9e152763afe77b6ffc966b570ee308fb6e
我使用的版本是：221d2a9e152763afe77b6ffc966b570ee308fb6e

## 我的问题
“评审委员会”的索引分词中带有“评审委员会”这个词，但是“商标评审委员会”的索引分词就没有“评审委员会”这个词。作为全切分的索引模式，我觉得对于“商标评审委员会”的切分应该将“评审委员会”也切出来。否则搜索""评审委员会""将会搜不到。

## 复现问题

### 触发代码

```
        List<Term> termList = IndexTokenizer.segment(""评审委员会"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
评审委员会/nt [0:5]
评审/vn [0:2]
委员会/nis [2:5]
委员/nnt [2:4]

        List<Term> termList2 = IndexTokenizer.segment(""商标评审委员会"");
        for (Term term : termList2)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
商标评审委员会/nt [0:7]
商标/n [0:2]
评审/vn [2:4]
委员会/nis [4:7]
委员/nnt [4:6]

```

## 其他信息
我查看了一下，感觉应该是在“使用用户词典合并粗分结果”这个部分出的问题。在Segment.java的384行将已经合并的词置空了，然后在Segment.java的302行将循环变量直接跳过了已合并的几项，导致了“商标评审委员会”这个词提出来之后，没有再继续判断是否有“评审委员会”，所以合并完用户词典后，词图里面没有“评审委员会”这个词，后面的索引模式全切分方法decorateResultForIndexMode也没办法识别这个词。

同理，只要是用户词典中存在两个互相包含的词，就只有一个能出来，例如：
```
铁道部运输局营运部货运营销计划处/nt [0:16]
铁道部/nis [0:3]
铁道/n [0:2]
运输局/nis [3:6]
运输/vn [3:5]
营运部/nz [6:9]
营运/vn [6:8]
货运/n [9:11]
运营/vn [10:12]
营销/vn [11:13]
计划处/nis [13:16]
计划/n [13:15]

铁道部运输局/nto [0:6]
铁道部/nis [0:3]
铁道/n [0:2]
运输局/nis [3:6]
运输/vn [3:5]

```

我试着改了几行代码，但是直接就报数组越界了，所以还是来向你求助了。"
分词词性重训练中某些词不在CoreNatureDictionary词表中,"我在训练集中有这样一句话      我/rr 喜欢/v 蓝宝石酒店/nt 可是/c 已经/d 回不去/vf 了/ule
生成的核心词典CoreNatureDictionary的词条中没有   蓝宝石酒店这个词条   怎么能使得生成的核心词表中有这个词。"
字母数字混合的编号分词错误,"HanLP版本`1.3.2`

测试用例如下，
```java
@Test public void wordSegment2() {
        HanLP.Config.enableDebug();
        assertThat(new NShortSegment().enableCustomDictionary(true).seg(""1000000438L01"").stream().map(w -> w.word))
                .containsExactly(""1000000438L01"");
    }
```

HanLP按词性把编号拆分了。如果想保留完整编号，应该如何设置？

> 打印词图：========按终点打印========
> to:  1, from:  0, weight:04.56, word:始##始@未##数
> to:  2, from:  1, weight:07.60, word:未##数@未##串
> to:  3, from:  2, weight:07.06, word:未##串@未##数
> to:  4, from:  3, weight:03.69, word:未##数@末##末
> 
> 粗分结果[1000000438/m, L/nx, 01/m]"
在C#中使用自定义词典，新建Nature失败,"当前最新版本号是：v1.3.2
我使用的版本是：v1.3.2


## 我的问题

我意图在C#中使用HanLP，按照教程[在CSharp中调用HanLP](http://www.hankcs.com/nlp/call-hanlp-in-csharp.html)我已经成功使用iKVM打包，并在C#运行用例。不过我想新建用户字典并新建Nature，以满足我识别自定义类型实体的要求，如新建名人类型：nrmy。

在Java中通过修改hanlp.properties可以添加新的类型nrmy，在写代码的时候添加Nature.create(""nrmy"")也没问题。以上两步做完之后Java代码可以识别新添加的nrmy实体。

不过C#代码会显示无法从hanlp.properties中读取新的类型nrmy。新的类型完全失效。

## 复现问题
```Java
        static void Main(string[] args)
        {
            java.lang.System.getProperties().setProperty(""java.class.path"", ""mypath"");
            Nature.create(""nrmy"");
            CustomDictionary.insert(""白富美"", ""nrmy 1024"");
            Console.WriteLine(HanLP.segment(""欢迎在CSharp中调用HanLP的API""));
            Console.ReadKey();
        }
```

## 报错：
WARNING: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类

Unhandled Exception: java.lang.IllegalArgumentException: Could not set the enum ---> java.lang.IllegalAccessException: Can not set static final [Lcom.hankcs.hanlp.corpus.tag.Nature; field com.hankcs.hanlp.corpus.tag.Nature.$VALUES to [Lcom.hankcs.hanlp.corpus.tag.Nature;
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value, FieldAccessor`1 acc)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.ObjectField.set(Object obj, Object value)
   at com.hankcs.hanlp.corpus.util.ReflectionHelper.setStaticFinalField(Field field, Object value)
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   --- End of inner exception stack trace ---
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(String name)
   at com.hankcs.hanlp.corpus.tag.Nature.create(String name)
   at HanLPSharp.Program.Main(String[] args) in mypath
"
等效词串都有哪些,"[Bigram分词中的等效词串](http://www.hankcs.com/nlp/segment/bigram-word-equivalent-word-in-string.html)
你的文章列举了一些等效词性，请问除了下面这些，还有哪些呢?
未##地, 始##始, 未##它, 未##团, 未##数, 未##专, 未##时, 未##串, 末##末, 未##人

这里的未有什么含义吗？未登录词？这样会不会影响到已经登录的字词呢？

eg:移除每天晚上二十一点的日程
在加入”移除  v 100""后，移和除还是分开的

> 版本1.3.2，NS分词

************START**************
打印词图：========按终点打印========
to:  1, from:  0, weight:04.60, word:始##始@移
to:  2, from:  0, weight:04.60, word:始##始@移除
to:  3, from:  1, weight:02.31, word:移@除
to:  4, from:  2, weight:11.25, word:移除@每
to:  4, from:  3, weight:05.70, word:除@每
to:  5, from:  2, weight:11.25, word:移除@未##时
to:  5, from:  3, weight:03.40, word:除@未##时
to:  6, from:  4, weight:10.58, word:每@天
to:  7, from:  5, weight:04.76, word:未##时@晚
to:  7, from:  6, weight:10.04, word:天@晚
to:  8, from:  5, weight:02.52, word:未##时@未##时
to:  8, from:  6, weight:04.07, word:天@未##时
to:  9, from:  7, weight:10.68, word:晚@上
to: 10, from:  8, weight:02.52, word:未##时@未##时
to: 10, from:  9, weight:04.09, word:上@未##时
to: 14, from: 11, weight:05.57, word:未##时@点
to: 15, from: 10, weight:03.04, word:未##时@的
to: 15, from: 12, weight:03.04, word:未##时@的
to: 15, from: 13, weight:03.04, word:未##时@的
to: 15, from: 14, weight:02.98, word:点@的
to: 16, from: 15, weight:05.62, word:的@日
to: 17, from: 15, weight:05.67, word:的@日程
to: 18, from: 16, weight:10.10, word:日@程
to: 19, from: 17, weight:03.52, word:日程@末##末
to: 19, from: 18, weight:06.15, word:程@末##末

粗分结果[移/v, 除/p, 每天/t, 晚上/t, 二十一点/t, 的/ude1, 日程/n]"
核心字典中“每xx”的词性是不是有误？,"版本1.3.2
毎	nz	10  // 其他专名？
每	rz	4131 // 指示代词？
每一年	nz	31 //应该为t?
每一方	nz	1
每一次	d	188
每一步	d	44
每七年	nz	1 //应该为t?
每个	r	2135   //应该为m?
每五年	nz	4 //应该为t?
每亩	r	117 //应该为m?
每人	r	904
每件	d	61
每份	r	17 //应该为m?
每位	r	201 
每况愈下	vl	22
每包	nz	16 //应该为m?
每匹	nz	2 //应该为m?
每十年	nz	5 //应该为t?
每半年	nz	24 //应该为t?
每吨	q	123 //应该为m?
每周	r	603 //应该为t?
每周三	nz	21 //应该为t?
每四年	nz	10  //应该为t?
每回	nz	8
每场	r	45
每块	r	16
每夜	nz	14 //应该为t?
每天	r	5072 //应该为t?
每头	nz	15
每套	r	23
每家	r	117
每局	r	2
每层	d	27
每年	r	3400 //应该为t?
每张	r	116
每当	p	185
每户	r	120
每批	nz	7
每排	nz	17
每支	r	16
每斤	nz	141
每日	r	1636 //应该为t?
每旬	nz	1
每时每刻	bl	15
每星期	d	17
每晚	r	248 //应该为t?
每月	r	1329 //应该为t?
每期	r	36
每条	d	54
每样	nz	5
每桶	r	14
每次	r	1501
每段路	nz	1
每每	d	59
每片	d	17
...
"
索引分词出现了重复的词,"hi hankcs，有个问题还需要麻烦你，关于这个问题我看了下代码，但具体算法没弄懂。
## 版本号
master分支
当前最新版本号是：d37f97c8d54acda7ca1c6a8baae5cf0ebbd6a775
我使用的版本是：d37f97c8d54acda7ca1c6a8baae5cf0ebbd6a775


## 我的问题
索引分词出现了重复的词

## 复现问题

### 触发代码

```
        List<Term> termList = IndexTokenizer.segment(""南京市长江大桥"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
```

### 实际输出
出现了两个长江大桥
```
南京市/ns [0:3]
南京/ns [0:2]
长江大桥/nz [1:5]
市长/nnt [2:4]
长江大桥/nz [3:7]
长江/ns [3:5]
大桥/n [5:7]
```

## 其他信息

看了下代码，是在ViterbiSegment这个类的56行，调用这个方法后导致的combineByCustomDictionary(vertexList, wordNetAll);
调用完后，词图貌似就有问题了
![qq20170502-115123](https://cloud.githubusercontent.com/assets/5125893/25603470/a55b1b3e-2f2e-11e7-8751-529a30eecafe.jpg)

"
分开不分开，打死也不分开^_^,"版本1.3.2
**新增一个星期五下午4点15分开下午讨论会的日程**

> 处理

> 删掉

未##数@分开 58

> 加入

开@上午 100
开@中午 100
开@下午 100
开@上午会 100
开@中午会 100
开@下午会 100
开@讨论会 100
开@上午讨论会 100
开@中午讨论会 100
开@下午讨论会 100

> 修改

未##数@分 363 -->未##数@分 10000

> DEGUG

打印词图：========按终点打印========
to:  1, from:  0, weight:04.56, word:始##始@新
to:  2, from:  0, weight:04.60, word:始##始@新增
to:  3, from:  1, weight:09.57, word:新@增
to:  4, from:  2, weight:01.07, word:新增@未##数
to:  4, from:  3, weight:01.16, word:增@未##数
to:  6, from:  4, weight:05.12, word:未##数@星
to:  6, from:  5, weight:08.77, word:个@星
to:  7, from:  4, weight:05.09, word:未##数@星期
to:  7, from:  5, weight:06.62, word:个@星期
to:  8, from:  4, weight:04.59, word:未##数@未##时
to:  8, from:  5, weight:05.18, word:个@未##时
to:  9, from:  6, weight:11.26, word:星@期
to: 10, from:  8, weight:05.77, word:未##时@下
to: 11, from:  8, weight:02.52, word:未##时@未##时
to: 12, from: 10, weight:09.41, word:下@午
to: 13, from: 11, weight:02.52, word:未##时@未##时
to: 13, from: 12, weight:11.60, word:午@未##时
to: 15, from: 13, weight:03.44, word:未##时@未##数
to: 15, from: 14, weight:02.68, word:点@未##数
to: 16, from: 15, weight:00.11, word:未##数@分
to: 17, from: 15, weight:09.93, word:未##数@分开
to: 18, from: 16, weight:10.66, word:分@开
to: 19, from: 17, weight:11.45, word:分开@下
to: 19, from: 18, weight:06.89, word:开@下
to: 20, from: 17, weight:04.73, word:分开@未##时
to: 20, from: 18, weight:06.89, word:开@未##时
to: 21, from: 19, weight:09.41, word:下@午
to: 22, from: 20, weight:05.78, word:未##时@讨
to: 22, from: 21, weight:11.60, word:午@讨
to: 23, from: 20, weight:05.79, word:未##时@讨论
to: 23, from: 21, weight:11.60, word:午@讨论
to: 24, from: 20, weight:05.80, word:未##时@讨论会
to: 24, from: 21, weight:11.60, word:午@讨论会
to: 25, from: 22, weight:11.39, word:讨@论
to: 26, from: 23, weight:10.83, word:讨论@会
to: 26, from: 25, weight:11.45, word:论@会
to: 27, from: 24, weight:02.67, word:讨论会@的
to: 27, from: 26, weight:04.51, word:会@的
to: 28, from: 27, weight:05.62, word:的@日
to: 29, from: 27, weight:05.67, word:的@日程
to: 30, from: 28, weight:10.10, word:日@程
to: 31, from: 29, weight:03.52, word:日程@末##末
to: 31, from: 30, weight:06.15, word:程@末##末

**粗分结果[新增/v, 一个/mq, 星期五/t, 下午/t, 4点/t, 15/m, 分开/vi, 下午/t, 讨论会/n, 的/ude1, 日程/n]**
人名角色观察：[  A 22202445 ][新增 A 22202445 ][一个 K 90 L 84 ][星期五 A 22202445 ][下午 L 15 K 12 ][4点 A 22202445 ][15 L 8 ][分开 L 4 ][下午 L 15 K 12 ][讨论会 A 22202445 ][的 L 15411 K 11354 M 96 C 1 ][日程 A 22202445 ][  A 22202445 ]
人名角色标注：[ /A ,新增/A ,一个/K ,星期五/A ,下午/K ,4点/A ,15/L ,分开/L ,下午/K ,讨论会/A ,的/K ,日程/A , /A]

**粗分结果[新增/v, 一个/mq, 星期五/t, 下午/t, 4点/t, 15分/t, 开/v, 下午/t, 讨论会/n, 的/ude1, 日程/n]**
人名角色观察：[  A 22202445 ][新增 A 22202445 ][一个 K 90 L 84 ][星期五 A 22202445 ][下午 L 15 K 12 ][4点 A 22202445 ][15分 A 22202445 ][开 C 865 L 71 D 53 K 13 E 7 ][下午 L 15 K 12 ][讨论会 A 22202445 ][的 L 15411 K 11354 M 96 C 1 ][日程 A 22202445 ][  A 22202445 ]
人名角色标注：[ /A ,新增/A ,一个/K ,星期五/A ,下午/K ,4点/A ,15分/A ,开/K ,下午/L ,讨论会/A ,的/K ,日程/A , /A]
四月 28, 2017 5:07:00 下午 com.hankcs.hanlp.dictionary.CoreDictionaryTransformMatrixDictionary <clinit>

**[新增/v, 一个/mq, 星期五/t, 下午/t, 4点/t, 15/m, 分开/vi, 下午/t, 讨论会/n, 的/ude1, 日程/n]**

"
CoreDictionary的totalFrequency 是个常量,"版本1.3.2
public static final int totalFrequency = 221894;
为什么要定为常量呢？核心字典不是也可以修改吗？"
NS分词直接就取第一个分词结果,"版本1.3.2
如 List<Vertex> vertexList = coarseResult.get(0);

**eg:创建一个下午3点举办下午茶的提醒**

”下午茶“被分成”下午 茶“
我尝试增大”下午茶“频率  和  删除”未##时@茶“也没有作用
只得暂时增加”举办@下午茶 1“来解决

打印词图：========按终点打印========
to:  1, from:  0, weight:04.60, word:始##始@创
to:  2, from:  0, weight:04.60, word:始##始@创建
to:  3, from:  1, weight:11.32, word:创@建
to:  4, from:  2, weight:03.01, word:创建@未##数
to:  4, from:  3, weight:02.21, word:建@未##数
to:  6, from:  4, weight:05.10, word:未##数@下
to:  6, from:  5, weight:08.38, word:个@下
to:  7, from:  4, weight:04.59, word:未##数@未##时
to:  7, from:  5, weight:05.18, word:个@未##时
to:  8, from:  6, weight:09.41, word:下@午
to:  9, from:  7, weight:02.52, word:未##时@未##时
to:  9, from:  8, weight:11.60, word:午@未##时
to: 11, from:  9, weight:05.80, word:未##时@举
to: 11, from: 10, weight:09.78, word:点@举
to: 12, from:  9, weight:05.69, word:未##时@举办
to: 12, from: 10, weight:09.78, word:点@举办
to: 13, from: 11, weight:11.40, word:举@办
to: 14, from: 12, weight:10.87, word:举办@下
to: 14, from: 13, weight:06.25, word:办@下
to: 15, from: 12, weight:03.78, word:举办@未##时
to: 15, from: 13, weight:05.21, word:办@未##时
to: 16, from: 12, weight:10.87, word:举办@下午茶
to: 16, from: 13, weight:10.63, word:办@下午茶
to: 17, from: 14, weight:09.41, word:下@午
to: 18, from: 15, weight:05.80, word:未##时@茶
to: 18, from: 17, weight:11.60, word:午@茶
to: 19, from: 16, weight:02.38, word:下午茶@的
to: 19, from: 18, weight:02.61, word:茶@的
to: 20, from: 19, weight:05.67, word:的@提
to: 21, from: 19, weight:05.64, word:的@提醒
to: 22, from: 20, weight:11.05, word:提@醒
to: 23, from: 21, weight:03.00, word:提醒@末##末
to: 23, from: 22, weight:04.25, word:醒@末##末

粗分结果[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午/t, 茶/n, 的/ude1, 提醒/v]
人名角色观察：[  A 22202445 ][创建 L 16 ][一个 K 90 L 84 ][下午 L 15 K 12 ][3点 A 22202445 ][举办 K 11 L 9 ][下午 L 15 K 12 ][茶 D 19 C 2 E 2 L 2 ][的 L 15411 K 11354 M 96 C 1 ][提醒 L 123 K 14 M 2 ][  A 22202445 ]
人名角色标注：[ /A ,创建/L ,一个/K ,下午/L ,3点/A ,举办/K ,下午/L ,茶/D ,的/L ,提醒/K , /A]
粗分结果[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午茶/nf, 的/ude1, 提醒/v]
人名角色观察：[  A 22202445 ][创建 L 16 ][一个 K 90 L 84 ][下午 L 15 K 12 ][3点 A 22202445 ][举办 K 11 L 9 ][下午茶 A 22202445 ][的 L 15411 K 11354 M 96 C 1 ][提醒 L 123 K 14 M 2 ][  A 22202445 ]
人名角色标注：[ /A ,创建/L ,一个/K ,下午/L ,3点/A ,举办/K ,下午茶/A ,的/K ,提醒/L , /A]
[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午/t, 茶/n, 的/ude1, 提醒/v]
打印词图：========按终点打印========
to:  1, from:  0, weight:04.60, word:始##始@创
to:  2, from:  0, weight:04.60, word:始##始@创建
to:  3, from:  1, weight:11.32, word:创@建
to:  4, from:  2, weight:03.01, word:创建@未##数
to:  4, from:  3, weight:02.21, word:建@未##数
to:  6, from:  4, weight:05.10, word:未##数@下
to:  6, from:  5, weight:08.38, word:个@下
to:  7, from:  4, weight:04.59, word:未##数@未##时
to:  7, from:  5, weight:05.18, word:个@未##时
to:  8, from:  6, weight:09.41, word:下@午
to:  9, from:  7, weight:02.52, word:未##时@未##时
to:  9, from:  8, weight:11.60, word:午@未##时
to: 11, from:  9, weight:05.80, word:未##时@举
to: 11, from: 10, weight:09.78, word:点@举
to: 12, from:  9, weight:05.69, word:未##时@举办
to: 12, from: 10, weight:09.78, word:点@举办
to: 13, from: 11, weight:11.40, word:举@办
to: 14, from: 12, weight:10.87, word:举办@中
to: 14, from: 13, weight:08.25, word:办@中
to: 15, from: 12, weight:03.78, word:举办@未##时
to: 15, from: 13, weight:05.21, word:办@未##时
to: 16, from: 14, weight:08.17, word:中@午
to: 17, from: 15, weight:05.80, word:未##时@茶
to: 17, from: 16, weight:11.60, word:午@茶
to: 18, from: 17, weight:02.61, word:茶@的
to: 19, from: 18, weight:05.67, word:的@提
to: 20, from: 18, weight:05.64, word:的@提醒
to: 21, from: 19, weight:11.05, word:提@醒
to: 22, from: 20, weight:03.00, word:提醒@末##末
to: 22, from: 21, weight:04.25, word:醒@末##末
"
变量名修正,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

修正变量名




"
移除逻辑重复的语句,"## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

删除了多余的操作，提高效率。



"
中文分词bug,"		System.out.println(NLPTokenizer.segment(""花了38元人民币""));
		System.out.println(HanLP.segment(""花了38元人民币""));
[花/n, 了/ule, 38/m, 元/q, 人民币/n]
[花/n, 了/ule, 38/m, 元/q, 人民币/n]"
分词问题,"**问题**
对句子""我想提点钱出来""进行分词，
得到结果: [我/rr, 想/v, 提/v, 点钱/nz, 出来/vf]，分词中“点钱”切分错误？

**版本**
- 程序版本号：hanlp-portable-1.2.7
- 数据版本号：data-for-1.3.2.zip

**触发代码**
```java
HanLP.segment(""我想提点钱出来"");
```
**代码输出**
``` 
[我/rr, 想/v, 提/v, 点钱/nz, 出来/vf]
```

**尝试**
分词结果有问题，把`点钱`作为一个词切分出来，而不是切分为`提点/v 钱/n`。
我在词库CoreNatureDictionary.txt中发现 “点钱 nz 110”，而没有“提点”词，所以我用CustomDictionary将“提点”加入，但是还是没有效果，麻烦帮忙看一下，非常感谢。"
句法依存——基于神经网络,"“建一个下午7点3刻的提醒”
1	建	v	v	_	7	定中关系	_	_
2	一个	m	mq	_	3	定中关系	_	_
3	下午	n	n	_	4	定中关系	_	_
4	7点	nt	t	_	5	定中关系	_	_
5	3刻	nt	t	_	1	动宾关系	_	_
6	的	u	ude1	_	5	右附加关系	_	_
7	提醒	v	vn	_	0	核心关系	_	_

建 --(定中关系)--> 提醒
一个 --(定中关系)--> 下午
下午 --(定中关系)--> 7点
7点 --(定中关系)--> 3刻
3刻 --(动宾关系)--> 建
的 --(右附加关系)--> 3刻
提醒 --(核心关系)--> ##核心##

我认为是提醒的词性影响了结果，分词用的是NS分词器，只是我没有想到什么办法修改提醒的词性为n。这里的词性应该是用HMMSegmentModel.bin来标注的吧！请问除了重新训练一个模型，还有什么方法解决这个问题"
发现个人名分词的bug,所有姓冯、于、文的名字分词都是错误的冯会被分为nz（其他专名）；于，会被分为p（介词）；文，会被分为ng（名词性语素）
情感分析,Hanlp为什么没有实现情感分析相关的功能呢？
发现个分词bug：对“阿里”，“华为”分词结果返回词性为ns（地名）,对“阿里”，“华为”分词结果返回词性为ns（地名）
python如何调用model下面的模型,"python如何调用model下面的模型,Hanlp 1.3.2"
关于model（模型）在下载的时候百度云实效还是给墙了？,"你好：
        我在下载data-1.28-full（大致）的时候，百度云连接失效，请问还有什么路径或者链接吗？我尝试了翻墙去链接还是不可以的。谢谢"
Crf分词结合ngram词典不能取得理想结果,"@hanks, 你好。
关于下列输入
CRFSegment segment = new CRFSegment().;
segment.enableCustomDictionary(true);
		List<Term> seg = segment.seg(""哪些药产自上海"");

一直出现不理想的分词结果

[哪些/r, 药产/nz, 自/p, 上海/ns]

理想的结果是 [哪些/r, 药/n,产自/v, 上海/ns]

已经按照您在其他issue里面说的修改了核心词典CoreNatureDictionary.mini.txt
其中包含（产自	v	999）新增，（药	n	41	v	1）药这个词是原来就有的
也修改了CoreNatureDictionary.ngram.mini.txt
（药@产自 10000）新增，
并且在自定义词典CustomDictionary.txt中新增（产自 v 999）
删除所有缓存，并且运行，还是出现[哪些/r, 药产/nz, 自/p, 上海/ns]结果

不知道我哪里做的不对，或者缺少了哪些步骤，还请务必告知
非常感谢！
@TylunasLi"
关于停用词的问题,"我在stopwords里加了停用词 ‘，’,然后在代码里加了停用词‘近日’，结果都没起作用
`      Segment segment = HanLP.newSegment();

        segment.enablePartOfSpeechTagging(true);
        segment.enableCustomDictionary(true);
        System.out.println(CoreStopWordDictionary.contains(""，""));
        CoreStopWordDictionary.add(""近日"");
        CustomDictionary.insert(""位于黄骅经济开发区"");
        CustomDictionary.add(""位于黄骅经济开发区"");
        System.out.println(segment.seg(""近日，位于黄骅经济开发区的河北宏泰专用汽车有限公司生产的新型铝合金罐式车。""));`
结果是：[近日, ，, 位于黄骅经济开发区, 的, 河北, 宏泰, 专用汽车, 有限公司, 生产的, 新型, 铝合金, 罐式, 车, 。]"
请问一下汉字繁简体转换的问题？,"你好，我最近使用的你的HanLP-1.2.8版本中的汉字繁简体转换。
我发现一个问题，有些繁体到简体的转换，不是按原字转换的，而是换了一个词，这给我带来很大问题。
比如：文字=文本
内容=属性
在你的词典文件TraditionalChinese.txt中可以查到。类似的情况有很多。

请问，这个问题可以通过配置解决吗？可以严格按原字转换吗？

另外，你的转换词典有些遗漏的转换不了，比如：勐戳， 減肥

我想问一下，我可以在TraditionalChinese.txt中添加新的转换词表，但是不知道怎么产生bin文件，不能起作用，可以让用户修改这个转换词典吗？怎么产生bin文件？

谢谢，你的分词工具做的不错。这个问题，希望能一起处理，谢谢。"
使用solr搜索分词后的结果搜不到，麻烦解答一下。,"使用hanlp分词,索引内容：“中医药大学附属医院”，solr测试字段用的是“hanlp”。
搜hanlp:""中医""能搜到，搜hanlp:""中医药""却搜不到。具体问题截图在这里：https://www.oschina.net/question/145106_2239234
麻烦作者帮忙看下，多谢！"
有关动态添加单词的问题,"hanks，你好，
在经过动态添加单词，或者单词+词性
CustomDictionary.insert(word);
CustomDictionary.add(word);
之后，应该通过什么方法才能获得该词的原分词结果和词性
比方说，强制添加了一个新词CustomDictionary.insert(“睡不着觉 xx 1”);
之后想获得该词的原来的分词结果
[睡/v, 不/d, 着/uz, 觉/v
可有方法？"
有关CRF分词语料的格式问题？,"@hankcs ，你好。
我想做一个自己的分词模型，想知道，你当初训练model使用的模型是哪一种？
例1：
才 d S
发 v B
觉 v E
已 d S
迷 v B
失 v E
了 u S
来 n B
路 n E
。 w S


例2：
据	S
了	B
解	E
，	S
目	B
前	E
省	S
农	B
业	M
厅	E
所	B
有	E
行	B
政	E


非常希望您能够告知，谢谢！
"
发现一个词性处理的问题 词性为w的内容会有乱码，这个是什么情况,"示例：
���������ֱ���ǻ��� w 0
���������ڽ��ϵ������µĽ���ȷʵ����� w 0
���������ܻ���������� w 0
���������� w 0
����������Ƥ w 0
����������ǿ�ҵĸɵĸ w 0
����������ȶȡ� w 0
����������Ȼ���ۣ�����̫������̼��� w 0
����������ʳ�ķ������������˳��ϸ� w 0
����������Եĳ����𣿿������ԣ��� w 0
����������　　 w 0
����������� w 0
�����������ǲ��ǿ��ž����㵽һ����� w 0
�����������ʳ�أ� w 0
�����������ҩ�ŵĴ����Ʒ�� w 0
�����������　　 w 0
������������ w 0
������������ˣ�Ů�˵���ѹ����ӳ w 0
������������һ�𿴿� w 0
������������� w 0
�������������¹�Ҷ��� w 0
�������������΢���� w 0
�������������　　 w 0
�������������� w 0
��������������� w 0
���������������/ w 0
����������������� w 0
�����������������ڽ�ͷ��� w 0
������������������� w 0
�������������������࣬��ʳ���Ļ��� w 0


请问这样的问题该如何处理，谢谢！"
关键词提取 如何让其识别外国人名,"代码：
        List<String> keyWords = 
                        HanLP.extractKeyword(""亨利·福特（HenryFord，1863年7月30日—1947年4月8日），美国汽车工程师与企业家，汽车公司的建立者。"",30);
        keyWords.forEach(System.out::println);

结果：
汽车 美国汽车工程师 企业家 HenryFord 公司 福特 建立

请问 如果想让关键词 启用人名识别 怎么启用？ 主要是目的 不是让其解析出“福特” 而是用关键词解析出“亨利·福特”"
都误标成人名   nr.txt中L、B、D含义,现在遇到问题：都买/nr    都碰/nr  都好大/nr  都和谁/nr等，想改都作为姓氏的频数。谁能告诉我nr.txt中的L、B、D等都表示什么意思       都 L 587 B 308 D 32 C 16 E 6 K 3
修正nodes最后一个元素大小为0时造成的异常,
维特比算法有一处BUG,"#### ViterbiSegment.java#148 行
`Vertex from = nodes[nodes.length - 1].getFirst();`
**测试代码：**

```java
System.out.println(new ViterbiSegment()
               .enableNameRecognize(true)
               .enableIndexMode(true)
               .enablePlaceRecognize(true)
               .enableOrganizationRecognize(true)
               .enableCustomDictionary(true).seg(""癌症死亡率与发病率逐年增高,已成为世界范围内的一大健康问题。多西他赛(docetaxel,DTX)是紫杉烷类抗癌药,其作用机制是抑制微管蛋白解聚,从而阻断肿瘤细胞增殖。多西他赛抗癌谱较广,可以用来治疗肺癌、乳腺癌、前列腺癌、卵巢癌,治疗效果是紫杉醇的2-4倍。但是其水溶性较差,临床注射液应用吐温80增溶,易引起毒副反应；其分布无特异性,易引起全身毒性。此外,由于紫杉烷类药物的大量使用,导致癌症细胞对紫杉烷类药物产生了多药耐药(multi-drug resistance,MDR)性,这成为DTX使用中的另一大问题。癌症细胞产生耐药性的主要原因是细胞膜转运蛋白p-glycoprotein(p-gp)的过量表达。由于高分子科学的快速发展,应用聚合物高分子作为药物载体包载疏水性药物以增加疏水药物溶解性和治疗效果的研究成为热点。两亲性嵌段聚合物可以在水中聚集为胶束；胶束具有亲-疏水核壳结构,内核疏水而外壳亲水,其疏水内核可以包载疏水性药物。聚合物胶束可以通过物理性包埋和化学结合两种方式包载药物。化学结合载药是将药物和聚合物嵌段通过化学键化学结合形成聚合物-药物结合物(polymer-drug conjugate),药物成为聚合物的一部分,参与胶束的组装。此方法可以增加胶束的载药量和稳定性；还可利用环境敏感的化学键共价连接药物和聚合物,起到在肿瘤部位集中释放药物,提高药物疗效的作用,这也是此类药物传递系统的一大创新点。肿瘤组织内具有较低的pH值,特殊酶和较强的还原性环境,基于此特点,可以采用pH敏感、酶敏感和氧化还原敏感的化学键连接药物,使聚合物具有环境敏感性质。本课题成功合成了具有氧化还原敏感的聚合物-药物结合物,使其在水中自组装,并进一步用于包载其他药物。选择两亲性聚合物甲氧基聚乙二醇-聚丙交酯乙交酯(mPEG-PLGA,PP)作为大分子骨架,通过氧化还原敏感的二硫键连接DTX,合成敏感的nPEG-PLGA-SS-DTX (PP-SS-DTX)结合物。利用此结合物进一步包载多西他赛或维拉帕米(verapamil, VRP),实现载药体系功能多样化。本课题研究包括以下几个方面：1、PP-SS-DTX结合物的合成和表征：选择含有二硫键的二硫代二丙酸(即DTDP)作为连接分子,通过三步化学反应成功合成了PP-SS-DTX结合物,并通过熔点测定、核磁共振氢谱(1H-NMR);和傅立叶变换红外色谱(FT-IR)验证其结构。此外,通过芘探针法测定其临界聚集浓度(critical aggregation concentration, CAC),发现PP-SS-DTX结合物的CAC值很小,为10.20 μmol/L。2、氧化还原敏感型双模式载多西他赛PP-SS-DTX/DTX胶束系统的评价：通过透析法制备双模式载多西他赛PP-SS-DTX/DTX胶束,双模式是指同一体系中通过化学结合和物理包埋两种模式包载DTX。通过TEM和DLS测定胶束的形态和粒径,所制备的PP-SS-DTX/DTX胶束呈球形,粒径为112.3 nm;PP-SS-DTX/DTX胶束具有较高的载药量,为(14.65±0.71)%。通过溶血试验初步判断PP-SS-DTX/DTX胶束具有一定生物相容性。为验证PP-SS-DTX/DTX胶束释药是否具有氧化还原敏感性,在释放介质中加入还原型物质DTT。释放实验结果表明,与DTX原料药溶液相比,PP-SS-DTX/DTX胶束具有药物缓释、氧化还原敏感性释药和程序性释药的特点。选择乳腺癌MCF-7细胞系和黑色素瘤B16F10细胞系进行体外细胞毒性实验与细胞摄取实验。在两种细胞系中,PP-SS-DTX/DTX胶束的毒性作用均明显强于DTX溶液。由于DTX不能发荧光,利用香豆素-6(coumarin-6, C-6)模拟疏水药物DTX,制备了PP-SS-DTX/C-6胶束,通过荧光倒置显微镜技术和流式细胞术定性、定量的研究细胞对PP-SS-DTX/C-6胶束的摄取。结果表明,MCF-7和B16F10两种细胞对PP-SS-DTX/C-6胶束的摄取效率高于C-6溶液。细胞对胶束制剂的高摄取效率,保证了药物浓集于病变细胞,是提高抗肿瘤效果的重要原因。3、PP-SS-DTX/VRP双模式双载药胶束系统的抗肿瘤多药耐药的评价：采用探头超声法制备PP-SS-DTX/VRP胶束,双模式是指分别通过化学结合和物理包埋方式载药,双载药是指一个胶束体系同时包载两种药物——多西他赛和维拉帕米。维拉帕米是有效的p-gp的拮抗剂,可与p-gp结合并使其失活,逆转癌症细胞的多药耐药。在制备胶束过程中,考察探头超声的超声时间对维拉帕米的包封率与载药量的影响,发现探头超声的时间越短,维拉帕米的载药量和包封率越高。通过TEM和DLS测定胶束的形态和粒径,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束均呈球形,粒径分别为(79.3±1.2)nm和(78.3±4.6)nm。PP-SS-DTX/VRP胶束中DTX载药量为(8.94±0.72)%,维拉帕米的载药量为(5.66±1.60)%,包封率为(53.49±10.96)%。为验证胶束的体外释放是否具有敏感性,在释放介质中加入DTT,实验发现DTX和VRP的释放均具有氧化还原敏感性。为研究胶束的抗肿瘤多药耐药的作用,选择乳腺癌MCF-7细胞和耐药的乳腺癌MCF-7/ADR细胞,进行DTX溶液、PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞毒性实验、细胞摄取实验和细胞凋亡实验。细胞毒性实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束对两种细胞系的细胞毒性作用均好于DTX溶液；此外,PP-SS-DTX/VRP胶束对MCF-7/ADR细胞的细胞毒性作用强于PP-SS-DTX胶束。由于DTX和VRP不能发荧光,因此选择p-gp底物罗丹明123(rhodamine 123,RH 123)作为荧光分子对胶束进行标记研究细胞摄取情况和p-gp抑制效果。结果表明,MCF-7对PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞摄取效率相似,且均大于RH 123溶液；MCF-7/ADR细胞对RH 123几乎无摄取,对PP-SS-DTX/VRP胶束的摄取大于对PP-SS-DTX胶束的摄取。细胞凋亡实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束培养的细胞凋亡数目多于DTX溶液组。以上实验结果表明,PP-SS-DTX包载VRP后,可以抑制p-gp的作用,增强耐药细胞对胶束的摄取,加快细胞凋亡的进程,从而提高多西他赛的抗肿瘤效应。4、PP-SS-DTX胶束体内药动学研究：实验动物选择Wistar大鼠,通过尾静脉注射给药,利用HPLC法检测大鼠血浆中DTX的含量。以DTX原料药溶液作对照,观察PP-SS-DTX胶束的体内药动学过程,得到血药浓度-时间曲线,并通过DSA 2.0软件对血药浓度-时间曲线拟合,得PP-SS-DTX胶束房室模型和药动学参数。将DTX制备成PP-SS-DTX胶束后,血药浓度-时间曲线变平缓；通过对曲线拟合,发现DTX溶液和PP-SS-DTX胶束均符合二室模型。将DTX制备成PP-SS-DTX胶束后,体内清除率降低,消除半衰期和体内滞留时间延长,有利于药物缓释并保证药物疗效。综上所述,本研究首次合成了具有氧化还原敏感的聚合物-药物结合物PP-SS-DTX,此结合物可以自组装为胶束,包载DTX或VRP,提高DTX的溶解度和抗肿瘤效果,并能够逆转DTX的MDR。本课题研究具有重要的意义,为提高疏水性药物溶解度、提高药物疗效、解决肿瘤细胞多药耐药的研究提供一定的理论基础。""));
```

**报错信息如下：**
```
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList.getFirst(LinkedList.java:242)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:148)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at TestMain.main(TestMain.java:11)
```

**其他说明：**
>不使用HanLp自带的自定义词典不会报错

**改进后的代码：**
```java
LinkedList<Vertex> node = null;
int index = 1;
while (node == null || node.size() <= 0){
     node = nodes[nodes.length - index];
     index++;
}
Vertex from = node.getFirst();
```"
HanLp 有敏感信息识别功能？,
HanLp 有敏感信息识别功能,
chartype识别能自定义么？,在进行分词调试过程中，字符类型是CharType.dat.yes从加载的。能否自定义chartype类型呢？实现更精细的字符类型控制？
自定义词典采用AhoCorasickDoubleArrayTrieSegment发生了重复分词的问题,"hankcs，你好。
只用一个自定义词典“mydict.txt”，里面内容有“北京”、“北京市”。对“北京市长安街”用AhoCorasickDoubleArrayTrieSegment分词，结果为【北京，北京市】，不符合我的预期。
我希望达到的效果是词典中一个词包含另一个词时，只采用最长的那个词匹配，即分词结果为【北京市】。请问我换一个算法还是怎么做？谢谢。"
自定义词典中标为ntc的词分词会显示为nt,"比如例句""教授在教授课程""，如果我在自定义词典加入“教授课程 ntc 1000”则默认分词HanLP.newsegment能够正确分词，但会将“教授课程”标为nt 
教授/nnt,在/p,教授课程/nt
然而教授课程不是只有自定义的ntc这个词行吗？为何会识别为nt
但是如果自定义词典中将教授课程改为全新的词性，或是一些较为常见的，如n,a，v之类的就没有问题
非常困扰，在此先感谢拉"
邮箱分词,"测试用例中采用sunny800629@sina.com进行功能测试，分词的效果是这样的：[0:5 1] sunni/nx
[5:11 1] 800629/m
[11:16 1] @sina/nx
[16:17 1] ./w
[17:20 1] com/nx
这里面的@sina，我想拆成@和sina两个词。经过调试在生成图网的时候就已经是这个样子了。请问，有什么解决办法么？"
修正一个拼音,
hankcs好，我看github上说有1.4版本的计划，请问1.4预计什么时间会出呢？可有一个大概出的时间区间？,RT
model.txt如何转换为model.txt.bin,"您好，我在[issue#45](https://github.com/hankcs/HanLP/issues/45)中看到您提到将CRF++训练出的model.txt转换成了model.txt.bin，然后将model.txt.bin文件作为HanLP的CRF分词模型，我想请问一下这个转换过程是如何完成的，目的是什么？还望不吝赐教，谢谢！
另，我的目的是想要使用我自己用CRF++训练出来的模型替换您HanLP提供的模型来定制CRF模型。"
请问这个的CRF 使用的model 和CRF++ 训练出来的model通用吗？,"我按照您的教程，用CRF++训练出模型，然后替换掉HanLP的CRF模型，报错：
`Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space`

所以想问一下，使用CRF++训练的模型在HanLP中通用吗？"
扩展识别器,有没有扩展识别器的接口实现啊？想实现一些自定义的识别实现。
HanLP Elasticsearch 5.x 插件,"我们实现了 elasticsearch 5.x 的插件：https://github.com/hualongdata/hanlp-ext 。
不过现在遇到个小问题需要手动设置 -Djava.security.policy 和 ES_CLASSPATH，希望有人能帮我解决。"
使用CustomDictionary.get获取核心词典单词为null，开启调试显示词库皆已加载,"以下每次测试前都删除了bin文件
以下测试均使用源码和jar包导入两种形式测试过，结果一致
望hankcs解惑

1.CustomDictionary.get函数获取核心词典返回为Null
结果为null
但HanLP.Config.enableDebug()开启调试后，调用分词函数HanLP.extractPhrase(""更多精彩请搜索王者荣耀"",10);结果可以看到 搜索/vn
此时调用CustomDictionary.get(“搜索”)获取信息仍然是Null

2.自定词库未载入
HanLP.Config.enableDebug()开启调试后提示加载了自定义词典，该词典只有一条记录（王者荣耀 nz 1），但是调用分词函数HanLP.extractPhrase(""更多精彩请搜索王者荣耀"",10);结果可以看到 王者/n, 荣耀/an.使用CustomDictionary.get(“王者荣耀”)返回为null

3.修改已有的自定义词典未生效
使用CustomDictionary.get(“b超”)获取到了“现代汉语补充词库.txt”中的第一条记录，于是我将""王者荣耀 nz 1""添加在这个词典的开头（失败后又尝试放在结尾，仍然失败）,使用CustomDictionary.get(“王者荣耀”)返回为null，CustomDictionary.get(“b超”)仍然有返回值。"
关于一词多词性的获取问题,"我自定义两个字典，
test1.txt
汽车 qchy 1
test2.txt
汽车 kxp 1
我想通过“汽车”这个词语获得到 qchy 和 kxp 两个词性。我在使用CustomDictionary.get（“汽车”)，只能获得
kxp 这个么一个。我知道还有一种就是 把这两个词性写入到同一行，但是到时候判断词性的时候就得使用indexOf(""kxp"")等这种方式遍历了。
请问有没有更好的办法解决？
------  奔溃了，一直提交不上去，不知道什么情况，打了好几遍-------"
不解：停用词词典中为什么有很多重复的词,"since
since
since
sincere
six
six
sixty
so
so
some
some
somehow
somehow
someone
something
sometime
sometimes
somewhere
still
still
such
such
system
take
ten
ten
than
that
that
that
that
that
the
the
the
their
their
their"
升级最新版本后角色标注识别的机构名称怎么全部匹配了,"我现在只是想识别出一个机构名称：陕西省渭南市蒲城迎宾商城有限公司
![image](https://cloud.githubusercontent.com/assets/20895017/24889354/13b04210-1e9c-11e7-891f-339a38b8a62a.png)
之前是根据长度优先匹配，显示的结果只有一个，现在结果是多个，需要怎么调整只让它显示的结果只有一个
最后显示的结果[陕西省/ns, 渭南市蒲城迎宾商城有限公司/nt]  之前最终的结果是[陕西省渭南市蒲城迎宾商城有限公司/nt]"
关于用CRF进行词性标注的几个问题,"在使用CRF++时，发现example的例子是有3列的，中间列是词性
我做了一个简单的语料
                ""放周杰伦"",
                ""放周杰伦的歌"",
                ""放电台"",
                ""播电台"",
                ""播电台北京怀旧金曲"",
                ""听电台"",
                ""要听电台"",
                ""我想听电台"",
                ""我要听电台"",
                ""放有声书"",
                ""我想听相声"",
                ""播鼓曲"",
                ""我想听鼓曲"",
                ""放越剧"",
                ""播越剧"",
                ""收听心动FM"",
                ""收听深圳动听102"",

放      v       S
周      nt      B
杰      nt      M
伦      nt      E
放      v       S
周      ns      B
杰      ns      M
伦      ns      E
的      ude1    S
歌      n       S
台      n       E
播      v       S
电      a       B
台      n       E
听      v       S
电      a       B
台      n       E
我      rr      S
想      v       S
听      v       S
电      a       B
...

**Q1: “周杰伦”是一首歌曲还是一个人名**
其中 “放周杰伦”词性标注为 “放/v, 周/nt,  杰/nt, 伦/nt”
其中 “放周杰伦的歌”词性标注为 “放/v, 周/ns,  杰/ns, 伦/ns, 的/ude1, 歌/n”
这里的周杰伦分别为nt,ns属性，nt和ns只是用来测试而已，其目的是能根据上下文件确定“周杰伦”是一首歌曲还是一个人名。

**QA:“周杰伦”被拆分成“周 杰 伦”三个字后，每个字应该标注什么词性？**
周杰伦本身是一个人名，拆分成“周 杰 伦”三个字后，应该如何对每个字进行词性标注呢？

**Q3:在hanlp在，通过CRF进行词性标注会不会生效？**
我发现hanlp的词性标注是通过自定义字典实现的，CRF本身只是分词，并没有进行词性标注。
1）如果我对CRF的模型本身已经加入词性一列进行训练，那通过hanlp的CRF能否标注出词性，试了一下，发现是没有的，用的是上面的例子，所有字词词性都是null
2）我理解CRF本身是不需要词典的（或者说词典本身就在训练的语料中）,而hanlp中的CRF似乎使用了core字典。
3）hanlp的CRF分词器没有使用实体识别，这是为什么呢？"
但是英文还有一个词根还原的功能，就是went能识别出go。这种情况hanlp有么,
Python 如何调用词共现统计？,"对 java 不甚熟悉，参考 https://github.com/hankcs/HanLP/blob/master/src/test/java/com/hankcs/demo/DemoOccurrence.java
折腾一段时间之后无果。。
求指教 :)"
python实现地理命名实体识别,我想问一下，对于地理命名实体识别，我怎么构建自己的分词词典。。。我先用python来实现这个问题。。
同义词是用什么算法？,"你好，hanks！
我发现你的同义词准确率挺高的，想知道你是用什么算法，如果能够有该算法的论文就更好了，太感谢！"
能否对英文文献进行处理吗？,想对英文内容进行摘要，关键词查找，评分等功能
DoubleArrayTrie的错误,"debug DemoNLPSegment时，发现得到“始##始”的wordID是42996，是正确的。但是""末##末""的wordID是75262，是错误的。正确的ID应该是75261。这是什么原因？"
如何使用自定义停用词，去除默认的停用词,"需要对一些无意义的词进行停用，但是发现如果使用默认的停用词就会把我关注的一些信息也过滤掉了；
我用是：NotionalTokenizer.segment(sentence)；"
Dependency viewer,可以给一个Dependency viewer下载链接吗？谢谢楼主了。
support custom dictionary with csv,
《自动摘要》文章字数达到一定量导致内存溢出," java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:74)
    	at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:253)
    	at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:484)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.getTextRankSummary(AutoSummaryUtil.java:299)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.truncExcerptContent(AutoSummaryUtil.java:133)

我用一一篇 文章 大概有16w字左右，debug 看了 那个 切割后的句子的 sentences size 有1w以上，然后 在调用 分词排序算法时 TextRankSentence textRank = new TextRankSentence(docs);就抛出内存溢出，求大神 帮帮解释下，这个大概消耗内存 是多少，处理的量范围；谢谢

public TextRankSentence(List<List<String>> docs)
    {
        this.docs = docs;
        bm25 = new BM25(docs);
        D = docs.size();
        weight = new double[D][D];
        weight_sum = new double[D];
        vertex = new double[D];
        top = new TreeMap<Double, Integer>(Collections.reverseOrder());
        solve();
    `}```
weight = new double[D][D]; 这个当  docs.size();超过1w是就报内存溢出；"
“听电台北京怀旧金曲”VS“听电台北京音乐频道”,"调用Segment segment = new NShortSegment().enableCustomDictionary(true);

1. **听电台北京怀旧金曲-->[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]**
打印词图：========按终点打印========
to:  1, from:  0, weight:04.60, word:始##始@听
to:  2, from:  1, weight:05.49, word:听@电
to:  3, from:  1, weight:10.85, word:听@电台
to:  4, from:  2, weight:10.35, word:电@台
to:  5, from:  2, weight:02.90, word:电@未##地
to:  6, from:  3, weight:11.44, word:电台@北
to:  6, from:  4, weight:10.57, word:台@北
to:  7, from:  3, weight:04.54, word:电台@未##地
to:  7, from:  4, weight:05.39, word:台@未##地
to:  8, from:  5, weight:04.28, word:未##地@京
to:  8, from:  6, weight:10.84, word:北@京
to:  9, from:  7, weight:07.56, word:未##地@怀
to:  9, from:  8, weight:11.18, word:京@怀
to: 10, from:  7, weight:09.53, word:未##地@怀旧
to: 10, from:  8, weight:11.18, word:京@怀旧
to: 11, from:  9, weight:11.50, word:怀@旧
to: 12, from: 10, weight:11.56, word:怀旧@金
to: 12, from: 11, weight:11.27, word:旧@金
to: 13, from: 10, weight:04.24, word:怀旧@金曲
to: 13, from: 11, weight:11.27, word:旧@金曲
to: 14, from: 12, weight:10.94, word:金@曲
to: 15, from: 13, weight:11.58, word:金曲@末##末
to: 15, from: 14, weight:03.15, word:曲@末##末

粗分结果[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]
人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电 E 20 K 19 C 14 L 6 D 3 ][台北 K 1 ][京 C 305 E 274 D 97 B 24 ][怀旧 L 4 ][金曲 A 22202445 ][  A 22202445 ]
人名角色标注：[ /A ,听/K ,电/E ,台北/K ,京/B ,怀旧/L ,金曲/A , /A]
粗分结果[听/v, 电台/nis, 北京/ns, 怀旧/vn, 金曲/nz]
人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电台 K 3 ][北京 K 37 L 28 ][怀旧 L 4 ][金曲 A 22202445 ][  A 22202445 ]
人名角色标注：[ /A ,听/K ,电台/K ,北京/K ,怀旧/L ,金曲/A , /A]
[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]

其中有一些边是找不到的，这个数值哪来的呢
“ ”@听 181
听@电 12 
听@电台 0
电@台 0
电@台北 354
电台@北 0
电台@北京 5 **查不到这个路径**
台@北 0
台@北京 21 **查不到这个路径**
台北@京 8 **查不到这个路径**
北@京 0
北京@怀 8  **查不到这个路径**
北京@怀旧 0
京@怀旧 0
怀@旧 0
怀旧@金 0
怀旧@金曲 2
旧@金曲 0
金@曲 0
金曲@“ ” 0
曲@“ ” 9


2. **听电台北京音乐频道-->[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]**
打印词图：========按终点打印========
to:  1, from:  0, weight:04.60, word:始##始@听
to:  2, from:  1, weight:05.49, word:听@电
to:  3, from:  1, weight:10.85, word:听@电台
to:  4, from:  2, weight:10.35, word:电@台
to:  5, from:  2, weight:02.90, word:电@未##地
to:  6, from:  3, weight:11.44, word:电台@北
to:  6, from:  4, weight:10.57, word:台@北
to:  7, from:  3, weight:04.54, word:电台@未##地
to:  7, from:  4, weight:05.39, word:台@未##地
to:  8, from:  5, weight:04.28, word:未##地@京
to:  8, from:  6, weight:10.84, word:北@京
to:  9, from:  7, weight:05.53, word:未##地@音
to:  9, from:  8, weight:11.18, word:京@音
to: 10, from:  7, weight:05.12, word:未##地@音乐
to: 10, from:  8, weight:11.18, word:京@音乐
to: 11, from:  9, weight:11.52, word:音@乐
to: 12, from: 10, weight:11.02, word:音乐@频
to: 12, from: 11, weight:11.33, word:乐@频
to: 13, from: 10, weight:04.64, word:音乐@频道
to: 13, from: 11, weight:11.33, word:乐@频道
to: 14, from: 12, weight:11.46, word:频@道
to: 15, from: 13, weight:05.94, word:频道@末##末
to: 15, from: 14, weight:04.92, word:道@末##末

粗分结果[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]
人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电台 K 3 ][北京 K 37 L 28 ][音乐 L 9 ][频道 K 19 ][  A 22202445 ]
人名角色标注：[ /A ,听/K ,电台/K ,北京/K ,音乐/L ,频道/K , /A]
粗分结果[听/v, 电/n, 台北/ns, 京/b, 音乐/n, 频道/n]
人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电 E 20 K 19 C 14 L 6 D 3 ][台北 K 1 ][京 C 305 E 274 D 97 B 24 ][音乐 L 9 ][频道 K 19 ][  A 22202445 ]
人名角色标注：[ /A ,听/K ,电/E ,台北/K ,京/B ,音乐/L ,频道/K , /A]
[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]"
你好，DoubleArrayTrie并没有看到有添加或删除某个字词的接口,比如“我要听雪漫频道”就被分成“我 要 听雪 漫 频道”，而我希望的是分成“我 要 听 雪漫 频道”。如果能动态地从DoubleArrayTrie结构动态删除某个字词，那我就不需要重启服务了
import junit.framework.TestCase这个是什么错误，要怎么解决,"Description	Resource	Path	Location	Type
The import junit cannot be resolved	AdjustCorpus.java	/wenben/src/com/hankcs/test/corpus	line 28	Java Problem
"
如何让多个tokenizer变量有各自的用户自定义词典CustomDictionary,"hi，hankcs，感谢你对中文NLP领域做出的贡献。

根据我自己的场景，请教一个问题：
比如有两个分词变量：tokenizer1和tokenizer2，需要动态增加用户自定义词典，如何让两个分词变量有各自的用户自定义词典，而不互相影响呢？
依据demo中调用CustomDictionary.add(""攻城狮"", ""nz 1024 n 1""); 然后，便会影响所有tokenizer的分词结果了。
请问：是否有方法让多个分词变量，有各自的用户自定义词典，而不互相影响呢？

谢谢！
"
为什么代码都是乱码,"

"
Python调用CRF分词,"@hankcs  
环境Python3，
参考demo后：https://github.com/hankcs/HanLP/tree/master/src/test/java/com/hankcs/demo 
CRF= JClass('com.hankcs.hanlp.seg.CRF.CRFSegment') 
#引入成功，
#但是  
CRF.seg('""威廉王子发表演说 呼吁保护野生动物\n""')
#报错：

Traceback (most recent call last):

  File ""<ipython-input-41-a8007c89d333>"", line 2, in <module>
    CRF.seg('""威廉王子发表演说 呼吁保护野生动物\n""')

RuntimeError: No matching overloads found. at native\common\jp_method.cpp:121
 求答复，thanks！"
root 地址支持设置成 resources 中的目录,现在 root 地址只能设置成系统地址，现在我希望将 data 直接打包在 resource 中，方便使用 docker 进行部署，不知道现在只支持从系统路径读取是否有别的考虑，如果不是，我是否可以添加这个功能？比如用resource:// 作为标识
DoubleArrayTrie Error ,"```
                DoubleArrayTrie<String> arrayTrie = new DoubleArrayTrie<String>();
		Map<String, String> v = new HashMap<>();
		v.put(""《1,2,3,4》"", ""model1"");
		v.put(""《1,2,3"", ""model3"");
		v.put(""《1,2"", ""model2"");
		v.put(""《1,"", ""model5"");
                System.err.println(arrayTrie.exactMatchSearch(""《1,""));

```
Error:
`
		p = b;
		int n = base[p];
		if (b == check[p] && n < 0) {
			result = -n - 1;
		}
`
错误表示 b == -1值 可是 又赋值给p 所以造成溢出
"
核心词典自定义词典同时出现时的词性优先级问题,"想请教一下关于优先级的问题，按照文档的描述自定义词典的优先级是全局最高的？但是在核心词典中存在某个词时自定义词典加入其他词性并不能识别出自定义词性，识别出的词性是核心词典中定义的概率最大的（只测试了标准分词的情况，同时开启了隐马尔科夫判断词性），此时如果把核心词典的面这个词条删掉就可以识别出自定义的词性。应该不是转移矩阵和频数的问题测试过各种的词性和频数，似乎都是核心词典优先。不知道是不是程序设定本身就是这样呢，如果是的话除了删除核心词典相关条目不知道有没有什么好的处理方法。或者还是隐马的作用范围只针对核心词典？
非常感谢！"
调用crf分词的时候出现ArrayIndexOutOfBoundsException错误,"
错误如下：
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:380)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:142)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at test.TegTest.main(TegTest.java:33)

找到是下面这句出错
String str = ""什么是谷精草""; 
List<Term> seg = segment.seg(str);

谷精草这个东西我通过自定义词典定义为其他的词语，此处不知道出现了什么错误，希望能够得到解答，谢谢"
关于如何分析句子且能够自定义的分类,"时间是很宝贵的，借用大家宝贵的时间帮我解一下疑惑，感激不尽。
-----------进入正题--------------
将要分析内容：B2B企业CMO的职责包括两块：一个是品牌，一个是销售线索。营销团队里边一定是有一群人是做品牌的，另一拨人做销售线索。简单说是一个部门是花钱的，另一个部门是去挣钱的，做品牌虽然不直接拿单，但能够给销售创造非常坚实的基础。
B2B企业有一个很大的优势，就是有着丰富并且明确的行业洞察，比如我们要做一个真正让世界变得更美好的机器人、自动化，得包含多少我们对这个行业的见解？但像卖碳酸饮料、卖巧克力的这些2C企业，就很难去表现这些。所以你看那些大的2B企业，都在不遗余力的去树立自己意见领袖的地位。例如西门子、IBM以及包括GE等在内。

希望通过上面是一篇文章的一部分，分析出以下结果：
1）相关企业：IBM,GE,西门子
2）相关行业：制造业
3）受众类别：2B
4）受众群体：管理人员、销售人员

个人思路：
我自己的思路是：里面出现了”IBM,GE,西门子“等词然后”西门子“又可以关联上制造业，文章中出现B2B字样，可以分析为2B，然后有CMO、营销团队可以对应上管理人员和销售人员。但是我不太明白如何将这些关联到不同的标签下。我的想法可能比较肤浅，还望指点。如果能够提供思路代码更好。
"
com.hankcs.hanlp.corpus.util.StringUtils有个bug,"com.hankcs.hanlp.corpus.util.StringUtils.PATTERN 应该为 ""&|[\uFE30-\uFFA0]|‘|’|“|”""
原来写错了"
solr6停止词,请问在solr6.4里面，是否支持中文停止词，我对默认的停止语字典做了修改后，在使用solr分析时，停止词无效。
hanlp.properties在maven项目路径的问题,"在eclipse中利用maven的pom.xml加入了hanlp
把hanlp.properties放在maven项目中的src下，通过maven命令编译项目，properties并没有自动复制到classpath中，每次都得手动把properties放到classpath，有没有什么解决办法那？？"
CRF分词模型 数字（m）和 英文字符（w）,看完您的CRF源码，有个疑惑，就是训练的时候，请问您是不是处理了一下，就是把连续的数字也换成了m进行训练，连续的英文字符换成w进行训练。
请问data\model\dependency\WordNature.txt.bin有介绍么？,"请问data\model\dependency\WordNature.txt.bin有介绍么？
我试用crf已存句法分析时，输入“”党参的定义是什么“”，此处“定义”的依存关系为“原处所”？
故而希望您能给出一个WordNaturedescription。谢谢！"
关于是否可以组建QQ社群的小建议,hankcs 你好，首先感谢你和你的团队以及好老板研发了HanLP并大公无私的开源。我是一个NLP的初学者（唯一的优势是java开发 经验有6年了），我通过百度找到由咱们这么一个工具，但是我有很多的疑惑，感觉没地方可以沟通，能否组织一个qq群，大家有问题可以问一些已经使用的比较熟练的高手们，然后大家都可以相互进步。也可以更好的发挥群众的力量。
"自定义字典报错java.lang.NumberFormatException: For input string: ""gi""","我把之前自定义词性为gi的字典扩充了一些内容进去，之后把缓存删掉重新启动就报了这个错误。。。。
三月 20, 2017 4:13:45 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.txt.bin (系统找不到指定的文件。)
三月 20, 2017 4:13:54 下午 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary loadDat
警告: 尝试载入缓存文件D:/data/dictionary/CoreNatureDictionary.ngram.txt.table.bin发生异常[java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.ngram.txt.table.bin (系统找不到指定的文件。)]，下面将载入源文件并自动缓存……
三月 20, 2017 4:13:58 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.io.FileNotFoundException: D:\data\dictionary\custom\CustomDictionary.txt.bin (系统找不到指定的文件。)
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典D:/data/dictionary/custom/tags.txt读取错误！java.lang.NumberFormatException: For input string: ""gi""
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：D:/data/dictionary/custom/tags.txt
导致加载这个字典失败！！！！！！！！！！！！这是为什么？？？？？？？？？？"
关于OrganizationDictionary 类中已经存在的角色规则组合,
姓氏总结，感觉没有全，应该是训练语料少引起的,
对词典加载的一些建议,
一些实体识别不准确的情况,"机构名前后的一些介词处理的不好，如：
南京的东南大学以前叫南京工学院
[南京/ns, 的东南大学/nt, 以前/f, 叫/vi, 南京工学院/nt]

在\data\dictionary\person\nr.txt中加了黄鹤楼 A 1还是没用
湖北最出名的烟是黄鹤楼
[湖北/ns, 最/d, 出名/a, 的/ude1, 烟/n, 是/vshi, 黄鹤楼/nr]

1998年3月1号是我的生日
[1998/m, 年/qt, 3/m, 月1号/nt, 是/vshi, 我/rr, 的/ude1, 生日/n]
"
王国强、高峰、汪洋、张朝阳光着头、韩寒、小四 ，这里面“张朝阳”这个人名分错了,"我在自定义词典中加入“张朝阳”这个人名，设定
张朝阳 nr 10000
照样分错
debug看了一下粗分网，里面没有""阳""字：
粗分结果[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张/q, 朝/tg, 阳光/n, 着/uzhe, 头/n, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]

我应该怎么做，才能把这个人名分对呢？"
修改一个提示问题,
用了下载的data词典(解压后1G)后分词效果反而变差了？？？,"# 没有使用data词典
< 问下你们有好一点的套餐吗，我现在这个套餐感觉流量不是很经用	问下#你们#有#好#一点#的#套餐#吗#，#我#现在#这个#套餐#感觉#流量#不是#很#经#用
< 问下你们有好一点的套餐吗，我现在这个套餐感觉流量不是很经用	问下#你们#有#好#一点#的#套餐#吗#，#我#现在#这个#套餐#感觉#流量#不是#很#经#用
< 问下办理国际漫游怎么办理	问下#办理#国际漫游#怎么#办理
< 问下宽带什么时候可以来人修，我下午有个远程会议	问下#宽带#什么#时候#可以#来人#修#，#我#下午#有#个#远程#会议

# 使用data数据词典
> 问下你们有好一点的套餐吗，我现在这个套餐感觉流量不是很经用	问#下#你们#有#好#一点#的#套餐#吗#，#我#现在#这个#套餐#感觉#流量#不#是#很#经#用
> 问下你们有好一点的套餐吗，我现在这个套餐感觉流量不是很经用	问#下#你们#有#好#一点#的#套餐#吗#，#我#现在#这个#套餐#感觉#流量#不#是#很#经#用
> 问下办理国际漫游怎么办理	问#下#办理#国际#漫游#怎么办#理
> 问下宽带什么时候可以来人修，我下午有个远程会议	问#下#宽带#什么#时候#可以#来人#修#，#我#下午#有#个#远程#会议"
CRF识别新词似乎不起效果,"我使用""你看过穆赫兰道吗""这句话做实验，没有得到结果。

然后我使用同样的一篇新闻做实验，在NLPIR上面可以得到新词“国际机场” ， “马移民局”等新词，但是用hanlp无法获得。

文章如下：

环球网报道 记者 王敏日前，网名为乔妹的台湾女游客通过脸谱发文表示，自己在入境马来西亚时，因护照破损，受到马来西亚海关非人道对待，她不但被没收了手机、护照等私人物品，还被关押长达35个小时。
“03/09晚上08：30我下飞机，我依照规定排队等候盖印出关，轮到我时，盖印人员他说我护照损坏，不能进去马来西亚，他要我等一下”
乔妹表示，他回来后询问她护照为何会损坏，她解释说，在日本购物免税单被日本海关撕下时造成的破损。海关人员随即将其带入办公室表示，马来西亚不接受并将护照没收。
乔妹随后被要求进入更里边的办公室，当她进入后，手机又被没收了。这时，乔妹再次被要求进入更后面的小房间里放置行李，在那里出现了一个很凶的男子。
“我把外套放在手提行李上，他就把我的外套很大力的丢向我，行李箱往上丢，还叫我把钱拿出来，我不想给他钱，假装听不懂，然后才有一个女生来，就叫我把钱放口袋”
乔妹随后手被铐起来带进了牢房，对方仅告诉她第二天早上可回台湾。“里面黑嘛嘛，有时才有开灯，厕所好脏也没有门，只能坐地上，累了睡地上，好多人”乔妹称她还遇到了另一个台湾人。
一个细节，乔妹表示，有马来西亚工作人员向其表示，只要1000马币便能先出去，但她却分明看到墙上密密麻麻的“不要给钱，都是骗钱”的字。
贴文发布后，在马来西亚，事件迅速热起，其后，台媒也相继跟进，做出了报道。马来西亚网站“辣手网”14日下午报道，马首相东亚特使兼民都鲁区国会议员张庆信认为，这件事关乎国家形象和官员素质，可能打击马来西亚旅游业。
张庆信表示，遭扣留的台湾旅客不是罪犯，不该用这么不人道的方式对待台湾旅客。因此向内政部反映。而据台“联合新闻网”报道称，马国副首相兼内政部长阿末扎希得知后大怒，已下令彻查。
台湾“外交部”得知消息后表示，已于第一时间主动掌握讯息，并经“驻处”洽系吉隆坡国际机场移民局执法组，转述贴文所述情节，了解当日相关情形。
而马来西亚则回复，该名女性于3月9日搭乘亚洲航空班机于晚间8时30分抵达吉隆坡第2国际机场，拟入境时，因所持护照毁损，被裁定拒绝入境并予以遣返，于是在3月11日上午9时20分搭乘亚洲航空第D7372号班机返回台湾桃园国际机场。
那么，马来西亚遣返的标准作业是什么呢，台“外交部”表示，根据了解，旅客经机场移民官裁定拒绝入境并予以遣返后，倘当日无法登机返回，旅客将被安置在机场之“照护室”，并尽速进行遣返。
旅客在留置期间相关情形，马移民局例不知会各相关驻马“使领馆”单位，若马移民局认为有必要将旅客移送至机场附近的“临时安置所”，予以较长时间的留置或调查时，则将同步知会。
事件同样很快便在社交网络上发酵，截止此文时，转发数达10280次。不少网友对乔妹的遭遇表示同情，但也有些网友质疑事件的真实性，甚至表示，乔妹并不是台湾人，所做只是为了炒作。
比如有网友即认为，有人因为要红而败坏，损人不利己的行为不一定高明，更何况是损害一个国家的名誉（马来西亚）。也有网民表示自己就是马来西亚人，要求出示护照。
就此，乔妹在脸谱表示，“大家一直说我想红，我想问这个很光荣吗？你们可以体会被关的心情吗？里面环境有多糟吗？”随后更是贴上护照证明自身。
乔妹的遭遇另一方面则招来了不少岛内外的共感，名叫“Kuan wei pan”的网友表示自己也有过相同的经验，并称“你永远不要让这里的人知道你的口袋有多深”。
也有不少马来西亚的网友为乔妹的遭遇感到抱歉，这位网友表示，自己作为一个马来西他人，都为这里的键盘侠感到心寒，这样的事情发生在谁的身上，都会崩溃的。
当然也有网友看的更多些，比如这位地址显示为台北的网友这样说，如果持有的是中华人民共和国的护照，被处理的过程就会不一样。"
python 词共现统计 Occurrence 如何停用 CustomDictionary,"和 DemoOccurrence 結果不同
同樣都是用
`在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法`

Demo 的 uniGram 結果
[信息=信息=1, 先进=先进=1, 图形图像=图形图像=1, 处理=处理=2, 技术=技术=1, 方面=方面=1, 比较=比较=1, 目前=目前=1, 算法=算法=2, 视频=视频=1, 计算机=计算机=1, 音视频=音视频=1]

我的結果
[先进=先进=1, 图形图像=图形图像=1, 处理=处理=2, 技术=技术=1, 比较=比较=1]

因為我的自訂詞典中

[在/p, 计算机/product, 音视频/product, 和/cc, 图形图像/nz, 技术/keyword, 等/udeng, 二/m, 维/b, 信息/product, 算法/product, 处理/keyword, 方面/product, 目前/product, 比较/keyword, 先进/a, 的/ude1, 视频/product, 处理/keyword, 算法/product]

CRF有停用自訂詞典的功能
`CRFSegment().enableCustomDictionary(false);`

我要如何在 使用 occurrence 時停用 CustomDictionary ?


-------------
另外，自訂字詞我是用 LexiconUtility.setAttribute 去加詞，因為我加的詞含有 `space`
"
隐马尔可夫词性标注概率转移矩阵问题,"你好，博主大人，这里在统计标签出现次数时是不是会重复计算呢？而且也无法保证了概率转移矩阵每行元素之和等于1呢？我感觉 total[j] += matrix[i][j];这句有重复计算的可能。
`// 需要统计一下每个标签出现的次数
            total = new int[ordinaryMax];
            for (int j = 0; j < ordinaryMax; ++j)
            {
                total[j] = 0;
                for (int i = 0; i < ordinaryMax; ++i)
                {
                    total[j] += matrix[i][j];
                    total[j] += matrix[j][i];
                }
            }`"
适配 redis 平台，读取字符串，字节数组和整型转换异常，出现负数情况,"ByteUtil.bytesHighFirstToInt(byte[] bytes, int start)
麻烦跟踪一下什么情况，谢谢！"
发现楼主还在更新，那我提交一个bug。 判断字符类型 这个函数有bug，我修正后的：," /**
     * 判断字符类型
     * @param str
     * @return
     */
    public static int charType(String str)
    {
        if (str != null && str.length() > 0)
        {
            if (""零○〇一二两三四五六七八九十廿百千万亿壹贰叁肆伍陆柒捌玖拾佰仟"".contains(str)) return CT_NUM;
            byte[] b;
            try
            {
                b = str.getBytes(""GBK"");
            }
            catch (UnsupportedEncodingException e)
            {
                b = str.getBytes();
                e.printStackTrace();
            }
            byte b1 = b[0];
            byte b2 = b.length > 1 ? b[1] : 0;
            int ub1 = getUnsigned(b1);
            int ub2 = getUnsigned(b2);
            if (ub1 < 128)
            {
            	if(b1>='A' && b1<='Z'){
            		return CT_LETTER;
            	}
            	if(b1>='a' && b1<='z'){
            		return CT_LETTER;
            	}
                if (' ' == b1 || '\t' == b1) return CT_OTHER;
                if ('\n' == b1 || '\r' == b1) return CT_DELIMITER;
                if (""*\""!,.?()[]{}_-+=/\\;:|"".indexOf((char) b1) != -1)
                    return CT_DELIMITER;
                if (""0123456789"".indexOf((char)b1) != -1)
                    return CT_NUM;
                return CT_SINGLE;
            }
            else if (ub1 == 162)
                return CT_INDEX;
            else if (ub1 == 163 && ub2 > 175 && ub2 < 186)
                return CT_NUM;
            else if (ub1 == 163
                    && (ub2 >= 193 && ub2 <= 218 || ub2 >= 225
                    && ub2 <= 250))
                return CT_LETTER;
            else if (ub1 == 161 || ub1 == 163)
                return CT_DELIMITER;
            else if (ub1 >= 176 && ub1 <= 247)
                return CT_CHINESE;

        }
        return CT_OTHER;
    }"
python 無法使用 HanLP.Config.ShowTermNature,"
- python 2.7 所顯示的 error 如下
AttributeError: type object 'com.hankcs.hanlp.HanLP' has no attribute 'Config'

其他問題
- CRFSegment 在.enableCustomDictionary(False) 所有的POStag 會變成 `/null`
在 DemoCRFSegment.java 中
把 30行的 ` HanLP.Config.ShowTermNature = false;    // 关闭词性显示` 註解後就會發現這個問題

- CRFSegment 在.enablePartOfSpeechTagging(False) 無效
我認為，要停用POSTag時，各個分詞器用自己的設定會比較直覺一些，在弄成 web API時，少動用全域設定會比較好
"
版本1.3.2CRFDependencyModelPath对应路径下并没有相应文件,已下载对的[data](https://pan.baidu.com/s/1pKUVNYF)包，并未发现data/model/dependency/CRFDependencyModelMini.txt ，是我下的不对？还是别的什么原因，能否告知一下。
那些不在CoreNatureDictionary.txt的中词会标注什么词性,"在查看人名词典nr.txt时。采用以下方式查找在nr.txt 但不在CoreNatureDictionary.txt中的词：
awk 'NR==FNR{a[$1]}NR>NFR{if(!($1 in a))print $0}' ../CoreNatureDictionary.txt nr.txt | wc -l
结果1901个词，这部分词该如何标注词性。人名识别中，词的角度有人名前缀，姓，名，后缀，与人名无关词等。如果这1901个词不在CoreNatureDictionary.txt, 那怎么会分出这些词来呢？ 如果分出来了，词性又是如何设置的呢？"
你的这个包充满了bug,"要不打包不行，要不效果不好，反正没什么好说的。都烦死了用这个包了。刚刚开始了跑这个异常：
`严重: 没有找到HanLP.properties，可能会导致找不到data` 本来是好好的，突然就这样了。怎么那么多问题。。。？？"
ES 集成 hanlp时报错。,"Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.seg.common.Vertex
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:68)
	at com.hankcs.lucene.HanLPTokenizer.incrementToken(HanLPTokenizer.java:76)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
"
Custom字典的增删问题,"1.  首先删除CustomDictionary.txt.bin文件, 并在CustomDictionary.txt文件增加""阿里巴巴"". 在这之前“阿里巴巴”被分成""阿里""，“巴巴”。
2.  这时分词能成功分出""阿里巴巴""
3.  删除CustomDictionary.txt.bin文件, 并在CustomDictionary.txt文件删除""阿里巴巴""
4.  这时分词依然能分词为""阿里巴巴""，  这是什么鬼？ 请测试验证"
用户自定义词典加载不了,"1、CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; MyDic.txt;
2、MyDic.txt：信息与计算科学 v 1
3、确保是uft-8
4、已删除缓存文件
5、LexiconUtility.getAttribute(""信息与计算科学"")
6、HanLP.Config.enableDebug()看不到有加载MyDic.txt
7、CustomDictionary.add(""信息与计算科学"")可以成功
谢谢！"
hanlp-portable 怎样通过properties 文件配置自定义词典呢。,
关于自定义字典识别问题,"我定义了IT技术相关字典，设置词性为：gi(计算机相关词汇)
javascript gi 1
java gi 1
spring gi 1
scala gi 1
python gi 1
...
我在对内容Java javascript html5css mybatis hibernate springmvc python scala进行分词的时候还是将默认词性识别成了：javajavascripthtml/nx, 5/m, cssmybatishibernatespringmvcpythonscala/nx（字母专词）
自定义字典地址没错，也清了缓存了
有遇到同样问题的吗"
在集成elasticsearch 2.4.4 时 报错，Could not initialize class com.hankcs.hanlp.seg.common.Vertex,"Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.seg.common.Vertex
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.hankcs.lucene.SegmentWrapper.next(SegmentWrapper.java:76)
	at com.hankcs.lucene.HanLPTokenizer.incrementToken(HanLPTokenizer.java:67)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:378)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
"
关于Project打包的问题,"我在项目里用了你的jia包（maven方式和直接下载jar的方式）我在IDE里运行都没问题，我是用Swing框架做了个图形化工具，然后打包完之后一旦运行就出问题，问题信息是：

> Error: A JNI error has occurred, please check your installation and try again
> Exception in thread ""main"" java.lang.SecurityException: Invalid signature file d
> igest for Manifest main attributes
>         at sun.security.util.SignatureFileVerifier.processImpl(Unknown Source)
>         at sun.security.util.SignatureFileVerifier.process(Unknown Source)
>         at java.util.jar.JarVerifier.processEntry(Unknown Source)
>         at java.util.jar.JarVerifier.update(Unknown Source)
>         at java.util.jar.JarFile.initializeVerifier(Unknown Source)
>         at java.util.jar.JarFile.getInputStream(Unknown Source)
>         at sun.misc.URLClassPath$JarLoader$2.getInputStream(Unknown Source)
>         at sun.misc.Resource.cachedInputStream(Unknown Source)
>         at sun.misc.Resource.getByteBuffer(Unknown Source)
>         at java.net.URLClassLoader.defineClass(Unknown Source)
>         at java.net.URLClassLoader.access$100(Unknown Source)
>         at java.net.URLClassLoader$1.run(Unknown Source)
>         at java.net.URLClassLoader$1.run(Unknown Source)
>         at java.security.AccessController.doPrivileged(Native Method)
>         at java.net.URLClassLoader.findClass(Unknown Source)
>         at java.lang.ClassLoader.loadClass(Unknown Source)
>         at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
>         at java.lang.ClassLoader.loadClass(Unknown Source)
>         at sun.launcher.LauncherHelper.checkAndLoadMain(Unknown Source)

我在网上查了很多种方法 都没办法，就是很多人说了在maven文件里加这些：

```

> <artifact>*:*</artifact>
> <excludes>
>         <exclude>META-INF/*.SF</exclude>
>          <exclude>META-INF/*.DSA</exclude>
>           <exclude>META-INF/*.RSA</exclude>
> </excludes>
```

可是无论怎么样都不行。请问这是什么问题，等待回复。。。"
关于Term的offset属性问题,请问怎么开启分词器的offset选项？
关于自定义字典的路径问题,"看hankcs给出的自定义字典的配置格式是这样的：
data/dictionary/custom/CustomDictionary.txt;CompanyName.txt;school.txt
但实际上这样配置却读不到，程序运行时直接找了根路径+CompanyName.txt文件，
改成：
data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CompanyName.txt;data/dictionary/custom/school.txt
这个样子的绝对路径就可以读到了，不知是哪里的错误，还是我理解有偏差，望指教"
中国人名识别命中率有点低,"我在网上找到sohu1.9G的语料，共110万篇文章。 然后分词，统计各词性的词。 发现人名好多是分错的。head部分结果如下：
都是	nr	50596	20170301
车内	nr	39683	20170301
都有	nr	18472	20170301
来看	nr	12741	20170301
都会	nr	11236	20170301
全车	nr	9696	20170301
刘翔	nr	8981	20170301
胡锦涛	nr	8511	20170301
才是	nr	8322	20170301
都将	nr	8162	20170301
都讯	nr	8150	20170301
苏宁	nr	8106	20170301
才能	nr	7878	20170301
都能	nr	7545	20170301
占比	nr	7511	20170301
都要	nr	7469	20170301
文中所	nr	7412	20170301
全系	nr	7187	20170301
景海鹏	nr	7184	20170301
房企	nr	7176	20170301
曾在	nr	6558	20170301
高出	nr	6360	20170301
孙杨	nr	6077	20170301
刘旺	nr	5987	20170301
安南	nr	5821	20170301
唯冠	nr	5678	20170301
令人	nr	5602	20170301
赛扬	nr	5477	20170301
才会	nr	5340	20170301
小微	nr	5263	20170301
杨幂	nr	5164	20170301
刚需	nr	5145	20170301
宣传	nr	4587	20170301"
NShortSegment分词 java.lang.OutOfMemoryError: GC overhead limit exceeded,"代码如下：
`NShortSegment segment=new NShortSegment().enableAllNamedEntityRecognize(true).enablePlaceRecognize(true);
						List<Term> terms=segment.seg(text);`

总是分到这段话时出来内存错误：
“第一部分马克思主义哲学原理　　复习总思路……………………………………………………………………1　　第一章马克思主义哲学是科学的世界观和方法论………………………3　　第二章世界的物质性和人的实践活动……………………………………15　　第三章世界的联系、发展及其规律………………………………………30　　第四章认识的本质和过程…………………………………………………51　　第五章人类社会的本质和基本结构………………………………………76　　第六章社会发展规律和历史创造者………………………………………97　　第七章社会发展和人的发展………………………………………………112　　第二部分马克思主义政治经济学原理　　复习总思路……………………………………………………………………125　　第一章导论…………………………………………………………………128　　第二章社会经济制度与经济运行的一般原理……………………………134　　第三章资本主义生产关系的实质及其发展阶段…………………………152　　第四章资本的运行…………………………………………………………183　　第五章社会主义生产关系的实质与经济制度……………………………199　　第六章社会主义市场经济体制和经济运行………………………………211　　第七章经济全球化与国际经济关系………………………………………224　　第三部分毛泽东思想概论　　复习总思路…………………………………………………………………231　　第一章毛泽东思想是马克思主义中国化的理论成果……………………234　　第二章新民主主义革命的总路线和基本纲领……………………………241　　第三章新民主主义革命的基本问题………………………………………259　　第四章社会主义改造的理论原则与经验总结……………………………276　　第五章社会主义若干重大理论问题的探索成果…………………………285　　第六章社会主义建设的方针政策…………………………………………296　　第七章掌握毛泽东思想的活的灵魂，坚持和发展毛泽东思想…………310　　第四部分邓小平理论和“三个代表”重要思想概论　　复习总思路…………………………………………………………………323　　第一章邓小平理论是当代中国的马克思主义……………………………327　　第二章“三个代表”重要思想是马克思主义中国化的最新理论成果…332　　第三章解放思想、实事求是、与时俱进…………………………………341　　第四章社会主义的本质和根本任务………………………………………348　　第五章社会主义初级阶段和党的基本路线、基本纲领…………………355　　第六章科学发展观和社会主义建设的发展战略…………………………364　　第七章中国特色社会主义经济……………………………………………389　　第八章中国特色社会主义政治……………………………………………402　　第九章中国特色社会主义文化……………………………………………415　　第十章“一国两制”和实现祖国的完全统一……………………………424　　第十一章维护世界和平，促进共同发展…………………………………430　　第十二章中国特色社会主义事业的依靠力量和领导核心………………437　　第五部分当代世界经济与政治　　复习总思路…………………………………………………………………446　　第一章当代世界经济的发展变化与基本趋势……………………………449　　第二章当代世界政治的发展变化与基本趋势……………………………462　　第三章当今时代主题与建立国际新秩序…………………………………479　　第四章战后发达资本主义国家的经济与政治……………………………487　　第五章战后发展中国家的政治与经济……………………………………502　　第六章战后社会主义国家的经济与政治…………………………………210　　第七章独联体成员国与冷战后东欧国家的经济与政治…………………518　　第八章中国对外关系及在世界上的地位与作用…………………………526更多信息请访问：新浪考研频道考研论坛考研博客圈　　特别说明：由于各方面情况的不断调整与变化，新浪网所提供的所有考试信息仅供参考，敬请考生以权威部门公布的正式信息为准。”

"
HanLP.properties文件需要自己写吗？,是自己创建添加root 配置 还有其他的配置吗？ 找不到参考文件
elasticsearch-analysis-hanlp添加自定义停用词影响了正常词汇的分词效果,"我看这个插件，用的好像是hanlp的维特比算法进行分词的？
我修改了停用词库，删除了默认文件内容，添加了自己定义的几个词，为啥原来能分的词（不包含自定义停用词）加完这个之后就分不了了呢"
如何过滤停用词,看到了停用词词典，但是不知道如何过滤停用词呢？
停词,"请问，我在模板里找不到停词的例子
如何使用停词功能呢"
动态加载词库,"大神你好，请问一下，这个可以支持    提供远程加载配置，设置监控，词典更新自动加载，无需重启。

类似这种做法： https://github.com/medcl/elasticsearch-analysis-ik/pull/40   这个可以在这边实现吗？

这样就不需要每次更新词典，又要重新发布到生产环境了。 "
某个句子分词有误,"测试了句子：对敌人很有用  和 对敌方很有用。
前者分词错误，后者正确。
前者被分成了  “对敌  人  很 有用”；后者分成了 “对  敌方 很  有用”  

另：“有用”为自定义词典添加的，用的CoreNatureDictionary.mini.txt，请问下哪里有 非mini版本的词库下载？

感谢作者。"
"新字詞中含有""space""","我在自定義辭典中 
data/dictionary/custom/myDic.txt 
新增字詞
`紅米note 4x  nm`

但在建立 CustomDictionary.txt.bin 時 系統報錯
是因為格式必須為
[新字詞][tab][POS tagging]
新字詞中不得有 space?

如果是這樣的話
""紅米note 4x"", ""mackbook pro""
該如何加入為 新字詞?
"
demo演示结果和实际运行结果不同,"demo某段结果为：
[欢迎/v, 新/a, 老/a, 师生/n, 前来/vi, 就餐/vi]
[工信处/nt, 女干事/n, 每月/r, 经过/p, 下属/v, 科室/n, 都/d, 要/v, 亲口/d, 交代/v, 24/m, 口/n, 交换机/n, 等/udeng, 技术性/n, 器件/n, 的/ude1, 安装/v, 工作/vn]
[随着/p, 页游/nz, 兴起/v, 到/v, 现在/t, 的/ude1, 页游/nz, 繁盛/a, ，/w, 依赖于/v, 存档/vi, 进行/vn, 逻辑/n, 判断/v, 的/ude1, 设计/vn, 减少/v, 了/ule, ，/w, 但/c, 这/rzv, 块/q, 也/d, 不能/v, 完全/ad, 忽略/v, 掉/v, 。/w]
[中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]
实际运行结果为：
[欢迎/v, 新/a, 老师/n, 生前/t, 来/v, 就餐/v]
[工信处/n, 女/b, 干事/n, 每月/r, 经过/p, 下属/v, 科室/n, 都要/nr, 亲口/d, 交代/v, 24/m, 口/q, 交换机/n, 等/u, 技术性/n, 器件/n, 的/uj, 安装/v, 工作/vn]
[随着/p, 页/q, 游兴/n, 起/v, 到/v, 现在/t, 的/uj, 页游/nz, 繁盛/an, ，/w, 依赖于/v, 存档/vn, 进行/v, 逻辑/n, 判断/v, 的/uj, 设计/vn, 减少/v, 了/ul, ，/w, 但/c, 这块/r, 也/d, 不能/v, 完全/ad, 忽略/v, 掉/v, 。/w]
[中国科学院/n, 计算/v, 技术/n, 研究所/n, 的/uj, 宗成庆/nr, 教授/n, 正在/d, 教授/n, 自然/d, 语言/n, 处理/v, 课程/n]"
自定义词典清空,我这有个词库是在数据库上，有人一直在维护增删词，我现在是通过com.hankcs.hanlp.dictionary.CustomDictionary#add 方法动态加载的，现在有个需求是刷新重新加载这个词典，请问有什么方法吗？是不是只要CustomDictionary.loadMainDictionary 这个方法就可以重载这个自定义词典？
机构名识别错误,"您好！请教一个HanLP分词的问题。""为广大运维者所喜爱"",对于这句话的“所”，应该是被动的意思，""运维者所""被识别成了机构名。请教下这个东西怎么优化好?
补充：这里HanLP经过核心词典和用户词典初分词后，分成了运维/者/所三个词，但是在机构名识别的过程中，机构名标注是不怎么管初分词性和前后语义关系的，只要符合机构名的模式匹配，就会不管三七二十一归为机构名。本人通过源代码理解这里是一个缺陷，不知道有没有理解错误。如果没有理解错误，有没有什么好的解决办法，因为无论是人名、地名、机构名识别都好，都是类似的缺陷，对一些文本尤其是特定领域的文本识别准确率是很低的。望赐教，谢谢！"
关于demo,可不可以提供更多关于train和evaluate的demo？这样用户可以根据自己的语料进行训练。wiki中给出的demo不够系统。
关于名称识别,@现在根据分词使用的方式，根据组词的方式组词后进行对名称的判别，这样就存在一个局限性。就拿我现在在一个文本中是识别未在词库中添加那组‘企业名称‘的词对企业名称识别时，很容出现识别出一半甚至不能识别，如果是一直通过添加词来解决这也会导致文件越来越大。想请问一下这个有什么好的建议或者解决方法，来补足这个缺陷。
词典分割符（空格）对于英文实体的问题,"例如organization.txt的第一条数据
Valor Capital Group	nhy	1
英文实体内存在空格，从而导致词典读取错误
建议修改方式:
１、（强烈推荐）解析词典方式：取倒数第一个为词频、倒数第二个为词性、其它为词
２、将词的分隔符设置为零宽字符等几乎不可能在实际文本中出现的词"
自定义词的词形分词后出现错误,"北京拉手网络技术有限公司 ntc 1  词典标注ntc

代码打印也是ntc 1        System.out.println(LexiconUtility.getAttribute(""北京拉手网络技术有限公司""));

但是在一个简历文本里面做了分词， 虽然识别出了这个实体，但是词性变成了nt。。
 
""工作描述：\t工作职责\n"" +
                ""负责拉手网后台系统产品（PC端和无线端）,包括：待办事项、任务管理、供应商管理、合同管理、团购单管理、变更与特批、客服中心、数据中心\n"" +"
ES使用portal默认词典，添加自定义停用词不起作用,求问大神如何解决呢
如何在文本中提取识别的地址，公司名称，邮箱等。提取方法有哪些？,"![image](https://cloud.githubusercontent.com/assets/19337606/23128473/0a8952de-f7ba-11e6-8640-e72ff313db35.png)
"
add nGram distance function between strings,"fork you code,change jdk1.8 ,do some code optimize,want to join the project.
the function is in [getNGRAMDistance in EditDistance.java ](https://github.com/DusonWang/HanLP/blob/master/src/main/java/com/hankcs/hanlp/algoritm/EditDistance.java) "
加载用户词典时出现数组溢出错误，我加载了自己的路径为root=D:/LearnHanLP/data/ CustomDictionaryPath=dictionary/custom/CustomDictionary.txt; economic.txt; stork.txt; ,"二月 13, 2017 5:38:41 下午 com.hankcs.hanlp.HanLP$Config$1 getProperty
警告: root=D:/LearnHanLP/data/ 这个目录下没有data
二月 13, 2017 5:38:41 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/LearnHanLP/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.io.FileNotFoundException: D:\LearnHanLP\data\dictionary\custom\CustomDictionary.txt.bin (系统找不到指定的文件。)
二月 13, 2017 5:38:41 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:288)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:55)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	at com.nlp.ner.Ner.main(Ner.java:29)"
很多人名nr被标记状态词zg，这是为什么呢,"比如
[按照/dg, 习近平/zg, 指示/yg, 和/pbei, 李克强/zg, 要求/yg, ，/xx, 外交部/ntcb, 和/pbei, 我国/yg, 驻/s, 马来西亚/nr2, 使领馆/yg, 加强/s, 同/dg

我看了下训练字典里面，习近平和李克强都是nr"
请问作者如何生成自己的stopwords.txt.bin？,"由于工作需要，我要往停用词表data\dictionary\stopwords.txt中添加自定义的词，添加后，删除了原先的stopwords.txt.bin后，启动项目，发现不能像用户自定义词典那样自动生成新的stopwords.txt.bin。

而且，把stopwords.txt.bin删除掉后，原则上停用词应该是能被分出来的对吧，但分词后，发现停用词仍然是起作用的，即仍然是分不出来的。

我想问：
1. 如何生成自己定义的停用词bin文件：stopwords.txt.bin
2. 为什么删除掉stopwords.txt.bin后，停用词依然起作用。

谢谢作者解答。"
Integrate HanLP to KNIME Error,"    在下试着于 Knime 的 Java Snippet Node 使用 HanLP, 但发现跑到
sentenceList = HanLP.extractSummary(document, 3);
就造成 Knime ""闪退"", 是否我程式哪裡写错?
/************************************************************************************************************/
// Your custom imports:
import java.util.List;
import com.hankcs.hanlp.HanLP;

// Your custom variables:
String document;
List<String> sentenceList;

// Enter your code here:		
document = ""算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。\n"" +
        ""算法可以宽泛的分为三类，\n"" +
        ""一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。\n"" +
        ""二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。\n"" +
        ""三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。"";
// below error ! 造成 Knime ""闪退""
sentenceList = HanLP.extractSummary(document, 3);
/************************************************************************************************************/

PS : I add ‘hanlp-1.3.2.jar’ to “Additional Libraries” at Node, not “Knime\plugins” subdirectory.
 
谢谢您"
Mac上pycharm在程序中第二次startjvm报错Unable to start JVM at native/common/jp_env.cpp:78,"<img width=""812"" alt=""2017-02-09 6 42 30"" src=""https://cloud.githubusercontent.com/assets/6072224/22779906/922f6c94-eef7-11e6-80e1-6d353a245f78.png"">
我做的是网页上用户每一次提交输入数据后都进行一次分词，第一次start进行分词没有问题 。之后就报错，使用isstart进行判断发现JVM没有shutdown  貌似是只有python程序结束这个JVM才会完全关闭。希望大大解答！！怎么办"
‘无锡金鑫集团’ 这个词怎么搞都分不对,"使用索引分词，预期结果是：无锡，无锡金，锡金，金鑫，集团。
得到的结果是：无，锡金，鑫，集团
该咋整？"
关于词的重叠无法被识别的问题,"例如：石/ng, 台东/ns, 柱/ng, 食品/n, 有限公司
由于“台东“已经被标记成ns，“石台“也被标记成ns，“东柱“也被标记成了ns 最后显示的结果，就是例子中显示的那样，怎么去修改才能让它变成  石台/ns 东柱/ns 食品/n 有限公司呢，在不删除 “台东“被标记成ns的情况下"
动态添加词无效,"我尝试用使用  CustomDictionary.add(""厚底"") 动态添加词，以便使“厚底情侣白鞋”能正确的进行分词，但是结果是 “厚底” 并没有生效。

```java
CustomDictionary.add(""厚底"");
CustomDictionary.add(""白鞋"");
System.out.println(HanLP.segment(""厚底""));
System.out.println(HanLP.segment(""厚底情侣白鞋""));
```

输出：
[厚底/nz]
[厚/a, 底情/n, 侣/n, 白鞋/nz]

然后我看你readme里面有写“在基于层叠隐马模型的最短路分词中，并不保证自定义词典中的词一定被切分出来。如果你认为这个词绝对应该切分出来，那么请将词频设大一些”，所以我尝试了将词频调大，但是不起作用，这个应该怎么处理？
"
运行时出现文件读取问题 这个com.hankcs.hanlp.corpus.io.IOUtil包运行出错,"![1](https://cloud.githubusercontent.com/assets/19542871/22288090/570e707e-e330-11e6-9def-0bfb92f50d1d.PNG)
![2](https://cloud.githubusercontent.com/assets/19542871/22288091/570f1614-e330-11e6-8a65-0373bd1c1554.PNG)
![3](https://cloud.githubusercontent.com/assets/19542871/22288092/5711489e-e330-11e6-989d-79e2419b39bf.PNG)

我指定的root是在C盘eclipse的workplace路径下，并且我的D盘并没有 Doc\语料库 这个文件夹，所以特来请求大神，帮我看一下是什么原因？？？拜托了"
NLPTokenizer.segment 多线程异常,"java.lang.NullPointerException
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.getState(Trie.java:209)
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.parseText(Trie.java:140)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3749)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:37)

hanlp-1.3.1 在多线程情况下，出现空指针异常。
NLPTokenizer.segment应该不是线程安全的吧，有没有线程安全的替代方法？"
CRF分词不能用,"我当用CRF分词器的时候Java会抛异常：

> ```
> 读取data/model/segment/CRFSegmentModel.txt.bin时发生异常java.lang.NullPointerException
> 在读取过程中发生错误java.lang.NullPointerException
> CRF分词模型加载 data/model/segment/CRFSegmentModel.txt 失败，耗时 55 ms
> java.io.FileNotFoundException: data\model\segment\CRFSegmentModel.txt (系统找不到指定的路径。)
> ```

请问这是怎么回事呢，怎么能解决。谢谢！！"
CRF model bin文件可以怎么生成对应的txt文件吗,"想在C++环境里用hanlp里的CRF分词模型，但不知道怎么解析。
如果能转换为对应的txt文件，就可以使用CRF++来转换读取了
如果不能生成为txt文件的话， @hankcs 可以分享一份吗，谢谢！"
名字识别的问题,"你好，
最近发现hanlp对名字的识别有时候会有些偏差，比如“全季吧”被识别成了nr。
看nr.txt里的内容没有看懂，请问有什么办法可以解决这个问题吗？
多谢！"
用户自定义词典不起作用，谢谢解答！,"1、CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; define.txt
2、define.txt：信息与计算科学 v 1
3、确保是uft-8
4、已删除缓存文件
谢谢！"
HanLP可以情感分析吗？,"您好！
       HanLP可以进行情感分析吗（正面、负面、中性）？能否提供一个思路。谢谢！"
一首日语歌曲分词错误,"一首日语歌曲，分词成下面的结果，如何修改字典表，才可以分词正确？
一/m, 首日/t, 语/ng, 歌曲/n"
一首日语歌曲分词错误，,"一首日语歌曲，分词成下面的结果，如何修改字典表，才可以分词正确？
一/m, 首日/t, 语/ng, 歌曲/n"
利用hanlp标注词性,如何利用hanlp标注（已经分好词）的词性？
用户意图分析,"有这样的一段话""hello,hello,hello 我要去徐家汇"" 采用什么样的模型可以分析出""我要去徐家汇""的意图了？ hankcs能否帮助看一下，多谢"
关于hanlp.properties中定义的data路径问题,"最近一直在使用hanlp分词工具，发现hanlp.properties在data/dictionary路径的定义上并不灵活，root只能定死绝对路径。在将程序打包放到spark集群上运行，不可行。尝试改IOUtils的   
public static InputStream newInputStream(String path) throws IOException
    {
        if (IOAdapter == null) return new FileInputStream(path);
        return IOAdapter.open(path);
    }
通过classloader的getresourceasStream方式读取，发现之后能实现将data放入到classpath下通过相对路径读取，但是牵一发动全身，很多地方都有异常，希望hancks大神有时间能解决下hanlp.properties只能通过绝对路径读入data的问题"
Hanlp的提取摘要的方法,"在Hanlp给出的例子中，main方法里面使用HanLP.extractPhrase(document,5)，这个方法都很正常，但是在其他的地方方法调用这个方法的时候就不成功"
hankcs，多模式字符串匹配由于模式不是特别多，我觉得不需要使用自动机，坏字符跳转预处理后，使用一个hashmap就可以了,
BM25文本相似度为负,在用textrank做关键句提取时发现，BM25计算的相似度结果部分为负。连句子与本身的相似度都是负的。应该是idf公式的分母多减了一个文档频率
jdk 1.7正常分词，jdk1.8分词数组越界,"jar版本1.3.2，我看其他人的issue 好像很多人也有这个问题
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 149
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:380)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:574)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:40)
	at test.main(test.java:8)
"
关于词的标注,除了在nt.txt中直接标记成C之外，还有没有其他的标注方式是这个词性变成C的 比如一个词 我需要怎么标注什么就能在使用的时候 让它变成角色 C
用户自定义词典,"请问如果在hanlp.properties文件中配置了用户自定义词典

CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 流行新歌词汇.txt; 电影大全词典.txt;　现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 
之后
 val content=""我喜欢看小岛惊魂，阿甘正传，致命ID，禁闭岛,小叮当与海盗仙子，肖申克的救赎.我喜欢听阿波狂想曲""
　println(StandardTokenizer.segment(content))

是优先去根据用户自定义的词典进行分词吗，如果我希望先标准分词，之后再用户自定义词典分词应该怎样操作呢？
有没有可以显示手动关闭用户自定义词典分词的功能，之后要用的时候再启用的函数呢？"
关于关键词提取问题，单个字的名词好像被过滤掉了,"你好 !我发现一个问题不知道是不是您故意为之。在提取关键词的时候单个字的名词 被过滤掉了吧，例如一篇关于时的文章 竟然没有时这个关键词，
我看了下你的源码 版本 1.3.1 com.hankcs.hanlp.summary.TextRankKeyword.class 类中的方法shouldInclude
/**
* 是否应当将这个term纳入计算，词性属于名词、动词、副词、形容词
*
* @param term
* @return 是否应当
*/
public boolean shouldInclude(Term term){
...
default:
{
if (term.word.trim().length() > 1 && !CoreStopWordDictionary.contains(term.word))
{
return true;
}
} //这句话是不是有点不妥？ 希望能及时收到回复！我的邮箱 515504936@qq.com
...
return false;
}"
繁简体转换：德國漢堡转换成了德国汉堡包,德國漢堡转换成了德国汉堡包。不知道hanlp源码是否可以编译。
采用自定义字典不同场景下的词性标注问题,"自定义song.txt 我在Nature里面定义了一种新的类型为song与之相对应，采用下面的维特比算法进行分词。
 HanLP.newSegment()enablePartOfSpeechTagging(true)
对于“后来”
  如何实现 后来的故事 以及刘若英的后来  中的“后来”两种不同的场景的标注。 前者标注为t，后者标注为song.

是否可以在转移矩阵中实现？ 我试了一下，感觉还是很繁琐的，而且也没有得到正确的结果。能否协助看一下呢？
"
自定义词典不起作用,"1、在data\dictionary\custom目录下新建自定义词典my.txt，内容如下：
      风中有朵雨做的云
      一千个伤心的理由
      忘情水
2、在hanlp.properties中增加my.txt
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; my.txt ns; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt;data/dictionary/person/nrf.txt nrf
3、测试：
   String content = ""张学友的一千个伤心的理由是一首好听的歌"";
   Segment seg = HanLP.newSegment().enableCustomDictionary(true);
   List<Term> termList = seg.seg(content);
   System.out.println(termList);
   List<String> keywordList = HanLP.extractKeyword(content, 5);
   System.out.println(keywordList);

   输出结果如下：
[张学友/nr, 的/ude1, 一千个/nz, 伤心/a, 的/ude1, 理由/n, 是/vshi, 一/m, 首/q, 好听/a, 的/ude1, 歌/n]
[张学友, 理由, 伤心, 一千个, 好听]

4、结论：
     自定义词典没起到作用
     （如果在分词之前执行了CustomDictionary.add(""一千个伤心的理由"");就能起作用）


请问我的用法有问题吗？
"
核心同义词词典中一个词有多个意思的时候，只返回了一个,"`CoreSynonymDictionary.get(""上海"")` 上海应该是在CoreSynonym.txt多处存在的。但是只会返回一个"
solr拼音分词插件问题,"`public class HanlpPinYinTokenFilter extends TokenFilter{

	private static final Logger logger = LoggerFactory.getLogger(HanlpPinYinTokenFilter.class);
	
	private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
	
	public HanlpPinYinTokenFilter(TokenStream input) {
		super(input);
	}

	@Override
	public boolean incrementToken() throws IOException {
		
		if(termAtt.length() > 0) {
			String pinYin = HanLP.convertToPinyinString(termAtt.toString(), """", false);
			termAtt.setEmpty();
			termAtt.append(pinYin);
			return false;
		}
		return true;
}`
想通过你的分词器做一个拼音分词，但是每次获取到仍旧是中文结果，但是打印termAtt的结果确实是pinyin的，这个就不太明白了，能帮忙看下么？ 谢谢了"
关于动态添加字典的问题,"hankcs，您好，非常感谢您为大家提供了如此方便的NLP工具。我现在遇到的问题是在分词之前可以获取到当前文档的很多实体缩写名称。然后想通过CustomDictionary.insert()方法动态添加字典增加识别的精确度。但是很多添加的字典没法准确识别，不知道是因为什么？是不是我使用的方法有错误？

@Test
	public void dictionaryTest() {
		Segment segment = HanLP.newSegment().enableCustomDictionary(true)
				.enableOrganizationRecognize(true);
		String str = ""现在总统数码港、自由闲、北海高岭、总统大酒店四家签订了协议"";
		List<Term> termList = segment.seg(str);
		CustomDictionary.insert(""总统数码港"", ""nt 1024"");
		CustomDictionary.insert(""自由闲"", ""nt 1024"");
		CustomDictionary.insert(""北海高岭"", ""nt 1024"");
		CustomDictionary.insert(""总统大酒店"", ""nt 1024"");
		System.out.println(termList);
	}
结果是这样的
[现在/t, 总统/nnt, 数码港/nz, 、/w, 自由/a, 闲/v, 、/w, 北海/ns, 高/a, 岭/ng, 、/w, 总统/nnt, 大酒店四家/nt, 签订/v, 了/ule, 协议/n]
"
日志问题,建议大大将日志改为slf4j日志组件，使用起来方便很多
CoreStopWordDictionary里面 FILTER 过滤器实际表现与注释不符,"注释为: ""核心停用词典的核心过滤器，词性属于名词、动词、副词、形容词，并且不在停用词表中才不会被过滤"" 但实际上, 就算是满足条件的名词/动词 只要长度为1, 也会被干掉. 对应 代码:
line 94: `if (term.word.length() > 1 && !CoreStopWordDictionary.contains(term.word))`"
数据包下载问题：data-for-1.3.0.zip,"啊哦，你所访问的页面不存在了。
可能的原因：
1.在地址栏中输入了错误的地址。
2.你点击的某个链接已过期。"
检查是否建立了failure表 ,"请问一下 在Trie文件中
![image](https://cloud.githubusercontent.com/assets/20895017/20953814/0fc56f82-bc71-11e6-86dd-58020b95298a.png)
这个表示在什么情况建立的  failure表指的又是又是什么"
如何合并（连接）两个term ？,"```java
        for(int i = 0; i < termList.size()-1; i++){
        	System.out.println(termList.get(i)+termList.get(i+1));
        }
```
这样不可以哦，应该如何连接？Thanks."
初始化分词器时，字符类型对应表加载失败（CharType.dat.yes）,"大神好，这个异常不是每一次都出现（偶尔出现）

下面我在eclipse上测试时的输出的信息

**********************************************************************************

十二月 06, 2016 1:59:13 下午 com.hankcs.hanlp.HanLP$Config <clinit>
严重: 没有找到HanLP.properties，可能会导致找不到data
========Tips========
请将HanLP.properties放在下列目录：
D:\workspace\****\target\classes
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data
现在HanLP将尝试从D:\workspace\****读取data……
十二月 06, 2016 1:59:13 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.nio.channels.ClosedByInterruptException
十二月 06, 2016 1:59:25 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取data/dictionary/other/CharType.dat.yes时发生异常java.nio.channels.ClosedByInterruptException
字符类型对应表加载失败：data/dictionary/other/CharType.dat.yes

****************************************************************************************

然后程序到这里就结束了

对应目录 文件是存在的 【data/dictionary/other/CharType.dat.yes】"
分词后Nature如何得到完整词性,"在分词后，统计各个词性下出现的词，以w为例，得到如下结果：
w

,，,）,（,。,、

按照http://www.hankcs.com/nlp/part-of-speech-tagging.html说明，像顿号、应该为：
wn
          顿号，全角：、
如何得到wn?

分词代码：
HanLP.segment(msg)"
Viterbi 算法中求解最大概率都变成加法，非乘法,com.hankcs.hanlp.algoritm 下的Viterbi算法求解最大概率都变成加法，这是为何？`V[0][y] = start_p[y] + emit_p[y][obs[0]];`
WordNet数组越界异常,"大神好 我从git上clone了今天的版本，昨天那多线程异常没出现
今天抛出一个更重大的Exception

java.lang.ArrayIndexOutOfBoundsException: 43
	at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:214)
	at com.hankcs.hanlp.seg.common.WordNet.insert(WordNet.java:166)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3777)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3754)
	at com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie.parseText(AhoCorasickDoubleArrayTrie.java:101)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3753)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:99)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)


执行代码：

public static void main(String[] args) {
		
		String str=""受约束，需要遵守心理学会所定的道德原则，所需要时须说明该实验与所能得到的知识的关系"";
		
		Segment defaultSegment = StandardTokenizer.SEGMENT
				.enablePartOfSpeechTagging(true).enableOffset(true)
				.enableNameRecognize(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true);
		
		try {
			List<Term> list = defaultSegment.seg(str);
		} catch (Exception e) {
			// TODO: handle exception
			
			e.printStackTrace();
			LOGGER.error(""分词异常"",e);
		}
		
		
	}
"
Trie抛出nullpoint异常," 大神好，我调用hanlp分词时， 抛出如下异常：

java.lang.NullPointerException
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.getState(Trie.java:212)
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.parseText(Trie.java:140)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3749)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	
【测试时，如果不开启命名实体识别功能，则正常运行】

我测试时，打印出来是下面的地方 currentState为null


/**
     * 跳转到下一个状态
     *
     * @param currentState 当前状态
     * @param character    接受字符
     * @return 跳转结果
     */
    private static State getState(State currentState, Character character)
    {
    	
        State newCurrentState = currentState.nextState(character);  // 先按success跳转
        while (newCurrentState == null) // 跳转失败的话，按failure跳转
        {
        	
            currentState = currentState.failure();

      **//这里我打印出 currentState为nullpoint**
           
            newCurrentState = currentState.nextState(character);
        }
       
        return newCurrentState;
    }


我的调用方式这这样的：

public HanlpTokenizer(String text) {
		//tokenizer = new StringTokenizer(tokens);
		//System.out.println(""当前文本:""+text);
		
		//System.out.println(""构造函数"");
		tokens = new ArrayList<String>();
		
		
		Segment defaultSegment = StandardTokenizer.SEGMENT
				.enablePartOfSpeechTagging(true).enableOffset(true)
				.enableNameRecognize(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true);
		
		if(text!=null&&text.trim().length()>0)
		{
			//System.out.println(text);
			//List<Term> list=null;
			try {
				//System.out.println(""输入：""+text);
				List<Term> list = defaultSegment.seg(text);
				//System.out.println(list);							
				
				
			} catch (Exception e) {
				// TODO: handle exception
				e.printStackTrace();
				System.out.println(text);
				
				//System.out.println(e);
			}
		}		
		
	}



最外层是在多线程里面调用的 谢谢"
请教日期实体词提取的问题,"我在二元继续词典里定义了未##数@年、未##数@月、未##数@日等
也清除了词典的缓存
但实际分词时2007年1月11日还是会被分成2007/m, 年/qt, 1/m, 月/n, 11/m, 日/b
如何能合并成“2007年”、“1月”、“11日”甚至是“2007年1月11日”呢？
请指教。"
关于分词的一个问题,在做一个爬虫的项目，用到Hanlp 分词，遇到一些问题。比如说：我的关键词设为“长安人民政府”，Hanlp分词会分出“长安”这个词来，然后爬虫去抓的时候，会把一些人名也抓进来，多出很多垃圾信息。这样该怎么处理？
索引分词粒度能配置吗？,具体问题：比如 “张常宁在常宁打球”  怎么样不对张常宁这个人名继续分割
关于角色标注的问题,"#我标注一个词 比如：丹芭碧  nr  16  之后会在程序中显示 丹芭碧 F 8290 B 769 A 266 X 6
现在我想通过同样的方式  怎么把  丹芭碧  这个词标注成C 呢  除了在nt.txt 直接标记成C之外，还有其他的方式让这个词变成 C吗？ "
在对简体中文调用HanLP.convertToSimplifiedChinese出现的一些bug,"因为在使用中我们不知道文章是否简体或者繁体，所以我们一概使用简体转化，在处理如下词汇时候问题严重

> '民乐', '奶油', '战列舰', '房价', '标致', '沃尓沃', '富豪', '大众', '马自达',
> '克拉', '厄瓜多', '塔吉克', '安的列', '尼日', '洛哈', '漢堡', '艾森豪', '裁判',
> '福斯', '芝士', '起司', '房屋', '忌廉', '平治',
> '空中客车', '笑星', '谐星', '奔驰', '零钱', '老年痴呆症'

最危险的是把“习近平治党”搞成了“习近奔驰党”，在天朝，这可是杀头的罪

```
In [5]: HanLP.convertToSimplifiedChinese('習近平治黨')
Out[5]: '习近奔驰党'

```"
HanLP.convertToSimplifiedChinese('奔驰')　=> 賓士,好明显的bug
使用 python 调用 Hanlp.parseDependency 时报错,"RT：
[按照http://www.hankcs.com/nlp/python-calls-hanlp.html](http://www.hankcs.com/nlp/python-calls-hanlp.html) 例子进行的测试
报错的log`java.lang.NullPointerExceptionPyRaisable: java.lang.NullPointerException`
求助~"
nr词典格式问题,"你好！我用的是1.3.1的程序和1.3.0的data。在分词时会有一些词被错误的标成了人名，比如考考你，都喜欢。按照您的说明，我把这些词加上 A 1添加在nr.txt的后面，但是运行时就出现错误了，提示是：十一月 24, 2016 10:27:04 上午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/data-for-1.3.0/data/dictionary/person/nr.txt.value.dat时发生异常java.io.FileNotFoundException: D:\data\data-for-1.3.0\data\dictionary\person\nr.txt.value.dat (系统找不到指定的文件。)
十一月 24, 2016 10:27:04 上午 com.hankcs.hanlp.dictionary.nr.NRDictionary onLoadValue
严重: 读取D:/data/data-for-1.3.0/data/dictionary/person/nr.txt失败[java.lang.NumberFormatException: For input string: ""1阿哈""]
该词典这一行格式不对：欧对 A 1阿哈 A 1考考你 A 1微盘 A 1布用谢 A 1南阳 A 1都喜欢 A 1
人名词典加载失败：D:/data/data-for-1.3.0/data/dictionary/person/nr.txt
请问这是什么原因？"
hanlp写的python函数，用flask做服务调用时，出现问题,
我发现有很多应该是名词的词被归为Nature.b区别词导致去停用词的时候被去掉,"比如：保安，机电工程师
使用List<Term> list = HanLP.segment(text);分词后为：[保安/b, ，/w, 机电/b, 工程师/nnt]
去停用词CoreStopWordDictionary.apply(list);后的结果  工程师/nnt
我发现CoreStopWordDictionary中有对                
                case 'm':
                case 'b':
                case 'c':
                case 'e':
                case 'o':
                case 'p':
                case 'q':
                case 'u':
                case 'y':
                case 'z':
                case 'r':
                case 'w':
词性开头的词都过滤了，我想问保安和机电为何会被为认为是b 区别词？"
使用HMM-Viterbi（标准分词）模式无法获取词性,"版本号：hanlp-1.3.1
方式一：使用HanLP静态分词器可以获取词性
String text = ""乐视超级手机能否承载贾布斯的生态梦"";
Segment segment = HanLP.newSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(text);
System.err.println(termList);

输出结果：[乐/a, 视/vg, 超级/b, 手机/n, 能否/v, 承载/v, 贾/nz, 布斯/nrf, 的/ude1, 生态/n, 梦/n]

方式二：直接new 一个HMMSegment类无法获取词性
String text = ""乐视超级手机能否承载贾布斯的生态梦"";
Segment segment =new HMMSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(text);
System.err.println(termList);

输出结果：[乐/null, 视/null, 超级/null, 手机/null, 能否/null, 承载/null, 贾布斯/null, 的/null, 生态/null, 梦/null]

不知这是不是bug
"
语义依存,有计划 做 语义依存吗？
"很高级  分词成了 很高  和 级  ,   要怎么做?","[java/nx, 是/v, 一/m, 种/q, 很高/d, 级/q, 的/uj, 语言/n]
"
自定义词典index分词：带“我”字词汇某些情况下无法分出,"使用index模式分词 + 自定义词典进行分词，设置如下：
配置文件中添加：mydic.txt n 作为自定义词典
词典中包含：

> 酷我音乐
> 酷我
> 酷狗音乐
> 酷狗

分词器配置：
`seg = HanLP.newSegment().enableIndexMode(true).enableNameRecognize(true).enableCustomDictionary(true).enableTranslatedNameRecognize(true);`

对“酷我音乐”分词，结果里没有酷我

> [酷我音乐/n, 音乐/n]

对酷狗音乐，则可以得到预期结果

> [酷狗音乐/n, 酷狗/n, 音乐/n]

后经测试，发现如果词汇中带由我字，则无法分出句子中较短的带“我”字的词汇，除非没有其他的分词可能，例如下面分词结果

> 天天酷我：[天天/d, 酷我/n]

类似的例子：
词表如下：

> 暴风集团股份有限公司
> 暴风
> 合一信息技术（北京）有限公司
> 合一
> 我查查信息技术（上海）有限公司
> 我查查

分词结果：

> 
> 暴风集团股份有限公司：[暴风集团股份有限公司/n, 暴风/n, 集团/nis, 股份/n, 有限/a, 有限公司/nis, 公司/nis] （包含暴风）
> 合一信息技术（北京）有限公司：[合一信息技术（北京）有限公司/n, 合一/vi, 信息/n, 信息技术/gi, 技术/n, 北京/ns, 有限/a, 有限公司/nis, 公司/nis]（包含合一）
> 我查查信息技术（上海）有限公司：[我查查信息技术（上海）有限公司/n, 查查/v, 信息/n, 信息技术/gi, 技术/n, 上海/ns, 有限/a, 有限公司/nis, 公司/nis]（不包含我查查）
> 我查查信息技术（上海）有限公司今天宣布：[我查查信息技术（上海）有限公司/n, 查查/v, 信息/n, 信息技术/gi, 技术/n, 上海/ns, 有限/a, 有限公司/nis, 公司/nis, 今天/t, 宣布/v] （不包含我查查）
> 我查查信息：[我查查/n, 查查/v, 信息/n]（没有其他更长的分词可能，所以包含我查查）

因为分词器设置了index模式，所以应该包含尽可能多的分词可能，因此这个问题是否是一个潜在问题？
"
请教解决方向：“后来”这个词，如何在不同场景下标注为正确的词性？,"遇到了一个case, 请教一下解决的方向。《后来》这首歌，在句中会被标注为 t. 如果加自定义词典 nz，在其他非指代这首歌的句中会被标注为 nz, 想要区分这两种场景下的词性，该如何解决？"
采用默认维特比分词，默认分词结果为什么为空的？,"key=电台情歌
粗分词网：
0:[ ]
1:[电, 电台]
2:[台]
3:[情, 情歌]
4:[歌]
5:[ ]

粗分结果[]
人名角色观察：[  A 22202445 ][电台情歌  A 22202445 ]
人名角色标注：[ /A ,电台情歌 /A]
地名角色观察：[  S 1139590 A 23975 ][电台情歌  Z 21619956 ]
地名角色标注：[ /A ,电台情歌 /A]
[]
"
类似  xxx-xxx-xxx  被识别成一个词,应该怎么处理下呢?
使用默认分词，全角标点符号词性标注均为“/xu”,"博主您好，源码是最新版本的。按照词典中的标注中文标点符号词性应该为“w”，Nature中更是定义了更细分的类型，但是分词后确都是“/xu”。
======================分词代码：
Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        Segment shortestSegment = new DijkstraSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        String[] testCase = new String[]{
                ""\""（F-35）“闪电工。，【】‘、”\""""
        };
        for (String sentence : testCase)
        {
            System.out.println(""N-最短分词："" + nShortSegment.seg(sentence) + ""\n最短路分词："" + shortestSegment.seg(sentence));
        }
==================分词结果：
[F-/nx, 35/m, “/xu, 闪电/n, 工/n, ”/xu]
N-最短分词：[""/w, （/xu, F-/nx, 35/m, ）/xu, “/xu, 闪/vx, 电工/nnt, 。/xu, ，/xu, 【/xu, 】/xu, ‘/xu, 、/xu, ”/xu, ""/w]
最短路分词：[""/w, （/xu, F-/nx, 35/m, ）/xu, “/xu, 闪/vx, 电工/nnt, 。/xu, ，/xu, 【/xu, 】/xu, ‘/xu, 、/xu, ”/xu, ""/w]"
ExtractSummary的问题,"您好，非常感谢您提供这么好的开源项目，但是在看ExtractSummary时发现，对博文中这个例子做summary，得到的结果是：[无限算法的产生是由于未能确定的定义终止条件, 这类算法在有限的时间内终止, 有限的非确定算法]，和文中所说不一致，请问问题出在哪？谢谢"
单个汉字的词性(nature)是如何给定的？,单个汉字如果没有出现在核心词典中，它的词性是如何给定的？
重启,不应该是zhong4qi3，应该是chong3qi3
关于机构识别的问题,1、企业名称可以识别但是企业的简称怎么去识别呢，有没有类似的方法呢
HanLP修改主词典需要注意什么？,"请问一下：
项目需要，想用自己的词库分词。
但是做带词性和词频的词典，还有对应的BiGram词典又没有工具做，所以我想把HanLP词典中和我自己词典中都有的词保留；HanLP词典中有的，我自己词典中没有的词删掉；HanLP词典中没有，我自己词典中有的词放到custom词典中，这样靠谱不？

我能想到的问题有两个：
1. CoreDictionaryPath这个配置对应的词典中的内容，除了带“##”的字符串外（这个删了会报java.lang.ArrayIndexOutOfBoundsException异常），别的词条我可以随意增删吗？
2. 改了CoreDictionaryPath那个词典，对应的BiGramDictionaryPath词典，如果还是用原来的，会不会有什么问题？
3. 还有没有别的问题？

谢谢。
"
建议采用MapDB项目提供的Map来减少内存占用。,"[MapDB](https://github.com/jankotek/mapdb)是一个基于mmap或RAF的通用容器库。
特点就是非常节约Java托管堆。
"
CRF分词与自定义词典的结合分词。,"Hi，你好，请教一个问题，关于使用CRF统计分词切词时，出现badcase修复，然后希望可以通过自定义词典的方式去调整切词结果。这个功能有实现吗？或者CRF切词出现了badcase，我这边有什么方法去调整呢。
"
認為-认为（而非：认為）,"to have a particular opinion about sth 認為；視為；相信—— 牛津高阶英汉双解词典第八版（繁体版）

作者是否考虑香港、台湾异体字处理，参见opencc项目
"
能否加入拆词和合词相应的支持,"“美国签证”->""美国""，""签证""
“中国”，""人民"" ->""中国人民""
参考`https://github.com/ysc/word`的12、refine
"
采用默认分词结果不准确,"采用默认分词
输入内容为： 金阳观音山支行，分词结果：
[金阳/ns, 观音山/nz, 支行/n]
但是采用 NShortSegment分词，结果为
[金阳观音山支行/nt]

默认的分词实例为：
public static Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
Nshort分词实例为：
private static Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);

另外一个问题是： 为什么Nshort分词不支持自定义字典？
"
surefire插件，避免mvn test ide console乱码,
使用StandardTokenizer分词时出现数组越界的错误（用CustomDictionary增加了一些词）,"Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 45
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:140)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:101)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:40)
"
如何将HanLP代码与data目录下的词典文件打成一个jar?,
简繁体转换是否用到jpinyin包？,
开启Normalization后分词结果出问题,"版本1.3.1

代码：
`HanLP.Config.Normalization = true;
StandardTokenizer.SEGMENT.enableOrganizationRecognize(true);
System.out.println(StandardTokenizer.segment(""我的爱就是爱自然语言处理。""));`

运行结果是：

> [我/rr, 的/ude1, 爱/v, 就是/v, 爱自然语言处理./nt]
"
portable-1.2.11版本“工信处”在index模式分词分成“工信处/n 工信处/nt”,"请问一下，为什么会分出来两个不同词性的词？而且我看词典也没找到工信处词性为n的词条。
"
标准分词，最短路分词，N最短路分词的区别,"我原来的理解是：
标准分词就是用动态规划去算词图的最短路径
最短路分词就是用Dijkstra算法算词图的最短路径
N最短路就是用那个什么NShortPath算法算词图的最短路径
这些分词方式仅仅是算法不同，最终的分词效果是一样的。

但文档中说：

```
N最短路分词器NShortSegment比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强。
```

才知道这些方式可能不仅仅是求最短路径的算法有区别，最终的分词效果都是会不一样的。

我想请问一下：
1. 这三个分词算法有什么区别？
2. 为什么能出不同的分词效果？
"
动态自定义词典,"saas云平台使用，每个客户都有自己的自定义词典，需要在每次分词过程中将客户的自定义词典传入分词方法。实现针对不同客户的不同分词效果。
分词的配置也是如此。
"
"分词中出现了漏字现象, 是否为bug","在不采用日文姓名识别的情况下，对“北川景子参演了林诣彬导演的《速度与激情3》” 进行分词，
结果为：
“[北川/nr, 景/ng, 参演/v, 了/ul, 林诣彬/nr, 导演/n, 的/uj, 《/w, 速度/n, 与/p, 激情/n, 3/m, 》/w]”
发现‘子’字丢失。
"
请问能否根据机构名、地址名识别方法自定义其它实体识别方法，有通用的模板么？谢谢！,
请问如果将自己的文本训练为CRF字典？,
MDAGset的作用?,
HanLP.s2hk报错未定义,"您好，新建了一个测试项目，引用1.2.11版本的jar 一些test下的代码报错，如标题所示，请本这是怎么回事？
"
同义词典的格式,"hankcs你好，可以解释一下同义词典的格式吗？
"
文本中有英文时，分词结果有问题,"hello

world
上面是原始文本（存储在一个文件），中间有个换行。分词结果为：
<hello

world>
尖括号括起来的是一个词，没有分词hello和world。
直接使用的HanLP.segment方法
如何解决？
"
用户词库的问题,"目前自带的CustomDictionary.txt.bin是17M多，我在CustomDictionary.txt中添加自定义词也可以起作用，但是新的自动生成的CustomDictionary.txt.bin只有685K，而且分词效果下降了，请问17M的CustomDictionary.txt.bin是如何生成的？还是需要运行什么函数？
"
可以提供生成语料库的测试用例吗？,"我想生成自己特定行业领域的词库，需要用到hanlp的词库生成工具，生成类似CoreNatureDictionary.ngram.txt和CoreNatureDictionary.txt的文件。希望能提供代码NatureDictionaryMaker.java下 139行列出的“D:\JavaProjects\CorpusToolBox\data\2014”路径下的文件，我方便整理语料格式。
"
CRF分词出现内存溢出,"Hankcs~我在使用各类分词器比较结果时，如果直接使用CRF分词器结果正常，如果前面代码已经运行过一个分词器，接着运行CRF的时候会报内存溢出。
"
和elasticsearch2.3.4集成后报错,"用1.2.5和es2.3.4集成后，插入es数据和测试es分词效果时报如下错误：
{
  ""error"" : {
    ""root_cause"" : [ {
      ""type"" : ""no_class_def_found_error"",
      ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
    } ],
    ""type"" : ""no_class_def_found_error"",
    ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
  },
  ""status"" : 500
}

堆栈为：

Caused by: NotSerializableExceptionWrapper[no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex]
        at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
        at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:62)
        at com.hylanda.hanlp.plugin.HanlpTokenizer.incrementToken(HanlpTokenizer.java:101)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
请问Vertex这个类初始化的时候是不是有操作文件的代码？我怀疑是权限问题
"
Hanlp的中文分词和依存句法分析使用的词库是否一致？,"请问hancks，Hanlp的中文分词和依存句法分析使用的词库是否一致？对同一段话，分词功能的段词方法是否会跟依存句法分析功能不一致？多谢大神！
"
拼音转换中英文字符丢失,"在中文与拼音的转换过程中，只要输入句子中含有非中文字符，就输出none，建议能够保留原文中的非中文字符和标点，如当前输入“截至2012年，”就会输出 “ jié zhì  none  nián none”，建议输出“ jié zhì  2012 nián，”
"
关于模型问题,"@hankcs 您好。 请问输入模型是否可以缩减尺寸？在Android设备上跑离线版本，发现使用依存语法接口的时候，载入模型有800M大小，导致设备不能正常运行。请问模型加载过程是用到相应接口的时候再加载还是初始化的时候统一加载？
"
Hanlp Segment类seg分词方法报内存溢出错误,"在使用Hanlp对获取的文本进行预处理的时候，当处理的是重复的相同内容文本时正常，当处理大量不同文本时报内存溢出的错误。
`for (File txt : files) {
                    content = readtxt(txt.getAbsolutePath());
                    // 分词
                    /*System.out.println(content);*/
                    word_arr = StopWordsHandler.mPhraseDel(segmentNoTag.seg(content));
                /*  word_arr = new ArrayList<String>();
                    for (int i = 0; i < 10000; i++){
                        word_arr.add(""大冒险"");
                    }*/
                    //System.out.println(""content:""+content+""；size：""+word_arr.size());
                    fileContent.add(StringUtils.strip(word_arr.toString().replaceAll("","", ""\t""),""[]"")+""\t"");
                    count++;
                }`
如果把content换成固定字符串就不会报错。请问是什么原因呢？
"
怎么将HanLP的data和source一起打包,"现在需要修改data中停用词的一些数据，然后将source和data重新打包成jar，然后再开发中使用，其中data打包成类似官方sbt中的二进制形式，请问要怎么做啊？ 
"
自定义词典分词问题,"遇到这么一个问题. 请指教
输入 ""微搜集成手册""     分词结果   [微, 搜集, 成, 手册]
如果把  ""微搜"" 加入自定义词典中.  分词结果还是这样.
在 WordBasedGenerativeModelSegment 中的GenerateWordNet方法 生成词网,注释了自定义分词的代码.   如果开启后. 可以得到分词结果  [微搜, 集成, 手册].   这段代码开启是否有其他影响
![image](https://cloud.githubusercontent.com/assets/4652356/17685809/050db75a-639a-11e6-9e72-58464762d8bf.png)

在生成词网后再引入自定义词典是不是就只能对未成词的字进行合并了
if (config.useCustomDictionary)
 {
        combineByCustomDictionary(vertexList);
}
"
使用AhoCorasickDoubleArrayTrieSegment分词，自定义词典加载成功，自定义词典中有一条为“酸 n 1000”和“碱 n 1000” “金属 n 1000”，如果待分词的句子为“酸的”，分词结果为“酸的 nz”。如果是“金属的”，结果就正常，为“金属 n  的nz”，经测试的，自定义词典中有单个字的情况都有这个bug,
自定义词性错误,"版本：HANLP-1.2.10 
环境: JDK 1.8.0_45 64位 WIN10
新增的功能自定义词性DEMO第一个可以正常运行，但是尝试将词性赋予到某个词的时候报错
比如：
Nature myNature = Nature.create(""DEFINED"");
 LexiconUtility.setAttribute(""某个词"", myNature );
从词性来看，不知道对这个词有没有特殊要求

DEMO第二部分插入到用户词典也报错
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
    at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:118)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:470)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
    at MainServer.main(MainServer.java:71)
"
全国行政区划地址,"最近工作需要从统计局网站爬的一个数据，看有需要不？百度云链接: https://pan.baidu.com/s/1kUADAYV 密码: hau3
"
java Servlet 中 out.println(word.HEAD);无法输出,"   对于CoNLL格式，很奇怪，其他字段都可以输出，唯独HEAD，加了这行代码就错误。
             CoNLLSentence sentence = HanLP.parseDependency(""""+u);
            //out.println(sentence);
            // 可以方便地遍历它
           for (CoNLLWord word : sentence)
            {
                out.println(word.LEMMA);
                out.println(word.CPOSTAG);
               // out.println(word.HEAD);
                out.println(word.DEPREL);  
            }
"
java servlet调用依赖分析，除了加载配置文件。还需要显式输出一下才可以,"博主，你好。感谢你的付出。我在调用依赖分析的时候。发现
                        Properties properties = new Properties();
                        //第一行代码
            properties.load(getServletContext().getResourceAsStream(""/WEB-INF/hanlp.properties""));
                        //第二行代码
            out.println(""root=""+properties.getProperty(""root""));
                         //第三行代码
                  如果只有第一行和第二行代码，调试不成功。如果执行一遍第三行代码，调试就通过。
                   如果调试通过，注释第三行代码，依然可以通过。不知道这个是怎么个原理。
"
如何获取portable分支获取的代码?,"@hankcs  你好:今天发现一个问题，就是通过git clone 命令获取源代码的时候，master和portable分支获取的代码是一样的，都是https://github.com/hankcs/HanLP.git这同一个地址，请问如何获取portable分支的代码？
"
中文数字分词效果不佳,"比如像如下分词

> 一百二十五乘以十二等于几
> 结果是：
> 
> [一百二十五/m, 乘以/matml, 十/m, 二等/b, 于/p, 几/d]

如果采取字符转换：一：1；二：2；。。。九：9；则不利于转换成阿拉伯数字（因为像一万零几的情况不好处理）。
所以要是分词能把中文切出来，像：

> [一百二十五/m, 乘以/matml, 十二/m等于/mateq, 几/d]
> 就会比较理想。

貌似**等于**在这个句子中无法很好被自定义词性标准切出来, 而“二等”如果在一起很容易被标准为'b'词性，但是等于的自定义词性的词频已经是设置很高了，而且如果把**等于**排到第一个或者在它的前面加一个”的结果“又可以被标准了。

请问，这样的问题该如何解决呢？

谢！！
R
"
使用语句依赖项分析数组越界,"对于文章长度比较短的，使用自带的依存关系处理没有任何问题，但是对于长文章却提示数组越界问题，请问一下应该怎样使依存关系处理长的文章？现在想到的一个方法是将长文章分几段进行处理，请问一下您有没有好的方法解决对于长文本的依存关系处理？谢谢~
"
用户自定义词典,"CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;
通过这种方式追加了自己创建的词典，但是使用的时候并没有调用
"
如果启用CustomDictionary，人称代词被标注为z而不是rr了,"请看一下这个问题该如何解决。

谢谢～
"
CRFSegment分词把人名标注为nz，而不是nr,"拉取了最新代码，发现采用CRFSegment分词，就把人名标注成为nz了。

如果用缺省的分词方法能正确标注人名为nr。

但问题是缺省的分词方法还是无法把自定义的词给分出来。。。
"
update .gitignore to add .settings folder,"Since Eclipse projects create .setttings folder automatically, it'd be best to add this folder into the .gitignore file
"
求助：ViterbiSegment个别句子机构识别异常，报NoSuchElementException,"```
val txt = ""而其他肢解出去的七个贝尔公司如西南贝尔、太平洋贝尔、大西洋贝尔、南方贝尔等只能经营短途电话业务。""
val seg_viterbi = new ViterbiSegment().enablePartOfSpeechTagging(true).enableOffset(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true).enableNumberQuantifierRecognize(true)
println(seg_viterbi.seg(txt))
```

```
Exception in thread ""main"" java.util.NoSuchElementException
    at java.util.LinkedList.getFirst(LinkedList.java:244)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:127)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:90)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:401)
    at cn.com.cetc.lab.test.TestMulti$.main(TestMulti.scala:17)
    at cn.com.cetc.lab.test.TestMulti.main(TestMulti.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
```

看了代码，没太看明白，不知道哪儿的问题。一般句子没有问题，个别句子会这样。
"
求助：时间相关分词问题,"**原句：“我想在明天上午9点听一个儿童故事”**
- 如果用默认的分词方法：HanLP.newSegment() 获得结果为：
  
  > [我/z, 想/s, 在/dg, 明/ad, 天上/i, 午/l, 9/m, 点/q, 听/s, 一个/Rg, 儿童故事/ngsch]
- 如果用CRFSegment结果变为：
  
  > [我/z, 想/s, 在/dg, 明天/i, 上午/tam, 9/nz, 点/q, 听/s, 一个/Rg, 儿童故事/ngsch]

**如果原句改为：“我想在明天上午九点听一个儿童故事”**
- 如果用默认的分词方法:
  [我/z, 想/s, 在/dg, 明/ad, 天上/i, 午/l, 九/m, 点/q, 听/s, 一个/Rg, 儿童故事/ngsch]
- 如果用CRFSegment：
  
  > [我/z, 想/s, 在/dg, 明天/i, 上午九/nz, 点/q, 听/s, 一个/Rg, 儿童故事/ngsch]

另外，我在分词中采用了个性化的词性如下：

```
  CustomDictionary.add(""故事"",    ""ngs 1024"");
  CustomDictionary.add(""儿童故事"", ""ngsch 2048"");
  CustomDictionary.add(""昨天"",    ""tzt 1024"");
  CustomDictionary.add(""今天"",    ""tjt 1024"");
  CustomDictionary.add(""明天"",    ""tmt 1024"");
  CustomDictionary.add(""后天"",    ""tht 1024"");
  CustomDictionary.add(""上午"",    ""tam 1024"");
  CustomDictionary.add(""下午"",    ""tpm 1024"");
```

请问，**如何能正确分出时间呢？**
比如能分出： [明天 上午 9 点] 

万分感谢～
"
data-for-1.2.10.zip 鍵結失效,"https://github.com/hankcs/HanLP/releases
新版数据包：data-for-1.2.10.zip

連結好像已經不在網盤
"
CRFSegment的字符正规化表加载失败,"下面这行代码
`ObjectInputStream in = new ObjectInputStream(new FileInputStream(HanLP.Config.CharTablePath));
`
直接导致了下面问题：

```
Aug 03, 2016 2:12:18 PM com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable <clinit>
SEVERE: 字符正规化表加载失败，原因如下：
java.io.StreamCorruptedException: invalid stream header: 003D200A
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:806)
    at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:370)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:47)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at com.hankcs.demo.DemoCRFSegment.main(DemoCRFSegment.java:62)
```

请帮忙查看一下。
"
HanLP.java 对CustomDictionaryPath 里面的路径设置错误。,"HanLP/src/main/java/com/hankcs/hanlp/HanLP.java

请按以下修改，否则成功无法加载。

```
-                String[] pathArray = p.getProperty(""CustomDictionaryPath"", ""dictionary/custom/CustomDictionary.txt"").split("";"");
+                String[] pathArray = p.getProperty(""CustomDictionaryPath"", ""data/dictionary/custom/CustomDictionary.txt"").split("";"");
```
"
CRFmodel计算结果与crf++的crf_test结果不一致。,"CRFmodel加载了crf++ 0.58版本训练出来的模型后，tag后的结果与采用该crf++自带的crf_test计算出来的结果不一样。这是为什么呢，算法有什么不一样的地方吗？从结果来看crf_test的结果要准确很多，CRFmodel计算出来的结果很多是错误的。
`CRFModel model = CRFModel.loadTxt(""F:\\studio\\CRF++-0.58\\CRF++-0.58\\example\\model.txt"",
                new CRFModel(new DoubleArrayTrie<FeatureFunction>()));
        System.out.println(model);
        Table table = new Table();
        table.v = new String[][]{
            {""微鲸"", ""ns"", null},
            {""创新"", ""vn"", null},
            {""营销"",  ""n"", null},
            {""抢占"",  ""v"", null},
            {""电视"",  ""n"", null},
            {""先机"",  ""n"", null}
        };
        model.tag(table);
        System.out.println(table);`

[crf_train.txt](https://github.com/hankcs/HanLP/files/392253/crf_train.txt)
[crf_test.txt](https://github.com/hankcs/HanLP/files/392252/crf_test.txt)
"
錯誤繁簡轉換字詞,"請更新相關字詞表，謝謝

單字轉換：
產/产 (沒有轉換)
溝/沟 (錯誤轉換為钩)
為/为 (沒有轉換)
僱/雇 (沒有轉換)

以下只能使用詞語表，不能單字轉換：
夥伴 -> 伙伴
由於 -> 由于
位於 -> 位于
"
请教一个问题，如何使用索引分词模式并删除停用词？,
使用java servlet 实现有点问题。,"博主，你好，辛苦了。我用java servlet 调用语言包，运行不成功，这是测试代码。调试运行，提示“网站无法显示该页面/  HTTP 500 /最可能的原因是:
•该网站正在进行维护。
•该网站有程序错误。
”
 请教哪里出问题了。

我用的编译器是eclipse-jee-neon-R-win32，   demo在控制台模式下没有问题。这是部分代码

import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import com.hankcs.hanlp.HanLP;

public class HelloWorld extends HttpServlet implements javax.servlet.Servlet{

```
    public HelloWorld() {
        super();
    }       
    protected void doGet(HttpServletRequest request, HttpServletResponse response) 
        throws ServletException, IOException {
        //response.getWriter().write(""Hello"");
        request.setCharacterEncoding(""utf-8"");
        response.setContentType(""text/html;charset=utf-8"");
        PrintWriter out = response.getWriter();
        String u=request.getParameter(""userinput"");
        out.println(""<br/>输入:""+u);
        out.println(""首次编译运行时"");
        out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！接下来请从其他Demo中体验HanLP丰富的功能~""));
    ....................................
```
"
CRFModel的构造函数有个问题，直接使用会失败！,"你好，想通过你的CRFModel的loadBin加载你自带的bin模型或使用loadTxt加载自己训练的crf++模型。加载没有报错，但是tag工作却提示空指针。

看了你CRFSegmentModel类的加载代码，这里你用的是`new BinTrie<FeatureFunction>()`初始化CRFModel，而在CRFModel的构造函数中使用的是`new DoubleArrayTrie<FeatureFunction>()`。这个是不是版本升级，不一致问题。

修改相关的DoubleArrayTrie为BinTrie后，tag函数工作正常。
"
CRF新词识别,"Segment segment = new CRFSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(""你看过穆赫兰道吗"");
System.out.println(termList);
for (Term term : termList)
{
    if (term.nature == null)
    {
        System.out.println(""识别到新词："" + term.word);
    }
}

用master分支上的代码运行这段代码，识别不出新词。运行结果如下：
[你/rr, 看过/v, 穆赫兰道/nz, 吗/y]
"
运行自定义词性Demo，NoSuchFieldException:$VALUES,"您好，使用MacBook Pro 10.11，Eclipse Mars.1 ，JDK1.7，运行1.2.10版本的DemoCustomNature.java后如下报错：
七月 24, 2016 6:55:03 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
Exception in thread ""main"" java.lang.IllegalArgumentException: Could not create enum
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:99)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
    at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
    at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
    at com.hankcs.demo.DemoCustomNature.main(DemoCustomNature.java:41)
Caused by: java.lang.IllegalArgumentException: Could not create the class
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:453)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:439)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:91)
    ... 4 more
Caused by: java.lang.NoSuchFieldException: $VALUES
    at java.lang.Class.getDeclaredField(Class.java:1961)
    at com.hankcs.hanlp.corpus.util.EnumBuster.findValuesField(EnumBuster.java:349)
    at com.hankcs.hanlp.corpus.util.EnumBuster.values(EnumBuster.java:429)
    at com.hankcs.hanlp.corpus.util.EnumBuster.access$0(EnumBuster.java:426)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:443)
"
"繁体字典增加""擷取=截取""","见wikipedia条目""数据库""
第一句""用户可以对文件中的数据运行新增、截取、更新、删除等操作""的繁简转换.
"
歧义分词解决,"我第一次试这个hanlp，觉得分词很好，试了一些容易引起歧义句子，大部分没问题。以下4句是CRFSeg出错的：
""在这些企业中国有企业有十个"",
""新建地铁中将禁止商业摊点"",
""方程的解除了零以外还有其它的…"",
""这的确定不下来"",

CRFSeg的结果：
[在, 这些, 企业, 中国, 有, 企业, 有, 十个]
[新建, 地铁, 中将, 禁止, 商业, 摊点]
[方程, 的, 解除, 了, 零以外, 还有, 其它, 的, …]
[这, 的, 确定, 不, 下来]

segment(标准分词)的结果：
[在/p, 这些/rz, 企业/n, 中/f, 国有企业/nz, 有/vyou, 十/m, 个/q]
[新建/v, 地铁/n, 中/f, 将/d, 禁止/v, 商业/n, 摊点/n]
[方程/n, 的/ude1, 解除/v, 了/ule, 零/m, 以外/f, 还有/v, 其它/rz, 的/ude1, …/w]
[这/rzv, 的/ude1, 确定/v, 不/d, 下来/vf]

标准分词中前两个对了，后两个依然是错的。我的问题是：
1、segment使用的什么方法？
2、可以通过什么办法进一步改进分词的准确率吗？（你的分词准确率已经很好了）

谢谢！

Henry
"
默认的繁体转换好奇怪,"舞台：舞颱
因为：囙爲
青梅竹马：青楳竹馬
"
按第二种方法配置，调用依赖解析Demo发生错误。,"博主，
你好，感谢你的付出。我在测试依存关系demo的时候发生了这个错误，请教哪里没有设置对？我是按第二种方法配置的。即使用了hanlp.properties
使用的工具是
Eclipse IDE for Java Developers
Version: Mars.2 Release (4.5.2)
demo 是 DemoDependencyParser

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.dependency.nnparser.Matrix.load(Matrix.java:1305)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:259)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:135)
    at com.hankcs.hanlp.dependency.nnparser.parser_dll.<clinit>(parser_dll.java:34)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.parse(NeuralNetworkDependencyParser.java:53)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.compute(NeuralNetworkDependencyParser.java:93)
    at com.hankcs.hanlp.HanLP.parseDependency(HanLP.java:407)
    at hanlp.DemoDependencyParser.main(DemoDependencyParser.java:26)
"
java.lang.ExceptionInInitializerError,"hankcs好，我用Python调用hanlp做依存句法分析时遇到如下问题，请问该怎么解决？ps：使用分词等功能时正常，多谢！！！
![sft cu_6w3pp c0 9aa11](https://cloud.githubusercontent.com/assets/19303343/16905158/5a4487a8-4cd4-11e6-97e0-8cf378bc35d9.png)
"
java.lang.ExceptionInInitializerError,"请问hankcs，调用依存句法分析，出现初始化错误该如何解决，多谢多谢！ps：分词等功能正常
![vcp wuh b8 5n h aopw9p](https://cloud.githubusercontent.com/assets/19303343/16901354/92c08520-4c73-11e6-8f71-0e8d36710819.png)
"
NLPTokenizer分词时用到的核心词典转移矩阵,"我现在用NLPTokenizer分词方法进行分词，在Nature中添加词性text，把核心词典中合肥市的词性标注为text，但是会出现下面的错误，
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
    at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:118)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:37)
    at com.hankcs.test.seg.Test.main(Test.java:11)

显示应该是核心词典转移矩阵的问题，核心矩阵中不包含新的词性，那这样的话，核心词典转移矩阵需要重新生成吗，要怎么生成啊
"
用户自己添加新的词性的问题,"现在发现1.2.10给出的用户自定义添加词性的demo不适用于NLPTokenizer.segment(text)分词，还没找出原因，希望能帮忙看一下，谢谢！
"
Class com.hankcs.hanlp.HanLP not found,"hankcs大神，调用hanlp时碰到如下问题，怎么解决？另外，对项目文档里的这句话也不能理解“最后将HanLP.properties放入classpath即可，对于任何项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。‘’,求解答？多谢！！！

![78_d1 v13tj1r_6 0 lqu9w](https://cloud.githubusercontent.com/assets/19303343/16736512/d3455124-47bf-11e6-94b4-b6d686e2c43c.png)
"
使用maven的jar包，程序執行時找不到詞典路徑。,"Exception如下：

```
Jul 07, 2016 5:00:15 PM com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable <clinit>
SEVERE: 字符正规化表加载失败，原因如下：
java.io.FileNotFoundException: data/dictionary/other/CharTable.bin.yes (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.<init>(FileInputStream.java:138)
    at java.io.FileInputStream.<init>(FileInputStream.java:93)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:368)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:46)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at io.smackds.processors.NLP$$anonfun$receive$1.applyOrElse(NLP.scala:31)
    at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
    at io.smackds.processors.NLP.aroundReceive(NLP.scala:19)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
    at akka.actor.ActorCell.invoke(ActorCell.scala:495)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
    at akka.dispatch.Mailbox.run(Mailbox.scala:224)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
```

但如果直接使用github的jar，則沒有錯誤。
我的hanlp.properties裡指定了root=/Users/stephen/devel/hanlp-1.2.10，並另外下載了完整的詞典檔。
"
add traditional chinese synonym and trans dict,"Added some words to the dictionary for Traditional Chinese and synonym
"
地址集体识别,"您好，我是第一次使用hanlp，我想请教一下对于“北京市海淀区学院路31号北京航空航天大学体育馆”这个整体地址有什么方法可以集体一次性识别出来吗？而不是北京市/ns海淀区/ns学院路/nz31/m号/q北京航空航天大学/nt体育馆/n这中怎么把他们变为北京市海淀区学院路31号北京航空航天大学体育馆/ns。麻烦您了
谢谢您。
"
HanLP自定义字典不生效,"您好，我按照官网的方法配置了properties，而且也进行了对bin的删除，但是就是不能进行对我的字典.txt里的添加的词进行识别，字典里定义了人才报告厅，麻烦您帮我看一下。
![qq 20160705130443](https://cloud.githubusercontent.com/assets/20194150/16574453/a1e3515c-42b1-11e6-91f3-750f619e34d2.png)
![qq 20160705130632](https://cloud.githubusercontent.com/assets/20194150/16574452/a1d56236-42b1-11e6-8783-48e99c4c4cd9.png)
![qq 20160705130748](https://cloud.githubusercontent.com/assets/20194150/16574451/a1d3eeba-42b1-11e6-9cd1-5384baadecda.png)
"
CharTable.convert 將逗句變成句號,"如題，應如何修正？

我留意到CharTable.java 有些修正的備註，不知能否使用

```
    CharTable.CONVERT['.'] = '.';
    CharTable.CONVERT['．'] = '.';
    CharTable.CONVERT['。'] = '，';
    CharTable.CONVERT['！'] = '，';
    CharTable.CONVERT['，'] = '，';
    CharTable.CONVERT['…'] = '，';
```

```
print txt
> 方說葛參選立會時曾許諾反對擴建堆填區，投票前卻反口，出賣將軍澳居民，故望改變葛想法，但否認曾捉着葛，也無用橫額圍過她，反是陪同葛到場的民建聯區議員李家良突伸手拉橫額，令葛和她也跌倒；葛又迴避問題走來走去，方見葛行錯路一度助她離開。


    return CharTable.convert(txt.replace(u'，',u'，'))

> 方说葛参选立会时曾许诺反对扩建堆填区。投票前却反口。出卖将军澳居民。故望改变葛想法。但否认曾捉着葛。也无用横额围过她。反是陪同葛到场的民建联区议员李家良突伸手拉横额。令葛和她也跌倒。葛又迴避问题走来走去。方见葛行错路一度助她离开。
```
"
CoNLLWord 的CPOSTAG 及POSTAG 問題,"根據

```
https://github.com/hankcs/HanLP/blob/9456aebab678ef0ecfcd0a367d29a04e878c112a/src/main/java/com/hankcs/hanlp/corpus/dependency/CoNll/CoNLLWord.java

sb.append(ID).append('\t').append(LEMMA).append('\t').append(LEMMA).append('\t').append(CPOSTAG).append('\t')
                .append(POSTAG).append('\t')
```

DemoDependencyParser 第4 和第5 個輸出分別是 CPOSTAG 及POSTAG。

我執行DemoDependencyParser 時，卻發現很多不明白的POSTAG , e.g. **_nnt**_, **_nis**_

```
java -classpath ""."" com/hankcs/demo/DemoDependencyParser ""至於副主席的形势更为混乱，据知建制派内部属意的公民力量温悦昌至今仍未报名，引来民建联凌文海、坑口乡事委员会主席成汉强、独立周贤明等多方有意染指，随时会有意外战果出现。""

1       至於    至於    i       i       _       2       定中关系        _       _
2       副主席  副主席  nz      nnt     _       4       定中关系        _       _

...

27      委员会  委员会  nt      nis     _       28      定中关系        _       _

```

如何理解POSTAG 和CPOSTAG?
"
自行添加custom词典问题,"目前应用是在一个具体领域，希望分词结果以出现在我自行添加的词典中的名词为最高优先级，词典添加方式参照了指引、在property文件中加了配置，并删除了bin文件让它重新生成。目前看效果有的词生效有的没有，还是会被切开。使用indextokenizer，不知道是否哪里没弄对？例子：“正中神经”，切为正中和神经，谢谢！
"
繁簡字典 - 基於=基于,"加入基於=基于
"
方言的字典處理挑戰,"你好，

我在建立方言(香港)字庫/同義詞

我發現方言的處理有些時候不容易處理，例如同義詞便可用三個不同的字典處理：
1. 繁體字典
什麼=甚麼
1. 同義詞
   不好=唔好
2. 方言CustomDictionary
   有無攪錯 

例子3 是比較好處理的，因為不影響其他字詞的分析。

例子1, 2 會影響現存字庫的分析結果。

我想這問題和一般 domain 問題的處理有不少相似。

例如能否在hanlp.properties 加入section/domain ，可以 在運算時換dictionary/corpus.
"
请问HanLP.properties,"请问hanlp.properties文件,
没看到发布页 https://github.com/hankcs/HanLP/releases
有这个文件的例子， 

请问是只要这样一行就好了么？
root=/myhanlpdata/
"
关于自定义词典的处理,"关于自定义词典的处理有几个疑问，请教一下
1.目前自定义词典都是用于对粗分词结果进行合并处理的，不知道这个是否理解有误？
2.对于粗分词为""AB/C""情况下，如果需要调整分词为""A/BC""是否必须修改核心词典CoreNatureDictionary.txt？
3.如果想动态修改核心词典，除了文件系统修改再触发加载词典这种方法外，是否可以直接通过内存变量修改实现？
"
假设有一个自定义的词库，输入一个词或者语句去寻找出最相近的词,"大神， 你这个项目目前能够实现这个需求吗， 文字上接近匹配就可以
"
按标点符号分割文章，用哪个方法,"按标点符号分割文章，用哪个方法
"
C#中调用Hanlp.dll文件执行程序出现以下错误,"![image](https://cloud.githubusercontent.com/assets/19245664/16475947/29d8d9b6-3eb5-11e6-8873-5a4770a76945.png)
"
这个能不能实现搜索联想推荐,"现在可以实现x5巧克力 分词推荐成巧克力。
 采用同义词词库推荐的话，有没有方法只获取到同义词词库中的词， 目前用elasticsearch的ik分词和同义词filter过滤， 或出现很多不相关的词，比如方便面，会出来方便，泡面...  怎么让不在分词词库中的词不出来。
可以认为有没有实现根据一个指定的词库，有同义词的概念，只分词成词库中词，不在词库中的丢弃
"
使用 File.getCanonicalPath() to 展开 hanlp.properties 里指定的 root ,"使用 `File.getCanonicalPath()` to 展开`hanlp.properties` 里指定的 root，这样可以在 `hanlp.properties` 里使用类似 `.` / `..` 等相对路径的表达
"
词的细分问题,"当我输入“洗车行，理发店，游泳馆”这类词时，它们被切分成""洗/nz, 车行/nis, ，/w, 理发店/nis, ，/w, 游泳馆/n]""。其实正确的切分应该时“洗车／行/, 理发／店/ , 游泳／馆/n]”，这样的话在搜索“洗车，理发，游泳”时也能精确匹配到
"
词典加载错误,"下载的600m的data词典，删除dic中bin类型的文件，在本地ide中能加载生成bin文件，也能正常分词，但是打包放到服务器上，生成bin 的时候报错，想请问下是什么原因呢，信息如下：
`at com.hankcs.hanlp.seg.Segment.quickAtomSegment(Segment.java:161)
        at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.GenerateWordNet(WordBasedGenerativeModelSegment.java:458)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:42)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)`

`Caused by: java.lang.ArrayIndexOutOfBoundsException: 32618
        at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToChar(ByteUtil.java:255)
        at com.hankcs.hanlp.corpus.io.ByteArray.nextChar(ByteArray.java:81)
        at com.hankcs.hanlp.dictionary.other.CharType.<clinit>(CharType.java:77)
        ... 15 more`
"
mac下 heap Size,"很有可能我是在问蠢问题：
我在windows运行无任何问题，1000次跑下来benchmark在122mb左右（自己的程序）
但帮助其他人部署在Mac下出现了heap size 问题
使用Intellij和Eclipse均无法跑起Demo1
-Xmx6g 传入VMoption还是无果，
-Xmx4g -Xmx2g -Xmn2g也已经试过

`
System.out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！""));
`

`usr/lib/jvm/java-8-oracle/bin/java -d64 -Xmx6g -Didea.launcher.port=7534 -Didea.launcher.bin.path=/home/master/Prog/idea-IU-145.1617.8/bin -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/home/master/Documents/<Project Name>/target/classes:/home/master/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/master/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/master/Documents/<Project Name>/lib/hanlp-1.2.9.jar:/home/master/Prog/idea-IU-145.1617.8/lib/idea_rt.jar com.intellij.rt.execution.application.AppMain Main
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.dictionary.CoreDictionary.loadDat(CoreDictionary.java:141)
    at com.hankcs.hanlp.dictionary.CoreDictionary.load(CoreDictionary.java:63)
    at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:40)
    at com.hankcs.hanlp.seg.common.Vertex.<clinit>(Vertex.java:56)
    at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
    at Main.main(Main.java:30)
`
"
 关于机构分词学习新词问题,"读了您写的文档，在使用如何使用功能写的非常详细。自定义的公司名语料怎么添加到机构分词模型中。
"
package com.hankcs.hanlp;将分号改为冒号,"导入到IDEA中需要将for (String path : classPath.split("";""))改为for (String path : classPath.split("":""))
"
新词发现的问题,"请教一个问题，从您的文档中，我知道可以透过自定义词典的方式来加入新词(TXT或代码中直接加入)，经实际测试，自定义的新词都可以被辨识的很好。但想请教一个更根源的应用问题，假设待处理的文档源源不断的进来，则可以透过什麽方式来发现新词呢？
比方您的文档提到，CRF发现新词能力较强，我做了一个实验，输入带有「理都懂，然并卵，城会玩，日了狗」等网路语言，我发现这些人民日报语料库里不大可能出现的词，都不会被识别为整词，而是被拆分为粒度更小的词，并识别出来，换言之，它们不被识别为新词。
即使是透过CRF这种辨识能力强的算法，这样的问题有没有解？是不是一定得透过语料库才有办法处理，比方在训练CRF模型的时候，语料库里就必须有这些网路词？
"
 新詞發現的問題,"请教一个问题，从您的文档中，我知道可以透过自定义词典的方式来加入新词(TXT或代码中直接加入)，经实际测试，自定义的新词都可以被辨识的很好。但想请教一个更根源的应用问题，假设待处理的文档源源不断的进来，则可以透过什麽方式来发现新词呢？
比方您的文档提到，CRF发现新词能力较强，我做了一个实验，输入带有「理都懂，然并卵，城会玩，日了狗」等网路语言，我发现这些人民日报语料库里不大可能出现的词，都不会被识别为整词，而是被拆分为粒度更小的词，并识别出来，换言之，它们不被识别为新词。
即使是透过CRF这种辨识能力强的算法，这样的问题有没有解？是不是一定得透过语料库才有办法处理，比方在训练CRF模型的时候，语料库里就必须有这些网路词？
"
"hankcs, 为啥在CoreNatureDictionary.tr.txt里没有Yg标签啊？","是因为它出现的次数老少了吗，在那个北大标注集里有这个标签的
"
请教一下,"小白请教一个问题，请问您这个data目录中的dictionary下面的数据是哪边来的啊？
"
开启词性问题？,"大神好，我用默认的标准分词器，显式开启词性与不设置，分词后都能够获取词性？

List<Term> list=HanLP.newSegment().enablePartOfSpeechTagging(true).seg(document);

List<Term> list = HanLP.segment(document);

上面两种 分词结果都能够获取词性（貌似源码里面默认配置是不开启词性的）
"
用户词典词性问题,"用户词典采用默认词性，也就是nz，能否在分词的时候，自动识别新添加词的词性，而不是全部都是名词？
"
TraditionalChineseToker cannot use CustomerDictionary,"I am testing different tokenizer performance, I found that the TraditionalChineseTokenizer does not apply the CustomDicionary as the others do.

```
CustomDictionary.add(""銀主盤"", ""nz 1024 n 1"");
  Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true);

        for (String sentence : testCase)
        {
            System.out.println( ""\n\n"" + ""Sentence"" + sentence);
            System.out.println(""HanLP"");
            List<Term> termList = segment.seg(sentence);
            System.out.println(termList);

            termList = TraditionalChineseTokenizer.segment(sentence);
            System.out.println(""TraditionalChineseTokenizer"");
            System.out.println(termList);


            termList = NLPTokenizer.segment(sentence);
            System.out.println(""NLPTokenizer"");
            System.out.println(termList);
        }
```

Results
Sentence (Only TraditionalChineseTokenizer does not apply the ""銀主盤"" new word in CustomDictionary.)

```

銀主盤,好多人等緊樓市大跌!
HanLP
[銀主盤/nz, ,/w, 好多/mq, 人/n, 等/udeng, 緊/n, 樓/n, 市/n, 大跌/v, !/w]
TraditionalChineseTokenizer
[銀/ng, 主盤/n, ,/w, 好多/mq, 人/n, 等/udeng, 緊/d, 樓市/n, 大跌/v, !/w]
NLPTokenizer
[銀主盤/nz, ,/w, 好多人/nt, 等/udeng, 緊/n, 樓/n, 市/n, 大跌/v, !/w]
```
"
CRFSegment 哪下載,"在data.zip 中沒有CRFSegmentModel.txt, 固DemoCRFSegment.java 有如下錯誤:

```
java -classpath "".:../hanlp/src/main/java/""  com/hankcs/demo/DemoCRFSegment
Jun 14, 2016 1:02:29 AM com.hankcs.hanlp.model.CRFSegmentModel <clinit>
SEVERE: CRF分词模型加载 hanlp/data/model/segment/CRFSegmentModel.txt 失败，耗时 5 ms
```

我在 http://www.hankcs.com也找不到相關文章。請問在哪可下載或怎樣建立？

謝謝.
"
追加繁簡體詞典,"在追加字典的例子中：

```
8. 用户自定义词典
...
追加词典

    CustomDictionary主词典文本路径是data/dictionary/custom/CustomDictionary.txt，用户可以在此增加自己的词语（不推荐）；也可以单独新建一个文本文件，通过配置文件CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;来追加词典（推荐）
```

我明白推荐新加一個詞典。

那我新的詞典應否把繁體和簡體分開？ (有沒有實際的效果/效能影響)。
例如我新加一個`經濟字典.txt`，是否如下新增最佳？

`````` 經濟字典.zh-TW.txt```
```經濟字典.zh-CN.txt```

``````
"
如何根据拼音（或者汉字）获取最相近的词组,"比如说用户输入了徐家汇，但是用户实际想得到的歌手许佳慧？ 可以拼音转接种增加相应的接口吗？
"
CRF的特征模板和特征函数,"对CRF不是特别理解想请教下
1 特征模板是自己手工编写的吗? 如果有有很多行普通文本，是1行文本就是一个特征模板吗？
2 对于CRF++,如果有语料和特征模板是不是就可以生成模型咯?
3  第2点如果是，那语料该从那里获取呢，它和(1)中的普通文本有什么关系。
"
问题请教,"http://www.hankcs.com/nlp/hanlp.html　这里说的以下功能，有实例吗？或是代码在哪？
语料库工具
分词语料预处理
词频词性词典制作
BiGram统计
词共现统计
CoNLL语料预处理
CoNLL UA/LA/DA评测工具
"
"如何建立 情感分析, 识别文本蕴涵(RTE)等平台","在英文文本挖掘，可以找到一些情感分析, 识别文本蕴涵 的應用和平台。

在中文文本挖掘，我找到的是東拼西湊的方案，例如 python 的nltk +jieba (分詞)。

作為統一的中文文本挖掘系統，HanLP 是非常合適的。 

HanLP 有沒有相關的方法/方案，能建成如下的平台：

情感分析、 识别文本蕴涵(RTE)系統、Hadoop/Spark 結合、机构名识别模块的核心模型 (http://www.hankcs.com/nlp/ner/place-name-recognition-model-of-the-stacked-hmm-viterbi-role-labeling.html)

我十分樂意參與相關開發。
"
如何打包hanlp portable,"最近有新的commit，想要自己打包portable版本
請問有有關的打包portable的教學嗎?
謝謝
"
自定义词典删除词语失效,"你好，在使用自定义词典的例子时，发现如下问题：
加入自定义词组能够使原先被分割开的固定搭配保持整体不切分，但是如果之后再使用remove等方法移除该组合，那么将不起作用。

```
    import com.hankcs.hanlp.HanLP;
    import com.hankcs.hanlp.dictionary.CustomDictionary;

    public class DemoCustomDictionary {
      public static void main(String[] args) {
        String customTerm = ""攻城狮"";
        String text = ""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰"";
        System.out.println(""原始分词结果"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
        // 动态增加
        CustomDictionary.add(customTerm);
        System.out.println(""添加自定义词组分词结果"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
        // 删除词语
        CustomDictionary.remove(customTerm);
        System.out.println(""删除自定义词组分词结果"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
      }
    }
```

最后输出结果

```
  原始分词结果
  CustomDictionary.get(customTerm)=null
  [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]
  添加自定义词组分词结果
  CustomDictionary.get(customTerm)=nz 1 
  [攻城狮/nz, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]
  删除自定义词组分词结果
  CustomDictionary.get(customTerm)=null
  [攻城狮/nz, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]
```

想问下是不允许这种临时性操作吗还是另有其他？

另外，HanLP是否支持用户自定义分词结果(已经过人为分词的语句之间使用空格或者###等特殊符号间隔)，然后通过一种特定的Tokenizer分割成数组形式然后传入Segment进行POS分析。
比如这句`我很想要个宝宝,但觉得好难才有,早二个月怀上了,但流去了`，
假设""正确""的分词结果为
`我 很 想 要 个 宝宝 , 但 觉得 好难 才 有 , 早 二个月 怀上 了 , 但 流去 了`，不知道该如何在HanLP操作得到分析结果等

感谢！

运行环境
HanLP v1.2.9 
Ubuntu 14.04 x86_64
JDK 1.8.0_73
"
多个标点符号分成一个词。,"主要有几方面分原因**:(**1)经济进入转型期,新常态下需求本身有所放缓。
比如这句中 :( 分成一个词了
这种问题可以解决吗？
"
自定义字典分词错误,"自定了单田芳，三国演义。 且系统允许自定义字典。 但是还是分成了单田芳三/nz, 国/n, 演义/n，能否帮查一下？
"
GC overhead limit exceeded,"java.lang.OutOfMemoryError: GC overhead limit exceeded
    at com.hankcs.hanlp.corpus.io.ByteArray.nextString(ByteArray.java:113)
    at com.hankcs.hanlp.model.maxent.MaxEntModel.create(MaxEntModel.java:328)
    at com.hankcs.hanlp.dependency.MaxEntDependencyParser.<clinit>(MaxEntDependencyParser.java:45)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at com.jfinal.ext.kit.Reflect.forName(Reflect.java:692)
    at com.jfinal.ext.kit.Reflect.on(Reflect.java:85)
    at com.jfinal.ext.kit.ClassSearcher.extraction(ClassSearcher.java:49)
    at com.jfinal.ext.kit.ClassSearcher.search(ClassSearcher.java:144)
    at com.jfinal.ext.route.AutoBindRoutes.config(AutoBindRoutes.java:73)
    at com.jfinal.config.Routes.add(Routes.java:40)
    at config.JfinalConfig.configRoute(JfinalConfig.java:36)
    at com.jfinal.core.Config.configJFinal(Config.java:47)
    at com.jfinal.core.JFinal.init(JFinal.java:65)
    at com.jfinal.core.JFinalFilter.init(JFinalFilter.java:49)
    at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:137)
    at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:831)
    at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:300)
    at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1341)
    at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1334)
    at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744)
    at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:497)
    at org.eclipse.jetty.maven.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:281)
    at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)
    at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:60)
    at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:154)
    at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)
    at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:60)

这是什么原因引起的呢？
"
NShortSegment跑飞问题,"使用new NShortSegment().enableAllNamedEntityRecognize(true)作为textcrank抽取关键词的分词器，持续提取关键词后，会发现大量GC,然后直接挂掉，在堆栈中，一直在running NShortPath.class 211行
"
如何在分词时去掉停用词？,"我想把分词结果中的“xx的”这种结果去掉，只留下xx，该怎么做？
"
SpeedTokenizer分词器词性提取不出来,"您好，StandardTokenizer标准分词器可以提取出词性，但是SpeedTokenizer极速分词器提取不出词性，是什么原因导致的。
"
NShortSegment用户字典问题,"在使用NShortSegment分词的时候，添加了用户自定义词，CustomDictionary.add(""自由侠"", ""n 100000000"");无法分出该词，其他分词器倒是可以，请问下是用户词典在该分词器中不起作用么？
"
关于词典打包问题,"我修改了hanlp算法里的功能，重新打包成jar文件，其他程序调用，运行成功。但是把调用程序打包成.jar文件严重: 没有找到HanLP.properties，可能会导致找不到data
========Tips========
请将HanLP.properties放在下列目录：
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data,请问这个问题怎么解决，我尝试把对应的properites版本修改打包调用的时候报错：核心词典加载失败。
"
如果crf训练等需要用到好一点的服务器,"如果crf训练等需要用到好一点的服务器
可以找我搞一个独立linux服务器环境给你使用

jimichan@gmail.com
"
data/dictionary/person/familyname.txt并不存在,"在FamilyName类中会去读取familyname.txt，然而这个文件并不存在
"
分词和句法依存的一些疑问,"比如样本数据如下：
李明和高洁一起来到岭南区的百货商场附近，联系了李师傅，手机号18622004501，共同乘坐了一辆牌照为京H56802的大众轿车去北京站。
1.手机、车牌这种号码分词和词性不准，能否将这类号码类分词纠正，能否扩展几个词性如：电话号码、车牌号码。
李明和/nr, 高洁/a 人名划分不对
2.句法分析参看了一下哈工大http://www.ltp-cloud.com/demo/，语义角色标注通过程序怎么获取？
"
高亮搜索结果位置偏差,"这个应该和 https://github.com/hankcs/hanlp-solr-plugin/commit/7a797a6ac899074f0725398ac00878eb178e6be4 这个issues 有关系吧？
"
修改核心词性词频词典,"比如你在data/dictionary/CoreNatureDictionary.txt中发现了一个不是词的词，或者词性标注得明显不对，那么你可以修改它，然后删除缓存文件使其生效。

请问，我现在修改了某词的词性，怎样才能使其生效？删除哪些缓存文件？？谢谢
"
DoubleArrayTrie的一个resize bug,"由于初始分配内存位65536*32为1M空间，所以程序中基本没有出发resize方法
但是如果将初始大小设置位10240 然后执行core词典build的时候
你会发现下面这个代码的 double l 会是个超级大数 1万多，会导致内存爆点。
你检查一下这边的逻辑是否有问题

```
    begin = pos - siblings.get(0).code; // 当前位置离第一个兄弟节点的距离
        if (allocSize <= (begin + siblings.get(siblings.size() - 1).code))
        {
            // progress can be zero // 防止progress产生除零错误
            double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0
                    * keySize / (progress + 1);
            resize((int) (allocSize * l));
        }
```
"
使用BitSet代替 boolean[] used 数组,"BitSet采用bit位，高效且避免空间占用
"
请问一下有没有把书名号，引号在分词的时候合并成一个词的情况加进来,"不会玩java，都不知道怎么改
"
分词错误,"您好，我在调用HanLP.segment(""我遗忘我的密码了"")进行分词时，发现分词不准确，结果为：[我/r, 遗/vg, 忘我/b, 的/uj, 密码/n, 了/ul]，而准确结果应该是：[我，遗忘，我，的，密码，了]，
这是什么原因？ 麻烦大神帮忙看看，谢谢。
"
v1.2.9 DemoCRFSegment error,"严重: 字符正规化表加载失败，原因如下：
java.io.FileNotFoundException: data\dictionary\other\CharTable.bin.yes (系统找不到指定的路径。)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.<init>(FileInputStream.java:138)
    at java.io.FileInputStream.<init>(FileInputStream.java:93)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:368)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:46)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:425)
    at com.madhouse.dsp.HanlpMain$$anonfun$1.apply(HanlpMain.scala:49)
    at com.madhouse.dsp.HanlpMain$$anonfun$1.apply(HanlpMain.scala:48)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    at com.madhouse.dsp.HanlpMain$.delayedEndpoint$com$madhouse$dsp$HanlpMain$1(HanlpMain.scala:48)
    at com.madhouse.dsp.HanlpMain$delayedInit$body.apply(HanlpMain.scala:18)
    at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
    at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
    at scala.App$$anonfun$main$1.apply(App.scala:76)
    at scala.App$$anonfun$main$1.apply(App.scala:76)
    at scala.collection.immutable.List.foreach(List.scala:381)
    at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
    at scala.App$class.main(App.scala:76)
    at com.madhouse.dsp.HanlpMain$.main(HanlpMain.scala:18)
    at com.madhouse.dsp.HanlpMain.main(HanlpMain.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
"
HanLP.parseDependency 问题,"HanLP.parseDependency(""美丽又善良的你被卑微的我深深的喜欢着"");
结果
美丽 --(定中关系)--> 你
又 --(状中结构)--> 善良
善良 --(并列关系)--> 美丽
的 --(右附加关系)--> 美丽
你 --(主谓关系)--> 喜欢
被 --(状中结构)--> 卑微
卑微 --(定中关系)--> 我
的 --(右附加关系)--> 卑微
我 --(主谓关系)--> 喜欢
深深 --(状中结构)--> 喜欢
的 --(右附加关系)--> 深深
喜欢 --(核心关系)--> ##核心##
着 --(右附加关系)--> 喜欢

其中这块应该不是主谓关系吧
你 --(主谓关系)--> 喜欢
"
请问词共现统计如何使用,"看到语料库工具里介绍有词共现统计功能，但是帮助文档里没有介绍，请问如何使用
"
将test*类名修改为Test*,"遵从Java的编程规范，将test_类名修改为Test_
"
char数组使用建议,"程序中多出使用
char[] chars = string.toCharArray()
然后通过下标访问char ，如 chars[i] 这类代码

其实JDK中toCharArray的具体实现是
 char result[] = new char[value.length];
        System.arraycopy(value, 0, result, 0, value.length);
相当于每次toCharArray就会多出堆上copy的动作

建议直接使用 
string.charAt(i)代替

测试结论：charAt方法在性能稍微优势的情况下，减少GC消耗

=========性能测试代码=============
public static void main(String[] args) {
        String string = ""abcedfghijklml"";
        int len = string.length();
        char[] cc = string.toCharArray();

```
    for(int k=0;k<len;k++){
        char x = cc[k];
    }

    for(int k=0;k<len;k++){
        char x = string.charAt(k);
    }
    long t1 = System.currentTimeMillis();
    for(int i=0;i<500000;i++){
        char[] c = string.toCharArray();

        for(int k=0;k<len;k++){
            char x = c[k];
        }
    }
    long t2 = System.currentTimeMillis();       
    for(int i=0;i<500000;i++){
        for(int k=0;k<len;k++){
            char x = string.charAt(k);
        }
    }
    long t3 = System.currentTimeMillis();

    System.out.println(t2-t1);
    System.out.println(t3-t2);
}
```
"
能否把繁简转换的字库分开？,"在进行繁简转换时会遇到以下问题：
1.一个简体字对应多个繁体字的情况，比如：
台    臺 檯  颱 台
几    幾 機 几
在字库里面写：
臺=台
檯=台
颱=台
现在如果只是转换简体 “台”，它会转成哪一个？最上面一个？

如果字库里面只记录第一条 “臺=台”  那如果繁体里面有 “檯” 字， 就转不了简体了。

请问可不可以把繁简转换和简繁转换的字库分开？
"
关于机构识别,"K,L,M,P,S,W这几个标注分别代表什么？我只找到了一部分的标注的意义。
但是这几个的意义在网页上找不到。
我最近手工标注了一批语料。
汇总的结果如下:
模式  正确  错误  总共  正确率
CD  513 2512    3025    0.169586777
GD  2055    564 2619    0.78465063
FD  1695    694 2389    0.709501884
CCD 555 1790    2345    0.236673774
有全部的标注语料，但是没贴出来。
有没有什么比较好的改进建议？
我目前想到的只有一些统计加规则的方法。比如：CD的正确率较低。就定一些类型C和D无法成词。
"
Question:关于portable,"您好，我想请教一下如果我在maven配置中version为portable的话是否本地不用再配置字典了（dictionary）？
"
Unable to start JVM at src/native/common/jp_env.cpp:54,"hancks你好，我在用python调用hanlp时，如果重复调用，程序会报错，并且提示：Unable to start JVM at src/native/common/jp_env.cpp:54，尝试过换编译器，未能解决，拜托给看看，先谢过！
代码如下：

![ail okn 15t 6 pkg petfn](https://cloud.githubusercontent.com/assets/19303343/15182773/c1d0a396-17c0-11e6-8d03-1d1e178586ec.png)
"
字典新增一列属性，代码修改量是不是很大。,"字典现在的格式是“词 词性 词频”，我想改为“词 词性 词频 专业词”，举例说明：
“查 v 1000” 改为 “查 v 1000 查询”，
"
繁体分词,"当前分词词典和模型好像都是基于简体中文的，效果不错。但是对于繁体中文,似乎效果不佳，即使加入一些词语，也还是会被当做简体中文出来。

```
    CustomDictionary.add(""捷運站"");
    CustomDictionary.add(""臺北"");

""臺北大眾捷運股份有限公司""
[臺/n, 北大/j, 眾/nz, 捷/j, 運/n, 股份有限公司/nis]
```

但是如果把文本转化为简体，分词就蛮不错的。

```
input = HanLP.convertToSimplifiedChinese(input);
[台北/ns, 大众/n, 轻轨/n, 股份有限公司/nis]
```
1. 是否有比较好的繁体词典推荐
2. 能否支持繁体中文分词（分词时候可以有繁体模型和简体模型）
3. 是否可以根据转化为简体的结果来分原来的句子（根据简体中文分词的位置）
"
"例子中""只有自信的程序员才能把握未来""","例子中""只有自信的程序员才能把握未来""，用MainPartExtractor提取主谓宾，结果是程序员才能把握未来，正确的结果应该是：程序员把握未来
是不是 ""才能"" 这个停用词没有启用，如果是，需要怎么启用。如果不是，请问问题出在哪？
"
弱问如何提取固定格式的文本？,"你好hankcs, 如日期[2016/m, 年/qt, 4/m, 月/n, 29/m, 日/b], 如何设定[m, qt, m, n ,m , b]规则来提取这样的日期格式文本。工具里有现成的么？因为用python，看java不太熟练。
bow
"
目前hanlp能对接ES么？,"<(￣︶￣)>！
"
超赞学习中,"超赞学习中
"
句法分析问题,"你好，请问用HanLp进行句法分析，是不进行句法成分标识的吗？
"
能否提供1.对系统架构做一个详细的讲解 2.代码文件和算法原理对应关系,
CRF识别的一个bug,"代码：
        Segment segment = new CRFSegment();
        segment.enableCustomDictionary(false);// 开启自定义词典
        segment.enablePartOfSpeechTagging(true);
        List<Term> termList = segment.seg(""更多采购"");
        System.out.println(termList);
        for (Term term : termList) {
            if (term.nature == null) {
                System.out.println(""识别到新词："" + term.word);
            }
        }

结果：
[更多/ad, 采购/null]
识别到新词：采购
但是采购这个词在词库中是存在的，并不是新词。
"
python 调用句法分析， 怎么获得对应的结果？,"使用python调用HanLP，用下面的代码调用得到了st， 请问怎么获得里面的内容呢？ 为什么用for word in st的方式获取不了。 python应该如何调用？ 有人可以写一个简单的例子吗，多谢！ （类似官网的例子：word.LEMMA, word.DEPREL, word.HEAD.LEMMA）

`st = HanLP.parseDependency(u""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"")
`
"
自定义词典的词数量或是档案大小是否有限制？,"hankcs,您好！

   想请教您自定义词典的词数量或是档案大小是否有限制？超过百万以上的词典是否可以载入？先前以solr5.2搭配载入200万以上的词典，系统会崩溃，想请您协助，谢谢

Vincent
"
CRF新词识别,"这个方法一旦开启自定义词典就无法识别新词了么？
Segment segment = new CRFSegment();
segment.enableCustomDictionary(true);// 开启自定义词典
所以enableCustomDictionary()必须设置为false?
感觉这样有点不太合理啊。
我还要吐槽！！！！
感觉对数字和字母的识别意义不大。如：
public static void main(String[] args) {
        Segment segment = new CRFSegment();
        segment.enableCustomDictionary(false);// 开启自定义词典
        segment.enablePartOfSpeechTagging(true);
        List<Term> termList = segment.seg(""    阿莫西林是""
                + ""TJGP-2015-ZP-5722,""
                + ""乐视超级手机能否承载贾布斯的生态梦 ""
                + ""停业整顿停业"");
        System.out.println(termList);
        for (Term term : termList) {
            if (term.nature == null) {
                System.out.println(""识别到新词："" + term.word);
            }
        }

```
}
```

结果：
识别到新词：  
识别到新词：阿莫西林
识别到新词：TJGP
识别到新词：-
识别到新词：2015
识别到新词：-
识别到新词：ZP
识别到新词：-
识别到新词：5722
识别到新词：,
识别到新词：贾布斯
识别到新词： 
识别到新词：停业

这些汉字的词，我觉得还行。但是这里各种非汉字都会被识别成新词。我觉得这个问题造成这个方法的可用性。大大降低啊。

我目前想做一个给很多新的文章，识别出文章里面新的词。
现有的方法：
使用过的方法。先分词，如果有一个一个的字没有成词。如：我的名字-李健博不会成任何词。就把他们组合起来。用HMM，或者CRF识别一下是不是新词。
优点：能够剔除上面[TJGP,2015]这样的词，召回应该是比较高的。
缺点：准确率低。大量的(实体名，尤其是人名，会混在里面)使得还需要大量人工。
创新方法：
用上面的方法和hanlp结合：
比如:李健博在hanlp中会被识别为人名，这样的词就不要了。
楼主觉得我这个创意咋样？因为我对hanlp不是太了解。所以希望楼主看一看这样做的可行性。
个人觉得hanlp的CRF方法很好，但是因为[TJGP,2015]这样的原因使得无法真正的投入使用，实在是十分可惜。
"
对简体字调用转为简体方法  输出有问题 ,"HanLP.convertToSimplifiedChinese(""更凶更猛"");   本来是想将这个简体词转化为简体  结果被返回为“更凶更勐”
"
"在nr.txt中添加了""才会有 A 1""为什么他还被识别为nr","识别的文章如下：

　　原标题：“原来严肃的说教还可以这么接地气”
　　随着网络的急速发展，人们发现，网络文化不仅是当代文化的重要组成，更是当代文化的一大亮点，它还能很好地吸收、融合传统文化，从而转化出独特的、受到网络时代广大群众喜爱的新文化形式、文化产品。
　　顺应时代潮流，才能有所作为。北京市多年积极指导、鼓励属地主要网站传承优秀传统文化，打造多彩网络文化，并发挥组织作用，协调属地主要网站联手举办丰富的网络文化活动。市网信办相关负责人表示，建设网络文化，必须坚持正确导向，积极宣传正能量，传播党的主张。但是，党的理论路线、方针政策往往庄重严肃，宏观性、理论性强，其大众化、普及性传播是个难点。近年来，北京网信办与属地主要网站充分沟通、合作，先通过专家讲座、编辑培训等方式，帮助网站将党的政策主张“揉碎”、“吃透”，再通过共同策划，用网络语言、网络表达方式进行梳理、呈现，传播效果明显。
　　这样的例子比比皆是，2013年，由新浪微博倡议的“光盘行动”成为当年的十大新闻热词。从这次行动的操作中可以看出积极运用新的方式，开发出受到网络时代大众喜爱的文化产品的重要意义。当初新浪微博接到的宣传任务是“倡导节约型社会”。考虑到“节约”二字太宽泛，不便操作，新浪微博的编辑们经过头脑风暴，提出了“光盘行动”这个形象、生动又贴地气的口号。“光盘行动”一经推出迅速得到广泛认可。网民成为“自来水”，自发呼吁亲人朋友践行“光盘行动”，形成了良好的社会效应。
　　2014年社会主义核心价值观的宣传报道中，属地网站通过深入学习、理解，将社会主义核心价值观通过网络语言进行解读。搜狐网的专题“你读得懂的社会主义核心价值观”，用俏皮幽默的动漫形式，从网民熟悉的生活角度切入，展示社会主义核心价值观在国家、社会和家庭三个层面的价值目标和准则，使其适合不同的人群去接收、消化，那些往日看上去就高大上的理论，拉近了和普通人的距离。
　　2016年全国“两会”报道中，新浪网将H5与音频嵌入技术结合，推出“听，习近平说”、“强哥这一年都批评了谁”。中华网提炼政府工作报告中的民生红利，用“H5+微信红包”的形式推出“你有一个来自李克强的红包”。搜狐打造全景策划“跟着代表上两会”，传递代表委员声音。新浪微博与VR结合的两会微直播，带领网民近距离看两会。这些精彩的产品大多在社交媒体形成了“刷屏”的现象。
　　相关负责人介绍，在互联网上传播党的声音，应该当好转换器。要紧跟网络发展，勇于创新、注重实效，快速学习新媒体手段，鼓励网站放手干，运用网络思维，采用网络语言，表达口语化、接地气，巧妙融合图片、图表、漫画、动漫、视频、H5、音频等新媒体手段，确保传播实效，打造充满活力的鼠标文化、移动文化，才能在网民生活工作中起到实实在在的指导作用。有网民留言表示，原以为严肃的说教，还可以这么接地气。本版文/本报记者 李泽伟
　　（北京青年报）
责任编辑：赵家明 SN146
人总在追求圆满。事情要做得圆满，交友要交得圆满，文字要写得圆满。只有在感觉到圆满之时，才会有真正的喜乐。
近日，美国总统奥巴马抵达沙特开启他的“告别之旅”，而结果却是无比冷清。与以往沙特国王亲赴机场“迎来送往”相比，奥巴马此次沙特之行可谓“寒酸至极”。
给领导写讲话稿的机会，材料狗的一生只有两次机会，第一没写好的时候，领导会很客气地把你叫到办公室来，和颜悦色地对你说：“小同志，这篇文章还有很多许多需要斟酌的地方，你再回去改改。”
中国大陆房地产市场启动于上世纪90年代中后期。如果以最高转让期限70年计算，较早一批商品房，实际剩下50年左右。
Copyright 
1996-2016 SINA Corporation, All Rights Reserved
新浪公司 
"
一个bug如果把机构名和数字识别同时打开,"切分下面这句话的时候会出exception
以每台约200元的价格送到苹果售后维修中心换新机（苹果的保修基本是免费换新机）
"
请问是否将来会加入情感分析接口,"1. 就是对文本进行一个情感值的量化分析，输出一个代表正负面情感的数值，比如0是绝对负面，1是绝对正面，输出的结果就在0~1之间这种类似的功能
"
发现新词汇,"如题，给一定的微博语料，能否提取出哪些词汇是新生词汇，如发现新的网络用语。这个功能能否实现。
"
简繁转换分歧词识别某些情况下转换错误,"今天在使用HanLP在做简繁转换的时候发现有如下分歧词识别错误：
""宏安地产"" -> ""巨集安地產""
""香港联交所主板IPO上市"" -> ""香港聯交所主機板IPO上市""

这明显是有问题的，我用chinese-utils的转换就是正确的，望修复
"
Python调用hanlp如何将HanLP.properties放图classpath中。,"这是项目文档说明：最后将HanLP.properties放入classpath即可，对于任何项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。

我的IDE是eclipse。请问python项目，应该将HanLP.properties放在哪里，怎么放入classpath中~
"
如何取自定义词表中最近似的单词？,"我现在自定义的字典中定义了“大王叫我来巡山”， 我输入了的字符为“大王派我来巡山”， 怎么样能让系统给出这个最相似的单词？ 能否指点一下多谢。
"
如何设置替换匹配,"我现在在自定义字段中设置了“中国好声音第一季”为音乐专辑类型， 我想后面无论是“中国好声音第二季”还是“中国好声音第三季”等等都可以识别为音乐专辑类型，这个该如何去做？
"
有个词语分的有误,"2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreDictionary load
信息: 核心词典开始加载:data/dictionary/CoreNatureDictionary.mini.txt
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
信息: data/dictionary/CoreNatureDictionary.mini.txt加载成功，85585个词条，耗时265ms
粗分词网：
0:[ ]
1:[江, 江苏]
2:[苏]
3:[句]
4:[容, 容人]
5:[人]
6:[ ]

2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
信息: 开始加载二元词典data/dictionary/CoreNatureDictionary.ngram.mini.txt.table
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
信息: data/dictionary/CoreNatureDictionary.ngram.mini.txt.table加载成功，耗时156ms
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
信息: 自定义词典开始加载:data/dictionary/custom/CustomDictionary.txt
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CustomDictionary <clinit>
信息: 自定义词典加载成功:83911个词条，耗时313ms
粗分结果[江苏/ns, 句/q, 容人/v]
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.common.CommonDictionary load
信息: 加载值data/dictionary/person/nr.txt.value.dat成功，耗时33ms
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.common.CommonDictionary load
信息: 加载键data/dictionary/person/nr.txt.trie.dat成功，耗时31ms
2016-4-12 16:18:01 com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
信息: data/dictionary/person/nr.txt加载成功，耗时313ms
人名角色观察：[  A 22202445 ][江苏 K 3 L 1 ][句 K 3 L 1 ][容人 A 22202445 ][  A 22202445 ]
人名角色标注：[ /A ,江苏/K ,句/K ,容人/A , /A]
江苏/ns
句/q
容人/v

照道理：应该分词成  江苏、句容、人  或者  江苏、句容人
"
初始化能否支持从Classpath读取data下的自定义字典元数据？,"如题，目前需要在hanlp.properties设置root的值为绝对路径，能否设置为classpath下的相对路径？
"
直接调用句法分析器，显示源码错误,"直接调用CRF句法分析
`System.out.println(CRFDependencyParser.compute(""把市场经济奉行的等价交换原则引入党的生活和国家机关政务活动中""));`
出现问题：

```
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.dependency.CRFDependencyParser.parse(CRFDependencyParser.java:123)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:78)
    at bistu.idcc.features.ParseDependency.main(ParseDependency.java:19)

```

请教一下，应该如何解决
"
NLPTokenizer不同语句分析的结果为什么会不同？,"List<Term> termList = NLPTokenizer.segment(""张三是我哥哥"");
 System.out.println(termList);

[张/q, 三/m, 是/vshi, 我/rr, 哥哥/n]

List<Term> termList = NLPTokenizer.segment(""张三教授正在教授自然语言处理课程"");
        System.out.println(termList);

[张三/nr, 教授/nnt, 正在/d, 教授/v, 自然/n, 语言/n, 处理/vn, 课程/n]
"
HanLP对于用户自定义词典的官方推荐方法是怎样的?,"你好,我在测试的时候,发现通过`CustomDictionary.add()`方法添加的新词不会自动持久化,没有动态修改二进制缓存文件.
HanLP官方推荐的方法是记录添加成功的自定义词语,然后在下次项目启动之前追加到自定义词典文件吗?
"
如何增加歌曲类的分词？,"比如说我想把长江之歌， 或者爱的奉献作为整个一个词可以吗？
"
依存句法分析问题,"试运行了一下1.2.7版本的demo，发现有问题。请看看怎么解决？
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.parse(NeuralNetworkDependencyParser.java:53)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.compute(NeuralNetworkDependencyParser.java:93)
    at com.hankcs.hanlp.HanLP.parseDependency(HanLP.java:407)
    at com.hankcs.demo.DemoDependencyParser.main(DemoDependencyParser.java:26)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Caused by: java.lang.NullPointerException at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.read_matrix(NeuralNetworkParser.java:294)at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.loadTxt(NeuralNetworkParser.java:166)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:136)
    at com.hankcs.hanlp.dependency.nnparser.parser_dll.<clinit>(parser_dll.java:34)
    ... 10 more
"
python开启JVM调用HanLP时报错Unable to start JVM at native\common\jp_env.cpp:60,"python开启JVM调用HanLP时报错Unable to start JVM at native\common\jp_env.cpp:60请问如何解决
"
关于HanLP计算文章相似度问题,"之前用Ansj，现刚发现HanLP，还不熟悉，但觉得HanLP非常不错。冒昧请教博主，我希望通过HanLP实现计算两个文章的相似度，盼大神提供使用HanLP实现文章相似度的大概思路或功能，感激不尽~我看https://github.com/ysc/word/tree/master/src/main/java/org/apdplat/word/analysis已有诸多算法实现，但我依然希望使用HanLP，可惜没有类似功能，着实遗憾，亟盼作者提供帮助。
"
提取关键词,"我有大量如下格式的文本

```
姓名：张三性别：男年龄18身-高180cm发 型-飞机头母亲姓名：张妈妈......
```

这里的“姓名 性别 年龄 身高 发型”仅仅为了说明，实际上有大量的类似关键词 ，且每个文本中关键词出现的次数是不定的(有的包含其中10个 有的包含其中一个) 但每个关键词最多只会出现一次(姓名和母亲姓名认为是两个关键词)
希望能够实现提取到所有关键词(移除关键词内的特殊符号如空格、-等)列表，同时能够在关键词处添加分隔符
如下

```
姓名：张三 性别：男 年龄18 身高180cm 发型-飞机头 母亲姓名：张妈妈......
```

是否可以使用您提供的库实现，可否给予一些专业意见
"
在phrase识别中能否指定最大到二阶共现？,
如何获得分词词频？,"使用segment的时候，分词的词频如何获得呢？
"
有没有一个完整一点的词性说明,
extractPhrase出现错误 RuntimeError: No matching overloads found. at src/native/common/jp_method.cpp:121,"调用 HanLP.extractPhrase(ss) 出现这个错误
"
hanlp.properties文件,"弱弱的问一句 这个文件是什么文件，为什么在github上面没有？
"
修正一个拼音,"将“nv3=n,u,3”改为“nv3=n,v,3”
"
汉字转拼音问题,"原文：吉林省长春药房,
拼音（数字音调）,ji2,lin2,sheng3,zhang3,chun1,yao4,fang2,
拼音（符号音调）,jí,lín,shěng,zhǎng,chūn,yào,fáng,
拼音（无音调）,ji,lin,sheng,zhang,chun,yao,fang,
声调,2,2,3,3,1,4,2,
声母,j,l,sh,zh,ch,y,f,
韵母,i,in,eng,ang,un,ao,ang,
输入法头,j,l,sh,zh,ch,y,f,

很明显zhang3这个拼音是错的，是否可以考虑加上分词来获取拼音呢？
"
三元组提取的问题," @hankcs 感谢你开发的HanLP工具，实在太棒了！

我刚学NLP，有个问题想向你请教。
我有这么个需求，输入是一个句子，输出是能表达句子语义的三元组（主谓宾）。例如：

原句：格言是简练而含义深刻并具有教育意义的警句。
结果：（格言，定义，简练而含义深刻并具有教育意义的警句）

原句：儿化具有区别词义、区分词性和表示感情色彩的作用。
结果：（儿化，作用，区别词义、区分词性和表示感情色彩）

原句：点号又分句末点号和句内点号。
结果：（点号，包含，句末点号和句内点号）

我看过你写的提取句子主谓宾的项目，所以我打算用类似的方法来做，但是自己刚入门NLP，不知道行不行。你有什么方法或者建议么？

谢谢！
"
人名识别不准确,"林军和大壮从侧门，被两个警察带进了市场旁边的一个大院内，随即直接进了办公楼里。

　　“这派出所挺大啊？”林军看着狭长的走廊，表情有些惊愕的说了一句。

　　“大哥，这是市局七处！”男警察斜眼回了一句。

　　市公安局第七刑侦大队，俗称七处，部门职责是主抓特大重点案件。

　　“咋给我们带这儿来了？”林军听后一愣。

　　“来，左边一个，右边一个，靠着暖气站好！”女警冲着林军和大壮，俏脸面无表情的说道。

　　二人听到这话，也没争辩，随后各自靠着暖气站了下来。而女警走进办公室取了两幅手铐，随即将二人分别铐在暖气管子上说道：“等着吧，一会派出所过来取你们俩。”

　　林军右手被铐上时,正好与女警脸对脸，随即他双眼本能的打量了女警一下。

　　她的长相有些特别，长发披肩，五官精致，但鼻梁很高，眼窝较深，一双灵动的大眼睛非常夺目。看着有点不像汉人五官，而是有点像史密斯夫妇中的安吉丽娜.茱莉！

　　女警身段挺直，个子起码一米七左右，上半身套着一件紧身的半袖警服衬衫，领口扣子系的一丝不苟。她下半身穿着黑蓝色宽松的长裤，脚上蹬着一双平底的黑色瓢鞋，整个人的气质给人一种充满活力，英姿飒爽的感觉。

　　“哎，郑警官，我俩这就是喝多了瞎闹腾，犯不上在你这儿占地方。你给我俩松开，我俩一块去派出所和解了得了。”大壮此刻已经有点被揍的醒酒了，他左手捂着还在淌血的嘴唇，随即含糊不清的喊道。

　　“闭上你的嘴，呆着！”女警厌恶的扫了他一眼，随后冲收发室喊道：“李叔，帮忙看一下，一会把他们交给派出所就行。”

　　“好叻！”　收发室的大爷回了一句。

　　随后女警踩着平底鞋就上了楼，而跟他一起的那个男警察转身再次去了市场，继续去给加班的同事买盒饭。

　　走廊内，工作人员来回穿梭，而林军和大壮相互对视了一眼。

　　“操你玛，你等出去的！我让你知道，你打我的那一酒瓶子有多无知！”大壮看着林军小声骂道。

　　林军将头扭过去，根本没回话。

　　......

　　四十分钟以后，派出所一个民警，带着一个二十六七的青年，并肩走进了走廊。民警进来以后，就直接走进了办公室，而青年腋下夹着包，脖子上挂着佛牌，手里搓着珠子冲大壮骂道：“一天净他妈给我惹事儿！”

　　“涛，你看他给我干的，嘴唇子都整豁豁了。”大壮指着自己的嘴唇子说道。

　　“你闭嘴吧！”青年回了一句，随后朝着民警走进的办公室走去。

　　二十分钟以后，民警和青年走了出来。

　　“王涛，谁是你朋友啊？”民警虎着脸，背手问道。

　　“就他！”叫王涛的青年指了指大壮。

　　民警扫了一眼林军和大壮，随即皱眉问道：“就这点破事儿，还用我调解啊？用验伤吗？”

　　“我不用！”大壮思考了一下，干脆的回道。

　　“我也不用！”林军扫了一眼三人，也面无表情的回道。

　　“真不用啊？”民警冲着林军再次问道。

　　“不用。”林军毫不犹豫的摇了摇头。

　　“打开，走吧！”民警随手拿着钥匙交给了王涛。

　　王涛接过钥匙，将大壮的铐子打开，然后又将钥匙扔给了林军。

　　“谭哥，麻烦了，明儿请你吃饭啊。”王涛笑着冲民警说道。

　　“轻点嘚瑟比啥都强，走吧，走吧。”民警淡然的摆了摆手。

　　“那我走了，谭哥！”

　　王涛冲民警打了个招呼，随后带着大壮扬长而去。林军摘下手铐以后，竖起大拇指冲民警说道：“这案子办的真利索！”

　　“你还有事儿啊？”民警回头，面无表情的问道。

　　“呵呵，没事儿。”林军放下手铐，随即头也不回的走出了七处。

　　.......

　　一个半小时以后，时间接近晚上七点多。

　　林军刚刚收拾完自己的小摊，并将烧烤用具放在了张小乐的三轮子上。

　　“今天不出了？”

　　张小乐站在一旁，张嘴问道。

　　“还出啥出，货都让他踩了。”林军有点心烦的回道。

　　“行，那我跟你把东西送回去。”张小乐穿着工作服，挺仗义的回了一句。

　　“不用了，你卖货吧，车借我用用就行。”林军骑上三轮子，咧嘴一笑说道。

　　“要不今天你别送了，东西直接扔我这儿得了。”张小乐明显有点担心的说道。

　　“呵呵。”林军一笑，也没多说，骑车就走了。

　　．．．．．．．

　　市场后方的小路上，林军健硕有力的双腿蹬着人力三轮车，顺着灯光昏暗的街道一路前行。

　　“咚咚咚！”

　　距离存放烧烤用具的车棚，还有一半路程时，小路对面突然泛起一阵农用三轮子的声响。

　　“吱嘎！”

　　林军踩了一脚刹车，右脚点地，眯着眼睛向前方望去。

　　“就那个傻Ｂ，一会给我往死怼他！”骑在三轮子上的大壮，满嘴漏风的大吼了一句。

　　大壮喊完，对方三轮车距离林军不超过二十米远后停滞，几乎同时，一副极为震撼的画面出现在林军眼中！

　　农用三轮子是摩托式的，马力很小，具体大小也就跟路边拉黑活的那种“摩的”差不多，而这车的车斗载重量，估计也就能拉几袋百斤重的大米。

　　但今天这个农用三轮子却突破了极限，就不足一米半长的车斗，竟然宛若春运火车车厢一般，拥挤得往下跳人！

　　一个，两个，三个.......

　　数秒过后，车斗之上竟然跳下来七个成年人！七个啊！天知道他们是怎么挤上去的，此场景即使跟印度三哥PK一下，那他妈也不差啥了！

　　“我操！”林军数着对方跳下来的人，脸色被雷的有点惊愕。

　　“呼啦啦！”

　　大壮跳下摩托车，右手从车斗中抽出一把片刀，随即带着七个人，手里拿着铁棍子，镐把子，镰刀，还有街头斗殴中百年难得一见的炉钩子等异样凶器，蜂拥着冲向林军。

　　“咣当当！”

　　林军下车，在自知无法躲避这场斗殴之时，立马回手从三轮车上抽出一根半米长的空心钢管，随后眉头都没皱一下，迈步就冲向人群。

　　双方碰见，基本没有废话，直接就开怼。

　　对方一个老农，抡着镰刀直接刨向林军，而林军侧身一闪，右臂摆动幅度很小，但右手攥着的钢管却闪电般的抽在了老农的手腕上。

　　当的一声，老农本能一缩手，林军手持钢管对着他脑袋，眨眼间就抽了三下，直接将其放倒。

　　其余众人冲上，林军左手抓过一人的脖领子，宛若拎着鸡崽子一般，直接将其摆在身前，随即他身体晃了一下，右手攥着钢管，对着旁边的大壮，反手就抽了过去！

　　“嘭！”

　　钢管抽在大壮嘴上，他疼的一蹦半米高。

　　“噼里啪啦！”

　　对方砸下来的武器，根本无处躲避的干在林军和对方那人的身上。

　　“往他手上砍！就照一万块钱干他了！”大壮捂着嘴，跳脚吼道。

　　林军额头，胳膊开始冒血，他左胳膊一甩，右腿一扫，直接将抓着的汉子绊倒。

　　“操你玛，我拿枪说话，拿刀吃饭的时候，你们还蹲地沟垄里唱东方红呢！”林军根本没管其他人，双手攥着钢管，胳膊卯足劲的往抡了数下。

　　“嘭！”

　　“嘭！”

　　“嘭！”

　　三声脆响，在林军身下这人的脑袋上，脖子上，后背上接连响起！

　　“呼啦啦！”

　　林军心黑手狠的干完这三下，人群顿时散开，众人看着他稍微有那么点犯怵！

　　“唰唰！”

　　与此同时，街口处有四台出租车匆忙赶来，这些车支着远光灯，停在路边。

　　“咣当！”

　　车门推开，张小乐扯脖子喊道：“军，谁他妈要干你啊？”

　　大壮团伙一看街口停了四台出租车，同时双眼又被大灯晃的看不清楚张小斌带来多少人，所以，他们第一时间掉头就跑，连能拉七个人的神奇农用三轮子都扔下了。

　　“咣当！”

　　林军脸不红气不喘的将钢管扔进自己的三轮车，随后伸手熟练的摸了一下后背。手指碰触皮肤，他感觉出后背没有刀伤，但回头再看右臂的时候，一个不足半指长的刀口，流着血，而皮肉已经翻开了。

　　“没事儿吧？”张小乐呼哧带喘的跑过来问道。

　　“没事儿，胳膊上划了一下。”林军拿起车上的餐巾纸，一下抽出了半盒的厚度堵在了伤口上，随即扭头冲着张小乐问道：“你都带谁过来的？”

　　“带个屁，四台出租车全是空的，现在的人，能借给你钱，就算好哥们了，哪有还能帮忙干仗的？”张小乐随口回了一句。

　　“谢了，乐乐！”林军愣了一下，随即认真的说道。

　　“谢的事儿回头再说，走吧，上医院看看！”张小乐拉着林军，继续说道：“他们这帮人，全是周边农村的，相互都认识，一会说不定叫来多少人！”

　　“他们跟谁玩的？”林军思考一下，直接问道。

　　“你要干啥啊？”张小乐一愣。

　　“这点破事儿不整明白了，我看是没完没了了。”林军低头回道。

　　“军，犯得上吗？”张小乐一听这话，顿时沉默几秒后皱眉问道。

　　“干都干了，你说咋整？今天要是没个结果，那明天我还能不能干活了？”林军简洁明了的回了一句，随即再次问道：“他们是跟谁玩的？”

　　“王涛。”张小乐思考了一下，随后还是如实相告。

　　“二十多岁，脖子上挂着佛牌儿，没事儿手里还愿意搓着珠子，是他吗？”林军脑中瞬间想起在七处走廊碰见的那个青年。

　　“对！”

　　“他不行，段位太低，他上面还有人吗？”林军摇头再问。

　　“大哥，你太狂点了吧？”张小乐愣了一下，随即惊愕的问道。

　　“这事儿跟你说不明白，一个段位，一个谈法！”林军干脆的回道。

　　“……王涛是跟满北伐玩的！”

　　“他在哪儿？”

　　“满北伐是整建筑的，手里有车队，人好像在江北望江别苑的三期工地里呢！”张小乐回了一句。

　　“谢了，你帮我把东西送回去，回来请你吃饭！”林军听完以后拍了拍张小乐的肩膀，随即转身就走。

　　两分钟以后，林军单人单骑，打了一辆出租车，直奔江北望江别苑。

这样一段文本，识别人名结果是：[林军, 史密斯, 安吉丽娜, 茱莉, 郑警官, 李叔, 操你玛, 都整豁, 王涛, 瑟比, 张小乐, 米高, 钱干他, 黑手, 张小斌, 连能拉, 都带谁]
请问识别出的这些不准确的，应该怎样才能尽量排除？
"
你好，我有个很棘手的问题，一个50M的String，给hanlp进行分词，hanlp的所有分词方法都试过了，总是报内存溢出，JVM设置的是1G，怎么办？,"Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.seg.CRF.CRFSegment.atomSegmentToTable(CRFSegment.java:251)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:48)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at Test.main(Test.java:70)
"
韵母识别问题,"用的hanlp-1.2.9+data-for-1.2.8-standard，用Java运行如下代码作为测试：
`String text=""依山会略句率学子"";`
`List<Pinyin> listPinyin = HanLP.convertToPinyinList(text);`
`for (Pinyin pinyin : listPinyin){`
`System.out.printf(""%s,"", pinyin.getYunmu());`
`}`
结果却是“i,an,ui,ue,u,v,ue,eng,”，最后“子”的韵母错了，这是为什么呢？
"
"使用HanLP搭配Solr 4.10與Tomcat 7.0.68，在使用""analysis""用來驗證中文分詞的結果，卻出現下列錯誤訊息""null:java.lang.RuntimeException: java.lang.AbstractMethodError""","您好，我使用HanLP搭配Solr 4.10與Tomcat 7.0.68，在solr admin 介面使用""analysis""用來驗證中文分詞的結果，卻出現下列錯誤訊息""null:java.lang.RuntimeException: java.lang.AbstractMethodError""，請問是什麼原因呢？我使用的插件版本為hanlp-portable-1.2.9.jar,hanlp-solr-plugin-1.0.3.jar，還請您協助我排除這個問題，謝謝！
"
维特比分隔识别人名的问题,"新华社专电（记者熊琳）《鬼吹灯》作者张牧野认为电影《九层妖塔》涉嫌侵犯著作权，将中国电影股份有限公司和导演陆川起诉至北京市西城区人民法院。记者20日从法院获悉，在受理该案后，应被告之一陆川申请，法院追加梦想者电影（北京）有限公司、乐视影业（北京）有限公司为本案共同被告。今年1月7日，《鬼吹灯》作者张牧野起诉称。电影《九层妖塔》系由《鬼吹灯之精绝古城》改编拍摄而成，但《九层妖塔》的故事情节、人物设置、故事背景均与原著相差甚远，超出了法律允许的必要的改动范围，构成对原著的歪曲和篡改，给原告造成了精神伤害，侵犯了原告的保护作品完整权。故将中国电影股份有限公司及陆川诉至法院，请求法院判令二被告立即停止侵权行为，向张牧野公开赔礼道歉、消除影响，并赔偿张牧野损失100万元人民币。本案立案后，被告之一陆川申请追加梦想者电影（北京）有限公司、乐视影业（北京）有限公司为共同被告。陆川认为梦想者电影（北京）有限公司与乐视影业（北京）有限公司为电影《九层妖塔》出品方即影片著作权人，应当作为共同被告参加本案诉讼。张牧野亦同意将梦想者电影公司和乐视影业追加为共同被告。西城法院在充分考虑原被告意见的基础上，为了便于查清案件事实，按照相关法律规定，已同意追加上述二公司为本案被告。

上面这段文本，在 PersonRecognition.Recognition.roleTag 后，熊琳 的角色标注为 [熊 B 885 D 16 C 4 E 4 K 1 ][琳 D 511 E 355 C 89 L 1 ]，这两个词是从词典里查出来的，但是在 PersonRecognition.Recognition.viterbiExCompute 后将 熊琳 的角色标注改为了 熊/D ,琳/L 。

这个是转移概率矩阵有问题吗？
"
用Hanlp中的NLPTokenizer分词时，对数据库中的45000数据没问题，当对50000数据分词后，就产生空指针错误！,"![image](https://cloud.githubusercontent.com/assets/17736168/13973846/ccb7b3a4-f0e1-11e5-8e26-f64a5a6d66d3.png)
![image](https://cloud.githubusercontent.com/assets/17736168/13973847/d2c1852c-f0e1-11e5-873e-9cc0a987cd0f.png)
有知道这是什么原因的么？
"
HanLP里有自定义词性的功能么？比如AnsjSeg中可以通过 FilterModifWord.insertStopNatures()来自定义词性,"我想给用户词典中的词定义为“userDefine”这个词性，比如AnsjSeg中可以通过 FilterModifWord.insertStopNatures(“userDefine”)来自定义词性，HanLP中除了修改Term.enum代码外，有没有直接可用的方法？
"
1.2.9出错：java.lang.ArrayIndexOutOfBoundsException: 64,"代码
try{
    termList = HanLP.segment(nr.toUpperCase());
}
catch (Exception e2) {
    e2.printStackTrace();
    continue;
}

出错信息
java.lang.ArrayIndexOutOfBoundsException: 64
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:140)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:101)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
"
python调用人名识别接口的问题,"我不太清楚是不是这样调用
Name_rec = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')
Name_rec.seg(u'北川景子参演了林诣彬的速度与激情3')
这样会出错，不知道怎么用这个接口
"
请问是否支持url识别？,"RT
"
baidu 盘墙外超级慢能不能提供别的链接?,"baidu 盘墙外超级慢能不能提供别的下载方法?
"
经常 Java heap space,"你好，程序跑一会之后，就经常报这个

```
java.lang.OutOfMemoryError: Java heap space
        at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:73)
        at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:245)
        at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:472)
```

是不是还要配置什么？
"
修改了hanlp.properties第一行的目录指向DATA的父目录，还是不能用句法分析,"jpype._jexception.LinkageErrorPyRaisable: java.lang.ExceptionInInitializerError
句法分析运行的时候报这个错误，请问怎么解决呀。
-Djava.class.path=C:\hanlp\hanlp-1.2.8.jar;C:\hanlp 这是HanLP.jar和HanLP.properties的位置。
root=C:/hanlp1  这是DATA的父目录。

C:\Users***\workspacex86\python1\src 这是项目的目录
请问要怎么解决呢，多谢了~
"
加载bin模型，提示outofmemory,"但加载txt模型没有问题.
环境：mac，idea
－xms也调的很大，不是－xms的问题。
crf＋＋版本0.53
"
最大熵依存句法分析器的实现,"hankcs,你说的字符串特征是什么？
"
viterbi所有概率取对数,"求解HMM模型，所有概率请提前取对数？什么原理
"
HanLP如何在Elasticsearch上使用？,"可否提供相应插件？
"
人名识别失败：习近平－－－git版本,"public class TestErrorSeg {
    public static void main(String[] args) {
        HanLP.Config.enableDebug();
        String str = ""习近平访美为全球经济注入新动力"";
        List<Term> terms = HanLP.newSegment().enableCustomDictionary(true).enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).seg(
                str);
        System.out.println(terms);
    }
}

人名角色观察：[  A 22202445 ][习近平 null][访美 L 3 ][为 L 1200 K 920 C 184 D 144 M 40 E 21 ][全球 L 11 K 1 ][经济 L 19 K 2 ][注入 K 3 L 2 ][新动力 A 22202445 ][  A 22202445 ]
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.algoritm.Viterbi.computeEnumSimply(Viterbi.java:254)
    at com.hankcs.hanlp.recognition.nr.PersonRecognition.viterbiExCompute(PersonRecognition.java:171)
    at com.hankcs.hanlp.recognition.nr.PersonRecognition.Recognition(PersonRecognition.java:50)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:76)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at org.nlp.segment.TestErrorSeg.main(TestErrorSeg.java:12)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
"
数组遍历越界－v1.2.8版本,"需要分词的字符串：“双十二”加油有折扣还有红包
分词方式： List<Term> terms = HanLP.newSegment().enableCustomDictionary(true).enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).seg(
                content.trim());

错误提示：Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 17
    at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
    at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:95)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at org.nlp.sentiment.ModelUtilsByRuleMap.calSentimentScore(ModelUtilsByRuleMap.java:177)
    at org.nlp.sentiment.ModelUtilsByRuleMap.calEmotionType(ModelUtilsByRuleMap.java:210)
"
hanlp分词，词性标注使用的语料库来源？,"之前只是在简单使用hanlp，现在想对自然语言处理有更深的了解。据我理解，像分词，词性标注等需要有一份提前标注好的语料库进行训练，不知道hanlp的来源是从哪里取得的？

我从hankcs的一些回答里知道应该是用人民日报的文本进行训练，但是人民日报的文本是生语料库，不知道hanlp是怎么处理的？
"
词典里收录很多动词＋名词合成的词是为了什么？,"比如”打篮球“这样的词出现在核心词典里，搞得分词总是不理想，特别是句法依存分析导致也很混乱。正常语义里，像这样的词不是应该分开考虑，这样主谓宾才能明确。
"
求教连接java的class丢失了，该如何找回？,"运行时突然出现jpype._jexception.ExceptionPyRaisable: java.lang.Exception: Class com.hankcs.hanlp.HanLP not found的错误，之前不小心卸载了python2换成3，现在又换成2就变成这样了，跪求高人的解决办法
"
jpype提示依存句法分析出错,"请问jpype._jexception.VirtualMachineErrorPyRaisable: java.lang.OutOfMemoryError: Java heap space
为何会超时呢？
"
三个人总是被错误区分为”三“”个人”,"句子里只要有“三个人”这样的词，总是会错误划分为“三”，“个人”。

我看CoreNatureDictionary里“个人”的词频很高，试着把它的词频降到１，发现也没有作用。只有把“个人”这个词条删除才能正确分词。可是“个人”确实是可以作为词出现，像这样的问题有办法解决吗？

我试了哈工大的语言云demo，发现是可以区分这两种情况的。
"
高手，问下词典自定义并怎么制作 ？,
人名识别角色标注的问题,"输入：区长庄木弟新年致辞
粗分结果[区长/nnt, 庄/ag, 木/ng, 弟/n, 新年/t, 致辞/vi]
人名角色观察：[  A 42634591 ][区长 G 1 K 1 ][庄 B 263 E 53 D 40 C 13 L 9 K 1 ][木 B 106 C 85 D 39 E 15 L 13 K 2 ][弟 D 60 E 11 K 10 C 2 ][新年 L 47 K 2 Z 1 ][致辞 L 13 ][  A 42634591 ]
人名角色标注：[ /A ,区长/K ,庄/B ,木/C ,弟/D ,新年/L ,致辞/L , /A]
识别出人名：庄木 BC
识别出人名：庄木弟 BCD

问题：
1.既然识别出正确人名庄木弟(BCD)，为何还要识别出庄木(BC)呢？直接选庄木弟(BCD)作为人名识别结果不可以吗？
2. 人名识别中，角色A表示其他角色。在语料中，词汇“新年”、""致辞""等应该都有可能存在A角色情况吧，怎么在角色观察中都没有A及其出现频次呢？作者是怎么生成词汇的角色&词频词典的？感觉有关A的数据都删除了吧？
"
地名识别模型的一点理解和困惑,"输入：“蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机”
输出：[蓝翔/nr, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红/a, 河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]
其中，【红河镇】识别错误。

探其原因，首先在粗分阶段，结果如下：
[蓝/a, 翔/vg, 给/p, 宁夏/ns, 固原市/ns, 彭/nz, 阳/ag, 县/n, 红/a, 河镇/ns, 黑/a, 牛/n, 沟/n, 村/n, 捐赠/v, 了/ule, 挖掘机/n]
然后，地名角色观察阶段：
[  S 1139590 A 23975 ][蓝 C 29 B 1 D 1 ][翔 C 50 D 10 A 5 ][给 A 453 B 79 X 6 ][宁夏 Z 41339414 ][固原市 Z 41339414 ][彭 C 85 ][阳 D 1255 C 81 B 1 ][县 H 6878 B 25 A 23 D 19 X 3 ][红 C 1000 B 46 A 3 ][河镇 Z 41339414 ][黑 C 960 B 25 ][牛 D 24 C 8 B 7 ][沟 H 107 D 90 E 36 C 27 B 14 A 3 ][村 H 4467 D 68 B 28 A 8 C 3 ][捐赠 B 10 A 1 ][了 A 4115 B 97 ][挖掘机 B 1 ][  B 1322 ]
最后，地名角色标注阶段：
[ /A ,蓝/C ,翔/D ,给/B ,宁夏/Z ,固原市/Z ,彭/C ,阳/D ,县/H ,红/B ,河镇/Z ,黑/C ,牛/D ,沟/E ,村/H ,捐赠/B ,了/A ,挖掘机/B , /A]

由于粗分后，""红河镇""被分为了""红/河镇""，而不是""红/河/镇""，那么在地名角色观察阶段，词“河镇”就没法用地名角色简表中的角色标记，“河”应该是D(中国地名第二个字)，""镇""应该是H(中国地名的后缀)，""河镇""应该是DH吧？即地名的尾部与后缀成词的现象，是不是要单独在设定一个新的角色标记？由于地名识别是在句子粗分词的基础上做的，所以地名识别的输入存在单词的情况是在所难免的，想必也会出现诸如“中国地名第一个字与第二个字成词”的情况，现有的地名角色简表也无法涵盖。

如果我理解错了，即""河镇""不是DH，那又是什么呢？我参看了 @hankcs 的blog关于地名识别的文章，其中地名角色观察阶段的“河镇”的角色是[H 1000]，不仅如此，""宁夏""、""固原市""的角色也是[H 1000]，我猜测作者可能是笔误，在HanLP代码中H代表中国地名后缀，而G代表整个地址，我猜测作者在blog上的意思是“河镇”、“宁夏”、“固原市”的观察角色都是真个地址，即G。如果是这样，那么为什么在我跑的程序中，“河镇”、“宁夏”、“固原市”的观察角色都是Z呢？作者是不是在地名辞典ns.txt中不加入“河镇”、“宁夏”、“固原市”作为地名的角色，而是只靠在核心词典中将这些常用地名识别出来，是这样的意图吗？
"
翻译人名、日本名识别为啥不采用HMM？,"中国人名识别采用HMM模型，而翻译人名、日本名识别采用规则来处理。
作者在翻译人名、日本名识别时有考虑采用HMM处理吗？是不是效果不好转而采用规则来识别呢？
"
PlaceDictionary调用的疑问,"句子""蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机""粗切分后得到 ""[蓝, 翔, 给, 宁夏, 固原市, 彭, 阳, 县, 红, 河镇, 黑, 牛, 沟, 村, 捐赠, 了, 挖掘机]。在地名识别阶段，在PlaceRecognition.java第109行涉及从地名词典ns.txt中读取地名的NS枚举类型的代码，如下：
EnumItem<NS> NSEnumItem = PlaceDictionary.dictionary.get(vertex.word);
问题是，我发现当程序执行到vertex.word是“宁夏”时，返回的NSEnumItem为null，而我查了ns.txt发现存在记录""宁夏 G 351""。请问是该返回null吗？我怎么觉得理应该返回""G 351""呢？哪里出现问题了？
"
Update TransformMatrixDictionary.java,"分母totalFrequency表示所有状态的总频数，我觉得应该是start_probability[from]，即状态from的频数。
"
读取状态转移矩阵模型时的代码计算有误吧,"package com.hankcs.hanlp.dictionary的class文件TransformMatrixDictionary.java：
第139-140行：
double frequency = matrix[from][to] + 1e-8;
transititon_probability[from][to] = -Math.log(frequency / totalFrequency);
这段代码应该是计算两个状态from,to的转移概率p( to | from )=#(from,to)/#(from)
分母totalFrequency表示所有状态的总频数，写错了吧？我觉得应该是start_probability[from]，即状态from的频数。
第140行代码应该是：
transititon_probability[from][to] = -Math.log(frequency / start_probability[from]);
"
"为什么在机构名识别代码中对nis词性hard code了K, 1000和D, 1000","为什么在机构名识别代码中对nis词性hard code了K, 1000和D, 1000，
导致分词有问题
 ""2007年1月9日，阿里巴巴集团在上海宣布旗下公司阿里软件正式成立。"",
N-最短分词：[2007年/t, 1月/t, 9日/t, ，/w, 阿里巴巴集团/nt, 在/p, 上海/ns, 宣布/v, 旗下/d, 公司阿里软件/nt, 正式成立/v, 。/w]
最短路分词：[2007/m, 年/qt, 1/m, 月/n, 9/m, 日/b, ，/w, 阿里巴巴集团/nt, 在/p, 上海/ns, 宣布/v, 旗下/d, 公司阿里软件/nt, 正式成立/v, 。/w]
"
HanLP词频,"HanLP词频怎么统计，rank没有加入词频权重。如何实现 ？
"
进行分词测试，怎么屏蔽自带的词典的影响,"我想要用SIGHAN Bakeoff 2005测试不同的分词算法的效果。这里要屏蔽Hanlp自带的词典的影响，我看data里有好多词典，要怎么做才行。

目前我的想法是用自定义的词典替换掉CoreNatureDictionary.txt，但是不知道其他词典对分词有什么影响？
"
Merge pull request #2 from hankcs/master,"Merge 02/01/2016
"
用相同的词库，比对了1.2.6和2.8，发现6效果好,"比如2.8分出4个何文，2.6分出何文全，j何文桂，何文玲，何文莲
"
Merge pull request #1 from hankcs/master,"update
"
命名实体识别问题," 文本 = ""交易账号：96600009934569490中国银行股份有限公司济南"";

分词结果 [交易/vn, 账号/n, ：/w, 96600009934569490中国银行股份有限公司济南/nt]

BasicTokenizer中SEGMENT = HanLP.newSegment().enableAllNamedEntityRecognize(true).enableCustomDictionary(true);

貌似只开启命名实体识别或是自定义词典中的一个就能将数字和机构名分开，若两个同时开启则当做一个整体，这个问题怎么解决呢
"
建议增加字符标准化方法,"把英文、数字的全角字符都转换为半角格式。
"
jvm内存问题,"有个问题很困惑，是不是jvm比较占内存，我在用python时，还是比较平稳的，但是在用jvm调用hanlp时，就会飙升上去
"
词典读取错误,"一月 19, 2016 3:55:39 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典D:/Workspace/NLP/data/dictionary/custom/word.txt读取错误！java.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.L

配置文件如下

root=D:/Workspace/NLP/
# 核心词典路径

CoreDictionaryPath=data/dictionary/CoreNatureDictionary.txt
# 2元语法词典路径

BiGramDictionaryPath=data/dictionary/CoreNatureDictionary.ngram.txt
# 停用词词典路径

CoreStopWordDictionaryPath=data/dictionary/stopwords.txt
# 同义词词典路径

CoreSynonymDictionaryDictionaryPath=data/dictionary/synonym/CoreSynonym.txt
# 人名词典路径

PersonDictionaryPath=data/dictionary/person/nr.txt
# 人名词典转移矩阵路径

PersonDictionaryTrPath=data/dictionary/person/nr.tr.txt
# 繁简词典路径

TraditionalChineseDictionaryPath=data/dictionary/tc/TraditionalChinese.txt
# 自定义词典路径，用;隔开多个自定义词典，空格开头表示在同一个目录，使用“文件名 词性”形式则表示这个词典的词性默认是该词性。优先级递减。
# 另外data/dictionary/custom/CustomDictionary.txt是个高质量的词库，请不要删除

CustomDictionaryPath=data/dictionary/custom/word.txt n;data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf;
# CRF分词模型路径

CRFSegmentModelPath=data/model/segment/CRFSegmentModel.txt
# HMM分词模型

HMMSegmentModelPath=data/model/segment/HMMSegmentModel.bin
# 分词结果是否展示词性

ShowTermNature=true
"
在linux下python时JVM路径问题,"我用了绝对路径，还是找不到，jpype._jexception.RuntimeExceptionPyRaisable: java.lang.RuntimeException: Class com.hankcs.hanlp.HanLP not found，不知道怎么解决
"
词性标准和依存句法分析问题,"您的词性标准是自己定义的还是参考某标准？谢谢
"
询问有关基于字标注的2阶HMM模型的学习资料,"我看到包`package com.hankcs.hanlp.seg.HMM;`中2-orderHMM的实现，但是作者好像没有介绍过，有没有一些书面的资料or解释可供参考？
"
运用简繁转换到安卓应用当中时txt字库的问题。,"我现在把整个工具的jar包导入到了安卓项目当中，不过txt等字库文件都还在电脑里，在java代码里调用简繁转换没有问题，但是请问怎么把txt放入安卓应用中？
因为jar包当中的路径是死的，我改不了，不然就可以把txt放入手机，路径改成相应的位置了。
"
词典中Frequency数值的意义,"请问词典中Frequency数值具有什么意义，是怎么来的，以及其大小对分词结果具有什么影响？
"
关于添加词典的问题,"我新添加一个词典，出现这个错误
严重: 自定义词典D:/hanlp/data/dictionary/custom/word.txt读取错误！java.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.INT'L，这是怎么回事呢
"
求问使用自定义辞典对初始分词结果修改的机制,"v1.2.8版本下的标准分词结果：
String text = ""攻城狮逆袭单身狗"";
System.out.println(HanLP.segment(text));
分词结果是：攻城/狮/逆袭/单身/狗

但若增加了自定义词，如下：
CustomDictionary.add(""城狮"",""nz 100000"");   //假设""城狮""是词
CustomDictionary.add(""单身狗"");
分词结果是：攻城/狮/逆袭/单身狗
单身狗的分词是对的，可是为什么没有把“城狮”分成一个词呢？
我知道现在的HanLP标准分词是先对句子粗分词，再利用自定义辞典进行修正。那么现在自定义辞典修正的基本思路是什么？(相关部分代码我没看明白)
自定义词的词频对修正有影响吗？(为什么我给""城狮""的词频设为100000还是没法分出来？)
"
维特比算法分词的平滑问题,"MathTools.java中有静态函数`calculateWeight(Vertex from, Vertex to)`计算两个结点的转移概率：
line 40: `double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`
既然有了平滑参数dSmoothingPara，为何要用dTemp调整nTwoWordsFreq / frequency的结果呢？
"
CharTable转换有误,"下载的v1.2.8版本，CharTable.java中利用CONVERT对字符转换，偶然发现：
全角和半角的逗号、问号、感叹号都会转换成全角的句号。
其他的还没测试。
"
词性错误问题,"比如“苏宁电器集团增持苏宁云商0.4%股份”得到的“增持”是nz，应该是动词，我想修改这个，应该怎么做
"
词性nx与nz的区别是什么？,"在Nature.java中说nz是其他专名，nx是字母专名，这是什么意思？另外，在包com.hankcs.hanlp.utility的Predifine.java预定义了
`/**
             * 专有名词 nx
             */
            TAG_PROPER = ""未##专"";
`
而没有预定义nz。
我在人民日报语料库中发现“人民网”的词性是nz，不是nt。
那么在程序中，nt、nz、nx到底是如何区别的？
"
当分词模型加载失败时，直接导致Tomcat死掉,"错误重现方法：
1.使用hanlp-1.2.8-release.zip中带的hanlp.properties，仅修改root属性
2.使用data-for-1.2.8-standard.zip（如果用full版date包则不会出现此错误）

建议：
1.应用内部发生错误时不应导致Tomcat死掉，建议增加相应错误预防处理机制或友好的异常机制，例如加载前先判断文件是否存在
2.standard和full分别提供hanlp.properties参考文件

参考日志堆栈信息如下（Tomcat版本8.0.18，不知与Tomcat有无关系）：
12-Jan-2016 01:05:47.794 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 13663 ms
12-Jan-2016 01:08:31.672 SEVERE [http-apr-8080-exec-2] com.hankcs.hanlp.model.CRFSegmentModel.<clinit> CRF分词模型加载 C:/xxxx/demo_prj/WebContent/WEB-INF/hanlp_data/data/model/segment/CRFSegmentModel.txt 失败，耗时 9 ms
12-Jan-2016 01:08:31.679 INFO [Thread-3] org.apache.coyote.AbstractProtocol.pause Pausing ProtocolHandler [""http-apr-8080""]
12-Jan-2016 01:08:31.684 INFO [Thread-3] org.apache.catalina.core.StandardService.stopInternal Stopping service Catalina
12-Jan-2016 01:08:33.857 WARNING [localhost-startStop-2] org.apache.catalina.loader.WebappClassLoaderBase.clearReferencesThreads The web application [demo_prj] is still processing a request that has yet to finish. This is very likely to create a memory leak. You can control the time allowed for requests to finish by using the unloadDelay attribute of the standard Context implementation. Stack trace of request processing thread:
 java.lang.Object.wait(Native Method)
 java.lang.Thread.join(Thread.java:1245)
 java.lang.Thread.join(Thread.java:1319)
 java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
 java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
 java.lang.Shutdown.runHooks(Shutdown.java:123)
 java.lang.Shutdown.sequence(Shutdown.java:167)
 java.lang.Shutdown.exit(Shutdown.java:212)
 java.lang.Runtime.exit(Runtime.java:109)
 java.lang.System.exit(System.java:968)
 com.hankcs.hanlp.model.CRFSegmentModel.<clinit>(CRFSegmentModel.java:43)
 com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:49)
 com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
 org.apache.jsp.hanlp.index_jsp._jspService(index_jsp.java:315)
 org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
 javax.servlet.http.HttpServlet.service(HttpServlet.java:725)
 org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:431)
 org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:396)
 org.apache.jasper.servlet.JspServlet.service(JspServlet.java:340)
 javax.servlet.http.HttpServlet.service(HttpServlet.java:725)
 org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291)
 org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
 org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
 org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239)
 org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
 org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219)
 org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106)
 org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
 org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142)
 org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79)
 org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:610)
 org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88)
 org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:516)
 org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1086)
 org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:659)
 org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:285)
 org.apache.tomcat.util.net.AprEndpoint$SocketProcessor.doRun(AprEndpoint.java:2431)
 org.apache.tomcat.util.net.AprEndpoint$SocketProcessor.run(AprEndpoint.java:2420)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
 java.lang.Thread.run(Thread.java:745)
12-Jan-2016 01:08:33.919 INFO [Thread-3] org.apache.coyote.AbstractProtocol.stop Stopping ProtocolHandler [""http-apr-8080""]
"
核心词典生成的示例代码出现错误,"HanLP版本v1.2.8，示例代码是项目wiki页面用于训练HMM-NGram模型的代码：
`
        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();
        CorpusLoader.walk(""path/to/your/corpus"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(Document document)
            {
                dictionaryMaker.compute(document.getComplexSentenceList());
            }
        });
        dictionaryMaker.saveTxtTo(""data/test/CoreNatureDictionary"");
`
错误原因是dictionaryMaker.compute()输入的链表存在CompoundWord，而Precompile.java中95行：
`
    public static Word compile(IWord word)
    {
        return compile((Word)word);
    }`
强行将word转为Word所致。

所以我想问，HanLP的API dictionaryMaker.compute()以后打算接受CompoundWord吗？现在wiki页面的调用会出问题。
"
关于基于神经网络的高性能依存句法分析器的问题,"您好，请问NNParserModel.txt.bin是如何训练出来的，训练部分的代码具体在哪
"
中文数字切断,"hanlp.ViterbiSegment(u‘十二、完成总经理交办的其他工作。’)

十 m
二 m
、 w
完成 v
总经理 nnt
交办 vn
的 ude1
其他 rzv
工作 vn
。 w
"
关于自定义词典修改后，编译的文件变大了,"为什么我自己修改自定义词典后，重新编译，文件比以前大很多呢，难道编译时有特殊方法
"
关于句法解析模型的问题,"你好！请问句法解析模型能否开放，学生在做句法解析调试模型时，缺少模型程序无法跑，非常感谢！
"
能否提供中国少数民族的姓名的拾取，精度如何。,"您好，目前使用您的项目测试中国少数民族姓名拾取情况，精准度不是太好。
如维族、蒙族、藏族人的姓名，希望能够为中国的人名称提取优先做一定的优化，谢谢。
"
怎么关掉extractKeyword中的输出呀？,"想减少系统的开销。。。
"
.jar需要放在哪里呀？,
依存分析时词性为“x”时报错。,"NeuralNetworkParser.java 461行 在词性为""x""时postags_alphabet.idOf(data.postags.get(i))返回null
貌似是字典树里没有存""x""这个词性标记,模型里不存在“x”这个词性？
"
关于句法分析的问题,"我现在可以利用hanlp进行句法分析，我看到你另外一个项目https://github.com/hankcs/MainPartExtractor，是关于提取句子主干的，我想问一下，我得到句法分析结果以后，我怎么来提取句子主干呢
"
女士、先生、老师（姓+职位）之类的人名无法识别,"例如：
赵科长 
李女士 
朱女士 
张老师 
张女士 
侯女士
相似的应该还：王医生、周律师等 
这点是否可以改进下
"
如何在python中使用CRF分词？,"就是在JClass导入这里，我不清楚导入哪个可以使用CRF分词
"
怎么删除缓存？,"```
       本人以训练新的词典，怎么删除项目中的缓存使其生效？谢谢
```
"
人名识别错误,"反馈一部分人名识别的问题:
我爱杨云宵
龙与凤的传说
陌上花开缓缓归
鱼和水的故事
陕西榕怡公司
陈雯晴要努力
"
python 调用时候出现的乱码问题,"本人环境是win7 64bit，python2.7.*版本，按照http://www.hankcs.com/nlp/python-calls-hanlp.html用的时候，在输出print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))时出现乱码，问下怎么可以解决，现在就是卡在<class 'jpype._jclass.java.util.ArrayList'>（(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))的数据类型是<class 'jpype._jclass.java.util.ArrayList'>）数据类型往python转化时不知道怎么做，对java一点都不了解。
"
你好，最近遇到一个关于hanlp.properties的问题,"我使用的IDE是Eclipse，配置好jar包和hanlp.properties文件后，能够正常使用，但在下次打开的时候，偶尔会出现hanlp.properties文件无缘无故的消失的问题，这个是怎么一回事啊？
"
关于自定义词典的问题,"您好 我配置好了property，使用CustomDictionary.add(""攻城狮"");和CustomDictionary.insert 方法插入，这两种方法有什么区别吗？貌似都是缓存的形式，当有insert语句时能够正确分词，去掉这句重新运行就不行了
"
关于DemoSuggester的最相似问题,"我采用的是3.3.1的版本,DemoSuggester类中查找相关度的句子,
比如说:[今天我吃饭了,今天我没吃饭]
查找关键词是吃饭,2个句子都出来了,能做否定词过滤吗?
在其次我查找没吃饭,吃饭也不因该出来.
"
您好，我在使用最新版本1.2.7的依存句法分析的时候遇到一个bug,"使用的是CRF的依存句法分析，出现了空指向异常：
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.exactMatchSearch(DoubleArrayTrie.java:674)
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.exactMatchSearch(DoubleArrayTrie.java:660)
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.get(DoubleArrayTrie.java:949)
    at com.hankcs.hanlp.model.bigram.BigramDependencyModel.get(BigramDependencyModel.java:108)
    at com.hankcs.hanlp.model.bigram.BigramDependencyModel.get(BigramDependencyModel.java:121)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.parse(CRFDependencyParser.java:153)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:78)
    at com.sxl.extractor.ParsingBaseNP.main(ParsingBaseNP.java:34)

请问，如何解决？
"
股份，繁体转简体,"股份，繁体转简体，变成了股分。

股份其实是一个简体，调用了HanLP.convertToSimplifiedChinese(""股份"")
"
能否指定使用一个用户词典,"请问对于一个分词，能不能指定使用一个词典进行分词，而且只用这一个用户词典，不使用核心词典等其它词典
"
请问能否使用户词典的优先级高于核心词典,"比如核心词典中有""坐火车 v 187""，而我想把它分为""坐   火车""，我在用户词典中加了""坐 v 1024"",""火车 n 1024""，但是并没有用。
"
CustomDirectionaryPath中存在的bug,"hi，hankcs:
     我现在使用的是1.2.7版本的HanLP，当我在增加一个自定义的词典的时候发现一个bug。我直接在行尾添加"" 追加名词.txt;""，结果在加载的时候定位到了data/dictionary/person/追加名词.txt。然后发现在/hanlp/HanLP.java中的201行到209行的实现中，值""data/dictionary/person/nrf.txt nrf""将行首获得的路径更改了（data/dictionary/custom/），这样就带来了一个问题，虽然这样可以在custom中调用其它路径下的词典（非data/dictionary/custom/下），但是会直接更改之后的词典的默认路径。
    我觉得可以强制只使用“data/dictionary/custom/”即第一个值的目录路径，或者在文档中注明custom词典添加格式（修改默认路径时"";""之后不能有空格，且之后的词典默认路径为此路径。添加默认路径下的词典时，"";""之后要有空格）。
    由于第二种方式不用改代码，所以先讨论下，再看看要不要来个Pull request。
   然后为什么不在项目中放上hanlp.properties呢，我找了好久没找到:)
"
无法在代码中配置数据目录，能否添加setProperty的方式在代码中自由使用,"作为Lib，直接调用jar，进行分词时，无法像你说的那样用配置文件配置data的目录，导致类似如下问题发生：

ERROR HanLP: HMM分词模型[ data/model/segment/HMMSegmentModel.bin ]不存在

看到HanLP里有一段
public static final class Config 写死了词典和模型数据的地址，尽管有一个root通过配置文件实现了设置，但是还不方便，能否加一个类似setProperty的方式，在代码中自由使用？

谢谢。

PS: 我用的scala
"
语义距离模块计算方法,"请问一下语义距离计算方法，具体原理是什么呢？有具体参考文献么？O(∩_∩)O谢谢
"
修改词性,"在一般的地址里 “号” 这个词是很特殊 比如 150号港汇广场 就成了 150号港/ns 了，怎么把""号""改成个名词，希望的结果是 150/m,号/n，港/n这样的就好。
"
建议文件读写的统一,"很多地方的文件读写都没有统一使用IOUtil类中的方法，
建议统一一下，
这样放到hadoop集群上运行就只需要统一修改IOUtil就可以了！
"
for a unexpected crash if the custom dictionary is empty.,
请教，如何做自己的词典？,"你好！
我打算使用 HanLP 做自己的词典，我仔细认真的阅读了官方文档，谁知文档巧妙的避开了这些内容，所以在这里请教下，怎么使用 HanLP 做自己的词典呢？

  顺颂
教祺
"
SetPropertiesFilePath,"## 添加指定PropertiesFile文件路径的方法。

由于IKVM转换的DLL文件暂无法支持java的getContextClassLoader().getResourceAsStream方法。出此下策，直接指定properties文件位置。添加此功能后，IKVM转换后的DLL文件可被.net程序直接使用。我试验了多种分词方法和文档摘要方法，均可执行。
"
关键词提取使用其他Tokenizer,"现在 TextRankKeyword 的代码里是使用 StandardTokenizer来进行分词的

https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/summary/TextRankKeyword.java#L45

我想问一下

**1. 能不能使用其他Tokenizer 比如  IndexTokenizer 还是因为某些原因只能用StandardTokenizer**, nlp我以前没有接触过...

比如:

``` java
public List<String> getKeyword(String content)
    {
        List<Term> termList = IndexTokenizer.segment(content);
        List<String> wordList = new ArrayList<String>();
```

**2. 返回全部的关键词和对应的rank, 现在的实现是从完整的结果的Map里取一部分(当size大或者结果少时是全部), 而且只取了key并没有给rank**

https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/summary/TextRankKeyword.java#L105

比如:

``` java
Map<String, Float> sortedScore = new HashMap<String, Float>();
// ... sort score
return sortedScore;
```

**3. 加入新的Term和工具类，在返回分词结果的同时返回部分(全部)term的rank. 因为调用TextRank时也要分一次词。或者允许传入已经分好的词, 第一个参数可以是 List<Term> , 第二个是文本**

比如

``` java
// 返回全部分词结果和对应的rank
public Map<String,Float> getTermAndRank(String content);
// 返回部分分词结果和对应的rank
public Map<String,Float> getTermAndRank(String content, Integer size);
// 使用已经分好的词来计算rank
public Map<String,Float> getRank(List<Term> termList, String content);
```

谢谢~ 
"
关于歧义词典,"hankcs，你好，

```
  我想问下现在的hanlp中分词是否支持歧义词典。例如 “远征求出货”，若按正常分词分成 [远/a, 征求/v, 出货/vi] ，但是我想分成 远征，求， 出货。 问是否有类似功能提。

谢谢。BTW，看过你网站的about，新生崇拜！
```

Thanks
pan
"
Portable同步升级到v1.2.7,
使用HMM分词后，结果的词性标注都为null是为什么,"使用的是HanLP-1.2.6，在运行DemoHMMSegment.java时，分词的词性结果都为null
"
Added the missing pick_sentence function,"Added the missing pick_sentence function
"
Add new methods in TextRankSentence class,"Add new methods in TextRankSentence class to support the newly added text summary API (which is based on user specified summary length). 

The return type of the method will be String, which is human-readable text based on the original text sequence.
"
Add a new text summary API,"Add a new text summary API, this API can output summarized text based on the maximum summary length (character length) specified by the user.
"
微调机构名识别模型,"新开头的一些机构的名称，“新”误被识别为“A”，成为机构的前缀词
"
微调机构名识别模型,"新开头的一些机构的名称，“新”误被识别为“A”，成为机构的前缀词
"
请教添加用户字典的问题,"首次使用NLP，请求帮忙。
这里有一个用户字典的问题，发现识别“张克智与潍坊地铁建设工程公司”里的机构实体出现以下结果：
[张克智/nr, 与潍坊地铁建设工程公司/nt]
想利用用户词典将“潍坊地铁建设工程公司”加到用户字典里，补充了/data/dictionary/custom/机构名词典.txt,添加了如下一行：
潍坊地铁建设工程公司 nt 1
在代码里设置了（enableCustomDictionary(true)), 发现仍然没有识别成功。
请教下这个做法对不对？谢谢！
"
［Reopen］HightLighter定位不准确的问题,"lucene插件已经更新到新版本，运行HighLighterTest.java没有问题。
但是，修改HighLighterTest.java的检索内容之后，还是会出现定位问题，麻烦帮我看一下，多谢。
下面是修改示例代码之后的内容，只是在内容中多加了几个 ""\n""   ：
  IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
            iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
            iwriter = new IndexWriter(directory, iwConfig);
            {
                // 加入一个文档
                Document doc = new Document();
                doc.add(new TextField(fieldName, ""我白天是一名语言\n\n\n\n学习者，晚上是一名初级码农。空的时候喜欢看算法和应用数学书，也喜欢悬疑推理小说，ACG方面喜欢型月、轨迹。喜欢有\n\n\n\n\n思想深度的事物，讨厌急躁、拜金与安逸的人。目前在魔都某女校学习，这是我的个人博客。闻道有先后，术业有专攻，请多多关照。你喜欢写代码吗？"", Field.Store.YES));
                doc.add(new TextField(""title"", ""关于hankcs"", Field.Store.YES));
                iwriter.addDocument(doc);
            }
            {
                // 再加入一个
                Document doc = new Document();
                doc.add(new TextField(fieldName, ""\n\n   \n\n\n\n\n\n\n\n\n程序员喜欢黑夜"", Field.Store.YES));
                doc.add(new TextField(""title"", ""关于程序员"", Field.Store.YES));
                iwriter.addDocument(doc);
            }
            iwriter.close();

运行结果如下：

test
Query = text:喜欢
命中：2
关于程序员 , 0.2972674

程序员<font color='red'>喜欢</font>黑夜
关于hankcs , 0.124633156
我白天是一名语言

学习者，晚上是一名<font color='red'>初级</font>码农。空的时候喜欢看算<font color='red'>法和</font>应用数学书，也喜欢悬疑推<font color='red'>理小</font>说，ACG方<font color='red'>面喜</font>欢型月、轨迹。喜欢<font color='red'>有
</font>

思想深度的事物，讨厌急躁、拜金与安逸的人。目前在魔都某女校学习，这是我的个人博客。闻道有先后，术业有专攻，请多多关照。你喜欢写代码吗？
"
Lucene5.3调用HanLP关键词HightLight定位错误,"在Lucene5.3中调用HanLP对指定目录下的文件进行检索（文件解析用的Tika），关键字高亮位置定位不对，调用IKAnalyzer5x定位就没问题，请问是什么情况？多谢。
代码如下：
package com.std.test;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.StringReader;
import java.nio.file.Paths;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.TextField;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleFragmenter;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.SAXException;

import com.hankcs.lucene5.HanLPAnalyzer;
import com.std.core.ik5x.IKAnalyzer5x;

public class TikaFileSearch {
    /**
     \* 多格式文件内容解析
     \* 
     \* @param file
     \* @return
     \* @throws IOException
     \* @throws SAXException
     \* @throws TikaException
     */
    public static String parseFile(File file) throws IOException, SAXException,
            TikaException {
        FileInputStream fs = new FileInputStream(file);
        Parser parse = new AutoDetectParser();// 自动获取一个合适的解析器类型
        // 如果文件很大，那么这个值可以适当调大
        BodyContentHandler handler = new BodyContentHandler(10000);
        Metadata metadata = new Metadata();
        ParseContext parseContext = new ParseContext();
        parse.parse(fs, handler, metadata, parseContext);
        System.out.println(""目标文件名称："" + file.getAbsolutePath());
        System.out.println(""目标文件内容："" + handler.toString());

```
    return handler.toString();
}

/**
 * 为目标文档创建索引
 * 
 * @throws Exception
 */
public static void indexDoc() throws Exception {
    File[] files = new File(""/Projects/IS/filesPath/"").listFiles();
    for (File file : files) {
        String content = parseFile(file);

        Analyzer analyzer = new HanLPAnalyzer();
        // 构造字段
        TextField contentField = new TextField(""content"", content,
                Store.YES);
        TextField nameField = new TextField(""name"", file.getName(),
                Store.YES);
        TextField pathField = new TextField(""path"", file.getAbsolutePath(),
                Store.YES);

        // 添加字段
        Document doc = new Document();
        doc.add(contentField);
        doc.add(nameField);
        doc.add(pathField);
        IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
        iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
        try {
            Directory fsDirectory = FSDirectory.open(Paths.get(
                    ""/Projects/IS/filesIndexPath/"", new String[0]));
            IndexWriter indexWriter = new IndexWriter(fsDirectory, iwConfig);
            indexWriter.addDocument(doc);

            indexWriter.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

/**
 * 搜索文件
 * 
 * @param field
 * @param keyword
 */
public static void search(String field, String keyword) {
    Analyzer analyzer = new HanLPAnalyzer();
    try {
        Directory fsDirectory = FSDirectory.open(Paths.get(
                ""/Projects/IS/filesIndexPath/"", new String[0]));
        DirectoryReader ireader = DirectoryReader.open(fsDirectory);

        IndexSearcher isearcher = new IndexSearcher(ireader);

        QueryParser qp = new QueryParser(field, analyzer); // 使用QueryParser查询分析器构造Query对象
        qp.setDefaultOperator(QueryParser.AND_OPERATOR);
        Query query = qp.parse(keyword);
        TopDocs topDocs = isearcher.search(query, 5); // 搜索相似度最高的5条记录
        System.out.println(""命中:"" + topDocs.totalHits);

        SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(
                ""<span style='color:green'>"", ""</span>"");
        Highlighter highlighter = new Highlighter(simpleHTMLFormatter,
                new QueryScorer(query));
        // 高亮htmlFormatter对象
        // 设置高亮附近的字数
        highlighter.setTextFragmenter(new SimpleFragmenter(200));

        ScoreDoc[] scoreDocs = topDocs.scoreDocs;
        for (int i = 0; i < topDocs.totalHits; i++) {
            Document targetDoc = isearcher.doc(scoreDocs[i].doc);
            String value = targetDoc.get(""content"");
            TokenStream tokenStream = analyzer.tokenStream(value,
                    new StringReader(value));
            String freg = highlighter.getBestFragment(tokenStream, value);

            System.out.println(freg);

        }

    } catch (Exception e) {

    }
}

public static void main(String[] args) throws Exception {

    indexDoc();
    search(""content"", ""民族"");
}
```

}

运行截图如下：
![079da201-023c-4232-a856-80fb063a7476](https://cloud.githubusercontent.com/assets/14268210/10640239/78728792-7846-11e5-8b93-443ca970dda1.png)

调用IK的运行结果如下：
![56e95ae2-12c8-4b04-a58c-10d00ed28921](https://cloud.githubusercontent.com/assets/14268210/10640257/8818074e-7846-11e5-85f9-208098472519.png)
"
如何才能分出带空格的英文/数字词,"比如 iPad Pro
比如 PM 2.5
谢谢！
"
segment.enableAllNamedEntityRecognize(true)，有些影响其他分词的结果,"“今天我们单位“，被当作一个词了。
Segment segment = HanLP.newSegment();
Segment segment = HanLP.newSegment();
segment = segment.enableMultithreading(true);
segment = segment.enableAllNamedEntityRecognize(true);
[今天我们单位/nt, 评选/vn, 出/vf, 了全单位/nt, 最/d, 漂亮/a, 的/ude1, 女同事/nz, ，/w, 但/c, 她/rr, 好像/v, 对/p, “/w, 局/n, 花/n, ”/w, 这个/rz, 头衔/n, 并不/d, 满意/v]

segment.enableAllNamedEntityRecognize(false)，结果好些。但是，又希望能识别实体。
[今天/t, 我们/rr, 单位/n, 评选/vn, 出/vf, 了/ule, 全/a, 单位/n, 最/d, 漂亮/a, 的/ude1, 女同事/nz, ，/w, 但/c, 她/rr, 好像/v, 对/p, “/w, 局/n, 花/n, ”/w, 这个/rz, 头衔/n, 并不/d, 满意/v]
"
"报错：enableNumberQuantifierRecognize 和 enableAllNamedEntityRecognize 同时用, 版本1.2.4","Segment segment = HanLP.newSegment();
segment = segment.enableAllNamedEntityRecognize(true);
segment = segment.enableNumberQuantifierRecognize(true);
System.out.println(segment.seg(""曾幻想过，若干年后的我就是这个样子的吗""));

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 21
    at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
    at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:95)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:420)
    at hanlpdemo.HanlpDemo.main(HanlpDemo.java:41)
"
DAT内存分配算法,"src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java

```
resize(65536 * 32); // 32个双字节
```

```
            if (allocSize <= (begin + siblings.get(siblings.size() - 1).code))
            {
                // progress can be zero // 防止progress产生除零错误
                double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0
                        * keySize / (progress + 1);
                resize((int) (allocSize * l));
            }
```

此段代码好像小规模字典不会运行到，例如在人名识别时，21个字典项的小字典，也需要开个65535内存。
"
比较简单的问题：.txt.bin文件如何提取？,"因为研究需要，想学习HanLP中的句法分析部分，按照介绍下载了data文件夹，发现里面的文件全部为txt.bin，而demo中提示
“严重: CRF分词模型加载 .../data/model/segment/CRFSegmentModel.txt 失败，耗时 12 ms”
"
CRF模型训练求教,"你好，大神，请教一下：如果crf模型能够识别""花千骨""这个新词，是不是需要在训练语料中药包含多个  花千骨 这个词才能训练出这个模型？另您能不能提供一下CRF模型的训练语料数据，邮箱为1527zhaobin@163.com，多谢！
"
"Term添加equals, hashCode方法及实现comparable接口","只是一个建议，我现在是继承了一个Term类来做各种集合相关的操作以及排序
不过如果Term原生就有的话就更好了
"
ArrayIndexOutOfBoundsException,"Exception in thread ""main"" java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:58)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 65535
        at com.hankcs.hanlp.collection.trie.bintrie.BinTrie.getChild(BinTrie.java:322)
        at com.hankcs.hanlp.collection.trie.bintrie.BaseNode.transition(BaseNode.java:56)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:239)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:55)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:436)
        at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
        at com.hankcs.hanlp.HanLP.segment(HanLP.java:371)
"
CustomDictionary.txt第一行不能加入词典,"不知道是为什么……
解决方法：前两行重复一下
"
停用词词典,"建议加一个「没有」
"
中文人名实体精度的问题,"我在使用hanlp 1.2.4 对如下句子分词的时候：
“并有望在那与1993年就结识的友人重聚。”

得到的结果是如下这样的：
并/cc  有望/v    在/p   那与/nr   1993/m  年/qt  就/d   结识/v    的/ude1    友人/n    重聚/vi   。/w

其中的“那与／nr”应该不是人名。我看源码好像是因为它这个词组符合构成人名的规则 所以把他组合成了人名？然后再进行hmm的识别？还希望作者说明一下如何改进这一块。
"
关于版权问题,"hankcs，你好。
之前我一直使用python+jpype的方式，使用你的HanLP。我的工作中所有的代码全部是基于python开发，并且考虑以后要在hadoop平台上使用。所以我用python实现了一个HanLP的子集。等成熟后也希望放到github上。python版本直接使用了data目录下的数据文件，希望可以得到你的授权。根据你的要求，我会在项目首页注明字典数据的来源，以及python版本与HanLP的关系。
"
这些天刚刚接触分词，请教一下加载CRF模型的问题,"我按照博主的方法在下载了data-for-1.2.4这个数据包，数据包里有data-for-1.2.4\data\model\segment\CRFSegmentModel.txt.bin 这个模型文件，请问我要怎么加载到程序里呢，加载过程中总是报错，谢谢博主
"
Lucene5.3调用HanLP创建索引报错,"在Lucene5.3调用HanLP创建索引的时候，报错如下：
Exception in thread ""main"" java.lang.AbstractMethodError: org.apache.lucene.analysis.Analyzer.createComponents(Ljava/lang/String;)Lorg/apache/lucene/analysis/Analyzer$TokenStreamComponents;
    at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:179)
    at org.apache.lucene.document.Field.tokenStream(Field.java:562)
    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:607)
    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:344)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:300)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1475)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1254)
    at com.ohoyee.test.search.SearchTest.createIndex(SearchTest.java:51)
    at com.ohoyee.test.search.SearchTest.main(SearchTest.java:104)

索引创建代码如下： 
static void createIndex() throws Exception {
        Directory dir = FSDirectory.open(Paths.get(indexPath, new String[0]));
        Analyzer analyzer = new HanLPAnalyzer();
        IndexWriterConfig iwc = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(dir, iwc);
        File[] files = new File(targetPath).listFiles();
        for (File file : files) {
            Document doc = new Document();
            String content = getContent(file);
            String name = file.getName();
            String path = file.getAbsolutePath();
            doc.add(new TextField(""content"", content, Store.YES));
            doc.add(new TextField(""name"", name, Store.YES));
            doc.add(new TextField(""path"", path, Store.YES));
            System.out.println(name + ""==="" + content + ""==="" + path);
            writer.addDocument(doc);
            writer.commit();
        }

```
}
```
"
配置文件大小写问题,"linux下大小写敏感，按照文档配置hanlp.properties会有问题，需要把hanlp.properties的全部字母小写。
另linux下出现“字符类型对应表加载失败”，解决如下。HanLP.Config.CharTypePath=""/var/server/java/data/dictionary/other/CharType.dat.yes"";
"
字符类型对应表加载失败,"在windows下正常，放到centos下去跑，出现这个提示。
"
有没有办法关闭分词后的后缀词性标注,"我想得到的结果是分词保留原来的形式，只是分词之后有空格分开，不会出现斜杠和后面的词性标注。
请问怎样办？
"
请问这个项目和哈工大ltp或者复旦的那个相比，主要区别是什么？,"如题，抱歉一下子读不完所有代码就直接请教一下了：）
"
CRF的分词结果如何去除停用词？,"你好，对CRF分词之后得到的List<Term>使用coreStopWordDictionary.apply之后全是空的，请问是什么情况？
版本1.2.4
"
关于提高机构识别精度问题,"找了一段财经媒体的报道，尝试对公司名称进行识别。内容是“2015年7月10日,公司全资子公司深圳市中航九方资产管理有限公司(以下简称“中航九方”)与深圳市中航华城置业发展有限公司(以下简称“中航华城置业”)签订了《租赁推广服务协议》和《管理、租赁、推广与咨询协议》,由中航九方受托为中航华城置业持有的深圳九方购物中心(以下简称“深圳九方”)(G/M、H地块)提供前期租赁推广服务以及商业运营、维修保养、财务管理等商业咨询服务。中航华城置业将按照前述协议约定分别向中航九方支付租赁推广服务费和咨询服务费,预计相关费用总收入约为451万元。”
## 我用的是上半年1.1.4以前的版本，识别出来的结果如下：
## [2015/m, 年/q, 7/m, 月/n, 10/m, 日/ag, ,/w, 公司/nit, 全资/nr, 子公司/nit, 深圳市/nr2, 中航/gg, 九/ag, 方资产管理有限公司/nsf, (/w, 以下/tg, 简称/s, “/xx, 中航/gg, 九/ag, 方/mq, ”/xx, )/w, 与/pbei, 深圳市中航华城置业发展有限公司/nsf, (/w, 以下/tg, 简称/s, “/xx, 中航华城/nr2, 置业/n, ”/xx, )/w, 签订/s, 了/u, 《/xx, 租赁/v, 推广/v, 服务/v, 协议/n, 》/xx, 和/pbei, 《/xx, 管理/v, 、/xx, 租赁/v, 、/xx, 推广/v, 与/pbei, 咨询/v, 协议/n, 》/xx, ,/w, 由/dg, 中航/nr, 九/ag, 方/mq, 受/s, 托/s, 为/dg, 中航华城/nr2, 置业/n, 持有/s, 的/ule, 深圳/nr2, 九方购物中心/nsf, (/w, 以下/tg, 简称/s, “/xx, 深圳/nr2, 九/ag, 方/mq, ”/xx, )/w, (/w, G/M/nx, 、/xx, H/nx, 地块/n, )/w, 提供/s, 前期/tg, 租赁/v, 推广/v, 服务/v, 以及/pbei, 商业/n, 运营/v, 、/xx, 维修/v, 保养/v, 、/xx, 财务/n, 管理/v, 等/ude3, 商业/n, 咨询服务/s, 。/xx, 中航华城/nr2, 置业/n, 将/qv, 按照/dg, 前述/ag, 协议/n, 约定/s, 分别/qv, 向/dg, 中航/nr, 九/ag, 方/mq, 支付/s, 租赁/v, 推广/v, 服务费/n, 和/pbei, 咨询服务/s, 费/n, ,/w, 预计/s, 相关/v, 费用/n, 总收入/n, 约/qv, 为/dg, 451/m, 万元/mq, 。/xx]

看上去只正确识别到了1个完全准确的全称，另外1个全称，3个简称似乎都没有识别精确。请看看是否有办法提高这个？谢谢！
"
我是觉得自定义词典优先级应该大于核心词典.,"我是觉得自定义词典优先级应该大于核心词典, 或者说有一个选项可以设置优先级. 像一些行业的词汇, 通过人民日报这种通俗语料库是识别不出来的. 比如说: 宝马X5, 八荣八耻, 三严三实. 这些专用词语在分词时候是应当被提取出来的. 当然, 你要说可以修改核心词典, 但我认为修改核心词典又是过重的做法, 词典都不好管理. 还是希望有自定义词典优先级的设置.
"
人名和組織機構識別經常包含標點的問題,"例如机构识别出：“美國，中華隊”
标点问题需要一个解决方案，CommonAhoCorasickSegmentUtil会把标点作为未知语素和前后的语素合并，显然不太合理。标点分词的优先级应该提高。
"
提取关键词、短语,"你好，HanLP提取新闻的关键词或短语的时候效果总不尽人意，通常提取出来的词不是特别能表达出新闻关键关注点，有啥好的解决方案吗？
"
请问如何用您的工具包实现新词发现功能呢？,"您好，看了您的工具包的代码，感觉写的非常好，我非常佩服。可能本人水平差一些，不知道您的工具有没有实现新词发现功能，若实现了怎么用，或如何利用现有功能实现新词发现功能。
"
CRFSegment.cs中tag方法有一行代码没看懂,"博主，在tag方法中，利用维特比方法计算字符串的标注路径时，有一段代码是if (matrix[pre][now] <= 0) continue； 为什么tag的转移概率<0，就略过了呢？
"
fix:繁体分词bug,"版本:208e338600be8ac219b9112a1163aaeef2df2b8a

```
TraditionalChineseTokenizer.segment(""认可程度"");
```

繁体分词导致程序异常出错。
"
fix:繁体分词bug,"版本:208e338600be8ac219b9112a1163aaeef2df2b8a

```
 TraditionalChineseTokenizer.segment(""认可程度"");
```

繁体分词，输入全为简体时分导致程序异常出错。
原因是简体输入在繁简转换时生成的term列表数量和后面简体分词时生成的term数量不一致。
"
有冲动想改成C#,
能否给一个CRF模型下载的链接？,
句法分析的逻辑建议,"比如针对专有名词/机构的标记逻辑
我经常在全聚汇吃饭
[我/rr, 经常/d, 在/p, 全聚/nr, 汇/v, 吃饭/vi, ，/w]

 ""我经常在大铜锅吃饭，"",
[我/rr, 经常/d, 在/p, 大/a, 铜/n, 锅/n, 吃饭/vi, ，/w]

""我经常在大铜锅海鲜楼吃饭，""
[我/rr, 经常/d, 在/p, 大/a, 铜/n, 锅/n, 海鲜/nf, 楼/n, 吃饭/vi, ，/w]

出现的位置最近的 介词【】动词之间且至少有一个名词出现，能否将这些词形成专有名词
"
人名称提取不准确,"1.发现Nature源码中有 
/**
     \* 蒙古姓名
     */
nr2, 但是测试并没有正确标记。比如
人名称识别的例子中：
签约仪式前，秦光荣、阿布杜勒·哈尼德、阿卜杜热西提、仇和、穆拉帝力、斯琴格日乐、 娜仁高娃、乌兰托雅和宫本正太郎等一同会见了企业家阿卜杜勒·哈米德。
使用这个句子一些人名称提取错误，或标注错误。
不太理解算法，是否一定需要加入字典？

2.再比如：
                ""请收回李珅300元钱"",
                ""请收回李绅300元钱"",
                ""请收回李西300元钱"",
                ""请收回李硒300元钱"",

标记人名：李珅 李西 正确；李绅 李硒 错误。

这个人名称标记，不用模型文件吗？
"
能否提供SOLR5.X的支持,"您可以参考IK分词器的SOLR分词源码（这个版本是支持solr5的修订版）
https://github.com/EugenePig/ik-analyzer-solr5
"
修改繁体分词bug,"繁体分词流程修改为先繁简转换，再根据分词结果返回原字符串，而不是直接使用简繁转换。
"
修正三个简繁转换的惯用法。,"""國立嘉義大學生命科學院食品科學系""
被错误地分词为：
""國立 嘉義 大斈 生命 科斈院 食品 科斈繫""
"
dfsfsdf,"fdfdf
"
Suggester 能否增加removeAllSentences方法，便于一次加载多次使用,"如：
    public void removeAllSentences() {
        for (IScorer scorer : scorerList)
        {
            scorer.removeAllSentences();
        }
    }
"
繁体中文识别人名,"""「國際足球總會」（FIFA）主席布拉特（Sepp Blatter）5月29日連任成功.""

将“任成功”识别为人名，感觉粗暴的将“任成功”加到nr.txt里似乎不妥，也许某些语境里真有人叫“任成功”，不知道有没有更好的处理办法。
"
Portable同步升级到v1.2.4,
Portable同步升级到v1.2.4,
时间标准化的问题,"不知道大神有没有时间加一个时间标准化的模块？另外，时间的判断好像也有些问题，例如  下午/t, 3/m, 时由/nr, 北京/ns, 出发/v, 这句话就没有把下午3时整个判断为时间，希望大神能改进下，谢谢。
"
机构名识别导致的切词错误,"按照要求，城镇居民医保和新农合政策范围内住院费用支付比例分别达到70%以上和75%左右。适当提高城镇居民医保和新农合门诊统筹待遇水平。 国务院医改办政策组负责人傅卫在接受记者采访时表示，从深化医改以来，基本医疗保障制度建设加快推进，医保保障水平也在不断提高，各级政府对城镇居民医保和新农合的补助标准从2008年的人均80元提高到了2013年的280元。到2015年，城镇居民医保和新农合政府补助标准将提高到每人每年360元以上。 不过，也有专家担心，随着保障水平提升，不断增加的医保基金支出正对医保基金“收支平衡、略有结余”的运行原则形成压力，甚至在一些地区造成超支风险。 记者了解到，国务院此前发文要求，在今年6月底前各省要全面启动城乡居民大病保险试点工作。大病医保的资金来源于现有医保基金的结余，即从城镇居民医保基金、新农合基金中划出，采取向商业保险机构购买保险的方式，不再额外增加群众个人缴费负担。 财经评论员余丰慧认为，如果提高大病保险保障水平，扩大报销种类，提高报销比例，而只在医保基金结余的存量上要资金来源，很快将造成两个方面的问题：一是可能影响到医保的资金支付，二是结余资金远远不够。 对此，《任务》要求，推进城乡居民基本医保制度整合和完善筹资机制。完善政府、单位和个人合理分担的基本医保筹资机制。此外，研究建立稳定可持续、动态调整的筹资机制，在逐步提高整体筹资标准的同时，按照积极稳妥、逐步到位的原则，逐步提高个人缴费占整体筹资的比重。 而根据此前相关要求，城镇居民医疗保险个人缴费应随总筹资水平作相应调整，个人缴费应占人均总筹资20%左右。对此，中央财经大学教授褚福灵在接受记者采访时表示，目前各地正研究建立城镇居民医疗保险财政补助和个人缴费科学合理、协同增长的机制，强化个人缴费义务。 惠民回购大型医疗设备降低检查费 《任务》指出，降低药品和高值医用耗材价格，降低大型医用设备检查、治疗价格，已贷款或集资购买的大型设备原则上由政府回购，回购有困难的限期降低价格。价格调整政策要与医保支付政策相衔接。 “老百姓抱怨看病贵的问题能够真正缓解了。”一位不愿透露姓名的业内人士指出，多年来患者抱怨看病贵，主要集中在诊疗费用上包括大型设备的检查费用等方面。政府也让医院降价，当大型设备的成本、维修费用都摆在那里，成本必然分摊到患者的检查费用中。 记者5月28日致电卫计委新闻宣传司副司长宋树立了解具体的回购政策，但截至记者发稿前，电话一直无法接通。 记者从食药总局主办的医疗器械质量万里行活动中了解到，国内大约近70%的大型医疗设备从销售到售后服务均被GE、西门子、飞利浦等跨国公司垄断。这体现在技术和耗材上的垄断直接导致市场竞争不充分，价格虚高。为了追求更高的利润，生产厂家从提供维修的配件中赚取高额差价成为主要盈利手段，售后服务提供商实际上变成了医疗设备配件销售商。 据对山东菏泽一家医院的调查，目前医疗设备厂家售后保质期一般都是一年，之后的维修有全保、技术保和单次维修3种情况。厂家对几种设备的维修报价如下：美国产64排VCT，一年全保38万元，一年技术保15万元，单次人工费3.1万元；美国产单排CT一年全保30万元，一年技术保13.6万元，单次人工费1.1万元……由于目前国家没有一个大致统一的定价标准，基本上由医院和厂家谈判，最后还是厂家说了算。 “不少进口的大型医疗设备维修费用占产品本身费用的三分之一或二分之一，甚至有的已经超过了产品价格。因此，卫生局让医院降低检查费用价格，根本就不可能，而用回购这一手段解决了诊疗费用高的问题。”上述人士指出。 鼓励实现跨省联合招标降低药价 “尽管跨省招标在政府采购中并不是一个新鲜事，但这是药品行业第一次提出。”中国医药企业管理协会副会长牛正乾28日表示，跨省联合招标可能推进招标环节的公开、透明。 《任务》在“加快推行公立医院改革”中指出，鼓励跨省联合招标采购保证药品质量安全，切实降低药品价格，有条件的地区要建立与基层基本药物采购联动的机制。逐步规范集中采购药品的剂型、规格和包装。推进高值医用耗材公开透明、公平竞争网上阳光采购。药品和高值医用耗材采购数据实行部门和区域共享。 中投顾问医药行业研究员许玲妮指出，县级医院联合招标能够降低药品配送费用，这部分差价分摊到药品上有利于降低药品价格，而且有利于平衡招标药品的质量和价格，改变以往备受业界诟病“唯低价是取”的招标规则。 不少医药企业的董事长都向记者表示，现行以省为单位的药品集中招标采购制度存在不少弊端，劳民伤财，应该进行改革。 许玲妮指出，药品招标采购规则存在漏洞，缺乏由政府部门提供必要的、较全面的药品信息。二是药品集中招标采购监督管理难以到位，招标后没有规范的信息反馈程序，监管部门无法对招标人合同履行情况进行有效监管，使集中招标采购活动“先热后凉，流于形式”。 “招投标本来是国际上通行的一种比较好的市场化采购方式，但目前我国政府包办的药品招标政策严重异化，成为药品进入市场所执行的二次行政管制。”葵花药业集团董事长关彦斌指出，现行药品招投标的实质是把药品进入医疗机构使用的正常市场行为变成了行政审批，并且同种产品不同区域年年审批，招标主管部门成了药品领域的最大审批权机构。 不过，许玲妮也指出，药品跨省联合招标采购是否可行与各省的区位有关，因为药企要承担运输成本，如果到两省距离不一样，承担的运费成本明显不同，如果按照统一招标并不合理。“比如北京、天津这两个直辖市，辖区较小，药企统一配送承担的费用差不多的情况下，跨省联合招标采购是可行的。”

启用机构名识别后，切词结果会把  ""葵花  当做机构名
"
如何添加新的停用词（还是不行）,"我在dictionary目录下的默认停用词文件中添加新的停用词，重新运行之后发现新的停用词没有去掉。
如果先删除缓存文件再添加词语也不行，但按照这种方法添加自定义词语是有效的，但添加停用词不行。
"
crf problem,"当采用data-for1.2.2中的crf训练数据进行分词时,出现错误,具体是
代码为:
CRFModel crfModel = CRFModel.loadTxt(""E:\scalaworkspace\Eyas\data\model\segment\CRFSegmentModel.txt.bin"");
        System.out.println(""locad finish"");
错误为
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""U 0 0 : (   ??z?_K????]S?况??{???k??n??""
"
自定义词典似乎没用,"System.out.println(CustomDictionary.add(""高大上"",""a 1024""));
termList = HanLP.segment(""外观绝对高大上，不信的是没见过."");
System.out.println(termList);

输出结果

[外观/n, 绝对/d, 高大/a, 上/f, ，/w, 不信/v, 的/ude1, 是/vshi, 没/d, 见过/v, ./w]
"
按照你手册上提供方法添加人名误判不成功,"按照你提供的方法，修改了data /dictionary / person / nr.txt，加入一个新词条，但是系统依然误判。
比如：“万余元” 本不该是人名，添加该词条，但是最终结果依然误判！求解决方法。
"
如何添加新的停用词（还是不行）,"我在dictionary目录下的默认停用词文件中添加新的停用词，重新运行之后发现新的停用词没有去掉
"
对不起，最近有点忙，回复可能会有点慢,"大家好，感谢提出的宝贵意见。
我粗略一看都是常规问题，也就一周集中处理一次了。
不好意思最近有点忙，可能得七月份才能恢复秒回的效率。
"
关键词提取算法会出现重复关键词,"List<String> textrank = HanLP.extractKeyword(article, 5);

输出：市场 设备 idc 穿戴 fitbit fitbit
输入：新浪科技讯 北京时间6月7日早间消息，IDC近期公布了“全球市场季度可穿戴设备跟踪报告”，第一季度Fitbit是全球排名第一的可穿戴设备厂商。然而，随着苹果Apple Watch的全面开售，Fitbit的优势很可能将不复存在。

　　IDC公布的数据显示，2015年第一季度，Fitbit、小米和Garmin是全球前三大可穿戴设备厂商。不过，苹果今年4月开始销售Apple Watch。因此当第二季度数据发布时，苹果将跻身这一排行的前列。

　　IDC可穿戴设备研究经理雷蒙·拉马斯(Ramon Llamas)表示：“Apple Watch很可能将成为其他可穿戴设备的对比对象。这将迫使竞争对手采取措施，以保持在市场的领先优势。”

　　整体来看，第一季度可穿戴设备市场同比增长200%，全球出货量为1140万个。这表明，这一市场非常强劲。

　　拉马斯表示：“第一季度，即‘后假日季’时段销售滑坡不明显，这表明可穿戴设备市场非常强劲。终端用户的兴趣不断提升，而相关厂商可以提供多样化的设备和体验。此外，新兴市场的需求正在上升，而厂商渴望把握这些新机会。”

　　导致Apple Watch无法主宰市场的一大障碍在于，这款产品的入门价格较高。IDC指出，价格下降是推动可穿戴设备销售火爆的原因之一。Apple Watch的起步价格为349美元，是其他可穿戴设备平均价格的3倍。IDC估计，有40%的可穿戴设备价格低于100美元。

　　IDC全球移动设备跟踪报告高级分析师杰特什·乌布拉尼(Jitesh Ubrani)表示：“与任何新生市场一样，价格下降非常猛烈。在平均价格下降的情况下，苹果携一款高价产品入市将检验用户是否愿意为某一品牌，或是受市场关注的产品而支付更高的价格。”

　　IDC的数据显示，第一季度，Fitbit的设备出货量为390万个，市场份额为34%，这是由于Charge、Charge HR和Surge等新产品的需求强劲。此外，用户也在继续追捧Fitbit的Flex、One和Zip等产品。IDC认为，同时专注于休闲和高端市场是Fitbit取得成功的重要原因。

　　Fitbit的市场份额比排名第二的小米高10%。小米排名第二主要是依靠小米手环在中国国内市场的销售。IDC认为，小米将很快进一步开拓国际市场，从而成为Fitbit的有力竞争对手。

　　与Fitbit类似，Garmin也提供了多样化的可穿戴设备产品。不过，Garmin的市场份额仅略高于6%。

　　三星排名第四。IDC分析师指出，三星的表现不佳主要是由于，其Gear设备只能连接某些高端的三星智能手机。

　　Jawbone、索尼和Pebble正在争夺市场第五的位置，而第一季度Jawbone取得了领先。拉马斯和乌尔巴尼表示，Jawbone的UP MOVE和UP24第一季度带来了帮助。而随着第二季度Jawbone再推出两款新产品，这样的优势还将得到加强。(维金)
"
正规化全角标点符号会转成半角逗号,"```
    System.out.println(CharTable.convert('！'));
```

结果是','，而不是'!'
"
能否提供主词典动态删除方法？,"看了下原码，发现BinTrie有提供remove方法，所以自定义词典可以动态删除。
不过DoubleArrayTrie好像并没有提供删除方法，请问是为什么？是否是因为数据结构的问题？

建议能提供主词典动态删除方法，因为若是把hanLP拿来做生产环境solr的分词，服务不能停，
如果要修改词库的话，建议能有动态删除所有词库的方法。

谢谢！
"
"""齐云山高纯山茶油""分词bug","偶然遇到这个广告词，目前的nlp分词结果是“齐云山高/ns, 纯/a, 山茶油/nf”
正确的应该是“齐云山 高纯 山茶油”
"
"分词器对""三年""的分词结果不是很理想","测试代码:
List<Term> termList = StandardTokenizer.segment(""三年"");
System.out.println(termList);
termList = StandardTokenizer.segment(""3年"");
System.out.println(termList);
termList = StandardTokenizer.segment(""三 年"");
System.out.println(termList);
termList = StandardTokenizer.segment(""三元"");
System.out.println(termList);
termList = StandardTokenizer.segment(""3元"");
System.out.println(termList);
======================分词结果======================================
[三/m, 年/n]
[3年/m]
[三/m,  /w, 年/q]
[三元/nz]
[3元/mq]

首先查看词库,找到关于年和元的相关信息:
年 D 219 B 61 C 42 L 20 K 12 E 8  
年 B 105 D 57 A 11 X 1 
年 q   2421    n   95  m   1  
年 qt 14340    

元 P 47 D 2 A 1  
元 D 957 C 672 E 60 B 22 K 3 L 1  
元 q   1536    tg  5   n   1

词库中对年的统计,明显量词的词频比较高,但是为什么分出来是名词呢.我的代码已经是最新的代码了.
"
分词bug,"对一片文本进行分词，发现会抛IllegalArgumentException异常，每次必现。不知道是什么原因，能解释一下吗？是我使用的方法不对吗？我跑了几千篇文章都没问题，只有这篇文章有问题。

测试文本：
　　昨日，陈燕和导盲犬到达雍和宫站准备换乘2号线。根据当天起实施的新规，导盲犬被允许乘坐地铁和火车。新京报记者 王贵彬 摄
　　 
　　地铁内一位小女孩见到导盲犬有些害怕，陈燕表示导盲犬很安全，“就是踩到它的脚，它也不会咬你。”新京报记者 王贵彬 摄
　　 
　　昨日，地铁西直门站，流动巡查的北京市轨道交通执法大队执法人员，胸前佩戴执法记录仪。
　　实习生 彭子洋 摄
　　轨道交通执法队执法首日未开罚单
　　《北京市轨道交通运营安全条例》实施，初期对乞讨卖艺以劝导为主；被拒11次导盲犬终乘地铁
　　新京报讯 实施首日，北京首支轨道交通执法队也于当天开始进地铁执法，当天并未开出罚单。交通执法部门相关负责人表示，法规实施前期以劝导为主。
　　最终目标：站均两名执法人员
　　根据《条例》规定，在地铁里乞讨、卖艺将被处以50元以上1000元以下罚款。在车站、车厢内派发广告等物品的，面临最高1万元的罚款。作为依照法规执法的北京市轨道交通执法大队昨天首次进地铁工作，当天上午，记者在西直门等地铁车站看到，在闸机处和换乘通道内，身着交通执法制服的执法人员两人一组流动巡查。
　　执法队相关负责人告诉记者，目前流动巡查对于发现违规的人员，可能首先进行劝导询问，一开始不会进行处罚。若出现严重情节，如不停止违法违规行为的，会采取强制措施以及处罚，“执法人员都要进行现场取证”。
　　据统计，轨道交通执法大队正式上岗的第一天，大队全员上岗，在兼顾地铁全线网的情况下，重点对1号线、2号线、4号线、5号线、10号线的70个重点车站进行监管，巡视车站97座，劝阻在地铁站内玩轮滑行为3起；劝阻摆摊小商贩4起；劝阻散发小广告1起；劝阻翻越闸机2起。
　　交通执法总队副总队长梁建伟介绍，轨道交通执法大队现正式编制共有88人，按照地铁运营公司管理范围分为5个中队，和一个专门负责应对重点车站重点时段或大客流冲击时安全保障的机动中队。
　　“我们的最终目标是318个站，平均每站两名执法人员”，梁建伟说，执法大队后续还会招聘人员辅助执法，同时借助地铁运营方为执法提供保障。
　　滑板车等或将禁入地铁
　　梁建伟介绍，执法队员职责包括对危害轨道交通设备设施安全的行为、危害轨道交通运营安全的行为及轨道交通相关单位不落实安全生产主体责任的行为实施行政处罚，其中有41项行政处罚，5项行政强制。
　　“法规实施前期，我们还是以劝阻为主。”梁建伟说，条例对在车站、车厢内乞讨卖艺、派发广告以及其他危害地铁安全的行为都有了规定，但实施上有具体问题，还需要制定细则来进一步明确，目前细则正在研究中，将于1个月到2个月内出台。
　　北京地铁运营公司相关负责人表示，《条例》中提到，在车站内禁止从事滑板、轮滑、自行车等运动。下一步，滑板、轮滑等运动器具可能禁止带入地铁。目前，有关禁带物品的目录相关部门还在进一步完善，之前允许带的大型工具等等，未来或列入违禁名单中。
　　■ 探访
　　乘客：打击乞讨不能仅靠罚款
　　昨日中午1时许，记者在地铁2号线宣武门站看到一位妇女拿着一个音箱，一边放音乐一边向乘客乞讨，一节车厢内，只有一位年轻的女乘客给了她1元钱。根据《北京市轨道交通运营安全条例》，对地铁乞讨卖艺等危害地铁运营安全的行为，轨道交通执法人员可实施行政处罚。
　　记者随后拨打了地铁车厢内贴着的北京地铁监督电话96165。得知是举报乞讨卖艺行为后，接线员询问了线路、车站、列车运行方向、所在车厢以及乞讨人员行进的方向等信息，并表示会马上联系车站工作人员。
　　“乞讨卖艺行为让人感到厌烦”，同车乘客谢女士表示，自己曾有过多次在地铁上睡着后被乞讨者吵醒的经历，但她同时认为对这些人员还是应以劝导为主。同车大多数乘客也认为，地铁中的乞讨行为一般都有组织，不处罚乞讨卖艺组织者，但靠罚款难以杜绝这样的行为。
　　此外，大多数乘客对于散发小广告“深恶痛绝”，称该行为使地铁车厢内环境脏乱。京港地铁安全部门负责人介绍，他们工作人员对地铁内散发小广告的行为只能劝阻，而不能没收他们的小广告。5月1日以后，有了条例的支持，会在这方面加强管理。
　　导盲犬珍妮终于“获准”乘地铁
　　此前被拒11次；《条例》明确视力残障者可携带导盲犬进站乘车
　　在经历了11次被拒绝以后，导盲犬珍妮昨天终于感受了一把北京地铁。此次《条例》明确，视力残障者可携带导盲犬进站乘车。
　　昨天上午，地铁5号线天通苑站，视障人士陈燕牵着导盲犬珍妮，来到地铁天通苑站，车站工作人员检查了视力残障证件、导盲犬证件后，向陈燕发放了福利票。
　　在工作人员引导下，陈燕和导盲犬顺利进站。站在带无障碍标志的候车门前，陈燕反复指一下标志，又拍一下珍妮的头，告诉它要记住这个位置。
　　嘀嘀嘀，地铁门打开，珍妮小心翼翼地走进车厢，它引导陈燕找到车厢接口处的空位置后停住，等陈燕站定后，珍妮静静的趴在脚边。车厢里其他乘客对导盲犬乘车非常感兴趣，但并没有人引逗，珍妮也丝毫没有躁动行为。5号线换乘2号线也有工作人员引导，这次，陈燕找到一个座位坐下，珍妮依旧卧在她脚边。
　　陈燕说，乘坐公共交通工具的能力，是导盲犬在训练时的必修课。珍妮昨天的表现很好，并没有对人群有恐惧感。她希望，通过这一次的实践，让更多人了解视力残障人士对公共出行的需求，了解导盲犬。目前全国“持证”的导盲犬也不过七八十只，北京市目前共有9只，市区有7只，郊区有2只。

代码如下：
        StringBuffer article = new StringBuffer();
        String line;
        try(BufferedReader out = new BufferedReader(new FileReader(""C:/Users/huan.wang/Desktop/corpus.txt""))) {
            while((line = out.readLine()) != null) {
                article.append(line);
            }
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        List<Term> words = HanLP.segment(article.toString());
        for(Term w : words) {
            System.out.println(w);
        }

异常栈信息：
Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1
    at java.util.ArrayList.<init>(Unknown Source)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.convert(WordBasedGenerativeModelSegment.java:241)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:114)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:384)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:371)
"
回复“CRF分词的纯Java实现”,"URL:http://www.hankcs.com/nlp/segment/crf-segmentation-of-the-pure-java-implementation.html

可能是我现在对HanLP的了解还太片面。请让我先说明下为什么会觉得有些不妥当。
在看这篇文章前，先看了《HanLP开源》，中间有提到
“HanLP几乎所有的功能都可以通过工具类HanLP快捷调用，当你想不起来调用方法时，只需键入HanLP.，IDE应当会给出提示，并展示HanLP完善的文档。
推荐用户始终通过工具类HanLP调用，这么做的好处是，将来HanLP升级后，用户无需修改调用代码。”
于是，在尝试测试本文的“你看过穆赫兰道吗”时，也试着用HanLP调用，但结果与你展示结果不同，查看源码发现HanLP默认使用的是ViterbiSegment对象，但又没有可以设置我需要的segment的地方，因此觉得现有代码并没有像你说的那样，能够几乎所有的功能都能被HanLP调用。

看过你的回复，知道了newSegment()的作用，也初步了解你要这么做的原因:避免产生线程不安全的问题，因此你使用了一个静态方法返回一个新对象的方式实现。另外你提到的“那么就得考虑用户的代码的副作用，他会不会把text的charArray改动了”我不是太明白。

为了达到至少我认为的“HanLP几乎所有的功能都可以通过工具类HanLP快捷调用”，我复制了你提供的HanLP源码，并做了一些改动：
    添加了如下代码：  
    public static final byte CRF_SEGMENT = 0;
    public static final byte HMM_SEGMENT = 1;
    public static final byte AHO_CORASICK_DOUBLE_ARRAY_TRIE_SEGMENT = 2;
    public static final byte DOUBLE_ARRAY_TRIE_SEGMENT = 3;
    public static final byte DIJKSTRA_SEGMENT = 4;
    public static final byte N_SHORT_SEGMENT = 5;
    public static final byte VITERBI_SEGMENT = 6;
    public static ThreadLocal<Segment> segmentLocal = new ThreadLocal<Segment>(){
        protected Segment initialValue() {
            return new ViterbiSegment();
        }
    };
    public static Segment setSegment(Segment segment){
        segmentLocal.set(segment);
        return segment;
    }
    public static Segment setSegment(byte segment){
        Segment segmentObj;
        switch(segment){
            case 0:
                segmentObj = new CRFSegment();
                break;
            case 1:
                segmentObj = new HMMSegment();
                break;
            case 2:
                segmentObj = new AhoCorasickDoubleArrayTrieSegment();
                break;
            case 3:
                segmentObj = new DoubleArrayTrieSegment();
                break;
            case 4:
                segmentObj = new DijkstraSegment();
                break;
            case 5:
                segmentObj = new NShortSegment();
                break;
            case 6:
                segmentObj = new ViterbiSegment();
                break;
            default:
                segmentObj = new ViterbiSegment();
        }
        return setSegment(segmentObj);
    }

改方法newSegment()为：
    public static Segment newSegment() {
        return segmentLocal.get();// 以Viterbi分词器做为默认分词工具，同时支持用户选择其他分词器
    }

除此之外，其他类都不需要改动。我想，ThreadLocal的运行机制能保证变量在单个线程内部共享但线程间不共享，除了减少内存占用外，不会产生线程不安全的问题，亦不会有资源争用的问题，还能向用户提供自己定义需要的segment的功能。

如下是两种实现“你看过穆赫兰道吗”的方式，(当然，看起来差不多)：
        CRFSegment segment = new CRFSegment();
        segment.enablePartOfSpeechTagging(true);
        System.out.println(""previous version:\t""+segment.seg(""你看过穆赫兰道吗""));

```
    NewHanLP.Config.enableDebug(true);
    NewHanLP.setSegment(NewHanLP.CRF_SEGMENT).enablePartOfSpeechTagging(true);
    System.out.println(""new version:\t\t""+NewHanLP.segment(""你看过穆赫兰道吗""));
```

输出：
previous version:       [你/rr, 看过/v, 穆赫兰道/null, 吗/y]
new version:        [你/rr, 看过/v, 穆赫兰道/null, 吗/y]

欢迎拍砖 :)
"
ViterbiSegment对应短词语辨识问题,"请问ViterbiSegment对应短词语辨识不佳,是不是跟训练语料是长文章有关？

例如 可乐、年货  这些词 会分成 [可 ,乐]  ,[年 , 货]
在CoreNatureDictionary.txt 已经有出现   可乐、年货 
但CoreNatureDictionary.ngram.txt 里没有 
可乐@末##末
年货@末##末

这些有比较优雅的方式解决吗？ 还是必须添加语料至CoreNatureDictionary.ngram.txt ?
"
CRF分词模型的训练 和 CRF分词视标点为新词问题,"CRF分词模型使用的是什么语料？以及-f, -c参数的选取。只能通过不断的测试，选取经过测试选取效果好的？

另，经使用1.2.2版的HanLP和data，测试CRF分词，测试数据如下：

```
""《夜晚的骰子》通过描述浅草的舞女在暗夜中扔骰子的情景,寄托了作者对庶民生活区的情感"",
""这个像是真的[委屈]前面那个打扮太江户了，一点不上品...@hankcs"", 
```

关闭词性标注的分词效果如下：

```
[《, 夜晚, 的, 骰子, 》, 通过, 描述, 浅草, 的, 舞女, 在, 暗夜, 中, 扔, 骰子, 的, 情景, ,, 寄托, 了, 作者, 对, 庶民, 生活区, 的, 情感]
[这个, 像, 是, 真的, [, 委屈, ], 前面, 那个, 打扮, 太, 江户, 了, ，, 一点, 不, 上品, ., ., ., @, hankcs]
```

再查阅词典CoreNatureDictionary.txt，不存在其中的标注为null词性的词视为新词，以上两例句识别出的新词分别为：

```
识别到新词：,(wc: 1)
识别到新词：@(wc: 1), hankcs(wc: 1), [(wc: 1), ](wc: 1), .(wc: 3)
```

整体效果不错，诸如""hankcs""词识别出了，但像"","", ""@"", ""["", ""]"", "".""等标点符号也被视为新词。能够在CRF分词之后加入停用词之类的做法，过滤掉此类的词呢？

谢谢！
"
关于分词器对空格的处理!,"先举例:
代码:
StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
List<Term> termList = StandardTokenizer.segment(""4月30号 9点钟"");
System.out.println(termList);
termList = StandardTokenizer.segment(""4月30号9点钟"");
System.out.println(termList);
termList = StandardTokenizer.segment(""4月30日9点钟"");
System.out.println(termList);
===========================结果====================================
[4月/mq, 30/m, 号null/nz, 9点/m, 钟/n]
[4月/mq, 30号9点/m, 钟/n]
[4月/mq, 30日/mq, 9点/m, 钟/n]

其实 我理想中的效果应该是:
[4月/mq, 30号/mq, 9点钟/mq]
[4月/mq, 30号/mq, 9点钟/mq]
[4月/mq, 30日/mq, 9点钟/mq]

请教博主,这个问题  该如何去解决呢?
"
CRF模型 地址,"您好，我是nlp新手，想使用CRF分词，但缺少模型文件，之前提供的地址也已失效，能够重新分享下，谢谢！
"
"建议在分词时,将字典中的忽略大小写功能添加到分词中!","在实际的搜索当中,用户是不考虑输入的字母是大写还是小写的问题的.所以博主可以考虑在分词的时候也不考虑输入文本的大小写问题,直接能够匹配词库中的词.举例如下:
List<Term> termList = NLPTokenizer.segment(""爱听4g"");
System.out.println(termList);
==============分词结果====================
[爱/v, 听/v, 4g/nz]

List<Term> termList = NLPTokenizer.segment(""爱听4G"");
System.out.println(termList);
==============分词结果====================
[爱听4G/nz]

其实我想要的结果是: 无论输入的是 爱听4g 还是 爱听4G 都能分出来[爱听4g/nz]
"
Portable同步升级到v1.2.2,"v1.2.2版本基本稳定了，于是将Portable版也升级到v1.2.2。
"
CRFSegment 如何忽略标点？,"segment.seg(sentence); 如何配置才能忽略句子中的标点？发现这个方法会把标点和词的组合作为新词
"
Portable,
关于jdk7中 使用TextRankKeyword提取关键词报Comparison method violates its general contract!异常,"测试代码:
String src = ""data/test.txt"";
        Scanner scanner = new Scanner(Paths.get(src),""gbk"");
        StringBuilder sb = new StringBuilder();
        while(scanner.hasNextLine()){
            sb.append(scanner.nextLine().trim());
        }
//      System.out.println(sb.toString());
        scanner.close();
        System.out.println(TextRankKeyword.getKeywordList(sb.toString(), 20));
# 

错误代码:
java.lang.IllegalArgumentException: Comparison method violates its general contract!
    at java.util.TimSort.mergeLo(Unknown Source)
    at java.util.TimSort.mergeAt(Unknown Source)
    at java.util.TimSort.mergeCollapse(Unknown Source)
    at java.util.TimSort.sort(Unknown Source)
    at java.util.TimSort.sort(Unknown Source)
    at java.util.Arrays.sort(Unknown Source)
    at java.util.Collections.sort(Unknown Source)
    at com.hankcs.hanlp.summary.TextRankKeyword.getKeyword(TextRankKeyword.java:115)
    at com.hankcs.hanlp.summary.TextRankKeyword.getKeywordList(TextRankKeyword.java:47)

经过网上搜索:
http://www.tuicool.com/articles/MZreyuv
http://blog.csdn.net/ghsau/article/details/42012365

发现是jdk7 中 Collections的排序算法已经发生变化,需要处理两个比较对象相等的情况.
由于TextRankKeyword中的比较对象是Float对象,所以我查了下Float的compare方法(Float是实现Comparable<Float>接口的).代码:
public static int compare(float f1, float f2) {
        if (f1 < f2)
            return -1;           // Neither val is NaN, thisVal is smaller
        if (f1 > f2)
            return 1;            // Neither val is NaN, thisVal is larger

```
    // Cannot use floatToRawIntBits because of possibility of NaNs.
    int thisBits    = Float.floatToIntBits(f1);
    int anotherBits = Float.floatToIntBits(f2);

    return (thisBits == anotherBits ?  0 : // Values are equal
            (thisBits < anotherBits ? -1 : // (-0.0, 0.0) or (!NaN, NaN)
             1));                          // (0.0, -0.0) or (NaN, !NaN)
}
```

所以我将博主的代码:
Collections.sort(entryList, new Comparator<Map.Entry<String, Float>>()
        {
            @Override
            public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2)
            {
                return (o1.getValue() - o2.getValue() > 0 ? -1 : 1);
            }
        });

改为了:
 Collections.sort(entryList, new Comparator<Map.Entry<String, Float>>()
        {
            @Override
            public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2)
            {
                return Float.compare(o1.getValue(),o1.getValue());
            }
        });

这样就不报错了.

请博主参考哈. 建议最好代码中的所有Float参数的比较实现都采用这中方式.
"
"同时开启标准分词和索引分词的数量词识别,然后索引数量词时发生数组越界异常.","测试代码:
public class TestHanLP {
    @Test
    public void test1(){
        StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
        IndexTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
        List<Term> termList = StandardTokenizer.segment(""此帐号有欠费业务是什么"");
        termList = IndexTokenizer.segment(""此帐号有欠费业务是什么"");
        termList = StandardTokenizer.segment(""15307971214话费还有多少"");
        termList = IndexTokenizer.segment(""15307971214话费还有多少"");
        System.out.println(termList);
    }
}  
在对""此帐号有欠费业务是什么""分词时很正常.
对""""15307971214话费还有多少""分词时发生数组越界异常错误.
# 异常错误:

java.lang.ArrayIndexOutOfBoundsException: 19
    at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:214)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.decorateResultForIndexMode(WordBasedGenerativeModelSegment.java:489)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:105)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:291)
    at com.hankcs.hanlp.tokenizer.IndexTokenizer.segment(IndexTokenizer.java:33)
    at com.xin.file.FileTest.test4(FileTest.java:93)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
"
加入了对数量词的识别!,"博主! 我加入了对数量词的识别! 主题代码如下:
package com.hankcs.hanlp.recognition.mq;

import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.corpus.tag.Nature;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.seg.common.Vertex;
import com.hankcs.hanlp.seg.common.WordNet;
import com.hankcs.hanlp.utility.Predefine;

import java.util.List;
import java.util.ListIterator;

import static com.hankcs.hanlp.dictionary.nr.NRConstant.WORD_ID;

/**
- 数量词识别
- @author hankcs
  _/
  public class TranslatedQuantifierRecognition
  {
  /_*
  - 执行识别
  - @param segResult 粗分结果
  - @param wordNetOptimum 粗分结果对应的词图
  - @param wordNetAll 全词图
    */
    public static void Recognition(List<Vertex> segResult, WordNet wordNetOptimum, WordNet wordNetAll)
    {
    StringBuilder sbQuantifier = new StringBuilder();
    int appendTimes = 0;
    ListIterator<Vertex> listIterator = segResult.listIterator();
    listIterator.next();
    int line = 1;
    int activeLine = 1;
    while (listIterator.hasNext())
    {
        Vertex vertex = listIterator.next();
        if (appendTimes > 0)
        {
            if (vertex.guessNature() == Nature.q ||vertex.guessNature() == Nature.qt
                    ||vertex.guessNature() == Nature.qv 
                    || vertex.guessNature() == Nature.qt
                    ||vertex.guessNature() == Nature.nx)
            {
                sbQuantifier.append(vertex.realWord);
                ++appendTimes;
            }
            else
            {
                // 识别结束
                if (appendTimes > 1)
                {
                    if (HanLP.Config.DEBUG)
                    {
                        System.out.println(""数量词识别出："" + sbQuantifier.toString());
                    }
                    wordNetOptimum.insert(activeLine, new Vertex(Predefine.TAG_QUANTIFIER, sbQuantifier.toString(), new CoreDictionary.Attribute(Nature.mq), WORD_ID), wordNetAll);
                }
                sbQuantifier.setLength(0);
                appendTimes = 0;
            }
        }
        else
        {
            // 数字m触发识别
            if (vertex.guessNature() == Nature.m)
            {
                sbQuantifier.append(vertex.realWord);
                ++appendTimes;
                activeLine = line;
            }
        }
    
    ```
    line += vertex.realWord.length();
    ```
    
    }
    }
    }
"
关于分词的问题?,"博主! 你好!
     分词""手机套"",结果：[手机/n, 套/q]
但是在词库CoreNatureDictionary.txt中找到有手机套的相关词，如下：
    Line 11870: 使手机 n 2
    Line 61022: 手机 n 6656
    Line 61023: 手机党 nz 2
    Line 61024: 手机卡 nz 17
    Line 61025: 手机套 nz 2
    Line 61026: 手机报 nz 37
    Line 137480: 部手机 n 53
为什么分词不是分成　[手机套/nz] 呢　
"
对于中文数量词改如何切分?,"这是我的切分例子:
未分词:十九元套餐包括什么
标准分词:[十/m, 九/b, 元/q, 套餐/n, 包括/v, 什么/ry]
智能分词:[十/m, 九/b, 元/q, 套餐/n, 包括/v, 什么/ry]
索引分词:[十/m, 九/b, 元/q, 套餐/n, 包括/v, 什么/ry]

理想效果：
[十九元/mq, 套餐/n, 包括/v, 什么/ry]
"
BaseNode编译错误,"发现BaseNode的walkToLoad方法有编译错误。
错误信息：

> The method walkToLoad(ByteArray, BaseNode.ValueArray) in the type BaseNode is not applicable for the arguments (ByteArray, BaseNode<V>.ValueArray)
"
运行TestSegment.java 测试类中的 testShortest、testNT方法 出现以下错误,"版本为当前最新1.1.5
java.lang.ExceptionInInitializerError
    at com.hankcs.hanlp.recognition.ns.PlaceRecognition.roleTag(PlaceRecognition.java:106)
    at com.hankcs.hanlp.recognition.ns.PlaceRecognition.Recognition(PlaceRecognition.java:36)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:75)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:242)
    at com.hankcs.test.seg.TestSegment.testShortest(TestSegment.java:71)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at junit.framework.TestCase.runTest(TestCase.java:168)
    at junit.framework.TestCase.runBare(TestCase.java:134)
    at junit.framework.TestResult$1.protect(TestResult.java:110)
    at junit.framework.TestResult.runProtected(TestResult.java:128)
    at junit.framework.TestResult.run(TestResult.java:113)
    at junit.framework.TestCase.run(TestCase.java:124)
    at junit.framework.TestSuite.runTest(TestSuite.java:232)
    at junit.framework.TestSuite.run(TestSuite.java:227)
    at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:131)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.get(DoubleArrayTrie.java:1210)
    at com.hankcs.hanlp.dictionary.CoreDictionary.get(CoreDictionary.java:183)
    at com.hankcs.hanlp.dictionary.ns.PlaceDictionary.<clinit>(PlaceDictionary.java:60)
    ... 23 more
"
大赞，不过摘要效果有点问题,"提供的例子结果是：
[无限算法的产生是由于未能确定的定义终止条件, 这类算法在有限的时间内终止, 这类算法在有限的一段时间内终止]

看来还是有很多优化空间的。

不过这么全面的NLP package已经很赞了，剩下的优化过程其实相对容易一些啦~
"
请问该如何切分*/,"输入  鱼300g*2/组

使用  HanLP.newSegment() 
切分结果 [鱼/n, 300g*2//nx, 组/n]

使用 AhoCorasickSegment().enablePartOfSpeechTagging(true)
切分结果 [鱼/nz, 300g*2//nx, 组/nz]

使用 NShortSegment()
切分结果 [鱼/n, 300g*2//nx, 组/n]

都是  [鱼, 300g*2/, 组]

请问该如何配置才能切出
[鱼, 300g, \* ,2,  / , 组]  or  [鱼, 300 , g, *,2 , / , 组] ?

/////////////////////
补上 鱼300克*2/组的情形

 [鱼/n, 300/m, 克/q, *2//nx, 组/n]
仍有 *2/ 切为一个词的情况发生
"
扩展词库加入英文，输入扩展英文连接另一英文，分词会报错。,"扩展词库加入英文，输入扩展英文连接另一英文，分词会报错。
原本以为是自定词性的问题，但将词性改为n，仍会报错

词库内容为
BENQ    n   1024
BENTLEY n   1024

输入""BENQphone"";
使用标准分词   HanLP.segment(text)

开启debug如下：

粗分词网：
0:[ ]
1:[BENQ]
2:[ENQphone]
3:[]
4:[]
5:[]
6:[]
7:[]
8:[]
9:[]
10:[ ]

会报出这样的错误 

Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1
    at java.util.ArrayList.<init>(ArrayList.java:142)
    at com.hankcs.hanlp.seg.HiddenMarkovModelSegment.convert(HiddenMarkovModelSegment.java:238)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:50)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:144)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:39)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:354)

原因是  com.hankcs.hanlp.seg.Viterbi.ViterbiSegment  中 47行
 List<Vertex> vertexList = viterbi(wordNetAll);

返回结果
vertexList =[ ]
vertexList.size() = 1

但输入 ""BENQBENTLEYphone""  

则输出没报错，但结果不是想要的

人名角色观察：[  A 42634591 ][BENQ A 42634591 ][B L 3 ][ENTLEYphone A 42634591 ][  A 42634591 ]
人名角色标注：[ /A ,BENQ/A ,B/L ,ENTLEYphone/A , /A]
[BENQ/n, B/nx, ENTLEYphone/nx]

请问该如何修改才能添加英文词库呢？
"
while 方法可能少写break,"java新手第一次使用github,如有缺失请见谅

使用 findbugs找到的，报以下错误
Value of Node.label from previous case is overwritten here due to switch statement fall through [Scariest(1), High confidence]

在 com.hankcs.hanlp.dependency.common.Node的第182行

```
        case wh:
            label = ""x"";
        case begin:
            label = ""root"";
            break;
```

case wh 没有break ,不确定这样是否会造成问题。
"
Bump tensorflow-gpu from 1.4.0 to 2.12.0,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.12.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.12.0</h2>
<h1>Release 2.12.0</h1>
<h2>TensorFlow</h2>
<h3>Breaking Changes</h3>
<ul>
<li>
<p>Build, Compilation and Packaging</p>
<ul>
<li>Removed redundant packages <code>tensorflow-gpu</code> and <code>tf-nightly-gpu</code>. These packages were removed and replaced with packages that direct users to switch to <code>tensorflow</code> or <code>tf-nightly</code> respectively. Since TensorFlow 2.1, the only difference between these two sets of packages was their names, so there is no loss of functionality or GPU support. See <a href=""https://pypi.org/project/tensorflow-gpu"">https://pypi.org/project/tensorflow-gpu</a> for more details.</li>
</ul>
</li>
<li>
<p><code>tf.function</code>:</p>
<ul>
<li><code>tf.function</code> now uses the Python inspect library directly for parsing the signature of the Python function it is decorated on. This change may break code where the function signature is malformed, but was ignored previously, such as:
<ul>
<li>Using <code>functools.wraps</code> on a function with different signature</li>
<li>Using <code>functools.partial</code> with an invalid <code>tf.function</code> input</li>
</ul>
</li>
<li><code>tf.function</code> now enforces input parameter names to be valid Python identifiers. Incompatible names are automatically sanitized similarly to existing SavedModel signature behavior.</li>
<li>Parameterless <code>tf.function</code>s are assumed to have an empty <code>input_signature</code> instead of an undefined one even if the <code>input_signature</code> is unspecified.</li>
<li><code>tf.types.experimental.TraceType</code> now requires an additional <code>placeholder_value</code> method to be defined.</li>
<li><code>tf.function</code> now traces with placeholder values generated by TraceType instead of the value itself.</li>
</ul>
</li>
<li>
<p>Experimental APIs <code>tf.config.experimental.enable_mlir_graph_optimization</code> and <code>tf.config.experimental.disable_mlir_graph_optimization</code> were removed.</p>
</li>
</ul>
<h3>Major Features and Improvements</h3>
<ul>
<li>
<p>Support for Python 3.11 has been added.</p>
</li>
<li>
<p>Support for Python 3.7 has been removed. We are not releasing any more patches for Python 3.7.</p>
</li>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Add 16-bit float type support for built-in op <code>fill</code>.</li>
<li>Transpose now supports 6D tensors.</li>
<li>Float LSTM now supports diagonal recurrent tensors: <a href=""https://arxiv.org/abs/1903.08023"">https://arxiv.org/abs/1903.08023</a></li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Coordination service now works with <code>dtensor.initialize_accelerator_system</code>, and enabled by default.</li>
<li>Add <code>tf.experimental.dtensor.is_dtensor</code> to check if a tensor is a DTensor instance.</li>
</ul>
</li>
<li>
<p><code>tf.data</code>:</p>
<ul>
<li>Added support for alternative checkpointing protocol which makes it possible to checkpoint the state of the input pipeline without having to store the contents of internal buffers. The new functionality can be enabled through the <code>experimental_symbolic_checkpoint</code> option of <code>tf.data.Options()</code>.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.random()</code> operation, which controls whether the sequence of generated random numbers should be re-randomized every epoch or not (the default behavior). If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>random()</code> operation will produce a different (deterministic) sequence of numbers every epoch.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.sample_from_datasets()</code> operation, which controls whether the sequence of generated random numbers used for sampling should be re-randomized every epoch or not. If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>sample_from_datasets()</code> operation will use a different (deterministic) sequence of numbers every epoch.</li>
</ul>
</li>
<li>
<p><code>tf.test</code>:</p>
<ul>
<li>Added <code>tf.test.experimental.sync_devices</code>, which is useful for accurately measuring performance in benchmarks.</li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.12.0</h1>
<h3>Breaking Changes</h3>
<ul>
<li>
<p>Build, Compilation and Packaging</p>
<ul>
<li>Removed redundant packages <code>tensorflow-gpu</code> and <code>tf-nightly-gpu</code>. These packages were removed and replaced with packages that direct users to switch to <code>tensorflow</code> or <code>tf-nightly</code> respectively. Since TensorFlow 2.1, the only difference between these two sets of packages was their names, so there is no loss of functionality or GPU support. See <a href=""https://pypi.org/project/tensorflow-gpu"">https://pypi.org/project/tensorflow-gpu</a> for more details.</li>
</ul>
</li>
<li>
<p><code>tf.function</code>:</p>
<ul>
<li><code>tf.function</code> now uses the Python inspect library directly for parsing the signature of the Python function it is decorated on. This change may break code where the function signature is malformed, but was ignored previously, such as:
<ul>
<li>Using <code>functools.wraps</code> on a function with different signature</li>
<li>Using <code>functools.partial</code> with an invalid <code>tf.function</code> input</li>
</ul>
</li>
<li><code>tf.function</code> now enforces input parameter names to be valid Python identifiers. Incompatible names are automatically sanitized similarly to existing SavedModel signature behavior.</li>
<li>Parameterless <code>tf.function</code>s are assumed to have an empty <code>input_signature</code> instead of an undefined one even if the <code>input_signature</code> is unspecified.</li>
<li><code>tf.types.experimental.TraceType</code> now requires an additional <code>placeholder_value</code> method to be defined.</li>
<li><code>tf.function</code> now traces with placeholder values generated by TraceType instead of the value itself.</li>
</ul>
</li>
<li>
<p>Experimental APIs <code>tf.config.experimental.enable_mlir_graph_optimization</code> and <code>tf.config.experimental.disable_mlir_graph_optimization</code> were removed.</p>
</li>
</ul>
<h3>Major Features and Improvements</h3>
<ul>
<li>
<p>Support for Python 3.11 has been added.</p>
</li>
<li>
<p>Support for Python 3.7 has been removed. We are not releasing any more patches for Python 3.7.</p>
</li>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Add 16-bit float type support for built-in op <code>fill</code>.</li>
<li>Transpose now supports 6D tensors.</li>
<li>Float LSTM now supports diagonal recurrent tensors: <a href=""https://arxiv.org/abs/1903.08023"">https://arxiv.org/abs/1903.08023</a></li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Coordination service now works with <code>dtensor.initialize_accelerator_system</code>, and enabled by default.</li>
<li>Add <code>tf.experimental.dtensor.is_dtensor</code> to check if a tensor is a DTensor instance.</li>
</ul>
</li>
<li>
<p><code>tf.data</code>:</p>
<ul>
<li>Added support for alternative checkpointing protocol which makes it possible to checkpoint the state of the input pipeline without having to store the contents of internal buffers. The new functionality can be enabled through the <code>experimental_symbolic_checkpoint</code> option of <code>tf.data.Options()</code>.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.random()</code> operation, which controls whether the sequence of generated random numbers should be re-randomized every epoch or not (the default behavior). If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>random()</code> operation will produce a different (deterministic) sequence of numbers every epoch.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.sample_from_datasets()</code> operation, which controls whether the sequence of generated random numbers used for sampling should be re-randomized every epoch or not. If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>sample_from_datasets()</code> operation will use a different (deterministic) sequence of numbers every epoch.</li>
</ul>
</li>
<li>
<p><code>tf.test</code>:</p>
<ul>
<li>Added <code>tf.test.experimental.sync_devices</code>, which is useful for accurately measuring performance in benchmarks.</li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Added experimental support to ReduceScatter fuse on GPU (NCCL).</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0db597d0d758aba578783b5bf46c889700a45085""><code>0db597d</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60051"">#60051</a> from tensorflow/venkat2469-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/1a12f5939238da4c4372a095e589f2801baecf41""><code>1a12f59</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/aa4d558019c48a7ab5ff403b56e7d80fda4d13ce""><code>aa4d558</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60050"">#60050</a> from tensorflow/venkat-patch-6</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bd1ab8a9ef0df2fbaefd6cfa994687c0aa1dca55""><code>bd1ab8a</code></a> Update the security section in RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4905be05fd63d9943dec222ee83dcc6f522b671b""><code>4905be0</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60049"">#60049</a> from tensorflow/venkat-patch-5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9f96caad340d0f4bdb10a1addd61fe8a40fa6fdb""><code>9f96caa</code></a> Update setup.py on TF release branch with released version of Estimator and k...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e719b6b83b60fb03ff06c5397686888834fc5296""><code>e719b6b</code></a> Update Relese.md (<a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60033"">#60033</a>)</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/64a9d548b520659a9a226250dfd204e0038069bc""><code>64a9d54</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60017"">#60017</a> from tensorflow/joefernandez-patch-2.12-release-notes</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7a4ebfd7f814a1b45a9d57c880fcbf8abb111fff""><code>7a4ebfd</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e0e10a9f2e3e09656f14b7b1339d94c7b43f84e6""><code>e0e10a9</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/59988"">#59988</a> from tensorflow-jenkins/version-numbers-2.12.0-8756</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.12.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.12.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Model_onehot.h5  was lost.Could you upload a new one? THkS,
Bump pillow from 6.0.0 to 9.3.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.3.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Initialize libtiff buffer when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Limit SAMPLESPERPIXEL to avoid runtime DOS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>Inline fname2char to fix memory leak <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6329"">#6329</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Fix memory leaks related to text features <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6330"">#6330</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Use double quotes for version check on old CPython on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6695"">#6695</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>GHA: replace deprecated set-output command with GITHUB_OUTPUT file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6697"">#6697</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Remove backup implementation of Round for Windows platforms <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6693"">#6693</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Upload fribidi.dll to GitHub Actions <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6532"">#6532</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Fixed set_variation_by_name offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6445"">#6445</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Windows build improvements <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6562"">#6562</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Fix malloc in _imagingft.c:font_setvaraxes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6690"">#6690</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Only use ASCII characters in C source file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6691"">#6691</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Release Python GIL when converting images using matrix operations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6418"">#6418</a> [<a href=""https://github.com/hmaarrfk""><code>@​hmaarrfk</code></a>]</li>
<li>Added ExifTags enums <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6630"">#6630</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Do not modify previous frame when calculating delta in PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6683"">#6683</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added support for reading BMP images with RLE4 compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6674"">#6674</a> [<a href=""https://github.com/npjg""><code>@​npjg</code></a>]</li>
<li>Decode JPEG compressed BLP1 data in original mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6678"">#6678</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>pylint warnings <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6659"">#6659</a> [<a href=""https://github.com/marksmayo""><code>@​marksmayo</code></a>]</li>
<li>Added GPS TIFF tag info <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6661"">#6661</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added conversion between RGB/RGBA/RGBX and LAB <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6647"">#6647</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Do not attempt normalization if mode is already normal <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6644"">#6644</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed seeking to an L frame in a GIF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6576"">#6576</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Consider all frames when selecting mode for PNG save_all <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6610"">#6610</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Don't reassign crc on ChunkStream close <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6627"">#6627</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Raise a warning if NumPy failed to raise an error during conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6594"">#6594</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Only read a maximum of 100 bytes at a time in IMT header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6623"">#6623</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Show all frames in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6611"">#6611</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Allow FLI palette chunk to not be first <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6626"">#6626</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>If first GIF frame has transparency for RGB_ALWAYS loading strategy, use RGBA mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6592"">#6592</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Round box position to integer when pasting embedded color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6517"">#6517</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Removed EXIF prefix when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6582"">#6582</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Pad IM palette to 768 bytes when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6579"">#6579</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added DDS BC6H reading <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6449"">#6449</a> [<a href=""https://github.com/ShadelessFox""><code>@​ShadelessFox</code></a>]</li>
<li>Added support for opening WhiteIsZero 16-bit integer TIFF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6642"">#6642</a> [<a href=""https://github.com/JayWiz""><code>@​JayWiz</code></a>]</li>
<li>Raise an error when allocating translucent color to RGB palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6654"">#6654</a> [<a href=""https://github.com/jsbueno""><code>@​jsbueno</code></a>]</li>
<li>Moved mode check outside of loops <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6650"">#6650</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added reading of TIFF child images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6569"">#6569</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved ImageOps palette handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6596"">#6596</a> [<a href=""https://github.com/PososikTeam""><code>@​PososikTeam</code></a>]</li>
<li>Defer parsing of palette into colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6567"">#6567</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Apply transparency to P images in ImageTk.PhotoImage <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6559"">#6559</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use rounding in ImageOps contain() and pad() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6522"">#6522</a> [<a href=""https://github.com/bibinhashley""><code>@​bibinhashley</code></a>]</li>
<li>Fixed GIF remapping to palette with duplicate entries <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6548"">#6548</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Allow remap_palette() to return an image with less than 256 palette entries <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6543"">#6543</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Corrected BMP and TGA palette size when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6500"">#6500</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.3.0 (2022-10-29)</h2>
<ul>
<li>
<p>Limit SAMPLESPERPIXEL to avoid runtime DOS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a>
[wiredfool]</p>
</li>
<li>
<p>Initialize libtiff buffer when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a>
[radarhere]</p>
</li>
<li>
<p>Inline fname2char to fix memory leak <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6329"">#6329</a>
[nulano]</p>
</li>
<li>
<p>Fix memory leaks related to text features <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6330"">#6330</a>
[nulano]</p>
</li>
<li>
<p>Use double quotes for version check on old CPython on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6695"">#6695</a>
[hugovk]</p>
</li>
<li>
<p>Remove backup implementation of Round for Windows platforms <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6693"">#6693</a>
[cgohlke]</p>
</li>
<li>
<p>Fixed set_variation_by_name offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6445"">#6445</a>
[radarhere]</p>
</li>
<li>
<p>Fix malloc in _imagingft.c:font_setvaraxes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6690"">#6690</a>
[cgohlke]</p>
</li>
<li>
<p>Release Python GIL when converting images using matrix operations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6418"">#6418</a>
[hmaarrfk]</p>
</li>
<li>
<p>Added ExifTags enums <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6630"">#6630</a>
[radarhere]</p>
</li>
<li>
<p>Do not modify previous frame when calculating delta in PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6683"">#6683</a>
[radarhere]</p>
</li>
<li>
<p>Added support for reading BMP images with RLE4 compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6674"">#6674</a>
[npjg, radarhere]</p>
</li>
<li>
<p>Decode JPEG compressed BLP1 data in original mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6678"">#6678</a>
[radarhere]</p>
</li>
<li>
<p>Added GPS TIFF tag info <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6661"">#6661</a>
[radarhere]</p>
</li>
<li>
<p>Added conversion between RGB/RGBA/RGBX and LAB <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6647"">#6647</a>
[radarhere]</p>
</li>
<li>
<p>Do not attempt normalization if mode is already normal <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6644"">#6644</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d594f4cb8dc47fb0c69ae58d9fff86faae4515bd""><code>d594f4c</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/909dc64ed5f676169aa3d9b0c26f132a06321b83""><code>909dc64</code></a> 9.3.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1a51ce7b955c65c8f2c6bc7772735b197b8a6aa3""><code>1a51ce7</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a> from hugovk/security-libtiff_buffer</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/2444cddab2f83f28687c7c20871574acbb6dbcf3""><code>2444cdd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a> from hugovk/security-samples_per_pixel-sec</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/744f455830871d61a8de0a5e629d4c2e33817cbb""><code>744f455</code></a> Added release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/0846bfae48513f2f51ca8547ed3b8954fa501fda""><code>0846bfa</code></a> Add to release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/799a6a01052cea3f417a571d7c64cd14acc18c64""><code>799a6a0</code></a> Fix linting</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/00b25fd3ac3648bc28eff5d4c4d816e605e3f05f""><code>00b25fd</code></a> Hide UserWarning in logs</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/05b175ef88c22f5c416bc9b8d5b897dea1abbf2c""><code>05b175e</code></a> Tighter test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/13f2c5ae14901c89c38f898496102afd9daeaf6d""><code>13f2c5a</code></a> Prevent DOS with large SAMPLESPERPIXEL in Tiff IFD</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.3.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.9.3,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.9.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.9.3</h2>
<h1>Release 2.9.3</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes an overflow in <code>tf.keras.losses.poisson</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41887"">CVE-2022-41887</a>)</li>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
<li>Fixes a heap OOB in <code>FractionalAvgPool</code> and <code>FractionalMaxPool</code>(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41900"">CVE-2022-41900</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> in <code>SparseMatrixNNZ</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41901"">CVE-2022-41901</a>)</li>
<li>Fixes an OOB write in grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41902"">CVE-2022-41902</a>)</li>
<li>Fixes a overflow in <code>ResizeNearestNeighborGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41907"">CVE-2022-41907</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>PyFunc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41908"">CVE-2022-41908</a>)</li>
<li>Fixes a segfault in <code>CompositeTensorVariantToComponents</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41909"">CVE-2022-41909</a>)</li>
<li>Fixes a invalid char to bool conversion in printing a tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41911"">CVE-2022-41911</a>)</li>
<li>Fixes a heap overflow in <code>QuantizeAndDequantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41910"">CVE-2022-41910</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> via missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>TensorListScatter</code> and <code>TensorListScatterV2</code> in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
</ul>
<h2>TensorFlow 2.9.2</h2>
<h1>Release 2.9.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a <code>CHECK</code> failure in tf.reshape caused by overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35934"">CVE-2022-35934</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes an OOB read in <code>Gather_nd</code> op in TF Lite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35937"">CVE-2022-35937</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>TensorListReserve</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35960"">CVE-2022-35960</a>)</li>
<li>Fixes an OOB write in <code>Scatter_nd</code> op in TF Lite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35939"">CVE-2022-35939</a>)</li>
<li>Fixes an integer overflow in <code>RaggedRangeOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35940"">CVE-2022-35940</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>AvgPoolOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35941"">CVE-2022-35941</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>UnbatchGradOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35952"">CVE-2022-35952</a>)</li>
<li>Fixes a segfault TFLite converter on per-channel quantized transposed convolutions (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-36027"">CVE-2022-36027</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>AvgPool3DGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35959"">CVE-2022-35959</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35963"">CVE-2022-35963</a>)</li>
<li>Fixes a segfault in <code>BlockLSTMGradV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35964"">CVE-2022-35964</a>)</li>
<li>Fixes a segfault in <code>LowerBound</code> and <code>UpperBound</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35965"">CVE-2022-35965</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.9.3</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes an overflow in <code>tf.keras.losses.poisson</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41887"">CVE-2022-41887</a>)</li>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
<li>Fixes a heap OOB in <code>FractionalAvgPool</code> and <code>FractionalMaxPool</code>(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41900"">CVE-2022-41900</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> in <code>SparseMatrixNNZ</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41901"">CVE-2022-41901</a>)</li>
<li>Fixes an OOB write in grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41902"">CVE-2022-41902</a>)</li>
<li>Fixes a overflow in <code>ResizeNearestNeighborGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41907"">CVE-2022-41907</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>PyFunc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41908"">CVE-2022-41908</a>)</li>
<li>Fixes a segfault in <code>CompositeTensorVariantToComponents</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41909"">CVE-2022-41909</a>)</li>
<li>Fixes a invalid char to bool conversion in printing a tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41911"">CVE-2022-41911</a>)</li>
<li>Fixes a heap overflow in <code>QuantizeAndDequantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41910"">CVE-2022-41910</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> via missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>TensorListScatter</code> and <code>TensorListScatterV2</code> in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
</ul>
<h1>Release 2.8.4</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a5ed5f39b675a1c6f315e0caf3ad4b38478fa571""><code>a5ed5f3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58584"">#58584</a> from tensorflow/vinila21-patch-2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/258f9a1251346d93e129c53f82d21732df6067f5""><code>258f9a1</code></a> Update py_func.cc</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/cd27cfb438b78a019ff8a215a9d6c58d10c062c3""><code>cd27cfb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58580"">#58580</a> from tensorflow-jenkins/version-numbers-2.9.3-24474</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3e75385ee6c9ef8f06d6848244e1421c603dd4a1""><code>3e75385</code></a> Update version numbers to 2.9.3</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bc72c39774b0a0cb38ed03e5ee09fa78103ed749""><code>bc72c39</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58482"">#58482</a> from tensorflow-jenkins/relnotes-2.9.3-25695</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3506c90f5ac0f471a6b1d60d4055b14ca3da170b""><code>3506c90</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8dcb48e384cd3914458f3c494f1da878ae8dc6d5""><code>8dcb48e</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4f34ec84994e63cf47c1d13748a404edd3d5a0d3""><code>4f34ec8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58576"">#58576</a> from pak-laura/c2.99f03a9d3bafe902c1e6beb105b2f2417...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6fc67e408f239384d26acabc34d287911af92dc8""><code>6fc67e4</code></a> Replace CHECK with returning an InternalError on failing to create python tuple</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5dbe90ad21068007cbc31a56e8ed514ec27e0b26""><code>5dbe90a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58570"">#58570</a> from tensorflow/r2.9-7b174a0f2e4</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.9.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.9.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump numpy from 1.16.4 to 1.22.0,"Bumps [numpy](https://github.com/numpy/numpy) from 1.16.4 to 1.22.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>
<blockquote>
<h2>v1.22.0</h2>
<h1>NumPy 1.22.0 Release Notes</h1>
<p>NumPy 1.22.0 is a big release featuring the work of 153 contributors
spread over 609 pull requests. There have been many improvements,
highlights are:</p>
<ul>
<li>Annotations of the main namespace are essentially complete. Upstream
is a moving target, so there will likely be further improvements,
but the major work is done. This is probably the most user visible
enhancement in this release.</li>
<li>A preliminary version of the proposed Array-API is provided. This is
a step in creating a standard collection of functions that can be
used across application such as CuPy and JAX.</li>
<li>NumPy now has a DLPack backend. DLPack provides a common interchange
format for array (tensor) data.</li>
<li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The
new methods provide a complete set of the methods commonly found in
the literature.</li>
<li>A new configurable allocator for use by downstream projects.</li>
</ul>
<p>These are in addition to the ongoing work to provide SIMD support for
commonly used functions, improvements to F2PY, and better documentation.</p>
<p>The Python versions supported in this release are 3.8-3.10, Python 3.7
has been dropped. Note that 32 bit wheels are only provided for Python
3.8 and 3.9 on Windows, all other wheels are 64 bits on account of
Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.
All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix
the occasional problems encountered by folks using truly huge arrays.</p>
<h2>Expired deprecations</h2>
<h3>Deprecated numeric style dtype strings have been removed</h3>
<p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,
and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>
<h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>
<p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that
users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both
deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with
the appropriate value for the <code>usemask</code> parameter.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>
<li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>
<li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>
<li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>
<li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>
<li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>
<li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>
<li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.16.4...v1.22.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.16.4&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.7.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.7.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.7.2</h2>
<h1>Release 2.7.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h2>TensorFlow 2.7.1</h2>
<h1>Release 2.7.1</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.7.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/dd7b8a3c1714d0052ce4b4a2fd8dcef927439a24""><code>dd7b8a3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56034"">#56034</a> from tensorflow-jenkins/relnotes-2.7.2-15779</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/1e7d6ea26dec19c8be5a67bdb4fa574a69f3da86""><code>1e7d6ea</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/50851350cbafeb82d8edc91cc9974b20db257bab""><code>5085135</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56069"">#56069</a> from tensorflow/mm-cp-52488e5072f6fe44411d70c6af09e...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/adafb45c7812dac1e84d4f23524106ba45d441c2""><code>adafb45</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56060"">#56060</a> from yongtang:curl-7.83.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/01cb1b8bb061c40a7b7b0f632235439ac7ba981e""><code>01cb1b8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56038"">#56038</a> from tensorflow-jenkins/version-numbers-2.7.2-4733</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8c90c2fa07e4376f032a425d863ef11ce357e3c5""><code>8c90c2f</code></a> Update version numbers to 2.7.2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/43f3cdc95f4dc6ea9f6979cdb82005b79103f591""><code>43f3cdc</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/98b0a48e852364f17f5f3b4b6525d8c3efd0e73d""><code>98b0a48</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/dfa5cf382323f0d3ffb4d96477d9d2fbd7d48abb""><code>dfa5cf3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56028"">#56028</a> from tensorflow/disable-tests-on-r2.7</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/501a65c3469bfeafb508c97db0654ad694460167""><code>501a65c</code></a> Disable timing out tests</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.7.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.7.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.6.4,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.6.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.6.4</h2>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h2>TensorFlow 2.6.3</h2>
<h1>Release 2.6.3</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h1>Release 2.8.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Added TFLite builtin op support for the following TF ops:
<ul>
<li><code>tf.raw_ops.Bucketize</code> op on CPU.</li>
<li><code>tf.where</code> op for data types
<code>tf.int32</code>/<code>tf.uint32</code>/<code>tf.int8</code>/<code>tf.uint8</code>/<code>tf.int64</code>.</li>
<li><code>tf.random.normal</code> op for output data type <code>tf.float32</code> on CPU.</li>
<li><code>tf.random.uniform</code> op for output data type <code>tf.float32</code> on CPU.</li>
<li><code>tf.random.categorical</code> op for output data type <code>tf.int64</code> on CPU.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>tensorflow.experimental.tensorrt</code>:</p>
<ul>
<li><code>conversion_params</code> is now deprecated inside <code>TrtGraphConverterV2</code> in
favor of direct arguments: <code>max_workspace_size_bytes</code>, <code>precision_mode</code>,
<code>minimum_segment_size</code>, <code>maximum_cached_engines</code>, <code>use_calibration</code> and</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/33ed2b11cb8e879d86c371700e6573db1814a69e""><code>33ed2b1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56102"">#56102</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e1ec480caf1279bdca5f3fbc243d39c35bb03fce""><code>e1ec480</code></a> Fix build due to importlib-metadata/setuptools</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/63f211ca6b60dfa3bc8451c2cd0a2630e3598b9a""><code>63f211c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56033"">#56033</a> from tensorflow-jenkins/relnotes-2.6.4-6677</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/22b8fe48ce23ae1db23b2daa7b6f1e502d253d86""><code>22b8fe4</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/ec30684dc0922060eedd6887881e7e5bbd5da009""><code>ec30684</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56070"">#56070</a> from tensorflow/mm-cp-adafb45c781-on-r2.6</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/38774ed7562851ee3aae328ad0c238b0492ac641""><code>38774ed</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56060"">#56060</a> from yongtang:curl-7.83.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9ef160463d1b1fef4aa1e55b5cab30422fcc95cc""><code>9ef1604</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56036"">#56036</a> from tensorflow-jenkins/version-numbers-2.6.4-9925</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a6526a3acba675a8425dfad323a634f537c4cbb9""><code>a6526a3</code></a> Update version numbers to 2.6.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/cb1a481ed144908b73c486db72b8315c89b1c0e5""><code>cb1a481</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4da550f0e6c1a02f045daca9d824a68b211f56ac""><code>4da550f</code></a> Insert release notes place-fill</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.6.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.6.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 9.0.1,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>In show_file, use os.remove to remove temporary images. CVE-2022-24303 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6010"">#6010</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>, <a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Restrict builtins within lambdas for ImageMath.eval. CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6009"">#6009</a> [radarhere]</li>
</ul>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@​kmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@​DWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@​aclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@​m-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@​cmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@​ilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@​infmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.1 (2022-02-03)</h2>
<ul>
<li>
<p>In show_file, use os.remove to remove temporary images. CVE-2022-24303 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6010"">#6010</a>
[radarhere, hugovk]</p>
</li>
<li>
<p>Restrict builtins within lambdas for ImageMath.eval. CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6009"">#6009</a>
[radarhere]</p>
</li>
</ul>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/6deac9e3a23caffbfdd75c00d3f0a1cd36cdbd5d""><code>6deac9e</code></a> 9.0.1 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/c04d812b902356b8c20ee2ab881e1d96f7d66b4b""><code>c04d812</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/4fabec36197735438c80c174d018498be606c46c""><code>4fabec3</code></a> Added release notes for 9.0.1</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/02affaa491df37117a7562e6ba6ac52c4c871195""><code>02affaa</code></a> Added delay after opening image with xdg-open</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ca0b58521881b95e47ea49d960d13d1c3dac823d""><code>ca0b585</code></a> Updated formatting</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/427221ef5f19157001bf8b1ad7cfe0b905ca8c26""><code>427221e</code></a> In show_file, use os.remove to remove temporary images</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/c930be0758ac02cf15a2b8d5409d50d443550581""><code>c930be0</code></a> Restrict builtins within lambdas for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/75b69dd239a4647032f67a80d9b444228af2b736""><code>75b69dd</code></a> Dont need to pin for GHA</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cd938a7744cb46e2ea525a0c3dd79aa08f98c150""><code>cd938a7</code></a> Autolink CWE numbers with sphinx-issues</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/2e9c461ca417083c43145a991bf9e1ec93237d89""><code>2e9c461</code></a> Add CVE IDs</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.3,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.3</h2>
<h1>Release 2.5.3</h1>
<p><strong>Note</strong>: This is the last release in the 2.5 series.</p>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
<li>Fixes a division by zero in <code>FractionalMaxPool</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21735"">CVE-2022-21735</a>)</li>
<li>Fixes a number of <code>CHECK</code>-fails when building invalid/overflowing tensor shapes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23569"">CVE-2022-23569</a>)</li>
<li>Fixes an undefined behavior in <code>SparseTensorSliceDataset</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21736"">CVE-2022-21736</a>)</li>
<li>Fixes an assertion failure based denial of service via faulty bin count operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21737"">CVE-2022-21737</a>)</li>
<li>Fixes a reference binding to null pointer in <code>QuantizedMaxPool</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21739"">CVE-2022-21739</a>)</li>
<li>Fixes an integer overflow leading to crash in <code>SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21738"">CVE-2022-21738</a>)</li>
<li>Fixes a heap overflow in <code>SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21740"">CVE-2022-21740</a>)</li>
<li>Fixes an FPE in <code>BiasAndClamp</code> in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23557"">CVE-2022-23557</a>)</li>
<li>Fixes an FPE in depthwise convolutions in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21741"">CVE-2022-21741</a>)</li>
<li>Fixes an integer overflow in TFLite array creation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23558"">CVE-2022-23558</a>)</li>
<li>Fixes an integer overflow in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23559"">CVE-2022-23559</a>)</li>
<li>Fixes a dangerous OOB write in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23561"">CVE-2022-23561</a>)</li>
<li>Fixes a vulnerability leading to read and write outside of bounds in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23560"">CVE-2022-23560</a>)</li>
<li>Fixes a set of vulnerabilities caused by using insecure temporary files (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23563"">CVE-2022-23563</a>)</li>
<li>Fixes an integer overflow in Range resulting in undefined behavior and OOM (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23562"">CVE-2022-23562</a>)</li>
<li>Fixes a vulnerability where missing validation causes <code>tf.sparse.split</code> to crash when <code>axis</code> is a tuple (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
<li>Fixes a <code>CHECK</code>-fail when decoding resource handles from proto (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23564"">CVE-2022-23564</a>)</li>
<li>Fixes a <code>CHECK</code>-fail with repeated <code>AttrDef</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23565"">CVE-2022-23565</a>)</li>
<li>Fixes a heap OOB write in Grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23566"">CVE-2022-23566</a>)</li>
<li>Fixes a <code>CHECK</code>-fail when decoding invalid tensors from proto (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23571"">CVE-2022-23571</a>)</li>
<li>Fixes an unitialized variable access in <code>AssignOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23573"">CVE-2022-23573</a>)</li>
<li>Fixes an integer overflow in <code>OpLevelCostEstimator::CalculateTensorSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23575"">CVE-2022-23575</a>)</li>
<li>Fixes an integer overflow in <code>OpLevelCostEstimator::CalculateOutputSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23576"">CVE-2022-23576</a>)</li>
<li>Fixes a null dereference in <code>GetInitOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23577"">CVE-2022-23577</a>)</li>
<li>Fixes a memory leak when a graph node is invalid (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23578"">CVE-2022-23578</a>)</li>
<li>Fixes an abort caused by allocating a vector that is too large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23580"">CVE-2022-23580</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures during Grappler's <code>IsSimplifiableReshape</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23581"">CVE-2022-23581</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures during Grappler's <code>SafeToRemoveIdentity</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23579"">CVE-2022-23579</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures in <code>TensorByteSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23582"">CVE-2022-23582</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures in binary ops due to type confusion (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23583"">CVE-2022-23583</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.3</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
<li>Fixes a division by zero in <code>FractionalMaxPool</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21735"">CVE-2022-21735</a>)</li>
<li>Fixes a number of <code>CHECK</code>-fails when building invalid/overflowing tensor
shapes
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23569"">CVE-2022-23569</a>)</li>
<li>Fixes an undefined behavior in <code>SparseTensorSliceDataset</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21736"">CVE-2022-21736</a>)</li>
<li>Fixes an assertion failure based denial of service via faulty bin count
operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21737"">CVE-2022-21737</a>)</li>
<li>Fixes a reference binding to null pointer in <code>QuantizedMaxPool</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21739"">CVE-2022-21739</a>)</li>
<li>Fixes an integer overflow leading to crash in <code>SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21738"">CVE-2022-21738</a>)</li>
<li>Fixes a heap overflow in <code>SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21740"">CVE-2022-21740</a>)</li>
<li>Fixes an FPE in <code>BiasAndClamp</code> in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23557"">CVE-2022-23557</a>)</li>
<li>Fixes an FPE in depthwise convolutions in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21741"">CVE-2022-21741</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/959e9b2a0c06df945f9fb66bd367af8832ca0d28""><code>959e9b2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54213"">#54213</a> from tensorflow/fix-sanity-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d05fcbc589a2d45c0d83a3ef384c7b982dc75d4a""><code>d05fcbc</code></a> Fix sanity build</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f2526a06250a1c70c882dff17b1f007642f5c7b8""><code>f2526a0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54205"">#54205</a> from tensorflow/disable-flaky-tests-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a5f94dff540d6351251eda7e380cb1c217b667df""><code>a5f94df</code></a> Disable flaky test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7babe52a746c9b42342b80622fe9d605fe681fac""><code>7babe52</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54201"">#54201</a> from tensorflow/cherrypick-510ae18200d0a4fad797c0bf...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e5d378c4557c84d0326af0bc61a6ec0d8df5856""><code>0e5d378</code></a> Set Env Variable to override Setuptools new behavior</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/fdd419543d8428bf924b765d837722b5a16cb1a8""><code>fdd4195</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54176"">#54176</a> from tensorflow-jenkins/relnotes-2.5.3-6805</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4083165848a741912bd37c6d75466e5508b5376b""><code>4083165</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a2bb7f160b8b7b3f955aecb7da3bdb12d79363ba""><code>a2bb7f1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54185"">#54185</a> from tensorflow/cherrypick-d437dec4d549fc30f9b85c75...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5777ea37552e85c8a1f25c25f8f533cb1299d682""><code>5777ea3</code></a> Update third_party/icu/workspace.bzl</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
libtiff,"when i am trying to install libtiff .it is showing this error
 Building wheel for libtiff (setup.py) ... error"
Bump pillow from 6.0.0 to 9.0.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@​kmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@​DWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@​aclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@​m-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@​cmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@​ilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@​infmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix pushes_fd method signature <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5833"">#5833</a> [<a href=""https://github.com/hoodmane""><code>@​hoodmane</code></a>]</li>
<li>Add support for pickling TrueType fonts <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5826"">#5826</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Only prefer command line tools SDK on macOS over default MacOSX SDK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5828"">#5828</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix compilation on 64-bit Termux <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5793"">#5793</a> [<a href=""https://github.com/landfillbaby""><code>@​landfillbaby</code></a>]</li>
<li>Replace 'setup.py sdist' with '-m build --sdist' <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5785"">#5785</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Use declarative package configuration <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5784"">#5784</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Use title for display in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5788"">#5788</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix for PyQt6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5775"">#5775</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
<li>
<p>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a>
[m-shinder, radarhere]</p>
</li>
<li>
<p>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a>
[cmbruns, radarhere]</p>
</li>
<li>
<p>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/82541b6dec8452cb612067fcebba1c5a1a2bfdc8""><code>82541b6</code></a> 9.0.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cae5ac495badd7c7ecfad8223a08f55f5d2eaacb""><code>cae5ac4</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5924"">#5924</a> from radarhere/cves</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ed4cf7813777ad8478cac46f448bc45416a2a99e""><code>ed4cf78</code></a> CVEs TBD</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d7f60d1d5a746eb01d4cb3c7fb05b6593f46b0f5""><code>d7f60d1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> from radarhere/imagemath_eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8531b01d6cdf0b70f256f93092caa2a5d91afc11""><code>8531b01</code></a> Restrict builtins for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1efb1d9fabd1dfdbf7982035eca0dae7306abef1""><code>1efb1d9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5922"">#5922</a> from radarhere/releasenotes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f6c78713a491764dfac576f6c42127755f2c62b3""><code>f6c7871</code></a> Added release notes for <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> and <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a></li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/032d2dc3658f94718109068ac70799313e440754""><code>032d2dc</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/baae9ec4b67c68e3adaf1208cf54e8de5e38a6fd""><code>baae9ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> from radarhere/jpeg_eoi</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1059eb537639925c96d3245dcd73c106d4266c83""><code>1059eb5</code></a> If appended EOI did not work, do not keep trying</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.2</h2>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory during shape inference in <code>Cudnn*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by <code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to <code>nullptr</code> in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">CVE-2021-41207</a>)</li>
<li>Fixes FPE issues in convolutions with zero size filters (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
<li>Fixes a segfault produced while copying constant resource tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41204"">CVE-2021-41204</a>)</li>
<li>Fixes a vulnerability caused by unitialized access in <code>EinsumHelper::ParseEquation</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41201"">CVE-2021-41201</a>)</li>
<li>Fixes several vulnerabilities and segfaults caused by missing validation during checkpoint loading (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41203"">CVE-2021-41203</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.range</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41202"">CVE-2021-41202</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.image.resize</code> when size is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41199"">CVE-2021-41199</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.tile</code> when tiling tensor is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41198"">CVE-2021-41198</a>)</li>
<li>Fixes a vulnerability produced due to incomplete validation in <code>tf.summary.create_file_writer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41200"">CVE-2021-41200</a>)</li>
<li>Fixes multiple crashes due to overflow and <code>CHECK</code>-fail in ops with large tensor shapes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes a crash in <code>max_pool3d</code> when size argument is 0 or negative (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41196"">CVE-2021-41196</a>)</li>
<li>Fixes a crash in <code>tf.math.segment_*</code> operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41195"">CVE-2021-41195</a>)</li>
<li>Updates <code>curl</code> to <code>7.78.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22922"">CVE-2021-22922</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22923"">CVE-2021-22923</a>,  <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22924"">CVE-2021-22924</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22925"">CVE-2021-22925</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22926"">CVE-2021-22926</a>.</li>
</ul>
<h2>TensorFlow 2.5.1</h2>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory
during shape inference in <code>Cudnn*</code> ops
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by
<code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix
multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to
<code>nullptr</code> in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code>
ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> ([CVE-2021-41207]
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207</a>))</li>
<li>Fixes FPE issues in convolutions with zero size filters
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple
TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/957590ea15cc03ee2e00fc61934647d54836676f""><code>957590e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52873"">#52873</a> from tensorflow-jenkins/relnotes-2.5.2-20787</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2e1d16d7aac34983e4ff0d55f434e4d07fea7bce""><code>2e1d16d</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2fa6dd95659985a9ee9429d146c29c27f12e342c""><code>2fa6dd9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52877"">#52877</a> from tensorflow-jenkins/version-numbers-2.5.2-192</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/480748994b151d28818cdfc659f4332bce8a97b2""><code>4807489</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52881"">#52881</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d398bdfd5d2190a3274141416c54f3e7207e96f3""><code>d398bdf</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/857ad5ef1eb23bbeb378b1f3c3cbe6f38286c2d0""><code>857ad5e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52878"">#52878</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6c2a215be0afd10008046bd072daaf81228a19a1""><code>6c2a215</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f5c57d495753bbc166abc928430ea808aa8aa6b3""><code>f5c57d4</code></a> Update version numbers to 2.5.2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e51f9495418eb373074a652b5bf4bba1c41aa132""><code>e51f949</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2620d2cd5c4e03df6d02ae18fda2a9fdc2466738""><code>2620d2c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52863"">#52863</a> from tensorflow/fix-build-3-on-r2.5</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump opencv-python from 4.1.0.25 to 4.2.0.32,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.2.0.32.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.2.0.32</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.2.0.</p>
<p>Changes:</p>
<ul>
<li>macOS environment updated from xcode8.3 to xcode 9.4</li>
<li>macOS uses now Qt 5 instead of Qt 4</li>
<li>Nasm version updated to Docker containers</li>
<li>multibuild updated</li>
</ul>
<p>Fixes:</p>
<ul>
<li>don't use deprecated brew tap-pin, instead refer to the full package name when installing <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/267"">#267</a></li>
<li>replace get_config_var() with get_config_vars() in setup.py <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/274"">#274</a></li>
<li>add workaround for DLL errors in Windows Server <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/264"">#264</a></li>
</ul>
<h2>4.1.2.30</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.2.</p>
<p>Changes:</p>
<ul>
<li>Python 3.8 builds added to the build matrix</li>
<li>Support for Python 3.4 builds dropped (Python 3.4 is in EOL)</li>
<li>multibuild updated</li>
<li>minor build logic changes</li>
<li>Docker images rebuilt</li>
</ul>
<p>Notes:</p>
<p>Please note that Python 2.7 enters into EOL phase in January 2020. <code>opencv-python</code> Python 2.7 wheels won't be provided after that.</p>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.2.0.32)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump opencv-python from 4.1.0.25 to 4.1.1.26,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.1.1.26.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<ul>
<li>FFmpeg has been compiled with https support on Linux builds <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/229"">#229</a></li>
<li>CI build logic related changes <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/197"">#197</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/227"">#227</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/228"">#228</a></li>
<li>Custom libjepg-turbo removed because it's provided by OpenCV <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/231"">#231</a></li>
<li>64-bit Qt builds are now smaller <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/236"">#236</a></li>
<li>Custom builds should be now rather easy to do locally <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/235"">#235</a>:
<ol>
<li>Clone this repository</li>
<li>Optional: set up ENABLE_CONTRIB and ENABLE_HEADLESS environment variables to 1 if needed</li>
<li>Optional: add additional Cmake arguments to CMAKE_ARGS environment variable</li>
<li>Run <code>python setup.py bdist_wheel</code></li>
</ol>
</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.1.1.26)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.3.2,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.3.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.3.2</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html</a></p>
<h2>Security</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
</ul>
<h2>Python 3.10 wheels</h2>
<ul>
<li>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</li>
</ul>
<h2>Fixed regressions</h2>
<ul>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a> [<a href=""https://github.com/t-vi""><code>@​t-vi</code></a>]</li>
</ul>
<h2>8.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Use snprintf instead of sprintf <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a> [<a href=""https://github.com/kmilos""><code>@​kmilos</code></a>]</li>
<li>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a> [<a href=""https://github.com/newpanjing""><code>@​newpanjing</code></a>]</li>
<li>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a> [<a href=""https://github.com/gofr""><code>@​gofr</code></a>]</li>
<li>Do not use background or transparency index for new color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5564"">#5564</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Simplified code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5315"">#5315</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.3.2 (2021-09-02)</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
<li>
<p>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</p>
</li>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1 (2021-07-06)</h2>
<ul>
<li>
<p>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a>
[radarhere]</p>
</li>
<li>
<p>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a>
[radarhere]</p>
</li>
<li>
<p>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a>
[t-vi, radarhere]</p>
</li>
</ul>
<h2>8.3.0 (2021-07-01)</h2>
<ul>
<li>
<p>Use snprintf instead of sprintf. CVE-2021-34552 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a>
[radarhere]</p>
</li>
<li>
<p>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a>
[kmilos]</p>
</li>
<li>
<p>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a>
[baletu, radarhere, newpanjing, hugovk]</p>
</li>
<li>
<p>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a>
[gofr, radarhere]</p>
</li>
<li>
<p>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8013f130a5077b238a4346b73e149432b180a8ea""><code>8013f13</code></a> 8.3.2 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/23c7ca82f09df6ba1047d2d96714eb825f0d7948""><code>23c7ca8</code></a> Update CHANGES.rst</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8450366be331762ae327036e3c6658c517b05638""><code>8450366</code></a> Update release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/a0afe89990f5ba40a019afc2f22e1b656f8cfd03""><code>a0afe89</code></a> Update test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/9e08eb8f78fdfd2f476e1b20b7cf38683754866b""><code>9e08eb8</code></a> Raise ValueError if color specifier is too long</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bd5cf7db87c6abf7c3510a50170851af5538249f""><code>bd5cf7d</code></a> FLI tests for Oss-fuzz crash.</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/94a0cf1b14f09626c7403af83fa9fef0dfc9bb47""><code>94a0cf1</code></a> Fix 6-byte OOB read in FliDecode</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cece64f4be10ab28b12a83a3555af579dad343a5""><code>cece64f</code></a> Add 8.3.2 (2021-09-02) [CI skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e42238637651f191c2fc6e3f4024348c126e0ccc""><code>e422386</code></a> Add release notes for Pillow 8.3.2</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/08dcbb873217874eee0830fc5aaa1f231c5af4fa""><code>08dcbb8</code></a> Pillow 8.3.2 supports Python 3.10 [ci skip]</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.3.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.3.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.1,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.1</h2>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642"">CVE-2021-37642</a>)</li>
<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641"">CVE-2021-37641</a>)</li>
<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644"">CVE-2021-37644</a>)</li>
<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643"">CVE-2021-37643</a>)</li>
<li>Fixes an integer overflow due to conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645"">CVE-2021-37645</a>)</li>
<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646"">CVE-2021-37646</a>)</li>
<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647"">CVE-2021-37647</a>)</li>
<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648"">CVE-2021-37648</a>)</li>
<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649"">CVE-2021-37649</a>)</li>
<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650"">CVE-2021-37650</a>)</li>
<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651"">CVE-2021-37651</a>)</li>
<li>Fixes a use after free in boosted trees creation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652"">CVE-2021-37652</a>)</li>
<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653"">CVE-2021-37653</a>)</li>
<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654"">CVE-2021-37654</a>)</li>
<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655"">CVE-2021-37655</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656"">CVE-2021-37656</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657"">CVE-2021-37657</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658"">CVE-2021-37658</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659"">CVE-2021-37659</a>)</li>
<li>Fixes a division by 0 in inplace operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660"">CVE-2021-37660</a>)</li>
<li>Fixes a crash caused by integer conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661"">CVE-2021-37661</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662"">CVE-2021-37662</a>)</li>
<li>Fixes a heap OOB in boosted trees (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664"">CVE-2021-37664</a>)</li>
<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663"">CVE-2021-37663</a>)</li>
<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665"">CVE-2021-37665</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666"">CVE-2021-37666</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667"">CVE-2021-37667</a>)</li>
<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668"">CVE-2021-37668</a>)</li>
<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669"">CVE-2021-37669</a>)</li>
<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670"">CVE-2021-37670</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671"">CVE-2021-37671</a>)</li>
<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672"">CVE-2021-37672</a>)</li>
<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673"">CVE-2021-37673</a>)</li>
<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674"">CVE-2021-37674</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676"">CVE-2021-37676</a>)</li>
<li>Fixes a division by 0 in most convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675"">CVE-2021-37675</a>)</li>
<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677"">CVE-2021-37677</a>)</li>
<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678"">CVE-2021-37678</a>)</li>
<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679"">CVE-2021-37679</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations
restoring tensors
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642"">CVE-2021-37642</a>)</li>
<li>Fixes a heap OOB in <code>RaggedGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641"">CVE-2021-37641</a>)</li>
<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644"">CVE-2021-37644</a>)</li>
<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643"">CVE-2021-37643</a>)</li>
<li>Fixes an integer overflow due to conversion to unsigned
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645"">CVE-2021-37645</a>)</li>
<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646"">CVE-2021-37646</a>)</li>
<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647"">CVE-2021-37647</a>)</li>
<li>Fixes an incorrect validation of <code>SaveV2</code> inputs
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648"">CVE-2021-37648</a>)</li>
<li>Fixes a null pointer dereference in <code>UncompressElement</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649"">CVE-2021-37649</a>)</li>
<li>Fixes a segfault and a heap buffer overflow in
<code>{Experimental,}DatasetToTFRecord</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650"">CVE-2021-37650</a>)</li>
<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651"">CVE-2021-37651</a>)</li>
<li>Fixes a use after free in boosted trees creation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652"">CVE-2021-37652</a>)</li>
<li>Fixes a division by 0 in <code>ResourceGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653"">CVE-2021-37653</a>)</li>
<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654"">CVE-2021-37654</a>)</li>
<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655"">CVE-2021-37655</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in
<code>RaggedTensorToSparse</code></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1""><code>8222c1c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381"">#51381</a> from tensorflow/mm-fix-r2.5-build</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d""><code>d584260</code></a> Disable broken/flaky test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7""><code>f6c6ce3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367"">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76""><code>3ca7812</code></a> Update version numbers to 2.5.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6""><code>4fdf683</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361"">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a""><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d""><code>bee1dc4</code></a> Update release notes for the new patch release</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84""><code>47beb4c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597"">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d""><code>6f39597</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383"">#49383</a> from ashahab/abin-load-segfault-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3""><code>0539b34</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979"">#48979</a> from liufengdb/r2.5-cherrypick</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.2.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.2.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.2.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Security fixes for 8.2.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Move getxmp() to JpegImageFile <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5376"">#5376</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a> [<a href=""https://github.com/UrielMaD""><code>@​UrielMaD</code></a>]</li>
<li>Compile LibTIFF with CMake on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5359"">#5359</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a> [<a href=""https://github.com/latosha-maltba""><code>@​latosha-maltba</code></a>]</li>
<li>Tiff crash fixes in TiffDecode.c <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5372"">#5372</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>Remove redundant check (addition to <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>) <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5366"">#5366</a> [<a href=""https://github.com/kkopachev""><code>@​kkopachev</code></a>]</li>
<li>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a> [<a href=""https://github.com/raygard""><code>@​raygard</code></a>]</li>
<li>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a> [<a href=""https://github.com/elejke""><code>@​elejke</code></a>]</li>
<li>Only import numpy when necessary <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5323"">#5323</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Add support for reading TIFFs with PlanarConfiguration=2  <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>More OSS-Fuzz support <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5328"">#5328</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Use quantization method attributes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5353"">#5353</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Removed build_distance_tables return value <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5363"">#5363</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Improved efficiency when creating GIF disposal images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5326"">#5326</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Replaced tiff_deflate with tiff_adobe_deflate compression when saving TIFF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5343"">#5343</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Save ICC profile from TIFF encoderinfo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5321"">#5321</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Moved RGB fix inside ImageQt class <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5268"">#5268</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix -Wformat error in TiffDecode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5305"">#5305</a> [<a href=""https://github.com/lukegb""><code>@​lukegb</code></a>]</li>
<li>Allow alpha_composite destination to be negative <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5313"">#5313</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Ensure file is closed if it is opened by ImageQt.ImageQt <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5260"">#5260</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added ImageDraw rounded_rectangle method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5208"">#5208</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added IPythonViewer <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5289"">#5289</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Only draw each rectangle outline pixel once <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5183"">#5183</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use mmap instead of built-in Win32 mapper <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5224"">#5224</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Handle PCX images with an odd stride <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5214"">#5214</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Only read different sizes for &quot;Large Thumbnail&quot; MPO frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5168"">#5168</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>Updated harfbuzz to 2.8.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5334"">#5334</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<h2>Deprecations</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.2.0 (2021-04-01)</h2>
<ul>
<li>
<p>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a>
[UrielMaD, radarhere]</p>
</li>
<li>
<p>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a>
[latosha-maltba, radarhere]</p>
</li>
<li>
<p>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a>
[zewt, radarhere]</p>
</li>
<li>
<p>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a>
[raygard]</p>
</li>
<li>
<p>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a>
[radarhere]</p>
</li>
<li>
<p>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a>
[radarhere]</p>
</li>
<li>
<p>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a>
[radarhere]</p>
</li>
<li>
<p>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a>
[elejke, radarhere]</p>
</li>
<li>
<p>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a>
[radarhere]</p>
</li>
<li>
<p>Add support for reading TIFFs with PlanarConfiguration=2 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>
[kkopachev, wiredfool, nulano]</p>
</li>
<li>
<p>Deprecated categories <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5351"">#5351</a>
[radarhere]</p>
</li>
<li>
<p>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a>
[nulano]</p>
</li>
<li>
<p>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a>
[nulano]</p>
</li>
<li>
<p>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a>
[radarhere]</p>
</li>
<li>
<p>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a>
[radarhere]</p>
</li>
<li>
<p>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a>
[radarhere, kkopachev]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e0e353c0ef7516979a9aedce3792596649ce4433""><code>e0e353c</code></a> 8.2.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ee635befc6497f1c6c4fdb58c232e62d922ec8b7""><code>ee635be</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> from hugovk/security-and-release-notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/694c84f88f4299319bac49b20bd9baae82ca41b8""><code>694c84f</code></a> Fix typo [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8febdad8dd51ad5c75a1db78492973588c7cbf6b""><code>8febdad</code></a> Review, typos and lint</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/fea419665b75f11910e44cfe6f89622fda63e78b""><code>fea4196</code></a> Reorder, roughly alphabetic</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/496245aa4365d0827390bd0b6fbd11287453b3a1""><code>496245a</code></a> Fix BLP DOS -- CVE-2021-28678</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/22e9bee4ef225c0edbb9323f94c26cee0c623497""><code>22e9bee</code></a> Fix DOS in PSDImagePlugin -- CVE-2021-28675</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ba65f0b08ee8b93195c3f3277820771f5b62aa52""><code>ba65f0b</code></a> Fix Memory DOS in ImageFont</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bb6c11fb889e6c11b0ee122b828132ee763b5856""><code>bb6c11f</code></a> Fix FLI DOS -- CVE-2021-28676</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/5a5e6db0abf4e7a638fb1b3408c4e495a096cb92""><code>5a5e6db</code></a> Fix EPS DOS on _open -- CVE-2021-28677</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.2.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.1.1,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.1.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.1.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.1.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.1.1.html</a></p>
<h2>8.1.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.1.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.1.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Fix TIFF OOB Write error <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5175"">#5175</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix for Buffer Read Overrun in PCX Decoding <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5174"">#5174</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix for SGI Decode buffer overrun <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5173"">#5173</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix OOB Read when saving GIF of xsize=1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5149"">#5149</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>Add support for PySide6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5161"">#5161</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>
<li>Moved QApplication into one test <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5167"">#5167</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use disposal settings from previous frame in APNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5126"">#5126</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Revert &quot;skip wheels on 3.10-dev due to wheel#354&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5163"">#5163</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Better _binary module use <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5156"">#5156</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Added exception explaining that <em>repr_png</em> saves to PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5139"">#5139</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Use previous disposal method in GIF load_end <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5125"">#5125</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Do not catch a ValueError only to raise another <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5090"">#5090</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Allow putpalette to accept 1024 integers to include alpha values <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5089"">#5089</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix OOB Read when writing TIFF with custom Metadata <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5148"">#5148</a> [<a href=""https://github.com/wiredfool""><code>@​wiredfool</code></a>]</li>
<li>Removed unused variable <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5140"">#5140</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix dereferencing of potential null pointers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5111"">#5111</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Fixed warnings assigning to &quot;unsigned char *&quot; from &quot;char *&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5127"">#5127</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Add append_images support for ICO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4568"">#4568</a> [<a href=""https://github.com/ziplantil""><code>@​ziplantil</code></a>]</li>
<li>Fixed comparison warnings <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5122"">#5122</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Block TIFFTAG_SUBIFD <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5120"">#5120</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Fix dereferencing potential null pointer <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5108"">#5108</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Replaced PyErr_NoMemory with ImagingError_MemoryError <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5113"">#5113</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Remove duplicate code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5109"">#5109</a> [<a href=""https://github.com/cgohlke""><code>@​cgohlke</code></a>]</li>
<li>Moved warning to end of execution <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4965"">#4965</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Removed unused fromstring and tostring C methods <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5026"">#5026</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>init() if one of the formats is unrecognised <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5037"">#5037</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>Updated libtiff to 4.2.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5153"">#5153</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated openjpeg to 2.4.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5151"">#5151</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated harfbuzz to 2.7.4 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5138"">#5138</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated harfbuzz to 2.7.3 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5128"">#5128</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated libraqm to 0.7.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5070"">#5070</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated libimagequant to 2.13.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5065"">#5065</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Update FriBiDi to 1.0.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5064"">#5064</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>
<li>Updated libraqm to 0.7.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5063"">#5063</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
<li>Updated libjpeg-turbo to 2.0.6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5044"">#5044</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>
</ul>
<h2>Deprecations</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.1.1 (2021-03-01)</h2>
<ul>
<li>
<p>Use more specific regex chars to prevent ReDoS. CVE-2021-25292
[hugovk]</p>
</li>
<li>
<p>Fix OOB Read in TiffDecode.c, and check the tile validity before reading. CVE-2021-25291
[wiredfool]</p>
</li>
<li>
<p>Fix negative size read in TiffDecode.c. CVE-2021-25290
[wiredfool]</p>
</li>
<li>
<p>Fix OOB read in SgiRleDecode.c. CVE-2021-25293
[wiredfool]</p>
</li>
<li>
<p>Incorrect error code checking in TiffDecode.c. CVE-2021-25289
[wiredfool]</p>
</li>
<li>
<p>PyModule_AddObject fix for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5194"">#5194</a>
[radarhere]</p>
</li>
</ul>
<h2>8.1.0 (2021-01-02)</h2>
<ul>
<li>
<p>Fix TIFF OOB Write error. CVE-2020-35654 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5175"">#5175</a>
[wiredfool]</p>
</li>
<li>
<p>Fix for Read Overflow in PCX Decoding. CVE-2020-35653 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5174"">#5174</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Fix for SGI Decode buffer overrun. CVE-2020-35655 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5173"">#5173</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Fix OOB Read when saving GIF of xsize=1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5149"">#5149</a>
[wiredfool]</p>
</li>
<li>
<p>Makefile updates <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5159"">#5159</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Add support for PySide6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5161"">#5161</a>
[hugovk]</p>
</li>
<li>
<p>Use disposal settings from previous frame in APNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5126"">#5126</a>
[radarhere]</p>
</li>
<li>
<p>Added exception explaining that <em>repr_png</em> saves to PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5139"">#5139</a>
[radarhere]</p>
</li>
<li>
<p>Use previous disposal method in GIF load_end <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5125"">#5125</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/741d8744a54bedbc49f16922c61a06fcb3681f53""><code>741d874</code></a> 8.1.1 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/179cd1c8f94aabc47e9e522e01683ea9aadbd3a5""><code>179cd1c</code></a> Added 8.1.1 release notes to index</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/7d296653da045e18b379c991797f933e054a7476""><code>7d29665</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d25036fca7c8658b698492088361453bb20073e2""><code>d25036f</code></a> Credits</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/973a4c333ab6d603e82f6eb2aa6f39d1cfcecccb""><code>973a4c3</code></a> Release notes for 8.1.1</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/521dab94c7ab72b037bd9a83e9663401e0fd2cee""><code>521dab9</code></a> Use more specific regex chars to prevent ReDoS</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8b8076bdcb3815be0ef0d279651d8d1342b8ea61""><code>8b8076b</code></a> Fix for CVE-2021-25291</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e25be1e33dc526bfd1094bc778a54d8e29bf66c9""><code>e25be1e</code></a> Fix negative size read in TiffDecode.c</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f891baa604636cd2506a9360d170bc2cf4963cc5""><code>f891baa</code></a> Fix OOB read in SgiRleDecode.c</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cbfdde7b1f2295059a20a539ee9960f0bec7b299""><code>cbfdde7</code></a> Incorrect error code checking in TiffDecode.c</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.1.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.1.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
resource exhausted at gtx1070,"dear friend！
when I use your code ，I have a problem about “ resource exhausted”
could you tell me what type about your GPU and my types is GTX1070（8G）
and could you give me  some advice at not change my GPU's type.

thanks! hope get your help!"
Bump tensorflow-gpu from 1.4.0 to 2.3.1,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.3.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.3.1</h2>
<h1>Release 2.3.1</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes several vulnerabilities in <code>RaggedCountSparseOutput</code> and <code>SparseCountSparseOutput</code> operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196"">CVE-2020-15196</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197"">CVE-2020-15197</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198"">CVE-2020-15198</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199"">CVE-2020-15199</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200"">CVE-2020-15200</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201"">CVE-2020-15201</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Fixes several vulnerabilities in TFLite implementation of segment sum (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212"">CVE-2020-15212</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213"">CVE-2020-15213</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214"">CVE-2020-15214</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes deprecated usage of <code>collections</code> API</li>
<li>Removes <code>scipy</code> dependency from <code>setup.py</code> since TensorFlow does not need it to install the pip package</li>
</ul>
<h2>TensorFlow 2.3.0</h2>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:
<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot"">snapshot</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/service"">tf.data service</a>.</li>
</ul>
</li>
</ul>
<p>In addition checkout the detailed <a href=""https://www.tensorflow.org/guide/data_performance_analysis"">guide</a> for analyzing input pipeline performance with TF Profiler.</p>
<ul>
<li>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy""><code>tf.distribute.TPUStrategy</code></a> is now a stable API and no longer considered experimental for TensorFlow. (earlier <code>tf.distribute.experimental.TPUStrategy</code>).</p>
</li>
<li>
<p><a href=""https://www.tensorflow.org/guide/profiler"">TF Profiler</a> introduces two new tools: a memory profiler to visualize your model’s memory usage over time and a <a href=""https://www.tensorflow.org/guide/profiler#events"">python tracer</a> which allows you to trace python function calls in your model. Usability improvements include better diagnostic messages and <a href=""https://tensorflow.org/guide/profiler#collect_performance_data"">profile options</a> to customize the host and device trace verbosity level.</p>
</li>
<li>
<p>Introduces experimental support for Keras Preprocessing Layers API (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing?version=nightly""><code>tf.keras.layers.experimental.preprocessing.*</code></a>) to handle data preprocessing operations, with support for composite tensor inputs. Please see below for additional details on these layers.</p>
</li>
<li>
<p>TFLite now properly supports dynamic shapes during conversion and inference. We’ve also added opt-in support on Android and iOS for <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack"">XNNPACK</a>, a highly optimized set of CPU kernels, as well as opt-in support for <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md#running-quantized-models-experimental"">executing quantized models on the GPU</a>.</p>
</li>
<li>
<p>Libtensorflow packages are available in GCS starting this release. We have also started to <a href=""https://github.com/tensorflow/tensorflow#official-builds"">release a nightly version of these packages</a>.</p>
</li>
<li>
<p>The experimental Python API <a href=""https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info""><code>tf.debugging.experimental.enable_dump_debug_info()</code></a> now allows you to instrument a TensorFlow program and dump debugging information to a directory on the file system. The directory can be read and visualized by a new interactive dashboard in TensorBoard 2.3 called <a href=""https://www.tensorflow.org/tensorboard/debugger_v2"">Debugger V2</a>, which reveals the details of the TensorFlow program including graph structures, history of op executions at the Python (eager) and intra-graph levels, the runtime dtype, shape, and numerical composistion of tensors, as well as their code locations.</p>
</li>
</ul>
<h2>Breaking Changes</h2>
<ul>
<li>Increases the <strong>minimum bazel version</strong> required to build TF to <strong>3.1.0</strong>.</li>
<li><code>tf.data</code>
<ul>
<li>Makes the following (breaking) changes to the <code>tf.data</code>.</li>
<li>C++ API: - <code>IteratorBase::RestoreInternal</code>, <code>IteratorBase::SaveInternal</code>, and <code>DatasetBase::CheckExternalState</code> become pure-virtual and subclasses are now expected to provide an implementation.</li>
<li>The deprecated <code>DatasetBase::IsStateful</code> method is removed in favor of <code>DatasetBase::CheckExternalState</code>.</li>
<li>Deprecated overrides of <code>DatasetBase::MakeIterator</code> and <code>MakeIteratorFromInputElement</code> are removed.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.3.1</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes several vulnerabilities in <code>RaggedCountSparseOutput</code> and
<code>SparseCountSparseOutput</code> operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196"">CVE-2020-15196</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197"">CVE-2020-15197</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198"">CVE-2020-15198</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199"">CVE-2020-15199</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200"">CVE-2020-15200</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201"">CVE-2020-15201</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Fixes several vulnerabilities in TFLite implementation of segment sum
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212"">CVE-2020-15212</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213"">CVE-2020-15213</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214"">CVE-2020-15214</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes deprecated usage of <code>collections</code> API</li>
<li>Removes <code>scipy</code> dependency from <code>setup.py</code> since TensorFlow does not need it
to install the pip package</li>
</ul>
<h1>Release 2.2.1</h1>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/fcc4b966f1265f466e82617020af93670141b009""><code>fcc4b96</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43446"">#43446</a> from tensorflow-jenkins/version-numbers-2.3.1-16251</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4cf223069a94c78b208e6c829d5f938a0fae7d07""><code>4cf2230</code></a> Update version numbers to 2.3.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/eee82247288e52e9b8a5c2badeb65f871b4da4c4""><code>eee8224</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43441"">#43441</a> from tensorflow-jenkins/relnotes-2.3.1-24672</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0d41b1dfc97500e1177cb718a0b14b04914df661""><code>0d41b1d</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d99bd631ea9b67ffc39c22b35fbf7deca77ad1f7""><code>d99bd63</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d71d3ce2520587b752e5d27b2d4a4ba8720e4bd5""><code>d71d3ce</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43414"">#43414</a> from tensorflow/mihaimaruseac-patch-1-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9c91596d4d24bc07b6d36ae48581a2e7b2584edf""><code>9c91596</code></a> Fix missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f9f12f61867159120ce6eb08fdbd225d454232b5""><code>f9f12f6</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43391"">#43391</a> from tensorflow/mihaimaruseac-patch-4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3ed271b0b05b4f1dfd5660944c54b5fe8cc3d8dc""><code>3ed271b</code></a> Solve leftover from merge conflict</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9cf3773b717dfd46b37be2ba8cad4f038a8ff6f7""><code>9cf3773</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43358"">#43358</a> from tensorflow/mm-patch-r2.3</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.3.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 1.15.4,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 1.15.4</h2>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses both NumPy and TensorFlow headers.</li>
</ul>
<h2>TensorFlow 1.15.3</h2>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Updates <code>sqlite3</code> to <code>3.31.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880"">CVE-2019-19880</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244"">CVE-2019-19244</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a></li>
<li>Updates <code>curl</code> to <code>7.69.1</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601"">CVE-2019-15601</a></li>
<li>Updates <code>libjpeg-turbo</code> to <code>2.0.4</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664"">CVE-2018-19664</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330"">CVE-2018-20330</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960"">CVE-2019-13960</a></li>
<li>Updates Apache Spark to <code>2.4.5</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099"">CVE-2019-10099</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190"">CVE-2018-17190</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770"">CVE-2018-11770</a></li>
</ul>
<h2>TensorFlow 1.15.2</h2>
<h1>Release 1.15.2</h1>
<p>Note that this release no longer has a single pip package for GPU and CPU. Please see <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36347"">#36347</a> for history and details</p>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes a security vulnerability where converting a Python string to a <code>tf.float16</code> value produces a segmentation fault (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215"">CVE-2020-5215</a>)</li>
<li>Updates <code>curl</code> to <code>7.66.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482"">CVE-2019-5482</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481"">CVE-2019-5481</a></li>
<li>Updates <code>sqlite3</code> to <code>3.30.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646"">CVE-2019-19646</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168"">CVE-2019-16168</a></li>
</ul>
<h2>TensorFlow 1.15.0</h2>
<h1>Release 1.15.0</h1>
<p>This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.</p>
<h2>Major Features and Improvements</h2>
<ul>
<li>As <a href=""https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0"">announced</a>, <code>tensorflow</code> pip package will by default include GPU support (same as <code>tensorflow-gpu</code> now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. <code>tensorflow-gpu</code> will still be available, and CPU-only packages can be downloaded at <code>tensorflow-cpu</code> for users who are concerned about package size.</li>
<li>TensorFlow 1.15 contains a complete implementation of the 2.0 API in its <code>compat.v2</code> module. It contains a copy of the 1.15 main module (without <code>contrib</code>) in the <code>compat.v1</code> module. TensorFlow 1.15 is able to emulate 2.0 behavior using the <code>enable_v2_behavior()</code> function.
This enables writing forward compatible code: by explicitly importing either <code>tensorflow.compat.v1</code> or <code>tensorflow.compat.v2</code>, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.</li>
<li><code>EagerTensor</code> now supports numpy buffer interface for tensors.</li>
<li>Add toggles <code>tf.enable_control_flow_v2()</code> and <code>tf.disable_control_flow_v2()</code> for enabling/disabling v2 control flow.</li>
<li>Enable v2 control flow as part of <code>tf.enable_v2_behavior()</code> and <code>TF2_BEHAVIOR=1</code>.</li>
<li>AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside <code>tf.function</code>-decorated functions. AutoGraph is also applied in functions used with <code>tf.data</code>, <code>tf.distribute</code> and <code>tf.keras</code> APIS.</li>
<li>Adds <code>enable_tensor_equality()</code>, which switches the behavior such that:
<ul>
<li>Tensors are no longer hashable.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>,
and
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses
both NumPy and TensorFlow headers.</li>
</ul>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/df8c55ce12b5cfc6f29b01889f7773911a75e6ef""><code>df8c55c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43442"">#43442</a> from tensorflow-jenkins/version-numbers-1.15.4-31571</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e8cbcb0b1756de4afda8677add8a55355720ab7""><code>0e8cbcb</code></a> Update version numbers to 1.15.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5b65bf202a00f558784e61b7dba5063195cce0f5""><code>5b65bf2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43437"">#43437</a> from tensorflow-jenkins/relnotes-1.15.4-10691</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/814e8d83f5966af55168bc1141dc8ba68561556f""><code>814e8d8</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/757085e3e62197ab5ad6a10c667aae08a8929556""><code>757085e</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e99e53dda53644e49f4b8b4ec16ef92f6399fc3b""><code>e99e53d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43410"">#43410</a> from tensorflow/mm-fix-1.15</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bad36df000e97cfe0a271e08778a81db4ce8834a""><code>bad36df</code></a> Add missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f3f1835aed4ab1874c0891c487cd6d0340fed67b""><code>f3f1835</code></a> No <code>disable_tfrt</code> present on this branch</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7ef5c62a21f2c03483c21566dd6c048218dced26""><code>7ef5c62</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43406"">#43406</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/abbf34a5885400f81620df23d9da70f30630e699""><code>abbf34a</code></a> Remove import that is not needed</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Pre-trained model link,"The pre-trained model link seems broken:

https://drive.google.com/file/d/1U21Bwchcm96-Nz1SVEo0IvSoPyqqA7gm/view?usp=sharing

Any chance you could upload it again?

Thank you for sharing your work!"
Library error libtiff,"I'm getting when trying to run unet.py
cannot import name 'tif_lzw' from 'libtiff' ."
could use RGB image of the test data?,
Bump tensorflow-gpu from 1.4.0 to 1.15.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.2.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.2
> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
>   * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
>   * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
>   * Expand support for TensorFlow operators in TensorRT conversion (e.g.
>     `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
>   * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
>      significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> 
> # Release 2.1.0
> 
> TensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.
> 
> ## Major Features and Improvements
> * The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install ""Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019"", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).
>   * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.
>   * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.
> * The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.
> * `tf.keras`
>   * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).
>   * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).
>   * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.
>   * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).
>   * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.
>   * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.
>   * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.
>   * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).
> * `tf.data`
>   * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.
>   * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.
>   * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`
> * `tf.debugging`
>   * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.
> * `tf.distribute`
>   * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.
>   * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.
> * `TensorRT`
>   * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.
> * Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to ""true"" or ""1"", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\*D and MaxPool\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.
> 
> ## Breaking Changes
> * Deletes `Operation.traceback_with_start_lines` for which we know of no usages.
> * Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.
> * Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> * The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.
> * `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.
> * `tf.config.experimental_list_devices` has been removed, please use
> `tf.config.list_logical_devices`.
> 
> ## Bug Fixes and Other Changes
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`5d80e1e`](https://github.com/tensorflow/tensorflow/commit/5d80e1e8e6ee999be7db39461e0e79c90403a2e4) Merge pull request [#36215](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36215) from tensorflow-jenkins/version-numbers-1.15.2-8214
- [`71e9d8f`](https://github.com/tensorflow/tensorflow/commit/71e9d8f8eddfe283943d62554d4c676bdaf79372) Update version numbers to 1.15.2
- [`e50120e`](https://github.com/tensorflow/tensorflow/commit/e50120ee34e1e29252f4cbc8ac4cd328e9a9840c) Merge pull request [#36214](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36214) from tensorflow-jenkins/relnotes-1.15.2-2203
- [`1a7e9fb`](https://github.com/tensorflow/tensorflow/commit/1a7e9fbf670ef9d03b2f8fdf1ae2276b2d100fab) Releasing 1.15.2 instead of 1.15.1
- [`85f7aab`](https://github.com/tensorflow/tensorflow/commit/85f7aab93b65ed1fcc589f54d40793b1afb65bf4) Insert release notes place-fill
- [`e75a6d6`](https://github.com/tensorflow/tensorflow/commit/e75a6d6e6e20df83f19e72e04c7984587d768bd3) Merge pull request [#36190](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36190) from tensorflow/mm-r1.15-fix-v2-build
- [`a6d8973`](https://github.com/tensorflow/tensorflow/commit/a6d897351e483dfd0418e5cad2900ad9ef24188c) Use `config=v1` as this is `r1.15` branch.
- [`fdb8589`](https://github.com/tensorflow/tensorflow/commit/fdb85890df5df1e6b3867c842aabb44f561b446d) Merge pull request [#35912](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/35912) from tensorflow-jenkins/relnotes-1.15.1-31298
- [`a6051e8`](https://github.com/tensorflow/tensorflow/commit/a6051e8094c5e7d26ec9573a740246c92e4057a2) Add CVE number for main patch
- [`360b2e3`](https://github.com/tensorflow/tensorflow/commit/360b2e318af2db59152e35be31c8aab1fb164088) Merge pull request [#34532](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/34532) from ROCmSoftwarePlatform/r1.15-rccl-upstream-patch
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.2)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.2)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 1.15.0,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.0.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
>   * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
>   * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
>   * Expand support for TensorFlow operators in TensorRT conversion (e.g.
>     `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
>   * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
>      significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. 
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * EagerTensor now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
>   * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.
>   * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.
> * `tf.lite`
>   * Add `GATHER` support to NN API delegate.
>   * tflite object detection script has a debug mode.
>   * Add delegate support for `QUANTIZE`.
>   * Added evaluation script for COCO minival.
>   * Add delegate support for `QUANTIZED_16BIT_LSTM`.
>   * Converts hardswish subgraphs into atomic ops.
> * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`590d6ee`](https://github.com/tensorflow/tensorflow/commit/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b) Merge pull request [#31861](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/31861) from tensorflow-jenkins/relnotes-1.15.0rc0-16184
- [`b27ac43`](https://github.com/tensorflow/tensorflow/commit/b27ac431aa37cfeb9d5c35cc50081cdb6763a40e) Update RELEASE.md
- [`07bf663`](https://github.com/tensorflow/tensorflow/commit/07bf6634f602757ef0b2106a92c519d09e80157e) Merge pull request [#33213](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33213) from Intel-tensorflow/mkl-dnn-0.20.6
- [`46f50ff`](https://github.com/tensorflow/tensorflow/commit/46f50ff8a0f099269ac29573bc6ac09d1bc6cab7) Merge pull request [#33262](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33262) from tensorflow/ggadde-1-15-cp2
- [`49c154e`](https://github.com/tensorflow/tensorflow/commit/49c154e17e9fdfe008f8b0b929d1a729e5939c51) Merge pull request [#33263](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33263) from tensorflow/ggadde-1-15-final-version
- [`a16adeb`](https://github.com/tensorflow/tensorflow/commit/a16adeb793b587a08958a72cbbf0d338e063a042) Update TensorFlow version to 1.15.0 in preparation for final relase.
- [`8d71a87`](https://github.com/tensorflow/tensorflow/commit/8d71a87b0e3de6d07588f9139660a77271d12498) Add saving of loaded/trained compatibility models in test and fix a compatibi...
- [`8c48aff`](https://github.com/tensorflow/tensorflow/commit/8c48affdf8ec0e5a9c5252f88e63aa5b97daf239) [Intel Mkl] Upgrading MKL-DNN to 0.20.6 to fix SGEMM regression
- [`38ea9bb`](https://github.com/tensorflow/tensorflow/commit/38ea9bbfea423eb968fcc70bc454471277c9537c) Merge pull request [#33120](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33120) from tensorflow/perf
- [`a8ef0f5`](https://github.com/tensorflow/tensorflow/commit/a8ef0f5d3bff3fe6f46b821832a4e9073dd7c01d) Automated rollback of commit db7e43192d405973c6c50f6e60e831a198bb4a49
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.0)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.0)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
which semantic segmentation software did you use for this project ?,
how to run this code on kaggle kernel??,when I was trying to run the code on kaggle kernel it shows import error for libtiff . can you help with this? i tried replacing libtiff with PIL but again some error came.
MemoryError,"I got some trouble, that when I read all train_x images, I got a problem named  MemoryError. The image size is 7200*6800*4 , I'm so confused, could you help me?
**the content of train.py  is:**
```
#!usr/bin/env python
#-*- coding:utf-8 _*-
#@author:mqray
#@file: train.py
#@time: 2019/6/28 12:35

import glob,os
from libtiff import TIFF
from funcs import *
from keras.preprocessing.image import ImageDataGenerator

model = model.UNet(16)

train_src_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_train\src\*.tif')
train_label_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_train\label\*.tif')
val_src_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_val\src\*.tif')
val_label_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_val\label\*.tif')
test_src_filelist =  glob.glob(r'E:\2019rscup_segamentation\data\main_test\*.tif')
print(train_src_filelist)
#训练集
train_x = []
for train_src in train_src_filelist:
    tif = TIFF.open(train_src)
    img = tif.read_image()
    crop_lists = crops(img)
    train_x = train_x + crop_lists
    # print(train_x.dtype)
# print(len(train_src_tmp))

trainx = np.asarray(train_x)


train_y = []
for train_label in train_label_filelist[0]:
    tif = TIFF.open(train_label)
    img = tif.read_image()

    crop_lists = crops(img)
    train_y = train_y + crop_lists
trainy = np.asarray(train_y)


#验证集
val_x = []
for val_src in val_src_filelist:
    tif = TIFF.open(val_src)
    img= tif.read_image()

    crop_lists = crops(img)
    val_x = val_x + crop_lists
valx = np.asarray(val_x)

val_y =[]
for val_label in val_label_filelist:
    tif = TIFF.open(val_label)
    img = tif.read_image()

    crop_lists = crops(img)
    val_y = val_y + crop_lists
valy = np.asarray(val_y)

color_dict = {0:(0,200,0),
              1:(150,250,0),
              2:(150,200,150),
              3:(200,0,200),
              4:(150,0,250),
              5:(150,150,250),
              6:(250,200,0),
              7:(200.200,0),
              8:(200,0,0),
              9:(250,0,150),
              10:(200,150,150),
              11:(250,150,150),
              12:(0,0,200),
              13:(0,150,200),
              14:(0,200,250),
              15:(0,0,0)}

'''
将标签值one-hot化
'''
trainy_hot = []
for i in range(trainy.shape[0]):
    hot_img = rgb_to_onehot(train_label_filelist[i], color_dict)
    trainy_hot.append(hot_img)
trainy_hot = np.asarray(trainy_hot)

val_hot = []
for i in range(valy.shape[0]):
    hot_img = rgb_to_onehot(val_label_filelist[i], color_dict)
    val_hot.append(hot_img)
val_hot = np.asarray(val_hot)

trainy  = trainy / np.max(trainy)
valy  = valy / np.max(valy)

# data augmentation

datagen_args = dict(rotation_range=45.,
                         width_shift_range=0.1,
                         height_shift_range=0.1,
                         shear_range=0.2,
                         zoom_range=0.2,
                         horizontal_flip=True,
                         vertical_flip=True,
                         fill_mode='reflect')
x_datagen = ImageDataGenerator(**datagen_args)
y_datagen = ImageDataGenerator(**datagen_args)
seed = 1
batch_size = 16
x_datagen.fit(train_x, augment=True, seed = seed)
y_datagen.fit(trainy, augment=True, seed = seed)
x_generator = x_datagen.flow(train_x, batch_size = 16, seed=seed)
y_generator = y_datagen.flow(trainy, batch_size = 16, seed=seed)
train_generator = zip(x_generator, y_generator)
X_datagen_val = ImageDataGenerator()
Y_datagen_val = ImageDataGenerator()
X_datagen_val.fit(valx, augment=True, seed=seed)
Y_datagen_val.fit(valy, augment=True, seed=seed)
X_test_augmented = X_datagen_val.flow(valx, batch_size=batch_size, seed=seed)
Y_test_augmented = Y_datagen_val.flow(valy, batch_size=batch_size, seed=seed)
test_generator = zip(X_test_augmented, Y_test_augmented)
history = model.fit_generator(train_generator, validation_data=test_generator, validation_steps=batch_size/2, epochs = 10, steps_per_epoch=len(x_generator))
model.save(""model_augment.h5"")



# history = model.fit(train_src_tmp,trainy_hot,epochs=1,validation_data=(val_src_x,val_hot),batch_size=1,verbose=1)
# model.save('model_onehot.h5')

print(history.history.keys())
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train','val'],'upper left')
plt.savefig('acc_plot.jpg')
plt.show()
plt.close()

print(history.history.keys())
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','val'],'upper left')
plt.savefig('loss_plot.jpg')
plt.show()
plt.close()
```

**and the model file is :**
```
#!usr/bin/env python
#-*- coding:utf-8 _*-
#@author:mqray
#@file: uunet.py
#@time: 2019/6/24 10:48

import PIL
from PIL import Image
import matplotlib.pyplot as plt
from libtiff import TIFF
from libtiff import TIFFfile, TIFFimage
from scipy.misc import imresize
import numpy as np
import glob
import cv2
import os
import math
import skimage.io as io
import skimage.transform as trans
from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K


# %matplotlib inline

def UNet(num_class,shape=(512, 512, 4)):
    # Left side of the U-Net
    inputs = Input(shape)
    #    in_shape = inputs.shape
    #    print(in_shape)
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv1)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv2)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv3)
    conv3 = BatchNormalization()(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv4)
    conv4 = BatchNormalization()(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    # Bottom of the U-Net
    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv5)
    conv5 = BatchNormalization()(conv5)
    drop5 = Dropout(0.5)(conv5)

    # Upsampling Starts, right side of the U-Net
    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(drop5))
    merge6 = concatenate([drop4, up6], axis=3)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge6)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv6)
    conv6 = BatchNormalization()(conv6)

    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv6))
    merge7 = concatenate([conv3, up7], axis=3)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge7)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv7)
    conv7 = BatchNormalization()(conv7)

    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv7))
    merge8 = concatenate([conv2, up8], axis=3)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge8)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv8)
    conv8 = BatchNormalization()(conv8)

    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv8))
    merge9 = concatenate([conv1, up9], axis=3)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge9)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv9)
    conv9 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv9)
    conv9 = BatchNormalization()(conv9)

    # Output layer of the U-Net with a softmax activation
    conv10 = Conv2D(num_class, 1, activation='softmax')(conv9)

    model = Model(input=inputs, output=conv10)

    model.compile(optimizer=Adam(lr=0.000001), loss='categorical_crossentropy', metrics=['accuracy'])

    model.summary()

    # filelist_modelweights = sorted(glob.glob('*.h5'), key=numericalSort)

    # if 'model_nocropping.h5' in filelist_modelweights:
    #   model.load_weights('model_nocropping.h5')
    return model

```

"
 ImportError: cannot import name imresize,Hey. I believe you've got a typo in scipy version in requirements.txt. According to this issue on scipy github https://github.com/scipy/scipy/issues/6212 there are no 'imresize' function in scipy.misc above version 1.3. Try changing it to 1.2.
NameError: name 'iou' is not defined in unet.py,"Hi, It seems like variable/function 'iou' is missing in unet.py

My stack trace: 
File ""/macierz/home/s174520/eye-in-the-sky/unet.py"", line 84, in UNet
    model.compile(optimizer = Adam(lr = 0.000001), loss = 'categorical_crossentropy', metrics = ['accuracy', iou])
NameError: name 'iou' is not defined

Edit:
I checked commit history. I think you have added iou function to test_unet.py file istead of unet.py and you're not passing it to UNet model."
requirements.txt file has incorrect structure,"Hi, could you please fix structure of the requirements.txt file that anybody can use pip install -r requirements.txt command? Also, you didn't mention any info about the versions of libraries you used. "
Issue with pretrained weight file model_onehot.h5,"I am trying to use pretrained weight file model_onehot.h5. I changed the file name at all relevant places in test_unet.py. However, when we load the weight file, it gives following error:

ValueError: Dimension 0 in both shapes must be equal, but are 1 and 9. Shapes are [1,1,16,3] and [9,16,1,1]. for 'Assign_82' (op: 'Assign') with input shapes: [1,1,16,3], [9,16,1,1].

Seems like when loading model.load_weights(weights_file), there is some inconsistency for dimensions."
Please mention the validation set in README.md,"Hi, I think everyone who presented in this competition just played it dirty by not mentioning the validation set. I'd like to request that you mention the images taken in the validation set so that reader will not be misled by the results if one tries to reproduce them. Btw nice code  we'll also make our code public soon :)"
Update test_unet.py,
How to get the RGB images?,"Very great work!
I want to ask, is your RGB images rendered by yourself? Is there color information?
If I want to get a dataset which includes RGB images and point clouds of each single object, is there such a data set?
Looking forward to your reply!
Thank you!"
"What is ""disf"" in ""PerspectiveGridGenerator.lua""","Hi, I'm trying to replicate your projection work with PyTorch. But I'm confused about the definition of ""focal length"" and ""disf"" in your code. That is:
___________________________________________________
focal_length = math.sqrt(3)/2 ? 
dmin = 1/(focal_length + math.sqrt(3))
dmax = 1/(focal_length)
for k=1,depth do
    disf = dmin + (k-1)/(depth-1) * (dmax-dmin)
baseGrid[k][i][j][1] = 1/disf
___________________________________________________
Please forgive my offense, I have followed your advice and read the appropriate books. I still don't understand how focal length is defined. And in the paper your point that ""the minimum and maximum disparity in the camera frame are denoted as dmin and dmax"", So who is a disparty, disf or 1/disf? Your help means a lot to me."
About focal_length and the translate matrix,"Thank you for what you have done!  I'm confused about the translate matrix .here are the specific question:
(1)Is the original focal length=1(i.e. if elevation=0deg) ?
(2)Why the translate matrix set to be as follows at first?
![1](https://user-images.githubusercontent.com/41320257/68128617-01dadd80-ff53-11e9-9272-0f3cca857986.png)

could you please explain it?
I'll appreciate for you reply!"
TensorFlow implement,"I am sorry to bother you . Recently  i want to reproduct the PTN with tensorflow implementation . I want to use my own data , but having problems in making tfrecords format data. Could you please show me the codes to make  tfrecords format data with the following features(float representations) : image , mask, vox . Looking forward to your reply , thanks."
"Hi, I can't download data ,model. Web pages cannot download data sets and models？？Thanks",
"Hello， I can't download your data, do you have other method for us to  download your data?",
 tensorflow implementation is broken,"Hi, @xcyan,

The [link ](https://github.com/tensorflow/models/tree/master/ptn) of your tensorflow implementation is broken, could you fix it?

THX"
Out of memory,"Hi,
I was just evaluating the pretrained model and encountered the following issue regarding the memory space of the gpu,
THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
/home/sbasavaraju/torch/install/bin/luajit: ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu:66
stack traceback:
	[C]: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function <...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:245>
	[C]: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	/home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	/home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'
	scripts/eval_quant_test.lua:63: in main chunk
	[C]: in function 'dofile'
	...raju/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

I have Ubuntu 14,04 with graphics model NVIDIA GeForce GTX 470 1 GB , is the graphics not sufficient to run the program ?

Thanks "
Data corrupted,"Hi Doc. @xcyan, I encountered the following problem during running ""eval_models.sh"". 
`torch/install/share/lua/5.1/torch/File.lua:351: read error: read 39866875 blocks instead of 46656000 at torch/pkg/torch/lib/TH/THDiskFile.c:356`
I thought it is because of the data corruption. Could you please share the checksum of the pretrained models for the data validation? Thank you. "
Only half of the training data is used,"In https://github.com/xcyan/nips16_PTN/blob/master/scripts/train_rotatorRNN_base.lua#L274  in the expression math.min(data:size() * opt.nview / 2 , opt.ntrain), I don't understand where the division by two is coming. data:size() is the number of trainings scenes you have, 4744 in this case, and you have 24 views per scene, so multiplication makes sense, but why divide it by two. Maybe I missed something, but it seems that only half of the views/data is used?"
Results of trained models don't match paper,"Hello,
after training the encoder(CNN-Vol) and the perspective transformer (PTN-Proj) I test the final model by changing the lines:
`base_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(0,1)_ks24_vs32/net-epoch-100.t7')
encoder = base_loader.encoder
base_voxel_dec = base_loader.voxel_dec

unsup_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(1,0)_ks24_vs32/net-epoch-100.t7')
unsup_voxel_dec = unsup_loader.voxel_dec

sup_loader = torch.load(opt.checkpoint_dir .. 'ptn_comb.t7')
sup_voxel_dec = sup_loader.voxel_dec`

The results on the testset are:
cat [chair]:	CNN-VOL IOU = 0.459553	PTN-COMB IOU = 0.162989	PTN-PROJ IOU = 0.472389
	
which are 4 to 5 points lower than reported in the paper. What could be the reasons for it? Also I noticed the pretrained ptn-comb model has some problems (0.16) when evaluated with my encoder (instead of the pretrained encoder). What is the reason for this?
"
Can't test the trained encoder,"Hi,
I trained the single class encoder with ./demo_pretrain_singleclass.sh. Now I wanted to evaluate the trained models. So in eval_quant_test.lua I just changed the name of the loaded file (cnn_vol.t7) to the last trained model:
base_loader = torch.load(opt.checkpoint_dir .. 'arch_rotatorRNN_singleclass_nv24_adam2_bs8_nz512_wd0.001_lbg10_ks16/net-epoch-20.t7')
encoder = base_loader.encoder
base_voxel_dec = base_loader.voxel_dec

When I run the testcript eval_models.sh I get following error:
/home/meeso/torch/install/bin/luajit: scripts/eval_quant_test.lua:90: attempt to index global 'base_voxel_dec' (a nil value)
stack traceback:
	scripts/eval_quant_test.lua:90: in main chunk
	[C]: in function 'dofile'
	...eeso/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

Any idea how I fix this?"
Results don't match paper,"Hi,
I downloaded the pretrained models and after running run ./eval_models.sh and got these numbers
CNN-VOL IOU = 0.500177	PTN-COMB IOU = 0.509016	PTN-PROJ IOU = 0.503761	
they are not far off, but still don't correspond to the paper numbers, any idea why I am getting different results?"
typo?,"There might be a typo in `scripts/eval_quant_test.lua`:
`require 'stn'     ------> require 'ptn'`"
latest TF 2.13,
Does it support causal mask?,"The effect is the same as the argument ""use_casual_mask"" in keras official layer class Attention.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention
Thanks for your useful module and expect your reply.
"
Add support for the bahdanau score,
Number of parameters in Attention layer,"Thank you for your contribution of attention python package.

When I am using it as a novice, I have two questions. 
If you have time available, can you give me a  #hand?

In the next example code you provided, 

1) Can you explain to me how to calculate the number of parameters in Attention layer (8192)?
    I can calculate the number of LSTM and Dense layers (16896, 33) 
    but despite many attempts, I can't figure it out how to calculate 8192 in the case of Attention layer.

2) This attention in the example belongs to Luong's version or Bahdanau's version?

----------------- Example code you provided --------------------

num_samples, time_steps, input_dim, output_dim = 100, 10, 1, 1
data_x = np.random.uniform(size=(num_samples, time_steps, input_dim))
data_y = np.random.uniform(size=(num_samples, output_dim))

model_input = Input(shape=(time_steps, input_dim))
x = LSTM(64, return_sequences=True)(model_input)
x = Attention(32)(x)
x = Dense(1)(x)
model = Model(model_input, x)

* Model Structure
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
 Layer (type) ______________________ Output Shape ______ Param   
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
 input_10 (InputLayer)  ___________  [(None, 10, 1)]  ____ 0                                                                 
 lstm_146 (LSTM) ________________  (None, 10, 64)  ____ 16896                                                   
 attention_146 (Attention) _______ (None, 32)  ________ 8192                                                        
 dense_283 (Dense) ______________ (None, 1)  _________ 33    
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Total params: 25,121
Trainable params: 25,121
Non-trainable params: 0
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
TypeError: __call__() takes 2 positional arguments but 3 were given,"when i use the code，got TypeError: __call__() takes 2 positional arguments but 3 were given

whta is the problem？ how to fix it ？

thinks"
Update lib + misc,
Please update version,"Running `pip install attention` installs version 3 (Sep 2020) but there have been updates since.  Currently I cannot specify the # of units without receiving an error:

> TypeError: __init__() takes 1 positional argument but 2 were given

The traceback shows an init method that does not handle the `units` parameter (old code)."
"TypeError: Expected `trainable` argument to be a boolean, but got: 64","I'm getting this error suddenly,
```
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-26-f6e02ef33fa0> in <module>()
----> 1 regressor = best_lstm_model(N_FEATURES, BATCH_SIZE, TIME_STEPS)

2 frames

<ipython-input-24-fcdb2d61d17a> in best_lstm_model(n_features, batch_size, look_back)
     34 
     35     d = Add(name='add')([b1, b2])
---> 36     a = Attention(64)(d)
     37 
     38     y = Dense(1)(a)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)
    339              trainable.dtype is tf.bool)):
    340       raise TypeError(
--> 341           'Expected `trainable` argument to be a boolean, '
    342           f'but got: {trainable}')
    343     self._trainable = trainable

TypeError: Expected `trainable` argument to be a boolean, but got: 64
```

Here's my model:
```
from tensorflow import keras as keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import CSVLogger

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Input, Flatten, Add, Concatenate, Dot, Multiply, Bidirectional, GaussianNoise
from tensorflow.keras.layers import Maximum, Average, Activation

from attention import Attention
import tensorflow
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# attention: https://github.com/philipperemy/keras-attention-mechanism

def best_lstm_model(n_features, batch_size, look_back):
    """"""
    Returns a keras LSTM model. Our architecture will be kept 
    in this method.
    """"""
    x_i1 = Input((look_back, 1), name='ip1')
    g = GaussianNoise(0.05, name='g')(x_i1)

    b1 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l1'), name='b1')(x_i1)
    b1 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l3'), name='b3')(b1)

    b2 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l2'), name='b2')(g)
    b2 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l4'), name='b4')(g)
    

    d = Add(name='add')([b1, b2])
    a = Attention(64)(d)

    y = Dense(1)(a)

    model = Model(x_i1, y)

    model.compile(optimizer = 'adam', loss = 'mean_squared_error')

    return model
```"
Attention not working for MLP,"I need to add attention to my following model. It works perfectly for LSTM model but I get the below error : 
```
def get_ANN_attention_model(num_hidden_layers, num_neurons_per_layer, dropout_rate, activation_func, train_X):
    with tf.device('/gpu:0'):
        model_input = tf.keras.Input(shape=(train_X.shape[1]))  # input layer.
        for i in range(num_hidden_layers):
            x = layers.Dense(num_neurons_per_layer,activation=activation_func,bias_regularizer=L1L2(l1=0.0, l2=0.0001),activity_regularizer=L1L2(1e-5,1e-4))(model_input)
            x = layers.Dropout(dropout_rate)(x)
            x = Attention(num_hidden_layers)(x)
        outputs = layers.Dense(1, activation='linear')(x)
        model = tf.keras.Model(inputs=model_input, outputs=outputs)
        model.summary()
    return model
```

**ERROR**
    hidden_size = int(hidden_states.shape[2])
  File ""C:\Users\bhask\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\tensor_shape.py"", line 896, in __getitem__
    return self._dims[key].value
IndexError: list index out of range"
Output with multiple time steps,"Hi, 

Can this be used for predicting output with multiple time-steps?
If no, how can the code be changed to accommodate this? Thanks. "
what do the h_t mean in the Attention model?,"Hi there!
Thanks so much for implementing this and all of the other work that you do!
I wanna know the meaning of h_t，i.e h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states) . Well, in Luong's paper the h_t was used as the input the hidden state. But how to explain it in a scene which is not seq2seq?
 "
Attention Mechanism not working,"Hi,
I have added an attention layer (following the example) to my simple LSTM network shown below. 

`timestep = timesteps`
`features = 11`
`model = Sequential()`
`model.add(LSTM(64, input_shape=(timestep,features), return_sequences=True))`
`model.add(Dropout(0.2))`
`model.add(LSTM(32, return_sequences=True))`
`model.add(LSTM(16, return_sequences=True))`
`model.add(Attention(32))`
`model.add(Dense(32))`
`model.add(Dense(16))`
`model.add(Dense(1))`
`print(model.summary())`
The code worked fine up till last week and I got a summary of model having attention layer details like this:
![image](https://user-images.githubusercontent.com/61421364/119895079-5ebe3000-bf0b-11eb-9f06-9298f9ea9f68.png)


However, now running the same code gives me a weird error. 
`ValueError: tf.function-decorated function tried to create variables on non-first call.`

What I noticed is that the model summary has changed too:
![image](https://user-images.githubusercontent.com/61421364/119895129-6bdb1f00-bf0b-11eb-9ca9-3fd20a70869d.png)


I am tight on time due an upcoming deadline. Any assistance would be highly appreciated. 
P.S. This was a fully working model that has stopped working all of a sudden for no apparent reason."
Add guidance to README to use Functional API for saving models that use this layer,"Hi there!

Thanks so much for implementing this and all of the other work that you do!

I ran in to an issue with loading a model uses this the Attention layer in a sequential model. However, the Attention layer is defined using the Function API and Keras does not like it when you try to load a mixed model. 

Specifically, my error was 

```
m = keras.models.load_model('saved_mixed_model_path',
            custom_objects = { 'Attention': Attention}
           )

=> ValueError: A merge layer should be called on a list of inputs.

```

To solve this, I had to convert my model to one that uses the functional API and retrain.

Part of my confusion stems from the examples where both the Sequential and Functional APIs are used. In [this example](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/example-attention.py) you successfully save and load a model using only Functional API. But in [this lstm example](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/find_max.py) the Sequential API is used and no loading/saving is done.

Could a caveat be added to the README.md saying that if you plan to load/save these models, only the Functional API should be used when building the model that uses the Attention layer?

Cheers"
Interpreting attention weights for more than one input features.,"How can we get attention weights for each input feature when our input consists of multiple inputs?
I am getting only one array of attention weights and I am not sure how to interpret it for multiple inputs.

shape of attention weights (attached as fig) is:
(300, 6) 
where 6 is the sequence_length/lookback steps/time steps. 

![attention_weight](https://user-images.githubusercontent.com/52854229/110059164-0877a000-7da7-11eb-84d1-4687600b6889.png)
"
Loading model problems,"When I'm trying to load a saved model, I get the following error. ! ""A `Dot` layer should be called on a list of 2 inputs""."
Using attention with multivariate timeseries data,"Hey, I' am trying to use attention with timeseries data that has more than 1 feature this leads to an incompatible shapes error. What changes do I make to get it to work?"
get_config,"Hi,
Perhaps do you have another implementation with the get_config function for saving the model in keras? I had been trying but I always get this error: 
raise ValueError('A `Dot` layer should be called '

ValueError: A `Dot` layer should be called on a list of 2 inputs.

Thanks!
"
attention when using more than one feature,"Hi Philip
Your example of attention has 1 feature (2000, 20,1),  my dataset has 60 features (200, 1000,60), in that case I have to do something different to what you do in your example?  

Thank you!"
weird attention weights when adding sequence of numbers.,"I am trying to slightly modify your example of [adding numbers](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/example-attention.py) such that the target is the sum of all the numbers in the sequence before delimiter. Below is the modified code

```python

def add_numbers_before_delimiter(n: int, seq_length: int, delimiter: float = 0.0,
                                         index_1: int = None) -> (np.array, np.array):
    """"""
    Task: Add all the numbers that come before the delimiter.
    x = [1, 2, 3, 0, 4, 5, 6, 7, 8, 9]. Result is y =  6.
    @param n: number of samples in (x, y).
    @param seq_length: length of the sequence of x.
    @param delimiter: value of the delimiter. Default is 0.0
    @param index_1: index of the number that comes after the first 0.
    @return: returns two numpy.array x and y of shape (n, seq_length, 1) and (n, 1).
    """"""
    x = np.random.uniform(0, 1, (n, seq_length))
    y = np.zeros(shape=(n, 1))
    for i in range(len(x)):
        if index_1 is None:
            a = np.random.choice(range(1, len(x[i])), size=1, replace=False)
        else:
            a = index_1
        y[i] =  np.sum(x[i, 0:a])
        x[i, a] = delimiter

    x = np.expand_dims(x, axis=-1)
    return x, y


def main():
    numpy.random.seed(7)

    # data. definition of the problem.
    seq_length = 20
    x_train, y_train = add_numbers_before_delimiter(20_000, seq_length)
    x_val, y_val = add_numbers_before_delimiter(4_000, seq_length)

    # just arbitrary values. it's for visual purposes. easy to see than random values.
    test_index_1 = 4
    x_test, _ = add_numbers_before_delimiter(10, seq_length, 0, test_index_1)
    # x_test_mask is just a mask that, if applied to x_test, would still contain the information to solve the problem.
    # we expect the attention map to look like this mask.
    x_test_mask = np.zeros_like(x_test[..., 0])
    x_test_mask[:, test_index_1:test_index_1 + 1] = 1

    model = Sequential([
        LSTM(100, input_shape=(seq_length, 1), return_sequences=True),
        SelfAttention(name='attention_weight'),
        Dropout(0.2),
        Dense(1, activation='linear')
    ])

    model.compile(loss='mse', optimizer='adam')
    print(model.summary())

    output_dir = 'task_add_two_numbers'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    max_epoch = int(sys.argv[1]) if len(sys.argv) > 1 else 200

    class VisualiseAttentionMap(Callback):

        def on_epoch_end(self, epoch, logs=None):
            attention_map = get_activations(model, x_test, layer_names='attention_weight')['attention_weight']

            # top is attention map.
            # bottom is ground truth.
            plt.imshow(np.concatenate([attention_map, x_test_mask]), cmap='hot')

            iteration_no = str(epoch).zfill(3)
            plt.axis('off')
            plt.title(f'Iteration {iteration_no} / {max_epoch}')
            plt.savefig(f'{output_dir}/epoch_{iteration_no}.png')
            plt.close()
            plt.clf()

    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=max_epoch,
              batch_size=64, callbacks=[VisualiseAttentionMap()])


if __name__ == '__main__':
    main()

```

I was expecting the model to focus on all values in `x_test` sequence before index `4`. However as you can see in gif, the model focuses on just one point. Can you please elaborate where I am mistaking? 

Thank in advance.

![add_numbers](https://user-images.githubusercontent.com/25817388/95296837-6bc2dc00-08b4-11eb-96bb-0f85023f82ac.gif)
"
add sequential examples + keras layer,"Update function to ""real"" Keras Layer."
2D attention,"
@philipperemy 

Do you know how I can apply the attention module to a 2D shaped input , I would like to apply to apply attention after the LSTM layer-

```
Layer (type)                    Output Shape         Param #     Connected to                     
features (InputLayer)           (None, 16, 1816)     0                                            
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 2048)         31662080    features[0][0]                   
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         2098176     lstm_1[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 1024)         0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 120)          123000      leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
feature_weights (InputLayer)    (None, 120)          0                                            
__________________________________________________________________________________________________
multiply_1 (Multiply)           (None, 120)          0           dense_3[0][0]                    
                                                                 feature_weights[0][0]            

Total params: 33,883,256
Trainable params: 33,883,256
Non-trainable params: 0
__________________________________________________________________________________________________
```
Would really appreciate your suggestion on how to modify attention_3D block to make it work for a 2D input as well. thanks."
Use this repository for CNN ,"Dear Sir,

Would it be possible to use this repo for CNN network also?

Thanks and regards."
"pip install and numpy, keras packages are forced to be uninstalled","Hi,

As I install the keras-attention-mechanism to my conda3 by pip, the essential packages of numpy and keras are unexpectedly being uninstalled. Do you know why? 

Bests,
Peiwan"
"Hiddent state parameter, what really should be passed? ","Hi, thanks for the implementation!
I have been trying to implement this code
`model = Sequential() 
model.add(Embedding(300000, 100, input_length=250))
model.add(LSTM(units=250, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))
model.add(attention_3d_block( )) 
model.add(Flatten())
 model.add(Dense(200, activation='relu'))
model.add(Dense(3, activation='softmax'))`


**Error**  `TypeError: attention_3d_block() missing 1 required positional argument: 'hidden_states'
`
I tried to explore the given documentation but I couldn't understand what really should be passed there. 
"
Restricting attention weights to domain,"In my application, the attention weights are centering on locations which are indicative of a subset of the classes. Therefore, while the algorithm performs well on this subset, it sometimes misclassifies on the other classes because the attention weights cause the obvious differences to be considered ""residual"". 

Is there a documented way of restricting the attention weights to a certain value or index domain to enforce constraints on its focus?  This question makes me think of NLP problems where frameworks commonly pair ML methodologies with a set of predetermined rules (usually defined with spacy).

Any thoughts? Thanks in advance."
Visualizing attention weights with input arrays,"When predicting on test data with the trained model, how can I visualize the attention weights? I'd like to study where the model designates as ""important areas"". 

For reference, my input data is usually of shape (100, 900, 4) with  3 output classification options.

Thanks!"
attention implementation help for OCR,"Hi,

how can i add the attention model fot keras image_ocr implementation 
"
CI + examples,
ask a problem about your code,"in your code ,you want to pay more attention on the 10th step. your Experimental results also prove it.
But, your code seems not foucs on the 10th step. please read following code.
` score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states) 
    #            score_first_part           dot        last_hidden_state     => attention_weights
    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)
    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)
    score = dot([score_first_part, h_t], [2, 1], name='attention_score') `

the way you calculate ‘score’ is  score_first_part dot h_t. 
the way you get h_t  :  h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state') .  in my view 'lambda x: x[:, -1, :]' means you choose the last step in the time sequence , in other word, you pay more attention on the 20th step.(in your  code you define TIME_STEPS = 20). 
so, if my understanding is right, you should change you code to be   h_t = Lambda(lambda x: x[:, 9, :], output_shape=(hidden_size,), name='last_hidden_state') . 
of course, my understanding perhaps wrong. i am lookingforward your reply . 
thank you."
where is dense attention implementation ？,
"what is the meaning of the second parameter in dot([], [1, 1], name='context_vector')","Hi, Thanks for your awesome work. 
I have a confusion about the code: context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')
What is the meaning of the second parameter?"
Update to the attention mechanism,Related to thread: https://github.com/philipperemy/keras-attention-mechanism/issues/14
Update README.md,mini (nano) grammar fix :)
TypeError: 'module' object is not callable,"> output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
At this line error happens:

output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
TypeError: 'module' object is not callable

Not sure what is wrong. Could you help to resolve?"
visualizing soft attention,How can we visualize the soft attention similar to the Bengio et al. paper?
What is the logic behind the attention layer?,"Ik would like to understand intuitively or theoretically, how the attention layer reflects the attention of the model for a prediction?
Because it is easy for the model to give equal weight for each input feature in the attention layer, and that defeats the purpose of the attention layer."
papers using dense attention mechanism,"Hello,

Is the dense attention mechanism based on a particulier paper?
Or are there papers using this mechanism?

"
How to do Stacked LSTM with attention using this framework ?,"hello, 

I have run your code successful. 

I have also include stacked  LSTM in your code : 

```
def model_attention_applied_before_lstm():
    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))
    attention_mul = attention_3d_block(inputs)
    lstm_units = 32
    attention_mul = LSTM(lstm_units, return_sequences=True)(attention_mul)
    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)
    output = Dense(1, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    return model

```

But maybe this is not the correct way to apply staked LSTM with attention right ?

My ultimate goal is to include attention into this code (classification of multivariate time series ) : 
```

class LSTMNet:
    @staticmethod
    def build(timeSteps,variables,classes):
        inputNet = Input(shape=(timeSteps,variables))
       lstm=Bidirectional(GRU(100,recurrent_dropout=0.4,dropout=0.4,return_sequences=True),merge_mode='concat')(inputNet) 
       lstm=Bidirectional(GRU(50,recurrent_dropout=0.4,dropout=0.4,return_sequences=True),merge_mode='concat')(lstm) 
        lstm=Bidirectional(GRU(20,recurrent_dropout=0.4,dropout=0.4,return_sequences=False),merge_mode='concat')(lstm) 
        # a softmax classifier
        classificationLayer=Dense(classes,activation='softmax')(lstm)
        model=Model(inputNet,classificationLayer)
        return model
```


Thanks in advance for any possible info "
why add a Dense(64) layer after the attention layer,what's the point of adding another `attention_mul = Dense(units=64)(attention_mul)` ?
"get_activations use  multi-input data, does not work.","Here is the error message
```
    layer_name='attention_vec')[0], axis=2).squeeze()
  File ""/Users/yu/proj/cancel_blame/code/src/lib/attention/attention_utils.py"", line 16, in get_activations
    layer_outputs = [func([inputs, 1.])[0] for func in funcs]
  File ""/Users/yu/proj/cancel_blame/code/src/lib/attention/attention_utils.py"", line 16, in <listcomp>
    layer_outputs = [func([inputs, 1.])[0] for func in funcs]
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2666, in __call__
    return self._call(inputs)
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2619, in _call
    dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))
AttributeError: 'list' object has no attribute 'dtype'
```"
"Update dependeicies to latest Tensorflow, with minior bug fixes.","The current code base is not working anymore even with the default `pip install -r requirements.txt`.

So I upgrade it to using the latest tensorflow (v1.12.0). Other dependencies are all updated as well."
Updated code to keras 2.2,
One to One keras model with Attention in Keras ,"Hello,

I have a keras model that has sequence of inputs and sequence of outputs where each input has an associated output(Label). lets say (part of speech tagging (POS tagging)

Seq_in[0][0:3]
array([[15],[28], [23]])


Seq_out[0][0:3]
array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
	   [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],
dtype=float32)


I want to build attention on top of the lstm layer. I am following this work "" Attention-Based Bidirectional Long Short-Term Memory Networks for
Relation Classification  "" Zhou et al, 2016



X_train, X_val, Y_train, Y_val = train_test_split(Seq_in,Seq_out, test_size=0.20)

TIME_STEPS = 500
INPUT_DIM = 1
lstm_units = 256



    
inputs = Input(shape=(TIME_STEPS, INPUT_DIM))

activations = Bidirectional(LSTM(lstm_units, return_sequences=True))(inputs) # First laer bidirictional 
activations = Dropout(0.2)(activations)
activations = Bidirectional(LSTM(lstm_units, return_sequences=True))(activations) # Second layer bidirectional 
activations = Dropout(0.2)(activations)
attention = Dense(1,activation='tanh')(activations) # This is equation (9) in the paper. Squashing each output state vector to a scaler. 
attention = Flatten()(attention)
attention = Activation('softmax')(attention) # This is equation (10) in the paper.
attention = RepeatVector(512)(attention) # Repeating the softmax vector to have the same dimintion as the output state  vector (512)
attention = Permute([2,1])(attention) # permute 

sent_representation = multiply([activations,attention]) # multiply the attention vector with the output state vector element-wise. 
sent_representation = Lambda(lambda xin: K.sum(xin, axis=-1))(sent_representation) # summation of all output state vectors 
sent_representation = RepeatVector(TIME_STEPS)(sent_representation) # Repeat vector to be the same diminsion as the time steps
sent_representation = concatenate([activations,sent_representation]) # concatenate the sentence representation to the output states 



output = Dense(15, activation='softmax')(sent_representation)#(out_attention_mul) # Find the softmax for the current label
model = Model(inputs=inputs, outputs=output)  
    

 
sgd = optimizers.SGD(lr=.1,momentum=0.9,decay=1e-3,nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(X_train,Y_train,epochs=2, validation_data=(X_val, Y_val),verbose=1)




__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 500, 1)       0                                            
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 500, 512)     528384      input_1[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500, 512)     0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 500, 512)     1574912     dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 500, 512)     0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 500, 1)       513         dropout_2[0][0]                  
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 500)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 500)          0           flatten_1[0][0]                  
__________________________________________________________________________________________________
repeat_vector_1 (RepeatVector)  (None, 512, 500)     0           activation_1[0][0]               
__________________________________________________________________________________________________
permute_1 (Permute)             (None, 500, 512)     0           repeat_vector_1[0][0]            
__________________________________________________________________________________________________
multiply_1 (Multiply)           (None, 500, 512)     0           dropout_2[0][0]                  
                                                                 permute_1[0][0]                  
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 500)          0           multiply_1[0][0]                 
__________________________________________________________________________________________________
repeat_vector_2 (RepeatVector)  (None, 500, 500)     0           lambda_1[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 500, 1012)    0           dropout_2[0][0]                  
                                                                 repeat_vector_2[0][0]            
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 500, 15)      15195       concatenate_1[0][0]              
==================================================================================================
Total params: 2,119,004
Trainable params: 2,119,004
Non-trainable params: 0
______________________________________________

I think this code performs what the paper does, except that the concatenate step merges the attention weights to all the output state vectors and do not change them for each time step so for each output label. 
So I think, for each time step output, I have to do something so the attention weights differ. Am I right? 
Any help is appreciated

Thanks in advance"
why Permute before attention dense layer in attention_3d_block?,"```
    a = Permute((2, 1))(inputs)
    a = Dense(TIME_STEPS, activation='softmax')(a)
```
this line ,why do you permute times_tep and input_dim
what if I don't permute , and followed by a dense layer with input_dim ? since dense layer is with the shape of  ""time_Step *time_step"" ,what is the difference when I change it to ""input_dim * input_dim""
`Dense(input_Dim activation='softmax')(a)`"
Is this attention is applicable for use with the encoder/decoder mechanism?,
You code is outdated!,"Your code doesn't fit to new versions of keras
To fix it change those strings in ""attention_dense.py"":

1. **""from keras.layers import Input, Dense, merge""** on **""from keras.layers import Input, Dense,multiply""**;
2. **""attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')""** on **""attention_mul = multiply([inputs, attention_probs],name='attention_mul')"" ;
and in ""attention_lstm.py""**:
in""attention_lstm.py"":

1. 1. **import multiply too**;
2. 2. **""output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')""** change on **""output_attention_mul = multiply([inputs, a_probs], name='attention_mul')""**"
How to implement Multi-Hop Attention using Keras?,MultiHopAttention was proposed by Fackbook.
Attention Visualization,"In the final visualization of the attention weights it says this is showing the attention over input dimensions but the x axis goes to the length of the time steps. So it is showing how important the time step is and not each feature. Shouldn't it be the other way? Where each x is a feature? 

When I apply this to my own dataset it just says the most recent time steps are the most important."
How to visualise as 2dimensional heatmap?,"lets say we are predicting with timestep of 24, and get 24 result as output. how can we visualise as heatmap like in https://github.com/datalogue/keras-attention"
some confusions.,"![attention_luong](https://user-images.githubusercontent.com/11025093/39625088-bbd45082-4fbd-11e8-85c1-5895f1824bed.png)
Hello , Thanks for an easy code to read. But i have some confusions.

1) your attention functions takes the hidden state of input i.e lstm outputs from encoders and then does all the processes then. but according to what I have read , it must form some kind of function with the hidden state of the target , like in the given picture . Why haven't you did that ? otherwise you are just making an lstm function manually.

2) Why have you used permute layers before softmax layer ?
3) why have you averaged the outputs of softmax layer ?"
IndexError: list index out of range,"Dear sir: when I run python attention_dense.py ,the following errors show:

----- activations -----
Traceback (most recent call last):
  File ""attention_dense.py"", line 39, in <module>
    attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()
IndexError: list index out of range

would you please help me ?thank you very much!"
bucketing problem,"My sequences have varying lengths and I’m using bucketing to solve the issue. Therefore I define the LSTM input shape as (None, None, features), i.e. there are no explicit timesteps. I wonder if the code can fit my input? Thanks."
这个是CNN版本的attention吗？,这个是keras实现的CNN加上attention的代码吗？
Questions on implementation details,"Update on 2019/2/14, nearly one year later:

The implementation in this repo is definitely bugged. Please refer to my implementation [in a reply below](https://github.com/philipperemy/keras-attention-mechanism/issues/14#issuecomment-371132446) for correction. My version has been working in our product since this thread and it outperforms both vanilla LSTM without attention and the incorrect version in this repo by a significant margin. I am not the only one raising the question [1].

Both this repo and my version of attention are intended for sequence-to-one networks (although it can be easily tweaked for seq2seq by replacing `h_t` with current state of the decoder step). If you are looking for a ready-to-use attention for sequence-to-sequence networks, check this out: https://github.com/farizrahman4u/seq2seq.

[1]: https://github.com/keras-team/keras/issues/4962#issuecomment-311885962

============Original answer==============

I am currently working on a text generation task and learnt attention from [TensorFlow tutorials](https://www.tensorflow.org/tutorials/seq2seq#intermediate). The implementation details seems quite different from your code.

This is how TensorFlow tutorial describes the process:

![image](https://user-images.githubusercontent.com/2417391/36968826-add96394-209e-11e8-801c-822a710157fa.png)

![image](https://user-images.githubusercontent.com/2417391/36968819-a598f03c-209e-11e8-99d9-c2a411e44303.png)

If I am understanding it correctly, all learnable parameters in the attention mechanism are stored in ![W], which has a shape of `(rnn_size, rnn_size)` (`rnn_size` is the size of hidden state). So first you need to use ![W] to calculate the score of each hidden state based on the value of the hidden state ![h_t] and ![h_s], but I am not seeing ![h_t] anywhere in your code. Instead, you applied a dense layer on all ![h_s]. And that means ![pre_act] __(Edit: h_t should be h_s in this equation)__  becomes the ![score] in the paper. This seems wrong.

In the next step you element-wise multiplies the attention weights with hidden states as equation (2). Then somehow missed the equation (3).

I noticed the tutorial is about Seq2Seq (Encoder-Decoder) model and your code is an RNN. Maybe that is why your code is different. Do you have any source on how attention is applied to a non Seq2Seq network?

Here is your code:

```python
def attention_3d_block(inputs):
    # inputs.shape = (batch_size, time_steps, input_dim)
    input_dim = int(inputs.shape[2])
    a = Permute((2, 1))(inputs)
    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.
    a = Dense(TIME_STEPS, activation='softmax')(a)
    if SINGLE_ATTENTION_VECTOR:
        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)
        a = RepeatVector(input_dim)(a)
    a_probs = Permute((2, 1), name='attention_vec')(a)
    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
    return output_attention_mul


def model_attention_applied_after_lstm():
    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))
    lstm_units = 32
    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)
    attention_mul = attention_3d_block(lstm_out)
    attention_mul = Flatten()(attention_mul)
    output = Dense(1, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    return model
```

[W]: https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cmathit%7B%5Cmathbf%7BW%7D%7D
[h_t]: https://latex.codecogs.com/gif.latex?%5Cinline%20h_t
[h_s]: https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbar%7Bh%7D_s
[pre_act]: https://latex.codecogs.com/gif.latex?%5Cinline%20h_t%5Ctimes%20%5Cmathbf%7B%5Cmathit%7BW_%7Bt%2Ct%27%7D%7D%7D%20&plus;%20b_%7Bt%27%7D
[score]: https://latex.codecogs.com/gif.latex?%5Cinline%20score%28%5Cmathbf%7B%5Cmathit%7Bh_t%7D%7D%2C%5Cmathbf%7B%5Cmathit%7B%5Cbar%7Bh%7D_s%7D%7D%29"
Many to many sequence generation,"Can you give an example of how to use this for many to many sequence generation with different input and output lengths (greater than 1)? For example, if we have input of 10 timesteps say [1,2,3,4,5,6,7,8,9,10] and we want to generate output [1,10]. "
 SINGLE_ATTENTION_VECTOR = false,"Do you have some reference paper, about  **SINGLE_ATTENTION_VECTOR = false** ?

As far as I know, most of papers will set SINGLE_ATTENTION_VECTOR = true.


"
2D LSTM attention,Can we use the same code for 2D LSTM attention ?
Change before and after cases in the IF condition.,
possible bug in attention_lstm.py,"lines 56-59 should be 

```
if APPLY_ATTENTION_BEFORE_LSTM:
  m = model_attention_applied_before_lstm()
else:
  m = model_attention_applied_after_lstm()
```"
fig ,"Hi, I am wondering the figures in your markdown.
What app you used to create these beautiful hand-written figures. 
Thx"
use attention_3d_block in many to many mapping,"Hi, I'm beginner of Keras and tring to use attention_3d_block in translation module.
I have input of 5 sentences, each sentences has padding to 6 words, each word is presented in 620 dim(as embedding dim). 
And the output is 5 sentences, sentences padding to 9 words, and word is presented in 1-of-k in 30 dim(as vocabulary size)
How to use attention_3d_block in this scenario as the LSTM is many to many?

![s b3v8 0fr ex 3 he0wk](https://user-images.githubusercontent.com/21202514/27814158-4571e454-60ad-11e7-8028-bd8f38d593f0.png)
"
attention_lstm.py and Tensorflow,"In the attention_3d_block, I have some questions/bug (I think). I am running on Tensorflow. 
(1) inputs doesn't have a shape method. So it crashes. I assume you meant to call the shape function on  the numpy array on inputs_1.
(2) Is there a reason for calling Permute?
(3) What is the Reshape layer supposed to do? After the call to Permute, isn't the output of the previous permute layer already in shape (Batch Size, input_dim, TIME_STEPS)?
(4) The next call to Dense expects ndim =2, not 3. So the code crashes for me. I assume you meant the previous Reshape layer to map the 3d input to 2d?
(5) I would just like to point out that APPLY_ATTENTION_BEFORE_LSTM is False iff you call model_attention_applied_before_lstm. "
attention_lstm.py does not work for Theano backend,"When I run the script attention_lstm.py, there is a problem in the line 17. Just like the following problem:
""input_dim=int(inputs.shape[2])""
""TypeError: int() argument must be a string or a number, not 'TensorVariable'"""
Small Improvement,Please have a look at this issue for more information: https://github.com/philipperemy/keras-attention-mechanism/issues/3#issuecomment-309312748
Is this Reshape step redundant?,"See this line of code:  https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py#L19

Isnt this redundant?  Because the Permute layer right before it will reshape the Tensor.  

Let me know if I'm missing something.  I am trying to understand attention and thus far your writeup is helping"
edit get_activations,edit output list comprehension
get_activations not producing list,"Thanks for uploading this to github! Great for learning more about attention models. When I run attention_dense.py, however, I get this error (after the model finishing training):

> ---------------------------------------------------------------------------
> IndexError                                Traceback (most recent call last)
> <ipython-input-9-5fdd7f84f2d8> in <module>()
>      37     # Attention vector corresponds to the second matrix.
>      38     # The first one is the Inputs output.
> ---> 39     attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()
>      40     print('attention =', attention_vector)
>      41 
> 
> IndexError: list index out of range

Any idea why the get_activations function isn't working properly?"
Can't enable NEGEX ('--negex') option,"How can I use custom options? I rewrote function extract_concepts from SubprocessBackend class and add some custom options. I added line ""command.append('--negex')"", but there is no difference in result between this approach and the first one. How can I enable negation detection display?"
"Hi, is there anyone use MetaMapLite, I always got an error with no data file","<img width=""744"" alt=""image"" src=""https://user-images.githubusercontent.com/95683271/222970722-bfe7c3cf-8f69-47d0-87a2-c1bcbaf922d5.png"">
<img width=""344"" alt=""image"" src=""https://user-images.githubusercontent.com/95683271/222970739-6f7bdaa2-80fd-46a2-907f-80fd0b73684b.png"">
"
Value extraction from MetaMap,"Hi Anthony,
Thanks for the pymetamap wrapper
I want to extract values of the keys that are being extracted
for example:
input_sent = [""Patient:   John, parker            MRN: 456            FIN: 123            Age: 29""]
from this input it is able to extract ""Patient"" and ""Age"" word but I want to extract the name and age number and such information in key value pair
"
MetaMap ERROR even though servers are running,"Hi, thanks for making this very cool library. I'm having trouble getting the example to work. I get the MetaMap error that I see other people are getting, but I *do* have the servers running, as you can see in this screenshot.

![image](https://user-images.githubusercontent.com/9650729/150874874-9798d64b-e430-46d4-ba92-ff9b27147214.png)

Any ideas what's going on?

P.S. Sifei Han says ""hi"". ;-)"
IndexError: list index out of range,"Hi Antony,

Thanks for your great software. But when I use this tool to process MIMIC dataset, I got the following problem,

File ""/usr/local/python38/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/SubprocessBackend.py"", line 243, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/usr/local/python38/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/Concept.py"", line 75, in load
    if fields[1] == 'MMI':
IndexError: list index out of range

Thanks for your help!
"
Resource error: insufficient memory,"I have the problem of insufficient memory. The sever memory is 256 GB, so ideally, the size is enough. I added `-Xmx16g -Xms2g` to the metamap server scripts to start the services. But I still encounter the error:
```
0it [00:00, ?it/s]! Resource error: insufficient memory
0it [02:27, ?it/s]
Traceback (most recent call last):
  File ""get_concepts.py"", line 133, in <module>
    process_json(task_path, task)
  File ""get_concepts.py"", line 110, in process_json
    results = extract_entities(documents)
  File ""get_concepts.py"", line 87, in extract_entities
    concepts = mm.extract_concepts(
  File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/SubprocessBackend.py"", line 243, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/Concept.py"", line 75, in load
    if fields[1] == 'MMI':
IndexError: list index out of range
```
Here is my function:
```python
def extract_entities(docs):
    results = []
    mm = MetaMap.get_instance('/data/tools/public_mm/bin/metamap')
    for doc in docs:
        concepts = mm.extract_concepts(
            [doc], word_sense_disambiguation=True, unique_acronym_variants=True,
            ignore_stop_phrases=True, no_derivational_variants=True,
        )
        results.append(concepts)

    # below implementation will cause insuficient memory error, even with 16G XMx
    # results = [[]] * len(docs)
    # concepts = mm.extract_concepts(docs, ids=list(range(len(docs))), word_sense_disambiguation=True)
    # for concept in concepts:
    #     results[int(concept.index)].append(concept)

    return results
```"
Why I get different scores use the same code?,"I use the same code you report in the README section, like that:
`meatapath = ""/HardDisk/lsc_part/app/MetaMap/public_mm_lite/""
mm = MetaMapLite.get_instance(meatapath)
sents =  ['Heart Attack', 'John had a huge heart attack']

concepts, error = mm.extract_concepts(sents,[1,2])
for concept in concepts:
	print(concept)
	print(concept.trigger, concept.score)`
but I get the result like:
![image](https://user-images.githubusercontent.com/26566742/96360570-e8757600-1150-11eb-93fc-1e4249795277.png)
the score and tree_codes different from you result :
Concept(index='1', mm='MM', score='14.64', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""Heart attack""-tx-1-""Heart Attack""]', location='TX', pos_info='1:12', tree_codes='C14.280.647.500;C14.907.585.500')
Concept(index='2', mm='MM', score='13.22', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""Heart attack""-tx-1-""heart attack""]', location='TX', pos_info='17:12', tree_codes='C14.280.647.500;C14.907.585.500')"
Support negated concepts,"MetaMap allows detection of negated concepts via the `--negex` option.

**Command**
```
echo ""Fever, but no headache"" | metamap --negex
```

**Output**

```
Phrase: Fever,
Meta Mapping (1000):
  1000   FEVER (Fever) [Sign or Symptom]

Phrase: but

Phrase: no headache.
Meta Mapping (1000):
  1000 N HEADACHE (Headache) [Sign or Symptom]
```

Can you add this option to the `extract_concepts` function?"
Different order return different number of concept,"Hi Anthony,

Thanks for your great work! That's really helpful since I can do all the implementation in python without switching.
The issue for me is every time I shuffle the sentence list, the number of concepts is different. And if I run metamap again on  the non-recognizable words, some turn to be recognized, which makes me confused.

So in my implementation, I have to run metamap constantly on the currently non-recognized sentences until there's no concept return. Actually, I'm still not confident about this approach and kind of worried about what's going on.

For example, there are 1000 sentences in my list. I use extract_concept and there are 379 results. And I run extract_concept again on the remaining 621 results, it returns 89 results this time. I keep running on the remaining until it returns no concept.

I am not sure it's problem with metamap or pymetamap. Let me know if you have any thoughts! Thanks in advance!

Best,
Yiheng"
error while running extract_concepts,"I tried to run the sample input. I used MetaMaplite. I got the following error when using extract_concepts().

```
File ""/usr/local/lib/python3.6/dist-packages/pymetamap-0.2-py3.6.egg/pymetamap/SubprocessBackendLite.py"", line 102, in extract_concepts
    with open(output_file_name) as fd:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpey8rfuom.mmi'.
```

Can you please help? Thanks.

"
position returned by pymetamap,"Hi Anthony, 

Firstly, thank you for the wonderful implementation of metamap. 

However, I was running into some issues while extracting the keywords using pymetamap.

For example, in the sentence itself ""John had a huge heart-attack"", could you please direct me to how to extract the exact position of the keyword identified by pymetamap. It shows position = 17:12, but in several cases, I see the exact character position is off by 1-2 characters. 

Could you provide some insight into this?"
MetaMap Error,"Hey Anthony, thanks a lot for this making this really useful wrapper. I've installed MetaMap and cloned your repository, however when I run
`mm= MetaMap.get_instance('/Users/usr/Downloads/metamapfolder/public_mm/bin/metamap18')`

`mm.extract_concepts('some sentence')`

I get the following error:
`MetaMap ERROR: Calling socket_client_open for TAGGER Server on host localhost and port 1795:
error(system_error,system_error(SPIO_E_NET_CONNREFUSED))`

I know this might not be related to pymetamap but to MetaMap itself, but maybe you know what the issue could be? 

Thanks a lot and all the best."
added no_nums arg,"This commit adds the `no_nums=[]` kwarg to extract_concepts and follows the pattern for including `--no_nums` for the CLI based on this document:
https://metamap.nlm.nih.gov/Docs/FAQ/NoNums.pdf"
converting MetaMap returned output in bytes to str for common handlin…,…g of python 2 and python 3. Resolves issue #9 raised on 11th Dec 2019.
Error Running Example ,"TypeError Traceback (most recent call last)
in 
1 sents = ['Heart Attack']
----> 2 concepts,error = mm.extract_concepts(sents,[1])
3 concepts

~/pyte/pymetamap/SubprocessBackend.py in extract_concepts(self, sentences, ids, composite_phrase, filename, file_format, allow_acronym_variants, word_sense_disambiguation, allow_large_n, strict_model, relaxed_model, allow_overmatches, allow_concept_gaps, term_processing, no_derivational_variants, derivational_variants, ignore_word_order, unique_acronym_variants, prefer_multiple_concepts, ignore_stop_phrases, compute_all_mappings, mm_data_version, exclude_sources, restrict_to_sources, restrict_to_sts, exclude_sts)
152 if ids is not None:
153 for identifier, sentence in zip(ids, sentences):
--> 154 input_text += '{0!r}|{1!r}\n'.format(identifier, sentence).encode('utf8')
155 else:
156 for sentence in sentences:

TypeError: can only concatenate str (not ""bytes"") to str

I tried running the example and this is what I get"
1. In MetaMapLite public_mm_lite folder needs to be passed. Hence changed input parameter name to metamap_home to make it clear.  2. user access check added as done for MetaMap,
MetaMap ERROR,">>> concepts,error = mm.extract_concepts(sents,[1,2])
Berkeley DB databases (USAbase 2018AB strict model) are open.
Static variants will come from table varsan in /home/ztt/NYTexperiments/pymetamap-master/public_mm/DB/DB.USAbase.2018AB.strict.
Derivational Variants: Adj/noun ONLY.
Variant generation mode: static.

### MetaMap ERROR: Calling socket_client_open for TAGGER Server on host localhost and port 1795:
error(system_error,system_error(SPIO_E_NET_CONNREFUSED))
--------------------------------------------------------------------------------------
Hello，how can I solve the problem above?"
Output one result with highest score,"Hi,

I wonder is there a way to output only one result with the highest score for each detected entity? For example,

```
sents = ['metastatic disease']
concepts,error = mm.extract_concepts(sents)

concepts
```

```
[ConceptMMI(index='00000000', mm='MMI', score='14.64', preferred_name='Neoplasm Metastasis', cui='C0027627', semtypes='[neop]', trigger='[""metastatic disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes='C04.697.650;C23.550.727.650'),

 ConceptMMI(index='00000000', mm='MMI', score='5.18', preferred_name='Metastatic Neoplasm', cui='C2939420', semtypes='[neop]', trigger='[""Metastatic Disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes=''),
 
ConceptMMI(index='00000000', mm='MMI', score='5.18', preferred_name='Secondary Neoplasm', cui='C2939419', semtypes='[neop]', trigger='[""Metastatic disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes='')]
```
 only output the first one - 'Neoplasm Metastasis'.

Thanks,
Wei"
Avoiding creating temporary input and output files when input is list of sentences,"Resolving https://github.com/AnthonyMRios/pymetamap/issues/37
Have tested on 2016v2 and used it in [NegBio](https://github.com/ncbi-nlp/NegBio/blob/master/negbio/pipeline/dner_mm.py#L48)
`concepts, error = mm.extract_concepts(sents, ids)`

Though haven't tested on Windows, but it should resolve the issue: https://github.com/AnthonyMRios/pymetamap/issues/10
(As mentioned in https://github.com/AnthonyMRios/pymetamap/issues/9#issuecomment-281675040 using NamedTemporaryFile was the bottleneck.)
Note: echo -e option doesn't works on Windows cmd, but will work if one runs on something like gitbash.

Have followed the approach mentioned in Taymon's answer and the corresponding comments:
https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes
"
Check added for MetaMap file existence and executable access for the user,"As discussed in https://github.com/AnthonyMRios/pymetamap/issues/9#issuecomment-487552373
made the changes.

Followed Jay's answer in https://stackoverflow.com/questions/377017/test-if-executable-exists-in-python"
Avoiding NamedTemporaryFile when input is list of sentences,"MetaMap2016 Usage Notes mentions

> There are two ways to use MetaMap interactively, reading input text from the keyboard and seeing output on the screen:
> 1. metamap [ options ]
> then type your input text, e.g., lung cancer, at the \|:"" prompt.
> 2. echo lung cancer | metamap [ options ]
>
> For processing an input file:
> metamap [ options ] InputFile OutputFile

In pymetamap, we are using the option:
> metamap [ options ] InputFile OutputFile

This is being done by creating temporary input and output file when list of sentences are passed.

Isn't it better if we choose the option:
> echo lung cancer | metamap [ options ]

This will reduce the I/O operations time.

I was making an attempt to choose the above option for the input: list of sentences.
```
~/kaushik/$ echo -e  ""1|'Heart Attack'\n2|'John had a huge heart attack'"" | /home/kaushik/lib/MetaMap/2016v2/public_mm/bin/metamap16 -N -Q 4 -y --sldiID --silent
/home/kaushik/lib/MetaMap/2016v2/public_mm/bin/SKRrun.16 /home/kaushik/lib/MetaMap/2016v2/public_mm/bin/metamap16.BINARY.Linux --lexicon db -Z 2016AA -N -Q 4 -y --sldiID --silent
1|MMI|14.64|Myocardial Infarction|C0027051|[dsyn]|[""-- Heart Attack""-tx-1-""Heart Attack""-noun-0]|TX|1/12|C14.280.647.500;C14.907.585.500
2|MMI|13.22|Myocardial Infarction|C0027051|[dsyn]|[""-- Heart Attack""-tx-1-""heart attack""-noun-0]|TX|17/12|C14.280.647.500;C14.907.585.500
```

Even with the option --silent, I found that MetaMap prints the command as initial part of the output.
If we can identify the lines which corresponds to MMI output format, then this problem can be solved.
"
Adding python wrapper for MetaMapLite,Issue: https://github.com/AnthonyMRios/pymetamap/issues/14
Access metamap if client and server are on different machine?,"If the MetaMap server (mmserver)  is running on different machine other than the one the Java api client is running, we can specify the hostname of the MetaMap server when instantiating the api. 

```
MetaMapApi api = new MetaMapApiImpl(""resource.example.org"");
```
Is such option available in pymetamap?"
Accessing elements within extract_concepts,"Hello Anthony,

Your python wrapper package is great. Quick question: Is there a way to access specific concept attributes within the list that is outputted by the extract_concepts function?

Thanks for all your help.

Haley Howell "
"Added options: allow overmatches, concept gaps, term processing requested in issue #30",
"Added options: restrict_to_sts, exclude_sts to restrict/exclude semantic types",
Assertion error if Metamap binary path not absolute,
Term processing / restrict to semantic types,"Hi - thank you for such an awesome tool! Comparing the tool with the metamap UI, there seem to be a few parameters missing such as:

Being able to only return certain semantic types
Option to select term processing / overmatches / concept gaps

Would it be possible to add these to 'SubprocessBackend.py'?"
what is the utility of backend='subprocess',"Hi Dear Anthony,

Thanks for your great software, I have another two quick questions that, 

1. what is the utility of 

> backend='subprocess'?

Because when I using pymetamap to extract concepts, the screen will show a lot of intermediate processes, is this 'backend='subprocess'' used to make the process silent?

2. Is it possible to get concepts by multi-threading?

Because I am working on a pretty large dataset, thus if I extract their concepts one by one, it will take pretty long time, thus I am thinking is there any chance to use the python multi-threading feature, like this:

> from multiprocessing import Pool
> p = Pool(5)
> def f(x):       <--------------in the function I will extract concepts
...     return x*x 
...
> p.map(f, [1,2,3])

Thanks a million in advance!
"
TypeError: can only join an iterable,"Hi Anthony,

Thanks for your great program! It is super convenient and easy to use!

I have successfully installed everything and was trying to run that Example Usage. However, I got a TypeError like this:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""pymetamap/SubprocessBackend.py"", line 123, in extract_concepts
>     command.append(str(','.join(exclude_sources)))

I try to print the exclude_resources and command, it looks like:
> (False, ['/project/public_mm/bin/metamap16', '-N', '-Q', '4', '-e'])

So basically the exclude_resources is False, it can't be joined by the string method.

Would you please take a look at this error, was I making any mistake?

Thanks a million!"
Filter sources,Add filter sources option.
Interpreting SCORE,"Is there a way to interpret the scores that I get from `pymetamap`. I see that I get different scores for matches from metamap run on the command line and from the pymetamap. I assume this is because of different default settings in each case.
I am trying to see if I can set a certain threshold and filter bad matches. I dont know if this approach is correct.

Example:

`text: John had a huge heart attack`


metamap command line:
`score=901`
output:
   `901   -- Heart Attack (Myocardial Infarction) [Disease or Syndrome]`

pymetamap:
`Score=13.22`
output:
`ConceptMMI(index='2', mm='MMI', score='13.22', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""-- Heart Attack""-tx-1-""heart attack""-noun-0]', location='TX', pos_info='17/12', tree_codes='C14.280.647.500;C14.907.585.500')`

Thanks,
Deepak"
change source,"Hi Anthony,

Firstly thank you for making this API. I wanted to know if there is a way to restrict matches to a certain vocabulary. This is the set of vocabularies  options I can select in Batch Metamap (https://ii.nlm.nih.gov/vocabs/1718/RsourceFrame_USAbase.html).

Thanks,
Deepak"
"fixed warnings using ""pyton setup.py check --resturcutredtext""",It should fix the problem of README.
Metamap Installation Issue,"
Metamap version: 2014 for linux
System: Ubuntu 16.04 LTS
JAVA:1.8 jre
I was following the metamap installation instruction from  **https://metamap.nlm.nih.gov/Installation.shtml**. 
But when I test **echo ""lung cancer"" | ./bin/metamap -I** ,  I got 
Control options:
  composite_phrases=4
  lexicon=db
  mm_data_year=2014AA
  show_cuis
Processing 00000000.tx.1: lung cancer

Phrase: lung cancer
Meta Mapping (1000):
  1000   C0684249:LUNG CANCER (Carcinoma of lung) [Neoplastic Process]
Meta Mapping (1000):
  1000   C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]
Unlike what I looks like in the instruction:
Phrase: ""lung cancer""
Meta Candidates (8):
  1000 C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]
  1000 C0684249:Lung Cancer (Carcinoma of lung) [Neoplastic Process]
   861 C0006826:Cancer (Malignant Neoplasms) [Neoplastic Process]
   861 C0024109:Lung [Body Part, Organ, or Organ Component]
   861 C0998265:Cancer (Cancer Genus) [Invertebrate]
   861 C1278908:Lung (Entire lung) [Body Part, Organ, or Organ Component]
   861 C1306459:Cancer (Primary malignant neoplasm) [Neoplastic Process]
   768 C0032285:Pneumonia [Disease or Syndrome]
Meta Mapping (1000):
  1000 C0684249:Lung Cancer (Carcinoma of lung) [Neoplastic Process]
Meta Mapping (1000):
  1000 C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]

And my servers works fine:
wei@wei-OptiPlex-790:/opt/public_mm$ sudo ./bin/skrmedpostctl start
$Starting skrmedpostctl: 
started.
wei@wei-OptiPlex-790:/opt/public_mm$ sudo ./bin/wsdserverctl start
$Starting wsdserverctl: 
started.
wei@wei-OptiPlex-790:/opt/public_mm$ loading properties file /opt/public_mm/WSD_Server/config/disambServer.cfg
WSD Server initializing disambiguation methods.
WSD Server databases and disambiguation methods have been initialized.

I totally dont understand.
Please Help!
"
Need help in running python wrapper for metamap,"> mm = MetaMap.get_instance('/opt/public_mm/bin/metamap12')

I cannot understand what do you mean by metamap binary? Can you share the link for downloading metamap binary?

I have following files in /public_mm/bin/ directory but couldn't figure out which one is binary:

>db_access.so metamap16.BINARY.Linux   nls_signal.so	 SKRrun.16.in
ginstall.tcl  metamap16.in	       qp_lexicon.so	 uninstall.sh.in
install.log   metamap16.sav	       qp_morph.so	 wsdserverctl.in
install.sh    metamap2016.TEMPLATE.in  SKRenv.16.in
install.tcl   metamap.in	       skrmedpostctl.in
"
dyld: Library not loaded,"When I run the example code, I got this error below. 
Any idea?

> dyld: Library not loaded: /usr/local/BerkeleyDB.4.8/lib/libdb-4.8.dylib
  Referenced from: /users/gracelee/documents/metamap/public_mm/bin/metamap14.BINARY.Darwin
  Reason: image not found
/users/gracelee/documents/metamap/public_mm/bin/metamap2014.TEMPLATE: line 146: 95257 Abort trap: 6           $COMMAND"
Add mm data version,
metamap,"<img width=""1280"" alt=""screen shot 2017-07-28 at 5 12 40 pm"" src=""https://user-images.githubusercontent.com/30526383/28715966-4e05c090-73b8-11e7-9ec6-59231773d624.png"">
I am getting output as empty list instead i should get list containing concepts could you please help me out with this . I want the similar output as yours.Does this code need any servers in metamap to run in the background?"
metamap,I have run the same code but the output it is giving is empty that is if i try to print the concepts it is giving empty list([]) for the text containing heart attack .
Update .gitignore,"I installed this as a submodule to my project. After running `python setup.py install`, it created two directories: 

```
build
dist
```

<img width=""662"" alt=""screen shot 2017-07-21 at 3 46 19 pm"" src=""https://user-images.githubusercontent.com/4664374/28479848-37fdf628-6e2c-11e7-81fa-1114a8e04b97.png"">


Adding this to the `.gitignore` and removed the `dists` directory"
Python 3 support,"Hi,

This pull requests fixes #11 
I tested it on python 3.4.3 and python 2.7.6.

In addition, the `str(metamap_filename)` conversion makes it possible to work with Pathlib's filenames in python 3.4"
How to use commands for Human Readable and Negation Detection with wrapper,"Hi there,

For MetaMap I know that the commands for Human Readable output and Negation Detection are -I and -negex respectively. How would I implement this with the python wrapper? I looked through the code and did not see a command that addresses these two options.

Thanks!"
Any support for MetaMapLite version?,"Hi, Thanks for your great program.

I wonder if you can make your program work with MetaMapLite version. MetaMap tends to be rigorous but slower than the lite version, and it would be nicer if you can support both versions. "
how to set options,"Dear Rios,

thanks for your great work, I can run it in my Mac computer now. 
Here I have one more questions:
 where can we set the output options like the metamap originally have, eg: set semtype to dysn only..."
extract_concepts not running," from pymetamap import MetaMap
>>> mm = MetaMap.get_instance('/Documents/public_mm/bin/metamap13')
>>> sents = ['Heart Attack', 'John had a huge heart attack']
>>> **concepts,error = mm.extract_concepts(sents,[1,2])**
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pymetamap/SubprocessBackend.py"", line 117, in extract_concepts
    metamap_process = subprocess.Popen(command, stdout=subprocess.PIPE)
  File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1343, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory

Getting this error for some time...Please help
"
String format issue with python3.4,"I ran into the following error with PyMetaMap on python 3.4:

> /SubprocessBackend.py"", line 80, in extract_concepts
 input_file.write(b'%r|%r\n' % (identifier, sentence))
TypeError:

 unsupported operand type(s) for %: 'bytes' and 'tuple'

And solved it by replacing the input string format with this:
`input_file.write(bytes(str(identifier) + '|' + sentence + '\n', encoding='UTF-8'))`

Bit of a hack, but works! "
Add Windows Support,#9 
Running error,"I have already installed the metamap13 and when i run the example under windows 7, i get the following error: 
![image](https://cloud.githubusercontent.com/assets/17524879/23212519/12852d3a-f942-11e6-8640-47010f42d5bb.png)
I have also already import the pymetamp and replace the address in the metamap.get_instance method:
![image](https://cloud.githubusercontent.com/assets/17524879/23212628/8f15431c-f942-11e6-841b-5d6647254e15.png)
I just don't know what to do, please help me ^^"
Added python 3.x support,"Hey @AnthonyMRios,

Can you please check this out? This pull request should be able to fix your issue #6 .

Best,
J"
installation error/Running error,"Installed the package fine but it wouln't run, i am not sure if this was an installation error;
`from pymetamap import MetaMap`
it return the error

> from Concept import ConceptMMI
> ImportError: No module named 'Concept'

Any idea what went wrong and how I could fix it?"
Add Compatibility with Python 3.5,
"TypeError: a bytes-like object is required, not 'str'","Hi,
I just discovered the Pymetamap package today and I am new to python.
I am using this package to analyze clinical trial inclusion criteria retrieved from mysql database in the form of dict object.
This is the object I took for experimental analysis:
`result_set={'criteria': 'Male physicians, ages 40 to 84. No history of stroke, myocardial infarction, cancer, or renal disease. No contraindications to aspirin or beta-carotene. No current usage of aspirin or Vitamin A tables greater than once per week.'}`

I first converted the dict object to string:
`str_json = json.dumps(result_set)
`
When I followed the example usage code and tried to run the line 
`     
concepts,error = mm.extract_concepts(str_json)
`

it returns the error: 

> TypeError: a bytes-like object is required, not 'str'

Then I tried to convert to bytes format by running:
`data=str.encode(str_json)
`
And checked the type of the newly generated object:
`type(data)`

It shows that data is of type 'bytes' already.

Thus I ran the concept extraction code again:
`concepts,error = mm.extract_concepts(data)
`

And it still returns the same error asking for a 'bytes-like object'.

Could you please help me figure out what is wrong here? Is there anything I should look into other than the data type conversion (since I already converted the data type)?

I am currently using Python 3 (Anaconda environment).

Thank you so much!!



Tianran
"
Added AA and UA concepts,
 __new__() takes exactly 11 arguments (9 given),"Hi, this is similar to the closed Issue, but I am also having trouble getting a particular list of sentences to return MetaMap concepts. I tried debugging this myself, but couldn't figure it out. I did make a little progress though:

With the following list of sentences: 

`test = ['The patient stated that her general practitioner (GP) and specialist did not think that the problems were the side effects of domperidone.', 'No further information was reported at the time of the report.', 'Other reference number include - MHRA: 01001307081; 01001307081: GB-JNJFOC-20150407438.', 'Duration of suspect drug ""Domperidone"" was reported as 5 years.', '""No medical assessment"".']` :

`concepts, error = mm.extract_concepts(test[0])` 

and 

`concepts, error = mm.extract_concepts(test[1])`

both work, but 

`concepts, error = mm.extract_concepts(test[0:1])` throws the error that I mentioned in the title. Full stack track reproduced below:

``` python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/SubprocessBackend.py"", line 132, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 39, in load
    corpus.append(Concept.from_mmi(line))
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 30, in from_mmi
    return this_class(**dict(zip(FIELD_NAMES, fields)))
TypeError: __new__() takes exactly 11 arguments (9 given)
__new__() takes exactly 11 arguments (9 given)
```

Thank you for your help! 
"
TypeError: __new__() takes exactly 11 arguments (10 given),"Hi Antony, 

I am trying to use your wrapper but I am having some issues with, but only when I submit my own text to extract the concepts from text. 

When I use the example text to run over MetaMap, everyting works just fine. But when I pass my own text, made out of title, abstract and meshcode for articles I get: 

```
concepts,error = mm.extract_concepts(sents,[1,2])
```

  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/SubprocessBackend.py"", line 132, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 39, in load
    corpus.append(Concept.from_mmi(line))
  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 30, in from_mmi
    return this_class(**dict(zip(FIELD_NAMES, fields)))
TypeError: **new**() takes exactly 11 arguments (10 given)

Any help would be welcome. 

[MetaMap_originalText.txt](https://github.com/AnthonyMRios/pymetamap/files/231943/MetaMap_originalText.txt)
"
Unable to use pymetamap in windows,"I installed the pymetamap setup in windows and try to use it, but the metamap instance was unable to extract concepts from the sentences. I tried to run the same code given i.e

> > > from pymetamap import MetaMap
> > > mm = MetaMap.get_instance('...../public_mm/bin/metamap14')
> > > sents = ['Heart Attack', 'John had a huge heart attack']
> > > concepts,error = mm.extract_concepts(sents,[1,2])
> > > but after this I get the error:
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""C:\Anaconda2\lib\site-packages\pymetamap-0.1-py2.7.egg\pymetamap\Subproc
> > > essBackend.py"", line 127, in extract_concepts
> > >     os.remove(input_file.name)
> > > WindowsError: [Error 32] The process cannot access the file because it is being
> > > used by another process: 'c:\users\abhila~1.kas\appdata\local\temp\tmp00rn
> > > u0'
> > > I have metamap14 installed on my pc.
"
What dataset does the pretrained weight train from?,Please tell us which dataset did the pretrained weight train from?
Updated Readme,"Changed the Run Section 
old
`python main.py config/test.json`

this one will not work with python2 also shows to give --config in python3 so changed to 
`python3 main.py --config config/test.json`"
Running Issue,"As I am not used to python, I need some help running the MobileNet. Please help :)

When I do 'python main.py config/test.json'
It prints out 
'main.py: error: unrecognized arguments: config/test.json
Add a config file using '--config file_name.json'

What do I have to do?

Thank you.
"
loss can't down,"Thank for your share? I have a question is as follows:
The loss function cannot be reduced and all samples are predicted to be a certain category

I am looking forward for your answer, thank you very much!"
how can i convert other pre-train weights to pkl format?,"i want to use MobileNet_v1_0.25_224 weights,but i don't know how to convert ckpt to pkl format. can you help me? Thanks.."
Some questions About mobilenet_v1.pkl,"Thank you for your job, Could you tell me How to get the pre-trained model. Is this that you use the ckpt  format file in Tensorflow/model  to convert or retrain with Imagenet data set   "
I have a question during code review.,"Hello. I'm studying about parameter reduction and newbie in deep learning stuffs.

I have a question about data_loader.py

I've ran main.py with config.json that included originally, and it works well i think.

but during data loading, there is only jpeg images and loaded only images without any kinds of class information.

So my question is,

How it(code) calculates predicted accuracy without image's original class information?

I can not find any kinds of loading class information in data_loader.py

plz help me guyz :)"
how to convert mobilenet_v1.pkl to tensorflow pb file?,"Could you guide how to convert mobilenet_v1.pkl to tensorflow pb file?

Thanks!"
How to use the pre-trained model?,"A little new to tf.
I get mobilenet_v1.pkl follow the link you give and load it with pickle.load, then I get a dict with 137 keys.
But how to restore the model form this dict?

Actually, I have implement a net follow this: https://arxiv.org/pdf/1712.07168.pdf.
In this paper, only part of the mobilenet is used as the encoder part. In this case, the pre-trained model can be used?"
Change: Small fixes,Slightly changes of not longer supported tf.to_float() and tf.diag_part() in equivalent tf2.10.0 supported functions
"Revert ""Modifications for TensorFlow 2.0 compatibility """,Reverts guillaumegenthial/tf_metrics#10
issue with installing the package (command errored out with exit status 1),"I attempted to install the package from Jupyter Notebook, using the code suggested in the raeadme file. Unfortunately, I obtain the following error. I also tried to install it from the terminal but nothing changes. Any clue about it? Any mistake in the code? Am I missing some required packages? Thanks 

<img width=""1135"" alt=""Schermata 2020-09-07 alle 17 50 45"" src=""https://user-images.githubusercontent.com/67710089/92464294-394a9400-f1cd-11ea-8a79-8327b3e0cb90.png"">
"
Modifications for TensorFlow 2.0 compatibility ,"Accessing TensorFlow 1.x functions from a TensorFlow 2.x environment requires the prefix `tf.compat.v1.`. This PR implements those changes.

Affected files: `tf_metrics/__init__.py`."
TypeError: '>' not supported between instances of 'NoneType' and 'int',"
/usr/local/lib/python3.6/dist-packages/tf_metrics/__init__.py in precision(labels, predictions, num_classes, pos_indices, weights, average)
     40     """"""
     41     cm, op = _streaming_confusion_matrix(
---> 42         labels, predictions, num_classes, weights)
     43     pr, _, _ = metrics_from_confusion_matrix(
     44         cm, pos_indices, average=average)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py in _streaming_confusion_matrix(labels, predictions, num_classes, weights)
    262 
    263   # Flatten the input if its rank > 1.
--> 264   if predictions.get_shape().ndims > 1:
    265     predictions = array_ops.reshape(predictions, [-1])
    266 

TypeError: '>' not supported between instances of 'NoneType' and 'int'

In python 3, the comparison operators raise an error. Refer to -> https://docs.python.org/3/whatsnew/3.0.html#ordering-comparisons
The ordering comparison operators (<, <=, >=, >) raise a TypeError exception when the operands don’t have a meaningful natural ordering. Thus, expressions like 1 < '', 0 > None or len <= len are no longer valid, and e.g. None < None raises TypeError instead of returning False. A corollary is that sorting a heterogeneous list no longer makes sense – all the elements must be comparable to each other. Note that this does not apply to the == and != operators: objects of different incomparable types always compare unequal to each other."
remove tensorflow from install requires,
enable gpu only if nvidia-smi succeeded,"What about 

def has_gpu():
    try:
        p = Popen([""nvidia-smi""], stdout=PIPE)
        stdout, stderror = p.communicate()
        if p.returncode != 0:
          return False
        return True

?"
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"Hi there, thanks for the very useful lib. 

As soon as I installed tf_metrics on my azure VM, I started getting the libcublas error for importing tensorflow which never occurred before. What's the installation  requirements for tf_metrics? Is there any caution for installing tf_metrics within a specific environment?

I use tensorflow '1.12.0-rc0' gpu with cuda-9.2"
Explicitly specify the encoding while reading the readme file.,"Since the readme file contains a Unicode character, on some systems such as docker container when the terminal encoding is not proper, the setup fails with the following error:

```
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 300: ordinal not in range(128)
```

The error is generated while reading the readme file in setup.py

This pull request will resolve this issue. The proposed solution is tested for Python 2.7 and Python 3.5."
f1 scores are unstable with different evaluation batch size,"Hello~

i used `tf_metrics` in https://github.com/dsindex/BERT-BiLSTM-CRF-NER/blob/master/bert_lstm_ner.py

```
    estimator = tf.contrib.tpu.TPUEstimator(
        use_tpu=FLAGS.use_tpu,
        model_fn=model_fn,
        config=run_config,
        train_batch_size=FLAGS.train_batch_size,
        eval_batch_size=FLAGS.eval_batch_size,
        predict_batch_size=FLAGS.predict_batch_size)

...

        def metric_fn(label_ids, pred_ids, per_example_loss, input_mask):
                    # ['<pad>'] + [""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""B-MISC"", ""I-MISC"", ""X""]
                    indices = [2, 3, 4, 5, 6, 7, 8, 9]
                    precision = tf_metrics.precision(label_ids, pred_ids, num_labels, indices, input_mask)
                    recall = tf_metrics.recall(label_ids, pred_ids, num_labels, indices, input_mask)
                    f = tf_metrics.f1(label_ids, pred_ids, num_labels, indices, input_mask)
                    accuracy = tf.metrics.accuracy(label_ids, pred_ids, input_mask)
                    loss = tf.metrics.mean(per_example_loss)
                    return {
                        'eval_precision': precision,
                        'eval_recall': recall,
                        'eval_f': f,
                        'eval_accuracy': accuracy,
                        'eval_loss': loss,
                    }
                eval_metrics = (metric_fn, [label_ids, pred_ids, per_example_loss, input_mask])
                output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                    mode=mode,
                    loss=total_loss,
                    eval_metrics=eval_metrics,
                    scaffold_fn=scaffold_fn)

...
```

and the training script https://github.com/dsindex/BERT-BiLSTM-CRF-NER/blob/master/train.sh

```
python bert_lstm_ner.py   \
        --task_name=""NER""  \
        --do_train=True   \
        --use_feature_based=False \
        --do_predict=True \
        --use_crf=True \
        --data_dir=${CDIR}/NERdata  \
        --vocab_file=${bert_model_dir}/vocab.txt  \
        --do_lower_case=${lowercase} \
        --bert_config_file=${bert_model_dir}/bert_config.json \
        --init_checkpoint=${bert_model_dir}/bert_model.ckpt   \
        --max_seq_length=150   \
        --lstm_size=256 \
        --train_batch_size=32   \
        --eval_batch_size=128   \
        --predict_batch_size=128   \
        --bert_dropout_rate=0.1 \
        --bilstm_dropout_rate=0.1 \
        --learning_rate=2e-5   \
        --num_train_epochs=100   \
        --data_config_path=${CDIR}/data.conf \
        --output_dir=${CDIR}/output/result_dir/
```

the situation is, 

when i set the `train_batch_size` 32 -> 6 and `eval_batch_size` 128 to 6, `eval_f` scores are not same.
exactly speaking, the lower batch_size the higher `eval_f` score we have.
is it normal?"
support  auc?,support  auc?
what's the meaning of positive class in multi-class task?,"Hey man, I want to ask the meaning of parameter `pos_indices`.

Which is the `pos_indices` in a multi-class task like 'Image Classify'? You can not say that cat or dog is positive if they have no opposite class, like good and bad."
Add GPU supoprt,"Currently, tf_metrics install tensorflow CPU as the dependency.
I think it would be better that this module can be used in GPU enable environment.
So I edited `setup.py` to be check whether GPU is enable and install tensorflow according to it.

(thank for sharing awesome NER tool!! tf_ner and tf_metrics is very useful for me!!)"
