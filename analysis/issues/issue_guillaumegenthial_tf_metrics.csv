title,body
fit error  help,"model = Sequential()

model.add(Conv2D(32, kernel_size=(5, 9),
                 activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 4)))

model.add(Conv2D(16, kernel_size=(5, 7), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 3)))

model.add(Flatten())

#num_classes*4 =104 

model.add(Dense(num_classes*4, activation='sigmoid'))


model.compile(loss=keras.losses.binary_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

ValueError: Error when checking target: expected dense_1 to have shape (104,) but got array with shape (26,)

whyÔºü help me~~~~thks~"
È™åËØÅÁ†ÅÈáçÂè†,
add a font file and fix the writing style,
4-6‰∏™Â≠óÁ¨¶ËÉΩÁî®‰∏Ä‰∏™Ê®°ÂûãÂÆûÁé∞ÂêóÔºü,‰Ω†Â•ΩÔºå‰Ω†ÁöÑÊ®°ÂûãÊàëÊàêÂäüÂ§çÂà∂‰∫ÜÔºå4‰∏™Â≠óÁ¨¶Ôºå6‰∏™Â≠óÁ¨¶ÂçïÁã¨ÊµãËØïÈÉΩËØÜÂà´ÁéáÂæàÈ´òÔºåÊúâÊ≤°ÊúâÂäûÊ≥ï‰∏Ä‰∏™Ê®°ÂûãÊó¢ÊîØÊåÅ4‰∏™Â≠óÁ¨¶ÂèàÊîØÊåÅ6‰∏™Â≠óÁ¨¶„ÄÇ
write labels err,"    writer.writerows(labels)
TypeError: a bytes-like object is required, not 'str'"
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,ÊåâÁÖß‰Ω†‰∏äÈù¢ÁöÑ‰ª£Á†ÅËøêË°åÂèëÁé∞Êä•Ëøô‰∏™ÈîôËØØÔºåÊü•‰∫Ü‰∏ãËØ¥ÊòéË∂ÖÂá∫‰∫ÜËåÉÂõ¥Ôºå‰∏çÂ§ßÁêÜËß£‰∏∫‰ªÄ‰πàËøûÊé•Áõ∏Âä†ÁöÑÊó∂ÂÄôÂá∫Èîô‰∫ÜÔºåË∞¢Ë∞¢ÂëäÁü•
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,"Ë£ÖÂ•Ω‰æùËµñÔºåÁîüÊàêÂõæÁâáÂêéÔºåÁõ¥Êé•ËøêË°åÔºåÊä•Â¶Ç‰∏ãÈîôËØØ
picnum :  6000
6000 (20, 80)
6000 4
Traceback (most recent call last):
  File ""cnn_end2end_ocr.py"", line 76, in <module>
    c = np.concatenate((c0,c1,c2,c3),axis=1)
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1"
Only extract one word from gumbel softmax,"In the code https://github.com/yala/text_nn/blob/master/rationale_net/utils/learn.py#L71-L85

```python
def get_hard_mask(z, return_ind=False):
    '''
        -z: torch Tensor where each element probablity of element
        being selected
        -args: experiment level config
        returns: A torch variable that is binary mask of z >= .5
    '''
    max_z, ind = torch.max(z, dim=-1)
    if return_ind:
        del z
        return ind
    masked = torch.ge(z, max_z.unsqueeze(-1)).float()
    del z
    return masked

```

because we take the max, usually, only one position will have the max value. 
In this case, if we have 100 words in the sentence, we only select one word as the rationale?
I thought we should select independently and choose those words with >0.5 probability.

Maybe we should change
```python
masked = torch.ge(z, max_z.unsqueeze(-1)).float()
```
to 
```python
masked = torch.ge(z, 0.5).float()
```
instead?"
Multiple GPUs is broken,"Hi Yala! 

Great package. Just letting you know, though, that computation on multiple GPU's is broken for two reasons:

1. The `model.py` file does not import 
``import torch.nn as nn``
that's an easy fix.

2. You have some class-attribute dependencies that are single-thread bound.
https://github.com/pytorch/pytorch/issues/8637

I'm not sure exactly what they are, but here is my error message, which matches the one in the issue I linked to above:

```
Traceback (most recent call last):
  File ""scripts/main.py"", line 35, in <module>
    epoch_stats, model, gen = train.train_model(train_data, dev_data, model, gen, args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 59, in train_model
    args=args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 198, in run_epoch
    mask, z = gen(x_indx)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/_utils.py"", line 369, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker
    output = module(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/generator.py"", line 55, in forward
    activ = self.cnn(x)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 55, in forward
    activ = self._conv(x)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 41, in _conv
    next_activ.append( conv(padded_activ) )
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 200, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)```

Alex"
Gumbel softmax instead of Reinforce,"Hi,

Thank you for your code ! I have seen you wrote Gumbel instead of Reinforce but I don't understand exactly how Reinforce was implemented before as I don't see any average over Z samples. Here I see that Z is sampled only once https://github.com/yala/text_nn/blob/master/rationale_net/models/generator.py#L40 .

Did I miss something ?

Thank you very much for your help"
Is log_softmax function missing in the Line 39 of generator.py?,"I think log_softmax function should be added in Line 39 if we want to use the Gumbel Softmax.

`logits = F.log_softrmax(self.hidden(activ))`

If I'm wrong, please let me know. 
Thanks.

![image](https://user-images.githubusercontent.com/17995697/54492275-0d17c500-4900-11e9-8bee-addd81d35934.png)
"
Can this be applied to a regression problem?,Can this be applied to a regression problem?
Is it possible to share the pre-trained embedding for beer reviews? ,"From https://github.com/yala/text_nn/blob/master/rationale_net/utils/embedding.py#L38 , it's trying to find `review+wiki.filtered.200.txt.gz`, is it possible to share the pre-trained embedding for beer reviews? "
Tutorial,Added tutorial
gpu-trained on cpu fix,
Refactor,Add rationale export in results file and clean up code
FileNotFoundError: [Errno 2] No such file or directory: 'pickle_files/embeddings.p',"In dataset.py we read
    embedding_path = 'pickle_files/embeddings.p'
    word_to_indx_path = 'pickle_files/vocabIndxDict.p'
    embedding_tensor = pickle.load(open(embedding_path,'rb'))
    word_to_indx = pickle.load(open(word_to_indx_path,'rb'))
However, the two files are not in the zip-code?"
AttributeError: 'Namespace' object has no attribute 'use_as_tagger',"Running from command line:
`CUDA_VISIBLE_DEVICES=2 python -u scripts/main.py  --batch_size 64 --cuda --dataset full_beer --embedding 
glove --dropout 0.05 --weight_decay 5e-06 --num_layers 1 --model_form cnn --hidden_dim 100 --epochs 50 --init_lr 0.0001 --num_workers
 0 --objective cross_entropy --patience 5 --save_dir snapshot --train --test --results_path logs/adhoc_0.results  --gumbel_decay 1e-5 --get_rationales
 --aspect aroma --selection_lambda .005 --continuity_lambda .01`

Gives an error:

`
Traceback (most recent call last):
  File ""scripts/main.py"", line 30, in <module>
    gen, model = model_utils.get_model(args, embeddings, train_data)
  File ""/home/mglowacki/Desktop/RNN_yala_pytorch_2/rationale_net/utils/model.py"", line 12, in get_model
    if args.use_as_tagger == True:
AttributeError: 'Namespace' object has no attribute 'use_as_tagger'
`
I've checked sourcecode but there is no --use_as_tagger parameter in args.
Btw. in the same file `model.py` there is an additional problem because `nn` is not imported.

"
fixing 0-dim tensor (scalar) access,"New versions of PyTorch will require proper usage for accessing 0 dimensional Tensors:
`tensor.item()` instead of `tensor.data[0]`"
Tagging,
added confusion matrix info,
How can I be able to learn with your repository,"How can I be able to learn with your repository

Note: I am a complete Noobie in the world of C# Programming but with Knowledge in JavaScript"
HPO wiki page,"Small edit - in the code example on the wiki page for hyperparameter optimization, `transform.Logarithmic` should now be `transform.Log10`"
how can i train the Neural Network with my own Training Pictures?,"let's say i have a list of Images ..how do i convert my images into F64matrix Form so i can train them with my Neural Network ..and how do i test the Network with an Image at the End
could you please make an example of this because you didn't mention that in the examples, you also didn't mention how to test the Network using a real Image..

Thanks in Advance "
Access OOB data and OOB error calculations of Random Forest,"Hi
Can you add access to the Out-of-Bag data and/or Out-of-Data error calculations for Random Forests?

Love this project,
Thanks

"
A way to Save Bayesian Optimizer progress and continue later.,"Hello, 
I would like to use the Bayesian Optimizer for Hyperparameter tuning.
Is there a way to save the current status of the optimizer and then resume later. I could not find one... 
Also I could not figure out a way to pass a cancellation token.
Great project.
Thank you."
Code sharing,"Hi @mdabros,

For about 15 years, I was the main maintainer of a project called the Accord.NET Framework, which was mainly a machine learning/statistics processing framework for .NET. I have recently archived the project as I couldn't keep up updating it. If there is anything that you would ever find useful in Accord/its codebase, I just wanted to let you know that I have granted license to anyone who would like to, to reuse any piece of code I have written myself (as per noted in the headers of each file of the project) under the MIT or BSD licenses.

I still continue to receive requests on how to use/adapt existing features, even after the project has been archived, so I guess there are still useful things that have been implemented in that framework. If you would like to adapt any of those into SharpLearning, please let me know.

All the best and merry Christmas if you celebrate it!

Cesar

"
Exception when serializing neural net to XML,"Hello,

I am using SharpLearning.Neural version 0.31.8 in a .Net Core 3.1 project. After training a neural network and obtaining the predictor model, I try to serialize it to an XML file using the suggested procedure:

```csharp
GenericXmlDataContractSerializer xmlSerializer = new GenericXmlDataContractSerializer();
xmlSerializer.Serialize<IPredictorModel<double>>(_model, () => new StreamWriter(filePath));
```

It fails with the following SerializationException:

System.Runtime.Serialization.SerializationException: An object of type 'SharpLearning.InputOutput.Serialization.GenericXmlDataContractSerializer+GenericResolver' which derives from DataContractResolver returned false from its TryResolveType method when attempting to resolve the name for an object of type 'MathNet.Numerics.LinearAlgebra.Storage.DenseVectorStorage`1[[System.Single, System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]]', indicating that the resolution failed. Change the TryResolveType implementation to return true.

Any help?
"
SharpLearning.XGBoost.dll is not compatible with .net core,"Hi,
SharpLearning.XGBoost.dll is not compatible with .net core due to PicNet.XGBoost (0.2.1) dependency.
Checked on 0.31.8 version. And  Core 2.1 version
Do you plan to fix it?"
Continuously improving a neural network over time using small batches.,"Hey, first of thanks for a fantastic library!

The library is really easy and simple to use if you have a large dataset and want to train a network in one go.

But I'm building a DQN and I want to continuously improve a neural network from small batches of training data, with as little overhead as possible. Is that something that's easily possible in SharpLearning?

Right now, the only way I see it can be achive, is by doing something like this:

    var net = new NeuralNet();
    // ...

    while (true)
    {
        var learner = new NeuralNetLearner(net, new CopyTargetEncoder(), new SquareLoss());
        // ...
        net = learner.Learn(observations, targets);
    }

However there's a lot of overhead and data copying going on there. Are there better ways to go about it?

Thanks  :)

**Edit 1** : Seems like my example doesn't work either since the weights are randomized when a learning begins.

**Edit 2**: My seccond attempt, throws a nullreference exception on net.Forward(input, output). (Allthough I imagine that this is not a very good way to go about it either? And probably wrong on many levels üòä)

    var delta = Matrix<float>.Build.Dense(1, 1);
    var input = Matrix<float>.Build.Dense(inputCount, 1);
    var output = Matrix<float>.Build.Dense(1, 1);

    while (true)
    {
            PopulateInput(input)
            net.Forward(input, output);
            var expected = GetExpected();
            delta[0, 0] = (float)(expected - output[0, 0]);
            net.Backward(delta);
    }"
Serialization Exception,"Hi. 

Not sure what happened, but something happened :) 

1. I updated all projects from .NET Core 3.0 to 3.1 
2. Made some changes in MySQL DB 

Now, I get various errors from `GenericXmlDataContractSerializer`. Surprisingly, exception happens only when I build project the second time, after the first build it works fine. I mentioned DB, because I serialize trained model using `MemoryStream` and save it as a byte[] to the MySQL column of type LongBlob. I'm also using models from `ML.NET` and export / import them from DB the same way and they work fine, so probably DB is not an issue. All projects in the solution are built as x64. Serializer fails on any model, either `RandomForest` or `AdaBoost`, with the same exception. 

**The issue** 

1. build the project and start debugging 
2. create, train model, and save it to DB as byte array using `GetPredictor` method below 
3. select byte array from DB, deserialize to a model, provide test data and get estimate - **OK** 
4. stop debugging, repeat steps 1-3, now prediction method fails with the exception below - **NOT OK**

**The question**

Maybe somebody knows what could be the reason for serializer to fall with the exception? Also, can I serialize trained model to `MemoryStream` using different serializer, without `GenericXmlDataContractSerializer`? 

**Most common exception**
```
System.Runtime.Serialization.SerializationException: Element 'http://schemas.datacontract.org/2004/07/Core.Learners.SharpLearning.EngineSpace:Model' contains data from a type that maps to the name 'SharpLearning.RandomForest.Models:ClassificationForestModel'. The deserializer has no knowledge of any type that maps to this name. Consider changing the implementation of the ResolveName method on your DataContractResolver to return a non-null value for name 'ClassificationForestModel' and namespace 'SharpLearning.RandomForest.Models' 
```

**After updating all Nuget packages I got another exception only once** 
```
Invalid XML at line 1 or something like that
```

**Serializing trained model to byte array and save to DB** 

```C#
public virtual ResponseModel<byte> GetPredictor(IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<byte>();

  using (var memoryStream = new MemoryStream())
  {
    var processor = GetInput(columns, inputs, nameof(PredictorLabelsEnum.Emotion));
    var learner = new ClassificationRandomForestLearner();
    var serializer = new GenericXmlDataContractSerializer();
    var container = new MapModel<int, string>
    {
      Map = processor.Map,
      Model = learner.Learn(processor.Input.Observations, processor.Input.Targets)
    };
    
    serializer.Serialize(container, () => new StreamWriter(memoryStream));
    responseModel.Items = memoryStream.ToArray().ToList();
  }

  return responseModel;
}
```

**Deserializing model from DB stream and getting prediction** 

```C#
public virtual ResponseModel<string> GetEstimate(IEnumerable<byte> predictor, IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<string>();

  using (var memoryStream = new MemoryStream(predictor.ToArray()))
  {
    var processor = GetInput(columns, inputs);
    var serializer = new GenericXmlDataContractSerializer();
    var model = serializer.Deserialize<MapModel<int, string>>(() => new StreamReader(memoryStream));
    var predictions = model.Predict(processor.Input.Observations);

    responseModel.Items.Add(predictions.OrderByDescending(o => o.Key).First().Value);
  }

  return responseModel;
}
```

Method `GetInput` in the code above is just a conversion from `IDataView` format in ML.NET to `ObservationSet` format in `SharpLearning`. `MapModel` is a [wrapper](https://github.com/mdabros/SharpLearning/issues/132) that allows to save text labels along with numeric ones. "
[PR] The proj files have been updated to enable SourceLink,"CSProj files have been updated to enable SourceLink in your nuget
---

*[This pull request was created with an automated workflow]*

I noticed that your repository and Nuget package are important for our .NET community, but you still haven't enabled SourceLink.

**We have to take 2 steps:**
1) Please approve this pull request and make .NET a better place for .NET developers and their debugging.
2) **Then just upload the .snupkg file** to https://www.nuget.org/ (now you can find the snupkg file along with the .nuget file)

You can find more information about SourceLine at the following links  
https://github.com/dotnet/sourcelink
https://www.hanselman.com/blog/ExploringNETCoresSourceLinkSteppingIntoTheSourceCodeOfNuGetPackagesYouDontOwn.aspx

If you are interesting about this automated workflow and how it works  
https://github.com/JTOne123/GitHubMassUpdater

*If you notice any flaws, please comment and I will try to make fixes manually*
"
Is there a way to keep textual labels / targets as a part of the trained model?,"First of all, thank you for sharing this library. 
Second, would be great to make mapping between columns and feature names mode obvious. 
If model was serialized and saved on one computer and deserialized and loaded on the other one, then second computer will have no idea what's the meaning of labels / targets, because model keeps them as double values. 

**Save model**

```C#

var labels = new[] { ""Good"", ""Bad"", ""Average"" ...  };
var labelKeys = labels.Select((v, i) => (double) i);  // take label key instead of name 
var learner = new ClassificationDecisionTreeLearner();
var model = learner.Learn(items, labelKeys); // is there any reason not to use string labels instead of doubles?

using (var memoryStream = new MemoryStream())
{
  var serializer = new GenericXmlDataContractSerializer();
  serializer.Serialize<IPredictorModel<double>>(model, () => new StreamWriter(memoryStream));
  db.Save(memoryStream.ToArray());  // convert XML to byte[] and save as Blob to DB
}
```

**Load model**

```C#
var xmlModel = db.Get(...).AsBlob().GetBytes(); // load saved model from blob column in DB

using (var memoryStream = new MemoryStream(xmlModel))
{
  var serializer = new GenericXmlDataContractSerializer();
  var xml = serializer.Deserialize<IPredictorModel<double>>(() => new StreamReader(memoryStream));
}
```

As a result, loaded model has property `Targets` that contains some double values, like 1, 2, 3, 4 and there is no way to understand that initially they meant ""Good"", ""Bad"", etc 

**Question**

Is there a way to save original **string labels / targets** as a part of the model to make prediction results human-readable? "
0.31.8.0: Ensure deterministic order of results from multithreaded optimizers,"Fix #130 and make order of results deterministic for all optimizers when running with parallel execution. Results will now also be the same between single threaded and multithreaded execution.

This affects all optimizers supporting parallel execution:
 - `BayesianOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`
 - `GridSearchOptimizer`
 - `ParticleSwarmOptimizer`
 - `RandomSearchOptimizer`
"
Order of results from RandomSearch is not deterministic with different iteration counts.,"Running the `RandomSearchOptimizer` with `runPrallel=false`, so not multithreading, with 100 iterations and 120 iterations seem to provide different order of results. Expectation would be that the fist 100 iteration would be the same, and this is not currently the case.
This is most likely caused by the use of `ConcurrentBag` to collect the results, which does not guarantee order.

This might also affect other Optimizers supporting parallel execution,"
Issue with loading model using GenericXmlDataContractSerializer: The deserializer has no knowledge of any type that maps to this name,"Thank you so much for developing this package. It's been working smoothly on my computer, but I might need some help on generating a dll file for others to run on their computers. I tried using this nuget package [https://github.com/Fody/Costura/graphs/contributors](url) to compile sharplearning dlls into the project dll and adding all sharplearning xml files to embedded resources. But I keep getting the same error saying that ""Element 'http://schemas.microsoft.com/2003/10/Serialization/:anyType' contains data from a type that maps to the name 'SharpLearning.GradientBoost.Models:RegressionGradientBoostModel'. The deserializer has no knowledge of any type that maps to this name. "" I was wondering where the deserializer knowledge is stored and how do I add them to the project dll file. Any comment or suggestion is appreciated. Thank you!"
0.31.7.0: Refactor BaysianOptimizer and add parallel computation,"This pull request refactors the `BayesianOptimizer` implementation to be use the same principles as the `SMACOptimizer`. The two optimizers are both model based optimizers, and should therefore be very similar in implementation. The `BayesianOptimizer` can be viewed a basic implementation of model based optimization, which the `SMACOptimizer` builds a few tricks on top of.
A base class for model based optimizers seems to be the next logical step, but that will follow in a later pull request.

The refactoring enables use of the `BayesianOptimizer` in an ""open loop"" style just like the `SMACOptimizer`. See the unit tests for an example.

This pull request also adds the option of parallel computation to the `BayesianOptimizer`. This work was originally added in #119.

Note that when running in parallel, and using the `Optimize(Func<double[], OptimizerResult> functionToMinimize)` method, the order of the results will not be reproducible. The individuel results will remain the same, but the order of the results will vary between runs.

I recommend only using the parallel version if the provided `functionToMinimize` is running serial computation, and is slow to compute."
"Minor typo fixes in layers (release postponed, so no version incrementation)",Fix typo in comment of DropoutLayer and SoftMaxLayer. Misspelled gradients.
0.31.5.0: Add sigmoid activation function,"Added sigmoid activation function, sigmoid short derivative, sigmoid test.
"
"TrimSplitLineTrimColumnsToDictionary throws a ""key already exists"" exception","Hi guys!
First off, great job! SharpLearning is very useful and well built.

One small bug I found while mistakenly creating a dataset based on a CSV file without the headers line (when calling 'ToF64Matrix()').

In SharpLearning.InputOutput.Csv.CsvParser -> Dictionary<string, int> TrimSplitLineTrimColumnsToDictionary(string line)
there's an iteration over the headers line, but it assumes all headers are distinct (and also that it is the headers line) - therefore an exception of ""key already exists in dictionary"" is thrown.
I think it should check if there's a duplication and throw a more explanatory error message in such case.

Let me know if you want me to fix it and add a pull request."
0.31.6.0: Adds implicit and explicit conversions from double[][] to F64Matrix,"This is a simple way to address the need to expose an API surface that accepts double[][] rather than an F64Matrix. Instead of changing all interfaces and implementations, I have made double[][] implicitly convertible to F64Matrix. This can be considered a convenience and a temporary workaround for [#20](https://github.com/mdabros/SharpLearning/issues/20) and [#115 ](https://github.com/mdabros/SharpLearning/issues/115)."
Add parallelism to Bayesian Optimizer. Also allow resampling non-deterministic algorithms,"[https://github.com/mdabros/SharpLearning/pull/119](https://github.com/mdabros/SharpLearning/pull/119)
"
retrain a Model,"How to Retain a model with new data ?
"
For Multiple files ,"is there a way of loading multiple files of same schema ,or do i have to combine  all files in to one big giant file ?"
Excellent Work,Thanks for x boost GPU learners .
"Adds parellelism to Bayesian optimizer by default and adds support for non-deterministic algorithms (release postponed, so no version incrementation)","I've had a second look at the Bayesian Optimizer and have introduced parallelism by default as with other optimizers. I've also introduced support for non-deterministic algorithms that may return different results for identical parameters. This also entailed a change to the standard serial behaviour so that instead of skipping an evaluation if the parameters did not change from the previous run, it will now store the results for all evaluations and skip any that have been run before. This should result in a performance improvement for the serial behaviour but will consume some memory."
"Hard dependency from Microsoft.IdentityModel.Clients.ActiveDirectory, version 3.17.2.31801",In my test project by saving of gradient boosting model I‚Äôve encounter strange implicit dependency from Microsoft.IdentityModel.Clients.ActiveDirectory. It doesn‚Äôt work (load assembly exception) in all referenced versions of this assembly except I reference pretty old one version 3.17.2. Even if I use dependentAssembly construction it doesn‚Äôt help. My project is net461.
"0.31.4.0: Add CrossValidationUtilities.GetKFoldCrossValidationIndexSets, Refactor CrossValidation.","Extract the internal `GetKFoldCrossValidationIndexSets` method form the `CrossValidation<T>` class. 
This enables calculation of KFold CrossValidation IndexSets for use outside the `CrossValidation<T>` it self.

Usage: 

```csharp
// Targets to create KFold Index Sets from.
var targets = new double[] { 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3 };
// Sampler to control the sampling of the sets. In this case Stratified.
var sampler = new StratifiedIndexSampler<double>(seed: 242);

var indexSets = CrossValidationUtilities.GetKFoldCrossValidationIndexSets(sampler,
    foldCount: 4, targets: targets);

foreach (var (trainingIndices, validationIndices) in indexSets)
{
    // Do model training and accumulate predictions,
    // to form a fully k-fold cross validated prediction array.
}
```

Note, that in the case of remainders from `samplesPerFold = targets.Length / foldCount`, the last validationIndices will contain the remaining values (making it larger compared to the others), and the last trainingIndices will exclude these (making it smaller than the others)."
SharpLearning.Core: Merge SharpLearning.Containers and SharpLearning.Common.Interfaces,"Once #20, and #112 has been completed, the code in SharpLearning.Containers should be reduced quite a bit, and only contain a few basic types and support extension methods. For this reason it would make sense to merge this with the common interfaces assembly into a SharpLearning.Core project that holds the basic essentials for SharpLearning."
Add multidimensional array extensions to replace the current properties and methods on the Matrix class,Switching from the matrix class to multidimensional arrays requires adding extension methods to provide to expose the same members that the current matrix class does.
Consolidate common project settings into Directory.Build.props,"Where possible, add common project settings to Directory.Build.props"
Update projects to target c# 7.3 to enable new language features,
Remove/clean unused types and classes,"Currently there are a few which contains functionality that is rarely used, or that doesn't quite fit into the current direction of the package. This includes:
 - `SharpLearning.Containers.Arithmetic`: MatrixF64 is mostly used as a container, and more efficient matrix arithmetic can found in other libraries, like mathnet.numerics.
 - `SharpLearning.Containers.ObservationTargetSet`: This can be replaced by using a value tuple instead.
 - `SharpLearning.Containers.ArrayExtensions`: Several methods are unused.
 - `SharpLearning.CrossValidation.ContinuousMungeAugmentator`: 
 - `SharpLearning.CrossValidation.NominalMungeAugmentator`:
"
"Update code style, line length, member order, etc.","Part of 2019 sommer cleaning to get the code base cleaned and more up to date. This pull request contains mainly code style changes. This includes:
 - Line lengths around 100 chars (unless specific circumstances, like constructor checks, and expected test results)
 - Remove and order usings.
 - Class member ordering to follow standard guidelines.
 - Use expression bodied methods where applicable.

The pull request also includes code changes to avoid duplicated code in a few places.

A few more exotic metrics has also been deleted:
 - DiscreteTargetMeanErrorRegressionMetric 
 - RocAucRegressionMetric

If anybody uses these, let me know, and I can readd them."
Make SharpLearning.Neural support .net core 2.0/.net standard 2.0 (update mathnet numerics to latest version (.NETStandard 2.0 compatible) ),"This pull request updates mathnet numerics to latest version (.NETStandard 2.0 compatible). This enables SharpLearning.Neural to have support for .net core 2.0/.net standard 2.0, and solves #26. "
Add azure pipelines yaml configuration,"This pull request adds `azure-pipelines.yaml` configuration to control CI and PR validation. This also adds both debug and release validation. Previously, only the release version was build and tested via CI. 

This addition is the bare minimum of the yaml configuration. BuildPlatform has not been added to the configuration yet. I could not make it work together with the dotnet build command. Information on this can be found here: [dotnet issue 10421](https://github.com/dotnet/cli/issues/10421). So currently, the build platform from the project files is used.

I am currently using 'tasks' for the individual steps, but it seems that 'scripts' are generally more popular for dotnet core pipelines. Here is a guide using scripts: [yaml-build-pipeline-net-core-azure-devops-tutorial)](https://www.nankov.com/posts/yaml-build-pipeline-net-core-azure-devops-tutorial)

Another example is the [TorchSharp pipeline](https://github.com/xamarin/TorchSharp/blob/master/azure-pipelines.yml), but that also handles additional steps for getting external resources etc.. 'Scripts' definitely seems more flexible than 'tasks', so I might switch to 'scripts' in the future when I have some more experience with them."
"Can this project be called under xamarin forms and run on android? I tried, loading the model failed.",
Wow! Amazing work! Thanks a lot!,"Hi!

I also wanted to thank you for this wonderful library! The API is super clean and love it so far.

One question I have is regarding the accuracy score like the accuracy_score() function in scikit learn that can be seen here: https://www.kaggle.com/mathvv/prediction-of-red-wine-quality-93-215
Like this:
`print('Random Forest:', accuracy_score(y_test, rf_pred)*100,'%')`
Which outputs this:
`Random Forest: 91.875 %`

Many thanks and congrats again!

Flo"
Understand class prediction results,"Hi,
maybe some silly questions...
I'm trying to train a random forest model with **two different classes**. I think I understood that the number of rows of the target vector must be equal to observations matrix (therefore regardless of the number of classes). So in the rows of the output vector I set the value 0 for the first class and 1 for the second class. **This is right?** 
I would also like to understand how to interpret the results, for example if for a set of features I have the prediction value 0.6 I must consider it a class of type ""0"" or a class of type ""1""? Do I have to cast to integer or I must to round it? Finally the ""variance"" values ‚Äã‚Äãcontained in CertaintyPrediction indicates the probability of the prediction (greater is better)?
many thanks
"
Add support for simple linear/logistic regression,"You've done a great job with the more sophisticated algorithms: would it be possible, for completeness, to throw in linear/logistic regression? I imagine it would be fairly quick comparatively."
"0.31.1.0: Add SmacOptimizer, update argument names on HyperbandOptimizer and BayesianOptimizer, clean up SharpLearning.Optimization.Test project","This pull request adds the `SmacOptimizer` to `SharpLearning.Optimization`. The implementation is based on the paper: [Sequential Model-Based Optimization for General Algorithm Configuration](https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf).

The algorithm combines bayesian optimization with greedy local search based on the current top solutions.

The `SmacOptimizer` implements the regular `IOptimizer` interface, but also surfaces the two primary methods for running the algorithm `ProposeParameterSets` and `RunParameterSets`. This makes it possible to use the optimizer in an ""open-loop"" style, and allows the optimizer to be easily used from or combined with other optimizers. An examples could be using the scheduling technique from the `HyberbandOptimizer` together with the model based sampling from the `SmacOptimizer`.

An example showing ""open-loop"" use can be found in the `SmacOptimizerTest` class."
Remove resources and add culture initialiser for test projects,"This should fix issue #34, and fix the current issue with azure dev ops pipelines.

This adds an `AssemblyInitializeCultureTest` class for all unit test project, which sets the culture settings to `CultureInfo.InvariantCulture`. This should make the unit tests pass on machines with different culture settings.
```CSharp
    [TestClass]
    public class AssemblyInitializeCultureTest
    {
        [AssemblyInitialize]
        public static void AssemblyInitializeCultureTest_InvariantCulture(TestContext c)
        {
            CultureInfo culture = CultureInfo.InvariantCulture;
            CultureInfo.DefaultThreadCurrentCulture = culture;
            CultureInfo.DefaultThreadCurrentUICulture = culture;
            Thread.CurrentThread.CurrentCulture = culture;
            Thread.CurrentThread.CurrentUICulture = culture;
        }
    }
```

This pull request also removes the use of resources in all test projects, and instead uses a `DataSetUtilities` class to handle small test datasets. This also cleans up a lot of CsvParser code for loading the data."
Make individual Tree models public on ForestModels. This is a breaking change. Also clean up SharpLearning.RandomForest.Test,"This pull request makes the individual tree models public on the forest models: `RegressionForestModel` and `ClassificationForestModel`. This makes it possible to use the individual trees from the model for custom predictions, or for calculating statistics. For instance, using a different ensemble strategy than average for regression and majority vote for classification. Getting the predictions for the individual trees also makes it possible to calculate statistics on the predictions to see how much the ensemble of models agrees or disagress.

The tree models are accessed through the `.Trees` property of the forest models:

```CSharp
var learner = new RegressionRandomForestLearner();
var forest = learner.Learn(observations, targets);
var trees = forest.Trees;
```

The trees can then be used individually afterwards:

```CSharp
var prediction = trees.Select(t => t.Predict(observation)).Average();
```

Note that this is a **breaking change**, since the trees have been promoted from a private member to a public property. This means that it will not be possible to load ForestModels trained with earlier versions of SharpLearning into this version. So retraining of models from the `SharpLearning.RandomForest` project is mandatory if updating to 0.31.0.0 and newer versions. This is sadly the downside of serializing the model code directly instead of using a custom format, which is the current strategy in SharpLearning.

This should solve #101 and #94 "
 Individual trees prediction of the classification RF model," #94
Add an additional methods which provides individual trees prediction of the classification RF model.

I ‚Äòm not sure the code is right .Please help me to check it.I need the methods a little hurry.Thanks very much!
JinHJ"
0.30.2: Add HyperbandOptimizer.OptimizeBest method,Add missing `OptimizeBest` method for the `HyperbandOptimizer`. This method returns the best result found by the optimizer
"0.30.1.0: Add HyperbandOptimizer, and update test adapters","This pull request adds the `HyperbandOptimizer` to `SharpLearning.Optimization`. The implementation is based on the original article [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) and the implementation by [fastml](http://fastml.com/tuning-hyperparams-fast-with-hyperband/).

Compared to the other optimizers from SharpLearning, Hyperband includes an extra parameter in the objective function, `unitsOfCompute`. Hyperband uses the `unitsOfCompute` parameter to control a budget of compute for each set of hyperparameters. Initially it will run each parameter set with very little compute budget to get a taste of how they perform. Then it takes the best performers and runs them on a larger budget.

The `unitOfCompute` parameter is used in the objective function, and could for instance be used to control the size of the training set, the number of trees in gradient boost, or the number of epochs for neural nets. One unit of compute could for instance be defined as 1000 samples in the training set. The `maximumUnitsOfCompute` is provided as an argument to the `HyperbandOptimizer`, and the optimzer will define a schedule for evaluating the hyperparameters on a budget.

A small experiment comparing the results and runtime of the `HyperbandOptimizer` vs. the `RandomSearchOptimizer`, optimizing a neural net (using CNTK) on the CIFAR-10 dataset:

**System**
**CPU: i7-4770**
**GPU: GTX1070**

The Hyperband optimizers uses the default parameters except for the `skipLastIterationOfEachRound` which is enabled for one of the runs. The default parameters result in a total of 209 different parameter sets tried with the Hyberband optimizers. The `unitsOfCompute` parameter in the objective function is used to control the size of the training set, where 1 unit of compute is set to 740 samples, which corresponds to the full training set size of 60.000 samples, when `maximumUnitsOfCompute` is set to 81, which is the default maximum. The full test set is used to track the test loss/accuracy in all rounds/iterations.

| Optimizer        | Time (hours)           | Test accuracy (%)  |
| ------------- |:-------------:| -----:|
| `RandomSearchOptimizer(iterations=100)`|  45.34 |  88.47 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=false)` |  10.46  |   87.93 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=true)`|  6.56  | 87.93 |

As can be seen the RandomSearchOptimizer finds a slightly better parameter set. However, the two Hyperband runs uses significant less time to find a solution that is nearly as good. In this case, skipping the last iteration of each round results in finding the same parameter set as when including all iterations. This will not be the case for all problem types, but it does provide a nice speed up for large problems.

In the end, Hyperband finds a solution that is almost as good, and reduces the time required by a factor of 4-7.

A future extension to the hyperband optimizer could be to use bayesian optimization instead of random search to select the parameter sets. This combination has proven very useful in: [Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/pdf/1807.01774.pdf)."
"Add support for Reinforcement Learning algorithms: QLearning, Sarsa etc.","There is currently support for most of the common (and some less common) ML algorithms in Sharp Learning. However, there does appear to be a lack in the area of Reinforcement Leaning and some might observe these algorithms are beginning to gain some traction.

If there is any appetite for extending into this area, I would propose as an initial baseline provision for QLearning and Sarsa, backed by Epsilon Greedy and Boltzmann approaches. A second stage could then continue with Thompson and UCB1 exploration, and finally the existing Neural Net and ensemble interfaces could probably produce a compound that resembled Deep Q Networks."
0.30.0: Optimizers Optimize method returns unfiltered results in chronological order,"The `Optimize ` method on the optimizers from `SharpLearning.Optimization` will now return all results, unfiltered and in chronological order. Before, the results would be filtered for `NaN` values and ordered from smallest to largest error. This change makes it easier to compare the iterations required to get a good solution between the optimizers. The `OptimizeBest` method, which returns the single best result from the optimizers, is unchanged and will provide exactly the same result as previously.

This is a potential breaking change, so if relaying on the order of the results from the `Optimize` method, these should now be sorted and/or filtered after the call to the optimizer. Like it is also done in the `OptimizeBest` method:

```CSharp
Optimize(functionToMinimize).Where(v => !double.IsNaN(v.Error)).OrderBy(r => r.Error).First();
```

Note, that the particle swarm optimizer only returns the latest result from each particle."
"0.29.1: Rename logarithmic transform to Log10 transform, and release contribution of parallel ParticleSwarm and parallel GlobalizedBoundedNelderMeadOptimizer.","This pull request renames the `Logarithmic` transform to `Log10` transform. This should fix issue #93.

This pull request also releases the contributions made by @jameschch in pull request #95, which adds parallel execution to the following optimizers:

 - `ParticleSwarmOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`

and adds selection of the degree of parallelism to:

 - `GridSearchOptimizer`
 - `RandomSearchOptimizer`"
"Adds parellelism to Particle Swarm optimizer, Adds configurable maxim‚Ä¶",This introduces parallel evaluation to the particle swarm and Nelder Mead optimizers. This also introduces a configurable maximum degree of parallelism to Random and Grid optimizers. The automatic scheduler is not effective for long-running problems that themselves are multi-threaded. This setting allows the user to strictly control the number of parallel operations.
Random Forest: how can I get each tree's prediction ?,"hi,Mads,
I hope to know how can I get each tree's prediction or which category it choose when I finished trained a RF model for my classify taskÔºüIt is useful for my task.
thanksÔºÅ
"
Optimization: Rename Transforms.Logarithmic to Transforms.Log10,"Currently, the name of the logarithmic transform is misleading, since in the .Net world log refers to log2, and the [LogarithmicTransform](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.Optimization/Transforms/LogarithmicTransform.cs) from SharpLearning uses Log10. 

So the transform, and the enum should be renamed to illustrate the use of Log10. For instance:
 - `Transforms.Log10`
 - `Log10Transform`"
Add IParameterSpec to SharpLearning.Optimization,"Primary change is the addition of an `IParameterSpec` interface, that replaces the previous `ParameterBounds` type. Two concrete implementations have been added:
 - *GridParameterSpec*:  Usable when a fixed set of parameters, needs to be searched.
 - *MinMaxParameterSpec*: Direct replacement of `ParameterBounds`. used for sampling values in the range [min;max].

The addition of the `IParameterSpec` and the  `GridParameterSpec`, makes the `GridSearchOptimizer` use a similar set of bounds as all the other optimizers. This makes it easier to switch to the `GridSearchOptimizer` in scenarios where autofac or other dependency injection frameworks are used.

The addition of the `GridParameterSpec`, also makes it possible to limit the sampling of the `RandomSearchOptimizer` to a fixed set of values. For instance:

```csharp
var parameterSpecs = new IParameterSpec[] 
{
    new GridParameterSpec(1, 10, 15, 20, 25),
    new MinMaxParameterSpec(0.0, 100.0, Transform.Linear)
};
var optimizer = new RandomSearchOptimizer(parameterSpecs, iterations: 100);
var actual = optimizer.OptimizeBest(Minimize);
```

In the above, the `RandomSearchOptimizer` will only sample randomly between the fixed values `1, 10, 15, 20, 25`, for the first parameter."
Add serializable feature transforms,"This pull request enables serialization of the features transforms available in the *SharpLearning.FeatureTransformations* project.

This is related to issue #90"
Serialisation of MinMaxTransformer,"Firstly, thanks for this excellent library - it is pretty well exactly what i have been looking for.

I have not found a way to serialise a MinMaxTransformer after use during training of a regression model 

Is there some other approach i should be using to normalise new data points for prediction in subsequent processing?"
"Error when training with GPU -  ""Unknown linear updater grow_gpu_hist""","I have downloaded and installed CUDA, my GPU is benchmark ""ASUS ROG strix OC 1080TI"",
when the learning starts some console prints are shown (attached screenshots) some error messages and sometimes the program even crashes,
should i build the program in certain way/download CUB or something else i missed?
Please help!

thx,
![capture](https://user-images.githubusercontent.com/13719129/47250162-af9d4c00-d425-11e8-8f29-ed3c8f3526ba.PNG)

"
Add Activation Functions other than ReLU please!,
SEHException when using xgboost with gpu,"I have installed latest Cuda version (9.2), and when setting the tree method to GPU* i get an SEHException ""external component has thrown an exception"", should i compile the program in a certain way after cuda installation? What should i do?, thanks!"
"Add overload for CsvRowExtensions.ToF64Matrix and ToF64Vector, to support other converters ",
Multiple output regression,"Most regression models are hardcoded with learning method: Learn(F64Matrix observations, double[] targets), what if we had multiple predicted variables.
I want to add method Learn(F64Matrix observations, F64Matrix targets)"
Full access to trained model structures,"Currently the trained models only expose members from IPredictorModel. For NNs, its impossible to get to the layered structures of the best model. Extend models to expose this"
GBM prediction confidence,"Hi, is it possible to add an option for getting the confidence of a prediction of a GBM?
To know how much the prediction ""can be trusted""
How can i do it by my self?
Thanks!"
Sample weight for XGBoost,"In Python XGBoost one can provide weights for each row of the data, see http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit. I tried to look for a way to specify such weights in SharpLearning, but could not find it. Is this possible?"
 Add check to GBMDecisionTreeLearner so we wont use more features than we can,
CrossValidation CrossValidate ProbabilityPredictions error means?,"Hi,
when I run ""CrossValidation_CrossValidate_ProbabilityPredictions"" example with my data, it gives these values:
**Cross-validation error: 6.88921441267999**
**Training error: 6.86281040890985**
I searched a lot but couldn't find any documantation abput it? Could you explain what's this values mean, and for best prediction what they should be?
Thanks.
Regards!"
F# unit tests,Let‚Äôs ‚Äûcontaminate‚Äú SharpLearning with some F# code. For data processing F# is quite good alternative to c#  
Text Classification,"I am pretty new to Machine Learning and all of these things
I am want to ask a question about text classification: 
Can I use this library for text classification and how? 



P.S I need to split one string and classify substrings by Categories\Groups\etc, for example ""Ray's Potato Chips with Ketchup taste 80g"" I need to split into 
Category ""Potato Chips"" 
Groups:""Ketchup"" 
Brand(or smthng):""Ray's"""
How to vectorize text?,"Hi, thanks for the great library!

My CSV has text in some of the columns. Some of them are categorical (e.g. month of the year) and some have free text (e.g., book title). Looks like `SharpLearning.InputOutput.Csv.CsvRowExtensions.ToF64Matrix` is trying to parse stringified numbers. What if my CSV consists of non-number values? Is there a recommended way or should I wire another lib to do TF-IDF/word2vec/char embedding/etc?"
How to load data from SQL server table,"All given samples contain CSV method only.

```
            #region Read data

            // Use StreamReader(filepath) when running from filesystem
            var parser = new CsvParser(() => new StringReader(Resources.winequality_white));
            var targetName = ""quality"";

            // read feature matrix (all columns different from the targetName)
            F64Matrix observations = parser.EnumerateRows(c => c != targetName).ToF64Matrix();

            // read targets
            var targets = parser.EnumerateRows(targetName).ToF64Vector();
```

I would like to load data from a List<Dto> object; how it can be possible?
Thanks!

edit:

```
    /// <summary>
    /// Parses the CsvRows to a double array. Only CsvRows with a single column can be used
    /// </summary>
    /// <param name=""dataRows""></param>
    /// <returns></returns>
    public static double[] ToF64Vector(this IEnumerable<CsvRow> dataRows)
    {
      if (dataRows.First<CsvRow>().ColumnNameToIndex.Count != 1)
        throw new ArgumentException(""Vector can only be genereded from a single column"");
      return dataRows.SelectMany<CsvRow, double>((Func<CsvRow, IEnumerable<double>>) (values => (IEnumerable<double>) values.Values.AsF64())).ToArray<double>();
    }
```

This code only works with CsvRow list."
Which model type should I use for financial price prediction?,"First of all thank you for the great library!
My question is simple: I want to predict next period price with pre-computed history values.
I have over 30 rows data for each price.
Price and datas are decimal.

**For example history:**
**Indicator1** - **Indicator 2** - **Indicator 3** - **Price**         - **Trend**
10,01121  - 23,56540    - 12.00001     - 12,23321   - UP
9,00001    - 3,00040    -   2.00001       - 1,23300     - DOWN
...
...
**And data to predict coming like** 
8,11211    - 1,00020    -   0.00021       - 3,5555     - **?**
I want to get TREND field.

Which model should I use? Any example will be perfect?
Regards!
"
Implement Coefficient of Determination (r-squared) metric,
Add ParameterType to optimization ParameterBound,"This pull request adds a `ParameterType` to the `ParameterBound` class in `SharpLearning.Optimization`. The parameter type specifies if the parameter is discrete or continous. This enables the parameter samplers used in the optimizers to sample from either a continous range or from a discrete range. Before this, all samplers would sample from a continous range.

For instance, during hyperparameter optimization of a gradient boost model, it makes more sense to sample the number of trees on a discrete range, and the learning rate on a continous range. If the number of trees is sampled on a continous range, the optimizer will try many continous values which will be cast to the same integer values in the learner. For instance 15.2325, 15.9343, and so on, will all be cast to 15. Using the discrete range will make the search more efficient.

The parameter type is specified on the `ParameterBound` class, together with the other options:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 10, max: 1000, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // iterations
    
    new ParameterBounds(min: 0.001, max:  0.2, transform: Transform.Logarithmic, 
         parameterType: ParameterType.Continuous), // learning rate
    
    new ParameterBounds(min: 1, max: 25, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // maximumTreeDepth
    
    new ParameterBounds(min: 0.5, max: 1.0, transform: Transform.Linear, 
         parameterType: ParameterType.Continuous), // subSampleRatio
};
```

The default value of the `ParameterType` on `ParameterBounds` is still `Continous`, so the default behavior of the optimizers will not change. 
"
Add proper optimizer seeding to all optimizers in SharpLearning.Optimization,"This pull request will add proper seeding to all optimizer algorithms in `SharpLearning.Optimization`. The technique used is a single seed is set through the constructor, this seed is used to create a `random` generator, which will then create seeds for all underlying algorithms used in the given optimizer.

Note, that since this PR will change default seeding of the optimizers, the default behavior of the optimizers will not be the same as before this addition.

This should solve issue #69 . "
Nuget package for SharpLearning.XGBoost can't install,"
Currently there is an issue when installing the SharpLearning.XGBoost package, Nuget will try to add a reference for the native xgboost dll:

![image](https://user-images.githubusercontent.com/9001637/40295861-298b3238-5cdb-11e8-9426-f7646974f015.png)

The reason for this is how the nuget package for PicNet.XGBoost.Net has been created. I have openened an issue to get this solved: https://github.com/PicNet/XGBoost.Net/issues/24
"
[Question] How would you use the model to make predictions on new data?,"I have read all the examples and gone through the source code, but haven't been able to answer the question.

I have setup a data set, trained and tested the model, but now I would like to use the model to make predictions on new data. How would I achieve this?

Example:
Target value has 3 classifications: good, bad, average

New data comes in -> use trained/tested model to make a prediction on the target value. Also, would it be possible to get a probability/confidence of the prediction of the target value i.e. 25% good, 50% average, 25% bad.
"
Most implementations of IOptimizer don't properly pass on the random seed to all internally used algorithms,"Hi mdabros,

I love the Optimizer classes of SharpDevelop, I use them heavily for hyperparameter tuning.
But I would like to use differend random seeds and parts of the Optimizer classes don't allow using them like that.
Example:
If you look at the constructor of your BayesianOptimizer you can see that it doesn't pass on the ""seed"" parameter to all other classes that BayesianOptimizer creates instances of, sometimes it will just forward a hardcoded 42 instead. 
I know, 42 is the answer, but I would prefer ""seed"" in this case... ;)
The other IOptimizer implementations have similar issues.
Would be nice if you could modify that some time... 

Thank you!

Best regards
Florian
"
Add SharpLearning.XGBoost project,"Add a more efficient alternative to `SharpLearning.GradientBoost`. XGBoost is faster on CPU and also supports GPU learning. However, it does have native dependencies, so might not be ideal for all platforms and situations.

A small test comparing the `RegressionXGBoostLearner` and the `RegressionGradientBoostLearner` from SharpLearning on a medium sized regression task. 

Dataset: [YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)
Rows: 515345
Cols: 90

Hardware:
CPU: Core i7-4770
GPU: GTX-1070

Model parameters:
`MaximumTreeDepth`: 7
`Estimators`: 152
`colSampleByTree`: 0.45
`colSampleByLevel`: 0.77

Training time compared using XGBoost in `histogram` and `exact` mode on GPU and CPU:

![image](https://user-images.githubusercontent.com/9001637/39971261-51276bf6-56f8-11e8-9d59-b50c46d4af7f.png)


As can be seen, XGBoost can be up to 70 times faster, when using the histogram based tree method. Using the exact method, which is more similar to the method from SharpLearning.GradientBoost, the speed up is still around 10 when using GPU, and 5 when using CPU. 

Missing tasks before the PR can be completed:
 - [x] Add argument checks to learners.
 - [x] Add unit test of conversion class.
 - [x] Add index support for learners, and input checks.
 - [x] Add probability support for classifier model.
 - [x] Add more learner and model tests.
 - [x] In the classification model, consider removing the targetNameToTargetIndex member, and adopt XGBoost¬¥s requirement of sequntial class labels starting at 0. Checks can be added to alert users before learning starts.
 - [x] Complete pull request to XGBoost.Net to enable GPU use and Booster selection.
 - [x] Add VariableImportance support to XGBoost models.
 - [x] Split objectives into regression and classification, so only compatible objectives are available for the learners .
 - [x] Consider splitting learners into `Linear`, `Tree` and `Dart`, to only show relevant hyperparameters for each in the constructors.
 - [x] Add enums for the DART specific parameters. 
 - [x] Complete pull request to XGBoost.Net to add DART parameters.
 - [x] Complete pull request to XGBoost.Net to fix `Booster.Dispose()`.
 - [x] Get XGBoost.Net to publish new nuget package.
 - [x] Change from local reference to updated XGBoost.Net package.
 - [x] Update readme.
 - [x] Check cross-validation and learning curves loops with XGBoost models (disposable).
 - [x] Package SharpLearing.XGBoost during build to avoid issue with ""dotnet pack"" and how the native dll is included in picnet.xgboost.net
 - [x] Add probability interfaces to xgboost classification learner.
 - [x] Add model converter from XGBoost to SharpLearing.GradientBoost. (Added but not completed).
 - [x] Consider using the SharpLearing.GradientBoost.Models instead of the XGBoost equivilants. This would enable standard serialization and features, and avoid having to deal with native resources when using the XGBoost models. (For now, it has been decided to use the XGBoostModels, and leave the conversion for another pull request)."
Fix the non-deterministic behaviour of the random forest and extra trees learners when running multi-threaded.,"This pull request will fix the non-deterministic behaviour of the random forest and extra trees learners. It will also ensure that the learners produce the same models when running single threaded and multi threaded.

The main changes to the learners are:
 - The random generator used for each tree is created before the learning process starts.
 - Each random generator is associated with a specific tree index.
 - The final order of the learned trees is made using the tree index.    

This pull request should solve issue #65 and is related to the previous pull request on that issue: #66 "
correct random seed problems in random forest learners,fix bug described in #65 
Random Forest: m_random and parallel RNG,"Hello, I would first like to thank you for making such an excellent and accessible repo. We're getting quite a bit of use out of it in multiple projects.

I've noticed that despite setting the seed in the Random Forest learner, I get slightly varying results from run-to-run given identical inputs. I suspect the problem is in the following code block:

```
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, new Random(m_random.Next())));
});
```

The parallel random number generation is a problem; the same random numbers are generated because the seed is set in the constructor, but there is a ""race"" between the threads to grab the next random number. Locking m_random would not help, I think. This should be fixed by generating a random number for each tree prior to entering the Parallel.ForEach loop, like so:

```
int[] randomNumbers = new int[m_trees];
for(int i = 0; i < randomNumbers.Length; i++)
{
   randomNumbers[i] = m_random.Next();
}
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, randomNumbers[work]));
});
```

Let me know if I'm overlooking something. I'd be happy to fork and make a pull request.

Rob"
An item with the same key has already been added,"Hello,

Might not be an actual issue, but more of a question on how to handle my error. I want to feed data into the parser that is in the SharpLearning.InputOutput.Csv package. Below is my code:

`string rawTarget = Transformations.ReturnColumnAsCSVString(Data, OutputColumn);

System.Windows.Forms.Clipboard.SetText(rawTarget);

var targetparser = new CsvParser(() => new StringReader(rawTarget));

var targets2 = targetparser.EnumerateRows(OutputColumn).ToF64Vector();`

So, first I pull my data into a string, this results in a string looking like this:
`""Vwap"";7049.4;6983.3;6981.8;6871.0;6846.7;6811.0`
(obviously there is a lot more)

Then I use the Stringreader to read my string and parse it to an F64 vector.  The error I get is:
`An item with the same key has already been added.`

I have also tried to convert the string to a stream, then use the Streamreader but this results in the exact same error. I am at a loss at how to solve this. 

Hope anyone can provide a solution! Thanks in advance!
"
Strongly-named assemblies,"First, let me say thanks for the great package: it works much better in our app than our previous (non-learning) solution.

We've got one issue to report: in our next release, all the various subsystems need to be in signed assemblies, which means we can't at the moment use SharpLearning since it would need to be called by a signed assembly and is itself unsigned. Any chance we could see signed versions of the SharpLearning Nuget packages?

Thanks,

Alistair
"
Possible evolution: trained neuralnet predict C output,"Hello,
I created a NeuralNet and trained it on my computer. 
I wish to port it on an embedded target now and i asked myself if a C code output of predict for a trained network could be possible. Is that the case ? If yes could you tell me some advises where to look at ?

Thank you"
change unsafe code to safe code,
Add support for enabling/disabling messages from learners during training.,"Currently, some of the learners in SharpLearning will output information during the training period. This includes the early stopping with gradient boost and neural networks. It should be possible to enable and disable these messages, and preferably, also possible to choose where the messages should be outputted. For instance to  `Console`, `Trace`, or alternatively a log file. 

This could be made by adding an `Action` to receive the message. This should probably be part of a separate interface, for learners supporting this. Something like:

```csharp
public interface ILogger
{
   public Action<string> Log { get; set }
}
```

The learners would then add the message to the log. Likewise, other algorithms like the optimizers could also implement this interface."
OutOfMemory Exception in F64Matrix constructor - maximum array bounds exceeded,"Hi there!

First: Thanks for the great work, excellent design you have there!

I am experiencing an OutOfMemory Exception in the constructor of F64Matrix that does not really come from memory shortage but from the fact that F64Matrix internally uses a single one dimensional double array that can quite easily exceed .NETs internal boundaries of maximum array dimensions.

In my case I tried to create a F64Matrix with 10 mio rows and 55 columns.

My preferred suggestion would be to either abstract the matrix to an IF64Matrix interface that probably only consists of the At() method overloads. This would enable users to provide a custom implementation that is capable of handling larger amounts of data, if needed even by swapping data from and to disk.
Another solution could be to change the internal implementation of F64Matrix to use an array of double arrays, which I believe could also help.

Thanks for your help and keep up the excellent work!

Best regards

Florian"
Add ParameterBounds and hyper-parameter scale transform to optimization ,"This pull request adds a ParameterBounds type to optimization to make it more clear to the user how to setup min and max bounds for the optimization parameters. This is illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
};
 ```
Note that the `GridSearchOptimizer` still uses jagged arrayes for defining parameter ranges. This is intended since a grid search typically invovles outlining all hyperparameters for the grid, and jagged arrays fits this purpose nicely.

It is now also possible to add a scale transform to the sampling of the hyper-parameters. This can be usefull when dealing with hyper-parameters like learning rate, that covers a large range close to zero, like 0.0001 to 1.0. In this case it would be better to sample using the logarithmic scale to get a better distribution of values across the entire range. Default transform is linear, and logarithmic scale can be applied like illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 100, max: 300, transform: Transform.Linear),
    new ParameterBounds(min: 0.001, max: 1.0, transform: Transform.Logarithmic),
};
 ```

Running the hyper-parameter tuning example from [SharpLearning.Examples](https://github.com/mdabros/SharpLearning.Examples/blob/master/src/Guides/HyperparameterTuningGuide.cs) before and after introducing logarithmic scale for the learning rate and subSampleRatio shows improved results:

**With linear scale**
Train error: 0.0174 - Test error: 0.3905

**With logarithmic scale**
Train error: 0.0011 - Test error: 0.3843

This will solve issue #57 .


"
Optimization: Add option for how to sample hyper parameters,"Currently, all optimizers in SharpLearning.Optimization use random uniform sampling for sampling hyper parameters from the provided min/max boundaries. This is not always optimal, for instance when dealing with a hyper parameters like learning rate that can span a large range of values, like 0.0001 to 1.0. Using random uniform sampling in this case might result in only sampling values in a small part of the range. In this case it would be much better to sample at uniform in the log space. Hence, it should be possible to select which space to sample from for each hyper parameter when setting og an optimizer. 

This should include at least random uniform form:
 - Linear (current method)
 - Logarithmic
 - Exponential

At the same time, setup of the hyper-parameter ranges could be changed from setting op an array of arrays to using a type to guide the user better:

**Current method**
```csharp
var parameters = new double[][]
{
   new double[] { 80, 300 }, // iterations (min: 80, max: 300)
   new double[] { 0.02, 0.2 }, // learning rate (min: 0.02, max: 0.2)
};
 ```

**Proposed method**
```csharp
var parameters = new OptimizerParameter[]
{
   new OptimizerParameter(min: 80, max: 300,  SamplingMethod.Linear), // iterations (min: 80, max: 300)
   new OptimizerParameter(min: 0.02, max: 0.2, SamplingMethod.Logarithmic),, // learning rate (min: 0.02, max: 0.2)
};
 ```"
[WIP] Backends testing and C# TF batch running,
[WIP] Add cntk cnn example (C# and Python),"Adding cntk convolutional neural network example in C# and Python [WIP].
 - [x] Examples should return the same result
 - [x] Examples should be as similar as possible


"
Add preserveObjectReferences option to GenericXmlDataContractSerializer,"The default settings for the GenericXmlDataContractSerializer is to perserve object references. This is required for the SharpLearning.Neural.Models to be serialized and deserialized correctly. However, for serializing simple types like a list of items, the resulting xml can be slightly more complicated. Therefore this pull request adds the option to not preserve object references when using the GenericXmlDataContractSerializer.

The recommended setting for serializing SharpLearning models is the default (true).

**XML with  perserveObjectReferences = true (default):**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" z:Id=""1"" z:Size=""5"" xmlns:z=""http://schemas.microsoft.com/2003/10/Serialization/"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key z:Id=""2"">Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""3"">Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""4"">Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""5"">Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""6"">Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```

**XML with  perserveObjectReferences = false:**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key>Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```"
Random Forest Regression generates constant values for prediction,"Hi,

I am using random Forest for learning. The output I get results into the initial 40 or so values varying (Float values) but after that it's just constant. 

I was wondering if you have seen this behavior. The data has 24 features and 500 observations. I get the 42 first prediction varying but after that it's just constant. 

I can provide the data and the code if you would like that.

regards,
Avi"
Add cntk python simple mnist examples,"As a starting point for simple python mnist example using cntk, copied the original example from [cntk](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)

Currently, I have not found a method for running/debugging this from visual studio, only from command prompt. More information can be found in the [readme](https://github.com/mdabros/SharpLearning/blob/backends-cntk-python/python/src/CntkPython/README.md)

@nietras Currently, the simple mnist example from CNTK is quite different from corresponding example in Tensorflow. We should decide how much we want to modify the CNTK example, and/or the tensorflow example, to make them as similar as possible.

Some differences: 
 - The CNTK example uses text files as input and tensorflow the raw data.
 - The CNTK example also uses the Dense operator from the layer API.
 - The CNTK exmaple uses all 60k examples for training and tensorflow only 10k."
Consider switching to XorShift for RNG,"XorShift is a simple alternative to the built-in Random class in C#, which provide better performance and randomness. I benchmarked the RNG implementations of Math.net as seen in the picture. The XorHome in the list is my own quick port of xoroshiro128+ from here: http://xoroshiro.di.unimi.it/xoroshiro128plus.c

![image](https://user-images.githubusercontent.com/657616/35411685-ec14329a-0219-11e8-8e19-82716a4361ef.png)

As seen in the picture, the Math.Net XorShift is much faster than the built-in random."
Backends Tensorflow Deep Mnist Raw (to backends!),
[WIP] Backends Tensorflow Deep Mnist Raw,
Backends TensorFlow prototyping,"Create PR to more easily follow changes.

I made the mistake of merging with master, so perhaps ""backends"" should merge with master too?"
Add CntkRawMnistTest. Showing a simple cnn using the cntk api,Training a simple cnn using the cntk api. 
Backends new cntk test project,"Changing the `SharpLearning.Backend.Cntk.Test` project format from new csproj to old style .net framework csproj. Also, change platform target for the project to x64 to work with CNTK.

"
*NOMERGE* Cntk experiment,Create PR to easier review the changes in this.
Failing unit test ClassificationGradientBoostLearner_LearnWithEarlyStopping,"This always fails on when I run `all.ps1`
cc: @mdabros 
```
Failed   ClassificationGradientBoostLearner_LearnWithEarlyStopping
Error Message:
   Assert.AreEqual failed. Expected a difference no greater than <1E-06> between expected value <0.162790697674419> and actual value <0.13953488372093>.
Stack Trace:
   at SharpLearning.GradientBoost.Test.Learners.ClassificationGradientBoostLearnerTest.ClassificationGradientBoostLearner_LearnWithEarlyStopping() in E:\oss\SharpLearning\src\SharpLearning.GradientBoost.Test\Learners\ClassificationGradientBoostLearnerTest.cs:line 120
Standard Output Messages:


Debug Trace:
Iteration 1 Validation Error: 0.674418604651163
   Iteration 11 Validation Error: 0.290697674418605
   Iteration 21 Validation Error: 0.244186046511628
   Iteration 31 Validation Error: 0.22093023255814
   Iteration 41 Validation Error: 0.186046511627907
   Iteration 51 Validation Error: 0.186046511627907
   Iteration 61 Validation Error: 0.197674418604651
   Iteration 71 Validation Error: 0.174418604651163
   Iteration 81 Validation Error: 0.13953488372093
   Iteration 91 Validation Error: 0.162790697674419
```
Both for `Debug` and `Release`.
"
0.26.7.0: TimeSeriesCrossValidation,"With most time series data, it is not possible to use traditional cross-validation methods, like the CrossValidators available in SharpLearning. The reason for this is, that shuffling the data will result in the learner and model using future data to predict past observations.
While it is possible to use the NoShuffleTrainingTestSplitter to create a single split without chainging the order, this will limit the size of test set and for smaller datasets reduce the robustness of the test set error/generalization error.

For this reason, this pull request introduces the TimeSeriesCrossValidation<T> class. Time series cross-validation is based on rolling validation, where the original order of the data is kept, and new observations in the test interval are predicted as hold-out samples and following included in the model one at a time. A nice illustration of this can be found here: [Cross-validation for time series](https://robjhyndman.com/hyndsight/tscv/). 

The TimeSeriesCrossValidation<T> class supports the following features:
 - InitialTrainingSetSize: Specify how much data the initial learner/model should use.
 - maxTrainingSetSize: Specify a max size for the training interval. If no max size is specified, this will correspond to an expading training interval. If a max is specified, this will correspond to a sliding training interval.
 - retrainInterval: Specify how often the model being validated should be retrained. If no interval is specified, the model will be retrained each time a new time step is predicted and included. If an interval is specified, the model will only be retrained at the specified interval and the existing model will be used for validation predictions inbetween the retrain intervals.

More information can be found in the documentation of the code and the unit tests.

A short example showing how to measure the mean square error using The TimeSeriesCrossValidation<T>class:

```c#
var tsCv = new TimeSeriesCrossValidation<double>(initialTrainingSize: 30);

// Calculate the validated predictions.
var timeSeriesPredictions = tsCv.Validate(new RegressionDecisionTreeLearner(), observations, targets);
// Get the targets corresponding to the validation predictions. 
var timeSeriesTargets = tsCv.GetValidationTargets(targets);

// Measure the mean square error
var metric = new MeanSquaredErrorRegressionMetric();
var mse = metric.Error(timeSeriesTargets, timeSeriesPredictions);
```


 "
Improve SequentialModelBasedOptimizer (now BayesianOptimizer),"While attending the NIPS 2017 conference I was lucky enough to have Frank Hutter, the author and co-author of several [Bayesian Optimization papers](http://ml.informatik.uni-freiburg.de/people/hutter/publications.html), explain me some insightful details about this type of optimization.

This pull request contains improvements to the SequentialModelBasedOptimizer, now renamed to BayesianOptimizer, based on some of the insights provided by Frank Hutter.

The main changes are:

- Model type changed from RandomForest to ExtraTrees. The important change here is how the split in the decision trees are calculated. RF: (v1 + v2)/ 2 vs. ET: random * (max - min)  + min, where random is between 0 and 1.
- Optimizer type for finding the maximum of the acquisition function changed to RandomSearchOptimizer. This should matter less, but compared to the ParticleSwarmOptimizer, this seemed to work better in practice.
- Refactored the BayesianOptimizer to be easier to extend with other model types, optimizers and acquisition functions in the future.
  - Currently the model type, optimizer type and acquisition function is hardcoded:
    - Model type: ExtraTrees
    - Optimizer type: RandomSearchOptimizer
    - Acquisition function: Expected improvement
  - A natural extension would be to make it possible to inject other types. Like a Gaussian Process instead of the ExtraTrees model and so forth. These changes will be included in a later pull request.

I ran a small test to illustrate the improvements, optimizing the hyper parameters of a classification decision tree learner on the [landsat satellite dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)) from UCI. As can be seen on the attached image, the updated BayesianOptimizer uses significant less function evaluations compared with the old implementation (SequentialModelBasedOptimizer), and the RandomSearchOptimizer, while also finding a better minimum.

![image](https://user-images.githubusercontent.com/9001637/34309947-faac473c-e754-11e7-8c50-e51d42c00670.png)
"
Fix for AccessViolationException in F64MatrixView and F64MatrixColumnView for large matrices,"#38 

Changed the pointer offsets applied in `F64MatrixColumnView.RowPtr(row)` and `F64MatrixView.this[row]` to use `long` instead of `int` - integer overflows were occurring when the length of the underlying `double[]` in `F64Matrix` passed `int.MaxValue / sizeof(double)`."
0.26.5.1: Add .net461 to target frameworks,"When using .net standard 2.0 class libraries from a .net framework application, all system dependencies will be copied to the build output. To avoid this, a net461 build has been added to the target frameworks. This means that future SharpLearning nuget packages will contain both a .netstandard2.0 build and a .net461 build.

This might change in the future if .net standard changes the way dependencies are handled."
Add contribution guide,"Adding a contribution guide to make it easier for other developers to contribute to SharpLearning. Some parts of the guide could use more details, but this should provide a start and make the contribution process more clear."
System.AccessViolationException when retrieving data from a large dataset,"In `F64MatrixColumnView.RowPtr`, integer overflow can occur when `row * m_strideInBytes` is larger than `int.MaxValue`, resulting in an invalid offset being applied to `m_dataPtr`:

    double* RowPtr(int row)
    {
        return (double*)((byte*)m_dataPtr + row * m_strideInBytes);
    }

I am happy to fix this (it just requires a cast to `long`), but I'm not sure how to contribute - do I branch from master, then push and create a pull request from the GitHub website? In the future, if I find a simple bug like this, should I raise an issue, or can I just push with details and let you decide whether it's a good fix?

I haven't contributed to an open source project before!"
Unnecessary System Files Generated,"For our project [MetaMorpheus](https://github.com/smith-chem-wisc/MetaMorpheus), after adding Sharplearning NuGet Package to our EngineLayer and TaskLayer, in the GUI WPF project, there is an excessive amount of unnecessary system dll files generated in the output folder after building (No matter release or debug). Here is a list of these [files](https://github.com/smith-chem-wisc/MetaMorpheus/issues/767#issuecomment-349014733). We really couldn't determine where is the problem since there is no trace in the .csproj files and references of GUI nor related projects. So please help us if you have any idea! Thanks a lot.
"
"CS0012	The type 'Object' is defined in an assembly that is not referenced. You must add a reference to assembly 'netstandard, Version=2.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.","Fresh download, after restoring packages via NuGet.

How to solve it????"
Backends prototyping,"Creating a PR for the backends stuff to more easily see the changes.

cc: @mdabros "
Unittests fail because of localization settings,"Some unittests compare against hardcoded strings written in the test method. These fail on systems that use ,(comma) as decimal separator instead of .(dot). These strings should probably be loaded from a resource or the entire library should work with invariant culture unless otherwise specified."
Predict overload for multiple observations added to IPredictor<TPrediction>,
Add TPrediction[] Predict(F64Matrix observations) to public interface IPredictor<TPrediction>,"I think it would be a good idea to add the `F64Matrix `overload to the `IPredictor `interface as it would make it easier to use the `IPredictorModel `interface in your code. The models seems to implement it already. 

It would add a dependency for `SharpLearning.Containers.Matrices` in `SharpLearning.Common.Interfaces`, but I think it is unlikely that you would use the SharpLearning library without referencing `SharpLearning.Containers.Matrices` anyway."
Duplicate efforts,"Hi @mdabros!

I've just found your library a couple days ago and couldn't help but notice the similarity between both of our projects, SharpLearning and [Accord.NET](https://github.com/accord-net/framework). Since we both share the same goal (bring serious machine learning to .NET), and instead of duplicating our efforts, wouldn't you be willing to join the Accord.NET project as well? 

Seeing your extremely well-organized repository and coding skills, you would be more than welcome in joining Accord.NET as one of its authors. 

Regards,
Cesar"
Change to unified project versioning,"After the migration from .net framework/desktop to .net core, I decided to switch to individual versioning for each project in SharpLearning. However, since vsts continuous integration/delivery currently does not support skipping already published nuget packages, I have decided to switch back to unified versioning across all projects. This is done differently with .net core projects compared to .net framework projects. I followed the advice from this answer on stackoverflow: [sharedassemblyinfo-equivalent-in-net-core-projects](https://stackoverflow.com/questions/42790536/sharedassemblyinfo-equivalent-in-net-core-projects). 

This solution also allows to have assembly attributes shared among the projects in one location."
Added dictionary version of KeyCombine which is a lot faster,moved unittests from CsvParserTests to CsvRowExtensionsTests
Add better error messages to learners when input data is not valid,"This pull request should provide better error messages and feedback from the learners when invalid input data is provided and solve issue #27. 

Added checks for all learners includes: 
 - Observations: Verify that row and column count is larger than zero.
 - Targets: Verify that row count is larger than zero
 - Observations and Targets: Verify that the row count of observations and targets are equal
 - Indices: Verify that there are no negative indices provided. Verify that the max index does not exceed the row count of observations and targets.  "
Better error messages from learners in case of dimensionality mismatch,"Currently, there are no checks to verify that the dimensions of the observation matrix and the target array matches before learning is started. This results in error messages from somewhere in the learner implementation, providing poor error messages and feedback to the user.

Checks should be added to all learners to ensure that the provided arguments and data is valid, before starting the learning process."
SharpLearning.Neural full .net core2.0/standard 2.0 support,"SharpLearning.Neural depends on [math.net](https://github.com/mathnet/mathnet-numerics), which does not currently support .net core 2.0/.net standard 2.0. Hence, SharpLearning.Neural will only work on .NET Desktop/Windows. 

Support for .net core 2.0/.net standard 2.0 is planned for math.net, so full support for SharpLearning.Neural will also be possible once this is implemented. Eventually, #9 might also solve this. "
Vsts continuous integration,"Merging the move to vsts continuous integration. This will also add continuous delivery, packing and pushing nuget packages with each commit to the master branch. The nuget steps are disabled for pull requests against the master branch."
Migrate everything to .NET Core 2 (.NET Standard 2.0),"In the end it was easier for me to start from scratch, so I didn't use the branch you had prepared (hopefully that's ok).

Here are a few things from the migration that I thought I should mention:
* I disabled the ""auto-generate AssemblyInfo"" feature of the new .csproj files so that existing `AssmeblyInfo.cs` continue to be used.
* I left the old `.nuspec` files in place.  These take precedence over the NuGet info in the new .csproj files.
* Updated the `SerializationString` value used in the `GenericBinarySerializer_Serialize` and `GenericBinarySerializer_Deserialize` tests to reflect the .NET Core behaviour of the `BinaryFormatter`."
AdaBoostLearners: Add subsample ratio pr. tree as hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, subSampleRatio, which controls how many training samples are forwarded to each tree in the ensemble. When subsampling is active, samples from the training data will be drawn with replacement, leading to more variation among the trees in the ensemble. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

In the RandomForest implementation of this feature, there is sampling with replacement, even if subSamplingRatio=1.0, this is part of the algorithms design. However, for the AdaBoost implementation of this feature, if subsampling is off (subSampleRatio=1.0), no sampling with replacement should be introduced, and the whole training set should be considered in each tree of the ensemble. This will result in the 'classic', AdaBoost algorithm, if the subSamplingRatio is set to 1.0. 

Besides the difference when subSampleRatio=1.0, the AdaBoost implementation should be very similar to the RandomForest implementation, which can be found here [RandomForest](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.RandomForest/Learners/ClassificationRandomForestLearner.cs)."
AdaBoostLearners: Add features pr. split as regularization hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, featuresPrSplit, which controls how many randomly selected features are considered during the decision trees search for a new split. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

Sine the DecisionTreeLearner used in adaboost already supports 'featuresPrSplit', the implementation should simply add the hyper parameter to the adaboost learner contructors and forward the parameter to the DecisionTreeLearner."
Better default parameters for DecisionTreeLearners,"Currently, the DecisionTreeLearners ([ClassificationDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/ClassificationDecisionTreeLearner.cs) and [RegressionDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/RegressionDecisionTreeLearner.cs)) does not have very good default parameters. With a maximumTreeDepth=2000, using the default paramters will, in most cases, result in a model that overfits the problem. Hence, a better set of default paramters should be found, that, in more cases results in a better regularized model."
Replace SharpLearning.Containers.Matrices.F64Matrix with multidimensional array,"In SharpLearning, the F64Matrix class, which is part of the [Learner interfaces](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Common.Interfaces), is mostly used as a container for holding the features for a learning problem. While SharpLearning does contain some [arithmetic extensions](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Containers/Arithmetic) for the F64Matrix, the arithmetic is not used by any of the learners. Also, more efficient implementations can be found in [Math.net](https://github.com/mathnet/mathnet-numerics). 

Therefore it might indicate, that the primary container for features in SharpLearning should rather be a standard .net type like multidimensional array (double[,]) or jagged array (double[][]), with some extension methods to add the current functionality of the F64Matrix. 

An alternative, also suggested in #6, would be to replace the F64Matrix directly by using Math.net as the matrix provider. However, since only the SharpLearning.Neural project is using matrix arithmetic and with the plan of using [CNTK as backend](https://github.com/mdabros/SharpLearning/issues/9), math.net is a large dependency to take, if only using the matrix class as a feature container. So currently, I am leaning more towards replacing F64Matrix with a standard .net type. However, to better handle integration between Math.Net and SharpLearning, maybe a separate project, SharpLearning.MathNet, could be added with efficient conversions between Math.net and SharpLearning containers (both copy and shared memory). This of course depends on what data structure ends up replacing F64Matrix, if any.

These are my current thoughts, and people are very welcome to discuss and pitch in with suggestions. 
"
Metrics: Consider adding support for sample weighted metrics,"When dealing with imbalanced data sets, it can be beneficial to use sample weighted metrics. This task should be split into several tasks, one for each metric, if sample weights are to be supported in the [metrics project](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Metrics).  "
LearningCurves: Add support for weighted learners,"Extend the [ILearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/ILearningCurvesCalculator.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
Implement support for sample weights in [LearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/LearningCurvesCalculator.cs). "
CrossValidation: Add support for weighted learners,"- Extend the [ICrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/ICrossValidation.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
- Implement support for sample weights in [CrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/CrossValidation.cs)."
Ensemble learners: Add support for sample weights,"Add support for sample weights to the Ensemble learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This task requires #14 to be done first, since the ensemble learners needs to be extended to also support weighted learners in the constructor.

Following the learners must implement weighted learner interfaces and should simply forward the sample weights the learners in the ensemble. The ensemble learners can be found here in the ensemble project: [Ensemble learners](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Ensemble/Learners)"
NeuralNet: Add support for sample weights,"Add support for sample weights to the NeuralNet learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This is currently on-hold until #9 has been decided."
Add IWeigtedLearner and IWeigtedIndexedLearner interfaces to Common.Interfaces,"Add interface for learners supporting sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights);
```

Add interface for learners supporting sample indices and sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights, int[] indices);
```
"
GradientBoost: Add support for sample weights,"Add support for sample weights to the GradientBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

The GBMDecisionTreeLearner, used by GradientBoost does not support sample weights, so adding sample weight support to the GradientBoost learners requires first adding it to the GBMDecisionTreeLearner. Adding sample weight support to the GBMDecisionTreeLearner, primarely requires using the wieghts in the loss functions: [GradientBoost Loss](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Loss)

Following, sample wieght support can be added to the GradientBoost learners. The learners can be found here in the GradientBoost project: [GradientBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Learners)"
ExtremelyRandomizedTrees: Add support for sample weights,"Add support for sample weights to the ExtremelyRandomizedTrees learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by ExtremelyRandomizedTrees, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project:
 [ExtremelyRandomizedTrees](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
RandomForest: Add support for sample weights,"Add support for sample weights to the RandomForest learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by RandomForest, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project: [RandomForest](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
AdaBoost: Add support for sample weights,"Add support for sample weights to the AdaBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by AdaBoost, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner in each boosting iteration. The learners can be found here in the AdaBoost project: [AdaBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.AdaBoost/Learners)

The work is currently in progress in the branch [adaboost-sample-weight-support](https://github.com/mdabros/SharpLearning/tree/adaboost-sample-weight-support)"
CNTK as backend for SharpLearning.Neural,"The Microsoft team working on [CNTK](https://github.com/Microsoft/CNTK) has recently released the initial version of the C#/.Net API with support for both evaluation and training of neural networks. A more feature complete version, with support for layers and other helpful features, should arrive before the end of the year. Currently, there seems to be a few performance related issues (https://github.com/Microsoft/CNTK/issues/2374 and https://github.com/Microsoft/CNTK/issues/2386) but hopefully these will be also be solved in the next release.

Using CNTK as backend for SharpLearning.Neural will add operators for more layer and network types, while also enabling GPU training and evaluation. Using a well supported deep learning toolkit as backend will also help to ensure that future operator, layer and network types will be available faster.

This task will require a large rewrite of SharpLearning.Neural, most likely only keeping the top level interface. However, since all the core operations are availible from CNTK, most of the hard work is already completed.

This task should be split into multiple others when a design of how CNTK should be integrated has been completed. A few considerations:
- Should the integrations be ""simple"", i.e. only have a NeuralNetLearner and NeuralNetModel in SharpLearning and use the layer construction and related functionality from CNTK directly?
- Should the integration hide CNTK behind an adapter to make it easier to support other deep learning toolkits like TensorFlow(Sharp)? "
.Net Core and .Net Standard support,"Add .NET Core and .Net Standard support to make SharpLearning available on more platforms. Porting to .Net Standard involves the following tasks:

- Retargeting the projects .NET Framework version to .NET Framework 4.6.2.
- Determining the portability of the code using API Portability Analyzer. This has been done, and only the GenericXmlDataContractSerializer from SharpLearning.InputOutput uses unsupported API calls.
- Change the implementation of GenericXmlDataContractSerializer to conform with .net standard 2.0. This is possible with the available API calls, however there are issues with serializing some of the Math.net containers used in the NeuralNet models. This might be solved together with #9, since CNTK will most likely replace math.net in the SharpLearning.Neural project.
- Change project format to .net core.

After the porting process the continuous integration on appveyor must be updated."
"Exception: Source array was not long enough. Check srcIndex and length, and the array's lower bounds","The following code throws a System.IndexOutOfRangeException on line 328 in GBMDecisionTreeLearner.cs

```
            var sut = new RegressionSquareLossGradientBoostLearner();

            Random rnd = new Random(42);
            var rows = 10000;
            var columns = 1;
            double[] values = new double[rows * columns];
            for (int i = 0; i < rows * columns; i++)
                values[i] = rnd.NextDouble();
            Containers.Matrices.F64Matrix observations = new Containers.Matrices.F64Matrix(values, 1, 10000);
            double[] targets = new double[rows];
            for (int i = 0; i < rows; i++)
                targets[i] = rnd.NextDouble();

            var model = sut.Learn(observations, targets);
```"
Math.NET Matrices,"This is a really great library. Was there a specific reason why you chose to roll your own Matrix class, rather than leveraging Math.NET?

Ideally I'd like to marry the two (not only for consistency with modules I've already written, but even for smaller things like using Matrix<float> rather than Matrix<double>). Before I jump in and start changing anything, though, I thought I'd check with the author to see if there was a specific reason behind it.

If I do proceed with integrating the two, more than happy to submit back a PR too, just let me know."
Thanks for developing and sharing this brilliant machine learning package in C#,
Please share your vision of .NET deep Learning,"@mdabros Pls apologize if I hijack your excellent work here.

Daniel from MSFT is gathering [a broad vision for .NET Deep Learning here.](https://github.com/Microsoft/CNTK/issues/960#issuecomment-315049580)  I think you may have unique view on this.  "
[Question] 2d - 3d output in neural networks,"Is it possible ? 
And if it's possible, how to train the network ?"
Feature Suggestion,"
First of all, I want to congratulate you for this project. I have a suggestion, couldn't figure where to write it other than issues on GitHub.

My suggestion is, number of observations (or better their indexes, one can count them) that fall to the left and right child of a node.
"
"Restructure repo, add scripts and nuget packaging",
ËØ∑ÈóÆÊï∞ÊçÆÈõÜÊù•Ê∫êÊòØÔºü,
python demo_single.py ---->TypeError: can't pickle weakref objects,"Traceback (most recent call last):
  File ""demo_single.py"", line 36, in <module>
    pickle.dump(clf, f)
TypeError: can't pickle weakref objects
"
Êä±Ê≠âÁúãÈîô‰∫Ü,‰∏∫‰ªÄ‰πà‰∫åÂàÜÁ±ª‰ªªÂä°Áî®categorical_crossentropyÔºåÂ§öÂàÜÁ±ª‰ªªÂä°Áî®binary_creossentory
predictÁöÑlabel‰∏•Èáç‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢ò,"ÈÅìÂèã‰Ω†Â•Ω~
‰ΩøÁî®DEMOÁöÑÊ≥ïÂæãÊï∞ÊçÆËøõË°åÊµãËØïÂèëÁé∞,predictÁöÑlabelÂá†‰πé(95%‰ª•‰∏ä)ÂÖ®ÈÉ®ÊòØ‰∏ÄÊ†∑ÁöÑ
ÊàëÊü•Áúã‰∫ÜÂâç‰∏â‰ΩçÁöÑÁ¥¢ÂºïÂÄº
prediction.argsort()[-3:][::-1]
Á±ª‰ºº:
[139,34,55]
[139,34,55]
........
[139,55,34]

Âü∫Êú¨ÂÆåÂÖ®‰∏ÄÊ†∑

ÊàëÂèëÁé∞predictionÂÜÖÁöÑÂÄºÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑ,‰ΩÜÊúÄÈ´òÁöÑÂá†‰∏™ÂÄºÈÉΩÊòØÂú®Âõ∫ÂÆö‰ΩçÁΩÆ
Êç¢ÊàêÊàëËá™Â∑±Áî®ÁöÑÊï∞ÊçÆ(Ê†áÁ≠æÊõ¥‰∏∞ÂØå)ËøõË°åÊµãËØï,ÂêåÊ†∑ÁöÑÈóÆÈ¢ò,ÊØèÊ¨°ÈÉΩËøòÊòØÂá†‰∏™Âõ∫ÂÆöÁöÑlabel
‰∏çÊòéÁôΩÊòØÂì™ÈáåÂá∫‰∫ÜÈóÆÈ¢ò





"
È™åËØÅ,ÂèØ‰ª•Áîª‰∏™roc-aucÊõ≤Á∫øÂØπÊ®°ÂûãÁöÑÊÄßËÉΩÂÅö‰∏™ËØÑ‰º∞
ËØ∑ÈóÆ‰∏Ä‰∏ãÔºåAliÊâìÊ†áÁ≠æ‰∏≠Ôºåorg_codeÊåáÁöÑÊòØ‰ªÄ‰πàÔºü,ÊòØregionIDËøòÊòØÔºü
Gutenberg file naming changed,"I believe the files changed a little bit on the Gutenberg website. 
The ASCII versions are now (mostly) found under an ""old"" folder. 
Also, this might be due to how my internet is set-up locally, but downloading via http creates empty files, hence I changed to https.

For the Pride and Prejudice book I couldn't find the same file, This one seems to be in UTF-8. Nonetheless everything seems to be fine with the pushed code. 

Best,"
Add soft K means,"http://www.inference.org.uk/mackay/itprnn/ps/284.292.pdf
http://www.inference.org.uk/mackay/itprnn/code/kmeans/

Example code:
```q
\l funq.q
\l iris.q
X:iris.X
\S -314159i
c:.ml.forgy[3] X
/ 
these all return the same answer (within epsilon 5e-5)
5.006    3.428    1.462    0.246   
5.901613 2.748387 4.393548 1.433871
6.85     3.073684 5.742105 2.071053
\
asc flip .ml.kmeans[X] over c
asc flip .ml.kmeanss[X] over c
asc flip .ml.kmeans1[100;X] over c1
```"
How to compile the code?,"I work under win10, win32-bit q . Useing MSYS2 to Compile., which is armed by the following packages:

- tar
- make
- patch
- mingw-w64-i686-gcc-fortran

under the target folder, type make much errors appear, as the attach file. NEED HELP!
[error.txt](https://github.com/psaris/funq/files/6866296/error.txt)
"
possible typo in book,"First congrats on the book! Looks great, very excited to go through it. Not sure if this is the right place to report typos but on p 16, section 2.4 in it refers to ""3...independent features (petal and sepal measurements..."" and ""4...dependent feature (species label)"" and then at the bottom of the page it says ""we have access to the independent (species labels) and dependent (sepal and petal measurements)""  The independent/dependent labels are reversed. I believe the first labels are correct but figured i'd let you know."
fixed weighted odds,Added missing multiplication operation.
Take advantage of symmetry of cNk (Pascal),More stable and faster for large n
fixing typos,
fix typos in comments,
fix typo in comments,
fix two typos in comments,
fix two typos,
Remove trailing whitespace,
fix correlated matrix in linreg.q,"was `X[0]` which produced results for `rho` as if it was `1-rho`. 
Test: plot for `rho:1f` should show a line but was showing circle"
Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5√ó3 positions and 5√ó4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5√ó3 positions and 5√ó4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4√ó3 positions and 4√ó4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter network„ÄÇ
"
Problem in training the segmentation network using the provided dataset,
Pose estimation has huge position and orientation error for one object and does not appear to decrease,"Hello,

So I generated my own training set and am attempting to train the pose estimation portion with just one object, I was able to go through all the steps to get the CAD model properly setup, the PCD files and do all the stuff with the redis-server. I ended up attempting to train the model, and after 3000 epochs, I find the error is quite large, roughly the same as what it was at the start (37889.62 m for position, 127.81 m for orientation). Any advice on what I might be doing incorrectly?
"
How can I get ground truth pose and the segmentation mask?,"Hi Jimmy

I have already downloaded the oil change dataset, I would like to ask how to get the ground truth poses and the segmentation masks from the .json file? (specifically, I want to get the ground truth pose and the segmentation mask of the blue oil funnel)

Thank you so much!"
Why do we need blender for demo?,"Hi, thanks for your great work. I wanted to see the results of your work, i.e. I will give a RGB image and expect 6dposes. I believe that blender was used to create the synthetic dataset for training Pose interpreter network. My question is why do we need blender for seeing the results.

My other question is if we require blender then how to use it on some online platform like google collab? "
end_to_end_eval position estimation (own dataset),"Hey there,

when I evaluate the data using eval.ipynb on the small images, which where used for training the pose estimation network, the position error is abot 1.5 cm. So that's good.

But when I use the end to end evaluation, the position error ist about 2 m. The error is only this big in the z coordinate. The x and y coordinates are fine.

Do you have any idea why it is estimating the position an the small pictures right, but on the masks it is getting from the segmentation network wrong.

Thanks in advance!

Kind regard!"
steps to train model again with another DRN,"Hello, 
I want to train another drn model on the same dataset. Please guide me through steps for end to end evaluation. Also, I can see DRNSeg being called during training. But where is drn22 being called by DRNSeg ?"
I think [this line](https://github.com/jimmyyhwu/pose-interpreter-networks/blob/master/pose_estimation/render_pose.py#L6) is supposed to take care of the import problem. I did not have to add any workaround to run the notebooks. Maybe you could try opening `jupyter notebook` in the same directory that the notebook is in?,
Rendering folder calling issue,"the code for segmentation works fine but creates a problem for me in end_to_end_visualization.py and pose_estimation/demo.py in importing 
from dataset import oilchange_scene

I think when at folder pose_estimation it needs to access oilchange.py file which is in dataset folder, and from folder I cannot import this way. Hence, it shows and error. Please suggest a work around for this."
pose estimation demo runtime error,"While running demo.ipynb the following error comes up. I am unsure is it the sound card error or CUDA error ? Please help.

ALSA lib confmisc.c:768:(parse_card) cannot find card '0'
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory
ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default
AL lib: (EE) ALCplaybackAlsa_open: Could not open playback device 'default': No such file or directory
found bundled python: /opt/blender/2.79/python
Import finished in 1.6221 sec.
Fra:1 Mem:56.16M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Creating Shadowbuffers
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Raytree.. preparing
Fra:1 Mem:91.02M (0.00M, Peak 91.02M) | Time:00:00.10 | Raytree.. building
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Raytree finished
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Creating Environment maps
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Caching Point Densities
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Loading voxel datasets
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Volume preprocessing
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:91.04M (0.00M, Peak 143.57M) | Time:00:01.32 | Scene, Part 13-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 3-20
Fra:1 Mem:91.37M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 11-20
Fra:1 Mem:91.30M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 9-20
Fra:1 Mem:91.21M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 7-20
Fra:1 Mem:91.48M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 19-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 15-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 5-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 18-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.40 | Scene, Part 10-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 1-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 4-20
Fra:1 Mem:91.91M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 17-20
Fra:1 Mem:91.82M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 2-20
Fra:1 Mem:91.36M (0.00M, Peak 143.57M) | Time:00:01.43 | Scene, Part 14-20
Fra:1 Mem:90.95M (0.00M, Peak 143.57M) | Time:00:01.44 | Scene, Part 8-20
Fra:1 Mem:90.57M (0.00M, Peak 143.57M) | Time:00:01.45 | Scene, Part 12-20
Fra:1 Mem:90.19M (0.00M, Peak 143.57M) | Time:00:01.46 | Scene, Part 20-20
Fra:1 Mem:89.99M (0.00M, Peak 143.57M) | Time:00:01.49 | Scene, Part 16-20
Fra:1 Mem:89.28M (0.00M, Peak 143.57M) | Time:00:01.56 | Scene, Part 6-20
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Determining resolution
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Initializing execution
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Tile 1-2
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.58 | Compositing | Tile 2-2
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Compositing | De-initializing execution
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Sce: Scene Ve:126775 Fa:253749 La:1
Saved: '/tmp/tmpz3aqpi1j/render.png'
 Time: 00:01.75 (Saving: 00:00.05)


Blender quit
Traceback (most recent call last):
  File ""<stdin>"", line 14, in <module>
  File ""<stdin>"", line 3, in visualize_batch
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 112, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 52, in forward
    x = self.resnet18(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 199, in forward
    x = self.conv1(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDNN_STATUS_MAPPING_ERROR"
No such file or directory: 'data/OilChangeDataset\\annotations\\val_20171103_OilChange.json',"I can not unzip the oil change dataset. Its showing corrupted file. 
Kindly help, please !!!!"
blender linking issue,"It seems to be easy but I am facing trouble with the process of linking blender. Kindly let me know where am I going wrong.
Step 1:  I downloaded blender-2.79b-linux-glibc219-x86_64.tar.bz2  from the link : https://download.blender.org/release/Blender2.79/

Step 2: created an empty directory blender in /usr/local/bin/blender 

Step 3: extracted .tar file at the downloaded location.

Step 4: to link the binary file : lm -s /blender_download_folder/blender /usr/local/bin/blender

This gives me an error ln: failed to create symbolic link '/usr/local/bin/blender/blender': File exists. I could not find any executable file in the downloaded folders. Please let me know if I am wrong or missing out something.

"
end_to_end_visualize.ipynb HTTP 403 forbidden error ,"I loaded all the pretrained models and dataset required. While running end_to_end_visualize.ipynb I get an error at line 
segm_model = segm_models.DRNSeg(segm_cfg.arch, segm_cfg.data.classes, None, pretrained=True)

Downloading: ""https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth"" 
 line 65, in load_url
    _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
in _download_url_to_file
    u = urlopen(url)
  File ""/usr/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.6/urllib/request.py"", line 532, in open
    response = meth(req, response)
  File ""/usr/lib/python3.6/urllib/request.py"", line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/usr/lib/python3.6/urllib/request.py"", line 570, in error
    return self._call_chain(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
"
Issue with Segmentation Eval 403 Forbidden,"Hi Jimmy,

I have downloaded the pretrained datasets, however when I attempt to run the segmentation eval Jupyter notebook. I receive an error in the third block. It seems that the code is downloading a checkpoint from a princeton website. Attempting to visit this site gives me a 403 Forbidden error. The address is: https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth. 

Do users need special permission for this?

Thanks"
ROS Dependencies,"in order to run end_to_end_visualize.ipynb  I installed all the packages from conda and tried to run the ipynb file. Earlier it gave me pypcd error. I cloned the github repo of the same. But again got stuck somewhere and got an error 
RuntimeError: invalid hash value (expected ""4bd2f8ea"", got ""f22d7c4219e69774cf0e6a9ebcaeb1a200420ae084036bf122712107be6576bb"")

Please let me know how to resolve these ROS dependency and make the code run just to see how it works."
Problem about evaluation with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object and I trained it, I had problems because my images hadn't the same size as others images used for models pretained, so I changed the train code and it worked. But now I have problems when I try to use eval.ipynb, at the line model.load_state_dict(checkpoint['state_dict']) I have this error :
RuntimeError: Error(s) in loading state_dict for DataParallel:
size mismatch for module.fc1.weight: copying a param of torch.Size([256, 40960]) from checkpoint, where the shape is torch.Size([256, 86528]) in current model.
size mismatch for module.fc_p1.weight: copying a param of torch.Size([15, 256]) from checkpoint, where the shape is torch.Size([3, 256]) in current model.
size mismatch for module.fc_p1.bias: copying a param of torch.Size([15]) from checkpoint, where the shape is torch.Size([3]) in current model.
size mismatch for module.fc_o1.weight: copying a param of torch.Size([20, 256]) from checkpoint, where the shape is torch.Size([4, 256]) in current model.
size mismatch for module.fc_o1.bias: copying a param of torch.Size([20]) from checkpoint, where the shape is torch.Size([4]) in current model.

I don't understand how I could change these sizes.

Thanks in advance.

"
Problem about pose_estimation training with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object to train the pose estimation. When I ran train.py there is the error : 
RuntimeError: invalid argument 2: size '[-1 x 40960]' is invalid for input with 2768896 elements at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THStorage.cpp:80

I went in the debug mode and I found that the problem is that the matrix x in models.py file at line 52 don't have the right size. Normally I should have matrix of shape [32,512,8,10] but I have [32,512,13,13] when I look x.shape. I don't understand how this matrix is created and so where does the problem come from. 

Could you give me any suggestion?

Thanks in advance."
Problem about orientation_error of validation set is extremely high compare to train set.,"Hi!
I used my own dataset which contains four objects to train the pose_interpreter_network and I found the orientation_error of val dosen't decline well when error of train set converged as the chart below.

![ÁÅ´ÁãêÊà™Âõæ_2019-05-07T11-28-27 356Z](https://user-images.githubusercontent.com/38148405/57296727-44d10b00-7100-11e9-9665-a0cd94afaaa1.png)

Consequently, the result of end_to_end_eval is unsatisfactory as well. Despite the positions of object are predicted correctly, most the orientations are wrong.
Could you give me any suggestion?
Thanks in advance."
Error when training pose_estimation about load pcd file.,"Hi, Jimmy. 
I have made my own dataset that contains 4 objects and pcd files. When I run:

> python train.py config/kinect1_mask.yml

There is an error occured:

> Traceback (most recent call last):
  File ""train.py"", line 295, in <module>
    main(cfg)
  File ""train.py"", line 215, in main
    criterion = PointsL1Loss(numpy_pcs).cuda()
  File ""train.py"", line 63, in __init__
    self._pcs = torch.from_numpy(np.array(numpy_pcs)).cuda()
ValueError: could not broadcast input array from shape (4,38720) into shape (4)

There are 38720 points in my first pcd file, and the format is same with your pcd files. It's like:

> VERSION 0.7
FIELDS x y z
SIZE 4 4 4
TYPE F F F
COUNT 1 1 1
WIDTH 38720
HEIGHT 1
VIEWPOINT 0 0 0 1 0 0 0
POINTS 38720
DATA ascii
0 0.00193 0
0 0 0
0.0019499999 0 0
0.0038999999 0 0
0.0058499998 0 0
0.0077999998 0 0
0.0097500002 0 0
...

My config file is like :

> data:
        root: data/kinect1_mask
        pcd_root: objects
        num_subsets: 10
        val_subset_num: 101
        objects: [
            box,
            bottle,
            cola_can,
            power_bank
        ]
        batch_size: 32
        workers: 4
arch:
        num_input_channels: 1
        num_shared_fc_layers: 1
        num_shared_fc_nodes: 256
        num_position_fc_layers: 1
        num_position_fc_nodes: 256
        num_orientation_fc_layers: 1
        num_orientation_fc_nodes: 256
loss: points # l1 | posecnn | points_simple | points
optimizer:
        lr: 0.01
        lr_decay_epochs: [700, 1400]
        momentum: 0.9
        weight_decay: 0.0001
training:
        logs_dir: logs/
        checkpoints_dir: checkpoints/
        experiment_name: kinect1_mask
        print_freq: 10
        checkpoint_epochs: 100
        epochs: 2100
        log_dir:
        resume:

Could you give me any suggestions?
Thanks in advance."
About ‚Äòend to end eval‚Äô error,"I add get_filtered_cat_ids() in the ‚Äòdatasets.py':

> def get_filtered_cat_ids(coco, img_ids):
    object_instances = coco.loadAnns(coco.getAnnIds(imgIds=img_ids))
    def catgory_filter():
        return lambda x: x['category_id'] in [1, 2, 4, 5, 6]
    return [object_instances for object_instances in filter(catgory_filter(), object_instances)]

Then I changed the __init__() in ‚ÄôEvalDataset‚Äòclass:

> def __init__(self, data_root, ann_file, camera_name, object_names, transform):
        self.data_root = data_root
        self.coco = COCO(os.path.join(self.data_root, 'annotations', ann_file))

        img_ids = get_filtered_img_ids(self.coco, camera_name)
        self.object_instances = get_filtered_cat_ids(self.coco, img_ids)

        self.object_names_map = {cat['id']: cat['name'] for cat in self.coco.dataset['categories']}
        #self.object_indices_map = {object_name: i for i, object_name in enumerate(object_names)}
        self.object_indices_map = {'blue_funnel':6,'funnel':4,'oil_bottle':1,'fluid_bottle':2,'engine':5}
        self.object_ids_map = {cat['name']: cat['id'] for cat in self.coco.dataset['categories']}
        #self.object_ids_map = self.object_indices_map

        self.transform = transform

The 'end to end eval' can read the dataset:
> loading annotations into memory...
Done (t=0.72s)
creating index...
index created!
using camera: kinect2

But, I get thsi error:

> RuntimeError                              Traceback (most recent call last)
<ipython-input-11-fed55fe84c4c> in <module>
      3 with torch.no_grad():
      4     for input, target, object_index, object_id in tqdm(val_loader):
----> 5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)
      7         orientation_errors.extend(orientation_error)

<ipython-input-10-ebee32e8a8cf> in forward_batch(model, input, target, object_index, object_id)
      5 
      6     position, orientation = model(input, object_index, object_id)
----> 7     print(target)
      8     position_error = (target[:, :3] - position).pow(2).sum(dim=1).sqrt()
      9     orientation_error = 180.0 / np.pi * pose_utils.batch_rotation_angle(target[:, 3:], orientation)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/tensor.py in __repr__(self)
     55         # characters to replace unicode characters with.
     56         if sys.version_info > (3,):
---> 57             return torch._tensor_str._str(self)
     58         else:
     59             if hasattr(sys.stdout, 'encoding'):

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in _str(self)
    254             suffix += ', dtype=' + str(self.dtype)
    255 
--> 256         formatter = _Formatter(get_summarized_data(self) if summarize else self)
    257         tensor_str = _tensor_str(self, indent, formatter, summarize)
    258 

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in __init__(self, tensor)
     80 
     81         else:
---> 82             copy = torch.empty(tensor.size(), dtype=torch.float64).copy_(tensor).view(tensor.nelement())
     83             copy_list = copy.tolist()
     84             try:

RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorCopy.cpp:70
"
a question about making own datasets ,"Hello, I now mark each image in my dataset with ‚Äùlabelme‚Äú and convert the annotation.json format to coco format, but these files are one by one, how to properly merge a annotation file, just like the annotations file you provide in Oilchangedatasets?
![image](https://user-images.githubusercontent.com/34231078/56178671-a805db00-6035-11e9-95d3-7ed905db812d.png)

"
"Error about ""end to end pose estimator"" use ""kinect2"" in Oilchange datasets ","When testing ‚Äúend to end system‚Äù in Oilchange datasetÔºåI used ‚Äúkinect2‚ÄúÔºåbut  I met  the key error. 

> KeyError                                  Traceback (most recent call last)
<ipython-input-169-fed55fe84c4c> in <module>
      2 orientation_errors = []
      3 with torch.no_grad():
----> 4     for input, target, object_index, object_id in tqdm(val_loader):
      5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py in __iter__(self, *args, **kwargs)
    219     def __iter__(self, *args, **kwargs):
    220         try:
--> 221             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    222                 # return super(tqdm...) will not catch exception
    223                 yield obj

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)
   1020                 """"""), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1021 
-> 1022             for obj in iterable:
   1023                 yield obj
   1024                 # Update and possibly print the progressbar.

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in <listcomp>(.0)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/codedisk/dsl_py/pose-interpreter-networks/datasets.py in __getitem__(self, index)
     50 
     51         object_name = self.object_names_map[ann['category_id']]
---> 52         object_index = self.object_indices_map[object_name]
     53         object_id = self.object_ids_map[object_name]
     54 

KeyError: 'oilfilter'

How to read  annotations in datasets which only  used for pose estimation?"
About trainning pose estimator ,"Hello, I changed the model in pose estimator from  resnet18 to  resnet50, I use the points loss, but when I train the estimator on th mask dataset, the orientation loss does not convergeÔºåcould you give me some suggestionÔºü
![image](https://user-images.githubusercontent.com/34231078/56078641-fb511100-5e1c-11e9-9bcd-8bac773f74c1.png)
![image](https://user-images.githubusercontent.com/34231078/56078642-fe4c0180-5e1c-11e9-9361-346168bec827.png)
![image](https://user-images.githubusercontent.com/34231078/56078643-0015c500-5e1d-11e9-81e7-800a13289c46.png)
"
Error about ros package,"Hello, when I run ""roslaunch pose_interpreter_networks pose_estimator.launch"", I met this error, as follow:

> [ERROR] [1554901200.082931]: bad callback: <bound method Subscriber.callback of <message_filters.Subscriber object at 0x7fb60d16e910>>
Traceback (most recent call last):
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/topics.py"", line 750, in _invoke_callback
    cb(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 75, in callback
    self.signalMessage(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 224, in add
    self.signalMessage(*msgs)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/home/dsl/catkin_ws/src/pose_interpreter_networks/src/pose_estimator.py"", line 117, in callback
    segm, object_names, positions, orientations = self.model(input)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 124, in forward
    return self.gather(outputs, self.output_device)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 136, in gather
    return gather(outputs, output_device, dim=self.dim)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 67, in gather
    return gather_map(outputs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
TypeError: zip argument #1 must support iteration

How to do to solve this problem?
"
How to use the pretrained object pose estimation model for end to end eval.,"Hi, Jimmy.
When I loaded the checkpoint of floating_kinect1_object in the end_to_end_eval.ipynb. There is a error 

> RuntimeError: Given groups=1, weight[64, 3, 7, 7], so expected input[1, 1, 240, 320] to have 3 channels, but got 1 channels instead

So I wander could the object chekpoint be used to end_to_end_eval/visualize.
Thanks you."
Where to download provided val?,"Hello, in poseprocess_wraper.py:
‚Äúparser.set_defaults(process_val=False)  # should download provided val set to match numbers in paper‚Äù
Where to download val set??
I set process_val=True to create val dataset Ôºå but when I train my pose estimatorÔºö
![image](https://user-images.githubusercontent.com/34231078/55678340-6d5fbc80-592a-11e9-99bc-62128ddebdf1.png)
I use kinect2.  I just foget to change ""experiment_name"""
Issue about  training pose estimation models.,"Hi, Jimmy.
When I ran 

> python train.py config/floating_kinect1_mask.yml

There was a error

> File ""/home/huo/virtuallist/py36/lib/python3.6/site-packages/pypcd/pypcd.py"", line 282, in point_cloud_from_fileobj
> if ln.startswith('DATA'):
> TypeError: startswith first arg must be bytes or a tuple of bytes, not str 

came out.

It seems because of loading pcd file incorrectly. I have tried to use the two fomat of pcd file in binary and ascii, but the same error still occurs. And if I modified the code `ln.startswith('DATA')` with `ln.startswith(b'DATA')`, there will be a error 

> File ""/home/huo/virtuallist/py36/lib/python3.6/re.py"", line 172, in match
>     return _compile(pattern, flags).match(string)
> TypeError: cannot use a string pattern on a bytes-like object

come out subsequently.

Could you please give some adcices to solve the peoblems above?
Thank you."
Segmentation training issues: StopIteration,"HI! Thank you for the great job.
I created a json file of my own annotated data by following the coco mask.py.
The part of the json file as below: 

> {...,
> ""image"": [{""license"": 0, ""file_name"": ""0000007149_rgb.png"", ""coco_url"": """", ""height"": 480, ""width"": 640, ""date_captured"": 1544011401.0, ""camera_id"": 0, ""flickr_url"": """", ""id"": 0},...],
> ""annotations"": [{""segmentation"": {""size"": [480, 640], ""counts"": ""QgY37h>f0ZOe0\\Od0\\Od0\\Od0[Of0[Oc0]O`0@3M2N2N3M2N2N2N3L3N2N2N3M2N2N2N2N1O001O0O2O0000001O000000O1000O10000000O10000O100000O0100000000O100000000O1000O10O1000000O100000000O0100000O100000000O1000O10O1000000O1000000O1000O1000O1000000O1000000O10O100000O1000000O10000000O0100000000O1000000O1000O10O100000000O100000O0100000000O10000O100000001N1000001N10001O0O101O000O2O0000001N101hMfFmLn0a0m;]OUDa0U=01N2O1O1O1N101O1O1N2O1O1N2N2MQeW3""}, ""area"": 34310, ""pose"": {""position"": {""x"": -0.0007623627074502259, ""y"": 0.34552469760243687, ""z"": 1.0687215939143497}, ""orientation"": {""x"": 0.7472581634432386, ""y"": -0.14419430357527752, ""z"": 0.10985900520900949, ""w"": 0.6393310871202543}}, ""iscrowd"": 0, ""image_id"": 0, ""bbox"": [225.0, 242.0, 193.0, 219.0], ""category_id"": 1, ""id"": 0},...],
> ""categories"": [{""supercategory"": ""objects"", ""mesh"": ""charge_pile.stl"", ""id"": 1, ""name"": ""charge_pile""}]}

Then I modified the config file(drn_d_22_ChargePile.yml) and utils.py related to the object classes in ""segmentation"" folder.
But when I run

>  python train.py config/drn_d_22_ChargePile.yml

There is a error ""StopIteration"" came out at:
`first_input_batch, first_target_batch = iter(val_loader).next()` in the train.py:217
and RuntimeWarning: invalid value encountered in true_divide at:
`return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))` in the utils.py:25
It seems my dataset wasn't loaded correctly.
Can you give me any suggestions about how to solve these problemÔºüThanks."
How to Create a RGB segmentation dataset for your environment,"How, I am a tiroÔºåI don‚Äòt konw how to create a RGB segmentation datase for translate learning. I just use labelme to buile .json file for  a single RGB image, just like this  
![image](https://user-images.githubusercontent.com/34231078/54470325-cd0df080-47e0-11e9-9c0b-be1a38439b1d.png)
However, It didn't like your annotation file

![image](https://user-images.githubusercontent.com/34231078/54470333-e747ce80-47e0-11e9-8502-fcfb4adc1b4a.png)
"
blender in pose estimation,"Hello, when I run the demo, visualization, and evaluation code in ""pose estimation"", the program has been rendering in the background of the blender, unable to proceed to the next step, and there has been no result. I encountered this problem for the first time and put the ‚Äúmesh‚Äù folder in the Oil Change dataset. The permissions of the .stl file in the file are changed to executable files, which can run normally. 
However, after a few days, I met the same problem that the program has been running can not produce results, I don't know how to solve this time."
Problem about training on LineMod dataset,"I want to train your network on linemod dataset, however, I have question writing the config file such as 
20171103_OilChange.json. I cannot understand the messages in the json file like the following...

{""segmentation"":{""counts"":""mYVe08]Q14M4L3L4N1N3M3L3N3N1M4N2L3N3mLZOPUOh0jj0ElTO?Qk0HhTO;Tk0LfTO7Wk02aTO0\\k08\\TOKbk0?STODjk0j0hSOYOUl0W3O1N2O00001O00O100O1O1O1O1O1O1N2O1O1O1O1O1O100O1O1O1O1O1O100O1O1O1O1O1O1O1O1O1O1O1O1O1000000O10000000000O100000000000000001O000000001O000000000000001O0000000000000000O10000O100O1O1O1N2O1O1O1O1O1N2O1O1N2N2O1O1O1O1O1N2O1O1N2O1O1O1N2O1O1O1N2O1O1O1O1O1N2N2O1O1N200O100O100O100O100O100O1O1N3O2J6D<E`0Ca0\\Oc0BQ[\\T1"",""size"":[1080,1920]},""area"":21399,""pose"":{""position"":{""y"":-0.3352892126408012,""x"":-0.1564572835692801,""z"":0.7772331158642385},""orientation"":{""y"":0.9502802730465292,""x"":-0.303411278174998,""z"":-0.01268932429862474,""w"":0.06890558746337358}},""iscrowd"":0,""image_id"":0,""bbox"":[643.0,0.0,173.0,179.0],""category_id"":6,""id"":0},

Could you please explain how did you create these .json files and what should I do to my own dataset such as LineMod to get these segmentation ground truth?
Thank you !!!


"
A bug,"File ""train.py"", line 52, in train
    output = model(input)
RuntimeError: CuDNN error: CUDNN_STATUS_MAPPING_ERROR
I run the code and It seems a bug here."
A BUG,"I run the code and there's a  ""CUDNN_STATUS_MAPPING_ERROR"" 
File ""train.py"", line 52, in train
    output = model(input)
it seems from here.

"
PermissionError: Permission denied: '/usr/local/bin/blender',"When I used anaconda3's python to run end_to_end_visualize.ipynb, this problem occurred. I copied the system's installed Blender library file to my anaconda3/share/ folder, but it didn't work. I don't know how to solve this problem.

The error message is as followsÔºö
---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
<ipython-input-11-a2f62c5336e7> in <module>()
     21         all_objects = np.zeros_like(resized_image)
     22         for i, object_name in enumerate(object_names):
---> 23             single_object = pose_renderers[object_names[i]].render(positions[i], orientations[i])
     24             all_objects = np.maximum(all_objects, single_object * object_colors[object_names[i]])
     25         rendered_pose = (1 - alpha) * resized_image + alpha * all_objects

~/pose-interpreter-networks/pose_estimation/utils.py in render(self, position, orientation)
     89                                    str(self.camera_parameters['p_x']), str(self.camera_parameters['p_y']),
     90                                    str(self.camera_scale),
---> 91                                    position, orientation])
     92             assert ret == 0
     93             image = np.asarray(Image.open(output_path))

~/anaconda3/lib/python3.6/subprocess.py in call(timeout, *popenargs, **kwargs)
    265     retcode = call([""ls"", ""-l""])
    266     """"""
--> 267     with Popen(*popenargs, **kwargs) as p:
    268         try:
    269             return p.wait(timeout=timeout)

~/anaconda3/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    707                                 c2pread, c2pwrite,
    708                                 errread, errwrite,
--> 709                                 restore_signals, start_new_session)
    710         except:
    711             # Cleanup if the child failed starting.

~/anaconda3/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)
   1342                         if errno_num == errno.ENOENT:
   1343                             err_msg += ': ' + repr(err_filename)
-> 1344                     raise child_exception_type(errno_num, err_msg, err_filename)
   1345                 raise child_exception_type(err_msg)
   1346 

PermissionError: [Errno 13] Permission denied: '/usr/local/bin/blender'"
Support for kernel mean embeddings and kernels on distributions,"My work involves embedding distributions in RKHS using [kernel mean embeddings](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions) or equivalently kernels on distributions [Ch. 2, especially the generalized RBF kernel](http://reports-archive.adm.cs.cmu.edu/anon/2016/CMU-CS-16-128.pdf).

Are there any plans of extending this library to this case (or other structured domains for that matter)?

In the near future I'll be implement algorithms that uses this and if this sounds interesting I'd be happy to clean up and contribute the resulting code!

Thanks for the hard work :)"
Using GPU to accelerate the eigen decomposition in the psuedo-inverse calculation in nystrom.jl,"Here is my implementation:
```julia
using CuArrays, CUDAnative, LinearAlgebra, MLKernel

function make_symmetric(A::Mat, uplo::Char='U') where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    return LinearAlgebra.copytri!(A |> Matrix{T}, uplo)
end

function nystrom_inv_gpu!(A::Mat) where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    A = cu(A)
    vals, vectors = CuArrays.CUSOLVER.syevd!('V', 'U', A)
    tol = eps(T)*size(A,1)
    max_eig = maximum(vals)
    # for i in eachindex(vals)
    #     vals[i] = abs(vals[i]) <= max_eig * tol ? zero(T) : one(T) / sqrt(vals[i])
    # end
    predicate = one(T) .* (vals .>= max_eig * tol)
    vals .= predicate .* CUDAnative.rsqrt.(vals .^ predicate)
    QD = CuArrays.CUBLAS.dgmm!('R', vectors, vals, vectors)
    W = CuArrays.CUBLAS.syrk('U', 'N', QD)
    return make_symmetric(W)
end
```"
nystrom factorization for sparse matrix,"``` julia
fac = nystrom(k, X)
```
```
ERROR: MethodError: no method matching nystrom(::PolynomialKernel{Float64,Int64}, ::SparseMatrixCSC{Float64,Int64})
Closest candidates are:
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}) where T<:Union{Float32, Float64} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}, ::Array{U<:Integer,1}) where {T<:Union{Float32, Float64}, U<:Integer} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
```"
add constructor for RBFKernel,"Hi, folks

I'm just wondering if it make more sense to use the convention in wiki ([here](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)) instead of just define it as an alias

I found it a little bit confusing (not very intuitive) while searching wiki, if this name does refer to RBF kernel, since it is completely the same to `SquaredExponentialKernel`..."
Add missing negative signs in the documentation,In the documentation of the exponential kernels some negative signs are missing.
ForwardDiff.jl Compatibility,"[ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) has the following constraints:

  - [ ] The target function can only be composed of generic Julia functions. ForwardDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of Ax_mul_Bx-style functions.
  - [ ] The target function must be written generically enough to accept numbers of type T<:Real as input (or arrays of these numbers). The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well."
Implement Automatic Relevance Determination,"Implement Automatic Relevance Determination as described in Rasmussen (page 2 of pdf, 106 of text):

> http://www.gaussianprocess.org/gpml/chapters/RW5.pdf"
ARD Implementation and Automatic Differentation compatibility,"Hello,

I made relatively heavy changes to your package with the objective to be able to have Automatic Relevance Determination and to be compatible with Automatic Differentation packages such as ForwardDiff.jl.

I changed the way distances were computed by directly applying the scale factor (appropriately) to the vectors. It might make the algorithm slower, I have not made benchmarks so far.
However it is still retrocompatible and using a scalar scaling work with everything.

I did not add tests yet for the ARD case, and I had to remove some cause of the needed transformation of the parameters (in the ""Testing constructors"" part) cause I did not know if the best option is to keep a copy of the original parameters or ignore this issue. Let me know if you would like other modifications.

Cheers, "
Remove hyperparameters,Addresses #65 
Remove integer parameter from PolynomialKernel,"Having a second parameter complicates conversions and adds little value since parameter `d` is typically small (usually 3). Therefore, the practical set of values that would be used can be represented by a floating point number."
Remove LinearKernel,The LinearKernel is used infrequently (never) and can be modelled with the PolynomialKernel with d=1.
Remove PeriodicKernel,"This is a construction of the form k(f(x), f(y)) which can be left to data preprocessing."
Rename GammaRationalKernel to GammaRationalQuadraticKernel,Keep naming consistent with Rasmussen's Gaussian Processes book where the extension of the Exponential Kernel is referred to as the Gamma Exponential Kernel
Error tagging new release,"The tag name ""0.4.0"" is not of the appropriate SemVer form (vX.Y.Z).
cc: @trthatcher"
Dev,
Wrong metric for Matern kernel,"I am only half sure about what I advanced but I think the metric used for the Matern Kernel is not correct. Currently it is using a Squared Euclidean (hence ||x-x'||^2).
But in the documentation (and in the definition of the Matern kernel according to Rasmussen and Williams) the distance used is the Euclidean distance (||x-x'||), also in the documentation the equation of Matern kernel should have the 2's in the square roots. 
"
Issue with new release,"I tried to `add` the package with Julia 1.0.1 but get the following error :
```julia
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ‚îú‚îÄpossible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ‚îú‚îÄrestricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 ‚îî‚îÄrestricted by julia compatibility requirements to versions: uninstalled ‚Äî no versions left
```

I think it has to do with the tag version in Project.toml compared to the actual github release tag"
Dev,
Dev 2,
Dev,Refactored code to support Documenter.jl
Update dependencies for 1.0 compatibility.,"Installing MLKernels on Julia 1.0 currently fails due to a dependency issue.
```
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ‚îú‚îÄpossible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ‚îú‚îÄrestricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 ‚îî‚îÄrestricted by julia compatibility requirements to versions: uninstalled ‚Äî no versions left
```

With the newest release of SpecialFunctions.jl (https://github.com/JuliaLang/METADATA.jl/pull/19167) we can now update the dependencies of MLKernels.jl. This enables full compatibility for Julia 1.0 (CI passes).

Please trigger a new release on METADATA.jl after merging this to allow users to finally use this great package with Julia 1.0.

A test coverage drop may occur, see the bug and discussion here: https://github.com/JuliaLang/julia/issues/28192."
Update requirements for doc build,
Move HyperParameters module into deprecated file. Add deprecation war‚Ä¶,‚Ä¶nings.
Dev,
Remove HyperParameter?,"Hej!

I like this package, and in principle I think it could be a nice building block for different GP packages such as https://github.com/PieterjanRobbe/GaussianRandomFields.jl or https://github.com/STOR-i/GaussianProcesses.jl. But I'm not sure whether the use of `HyperParameter`s is actually needed; to me it seems it adds an additional layer of complexity that might not always be desired. My suggestion would be to follow the same approach as https://github.com/JuliaStats/Distributions.jl: restrict parameters `<:Real` of immutable kernel functions to certain intervals such as positive real line with the help of inner constructors. This would provide users with a very simple and completely flexible way of updating parameters by just creating new kernel functions with updated parameter values. For increased convenience one could even think about providing default transformations from https://github.com/tpapp/ContinuousTransformations.jl that would provide an easy way to map real values to required intervals and transforming likelihoods etc in a correct way."
Add Julia 1.0 to CI.,"Add testing for Julia 1.0.
Add windows testing."
more Julia 0.7 compat,Test has to be a dependency now
increase version to 0.4.0,was 0.3.9999
Julia 07 compat,
0.7 compat,Is there any work for 0.7 compatibility on the way?
Kernel combination implementation and derivatives,"Hello,
It would be amazing for the package to feature simple kernel combination such as kernel sum and product.
I wrote my own kernel function module because I needed it imperatively but yours is a lot more robust, maybe I could implement it given some directions.
Thanks!"
Kernels for HAC estimators?,"I am unsure if this package is the right place to implement these, but wanted to ask. For robust variance covariance estimators sometimes kernels are implemented to account for spatial or temporal correlation of an assumed or estimated form (Heteroscedastic and Autocorrelation Consistent / HAC variance covariance estimators). Would this package be a good place to have these kernels implemented? Here is a reference of the R implementation of these [Sandwich](https://www.jstatsoft.org/article/view/v011i10/v11i10.pdf) and here is another implementation in Julia [CovarianceMatrices.jl](https://github.com/gragusa/CovarianceMatrices.jl)."
Dev,
WIP: Fixes for Julia v0.6,"- [x] Fix deprecation warnings
- [ ] Fix kernel macros"
Using Distances.jl and automatic differentiation,"I just made a PR JuliaStats/Distances.jl#72 to make distances compatible with automatic differentiation. By using Distances.jl we could get

- ARD kernels using the weighted metrics
- easy higher order derivatives with automatic differentiation

Using a more general `Metric` type in the kernel computations could also enable kernels to be computed over other metric spaces than R^n (e.g. graphs).

Relevant issues:
#46 #53 #52 "
threading support,"I am doing some tests with the `@threads` macro, in case you are interested:

https://github.com/gdkrmr/MLKernels.jl/tree/threads

For the Gaussian kernel I get around 60% speedup for two threads."
DomainError in tests,"in `v0.2.0`
```
INFO: Testing MLKernels.kernelmatrix!
ERROR: LoadError: LoadError: DomainError:
 in exponentialkernel at /home/me/.julia/v0.5/MLKernels/src/kernel.jl:110 [inlined]
```
`z` can be smaller than zero, I think this is due to calculating the squared euclidean distance as x^2 - 2xy + y^2"
pairwise_aggregate inconsistencies,"sometimes you use `f::Type` and sometimes only `::Type` in `pairwise_aggregate`:

https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L19
https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L27"
julia 0.6 compatibility,What are your plans on supporting Julia 0.6?
tag a new version,"I am developing a package that depends on MLKernels, but the latest tagged version is 2 years old, which makes it hard to specify dependencies.

If you are interested: https://github.com/gdkrmr/KernelRidgeRegression.jl"
More detailed categorization of kernels,"I think it would be useful to have a more detailed categorization of the kernels. In chapter 4 of [this book](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) kernels (covariance functions) are categorized by the types of their inputs. This would enable specifying the pairwise functions in the type hierarchy instead of specifying them for each kernel separately.

For example:
- stationary kernels satisfy `K(x, y) = k(x-y)`
- isotropic kernels satisfy `K(x, y) = k(|x-y|)`"
Random Fourier Features,"Random Fourier features is a technique for approximating inner products in the RKHS using a randomized feature map. It would be great to have this in MLKernels.

Here's a paper introducing them:
[Random Features for Large-Scale Kernel Machines](http://pages.cs.wisc.edu/~brecht/papers/07.rah.rec.nips.pdf)

A blog post demonstrating their use:
[Random Fourier Features for Kernel Density Estimation](https://mlstat.wordpress.com/2010/10/04/random-fourier-features-for-kernel-density-estimation/)

The kernel being approximated needs to have some specific properties. Mainly it needs to be stationary (shift-invariant) and scaled properly.

I need to look into this a bit more, but it seems like it might not be too difficult to implement. The main part being a function `spectraldensity`, which takes a kernel as an argument and returns a distribution to sample from."
LoadError with 0.5,"I get the following syntax error and deprecation warnings with version 0.5.1 of Julia.

```julia
julia> Pkg.free(""MLKernels"")
INFO: Freeing MLKernels
INFO: No packages to install, update or remove

julia> using MLKernels
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:14
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:17
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
ERROR: LoadError: LoadError: syntax: space before ""("" not allowed in ""? (""
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/pairwise.jl, in expression starting on line 45
while loading .../.julia/v0.5/MLKernels/src/MLKernels.jl, in expression starting on line 57
```
These do not appear in the `dev` branch or when using version 0.4.5 of Julia."
Kernel Derivatives,"There's two components to this enhancement.

### Optimization
Define a `theta` and `eta` (inverse `theta`) function to transform parameters between an open bounded interval to a closed bounded interval (or eliminate the bounds entirely) for use in optimization methods. This is similar to how link functions work in logistic regression - unconstrained optimization is used to set a parameter value in the interval (0,1) using the logit link function.
- [x] `theta` - given an interval and a value, applies a transformation that eliminates finite open bounds
- [x] `eta` - given an interval and a value, reverses the value back to the original parameter space
- [x] `gettheta` returns the theta transformed variable when applied to `HyperParameters` and a vector of theta transformed variables when used on a  `Kernel`
- [x] `settheta!` this function is used to update `HyperParameter`s or `Kernel`s given a vector of theta-transformed variables
- [x] `checktheta` used to check if the provided vector (or scalar if working with a HyperParameter) is a valid update
- [x] `upperboundtheta` returns the theta-transformed upper bound. For example, in the case that a parameter is restricted to (0,1], the transformed upper bound will be log(1)
- [x] `lowerboundtheta` returns the theta-transformed lower bound. For example, in the case that a parameter is restricted to (0,1], the transformed lower bound will be -Infinity

### Derivatives
Derivatives will be with respect to `theta` as described above.
- [ ] `gradeta` derivative of `eta` function. Using chain rule, this is applied to `gradkappa` to get the derivative with respect to theta. Not exported.
- [ ] `gradkappa` derivative of the scalar part of a `Kernel`. This must be defined for each kernel. It will be manual, so the derivative will be analytical or a hand coded numerical derivative. It will only be defined for parameters of the kernel. Not exported. Ex. `dkappa(k, Val{:alpha}, z)`
- [ ] `gradkernel` derivative of `kernel`. Second argument will be the variable the derivative is with respect to. A value type with the field name as a parameter will be used. Ex. `dkernel(k, Val{:alpha}, x, y)`
- [ ] `gradkernelmatrix` derivative matrix."
Equality test for Kernels,"```julia
julia> GaussianKernel(1.0) == GaussianKernel(1.0)
false
```

my best guess:

```julia
julia> isimmutable(GaussianKernel(1.0))
true

julia> isimmutable(GaussianKernel(1.0).alpha)
false
```
confuses julia"
performance,"The following code shaves off more than 0.5 seconds when X has `20_000` columns and `10` rows:

```julia
n = 20000
x = randn(10, n)

function gauss{T}(X::Matrix{T}, alpha::T)
    n = size(X, 2)
    xy = LinAlg.BLAS.syrk('U', 'T', one(T), X)
    x2 = [ sum(X[:, i] .^ 2) for i in 1:n ]
    
    LinAlg.BLAS.syr2k!('U', 'N', one(T), x2, ones(T, n), T(-2), xy)
    
    @inbounds for i in 1:n
        for j in 1:i
            xy[j, i] = exp(-alpha * xy[j, i])
        end
    end
    LinAlg.copytri!(xy, 'U')
end
gauss(x, 1.0)
```

compared to

```julia
kernelmatrix!(
    ColumnMajor(),
    Matrix{Float64}(n, n), 
    GaussianKernel(1.0), 
    x, true
)
```
I think one of the reasons is that `LinAlg.syrk_wrapper!` always copies the triangle in the matrix, even though you try to avoid that. I am not sure how much using `syr2k` saves.
"
MLKernels with julia v0.5,"`MLKernels` is nice, but there are a few issues when using with julia 0.5.
Here is what I've found:
- In `kernelapproximation.jl`: replace `Base.blasfunc` with `Base.LinAlg.BLAS.@basefunc` and `chksquare` with `checksquare`
- In `meta.jl`: replace `super` with `supertype`
- In`pairwise.jl`: replace text like `i = store_upper ? (1:j) : (j:n)` with `i = (store_upper ? (1:j) : (j:n))`
- In `kernelfunctions.jl`: in the first 4 `call` statements, there is the issue of adding methods to an abstract type in Julia 0.5 e.g. sisl/BayesNets.jl#28 . What functionality in `MLKernels` would be lost if these statements were excluded? 
"
kernelmatrix function,"<img width=""1087"" alt=""screenshot 2016-06-30 01 26 14"" src=""https://cloud.githubusercontent.com/assets/1314675/16481762/c665400c-3e61-11e6-8ea9-cc8db4caaba7.png"">

I also get this error with `kernel` function sometimes.
"
kernel function,"I just started using your latest version of MLKernels.  I'm having an issue which may just be a syntax problem on my end.  I'm attempting to use the kernel function like I have in the past but here's the result:

``` julia
julia> kernel(GaussianKernel(), 2., 4.)
```

```
ERROR: type KernelComposition has no field k
 in kernel at /Users/joshualeond/.julia/v0.4/MLKernels/src/kernelfunction.jl:14
```

I usually use the kernel to compare arrays and have the same result except the error is at line 16.  The version that isn't working for me is v0.1.0+ which I cloned from Github and the previous version I was using was v0.1.0 installed from Metadata. 
"
Hyperparameters,"The parameters for Kernels should be abstracted to a `HyperParameter` type that can be used to aid in optimization and ensure consistency of constraints.
"
Update to 0.4 compatibility,"This PR makes the full move to work in 0.4+
"
Positive Definite vs Mercer,"I see that you treat positive definite kernels as being synonymous to mercer kernels, while as far as I understand there is a slight difference in that a mercer is more restrictive (e.g. continuity).

Was this an oversight, or a conscious design decision (for simplification maybe)?

source: [here at 36:40](http://videolectures.net/mlss09uk_schoelkopf_km/)
"
Use of triangular dispatch invalid,"This definition for ARD seems to invalid in Julia 0.4, and I think should be invalid in 0.3 too but might not be due to a change.

https://github.com/trthatcher/MLKernels.jl/blob/c1532ce1d3b43806664296b82b4a0bd0b281190c/src/kernels.jl#L62

```
immutable ARD{T<:FloatingPoint,K<:StandardKernel{T}} <: SimpleKernel{T}
```

In particular the `K` part. I can't find exactly where it was changed, but here is related discussion

https://github.com/JuliaLang/julia/issues/6620
https://github.com/JuliaLang/julia/issues/3766
https://github.com/JuliaLang/julia/issues/8974#issuecomment-62552208

I'm a bit confused on how this all works internally, might be worth posting on julia-users if you want to get it working on 0.4 at some point.
"
Kernel/Kernel Matrix Computation,"@st-- 

So I was thinking a bit about the approach to the kernel/kernel matrix computation.

If you have a function f:RxR -> R, then you can apply it element-wise to two vectors and sum the results. So for x = [x1,x2] and y = [y1,y2] the dot product the function would be f(x,y) = xy. Then k(x,y) = f(x1,y1) + f(x2,y2) = x1y1 + x2y2. The squared distance is just f(x,y) = (x-y)^2

If f is a valid positive or negative definite kernel in RxR, then we have a way to extend the kernel to R^n x R^2 by summing the element-wise results.

I was thinking we could take a more modular approach. Technically, the kernels we have now are a composition of a positive definite kernel (the polynomial kernel takes the dot product) or a negative definite positive-valued kernel (the euclidean distance in what we have implemented) and we could define these 'input' kernels in terms of `f`.

I was thinking we could abstract `kernelmatrix` into two functions: generic `kernelize` (the way it currently operates) and a generic `pairwise` which operates on two vectors/matrices. We would create a new class of kernels and `pairwise` would dispatch on those types. Examples of those new base kernels would be `SquaredDistance` and `ScalarProduct`. However, we can easily extend it:
- `f(x,y) = sin(x-y)^2` (sine squared kernel - also negative definite)
- `f(x,y) = (x-y)^2/(x+y)` (chi squared kernel for R+ x R+)

These can all be extended naturally to cover the periodic kernel weights... I was thinking two weight vectors and `f` could be defined like this:

k(x,y) = u1_sin(w1_x1 - w1_y1)^2 + u2_sin(w2_x2 - w2_y2)^2

Where u and w are your weight vectors. (I know this could add redundancy - just an idea)

Long story short, three levels:
- 'Base' simple kernels that will be defined for `pairwise` - ARD would be defined at this level
- Derived kernels that are a function of the matrix that pairwise returns
- Composite kernels as we have them now.

Anyway, I think an approach along these lines would also give us a nice modular approach to the derivatives. We can always ensure there's generic fall-back methods basically exactly how we have them now. Let me know your thoughts
"
Consistency in Exceptions: ArgumentError vs DimensionMismatch,"`kernelmatrix!` for StandardKernel throws ArgumentError, `kernelmatrix!` for SquaredDistanceKernel or ScalarProductKernel throws DimensionMismatch (by `scprodmatrix!`/`sqdistmatrix!`). `matrix_prod!` and `matrix_sum!` for generic Arrays call error(), but for matrices with is_upper argument the former throws DimensionMismatch and the latter throws ArgumentError. I think this should all be made a bit more consistent.

Is there a specific reason to use error() rather than throw(<some exception>) in some cases ?

(NB. there should be `@test_throw`s as well, which I'm about to add.)
"
Merge Development into Master,
Kernel derivatives need to be a separate package,"@st-- 

I'm trying to be pragmatic when it comes to this package. I originally envisioned that this package would provide the following:
- A vetted set of mainstream machine learning kernels (with some simple combination rules)
- The ability to compute a kernel matrix quickly
- The ability to compute a kernel matrix approximation

Unfortunately with the kernel derivatives, I feel the package moves too far away from that. My concerns are that: 
- Derivatives may not be defined and vector parameters add a layer of complexity
- Many derivatives that do exist are intractable - reliance on analytic derivatives is complex/unfeasible in many cases and raises floating point accuracy in others
- Derivatives appear to be catering to a very specific need - it's a specialized component being added to a generic package. 

As such, it's bespoke code and necessitates its own package. 

That being said, I appreciate all the help and I'm more than willing to help you out where ever I can.
"
MultiQuadraticKernel missing,"Maybe that's intentional, but I just noticed that the MultiQuadraticKernel in master doesn't have any equivalent in developments. The InverseMultiQuadraticKernel maps to RationalQuadraticKernel with beta=0.5, but the MultiQuadraticKernel would be beta=-0.5 and you only allow positive values for beta...
"
@inbounds,"Various functions, particularly in vectorfunctions.jl and kernelderiv.jl, use `@inbounds` without making sure all the arguments have the right length etc. Is this fine, because we know how we call them, and we do check array dimensions etc. in the parent routines? Or should we add additional bounds checks in there?
"
ARD parameter (weights) derivative,"Just realised that `kernelmatrix_dp` was missing, and added it in ab3c09258fdcc322f0748ac9467f0b8ac86f518c -- piggy-backing on `kernelmatrix` seemed easiest. The only issue with this is that, currently, `kernel_dp(k::ARD, :weights, x, y)` returns an array with the length of `k.weights`. From a computational POV it makes sense to calculate all dk/dw[i]s at once (and usually you'd want all derivatives, not just one of them, I think), but from an interface/API POV it would be better if `kernel_dp` always returned a scalar... how to reconcile that ?
"
Segfaults,"I was going to add `@test_throw` tests for the various ArgumentErrors e.g. in kernelmatrix, and to figure out which way around things should go I played around in the REPL with things like `MLKernels.kernelmatrix!(zeros(4,4), ExponentialKernel(), ones(4,6), true)`, transposed dimensions for X, different values for is_trans, leaving out is_trans entirely - which sometimes led to instant segfaults, sometimes with a bit of delay. Don't have the time anymore to investigate, but I suspect it's something where we used `@inbounds` but didn't have enough checks to make sure we weren't going to exceed the array bounds!
"
"Limits on parameter ranges: PolynomialKernel,","PolynomialKernel required d to be integer. ExponentialKernel, RationalQuadraticKernel, PowerKernel, LogKernel all require gamma to be <= 1. What's the reason for that ? (Maybe a kernel would not be Mercer if gamma > 1 (or d real, for PolynomialKernel), but it might still be useful. We could just amend ismercer() so that it actually checks the parameter value?)
"
Approach to derivatives,"Types can be parameterized by symbols, which means we can take a different approach to derivatives. I've included a [gist](https://gist.github.com/trthatcher/acd86f5a9b911737d419) to illustrate how we could take advantage of this (""Approach 2"") 

It would cut down on the number of functions and it seems to perform better on my machine (and definitely no worse).
"
Definity of kernels,"I had assumed the Sigmoid kernel was c.p.d., but according to the [paper I just read](http://vip.uwaterloo.ca/files/publications/Carrington%20et%20al%20-%20A%20New%20Mercer%20Sigmoid%20Kernel.pdf) about the Mercer Sigmoid kernel apparently that's only true for some parameter values, depending on the data set... so I suppose we should have iscondposdef(::SigmoidKernel) = false?

Also, I think we might've been working with `isposdef(::Kernel)` meaning ""positive semi-definite"", but Base.isposdef is ""strictly positive definite"" (e.g. isposdef(0) = false...), and if we extend a Base function we should follow that. For kernels it doesn't seem to make much difference whether it's positive semi-definite or strictly positive definite, so maybe move to our own function? We could have ismercer()... as positive semi-definity is sufficient and necessary for a kernel being a Mercer kernel.
"
Remove Separable Kernel (Mercer Sigmoid Kernel),"The separable kernel is the dot product kernel applied to an element-wise transformation of the original data vectors. The element-wise transformation is 100% equivalent to pre-processing one's data.

I don't think it's adding any value - any opposition to removal of this type/kernel?
"
Nystrom kernel approximation,"I believe the Nystrom kernel approximation should satisfy the following invariant:
`nystrom(kernel, X, [1:size(X,1)]) == kernelmatrix(kernel, X)` (up to floating point inaccuracies)
The current implementation is definitely broken.
If I use just Base routines, as follows, I get what I believe is the correct approximation:

```
function basenystrom(kernel::Kernel, X::Matrix, xs::Vector)
    C = kernelmatrix(kernel, X, X[xs,:])
    D = C[xs,:]
    SVD = svdfact(D)
    DVC = diagm(1./sqrt(SVD[:S])) * SVD[:Vt] * C'
    MLKernels.syml(BLAS.syrk('U', 'T', 1, DVC))
end
```

But I don't understand your optimised version well enough to figure out where it's going wrong...

I've added this as test/test_approx.jl; have a look and have fun fiddling with your implementation until that test passes. ;-) It's not yet added to runtests.jl so it doesn't break the overall test.
"
Edge cases,"LogKernel is not pos.def. anyway; constructor says gamma has to be in (0,1] but only checks for gamma>0. Is there a reason for gamma<=1? If not, error message should be adjusted, otherwise code & test...
"
GammaRationalQuadraticKernel parameters,"So GammaRationalQuadraticKernel(alpha,beta,gamma) is a generalised form of the RationalQuadraticKernel(alpha,beta) is a generalised form of the InverseQuadraticKernel(alpha).
InverseQuadraticKernel(alpha) = RationalQuadraticKernel(alpha,beta=1), and
RationalQuadraticKernel(alpha,beta) = GammaRationalQuadraticKernel(alpha,beta,gamma=1).
But the default value of GammaRationalQuadraticKernel is gamma=0.5. Just checking if that is intentional?
"
Why split up test_standardkernels?,"The kernels are all very similar in their tests - wouldn't splitting it up risk missing some tests for some of them / testing them vaguely differently? It's a lot of code duplication for which I can't see the gain... maybe you've got a good reason though ? Mainly curious.
"
Periodic Kernel does not appear to be a Squared Distance kernel,"I was trying to find some documentation on the periodic kernel. In all the papers I look at, it seems like the sin() function is applied to the element-wise distances and squared rather than taking the squared sine of the distance.

sum sin(xi - yi)^2 for all i

rather than sin(||x-y||)^2

Here's a paper for example:

http://jmlr.org/proceedings/papers/v33/hajighassemi14.pdf
"
Release,"@st-- 

I would _really_ like to finish up the non-derivative portion of the package ASAP (next 10 days or so) so that I may use it in my other projects and so that I can release a version of this package to http://pkg.julialang.org/. Originally, this would have happened last week, but the derivatives and ARD obviously added to that time. That's okay (:

I'm going through all the standard kernel definitions and correcting/refining all definitions to ensure they are clean and correct. I'm also going to revisit the standard kernel tests (broken on my last commit - don't worry, will fix up). The composite kernels will need to be sorted for the recursive definition.

Regarding the kernel derivatives, there's a couple options since they probably won't be ready. First, exclude them from the first release and just keep them in a development branch. Alternatively, they could be included in the package code, but not extracted/documented. You would need to explicitly import them. Lastly, they could be broken out into their own package with this one as a dependency (you would be the owner of the deriv package - but I could still co-maintain).

How would you like to approach it?
"
sqdist_d*() and scprod_d*(),"It seems like each version is used in only one place (the derivative for the corresponding class of kernel including ARD). Making use of these functions uses at least two - currently three - scans of the array that the kernel function returns.

Are these functions providing any real value? It seems like there's two definitions with multiple array scans when only a single definition and one scan is required.
"
Only integer exponents?,"Does the exponent e.g. for PowerKernel need to be integer? Can it not interpolate with real exponents?
"
Re-implementing Base functions,"In one of your recent commits you made use of (Base.)scale!() which I hadn't come across before, but it looks like it's basically the same functionality as gdmm!/dgmm! in matrixfunctions.jl - the only difference is that the Base version doesn't use `@inbounds`... does that macro make things so much faster that it's worth keeping an in-package version of matrix-vector scaling ?
"
N vs. T,"How useful is it to basically duplicate have the code to allow both row-wise and column-wise data matrices ? From an implementation point of view it'd be a lot simpler to just decide on one, and require users to call e.g. kernelmatrix(k, X', Y') when needed, which is a one-off overhead.

The only difference is whether the access is X[i,:] and Y[j,:] or X[:,i] and Y[:,j]... so I played around with macros and came up with one which takes a condition, a tuple of symbols, and a code block and transforms all array references to objects listed in the tuple of symbols (84ca8bffcc825aaea48708b4d8fc9e9c6984c446, 1564f8a3c4a68ac97bcd5eeda4464a6120fa4e7b). But that doesn't work so well for the cases where you have e.g. N_sqdist and T_sqdist...
"
scprod and sqdist including weighted versions and derivatives and tests,"Sorry, I had added this as well but hadn't put it in there. Merging now...
"
"kernelmatrix(k, X, Y) calculates full matrix, not only upper-right triangle as commented","Now the question is, are there going to be any kernels which are non-symmetric, for which k(x, y) != k(y, x)? If not, then kernelmatrix(k, X, Y) should only calculate half the matrix, as already the case for kernelmatrix(k, X) [which calculates kernelmatrix(k, X, X)].

On second thought, why is there a special case for kernelmatrix(k, X, X)? Should be the same code as for kernelmatrix(k, X, Y)...
"
Roadmap,"@st-- 

I think we should establish a roadmap. For the most part, I already have what I need from this package. All I need to do is expand/finish the approximations.

We have a good foundation for the kernel derivatives defined for the Gaussian kernel. I can expand it to include all the other kernels, it's just a matter of sinking some time into it. 

What are you priorities in terms of the features? How do you want to approach it?
"
Change norm2 -> sqdist & add derivatives for ScaledKernel,
positive definiteness of composite kernels,"Are you sure that a kernel sum of one positive definite and one NOT positive definite kernel is still positive definite ? That's what the code says: `isposdef(œà::KernelSum) = isposdef(œà.k1) | isposdef(œà.k2)`, but I really wouldn't've thought so..
"
Kernel function discussion/kernels with variable-dimensional parameters (ARD),"I just pushed an implementation of integer-based parameter derivatives based on calling names() on the kernel object to get its field. After all that I remembered an important use case in Gaussian Processes: the ""Automatic Relevance Determination"" (ARD) kernel. This is basically a Gaussian kernel, but with a sigma _vector_ (same length as data dimension):

Instead of the 1D Gaussian, `exp(- vecnorm(x - y)^2 / 2k.sigma^2)`, you would use `exp(- vecnorm((x - y) ./ k.sigmas)^2 / 2)`, where k.sigmas is a vector of the same dimensions as x and y.

This crops up not just for euclidean distance kernels, but also scalar product kernels - e.g. a LinearKernel with different scaling for each dimension...

Any idea on how to best implement that ?
"
Memory usage / in-place covariance matrix calculations,"Reading up on Coverage.jl I was curious to check the memory allocation behaviour of the code. Most of it is constructing Kernels, which I think is fine. But it occurred to me that e.g. the second derivatives construct n x n matrices, and memory allocation is one of the biggest speed hits. So in the future it might be worth rewriting the kernel functions such that they can write directly into the covariance matrix. (This will require some more careful thought, and isn't urgent, but I wanted to bring it up now before I forget again!)
"
Composited composite kernels,"Currently, a CompositeKernel can only be made out of StandardKernels, which means something like GaussianKernel()+GaussianKernel()*GaussianKernel() doesn't work. Is there a good reason for that?
"
Fix GaussianKernel kernelize* functions,"Parentheses are significant :)
"
Speed vs code clarity,"So quite a bit of the code looks like it's rather optimised for some things - e.g. euclidean_distance() with BLAS.axpy! and BLAS.dot - is that actually that much faster than Base.dot(x, y) that it's worth it ? And what happens if x and y happen to have different lengths (_shouldn't_ happen, but you never know)? Base.dot() would just throw a DimensionMismatch. The BLAS stuff doesn't complain, just computes a different number...

On a very similar note, is it worth writing convert(T,2) instead of just 2? Doesn't the compiler automatically convert types?

And on a not so related note, but too small to bother opening a separate issue: the Coveralls thing seems to think that kernelize_scalar() for the MercerSigmoidKernel (separablekernels.jl) isn't covered - but I think it should be ? When I add a println() statement to the function, it gets printed out lots of times. Any idea what's going on?
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
Derivatives (wrt parameters and input values),"Hi,

to be able to use this kernel module in my Gaussian Process regression code - instead of rolling my own covariance kernels, which do basically the same, just not as fancy as your code - I will need the derivatives of kernels, both with respect to the parameters (for optimizing log marginal likelihoods) and with respect to input values (for training from and prediction of derivatives of function values, not just the function values themselves). What are your thoughts (if any) on the interface for that ?

Have a look at https://github.com/st--/juliagp/blob/master/covariances.jl for how I've implemented it so far...

-ST
"
Refactor conversion loop to reduce code redundancy,"Hi,
I just came across your Kernel package for Julia, which I'd like to incorporate into my Gaussian Process code - rather than duplicating all the covariance kernels! To get a bit used to things I just refactored the loops at the bottom of the kernel files, removing code redundancy. All tests running fine.
"
can I use this module in virtual environment?,"Now, I try to install in virtual-env.
but I couldn't find the way for install.
could you help me?"
Fix absl compatibility issues in TF 1.13,"Ref https://github.com/tensorflow/tensorflow/issues/22766
Ref https://github.com/tensorflow/tensorflow/issues/22113"
How to use extra loss with Keras model?,"Hi

I am using Keras api for model training and testing. May I know how to use the loss function in this project together with Keras api?

Thank you!"
error in 'make',"When I run 'make' in build directory , there some errors:

[100%] Linking CXX shared library libextra_losses.so
/usr/bin/ld: Êâæ‰∏çÂà∞ -ltensorflow_framework
collect2: error: ld returned 1 exit status
CMakeFiles/extra_losses.dir/build.make:166: recipe for target 'libextra_losses.so' failed
make[2]: *** [libextra_losses.so] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/extra_losses.dir/all' failed
make[1]: *** [CMakeFiles/extra_losses.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2

do you know the reason? thank you for your help.

ubuntu16.04+python3.6(anaconda3)+tensorflow1.6
thank you very much."
Where is cuda_config.h?,"      In the ""readme.md"", there is ""copy the header file ""cuda\_config.h"" from ""your\_python\_path/site-packages/external/local\_config\_cuda/cuda/cuda/cuda\_config.h"" to ""your\_python\_path/site-packages/tensorflow/include/tensorflow/stream\_executor/cuda/cuda\_config.h"".
      But  I can't find the path or ""cuda_config.h"" anywhere. It may lead to the failure of ""make"" command.  Can you fix it?"
 undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv,"I have Ubuntu 16.04, Python 3.5.2 in a virtual env, and Tensorflow 1.6 GPU.
I have 2 1080ti.
I have run  the ""cd build && cmake .."" and the output is:

> `-- The C compiler identification is GNU 5.4.0
> -- The CXX compiler identification is GNU 5.4.0
> -- Check for working C compiler: /usr/bin/cc
> -- Check for working C compiler: /usr/bin/cc -- works
> -- Detecting C compiler ABI info
> -- Detecting C compiler ABI info - done
> -- Detecting C compile features
> -- Detecting C compile features - done
> -- Check for working CXX compiler: /usr/bin/c++
> -- Check for working CXX compiler: /usr/bin/c++ -- works
> -- Detecting CXX compiler ABI info
> -- Detecting CXX compiler ABI info - done
> -- Detecting CXX compile features
> -- Detecting CXX compile features - done
> -- Looking for pthread.h
> -- Looking for pthread.h - found
> -- Looking for pthread_create
> -- Looking for pthread_create - not found
> -- Looking for pthread_create in pthreads
> -- Looking for pthread_create in pthreads - not found
> -- Looking for pthread_create in pthread
> -- Looking for pthread_create in pthread - found
> -- Found Threads: TRUE  
> -- Found CUDA: /usr/local/cuda-9.0 (found version ""9.0"") 
> -- Found CWD: /home/vision/matthew/tf.extra_losses/build
> -- Found GPU_CAPABILITY: gencode arch=compute_61,code=sm_61
> 
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> -- Found TF_INC: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include
> -- Found TF_INC_EXTERNAL: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/external/nsync/public
> -- Found TF_LIB: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow
> -- Configuring done
> -- Generating done
> -- Build files have been written to: /home/vision/matthew/tf.extra_losses/build
> `

**And Also 'make' command:**

> `[ 16%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_grad_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> [ 33%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> Scanning dependencies of target extra_losses
> [ 50%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:274:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:268:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:291:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:285:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:62:40: warning: variable ‚Äònum_dimensions‚Äô set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:93:40: warning: variable ‚Äònum_dimensions‚Äô set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> [ 66%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_grad_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:293:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:287:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:310:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:304:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> [ 83%] Building CXX object CMakeFiles/extra_losses.dir/common.cc.o
> In file included from /home/vision/matthew/tf.extra_losses/common.cc:22:0:
> /home/vision/matthew/tf.extra_losses/common.h:46:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_pow2(const T x)
>                                      ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) Target binary_cast(Source s)
>                                        ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
> /home/vision/matthew/tf.extra_losses/common.h:52:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_aligned(const T ptr, const size_t alignment)
>                                      ^
> [100%] Linking CXX shared library libextra_losses.so
> [100%] Built target extra_losses
> `

**but unfortunately  got an error in test_op.py execution:**

 

> 
> `/home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> Traceback (most recent call last):
>   File ""test_op.py"", line 36, in <module>
>     op_module = load_op_module(LIB_NAME)
>   File ""test_op.py"", line 33, in load_op_module
>     oplib = tf.load_op_library(lib_path)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
>     lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: /home/vision/matthew/tf.extra_losses/build/libextra_losses.so: undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv
> `
> 

What should  do?!"
How to get results from embedding training?,"Hi

 I've successfully made the model train, but I'm having trouble printing results, can you please elaborate on how to:

""use these embeddings, place them in the same folder as main.py, load the embeddings and use them.""

Specifically, what does ""loading and using them mean"" in the context of getting H@1, MRR etc.?"
How is background information used in the model ?,"Your paper mentions about using background information for filtering training dataset and tying parameters of relation in ""SimplE-bk"".  Is the code of this model available?"
Can I use GPU accelerate the training process?,"The training process (except the early stop process and test process) take me 7 hours on wn18 dataset using the default parameters on a Linux machine (One GTX 1080 ti). Is this normal?

And I find the early stop process also need 6 hours on wn18 dataset, I guess the time consuming is mainly from ""self.reader.replace_raw"" and ""get_rank"" step. Is this normal?"
a question about simplE_ignr.py,"I really like this work, but I have one thing confused.
In the 34th line of simplE_ignr.py, you use the code ""self.labels = tf.tile(self.y, [2])"", why do you use this code (what's its effect), and why don't you use the code in simplE_avg.py?
Looking forward to your kind feedback."
The evaluations,"I really appreciate your work but I'm not sure about one thing: In `Reader.py next_batch()`, if the `neg_ratio` is larger than 1, the size of `all_triples_and_labels` would be `(1+neg_ratio) * len(bp_triples)`Ôºåthen the evaluations should all be divided by  `(1+neg_ratio) * len(triples)`. Looking forward to your reply.
"
GPU not being used,i used --cuda on google colab but gpu is not being used how to fix it
Hi! the training can not convergence!,"I train the model on val2017 dataset and my own dataset, but the valid result is about 21db, have you tested them ?Thank you!"
Methods for adding poission noise in official implementations,"Hi Authors,

I  found the official implementations of adding poission noise use the following methods:
https://github.com/NVlabs/noise2noise/blob/c40a0481198bb524d0b70c2cc452f21bd7aec85c/train.py#L47

        return np.random.poisson(chi*(x+0.5))/chi - 0.5

Do you think we can utilize this method?"
Using my own noisy-noisy pairs,"Dear Authors,

There are no instructions on how to use the code for my own noisy-noisy pairs. The current code takes a dataset of images as input and applies two independent noises on each instance, leading to pairs of noisy-noisy train set. But what if I already have my own pairs of noisy-noisy images and want to train the network on them? Is this possible with current architecture?

Best,
Ali. "
just for the test.py,"Hello, I would like to ask, why the test image I input is 1024 * 1024, but the output is indeed 256 * 256, and I did not see where the picture was cropped, I look forward to your reply, thank you."
"Confused about the unet.py code, is it reasonable?","Hi I checked the code feel confused with the code in unet.py:
```
    def forward(self, x):
        """"""Through encoder, then decoder by adding U-skip connections. """"""

        # Encoder
        pool1 = self._block1(x)
        pool2 = self._block2(pool1)
        pool3 = self._block2(pool2)
        pool4 = self._block2(pool3)
        pool5 = self._block2(pool4)
```

pool2-pool5 is computed by self._block2, So it means pool2-pool5 re-use the same conv weight and bias. Does it accepted? I think the u-net should use different conv weight of different layer."
Sharing Results from Monto Carlo Renderings,"Hi ,

Can you please share the results of Monte Carlo Renderings N2N ? 

Regards
Muzahid"
OpenEXR Error while running the train files.,"Traceback (most recent call last):
  File ""train.py"", line 7, in <module>
    from datasets import load_dataset
  File ""/Users/snehagathani/Desktop/noise/src/datasets.py"", line 9, in <module>
    from utils import load_hdr_as_tensor
  File ""/Users/snehagathani/Desktop/noise/src/utils.py"", line 12, in <module>
    import OpenEXR
ImportError: dlopen(/anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so, 2): Symbol not found: __ZN7Imf_2_314TypedAttributeISsE13readValueFromERNS_7IStreamEii
  Referenced from: /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so
  Expected in: flat namespace
 in /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so


This error comes for all the training files."
datasets.py  issues?,"Hello,there may be problems with this place
`raise ValueError('Invalid noise type: {}'.format(noise_type))` 
 should be 
`raise ValueError('Invalid noise type: {}'.format(self.noise_type))`"
something wrong with your endcoder?,"https://github.com/joeylitalien/noise2noise-pytorch/blob/7942c06f924e2244d91fc8c1aff1c2e3991e0eae/src/unet.py#L82
I think enconv(i), i=2,...,5 should be defined separatedly or they will share the same weights.
"
About U-Net model and data pre-processing,"I am curious about the differences from the paper model
1) Why did you choose to use ConvTranspose instead of Upsampling
2) You used RELU in all parts and Leaky RELU after the last layer. But from the paper I think the author meant Leaky RELU everywhere except the last layer. And In the last layer only linear activation

Unrelated question
3) Did you use any pre-processing for the images e.g. means subtraction, normalization etc. I think that would be needed since we don't have BN layers


I am trying to implement the model in Tensorflow and having the problem of INF loss that starts in the 2nd epoch. So I hope your answers would help me. Thank you in advance!

Update: I seem to have solved the problem by adding Batch Norm layers to the UNET model. But still I am puzzled how the authors managed to get a stable training without Batch Norm"
Create LICENSE,
Failed to load https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.19041-SP0
Python: 3.8.11
PyTorch: 1.9.1+cpu
HanLP: 2.1.0-beta.50
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
d:\PycharmProjects\mednlp_project\mednlp_train\mishu_entity_extract.ipynb ÂçïÂÖÉÊ†º 64 in ()
      1 import hanlp
      3 HanLP = hanlp.pipeline() \
      4     .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
      5     .append(hanlp.load('FINE_ELECTRA_SMALL_ZH'), output_key='tok') \
      6     .append(hanlp.load('CTB9_POS_ELECTRA_SMALL'), output_key='pos') \
      7     .append(hanlp.load('MSRA_NER_ELECTRA_SMALL_ZH'), output_key='ner', input_key='tok') \
----> 8     .append(hanlp.load('PMT1_DEP_ELECTRA_SMALL', conll=0), output_key='dep', input_key='tok')\
      9     .append(hanlp.load('CTB9_CON_ELECTRA_SMALL'), output_key='con', input_key='tok')

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\__init__.py:43, in load(save_dir, verbose, **kwargs)
     41     from hanlp_common.constant import HANLP_VERBOSE
     42     verbose = HANLP_VERBOSE
---> 43 return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\utils\component_util.py:186, in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    184 except:
    185     pass
--> 186 raise e from None

File d:\ProgramData\Anaconda3\envs\mednlp\lib\site-packages\hanlp\utils\component_util.py:106, in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    104 else:
    105     if os.path.isfile(os.path.join(save_dir, 'config.json')):
...
    459     if not _raise_exceptions_for_missing_entries:

OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like hfl/chinese-electra-180g-small-discriminator is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
=================================ERROR LOG ENDS=================================
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
hanlp.load(SIGHAN2005_MSR_CONVSEG) Âç°‰Ωè‰∫Ü,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
Á¨¨‰∏ÄÊ¨°import hanlp
hanlp.load(SIGHAN2005_MSR_CONVSEG) Âç°‰Ωè‰∫Ü

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
from hanlp.utils.rules import split_sentence
from hanlp.pretrained.tok import SIGHAN2005_MSR_CONVSEG

tok = hanlp.load(SIGHAN2005_MSR_CONVSEG)
```

ËæìÂá∫
```
2023-09-08 17:42:46.670747: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-08 17:42:46.716386: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-08 17:42:46.716815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-08 17:42:47.808801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
Áúã‰πãÂâçÁöÑËæìÂá∫Ôºå‰∏ãËΩΩÂ∑≤ÁªèÂÆåÊàê

ËæìÂÖ•ctrl-dÔºå‰ºöËæìÂá∫Ôºö
```
terminate called after throwing an instance of 'std::runtime_error'
  what():  random_device could not be read
```

ËØï‰∫ÜÂè¶‰∏Ä‰∏™tok.SIGHAN2005_PKU_CONVSEGÔºå‰πüÊòØËøôÊ†∑
COARSE_ELECTRA_SMALL_ZHÊ≤°ÈóÆÈ¢ò

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
os: Fedora Linux 38 (Workstation Edition)
kernel: 6.4.14
python: 3.11.4
hanlp: 2.1.0b50

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
ÂßãÁªàÊä•file is not a zip file,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
A clear and concise description of what the bug is. hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)Êä•ÈîôÔºåËØ¥load_from_meta_file raise e from None,Á≠â‰∫éÊòØloadÔºàÔºâÂáΩÊï∞‰∏ãËΩΩÁöÑ‰∏úË•øÊúâÈóÆÈ¢òÔºåÂú®zipfileËß£ÂéãÁöÑÊó∂ÂÄôËØ¥‰∏çÊòØzipÊñá‰ª∂„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)

```python
```

**Describe the current behavior**
A clear and concise description of what happened.Âú®.hanlp/tok/ÂÆâË£ÖÁΩë‰∏ä‰∏ãËΩΩÁöÑÂåÖ‰πüËøòÊòØ‰∏çËÉΩË∑ëÈÄö

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.8.17
- HanLP version:2.1.0-beta.50
- Pytorch 2.0.1+cu117
- linux-4.15.0-142-generate-x86_64-with-glibc2.17

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
a bug,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
ËøêË°årecongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)Êó∂Êä•Èîô

Failed to load https://file.hankcs.com/hanlp/ner/msra_ner_albert_base_20211228_173323.zip
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.22621-SP0
Python: 3.9.13
PyTorch: 2.0.1+cu118
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50



RuntimeError: Failed to import transformers.models.albert.modeling_tf_albert because of the following error (look up to see its traceback):
No module named 'keras.engine'

**Code to reproduce the issue**
import hanlp
recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)

```python
```

**Describe the current behavior**
Â∑≤ÁªèÊàêÂäü‰∏ãËΩΩ‰∫ÜÊ®°ÂûãÔºå‰ΩÜÊòØÂú®Âä†ËΩΩÁöÑÊó∂ÂÄôÊä•Èîô
hanlpÁâàÊú¨‰∏∫2.1.0b50

**Expected behavior**
Â∏åÊúõËÉΩÂ§üÂ∞Ü‰∏ãËΩΩÁöÑÊ®°ÂûãÂä†ËΩΩËøõÊù•‰ΩøÂæó‰ª£Á†ÅËÉΩÂ§üÊàêÂäüËøêË°å

**System information**
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.22621-SP0
Python: 3.9.13
PyTorch: 2.0.1+cu118
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50

**Other info / logs**
Ê∫ê‰ª£Á†ÅÔºö
import hanlp
recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Â∏åÊúõÂèØ‰ª•Â¢ûÂä†Ëá™ÂÆö‰πâËØçÂÖ∏ÂäüËÉΩÔºåÂØπ‰∫éÂàÜÈîôÁöÑËØçËØ≠ÂèØ‰ª•‰∫∫‰∏∫Á∫†Ê≠£„ÄÇ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
ÁõÆÂâçÊ≤°ÊúâÂèëÁé∞Â≠òÂú®Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑÂäüËÉΩ„ÄÇÂú®‰∏Ä‰∫õÁâπÂÆöÂú∫ÊôØ‰∏≠ÔºåÂ¶ÇÔºöËû∫Ê†ìÁ¥ßÂõ∫Ôºå‰ºöË¢´ÂàÜËØç‰∏∫ Ëû∫Ê†ì„ÄÅÁ¥ß„ÄÅÂõ∫„ÄÇ‰ΩÜÊòØÂú®ËØ•Âú∫ÊôØ‰∏≠ÔºåÊàë‰ª¨Êõ¥Â∏åÊúõÂæóÂà∞Ëû∫Ê†ì„ÄÅÁ¥ßÂõ∫„ÄÇ
**Will this change the current api? How?**
‰πüËÆ∏ÈúÄË¶ÅÂ¢ûÂä†‰∏Ä‰∏™add_word()Êé•Âè£ÂéªÂÆûÁé∞ËØ•ÂäüËÉΩ„ÄÇ
**Who will benefit with this feature?**
ÊØè‰∏Ä‰∏™‰ΩøÁî®hanlpÂπ∂Â∏åÊúõhanlpËÉΩË∂äÊù•Ë∂äÂ•ΩÁöÑ‰∫∫ÈÉΩ‰ºöËé∑Áõä„ÄÇ
**Are you willing to contribute it (Yes/No):**
ÂäõÊúâ‰∏çÈÄÆ
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Dedian
- Python version: 3.7.4
- HanLP version: 2.1.0b50

**Any other info**
no
* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
‰øÆÂ§çViterbiSegmentÂàÜËØçÂô®‰∏≠Âä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Êó∂Êú™ÊõøÊç¢DoubleArrayTrieÂØºËá¥ÂàÜËØç‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÈóÆÈ¢ò,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ‚ú®.
‚ö†Ô∏èChanges must be made on dev branch.
-->

# ‰øÆÂ§çViterbiSegmentÂàÜËØçÂô®‰∏≠Âä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Êó∂Êú™ÊõøÊç¢DoubleArrayTrieÂØºËá¥ÂàÜËØç‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÈóÆÈ¢ò

## Description

ViterbiSegmentÂä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Êó∂Êú™Ê≠£Á°ÆÊõøÊç¢DoubleArrayTrie, ÂØºËá¥Â∫îËØ•Ë¢´ÂàáÂàÜÂá∫ÁöÑËØçÊù°Êú™Ë¢´ÂàáÂàÜ

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?
com/hankcs/hanlp/seg/SegmentTest.java
```java
    public void testExtendViterbi() throws Exception
    {
        HanLP.Config.enableDebug(false);
        String path = System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/CustomDictionary.txt;"" +
            System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt"";
        path = path.replace(""\\"", ""/"");
        String text = ""‰∏ÄÂçäÂ§©Â∏ïÂÖãÊñØÊõºÊòØËµ∞‰∏çÂá∫‰∏ÅÂ≠óÊ°•ÈïáÁöÑ"";
        Segment segment = HanLP.newSegment().enableCustomDictionary(false);
        Segment seg = new ViterbiSegment(path);
        System.out.println(""‰∏çÂêØÁî®Â≠óÂÖ∏ÁöÑÂàÜËØçÁªìÊûúÔºö"" + segment.seg(text));
        System.out.println(""ÈªòËÆ§ÂàÜËØçÁªìÊûúÔºö"" + HanLP.segment(text));
        seg.enableCustomDictionaryForcing(true).enableCustomDictionary(true);
        List<Term> termList = seg.seg(text);
        System.out.println(""Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑÂàÜËØçÁªìÊûúÔºö"" + termList);
    }
```
![image](https://github.com/hankcs/HanLP/assets/27196120/73ae3c04-c3dc-4a87-ba0c-287c127b9829)


## Checklist

Check all items that apply.

- [x] ‚ö†Ô∏èChanges **must** be made on `dev` branch instead of `master`
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
ViterbiSegmentÂä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Êó∂Êú™Ê≠£Á°ÆÊõøÊç¢DoubleArrayTrie,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
ViterbiSegmentÂä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Êó∂Êú™Ê≠£Á°ÆÊõøÊç¢DoubleArrayTrie

**Code to reproduce the issue**
com/hankcs/hanlp/seg/Viterbi/ViterbiSegment.java
```java
    private void loadCustomDic(String customPath, boolean isCache)
    {
        if (TextUtility.isBlank(customPath))
        {
            return;
        }
        logger.info(""ÂºÄÂßãÂä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏:"" + customPath);
        DoubleArrayTrie<CoreDictionary.Attribute> dat = new DoubleArrayTrie<CoreDictionary.Attribute>();
        String path[] = customPath.split("";"");
        String mainPath = path[0];
        StringBuilder combinePath = new StringBuilder();
        for (String aPath : path)
        {
            combinePath.append(aPath.trim());
        }
        File file = new File(mainPath);
        mainPath = file.getParent() + ""/"" + Math.abs(combinePath.toString().hashCode());
        mainPath = mainPath.replace(""\\"", ""/"");
        DynamicCustomDictionary.loadMainDictionary(mainPath, path, dat, isCache, config.normalization);
    }
```
com/hankcs/hanlp/seg/SegmentTest.java
```java
    public void testExtendViterbi() throws Exception
    {
        HanLP.Config.enableDebug(false);
        String path = System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/CustomDictionary.txt;"" +
            System.getProperty(""user.dir"") + ""/"" + ""data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt"";
        path = path.replace(""\\"", ""/"");
        String text = ""‰∏ÄÂçäÂ§©Â∏ïÂÖãÊñØÊõºÊòØËµ∞‰∏çÂá∫‰∏ÅÂ≠óÊ°•ÈïáÁöÑ"";
        Segment segment = HanLP.newSegment().enableCustomDictionary(false);
        Segment seg = new ViterbiSegment(path);
        System.out.println(""‰∏çÂêØÁî®Â≠óÂÖ∏ÁöÑÂàÜËØçÁªìÊûúÔºö"" + segment.seg(text));
        System.out.println(""ÈªòËÆ§ÂàÜËØçÁªìÊûúÔºö"" + HanLP.segment(text));
        seg.enableCustomDictionaryForcing(true).enableCustomDictionary(true);
        List<Term> termList = seg.seg(text);
        System.out.println(""Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑÂàÜËØçÁªìÊûúÔºö"" + termList);
    }
```

**Describe the current behavior**
Âä†ËΩΩCustomDictionary.txt‰∏éÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt‰∏≠, Â∫îËØ•ÂåÖÂê´'‰∏ÅÂ≠óÊ°•Èïá'ËØçÊù°, ‰ΩÜÂÆûÈôÖÁöÑÂàÜËØç‰∏≠Âπ∂Êú™ÂàáÂá∫
![image](https://github.com/hankcs/HanLP/assets/27196120/fa80c919-a984-446e-a2fa-bf03d27b5147)
![image](https://github.com/hankcs/HanLP/assets/27196120/f25fff1a-1e95-47aa-bc9a-f5957292fae1)

**Expected behavior**
'‰∏ÅÂ≠óÊ°•Èïá'ËØçÊù°Â∫îË¢´ÂàáÂá∫

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macos 13.3.1 (a) (22E772610a)
- Python version: n/a
- HanLP version: 1.8.4

**Other info / logs**
com/hankcs/hanlp/seg/Viterbi/ViterbiSegment.java‰∏≠ÁöÑloadCustomDic(String customPath, boolean isCache)Âú®Âä†ËΩΩÂÆåDoubleArrayTrieÂêéÂ∫îÊõøÊç¢ÂØπÂ∫îËØçÂÖ∏
![image](https://github.com/hankcs/HanLP/assets/27196120/fa42b9bf-77c4-4489-b07a-adb17772f8a3)


* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Ë∞ÉÁî®Á≤óÁ≤íÂ∫¶ÂàÜËØçAPIÁñëÊòØÂ≠òÂú®ÂÜÖÂ≠òÊ≥ÑÊºèÔºü,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
ÈíàÂØπ‰∏ÄÊâπÊ≥ïÂæãÊñá‰π¶ÔºàÊ∞ë‰∫ãÂà§ÂÜ≥‰π¶ÔºåÊñá‰ª∂Â§ßÂ∞è‰ªé2k-10k‰∏çÁ≠âÔºâÔºåÂæ™ÁéØË∞ÉÁî®Á≤óÁ≤íÂ∫¶ÂàÜËØçAPIËøõË°åÂÖ®ÊñáÂàÜËØçÔºàCPU modeÔºâÔºåÂèëÁé∞ÂÜÖÂ≠ò‰ºöÊåÅÁª≠‰∏äÊ∂®ÔºåÊúÄÁªàÂÜÖÂ≠òÊ∫¢Âá∫„ÄÇ

**Code to reproduce the issue**

```python
import os
import hanlp
import time
import psutil

def batch_parse_info(root_path):
    tok_model = hanlp.load('COARSE_ELECTRA_SMALL_ZH')

    fs_list = os.listdir(root_path)
    for fs in fs_list:
        full_fs = root_path + os.sep + fs

        content = ''
        with open(full_fs, 'r', encoding='utf-8') as f:
            for line in f:
                content += line + '\n'

        process = psutil.Process()
        memory_info1 = process.memory_info()
        result = tok_model(content)
        memory_info = process.memory_info()

        print(f""Memory infoÔºö{memory_info1.rss/1024.0/1024.0} MB, {memory_info.rss/1024.0/1024.0} MB, {(memory_info.rss-memory_info1.rss)/1024.0/1024.0} MB"")

if __name__ == '__main__':
    root_path = r'C:\Users\Jerson\MySource\anaconda\private-projects\data\final\liuyahaofile\Ê∞ë‰∫ãÂà§ÂÜ≥‰π¶\ÂéüÂëä'

    tm_beg = time.time()
    batch_parse_info(root_path)
    tm_end = time.time()
    
    print(f'Cost time: {tm_end-tm_beg}s')
```

**Describe the current behavior**

‰ª•‰∏ãÊòØÈÉ®ÂàÜÊó•ÂøóÔºö
Memory infoÔºö194.4296875 MB, 212.2265625 MB, 17.796875 MB
Memory infoÔºö212.2265625 MB, 212.35546875 MB, 0.12890625 MB
Memory infoÔºö212.35546875 MB, 213.3359375 MB, 0.98046875 MB
Memory infoÔºö213.3359375 MB, 214.21484375 MB, 0.87890625 MB
Memory infoÔºö214.21484375 MB, 215.55859375 MB, 1.34375 MB
Memory infoÔºö215.55859375 MB, 215.63671875 MB, 0.078125 MB
Memory infoÔºö215.63671875 MB, 216.0234375 MB, 0.38671875 MB
Memory infoÔºö216.0234375 MB, 216.02734375 MB, 0.00390625 MB
Memory infoÔºö216.02734375 MB, 216.03515625 MB, 0.0078125 MB
Memory infoÔºö216.03515625 MB, 216.0625 MB, 0.02734375 MB
Memory infoÔºö216.0625 MB, 216.06640625 MB, 0.00390625 MB
Memory infoÔºö216.06640625 MB, 216.0703125 MB, 0.00390625 MB
Memory infoÔºö216.0703125 MB, 216.0625 MB, -0.0078125 MB
Memory infoÔºö216.06640625 MB, 216.0703125 MB, 0.00390625 MB
Memory infoÔºö216.07421875 MB, 216.08203125 MB, 0.0078125 MB
Memory infoÔºö216.08203125 MB, 216.078125 MB, -0.00390625 MB
Memory infoÔºö216.078125 MB, 221.6015625 MB, 5.5234375 MB
Memory infoÔºö221.6015625 MB, 221.87890625 MB, 0.27734375 MB
Memory infoÔºö221.87890625 MB, 221.875 MB, -0.00390625 MB
Memory infoÔºö221.875 MB, 222.41015625 MB, 0.53515625 MB
Memory infoÔºö222.41015625 MB, 223.0 MB, 0.58984375 MB
Memory infoÔºö223.0 MB, 223.16015625 MB, 0.16015625 MB
Memory infoÔºö223.1640625 MB, 223.15625 MB, -0.0078125 MB
Memory infoÔºö223.15625 MB, 225.859375 MB, 2.703125 MB
Memory infoÔºö225.859375 MB, 225.6171875 MB, -0.2421875 MB
Memory infoÔºö225.6171875 MB, 225.37109375 MB, -0.24609375 MB
Memory infoÔºö225.37109375 MB, 225.36328125 MB, -0.0078125 MB
Memory infoÔºö225.3671875 MB, 225.37890625 MB, 0.01171875 MB
Memory infoÔºö225.37890625 MB, 225.390625 MB, 0.01171875 MB
Memory infoÔºö225.390625 MB, 225.41796875 MB, 0.02734375 MB
Memory infoÔºö225.42578125 MB, 227.04296875 MB, 1.6171875 MB
Memory infoÔºö227.046875 MB, 226.80078125 MB, -0.24609375 MB
Memory infoÔºö226.80078125 MB, 226.8125 MB, 0.01171875 MB
Memory infoÔºö226.8125 MB, 226.81640625 MB, 0.00390625 MB
Memory infoÔºö226.81640625 MB, 226.80859375 MB, -0.0078125 MB
Memory infoÔºö225.80859375 MB, 226.3125 MB, 0.50390625 MB
Memory infoÔºö226.3125 MB, 226.44140625 MB, 0.12890625 MB
Memory infoÔºö226.44140625 MB, 226.43359375 MB, -0.0078125 MB
Memory infoÔºö226.43359375 MB, 226.4375 MB, 0.00390625 MB
Memory infoÔºö226.4375 MB, 226.375 MB, -0.0625 MB
Memory infoÔºö226.375 MB, 226.58203125 MB, 0.20703125 MB
Memory infoÔºö226.58203125 MB, 226.73046875 MB, 0.1484375 MB
Memory infoÔºö226.73046875 MB, 226.71875 MB, -0.01171875 MB
Memory infoÔºö226.71875 MB, 226.734375 MB, 0.015625 MB
Memory infoÔºö226.734375 MB, 226.7265625 MB, -0.0078125 MB
Memory infoÔºö226.7265625 MB, 229.109375 MB, 2.3828125 MB
Memory infoÔºö229.109375 MB, 227.86328125 MB, -1.24609375 MB
Memory infoÔºö227.86328125 MB, 229.11328125 MB, 1.25 MB
Memory infoÔºö229.11328125 MB, 229.14453125 MB, 0.03125 MB
Memory infoÔºö229.14453125 MB, 228.89453125 MB, -0.25 MB
Memory infoÔºö228.89453125 MB, 228.90625 MB, 0.01171875 MB
Memory infoÔºö228.90625 MB, 228.90625 MB, 0.0 MB
Memory infoÔºö228.90625 MB, 228.91015625 MB, 0.00390625 MB
Memory infoÔºö228.91015625 MB, 230.1484375 MB, 1.23828125 MB
Memory infoÔºö230.1484375 MB, 230.1484375 MB, 0.0 MB
Memory infoÔºö230.1484375 MB, 230.16015625 MB, 0.01171875 MB
Memory infoÔºö230.16015625 MB, 230.171875 MB, 0.01171875 MB
Memory infoÔºö230.171875 MB, 229.9296875 MB, -0.2421875 MB
Memory infoÔºö229.9296875 MB, 229.9296875 MB, 0.0 MB
Memory infoÔºö229.9296875 MB, 229.94140625 MB, 0.01171875 MB
Memory infoÔºö229.94140625 MB, 229.9453125 MB, 0.00390625 MB
Memory infoÔºö229.9453125 MB, 231.2109375 MB, 1.265625 MB
Memory infoÔºö231.2109375 MB, 231.21484375 MB, 0.00390625 MB
Memory infoÔºö231.21484375 MB, 230.96484375 MB, -0.25 MB
Memory infoÔºö230.96484375 MB, 230.9765625 MB, 0.01171875 MB
Memory infoÔºö230.9765625 MB, 230.984375 MB, 0.0078125 MB
Memory infoÔºö230.984375 MB, 230.9765625 MB, -0.0078125 MB
Memory infoÔºö230.9765625 MB, 230.9765625 MB, 0.0 MB
Memory infoÔºö230.9765625 MB, 230.98046875 MB, 0.00390625 MB
Memory infoÔºö230.98046875 MB, 230.984375 MB, 0.00390625 MB
Memory infoÔºö230.984375 MB, 230.98046875 MB, -0.00390625 MB
Memory infoÔºö230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory infoÔºö230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory infoÔºö230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory infoÔºö230.98046875 MB, 230.98046875 MB, 0.0 MB
Memory infoÔºö230.98046875 MB, 230.984375 MB, 0.00390625 MB
Memory infoÔºö230.984375 MB, 230.98828125 MB, 0.00390625 MB
Memory infoÔºö230.98828125 MB, 230.9921875 MB, 0.00390625 MB
Memory infoÔºö230.9921875 MB, 230.99609375 MB, 0.00390625 MB
Memory infoÔºö230.99609375 MB, 231.0 MB, 0.00390625 MB
Memory infoÔºö231.0 MB, 231.00390625 MB, 0.00390625 MB
Memory infoÔºö231.00390625 MB, 231.00390625 MB, 0.0 MB
Memory infoÔºö231.00390625 MB, 231.0078125 MB, 0.00390625 MB
Memory infoÔºö231.0078125 MB, 231.01171875 MB, 0.00390625 MB
Memory infoÔºö231.015625 MB, 231.015625 MB, 0.0 MB
Memory infoÔºö231.015625 MB, 231.0078125 MB, -0.0078125 MB
Memory infoÔºö231.0078125 MB, 231.01953125 MB, 0.01171875 MB
Memory infoÔºö231.01953125 MB, 231.0234375 MB, 0.00390625 MB
Memory infoÔºö231.0234375 MB, 231.0078125 MB, -0.015625 MB
Memory infoÔºö231.0078125 MB, 231.0078125 MB, 0.0 MB
Memory infoÔºö231.0078125 MB, 231.0078125 MB, 0.0 MB
Memory infoÔºö231.0078125 MB, 231.01171875 MB, 0.00390625 MB
Memory infoÔºö231.01171875 MB, 231.01953125 MB, 0.0078125 MB
Memory infoÔºö231.01953125 MB, 231.61328125 MB, 0.59375 MB
Memory infoÔºö231.61328125 MB, 231.62890625 MB, 0.015625 MB
Memory infoÔºö231.62890625 MB, 231.64453125 MB, 0.015625 MB
Memory infoÔºö231.64453125 MB, 231.64453125 MB, 0.0 MB
Memory infoÔºö231.64453125 MB, 231.65234375 MB, 0.0078125 MB
Memory infoÔºö231.65234375 MB, 231.640625 MB, -0.01171875 MB
Memory infoÔºö231.640625 MB, 232.265625 MB, 0.625 MB
Memory infoÔºö232.265625 MB, 232.28125 MB, 0.015625 MB
Memory infoÔºö232.28125 MB, 232.48828125 MB, 0.20703125 MB
Memory infoÔºö232.48828125 MB, 232.50390625 MB, 0.015625 MB
Memory infoÔºö232.50390625 MB, 232.26953125 MB, -0.234375 MB
Memory infoÔºö232.26953125 MB, 232.265625 MB, -0.00390625 MB
Memory infoÔºö232.265625 MB, 232.26171875 MB, -0.00390625 MB
Memory infoÔºö232.26171875 MB, 232.265625 MB, 0.00390625 MB
Memory infoÔºö232.265625 MB, 232.265625 MB, 0.0 MB
Memory infoÔºö232.265625 MB, 232.26953125 MB, 0.00390625 MB
Memory infoÔºö232.26953125 MB, 232.265625 MB, -0.00390625 MB
Memory infoÔºö232.265625 MB, 232.265625 MB, 0.0 MB


**Expected behavior**

ÂÜÖÂ≠òËæÉÁ®≥ÂÆöÔºå‰∏ç‰ºöÊåÅÁª≠‰∏äÊ∂®

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10/ Linux version 5.10.102.1-microsoft-standard-WSL2
- Python version: 3.8.5
- HanLP version: 2.1.0b42. ‰πüÂ∞ùËØïÂçáÁ∫ßÂà∞2.1.0b50ÔºåÈóÆÈ¢ò‰æùÊóß

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
"a finer tokenizer than current FINE, to be able cut people names to first and last name","<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Current behavior is always treat Personal name as one word,

https://github.com/hankcs/HanLP/issues/1829#issuecomment-1653800092

>2.2.1 Personal name
> Treat it as one word. Don‚Äôt give the internal structure unless there is a space between two names (in foreign alphabet).

but this isn't enough for applications scenarios where need a finer tokenizer, e.g. in a search index'ing application, need search query phrase `Â≠îÊòé` be able to match out `Ë´∏ËëõÂ≠îÊòé` and also `„Éë„É™„ÉîÂ≠îÊòé` in pre-index'ed vectors,

need some config to be able set, let user of the tokenizer to decide always treat names as one word, or to cut first/last in half,
especially for 

need a config to set minimum length for this behavior, for short names `ÂºµÈ£õ` `Â≤≥È£õ` might be okay to treat as one word,
but should have a way to cut longer names as `Ë´∏ËëõÈùí‰∫ë` `ÈæêÈùí‰∫ë` `ÂàòÈùí‰∫ë` to two parts, enable end user can search by `Èùí‰∫ë` to find out all text talking about `Èùí‰∫ë`

to even longer foreign names `Á±≥Ëò≠ÊòÜÂæ∑Êãâ` `ÂîêÁ¥çÂæ∑Â∑ùÊôÆ` `Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨` there should have a way to cut into halves,

compare with the alternative library `nodejieba`, which has no problem to cut `Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ` to 2 parts:

```js
> var nodejieba = require('nodejieba');
> nodejieba.cut('‰∏∫‰ªÄ‰πà‰ΩôÂçéËØ¥Á±≥ÂÖ∞¬∑ÊòÜÂæ∑ÊãâÊòØ‰∏âÊµÅÂ∞èËØ¥ÂÆ∂Ôºü')
[
  '‰∏∫‰ªÄ‰πà', '‰Ωô',
  'Âçé',     'ËØ¥',
  'Á±≥ÂÖ∞',   '¬∑',
  'ÊòÜÂæ∑Êãâ', 'ÊòØ',
  '‰∏âÊµÅ',   'Â∞èËØ¥ÂÆ∂',
  'Ôºü'
]
```

```py

In [107]: tok([""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ"", ""‰∏∫‰ªÄ‰πà‰ΩôÂçéËØ¥Á±≥ÂÖ∞¬∑ÊòÜÂæ∑ÊãâÊòØ‰∏âÊµÅÂ∞èËØ¥ÂÆ∂Ôºü"", ""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""ÂîêÁ¥çÂæ∑Â∑ùÊôÆ"", ""Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨"", ""Ë´∏Ëëõ‰∫Æ"", ""Ë´∏ËëõÂ≠îÊòé"", ""Ë´∏Ëëõ‰∫ÆÂÖàÁîü"", ""„Éë„É™„ÉîÂ≠îÊòé""])
Out[107]: 
[['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ'],
 ['‰∏∫‰ªÄ‰πà', '‰ΩôÂçé', 'ËØ¥', 'Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ', 'ÊòØ', '‰∏âÊµÅ', 'Â∞èËØ¥ÂÆ∂', 'Ôºü'],
 ['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['ÂîêÁ¥çÂæ∑Â∑ùÊôÆ'],
 ['Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨'],
 ['Ë´∏Ëëõ‰∫Æ'],
 ['Ë´∏ËëõÂ≠îÊòé'],
 ['Ë´∏Ëëõ‰∫Æ', 'ÂÖàÁîü'],
 ['„Éë„É™„Éî', 'Â≠îÊòé']]


from hanlp_restful import HanLPClient
HanLP = HanLPClient('https://hanlp.hankcs.com/api', auth=None, language='zh') # auth‰∏çÂ°´ÂàôÂåøÂêçÔºåzh‰∏≠ÊñáÔºåmulÂ§öËØ≠Áßç
HanLP([""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ"", ""‰∏∫‰ªÄ‰πà‰ΩôÂçéËØ¥Á±≥ÂÖ∞¬∑ÊòÜÂæ∑ÊãâÊòØ‰∏âÊµÅÂ∞èËØ¥ÂÆ∂Ôºü"", ""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""ÂîêÁ¥çÂæ∑Â∑ùÊôÆ"", ""Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨"", ""Ë´∏Ëëõ‰∫Æ"", ""Ë´∏ËëõÂ≠îÊòé"", ""Ë´∏Ëëõ‰∫ÆÂÖàÁîü"", ""Âë®ÊÅ©Êù•"", ""Á±≥Ëò≠Â∏Ç"", ""Ê¥õÊùâÁ£ØÁ∏£"", ""Âçó‰∫¨Â∏Ç"",]) .pretty_print()

Dep 
‚îÄ‚îÄ‚îÄ 
‚îå‚îÄ‚ñ∫ 
‚îî‚îÄ‚îÄ 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
name 
root 	Po 
‚îÄ‚îÄ 
NR 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	NER Type     
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫LOCATION 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON   	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	Po    3  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îê    
NR‚îÄ‚îÄ‚î¥‚ñ∫TOP

 
 
 	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ 	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP

Dep Tree  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
     ‚îå‚îÄ‚îÄ‚ñ∫ 
     ‚îÇ‚îå‚îÄ‚ñ∫ 
‚îå‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ 
‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñ∫ 
‚îÇ‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚ñ∫ 
‚îÇ‚îÇ  ‚îÇ‚îÇ‚îå‚îÄ‚ñ∫ 
‚îÇ‚îî‚îÄ‚ñ∫‚îî‚î¥‚î¥‚îÄ‚îÄ 
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‰∏∫‰ªÄ‰πà    
‰ΩôÂçé     
ËØ¥      
Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ 
ÊòØ      
‰∏âÊµÅ     
Â∞èËØ¥ÂÆ∂    
Ôºü      	Relati 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
advmod 
nsubj  
root   
nsubj  
cop    
amod   
ccomp  
punct  	Po 
‚îÄ‚îÄ 
AD 
NR 
VV 
NR 
VC 
JJ 
NN 
PU 	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‰∏∫‰ªÄ‰πà    
‰ΩôÂçé     
ËØ¥      
Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ 
ÊòØ      
‰∏âÊµÅ     
Â∞èËØ¥ÂÆ∂    
Ôºü      	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
           
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 
           
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 
           
           
           
           	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‰∏∫‰ªÄ‰πà    
‰ΩôÂçé     
ËØ¥      
Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ 
ÊòØ      
‰∏âÊµÅ     
Â∞èËØ¥ÂÆ∂    
Ôºü      	SRL PA1      
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫ARGM-ADV 
‚îÄ‚îÄ‚îÄ‚ñ∫ARG0     
‚ïü‚îÄ‚îÄ‚ñ∫PRED     
‚óÑ‚îÄ‚îê          
  ‚îÇ          
  ‚îú‚ñ∫ARG1     
‚óÑ‚îÄ‚îò          
             	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‰∏∫‰ªÄ‰πà    
‰ΩôÂçé     
ËØ¥      
Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ 
ÊòØ      
‰∏âÊµÅ     
Â∞èËØ¥ÂÆ∂    
Ôºü      	SRL PA2  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
         
         
         
‚îÄ‚îÄ‚îÄ‚ñ∫ARG0 
‚ïü‚îÄ‚îÄ‚ñ∫PRED 
‚óÑ‚îÄ‚îê      
‚óÑ‚îÄ‚î¥‚ñ∫ARG1 
         	Token  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‰∏∫‰ªÄ‰πà    
‰ΩôÂçé     
ËØ¥      
Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ 
ÊòØ      
‰∏âÊµÅ     
Â∞èËØ¥ÂÆ∂    
Ôºü      	Po    3       4       5       6       7       8 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AD‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ADVP‚îÄ‚îÄ‚îê   
NR‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫NP‚îÄ‚îÄ‚îÄ‚îÄ‚î§   
VV‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ   
NR‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫NP ‚îÄ‚îÄ‚îÄ‚îê       ‚îú‚ñ∫VP‚îÄ‚îÄ‚îÄ‚îÄ‚î§   
VC‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îú‚ñ∫IP ‚îÄ‚îÄ‚îÄ‚îò       ‚îú‚ñ∫IP
JJ‚îÄ‚îÄ‚îÄ‚ñ∫ADJP‚îÄ‚îÄ‚îê       ‚îú‚ñ∫VP ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ   
NN‚îÄ‚îÄ‚îÄ‚ñ∫NP ‚îÄ‚îÄ‚îÄ‚î¥‚ñ∫NP ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ   
PU‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   

Dep 
‚îÄ‚îÄ‚îÄ 
‚îå‚îÄ‚ñ∫ 
‚îî‚îÄ‚îÄ 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
name 
root 	Po 
‚îÄ‚îÄ 
NR 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	NER Type     
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫LOCATION 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON   	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠  
ÊòÜÂæ∑Êãâ 	Po    3  
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îê    
NR‚îÄ‚îÄ‚î¥‚ñ∫TOP

 
 
 	Token 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
ÂîêÁ¥çÂæ∑Â∑ùÊôÆ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Token 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
ÂîêÁ¥çÂæ∑Â∑ùÊôÆ 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Token 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
ÂîêÁ¥çÂæ∑Â∑ùÊôÆ 	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP

 
 
 	Token   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Token   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨ 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Token   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨ 	PoS
‚îÄ‚îÄ‚îÄ
NR 

 
 
 	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP

 
 
 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ë´∏ËëõÂ≠îÊòé 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ë´∏ËëõÂ≠îÊòé 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ë´∏ËëõÂ≠îÊòé 	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP

Dep 
‚îÄ‚îÄ‚îÄ 
‚îå‚îÄ‚ñ∫ 
‚îî‚îÄ‚îÄ 	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 
ÂÖàÁîü  	Relation    
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
compound:nn 
root        	Po 
‚îÄ‚îÄ 
NR 
NN 	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 
ÂÖàÁîü  	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 
           	Tok 
‚îÄ‚îÄ‚îÄ 
Ë´∏Ëëõ‰∫Æ 
ÂÖàÁîü  	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îê   
NN‚îÄ‚îÄ‚î¥‚ñ∫NP

 
 
 	Tok 
‚îÄ‚îÄ‚îÄ 
Âë®ÊÅ©Êù• 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Âë®ÊÅ©Êù• 	NER Type   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫PERSON 	Tok 
‚îÄ‚îÄ‚îÄ 
Âë®ÊÅ©Êù• 	Po    3 
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP

 
 
 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠Â∏Ç 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠Â∏Ç 	NER Type     
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫LOCATION 	Tok 
‚îÄ‚îÄ‚îÄ 
Á±≥Ëò≠Â∏Ç 	Po    3     4   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP‚îÄ‚îÄ‚îÄ‚ñ∫FRAG

 
 
 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ê¥õÊùâÁ£ØÁ∏£ 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ê¥õÊùâÁ£ØÁ∏£ 	NER Type     
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫LOCATION 	Toke 
‚îÄ‚îÄ‚îÄ‚îÄ 
Ê¥õÊùâÁ£ØÁ∏£ 	Po    3     4   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP‚îÄ‚îÄ‚îÄ‚ñ∫FRAG

 
 
 	Tok 
‚îÄ‚îÄ‚îÄ 
Âçó‰∫¨Â∏Ç 	Rela 
‚îÄ‚îÄ‚îÄ‚îÄ 
root 	Po 
‚îÄ‚îÄ 
NR 	Tok 
‚îÄ‚îÄ‚îÄ 
Âçó‰∫¨Â∏Ç 	NER Type     
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 
‚îÄ‚îÄ‚îÄ‚ñ∫LOCATION 	Tok 
‚îÄ‚îÄ‚îÄ 
Âçó‰∫¨Â∏Ç 	Po    3     4   
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NR‚îÄ‚îÄ‚îÄ‚ñ∫NP‚îÄ‚îÄ‚îÄ‚ñ∫FRAG
```

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
anyone who need a finer tokenizer,

**Are you willing to contribute it (Yes/No):**
Yes, let me know how

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.10
- HanLP version:
In [87]: hanlp.version
Out[87]: '2.1.0-beta.50'

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Âä†ËΩΩnerÊ®°ÂûãÊä•Èîô,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
‰ΩøÁî®MSRA_NER_BERT_BASE_ZHÊ®°ÂûãË∞ÉÁî®nerÔºå‰ºöÂéª‰∏ãËΩΩÊ®°ÂûãÔºå‰ΩÜÊòØÂä†ËΩΩË∑ØÂæÑ‰∏ãÊ®°ÂûãÊñá‰ª∂ÊòØÂ≠òÂú®ÁöÑ.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
ner = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
sen_ner = tok(sentence)
print(sen_ner)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
OS: Windows-10-10.0.22621-SP0
Python: 3.8.12
PyTorch: 2.0.1+cpu
TensorFlow: 2.13.0
HanLP: 2.1.0-beta.50

**Other info / logs**
Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\PowerChen\work\workspace\Idea\python\hanlp-server\test\server\hanlp_api.py"", line 18, in <module>
    ner_handle = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 186, in load_from_meta_file
    raise e from None
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 106, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 215, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 225, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\components\taggers\transformers\transformer_tagger_tf.py"", line 34, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\layers\transformers\loader_tf.py"", line 11, in build_transformer
    tokenizer = AutoTokenizer_.from_pretrained(transformer)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\hanlp\layers\transformers\pt_imports.py"", line 65, in from_pretrained
    tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\models\auto\tokenization_auto.py"", line 658, in from_pretrained
    config = AutoConfig.from_pretrained(
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 944, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\configuration_utils.py"", line 574, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\configuration_utils.py"", line 629, in _get_config_dict
    resolved_config_file = cached_file(
  File ""C:\PowerChen\work\environment\miniconda3\lib\site-packages\transformers\utils\hub.py"", line 452, in cached_file
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like bert-base-chinese is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
find a category of words that HanLP can't tokenize well: Á±≥Ëò≠ÊòÜÂæ∑Êãâ / ÂîêÁ¥çÂæ∑Â∑ùÊôÆ / Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨ /etc,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
find a category of words (Â§ñÂúã‰∫∫Âêç) that HanLP can't tokenize well: Á±≥Ëò≠ÊòÜÂæ∑Êãâ / ÂîêÁ¥çÂæ∑Â∑ùÊôÆ / Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨ /etc

**Code to reproduce the issue**

```python
In [88]: tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
                                   
In [89]: tok([""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ"", ""‰∏∫‰ªÄ‰πà‰ΩôÂçéËØ¥Á±≥ÂÖ∞¬∑ÊòÜÂæ∑ÊãâÊòØ‰∏âÊµÅÂ∞èËØ¥ÂÆ∂Ôºü"", ""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""ÂîêÁ¥çÂæ∑Â∑ùÊôÆ"", ""Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨""])
Out[89]: 
[['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ'],
 ['‰∏∫‰ªÄ‰πà', '‰ΩôÂçé', 'ËØ¥', 'Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ', 'ÊòØ', '‰∏âÊµÅ', 'Â∞èËØ¥ÂÆ∂', 'Ôºü'],
 ['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['ÂîêÁ¥çÂæ∑Â∑ùÊôÆ'],
 ['Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨']]

In [84]: tok = hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH)
Downloading https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip to /home/ubuntu/.hanlp/tok/fine_electra_small_20220615_231803.zip
100%  43.5 MiB   2.6 MiB/s ETA:  0 s [=============================================================]
Decompressing /home/ubuntu/.hanlp/tok/fine_electra_small_20220615_231803.zip to /home/ubuntu/.hanlp/tok
                                   
In [85]: tok([""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ"", ""‰∏∫‰ªÄ‰πà‰ΩôÂçéËØ¥Á±≥ÂÖ∞¬∑ÊòÜÂæ∑ÊãâÊòØ‰∏âÊµÅÂ∞èËØ¥ÂÆ∂Ôºü"", ""Á±≥Ëò≠ÊòÜÂæ∑Êãâ"", ""ÂîêÁ¥çÂæ∑Â∑ùÊôÆ"", ""Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨""])
Out[85]: 
[['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['Á±≥Ëò≠¬∑ÊòÜÂæ∑Êãâ'],
 ['‰∏∫‰ªÄ‰πà', '‰ΩôÂçé', 'ËØ¥', 'Á±≥ÂÖ∞¬∑ÊòÜÂæ∑Êãâ', 'ÊòØ', '‰∏âÊµÅ', 'Â∞èËØ¥ÂÆ∂', 'Ôºü'],
 ['Á±≥Ëò≠ÊòÜÂæ∑Êãâ'],
 ['ÂîêÁ¥çÂæ∑', 'Â∑ùÊôÆ'],
 ['Á±≥Ê≠áÁàæ¬∑Â••Â∑¥È¶¨']]
```

**Describe the current behavior**
all names seem to be always not cut

**Expected behavior**
those foreign full names should be tokenized between their first name and last name, the FINE_ one seems be able to cut 'ÂîêÁ¥çÂæ∑' 'Â∑ùÊôÆ' but not much better, many other foreign full names are still not cut

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.10
- HanLP version:

In [87]: hanlp.__version__
Out[87]: '2.1.0-beta.50'


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
Unable to run in Docker; Can HanLP have some way to pre-install the pretrained models files? at pip install time?,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
When running inside docker with another username, it can't find the pretrained model files,

**Code to reproduce the issue**
the HanLP is trying to download the pretrained on first-time, this may not work under environments with no Internet access, (e.g. behind a Corporate firewall,)

so I use a little trick to run a this part code in docker building phase, to pre-warm up HanLP to not have to download again, it does download the necessary pretrained models files, and indeed work if run the container with same root username,
```log
Step 22/26 : RUN python3 ./app/text/tokenizer.py
 ---> Running in 241d468253cd
Downloading https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip to /root/.hanlp/tok/fine_electra_small_20220615_231803.zip
100%  43.5 MiB 440.1 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/tok/fine_electra_small_20220615_231803.zip to /root/.hanlp/tok
Downloading https://file.hankcs.com/hanlp/utils/char_table_20210602_202632.json.zip to /root/.hanlp/utils/char_table_20210602_202632.json.zip
100%  26.7 KiB  26.7 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/utils/char_table_20210602_202632.json.zip to /root/.hanlp/utils
Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_small_20210706_125427.zip to /root/.hanlp/transformers/electra_zh_small_20210706_125427.zip
100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]
Decompressing /root/.hanlp/transformers/electra_zh_small_20210706_125427.zip to /root/.hanlp/transformers
```

**Describe the current behavior**
however, it downloaded those files to `/root/.hanlp/...`, and at run-time it often be run under another username, (e.g. AWS Lambda Container use another temporary generated username), then the same code would try to download again to `/home/username/.hanlp/...`

**Expected behavior**
Can HanLP have some way to pre-install the pretrained models files? at pip install time?

Or support a common path like `PRETRAIN_FILES_DIR=/common/path/`, so any user run this would get it from a common path;

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- Python version: `3.10`
- HanLP version: `hanlp==2.1.0b50`  (we use this but this is not HanLP version specific)

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
"
python3.10Êó∂ÔºåtfÊîØÊåÅÁâàÊú¨ÊúÄ‰ΩéÊòØ2.8.0Ôºåsetup‰∏≠Âç¥Âè™ËÉΩÁ≠â‰∫é2.6.0,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
- python3.10Êó∂ÔºåÂÆâË£Ö`extras_require=tf`Êó∂Ôºå`tensorflow==2.6.0`‰∏çËÉΩË¢´Êª°Ë∂≥
https://github.com/hankcs/HanLP/blob/31c34ec86f71fe91f1fe6d86e7ca8575c80e2306/setup.py#L24

- Âõ†‰∏∫[pypi](https://pypi.org/project/tensorflow/2.6.0/#files)‰∏≠ÊúÄÂ§öÊîØÊåÅÂà∞python3.9

    ![image](https://github.com/hankcs/HanLP/assets/28639377/f5a7632b-193b-43f9-8f4b-358c5d1eca4e)

**Expected behavior**
- ÊúüÂæÖÂ∫ìÊ£ÄÊü•Êó∂ÔºåÂèØÈÄÇÂΩìÊîæÊùæÁâàÊú¨ÈôêÂà∂

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- Python version: 3.10
- HanLP version: 2.1.0b49

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
branch:portable - com/hankcs/hanlp/collection/trie/bintrie/BinTrie.javaÂÖ®ÂàáÂàÜÈóÆÈ¢ò,"Âêå:https://bbs.hankcs.com/t/2-4-5/5393
**Describe the bug**
Âú®2.4.5 ÂâçÁºÄÊ†ëÁöÑÂ¶ôÁî®‰ª£Á†Åcom/hankcs/hanlp/collection/trie/bintrie/BinTrie.java
‰∏≠, ÂÅáËÆæËØçÂÖ∏‰∏∫[‰∏≠Âçé, ‰∏≠Âçé‰∫∫"", Âçé‰∫∫], ËæìÂÖ•ÊñáÊú¨‰∏∫[‰∏≠Âçé‰∫∫], ÈÇ£‰πàiËøô‰∏™ÂèòÈáè‰ºö‰∏çÊñ≠Ëá™Â¢û ÊúÄÁªàÂàáÂá∫ÁöÑËØçÂè™Êúâ[‰∏≠Âçé, ‰∏≠Âçé‰∫∫], ‰ΩÜÊòØËøô‰∏™ÊñπÊ≥ïÊòØÂÖ®ÂàáÂàÜÂêóÔºüÊòØÂê¶Â∫îËØ•ÂàáÂá∫[Âçé‰∫∫]Ëøô‰∏™ËØç, ‰∏çÁü•ÈÅìÊòØÂê¶ÊàëÁêÜËß£ÊúâËØØ, ÊÑüË∞¢Ëß£Á≠î

**Code to reproduce the issue**
`public class BinTriedTest {
    public static void main(String[] args) throws IOException {
        // Âä†ËΩΩËØçÂÖ∏
        TreeMap<String, CoreDictionary.Attribute> dictionary =
            IOUtil.loadDictionary(""data/dictionary/TestDictionary.txt"");
        final BinTrie<CoreDictionary.Attribute> binTrie = new BinTrie<CoreDictionary.Attribute>(dictionary);
        List<String> wordlist = segmentFully(""‰∏≠Âçé‰∫∫"", binTrie);
        System.out.println(wordlist);
    }

    public static List<String> segmentFully(final String text, BinTrie<CoreDictionary.Attribute> dictionary)
    {
        final List<String> wordList = new LinkedList<String>();
        dictionary.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
        {
            @Override
            public void hit(int begin, int end, CoreDictionary.Attribute value)
            {
                wordList.add(text.substring(begin, end));
            }
        });
        return wordList;
    }}`
![image](https://github.com/hankcs/HanLP/assets/27196120/6020e0a2-6813-485a-9c1d-dfe16fc9a43c)
![image](https://github.com/hankcs/HanLP/assets/27196120/9a431398-ec6c-44eb-88e8-8f504e10a05a)
![image](https://github.com/hankcs/HanLP/assets/27196120/b7eb537e-955b-4415-94e9-c5be6fa92f0f)


**Describe the current behavior**
![image](https://github.com/hankcs/HanLP/assets/27196120/eecbc8a8-7071-4d31-9ed6-2f97226a19e3)

output: [‰∏≠Âçé, ‰∏≠Âçé‰∫∫]

**Expected behavior**
expected output: [‰∏≠Âçé, ‰∏≠Âçé‰∫∫,Âçé‰∫∫]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:portable

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Âú®‰ΩøÁî®ÊñáÊú¨Áõ∏‰ººÂ∫¶ÊØîËæÉÊó∂Ôºå‰∏§‰∏™Â≠óÁ¨¶‰∏≤‰∫§Êç¢‰∏Ä‰∏ã‰ΩçÁΩÆÔºåÂæóÂá∫Êù•ÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶‰∏ç‰∏ÄÊ†∑,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
Âú®‰ΩøÁî®ÊñáÊú¨Áõ∏‰ººÂ∫¶ÊØîËæÉÊó∂Ôºå‰∏§‰∏™Â≠óÁ¨¶‰∏≤‰∫§Êç¢‰∏Ä‰∏ã‰ΩçÁΩÆÔºåÂæóÂá∫Êù•ÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶‰∏ç‰∏ÄÊ†∑

**Code to reproduce the issue**
![image](https://user-images.githubusercontent.com/77441902/236605940-f5f62acf-d3f8-4a9b-adb1-b2d2edf342d3.png)


```python
```

**Describe the current behavior**
ÁÇπÂáªËøêË°åÂêéÔºåÂèëÁé∞ÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑÂÄº‰∏çÂêå

**Expected behavior**
‰∫§Êç¢Â≠óÁ¨¶‰∏≤ÁöÑ‰ΩçÁΩÆÔºåÂ∫îËØ•ÂæóÂà∞ÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶ÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Create NLP assignment,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ‚ú®.
‚ö†Ô∏èChanges must be made on dev branch.
-->

# Title of Your Pull Request

## Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

## Checklist

Check all items that apply.

- [ ] ‚ö†Ô∏èChanges **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have checked my code and corrected any misspellings
"
ÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁöÑsampler_builder„ÄÅbatch_sizeÂèÇÊï∞,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
ÊåáÂÆöbatch_sizeÂèÇÊï∞‰∏çËµ∑‰ΩúÁî®

**Code to reproduce the issue**
```python
import hanlp
from hanlp import pretrained

tokenizer_model = pretrained.tok.COARSE_ELECTRA_SMALL_ZH
tokenizer = hanlp.load(tokenizer_model, devices=0)
constituency_model = pretrained.constituency.CTB9_CON_ELECTRA_SMALL
parser = hanlp.load(constituency_model)

s = ""Ê†èÁõÆ‰∏ãÁöÑ‰∏Ä‰∏™ËäÇÁõÆÔºåËøô‰∏™ËäÇÁõÆ‰∏ªË¶ÅÊé¢ËÆ®ÁöÑÊòØËã±ËØ≠Âè£ËØ≠ÁöÑÂ∏∏Áî®Ë°®Ëææ„ÄÅËØçÊ±áÁöÑËµ∑Ê∫êÂíåÂÖ∂ËÉåÂêéÁöÑÊïÖ‰∫ã„ÄÇ""
sentences = [s] * 100
tokens = tokenizer(sentences, batch_size=bs)
parseing_tree = parser(batch_tokens, batch_size=bs)
```

**Describe the current behavior**
ÊåáÂÆöbatch_sizeÊòØ‰∏çÁîüÊïàÁöÑÔºå‰ºöËá™Âä®Âä†ËΩΩconfig‰∏≠ÁöÑsampler_builder„ÄÇ
sampler_builder‰∏≠Ôºö
- TokenizerÁöÑbatch_sizeÊòØ32
- constituencyÁöÑbatch_sizeÊòØ10

ÂØπ‰∫éTokenizerÔºåÊ∑ªÂä†`sampler_builder=None`Â∞±ÂèØ‰ª•‰Ωøbatch_sizeÁîüÊïà: 
```
tokens = tokenizer(sentences, sampler_builder=None, batch_size=bs)
```
ÂØπ‰∫éconstituencyÔºåÂç≥‰ΩøÊ∑ªÂä†`sampler_builder=None`Ôºå‰πü‰∏ç‰ºö‰Ωøbatch_sizeÁîüÊïàÔºå
```
parseing_tree = parser(batch_tokens, sampler_builder=None, batch_size=bs)
```

**Expected behavior**
ÂØπ‰∫éTokenizerÂíåconstituencyÔºö
- ÊåáÂÆö‰∫Übatch_sizeÂèÇÊï∞Â∞±Ëµ∑‰ΩúÁî®Ôºå‰∏çÈúÄË¶ÅÂíåsampler_builder=NoneÈÖçÂêà
     - ÔºàÂç≥ÂΩìÊåáÂÆö‰∫Übatch_sizeÂ∞±Ë¶ÅÂ±èËîΩconfig‰∏≠ÁöÑsampler_builderÔºâ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 LTS
- Python version: Python 3.8.16
- HanLP version: 2.1.0b48

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
ÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁöÑÊ†áÊ≥®Ê†áÂáÜ,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
‰∏•Ê†ºÊÑè‰πâ‰∏äÔºåËøô‰∏çÁÆó‰∏Ä‰∏™BugÔºåÊòØ‰∏Ä‰∏™ÈóÆÈ¢ò„ÄÇÔºàhttps://bbs.hankcs.com/ ‰∏äÊ≤°ÊúâÊ≥®ÂÜåÊàêÂäüÔºâ

Github Readme‰∏≠ÔºåËØ¥ÊòéÁöÑ‚ÄúÊàêÂàÜÂè•Ê≥ïÂàÜÊûê‚ÄùÁöÑÊ†áÊ≥®Ê†áÂáÜÊòØChinese Tree BankÔºöhttps://hanlp.hankcs.com/docs/annotations/constituency/ctb.html
‰∏äËø∞È°µÈù¢‰∏äÁöÑÊâÄÊúâTagÔºö
```
'ADJP', 'ADVP', 'CLP', 'CP', 'DNP', 'DP', 'DVP', 'FRAG', 'INTJ',
'IP', 'LCP', 'LST', 'MSP', 'NN', 'NP', 'PP', 'PRN', 'QP', 'ROOT', 'UCP',
'VCD', 'VCP', 'VNV', 'VP', 'VPT', 'VRD', 'VSB'
```

ËÄåÊàëÂú®Ëá™Â∑±Êï∞ÊçÆÈõÜ‰∏äÁîüÊàêÁöÑÊâÄÊúâTagÔºö
```
'CLP', 'QP', 'NP', 'ADJP', 'ADVP', 'IP', 'VP', 'PP', 'VSB',
'TOP', 'DP', 'LCP', 'CP', 'DNP', 'PRN', 'DVP', 'VRD', 'VCP',
'UCP', 'VCD', 'VPT', 'VNV', 'DFL', 'FLR', 'INTJ', 'LST', 'INC',
'EMO', 'FRAG', 'SKIP', 'IMG', 'IP=9', 'OTH', 'NP=2', 'NP=3', 'TYPO', 'IP=4'
```
ÂÆûÈôÖÁîüÊàêÁöÑTag‰∏éÁΩëÈ°µ‰∏äÁöÑ‰∏ç‰∏ÄËá¥Ôºö
- ROOT -> TOP
- Ê≤°ÊúâÁîüÊàêËøá'MSP'Âíå'NN'ÔºàÊï∞ÊçÆÈáè‰∏çÂ∞èÔºâ
- ÁîüÊàêÁöÑ'DFL'„ÄÅ'EMO'„ÄÅ'FLR'„ÄÅ'IMG'„ÄÅ'INC'„ÄÅ'IP=4'„ÄÅ'IP=9'„ÄÅ'NP=2'„ÄÅ'NP=3'„ÄÅ'OTH'„ÄÅ'SKIP'„ÄÅ'TYPO'‰∏çÂú®ÁΩëÈ°µ‰∏ä


**Code to reproduce the issue**
None

**Describe the current behavior**
None

**Expected behavior**
Â∏åÊúõÂæóÂà∞‰∏Ä‰∏™ÊâÄÊúâÂèØËÉΩÁîüÊàêÁöÑTagÔºàÊ†áÁ≠æÔºâÂàóË°®„ÄÇ
Êõ¥Êñ∞ÁΩëÈ°µ‰∏äÁöÑÊ†áÊ≥®Ê†áÂáÜ„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): None
- Python version: None
- HanLP version: None

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
ÊàêÂàÜÂè•Ê≥ïÂàÜÊûê‰ºö‰øÆÊîπÂéüÂßãÊñáÊú¨ÔºåÂ¶Ç‰øÆÊîπ‰∏≠ÊñáÁöÑÊ†áÁÇπÁ¨¶Âè∑Ôºü,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**ÊàêÂàÜÂè•Ê≥ïÂàÜÊûê‰øÆÊîπÂéüÂßãÊñáÊú¨**

A clear and concise description of what the bug is.

**Code to reproduce the issue**
```python
import hanlp
from hanlp import pretrained

tokenizer_model = pretrained.tok.COARSE_ELECTRA_SMALL_ZH
tokenizer = hanlp.load(tokenizer_model)

toks = tokenizer(""Ê†èÁõÆ‰∏ãÁöÑ‰∏Ä‰∏™ËäÇÁõÆÔºåËøô‰∏™ËäÇÁõÆ‰∏ªË¶ÅÊé¢ËÆ®ÁöÑÊòØËã±ËØ≠Âè£ËØ≠ÁöÑÂ∏∏Áî®Ë°®Ëææ„ÄÅËØçÊ±áÁöÑËµ∑Ê∫êÂíåÂÖ∂ËÉåÂêéÁöÑÊïÖ‰∫ã„ÄÇ"")
print(toks)

constituency_model = pretrained.constituency.CTB9_CON_ELECTRA_SMALL
parser = hanlp.load(constituency_model)
tree = parser(toks)
print(tree)
```

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/53685945/230709750-3b17e2c4-4eb3-4fa2-a491-4da61f79a4d3.png)
‰∏∫‰∫ÜÂ•ΩÂ±ïÁ§∫Ôºå‰øÆÊîπ‰∫ÜtokenÊòØËæìÂá∫Ê†ºÂºèÔºà‰∏∫‰∫Ü‰ª£Á†ÅÁöÑÁÆÄÊ¥ÅÔºåÊ≤°ÊúâÂåÖÂê´Âú®ÊºîÁ§∫‰ª£Á†Å‰∏≠Ôºâ



**Expected behavior**
‰øùÊåÅÂéüÊù•ÁöÑ‰∏≠ÊñáÊ†áÁÇπ„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.9.2009 (Core)
- Python version: Python 3.8.16
- HanLP version: 2.1.0b47

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Âä†ËΩΩMSRA_NER_ALBERT_BASE_ZHÂ§±Ë¥•,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
‰ΩøÁî®pip install --upgrade hanlp[full]ÂÆâË£ÖÊúÄÊñ∞ÁâàÊú¨ÁöÑhanlpÔºåÂπ∂‰ΩøÁî®ner = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)ËøõË°åÊ®°ÂûãÂä†ËΩΩÔºå‰ΩÜÊä•Â¶Ç‰∏ãÈîôËØØÔºö
![image](https://user-images.githubusercontent.com/20340633/227485545-c2209a84-f2f8-4be4-afa7-dfbac81b528d.png)


**Expected behavior**
ÊúüÊúõËÉΩÊ≠£Â∏∏Âä†ËΩΩ

**System information**
- OS Platform and Distribution:inux-5.4.0-139-generic-x86_64-with-glibc2.27
- Python version:3.8.0
- HanLP version:2.1.0-beta.46

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Ë™øÁî®Pretrained TOKË∑üNERÁöÑÊôÇÂÄôÂ†±ÈåØ,"**Describe the bug**
Ë™øÁî®ÈÉ®ÂàÜPretrained TOKË∑üNERÁöÑÊôÇÂÄôÂ†±ÈåØÔºåÂ∑≤Ë©¶ÈÅéÁõ¥Êé•Ë™øÁî®Â∑≤‰∏ãËºâÂà∞Êú¨Âú∞ÁöÑModel‰πüÊòØÂá∫ÁèæÁõ∏ÂêåÁöÑÂ†±ÈåØ
NER:
hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_CASED_EN
hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH
hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH
TOK:
hanlp.pretrained.tok.CTB6_CONVSEG
hanlp.pretrained.tok.LARGE_ALBERT_BASE
hanlp.pretrained.tok.PKU_NAME_MERGED_SIX_MONTHS_CONVSEG
hanlp.pretrained.tok.SIGHAN2005_MSR_CONVSEG
hanlp.pretrained.tok.SIGHAN2005_PKU_CONVSEG

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
NER_1 = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(hanlp.load('UD_TOK_MMINILMV2L12'), output_key='tok') \
    .append(hanlp.load('CONLL03_NER_BERT_BASE_CASED_EN'), output_key='ner', input_key='tok')
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS: Windows-10-10.0.19041-SP0
- Python: 3.9.0
- PyTorch: 1.13.1+cpu
- HanLP version: hanlp==2.1.0b45

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Decompressing C:\Users\AppData\Roaming\hanlp\ner/ner_conll03_bert_base_cased_en_20211227_121443.zip to C:\Users\AppData\Roaming\hanlp\ner
Failed to load https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_cased_en_20211227_121443.zip
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
......
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Êñá‰ª∂ÊµÅÊú™Ê≠£Á°ÆÂÖ≥Èó≠,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
A clear and concise description of what the bug is.
VectorsReaderÁ±ªÁöÑreadVectorFile()ÊñπÊ≥ïÊú™Ê≠£Á°ÆÂÖ≥Èó≠Êñá‰ª∂ÊµÅÔºåÂØºËá¥ËµÑÊ∫êÊ≥ÑÊºè
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
‰ΩøÁî®Word2VecTrainerÁöÑtrainÊñπÊ≥ïÔºåÊàñËÄÖnew ‰∏Ä‰∏™WordVectorModelÂØπË±°ÔºåËøõÁ®ãËøõË°å‰∏≠ÔºåÂà†Èô§Ê®°ÂûãÊñá‰ª∂ÊØîÂ¶Çmodel.txtÔºå‰ºöÊó†Ê≥ïÂà†Èô§
```java
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.
ÊàëÂú®‰ΩøÁî®ËØ•jarËøõË°åËØçÂêëÈáèÊ®°ÂûãËÆ≠ÁªÉÊàñËÄÖÊñáÊ°£ËΩ¨Êç¢‰∏∫ÂêëÈáèÂêéÔºåÊ®°ÂûãÊñá‰ª∂ÂßãÁªàÂà†Èô§‰∏çÊéâÔºåÊñá‰ª∂ËµÑÊ∫êÁßØÂéã
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):‰ªª‰ΩïÁ≥ªÁªüÈÉΩ‰ºöÂá∫Áé∞
- Python version:
- HanLP version:ÁõÆÂâçÂèëÁé∞1.5-1.8.3ÁöÑÁâàÊú¨ÈÉΩ‰ºöÂá∫Áé∞ËØ•ÈóÆÈ¢ò

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[VectorsReader.java](https://github.com/hankcs/HanLP/blob/v1.8.3/src/main/java/com/hankcs/hanlp/mining/word2vec/VectorsReader.java)
* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
Â¢ûÂä†Âø´ÈÄüÊûÑÂª∫ÈÖçÁΩÆ,"<!--
Thank you for being interested in contributing to HanLP! You are awesome ‚ú®.
‚ö†Ô∏èChanges must be made on dev branch.
-->

# Â¢ûÂä†Âø´ÈÄüÊûÑÂª∫ÈÖçÁΩÆ

## Description

Â¢ûÂä†Âø´ÈÄüÊûÑÂª∫ÈÖçÁΩÆÔºöenableFastBuildÔºåÊâìÂºÄÂêéÊûÑÂª∫Êó∂nextCheckPos‰ºöÂä†1Ôºå‰∫ßÁîüÊõ¥Â§öÁ©∫Èó≤Á©∫Èó¥‰ª•Èôç‰ΩéÊûÑÂª∫ËÄóÊó∂„ÄÇ

Fixes https://github.com/hankcs/HanLP/issues/1801

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

ËØ¶ËßÅÂçïÊµãDoubleArrayTrieTest.testEnableFastBuildÂíåAhoCorasickDoubleArrayTrieTest.testEnableFastBuild

## Checklist

Check all items that apply.

- [x] ‚ö†Ô∏èChanges **must** be made on `dev` branch instead of `master`
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
Â∏åÊúõÂ¢ûÂä†tok‰øùÂ≠òÁ©∫Ê†ºÁöÑÈÄâÈ°πÔºå‰ª•‰æøÂàÜËØçÂêéËøòÂéüÊñáÊú¨,"**Describe the feature and the current behavior/state.**

ÊñáÊú¨ÁöÑÁ©∫Ê†ºÔºàÂÖ®ÂΩ¢ÂíåÂçäÂΩ¢Ôºâ‰ºöÂú®tokËàçÂºÉ

**Will this change the current api? How?**

‰∏çÁü•ÈÅì

**Who will benefit with this feature?**

‰ΩøÁî®ÁÆÄÁπÅËΩ¨Êç¢ÁöÑ‰∫∫

**Are you willing to contribute it (Yes/No):**

ÂäõÊúâ‰∏çÈÄÆ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Python version: 3.10.9
- HanLP version: 2.1.0b45ÔºåÁî®`pip install hanlp`ÂÆâË£Ö

**Any other info**


Êàë‰∏ªË¶ÅÊòØÊÉ≥Áî®hanlpÊù•ËøõË°åÊñáÊú¨ÁÆÄÁπÅËΩ¨Êç¢

Âõ†‰∏∫openccÁöÑÁÆÄÁπÅËΩ¨Êç¢ÊúâÊó∂‰ºöÂá∫Áé∞ÈóÆÈ¢òÔºà‰æãÂ¶Ç`Âè™`Âíå`Èöª`ÁöÑËΩ¨Êç¢Ôºâ
Âú®ÂÖ∂github [#224 (comment)](https://github.com/BYVoid/OpenCC/issues/224#issuecomment-283668276)ÁöÑËÆ®ËÆ∫‰∏≠ÔºåÁúãÂà∞Êúâ‰∫∫‰ΩøÁî®HanLPÂàÜËØçÂÜç‰∏¢Áªôopencc
ÊâÄ‰ª•ËØï‰∫Ü‰∏ÄÊï¥Â§©ÔºåÊÑüËßâ‰∏çÈîô
‰ΩÜÊòØÂõ†‰∏∫tokÊú™ËÉΩ‰øùÂ≠òÁ©∫Ê†º‰ª•ÊñáÊú¨Êú™ËÉΩÊàêÂäüËøòÂéü

‰æãÂ≠ê

```python
import hanlp
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
print(tok(['2021Âπ¥HanLPv2.1‰∏∫Áîü‰∫ßÁéØÂ¢ÉÂ∏¶Êù•Ê¨°‰∏ñ‰ª£ÊúÄÂÖàËøõÁöÑÂ§öËØ≠ÁßçNeuro-linguistic programmingÊäÄÊúØ„ÄÇ', 'ÈòøÂ©Ü‰∏ªÊù•Âà∞Âåó‰∫¨Á´ãÊñπÂ∫≠ÂèÇËßÇËá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏„ÄÇ']))
```
ËæìÂá∫‰∏∫Ôºö
```python
[['2021Âπ¥', 'HanLPv2.1', '‰∏∫', 'Áîü‰∫ß', 'ÁéØÂ¢É', 'Â∏¶Êù•', 'Ê¨°‰∏ñ‰ª£', 'ÊúÄ', 'ÂÖàËøõ', 'ÁöÑ', 'Â§ö', 'ËØ≠Áßç', 'Neuro-linguistic', 'programming', 'ÊäÄÊúØ', '„ÄÇ'], ['ÈòøÂ©Ü', '‰∏ª', 'Êù•Âà∞', 'Âåó‰∫¨Á´ãÊñπÂ∫≠', 'ÂèÇËßÇ', 'Ëá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏', '„ÄÇ']]
```

`Neuro-linguistic programming` ‰∏§‰∏™ËØç‰∏≠ÁöÑÁ©∫Ê†ºÊ∂àÂ§±‰∫Ü
ÊääËøôÊÆµËæìÂá∫‰∏¢ÁªôopenccÂÜçËøòÂéüÂêé
Â∞±‰ºöÂèòÊàê`Neuro-linguisticprogramming`

Âõ†‰∏∫ÊàëÁºñÁ®ãËÉΩÂäõÊûÅÂ∫¶ÊúâÈôê
Áé∞Âú®ÊàëÂè™ÊòØ‰ΩøÁî®pythonËØªÂèñtxtÊ°£
ÂÜçÂÉè‰∏äÈù¢ÈÇ£Ê†∑pythonÁöÑhanlpÁöÑtokÂàÜËØç
ÂÜç‰ΩøÁî®json.dumpsÊéâËøõterminal
Âú®terminalÁî®`opencc`ËøõË°åÁÆÄÁπÅËΩ¨Êç¢
ÂÜç‰ΩøÁî®`jq`,`sed`Á≠âÂ∑•ÂÖ∑ËøòÂéüÊñáÊú¨

ÊàñËÄÖÊúâÊ≤°Êúâ‰ªÄ‰πàÊõ¥ÊúâÊïàÁöÑÂàÜËØçÁÆÄÁπÅËΩ¨Êç¢ÊñπÊ≥ïÔºü
Ë∞¢Ë∞¢ÔºÅ


* [x] I've carefully completed this form."
‰øÆÊîπ‰∏ÄË°å‰ª£Á†ÅÂèØ‰ª•ËÆ©DATÊûÑÂª∫ÈÄüÂ∫¶ÊèêÂçáNÂÄç,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**

https://github.com/hankcs/HanLP/blob/1323221c38e9188b19cef3f770eec40148a459ac/src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java#L167

`int pos = Math.max(siblings.get(0).code + 1, nextCheckPos) - 1;`
ÊîπÊàê
`int pos = Math.max(siblings.get(0).code, nextCheckPos);`

ÊûÑÂª∫ÈÄüÂ∫¶ÂèØ‰ª•ÊèêÂçáÂæàÂ§öËÄå‰∏îË°®Áé∞Á®≥ÂÆöÔºåÁº∫ÁÇπÊòØÊúÄÁªàÊûÑÂª∫Âá∫ÁöÑDATÂ§ßÂ∞èÂæÆÂ¢û„ÄÇ‰∏ãÈù¢ÊòØÊàëÁöÑÊµãËØïÊï∞ÊçÆÔºö

<img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1414826/213084924-2dd7248d-d0ee-4476-b110-74b93be7a1d9.png"">

ÂÖ∂‰∏≠‰øùÁïônextCheckPos‰∏∫ÂéüÁâà‰ª£Á†ÅÔºåÂéªÊéânextCheckPosÁî®‰ΩúÂØπÊØîÔºåÊ†∏ÂøÉÂæ™ÁéØËÆ°Êï∞ÊòØÂú®Âæ™ÁéØ‰ΩìÂÜÖÂÅö‰∫Ü‰∏Ä‰∏™`count++`ËÆ°Êï∞Ôºö
![image](https://user-images.githubusercontent.com/1414826/213085116-83c7c0c8-fa47-4b14-95fa-9307af0d067a.png)


**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Anyone who uses DAT

**Are you willing to contribute it (Yes/No):**

Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur
- Python version:
- HanLP version: hanlp-portable-1.8.3.jar

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
‰∏∫ÂàÜËØçÂô®ÊåáÂÆö‰∏Ä‰ªΩ‰∏çÂêåÁöÑËØçÂÖ∏‰∏çÁîüÊïà,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
https://github.com/hankcs/HanLP/blob/1.x/src/test/java/com/hankcs/demo/DemoCustomDictionary.java
‰ª£Á†Å72Ë°åÔºåËØçÂÖ∏ÊåáÂÆö‰∏çÁîüÊïà

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Java
// ÊØè‰∏™ÂàÜËØçÂô®ÈÉΩÊúâ‰∏Ä‰ªΩËØçÂÖ∏ÔºåÈªòËÆ§ÂÖ¨Áî® CustomDictionary.DEFAULTÔºå‰Ω†ÂèØ‰ª•‰∏∫‰ªª‰ΩïÂàÜËØçÂô®ÊåáÂÆö‰∏Ä‰ªΩ‰∏çÂêåÁöÑËØçÂÖ∏
        DynamicCustomDictionary myDictionary = new DynamicCustomDictionary(""data/dictionary/custom/CustomDictionary.txt"", ""data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt"");
```

**Describe the current behavior**
A clear and concise description of what happened.
Êó†Ê≥ïÊåáÂÆö‰∏çÂêåÁöÑËØçÂÖ∏ÔºåÊ∫ê‰ª£Á†ÅËØçÂÖ∏Âä†ËΩΩÂè™Âä†ËΩΩ`HanLP.Config.CustomDictionaryPath`ÈÖçÁΩÆÁõÆÂΩï‰∏ãÁöÑÁî®Êà∑ËØçÂÖ∏ÔºåËØ•ÊñπÊ≥ï‰º†ÂèÇÊú™‰ΩøÁî®
![image](https://user-images.githubusercontent.com/71128756/212274068-98991256-c1bc-46dd-9d29-351f5e6bf5c0.png)
`src/main/java/com/hankcs/hanlp/dictionary/DynamicCustomDictionary.loadMainDictionary`„ÄÇ
Ëé∑ÂèñÊú¨Âú∞ËØçÂÖ∏Êõ¥Êñ∞Áä∂ÊÄÅ‰πüÂè™‰ªé`HanLP.Config.CustomDictionaryPath`ÁõÆÂΩïÂà§Êñ≠ÔºåÊú™‰ΩøÁî®ÊñπÊ≥ïÂèÇÊï∞„ÄÇ
`src/main/java/com/hankcs/hanlp/dictionary/DynamicCustomDictionary.isDicNeedUpdate`„ÄÇ
![image](https://user-images.githubusercontent.com/71128756/212274394-d7b09aca-2378-4685-9fb4-1783d0ce6a1d.png)


**Expected behavior**
Â∏åÊúõÊåáÂÆöËØçÂÖ∏ÂèØ‰ª•ÁîüÊïà

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- JDK version:1.8
- HanLP version:portable-1.8.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
‰∫∫ÂêçËØÜÂà´ÂØπÂßì‚ÄúÂº†‚ÄùËØÜÂà´‰∏çÂ§™ÂáÜÁ°Æ,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
ÊäΩÂèñ‰∫Ü‰∏Ä‰∫õÁü≠ËØ≠ÂèëÁé∞Âº†ÁâπÂà´ÂÆπÊòìÊ≤°ËØÜÂà´Âá∫Êù•„ÄÇ
Â¶Ç‰∏ãÊòØÂÖ∑‰ΩìÁöÑ‰æãÂ≠ê
Âº†ÂÖàÁîüÂØπÊé•ÂüéË•ø ÂàÜËØç: [Âº†ÂÖàÁîü/nz, ÂØπÊé•/v, ÂüéË•ø/d]
Âº†ÂÖàÁîüÂºÄÂ∞Å ÂàÜËØç: [Âº†ÂÖàÁîü/nz, ÂºÄÂ∞Å/ns]
Âº†ÈòøÂß® ÂàÜËØç: [Âº†/q, ÈòøÂß®/n]

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```   Segment segment = HanLP.newSegment().enableAllNamedEntityRecognize(true);
        List<Term> list = segment.seg(str);
        log.info(""##{} ÂàÜËØç: {}"", str, ArrayUtils.toString(list));
        CoreStopWordDictionary.apply(list);
```

**Describe the current behavior**
ÂæàÂ§öËØÜÂà´Âá∫Êù•

**Expected behavior**
Âº†ÂÖàÁîü ËØÜÂà´Âá∫ Âº† nrÔºåÊàñËÄÖ Âº†ÂÖàÁîü nr

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mocos 11.3.1
- Java version: 8
- HanLP version: portable-1.8.3

**Other info / logs**
Êó†

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
loadÂáΩÊï∞Âá∫Áé∞ÈîôËØØÊó•Âøó,"<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
Âä†ËΩΩ‰∏ç‰∫Ü‰∏ãËΩΩÂà∞Êú¨Âú∞Ë∑ØÂæÑÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå‰∏çÁü•ÈÅìÊòØÂê¶ÊòØÁâàÊú¨ÈóÆÈ¢òÔºü

**Code to reproduce the issue**
```
import hanlp

tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
```

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**System information**
OS: Linux-4.4.0-210-generic-x86_64-with-debian-buster-sid
Python: 3.7.13
PyTorch: 1.13.0+cu117
HanLP: 2.1.0-beta.44
---------------------------------------------------------------------------
BadZipFile                                Traceback (most recent call last)
/tmp/ipykernel_25466/3441843643.py in <module>
      1 import hanlp
      2 
----> 3 tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
      4 tok(['ÂïÜÂìÅÂíåÊúçÂä°„ÄÇ', 'ÊôìÁæéÁÑ∞Êù•Âà∞Âåó‰∫¨Á´ãÊñπÂ∫≠ÂèÇËßÇËá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏'])

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    180         except:
    181             pass
--> 182         raise e from None
    183 
    184 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    104             else:
    105                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
--> 106                     obj.load(save_dir, verbose=verbose, **kwargs)
    107                 else:
    108                     obj.load(metapath, **kwargs)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/torch_component.py in load(self, save_dir, devices, verbose, **kwargs)
    171         if devices is None and self.model:
    172             devices = self.devices
--> 173         self.load_config(save_dir, **kwargs)
    174         self.load_vocabs(save_dir)
    175         if verbose:

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/torch_component.py in load_config(self, save_dir, filename, **kwargs)
    123         for k, v in self.config.items():
    124             if isinstance(v, dict) and 'classpath' in v:
--> 125                 self.config[k] = Configurable.from_config(v)
    126         self.on_config_ready(**self.config, save_dir=save_dir)
    127 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp_common/configurable.py in from_config(config, **kwargs)
     30             return cls(**deserialized_config)
     31         else:
---> 32             return cls.from_config(deserialized_config)
     33 
     34 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/transform.py in from_config(cls, config)
    256         config = dict(config)
    257         config.pop('classpath')
--> 258         return cls(**config)
    259 
    260 

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/common/transform.py in __init__(self, mapper, src, dst)
    478         self.mapper = mapper
    479         if isinstance(mapper, str):
--> 480             mapper = get_resource(mapper)
    481         if isinstance(mapper, str):
    482             self._table = load_json(mapper)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in get_resource(path, save_dir, extract, prefix, append_location, verbose)
    339             path = realpath
    340     if extract and compressed:
--> 341         path = uncompress(path, verbose=verbose)
    342         if anchor:
    343             path = path_join(path, anchor)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in uncompress(path, dest, remove, verbose)
    258                 elif os.path.isdir(prefix):
    259                     shutil.rmtree(prefix)
--> 260             raise e
    261     if remove:
    262         remove_file(path)

~/anaconda3/envs/topic/lib/python3.7/site-packages/hanlp/utils/io_util.py in uncompress(path, dest, remove, verbose)
    223     else:
    224         try:
--> 225             with zipfile.ZipFile(path, ""r"") if ext == '.zip' else tarfile.open(path, 'r:*') as archive:
    226                 if not dest:
    227                     namelist = sorted(archive.namelist() if file_is_zip else archive.getnames())

~/anaconda3/envs/topic/lib/python3.7/zipfile.py in __init__(self, file, mode, compression, allowZip64, compresslevel)
   1256         try:
   1257             if mode == 'r':
-> 1258                 self._RealGetContents()
   1259             elif mode in ('w', 'x'):
   1260                 # set the modified flag so central directory gets written

~/anaconda3/envs/topic/lib/python3.7/zipfile.py in _RealGetContents(self)
   1323             raise BadZipFile(""File is not a zip file"")
   1324         if not endrec:
-> 1325             raise BadZipFile(""File is not a zip file"")
   1326         if self.debug > 1:
   1327             print(endrec)

BadZipFile: File is not a zip file

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
fix(sec): upgrade com.fasterxml.jackson.core:jackson-databind to 2.14.0-rc1,"### What happenedÔºü
There are 1 security vulnerabilities found in com.fasterxml.jackson.core:jackson-databind 2.13.2.2
- [CVE-2022-42004](https://www.oscs1024.com/hd/CVE-2022-42004)


### What did I doÔºü
Upgrade com.fasterxml.jackson.core:jackson-databind from 2.13.2.2 to 2.14.0-rc1 for vulnerability fix

### What did you expect to happenÔºü
Ideally, no insecure libs should be used.

### The specification of the pull request
[PR Specification](https://www.oscs1024.com/docs/pr-specification/) from OSCS"
HanLPËØ≠‰πâÁõ∏‰ººÂ∫¶ÔºåÂ∏åÊúõÂèØ‰ª•ËæìÂá∫Âè•Â≠êÁöÑembedding‰ª•‰æøÂÅöÂ≠òÂÇ®ÔºåÊèêÈ´òÊïàÁéá,"**Describe the feature and the current behavior/state.**
ÂΩìÂâç‰ΩøÁî®stsÔºåËæìÂÖ•‰∏§‰∏™Âè•Â≠êÔºåÂØπ‰∫éÂ§ßÈáèÂè•Â≠êÊØîËæÉÔºåÊïàÁéáÂ§™‰ΩéÔºåËôΩÁÑ∂ÂèØ‰ª•batchÊù•ÂÅöÔºå‰ΩÜÊïàÁéáËøòÊòØ‰∏çÂ§ü

**Will this change the current api? How?**
ÂèØ‰ª•Âú®stsÈáåÂ¢ûÂä†‰∏Ä‰∏™ËæìÂá∫

**Who will benefit with this feature?**
sts‰ΩøÁî®ËÄÖ

**Are you willing to contribute it (Yes/No):**
No

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
HanLPËØ≠‰πâÁõ∏‰ººÂ∫¶ÊØîËæÉÁöÑÊïàÊûú‰∏çÈîôÔºåÈùûÂ∏∏ÊÑüË∞¢‰ΩúËÄÖÁöÑË¥°ÁåÆÔºå‰ΩÜÁé∞Âú®ÊúâÂ§ßÈáèÂè•Â≠êÈúÄË¶ÅÊØîËæÉÔºåÂ∏åÊúõHanLPËÉΩÂ¢ûÂä†ËæìÂá∫Âè•Â≠êembeddingÁöÑÂäüËÉΩÔºåÂÖàÂ≠òÂÇ®Ôºå‰ΩøÁî®Êó∂ÁÆócosË∑ùÁ¶ªÔºåÊèêÈ´òÂÆûÈôÖ‰ΩøÁî®‰∏≠ÁöÑÊØîËæÉÊïàÁéá

* [x] I've carefully completed this form."
"Java API, com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer is not serializable.","**Describe the bug**
Java API, while com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.class be used as a member variable in Spark, it throws ""Caused by: java.io.NotSerializableException: com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer"".
That is because spark try to serialize `PerceptronLexicalAnalyzer `, but this class didn`t implements Serializable interface.
**Code to reproduce the issue**

public class Test{
    private final PerceptronLexicalAnalyzer analyzer;
    public Test() throws IOException {
        analyzer = new PerceptronLexicalAnalyzer(""./cws.bin"",
                ""./pos.bin"",
                ""./ner.bin"");
    }
    public static void main(String[] args) throws IOException {
        Test a = new Test();
        try {
            FileOutputStream fileOut = new FileOutputStream(""./test.ser"");
            ObjectOutputStream out = new ObjectOutputStream(fileOut);
            out.writeObject(a);
            out.close();
            fileOut.close();
            System.out.println(""Serialized data is saved in ./test.ser"");
        } catch (IOException i) {
            i.printStackTrace();
        }
    }
}

**Describe the current behavior**
Caused by: java.io.NotSerializableException: com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.

**Expected behavior**
No Exception been threw.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- HanLP version: portable-1.8.3


* [x] I've completed this form and searched the web for solutions."
Êåá‰ª£Ê∂àËß£ÁöÑÁÆóÊ≥ïÔºèÊ®°Âûã‰ªÄ‰πàÊó∂ÂÄô‰ºöÂºÄÊ∫êÔºü,"**Describe the feature and the current behavior/state.**
Êåá‰ª£Ê∂àËß£ÁÆóÊ≥ïÔºèÊ®°ÂûãÁöÑÂºÄÊ∫êÔºåÁõÆÂâçÂè™Êúârest api

**Will this change the current api? How?**
‰∏ç‰ºö

**Who will benefit with this feature?**
ÈúÄË¶ÅÂ§ßÈáèÊâßË°å‰ªªÂä°ÁöÑÁî®Êà∑

**Are you willing to contribute it (Yes/No):**
No

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
HanLPÁöÑÊåá‰ª£Ê∂àËß£ÊïàÊûúÂ§™ÊÉä‰∫∫‰∫ÜÔºå‰∏çÁü•ÈÅìÊòØÂõ†‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆÁöÑËßÑÊ®°Â§ß‰∏éÈ´òË¥®ÈáèÔºåËøòÊòØÁÆóÊ≥ïÁöÑÊîπËøõÔºü
ËôΩÁÑ∂Âç≥‰Ωø‰∏çÂºÄÊ∫êÔºåÊúâÂÖçË¥πÁöÑapiË∞ÉÁî®Ôºå‰πüÈùûÂ∏∏‰∏çÈîô‰∫Ü„ÄÇ‰ΩÜÂ¶ÇÊûúËÉΩËØ¶ÁªÜÂú∞‰ªãÁªç‰∏Ä‰∏ãÊï∞ÊçÆÂíåÁÆóÊ≥ïÁõ∏ÂÖ≥ÁöÑÁªÜËäÇÔºåÈÇ£Â∞±Â§™Â•Ω‰∫Ü„ÄÇ

* [x] I've carefully completed this form."
"SpringBootÂä†ËΩΩÁõ∏ÂØπË∑ØÂæÑdata,Êä•Êï∞ÁªÑË∂äÁïåÂºÇÂ∏∏","<!--
ÊÑüË∞¢ÊâæÂá∫bugÔºåËØ∑ËÆ§ÁúüÂ°´ÂÜô‰∏ãË°®Ôºö
-->

**Describe the bug**
SpringBoot‰ΩøÁî®portable-1.8.3ÁâàÊú¨,‰øÆÊîπ‰∫ÜrootË∑ØÂæÑ‰∏∫Áõ∏ÂØπË∑ØÂæÑ,ÊîæÁΩÆÂú®‰∫Üresources/nlpÁõÆÂΩï,Ëá™ÂÆö‰πâ‰∫ÜIOAdapter,‰ΩøÁî®NLPTokenizerÊó∂Êä•Èîô,Êä•Èîô‰ø°ÊÅØÂ¶Ç‰∏ã
```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.<clinit>(AbstractLexicalAnalyzer.java:57)
	at com.hankcs.hanlp.tokenizer.NLPTokenizer.<clinit>(NLPTokenizer.java:39)
	at com.holly.top.springframework.config.TestNlp.main(TestNlp.java:21)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 32621
	at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToChar(ByteUtil.java:255)
	at com.hankcs.hanlp.corpus.io.ByteArray.nextChar(ByteArray.java:87)
	at com.hankcs.hanlp.dictionary.other.CharType.<clinit>(CharType.java:94)
	... 3 more
```

**Code to reproduce the issue**
```
public static void main(String[] args) {
        List<Term> ff=NLPTokenizer.segment(""ÊàëÁöÑÂêçÂ≠óÂè´ÈÇìÊ¨£Èõ®"");
        for (Term term : ff) {
            System.out.println(term);
        }
    }
```


**Describe the current behavior**
IOÈÄÇÈÖçÂô®,Â∑≤Âú®hanlp.properties‰∏≠ÊâìÂºÄ
```
public class ResourceFileIoAdapter implements IIOAdapter {
    @Override
    public InputStream open(String path) throws IOException {
        ClassPathResource resource = new ClassPathResource(path);
        InputStream is = new FileInputStream(resource.getFile());
        return is;
    }

    @Override
    public OutputStream create(String path) throws IOException {
        ClassPathResource resource = new ClassPathResource(path);
        OutputStream os = new FileOutputStream(resource.getFile());
        return os;
    }
}
```


**Expected behavior**
Â∏åÊúõËÉΩÂ∞ÜdataÊâìÂåÖÂà∞jarÂåÖ‰∏≠,‰ΩøÁî®NlpÂàÜËØçÂô®

**System information**
- HanLP version: portable-1.8.3

**Other info / logs**
ÊàëÁöÑÈÖçÁΩÆ‰ø°ÊÅØÂ¶Ç‰∏ã
```
root=nlp/
#IOÈÄÇÈÖçÂô®ÔºåÂÆûÁé∞com.hankcs.hanlp.corpus.io.IIOAdapterÊé•Âè£‰ª•Âú®‰∏çÂêåÁöÑÂπ≥Âè∞ÔºàHadoop„ÄÅRedisÁ≠âÔºâ‰∏äËøêË°åHanLP
#ÈªòËÆ§ÁöÑIOÈÄÇÈÖçÂô®Â¶Ç‰∏ãÔºåËØ•ÈÄÇÈÖçÂô®ÊòØÂü∫‰∫éÊôÆÈÄöÊñá‰ª∂Á≥ªÁªüÁöÑ„ÄÇ
IOAdapter=com.holly.top.springframework.config.ResourceFileIoAdapter
```
Êñá‰ª∂ÁõÆÂΩï
![image](https://user-images.githubusercontent.com/76762269/194991248-675e17b4-16d7-4889-aba0-ba4c647f1d81.png)

* [x] I've completed this form and searched the web for solutions.
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->
<!-- ‚¨ÜÔ∏èÊ≠§Â§ÑÂä°ÂøÖÂãæÈÄâÔºåÂê¶Âàô‰Ω†ÁöÑissue‰ºöË¢´Êú∫Âô®‰∫∫Ëá™Âä®Âà†Èô§ÔºÅ -->"
add TSL cert verify switch to support network env behind private TSL ‚Ä¶,"‚Ä¶gateway

<!--
Thank you for being interested in contributing to HanLP! You are awesome ‚ú®.
‚ö†Ô∏èChanges must be made on dev branch.
-->

# Allow user to ignore TSL cert check when using hanlp_restful API

## Description
When user is working in a network behind a gateway, which blocks / modifies the TSL certification, user need to add a cert file or ignore TSL cert check in order to user hanlp_restful API.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

## How Has This Been Tested?
1. It is used including this modification in the really world use case and works fine.
2. It failed to pass unittest `python -m unittest discover ./tests` due to API access limit (2 times/min allowed only ~~)

## Checklist

Check all items that apply.

- [x] ‚ö†Ô∏èChanges **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [] New and existing unit tests pass locally with my changes
- [x] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have checked my code and corrected any misspellings
"
CSV ËØçÂÖ∏‰∏≠‰∏çËÉΩÂåÖÂê´ÈÄóÂè∑,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**

‰ΩøÁî® CSV Êñá‰ª∂‰Ωú‰∏∫ËØçÂÖ∏Êó∂ÔºåÁî±‰∫éÈÉ®ÂàÜËØçÂê´ÊúâÈÄóÂè∑‰ºöÂØºËá¥ËØçÂÖ∏Â§±Ë¥•„ÄÇ

‰ªé‰ª£Á†Å‰∏äÁúãÔºåHanLP Âè™ÊòØÂçïÁ∫ØÁöÑ‰ΩøÁî®ÈÄóÂè∑ÂàáÂàÜÊØè‰∏ÄË°åÔºåÂπ∂Ê≤°ÊúâÂ§ÑÁêÜ CSV ËΩ¨‰πâÁöÑÊÉÖÂÜµ„ÄÇ

ÂàóÊï∞ÊçÆ‰∏≠Â≠òÂú® `""`, `,` Á¨¶Âè∑Êó∂‰ºöÂ∞ÜËØ•Âàó‰ΩøÁî® `""""` ËøõË°åËΩ¨‰πâ„ÄÇ

**Code to reproduce the issue**

Â∞Ü‰ª•‰∏ãÊñáÊú¨Áõ¥Êé•‰øùÂ≠ò‰∏∫ csv Êñá‰ª∂Âπ∂Âä†ËΩΩËØçÂÖ∏„ÄÇ

```csv
19th century music
20 century British history
21st Century Music
21st century science & technology
2D Materials
3 Biotech
3D Printing and Additive Manufacturing
3D Printing in Medicine
3D Research
""3L: Language, Linguistics, Literature""
```

**Describe the current behavior**

```
Exception in thread ""main"" java.lang.NumberFormatException: For input string: "" Linguistics""
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.base/java.lang.Integer.parseInt(Integer.java:638)
	at java.base/java.lang.Integer.parseInt(Integer.java:770)
	at com.hankcs.hanlp.corpus.io.IOUtil.loadDictionary(IOUtil.java:794)
	at com.hankcs.hanlp.corpus.io.IOUtil.loadDictionary(IOUtil.java:752)
	at com.hankcs.hanlp.seg.Other.DoubleArrayTrieSegment.<init>(DoubleArrayTrieSegment.java:68)
	at org.grobid.core.lexicon.DictSegmenterKt.main(DictSegmenter.kt:6)
	at org.grobid.core.lexicon.DictSegmenterKt.main(DictSegmenter.kt)
```

**Expected behavior**

Ê≠£Â∏∏Âä†ËΩΩ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.1 LTS
- HanLP version:  com.hankcs:hanlp:portable-1.8.3


* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
BinTrieÂíåDoubleArrayTrieÂ∫èÂàóÂåñÂ§±Ë¥•,"**Describe the bug**
ËøôËæπ‰ΩøÁî®HanLP 1.8.3ÁöÑjarËøõË°åÂ≠¶‰π†ÊµãËØïÊó∂ÔºåÂèëÁé∞BinTrieÂíåDoubleArrayTrieÈÉΩÊòØÂèØÂ∫èÂàóÂåñÁöÑÔºå‰ΩÜÊòØÊâßË°åÂ∫èÂàóÂåñÊó∂ÔºåÊèêÁ§∫hanlp.corpus.tag.Nature‰∏çÊòØÂ∫èÂàóÂåñÂÆûÁé∞~ËøõË°åÂºÄÂèëÊó∂‰ºöÂØºËá¥ÂºÇÂ∏∏ÁªìÊùü„ÄÇ


**Describe the current behavior**
Â∫èÂàóÂåñÊó∂ÊäõÂá∫ÂºÇÂ∏∏ÔºåÂ∫èÂàóÂåñÂ§±Ë¥•„ÄÇ

**Expected behavior**
Â∏åÊúõÂèØ‰ª•Â∫èÂàóÂåñÊ≠£Â∏∏~

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Java version:JDK 11
- HanLP version:1.8.3

**Other info / logs**
java.io.NotSerializableException: com.hankcs.hanlp.corpus.tag.Nature

* [x] I've completed this form and searched the web for solutions."
Êó•ÊñáÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁªìÊûúÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
Âú®‰ΩøÁî®hanlp_restfulÊàñhanlpÊó∂Ôºå‰ΩøÁî®Á§∫‰æã‰ª£Á†ÅÊó†Ê≥ïÂ§çÂàªdoc‰∏≠Êó•ËØ≠ÊñáÊú¨ÁöÑÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁªìÊûúÔºå‰∏îÁªìÊûúÂ§ßÈÉ®ÂàÜÈÉΩÊòØÈîôÁöÑÔºåËã±Êñá‰∏≠ÊñáÊ≠£Â∏∏„ÄÇ
È∫ªÁÉ¶ÂºÄÂèëËÄÖÂ§ßÂ§ßÁúã‰∏Ä‰∏ãÔºåË∞¢Ë∞¢ÔºÅ


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
‰ª£Á†Å
```
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)
print(HanLP(['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.',
             '2021Âπ¥„ÄÅHanLPv2.1„ÅØÊ¨°‰∏ñ‰ª£„ÅÆÊúÄÂÖàÁ´ØÂ§öË®ÄË™ûNLPÊäÄË°ì„ÇíÊú¨Áï™Áí∞Â¢É„Å´Â∞éÂÖ•„Åó„Åæ„Åô„ÄÇ',
             '2021Âπ¥ HanLPv2.1‰∏∫Áîü‰∫ßÁéØÂ¢ÉÂ∏¶Êù•Ê¨°‰∏ñ‰ª£ÊúÄÂÖàËøõÁöÑÂ§öËØ≠ÁßçNLPÊäÄÊúØ„ÄÇ']))
```
ÁõÆÊ†áÊó•ËØ≠ÊñáÊú¨
```
2021Âπ¥„ÄÅHanLPv2.1„ÅØÊ¨°‰∏ñ‰ª£„ÅÆÊúÄÂÖàÁ´ØÂ§öË®ÄË™ûNLPÊäÄË°ì„ÇíÊú¨Áï™Áí∞Â¢É„Å´Â∞éÂÖ•„Åó„Åæ„Åô„ÄÇ
```

**Describe the current behavior**
A clear and concise description of what happened.
ÊñáÊ°£‰∏≠ÊèèËø∞ÁöÑËØ•Á§∫‰æã‰∏≠Êó•ËØ≠ÊñáÊú¨ÁöÑÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁªìÊûúÂ∫î‰∏∫Ôºö
```
 [""TOP"", [[""IP"", [[""NUM"", [""2021""]], [""NOUN"", [""Âπ¥""]], [""PUNCT"", [""„ÄÅ""]], [""NOUN"", [""HanLPv2.1""]], [""IP"", [[""VP"", [[""VP"", [[""ADP"", [""„ÅØ""]], [""NOUN"", [""Ê¨°""]], [""NOUN"", [""‰∏ñ‰ª£""]], [""ADP"", [""„ÅÆ""]], [""ADJP"", [[""ADJP"", [[""ADJP"", [[""NOUN"", [""ÊúÄ""]]]], [""ADJP"", [[""NOUN"", [""ÂÖàÁ´Ø""]]]]]], [""ADJP"", [[""NOUN"", [""Â§ö""]]]]]]]]]]]], [""NP"", [[""NP"", [[""NP"", [[""NP"", [[""NP"", [[""NOUN"", [""Ë®ÄË™û""]], [""NOUN"", [""NLP""]], [""NOUN"", [""ÊäÄË°ì""]]]], [""ADP"", [""„Çí""]]]], [""NOUN"", [""Êú¨Áï™""]], [""NOUN"", [""Áí∞Â¢É""]]]], [""PP"", [[""ADP"", [""„Å´""]]]]]], [""VP"", [[""VERB"", [""Â∞éÂÖ•""]], [""AUX"", [""„Åó„Åæ„Åô""]]]]]], [""PUNCT"", [""„ÄÇ""]]]]]],
```
ÂÆûÈôÖËøêË°åÁªìÊûú‰∏∫Ôºö
```
[""TOP"", [[""S"", [[""NP"", [[""NP"", [[""NP"", [[""NOUN"", [""2021Âπ¥""]]]], [""PUNCT"", [""„ÄÅ""]], [""PROPN"", [""HanLPv2.1""]], [""ADP"", [""„ÅØ""]], [""NP"", [[""NOUN"", [""Ê¨°‰∏ñ‰ª£""]]]]]], [""ADP"", [""„ÅÆ""]], [""NOUN"", [""ÊúÄ""]], [""NOUN"", [""ÂÖàÁ´Ø""]], [""NOUN"", [""Â§öË®ÄË™û""]], [""NP"", [[""NOUN"", [""NLP""]], [""NOUN"", [""ÊäÄË°ì""]], [""NP"", [[""NP"", [[""NOUN"", [""„ÇíÊú¨Áï™Áí∞Â¢É""]]]], [""IP"", [[""VP"", [[""VPT"", [[""ADP"", [""„Å´""]], [""NP"", [[""NOUN"", [""Â∞éÂÖ•""]]]]]]]]]]]], [""VERB"", [""„Åó""]]]]]], [""NP"", [[""AUX"", [""„Åæ„Åô""]]]], [""PUNCT"", [""„ÄÇ""]]]]]]
```
**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7.10
- HanLP version: 2.1.0b41

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Êú¨‰∫∫‰∏ªË¶ÅÁî®‰∫éÂàÜÊûêÊó•ËØ≠‰∏≠ÁöÑÂêçËØçÁü≠ËØ≠ÔºàNPÔºâ„ÄÇ
‰∏äÈù¢ÊèèËø∞ÁöÑÁ§∫‰æãÊó•ËØ≠ÊñáÊú¨ÂàÜÊûêÂá∫ÁöÑÁªìÊûú‰∏∫Ôºö
```
2021Âπ¥„ÄÅHanLPv2.1„ÅØÊ¨°‰∏ñ‰ª£„ÅÆÊúÄÂÖàÁ´ØÂ§öË®ÄË™ûNLPÊäÄË°ì„ÇíÊú¨Áï™Áí∞Â¢É„Å´Â∞éÂÖ•„Åó
2021Âπ¥„ÄÅHanLPv2.1„ÅØÊ¨°‰∏ñ‰ª£
2021Âπ¥
Ê¨°‰∏ñ‰ª£
NLPÊäÄË°ì„ÇíÊú¨Áï™Áí∞Â¢É„Å´Â∞éÂÖ•„Åó
„ÇíÊú¨Áï™Áí∞Â¢É„Å´Â∞éÂÖ•
„ÇíÊú¨Áï™Áí∞Â¢É
Â∞éÂÖ•
„Åæ„Åô
```
Âè™Êúâ‰∏§Êù°Ê≠£Á°Æ„ÄÇÊòØ‰∏çÊòØÊ®°ÂûãÁâàÊú¨Ëø≠‰ª£Êó∂ÔºåÊó•ËØ≠Âè•Ê≥ïÊàêÂàÜÂàÜÊûê‰ªªÂä°ÊºèÊéâ‰∫ÜÔºüÈ∫ªÁÉ¶ÂºÄÂèëËÄÖÂ§ßÂ§ßÁúã‰∏Ä‰∏ãÔºåË∞¢Ë∞¢ÔºÅ

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
java  DynamicCustomDictionary load ËØçÂÖ∏Êó∂ÂÄô‰∏çÁîüÊïà,"
**Describe the bug**
DynamicCustomDictionary

**Code to reproduce the issue**
  dictionary.load(""/home/duanfa/trash/845.txt"");
  DynamicCustomDictionary load ËØçÂÖ∏Êó∂ÂÄô‰∏çÁîüÊïà 
845.txt Êñá‰ª∂ÂÜÖÂÆπ

Áà±ÊÖïÂÆòÊñπËä±Âõ≠ÈÄüÂÜôÂÜÖË°£Â•≥Êó†Èí¢Âúà‰∏≠ÂéöÊùØÂ∞èËÉ∏ËÅöÊã¢Ëïæ‰∏ùÊñáËÉ∏AM171791 nz 1
Áà±ÊÖïÂÆòÊñπËä± nz 1


 Áî®  insert ÁîüÊïà
  dictionary.insert(""Áà±ÊÖïÂÆòÊñπËä±Âõ≠ÈÄüÂÜôÂÜÖË°£Â•≥Êó†Èí¢Âúà‰∏≠ÂéöÊùØÂ∞èËÉ∏ËÅöÊã¢Ëïæ‰∏ùÊñáËÉ∏AM171791"", ""N 1"");

ÊàñËÄÖÂú®hanlp.propertiesÈáåÈù¢ÈÖçÁΩÆ
CustomDictionaryPath=data/dictionary/custom/duanfa/845.txt 
ÁîüÊïàÂêéÔºåÊää data/dictionary/custom/duanfa/845.txt.bin  Êã∑Ë¥ùÂà∞ /home/duanfa/trash/  ÁõÆÂΩï‰∏ãÔºådictionary.load(""/home/duanfa/trash/845.txt"");Â∞±ÂèØ‰ª•ÁîüÊïà‰∫Ü

ÁâàÊú¨
 <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.8.3</version>
        </dependency>

```import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.DynamicCustomDictionary;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;

public class LoadCustomFile {
	public DynamicCustomDictionary dictionary = new DynamicCustomDictionary();
	public Segment hanlpSegmenter;

	public LoadCustomFile() {
		try {
			hanlpSegmenter = HanLP.newSegment();
			hanlpSegmenter.enableCustomDictionary(dictionary).enableCustomDictionaryForcing(true);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	public static void main(String[] args) {
		String text = ""Áà±ÊÖïÂÆòÊñπËä±Âõ≠ÈÄüÂÜôÂÜÖË°£Â•≥Êó†Èí¢Âúà‰∏≠ÂéöÊùØÂ∞èËÉ∏ËÅöÊã¢Ëïæ‰∏ùÊñáËÉ∏AM171791"";
		LoadCustomFile lcf = new LoadCustomFile();
		lcf.dictionary.load(""/home/duanfa/trash/845.txt"");
		for (Term term : lcf.hanlpSegmenter.seg(text)) {
			System.out.println(term);
		}
	}
}
```
‰ª£Á†ÅÊâìÂç∞ÁªìÊûúÔºö
Áà±ÊÖï/vn
ÂÆòÊñπ/n
Ëä±Âõ≠/n
ÈÄüÂÜô/n
ÂÜÖË°£/n
Â•≥/b
Êó†/v
Èí¢Âúà/n
‰∏≠/f
Âéö/a
ÊùØ/q
Â∞è/a
ËÉ∏/ng
ËÅöÊã¢/v
Ëïæ/ng
‰∏ù/q
Êñá/ng
ËÉ∏/ng
AM/nx
171791/m



**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ÂÖ≥‰∫é java ÁâàÊú¨‰∏≠‰ΩøÁî®ÈùôÊÄÅÂèòÈáèÂ≠òÂÇ®Â≠óÂÖ∏Ë∑ØÂæÑÁöÑÈóÆÈ¢ò,"**Describe the feature and the current behavior/state.**
ÂΩìÂâç java ÁâàÊú¨‰ΩøÁî®ÈùôÊÄÅÂèòÈáèÂÇ®Â≠òÂ≠óÂÖ∏Ë∑ØÂæÑ, ÂÖ∂Êúâ‰∏™ÂºäÁ´ØÊòØÊó†Ê≥ïÂú®‰∏Ä‰∏™ classloader ‰∏ãÂêåÊó∂ÂºÄÂêØÂ§ö‰∏™‰ΩøÁî®‰∏çÂêåË∑ØÂæÑÁöÑ hanlp ÁöÑ instance. 
ÊØîÂ¶ÇËØ¥ÊàëÊÉ≥ÂºÄÂêØ‰∏§‰∏™ÂÆû‰æã, ‰∏Ä‰∏™Áî®ÂÆÉÊù•Â§ÑÁêÜÁπÅ‰Ωì, ‰∏Ä‰∏™Áî®ÂÖ∂Â§ÑÁêÜÁÆÄ‰Ωì. Ëøô‰∏™Âú®ÂΩìÂâçÂÆûÁé∞‰∏ãÂæàÈöæÂÆûÁé∞, Âõ†‰∏∫‰∏§ËÄÖÁöÑËØçÂÖ∏Ë∑ØÂæÑ‰ºöÂÜ≤Á™Å. 

**Will this change the current api? How?**
‰∏ç‰ºöÊõ¥Êîπapi,Â±û‰∫éÂÜÖÈÉ®ÂÆûÁé∞Êõ¥Êîπ

**Who will benefit with this feature?**
ÊâÄÊúâÁöÑ Java ÁâàÊú¨Áî®Êà∑

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ‰∏é OS Êó†ÂÖ≥
- Python version: xxx
- HanLP version: 1.8.3-portable

**Any other info**
 No

* [x] I've carefully completed this form."
Create .travis.yml,"Sorry, I selected the wrong option while following the tutorial! Please discard this change attempt. Thank you.

<!--
Thank you for being interested in contributing to HanLP! You are awesome ‚ú®.
‚ö†Ô∏èChanges must be made on dev branch.
-->

# Title of Your Pull Request

## Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

## Checklist

Check all items that apply.

- [ ] ‚ö†Ô∏èChanges **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have checked my code and corrected any misspellings
"
ÂàóË°®ÂΩ¢ÂºèÁöÑËæìÂÖ•Ôºå‰∏çÂêåÊ®°ÂûãÁöÑÁªìÊûúÈïøÂ∫¶‰∏ç‰∏ÄËá¥,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Â§ÑÁêÜÂàóË°®ÂΩ¢ÂºèÁöÑËæìÂÖ•ÔºåtokÂØπ‰∫éÁ©∫Â≠óÁ¨¶‰∏≤ÊàñÂçï‰∏™Ê†áÁÇπÊòØÊúâËøîÂõûÂÄºÁöÑÔºåpos ÂíådepÂàôÊ≤°ÊúâËøîÂõûÂÄºÔºåÂØºËá¥tokÁöÑÁªìÊûú‰∏çËÉΩÂíåpos Âíå depÁöÑÁªìÊûú‰∏Ä‰∏ÄÂØπÈΩê
**Code to reproduce the issue**

```python
import hanlp
tok  = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
pos = hanlp.load(hanlp.pretrained.pos.PKU_POS_ELECTRA_SMALL)
dep = hanlp.load(hanlp.pretrained.dep.PMT1_DEP_ELECTRA_SMALL)

text = ['‰Ω†Â•Ω', '']
tok_r = tok(text)
# [['‰Ω†Â•Ω'], ['']]
pos_r = pos(tok_r)
# [['l']]  ÊúüÊúõÁöÑËøîÂõûÂ∫îËØ•ÊòØ [['l'], []]
dep_r = dep(tok_r)
# [[{'id': 1, 'form': '‰Ω†Â•Ω', 'cpos': None, 'pos': None, 'head': 0, 'deprel': 'HED', 'lemma': None, 'feats': None, 'phead': None, 'pdeprel': None}]]
len(tok_r) != len(pos_r) ==len(dep_r)
# True
```

**Describe the current behavior**
Â§ÑÁêÜÂàóË°®ÂΩ¢ÂºèÁöÑËæìÂÖ•ÔºåtokÂØπ‰∫éÁ©∫Â≠óÁ¨¶‰∏≤ÊàñÂçï‰∏™Ê†áÁÇπÊòØÊúâËøîÂõûÂÄºÁöÑÔºåpos ÂíådepÂàôÊ≤°ÊúâËøîÂõûÂÄºÔºåÂØºËá¥tokÁöÑÁªìÊûú‰∏çËÉΩÂíåpos Âíå depÁöÑÁªìÊûú‰∏Ä‰∏ÄÂØπÈΩê

**Expected behavior**
ÂØπ‰∫éÁ©∫Â≠óÁ¨¶‰∏≤ÊàñÂçï‰∏™Ê†áÁÇπÁ≠âÔºåpos Âíådep‰πüÈúÄË¶ÅÊúâ‰∏™Á©∫ÂàóË°®ÁöÑÁªìÊûúËøõË°åÂç†‰ΩçÔºåÁî®‰∫éÂØπÊñáÊú¨ËøõË°åÊ†áÊ≥®„ÄÇ

**System information**
- macos 11.6 
- Python version: 3.8.12
- HanLP version: 2.1.0b37

**Other info / logs**
no

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
bugfix: ‰øÆÂ§ç bintrie Ê†ëÂÖ®ÂàÜËØçÊó∂ ÊèêÂâçË∑≥Âá∫Âæ™ÁéØ bug,"# bintrie ‰∏çËÉΩÂÆåÂÖ®ÂàÜËØç

## Description

‰ΩøÁî® BinTrie ÁöÑ‰ª£Á†ÅÊüê‰∫õÂú∫ÊôØ‰∏çËÉΩËøõË°åÊúâÊïàÁöÑÂÆåÂÖ®ÂàáËØç


‰∏ãÈù¢ÊòØ bug ÊºîÁ§∫:

```java
import com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie;
import org.junit.Before;
import org.junit.Test;

public class BinTrieParseTextTest {

    private BinTrie<Integer> trie;

    @Before
    public void setup() {
        this.trie = new BinTrie<Integer>();
        String[] words = new String[]{""Ê∫ú"", ""ÂÑø"", ""Ê∫úÂÑø"", ""‰∏ÄÊ∫úÂÑø"", ""‰∏ÄÊ∫ú""};
        /*ÊûÑÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑËØçÂÖ∏Ôºå ‰ªé core dict Êñá‰ª∂‰∏≠Êâ£Âá∫ÁöÑ‰∏ÄÈÉ®ÂàÜ*/
        for (int i = 0; i < words.length; i++) {
            this.trie.put(words[i], i);
        }
    }


    @Test
    public void justForShowBugs() {
        showParseText(""‰∏ÄÊ∫úÂÑø"");

        /*Êàë‰ª¨Âú® ‰∏ÄÊ∫úÂÑøÂêéÈù¢Èöè‰æø+‰∏Ä‰∏™Â≠óÁ¨¶ÔºåËøôÈáåÊàë‰ª¨Âä†‰∏Ä‰∏™Á©∫Ê†º ‰ºöÂÆåÂÖ®‰∏çÂêå*/
        showParseText(""‰∏ÄÊ∫úÂÑø"" + "" "");

    }


    private void showParseText(final String text) {
        System.out.printf(""========ËøõË°åÂÆåÂÖ®ÂàáËØç%sÁöÑÊºîÁ§∫======\n"", text);
        this.trie.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<Integer>() {
            @Override
            public void hit(int begin, int end, Integer value) {
                System.out.println(text.substring(begin, end));
            }
        });

        System.out.println(""==========================="");


    }
}

```

ËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ã:

```
========ËøõË°åÂÆåÂÖ®ÂàáËØç‰∏ÄÊ∫úÂÑøÁöÑÊºîÁ§∫======
‰∏ÄÊ∫ú
‰∏ÄÊ∫úÂÑø
===========================
========ËøõË°åÂÆåÂÖ®ÂàáËØç‰∏ÄÊ∫úÂÑø ÁöÑÊºîÁ§∫======
‰∏ÄÊ∫ú
‰∏ÄÊ∫úÂÑø
Ê∫ú
Ê∫úÂÑø
ÂÑø
===========================
```


- Áé∞Ë±°: ÂèëÁé∞Âú® ""‰∏ÄÊ∫úÂÑø"" ÁöÑÊÉÖÂÜµÂàÜËØç‰∏çÂÆåÂÖ®Ôºå ËÄåÊää BinTrie Êîπ‰∏∫ DoubleArrayTrie ÂàôÊ≤°ÊúâÈóÆÈ¢ò.
- ÂéüÂõ†: debug ÂèëÁé∞ bintrie Âú®ÂàÜËØçÂëΩ‰∏≠‰∫ÜÊúÄÂêé‰∏Ä‰∏™Â≠óÁ¨¶ÁöÑÊó∂ÂÄô ‰ºöÊèêÂâçË∑≥Âá∫Âæ™ÁéØ.


## How Has This Been Tested?

ÊµãËØï‰ª£Á†ÅËßÅ `com.hankcs.hanlp.collection.trie.bintrie.BinTrieParseTextTest.java`


"
demoËÑöÊú¨Êó†Ê≥ïÂ§çÁé∞sotaÊïàÊûú,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÊâßË°åsotaËÑöÊú¨Êó†Ê≥ïÂ§çÁé∞ÊïàÊûú

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
1„ÄÅ‰∏ãËΩΩsotaËÑöÊú¨ HanLP/ plugins / hanlp_demo / hanlp_demo / zh / train_sota_bert_pku.py 
2„ÄÅÊâìÂºÄpython shell ÊâßË°å ËÑöÊú¨‰∏≠ÁöÑpython‰ª£Á†Å ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.components.tokenizers.transformer import TransformerTaggingTokenizer
from hanlp.datasets.tokenization.sighan2005.pku import SIGHAN2005_PKU_TRAIN_ALL, SIGHAN2005_PKU_TEST
from tests import cdroot

cdroot()
tokenizer = TransformerTaggingTokenizer()
save_dir = 'data/model/cws/sighan2005_pku_bert_base_96.66'
tokenizer.fit(
    SIGHAN2005_PKU_TRAIN_ALL,
    SIGHAN2005_PKU_TEST,  # Conventionally, no devset is used. See Tian et al. (2020).
    save_dir,
    'bert-base-chinese',
    max_seq_len=300,
    char_level=True,
    hard_constraint=True,
    sampler_builder=SortingSamplerBuilder(batch_size=32),
    epochs=10,
    adam_epsilon=1e-6,
    warmup_steps=0.1,
    weight_decay=0.01,
    word_dropout=0.1,
    seed=1609422632,
)
tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
- Python version: 3.8.8
- HanLP version: 2.1.0b37

**Other info / logs**
623/623 loss: 1416.3794 P: 59.23% R: 65.39% F1: 62.16% ET: 2 m 6 s  
  63/63 loss: 451.0509 P: 89.72% R: 89.54% F1: 89.63% ET: 4 s   
2 m 9 s / 21 m 33 s ETA: 19 m 24 s (saved)
Epoch 2 / 10:
623/623 loss: 286.6173 P: 31.52% R: 63.34% F1: 42.09% ET: 2 m 8 s   
  63/63 loss: 486.3771 P: 0.37% R: 8.27% F1: 0.71% ET: 4 s   
4 m 22 s / 21 m 50 s ETA: 17 m 28 s (1)
Epoch 3 / 10:
623/623 loss: 211.5965 P: 21.55% R: 61.44% F1: 31.91% ET: 2 m 8 s   
  63/63 loss: 470.1510 P: 0.39% R: 8.66% F1: 0.75% ET: 4 s   
6 m 34 s / 21 m 53 s ETA: 15 m 19 s (2)
Epoch 4 / 10:
623/623 loss: 173.5003 P: 16.42% R: 59.68% F1: 25.75% ET: 2 m 9 s   
  63/63 loss: 469.0070 P: 0.37% R: 8.32% F1: 0.71% ET: 4 s   
8 m 47 s / 21 m 57 s ETA: 13 m 10 s (3)
Epoch 5 / 10:
623/623 loss: 149.7656 P: 13.30% R: 58.04% F1: 21.63% ET: 2 m 9 s  
  63/63 loss: 482.1117 P: 0.38% R: 8.61% F1: 0.73% ET: 4 s   
10 m 59 s / 21 m 59 s ETA: 10 m 59 s (4)
Epoch 6 / 10:
623/623 loss: 131.2736 P: 11.19% R: 56.51% F1: 18.69% ET: 2 m 9 s  
  63/63 loss: 530.2560 P: 0.36% R: 8.13% F1: 0.69% ET: 4 s   
13 m 12 s / 22 m 0 s ETA: 8 m 48 s (5) early stop
Max score of dev is P: 89.72% R: 89.54% F1: 89.63% at epoch 1
Average time of each epoch is 2 m 12 s
13 m 12 s elapsed
P: 89.72% R: 89.54% F1: 89.63%
>>> tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)
Pruned 0 (0.0%) samples out of 2004.                             
63/63 loss: 451.0509 P: 89.72% R: 89.54% F1: 89.63% ET: 4 s   
speed: 531 samples/second
(P: 89.72% R: 89.54% F1: 89.63%, (451.0508732871404, P: 89.72% R: 89.54% F1: 89.63%))

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Ëá™ÂÆö‰πâËØçÂÖ∏Âº∫Âà∂Ê®°Âºè‰∏çËµ∑‰ΩúÁî®,"**Describe the bug**
Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂº∫Âà∂Ê®°Âºè‰∏çËµ∑‰ΩúÁî®

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import hanlp
HanLP = hanlp.load(‚Äúcomponent‚Äù)
text = ‚ÄúÊµ∑Ëà™haihangMU3456Ëà™Áè≠‚Äù
HanLP.dict_force = {‚Äúhaihang‚Äù}
print(""-->"", HanLP([text])[‚Äútok/fine‚Äù])
--> [[‚ÄòÊµ∑Ëà™‚Äô, ‚ÄòhaihangMU3456‚Äô, ‚ÄòËà™Áè≠‚Äô]]
```

**System information**
- OS Platform and Distribution: macOS big sur 11.3.1
- Python version: 3.7
- HanLP version: 2.1


* [x] I've completed this form and searched the web for solutions."
Â¶Ç‰ΩïÂä†ËΩΩmulti task learningÈ¢ÑËÆ≠ÁªÉÁöÑÊüê‰∏™Ê®°Âûã,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->

ÊÇ®Â•ΩÔºåÈùûÂ∏∏‰∏çÂ•ΩÊÑèÊÄùÊâìÊâ∞ÔºåÂú®readmeÈáåÈù¢ÊúâËøô‰πà‰∏ÄÂè•ËØùÔºö
```
Ê†πÊçÆÊàë‰ª¨ÁöÑ[ÊúÄÊñ∞Á†îÁ©∂](https://aclanthology.org/2021.emnlp-main.451)ÔºåÂçï‰ªªÂä°Â≠¶‰π†ÁöÑÊÄßËÉΩÂæÄÂæÄ‰ºò‰∫éÂ§ö‰ªªÂä°Â≠¶‰π†„ÄÇÂú®‰πéÁ≤æÂ∫¶Áîö‰∫éÈÄüÂ∫¶ÁöÑËØùÔºåÂª∫ËÆÆ‰ΩøÁî®[Âçï‰ªªÂä°Ê®°Âûã](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/index.html)„ÄÇ
```

ÊâÄ‰ª•ÊàëÂ∞ÜSRLÊ®°Âûã‰ªémulti task learningÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊäΩÂá∫Êù•Ôºå‰ΩÜÊòØÈ¢ÑÊµãÁöÑÁªìÊûúÂπ∂‰∏ç‰∏ÄÊ†∑ÔºåÊâÄ‰ª•È∫ªÁÉ¶‰ΩúËÄÖÂ§ßÂ§ßÂ∏ÆÂøôÁúã‰∏ãÂÖ∑‰ΩìÈóÆÈ¢òÂú®Âì™ÔºåÂ∫îËØ•ÊÄé‰πà‰øÆÊîπ„ÄÇÊõ¥ÂÖ∑‰Ωì‰ª£Á†ÅËØ∑ÁúãÔºö[bbs hankcs Â¶Ç‰ΩïÂ∞Ümulti taskËÆ≠ÁªÉÁöÑÊüê‰∏™Ê®°ÂûãÊäΩÂèñÂá∫Êù• ](https://bbs.hankcs.com/t/multi-task/4874)„ÄÇ


ÈùûÂ∏∏ÊÑüË∞¢ÔºÅÔºÅ

"
[Èùûbug]transformer_encodeËøõË°ågather index spanÊó∂‰∏çÂêàÁêÜ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
‰Ω†Â•ΩÔºåÊàëÂú®ÁúãÂà∞[transformer_tokenizer.py](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/transform/transformer_tokenizer.py#L244)Êó∂Ôºå‰Ω†ÂØπCLSÂíåSEP‰πüËÆ°ÁÆó‰∫Ü`token_token_span`ÔºåÊé•ÁùÄ‰Ω†‰ºöË∞ÉÁî®[pick_tensor_for_each_token](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/layers/transformers/utils.py#L82)Ëøô‰∏™ÂáΩÊï∞Êù•Ëé∑ÂèñÈ¶ñÂ≠óÂêëÈáèÊàñËÄÖÂπ≥ÂùáÂêëÈáèÔºåÈöèÂêé‰Ω†‰ºöÂú®ÊØè‰∏Ä‰∏™‰ªªÂä°ÈáåÈù¢Ëé∑ÂèñÂè•Â≠êÂêëÈáèÊó∂ÈÉΩÊòØÈÄöËøá`encoder_hidden[:, 1:-1:, ]`Êù•Ë°®Á§∫ÔºåÂÖ∂‰∏≠`-1`Âú®ÊúâpaddingÁöÑÊÉÖÂÜµ‰∏ã‰∏ç‰ºöÊòØ[SEP]„ÄÇ


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
hanlp.common.transform.NormalizeCharacterËΩ¨Êç¢ÂêéÂíåÂéüÂè•‰∏çÂêå,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

[ËøôÈáå](https://github.com/hankcs/HanLP/blob/eb3e891f39949568ff1bfb2751ef9a585aa695a8/hanlp/common/transform.py#L493)ÔºåÊØîÂ¶Ç‰ΩøÁî®Âè•Â≠ê`‰∏éÊ≠§ÂêåÊó∂ÔºåÊàë‰ª¨‰πü‰∏∫Áî®Êà∑Êèê‰æõ‰∫ÜÂÆ¢ÊúçÁî≥ËØâÊ∏†ÈÅì„ÄÅÊñáÊ°£ÊâæÂõûË∑ØÂæÑ„ÄÇ`ÁªèËøáÊ≠§ÂáΩÊï∞ËΩ¨Êç¢ÂêéÂèòÊàê‰∫Ü`‰∏éÊ≠§ÂêåÊó∂,Êàë‰ª¨‰πü‰∏∫Áî®Êà∑Êèê‰æõ‰∫ÜÂÆ¢ÊúçÁî≥ËØâÊ∏†ÈÅì,ÊñáÊ°£ÊâæÂõûË∑ØÂæÑ„ÄÇ`


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python

```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Ê±ÇÊïôÔºöload BERT Ê®°ÂûãÊä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20211227_114712.zip.

**Code to reproduce the issue**
recog = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

```python
```

**Describe the current behavior**
Êä•ÈîôÂ¶Ç‰∏ãtraceback

**Expected behavior**
Ê∂àÈô§Êä•Èîô

**System information**
OS: Windows-10-10.0.19041-SP0
Python: 3.8.3
PyTorch: 1.12.0+cpu
TensorFlow: 2.6.0
HanLP: 2.1.0-beta.36

**Other info / logs**

OSError                                   Traceback (most recent call last)
<ipython-input-6-0638e9495b92> in <module>
----> 1 recog = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

~\AppData\Roaming\Python\Python38\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    173         except:
    174             pass
--> 175         raise e from None
    176 
    177 

~\AppData\Roaming\Python\Python38\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     97             else:
     98                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 99                     obj.load(save_dir, verbose=verbose, **kwargs)
    100                 else:
    101                     obj.load(metapath, **kwargs)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\common\keras_component.py in load(self, save_dir, logger, **kwargs)
    212         self.load_config(save_dir)
    213         self.load_vocabs(save_dir)
--> 214         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    215         self.load_weights(save_dir, **kwargs)
    216         self.load_meta(save_dir)

~\AppData\Roaming\Python\Python38\site-packages\hanlp\common\keras_component.py in build(self, logger, **kwargs)
    222     def build(self, logger, **kwargs):
    223         self.transform.build_config()
--> 224         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
    225                                                    loss=kwargs.get('loss', None)))
    226         self.transform.lock_vocabs()

~\AppData\Roaming\Python\Python38\site-packages\hanlp\components\taggers\transformers\transformer_tagger_tf.py in build_model(self, transformer, max_seq_length, **kwargs)
     32 
     33     def build_model(self, transformer, max_seq_length, **kwargs) -> tf.keras.Model:
---> 34         model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
     35         self.transform.tokenizer = tokenizer
     36         return model

~\AppData\Roaming\Python\Python38\site-packages\hanlp\layers\transformers\loader_tf.py in build_transformer(transformer, max_seq_length, num_labels, tagging, tokenizer_only)
      9 
     10 def build_transformer(transformer, max_seq_length, num_labels, tagging=True, tokenizer_only=False):
---> 11     tokenizer = AutoTokenizer_.from_pretrained(transformer)
     12     if tokenizer_only:
     13         return tokenizer

~\AppData\Roaming\Python\Python38\site-packages\hanlp\layers\transformers\pt_imports.py in from_pretrained(cls, pretrained_model_name_or_path, use_fast, do_basic_tokenize)
     66         if use_fast and not do_basic_tokenize:
     67             warnings.warn('`do_basic_tokenize=False` might not work when `use_fast=True`')
---> 68         tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
     69                                         do_basic_tokenize=do_basic_tokenize,
     70                                         **additional_config)

~\AppData\Roaming\Python\Python38\site-packages\transformers\models\auto\tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    533         if config_tokenizer_class is None:
    534             if not isinstance(config, PretrainedConfig):
--> 535                 config = AutoConfig.from_pretrained(
    536                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs
    537                 )

~\AppData\Roaming\Python\Python38\site-packages\transformers\models\auto\configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    703         kwargs[""name_or_path""] = pretrained_model_name_or_path
    704         trust_remote_code = kwargs.pop(""trust_remote_code"", False)
--> 705         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    706         if ""auto_map"" in config_dict and ""AutoConfig"" in config_dict[""auto_map""]:
    707             if not trust_remote_code:

~\AppData\Roaming\Python\Python38\site-packages\transformers\configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    551         original_kwargs = copy.deepcopy(kwargs)
    552         # Get config dict associated with the base config file
--> 553         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
    554 
    555         # That config file may point us toward another config file to use.

~\AppData\Roaming\Python\Python38\site-packages\transformers\configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    639             )
    640         except EnvironmentError:
--> 641             raise EnvironmentError(
    642                 f""Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from ""
    643                 ""'https://huggingface.co/models', make sure you don't have a local directory with the same name. ""

OSError: Can't load config for 'bert-base-chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-chinese' is the correct path to a directory containing a config.json file
=================================ERROR LOG ENDS=================================


* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Hanlp nativeÊó†Ê≥ï‰∏ãËΩΩÊ®°Âûã,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**

Êó†Ê≥ï‰∏ãËΩΩhanlp nativeÊ®°Âûã failed to download a hanlp native model

Download failed due to ConnectionError(MaxRetryError(""HTTPSConnectionPool(host='file.hankcs.workers.dev', port=443): Max retries exceeded with url: /hanlp/transformers/electra_zh_base_20210706_125233.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024B8D5CA4C0>: Failed to establish a new connection: [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•ÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØïÂ§±Ë¥•„ÄÇ'))"")).

Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip.



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



```python
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)
```


**Describe the current behavior**
Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•

**Expected behavior**
Ê®°ÂûãÊàêÂäü‰∏ãËΩΩ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version:  Python 3.8.8 
- HanLP version: 2.1.0b35

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ConnectionError                           Traceback (most recent call last)
<ipython-input-33-7a52ebc07181> in <module>
----> 1 HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)

E:\anacoda\lib\site-packages\hanlp\__init__.py in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

E:\anacoda\lib\site-packages\hanlp\utils\component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     30     identifier = save_dir
     31     load_path = save_dir
---> 32     save_dir = get_resource(save_dir)
     33     if save_dir.endswith('.json'):
     34         meta_filename = os.path.basename(save_dir)

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in get_resource(path, save_dir, extract, prefix, append_location, verbose)
    338             realpath += compressed
    339         if not os.path.isfile(realpath):
--> 340             path = download(url=path, save_path=realpath, verbose=verbose)
    341         else:
    342             path = realpath

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in download(url, save_path, save_dir, prefix, append_location, verbose)
    183             elif hasattr(e, 'args') and e.args and isinstance(e.args, tuple) and isinstance(e.args[0], str):
    184                 e.args = (e.args[0] + '\n' + remove_color_tag(message),) + e.args[1:]
--> 185             raise e from None
    186         remove_file(save_path)
    187         os.rename(tmp_path, save_path)

E:\anacoda\lib\site-packages\hanlp\utils\io_util.py in download(url, save_path, save_dir, prefix, append_location, verbose)
    153             if verbose:
    154                 downloader.subscribe(DownloadCallback(show_header=False))
--> 155             downloader.start_sync()
    156         except BaseException as e:
    157             remove_file(tmp_path)

E:\anacoda\lib\site-packages\hanlp_downloader\down.py in start_sync(self)
    112             raise RuntimeError('Download has already been started.')
    113 
--> 114         self.run()
    115 
    116     def pause(self):

E:\anacoda\lib\site-packages\hanlp_downloader\down.py in run(self)
    150         s.mount('https://', adapter)
    151 
--> 152         r = s.get(self.url, stream=True, headers=self.headers)
    153         if r.status_code != 200:
    154             raise HTTPError(self.url, r.status_code, f'Internet error', r.headers, None)

E:\anacoda\lib\site-packages\requests\sessions.py in get(self, url, **kwargs)
    553 
    554         kwargs.setdefault('allow_redirects', True)
--> 555         return self.request('GET', url, **kwargs)
    556 
    557     def options(self, url, **kwargs):

E:\anacoda\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    540         }
    541         send_kwargs.update(settings)
--> 542         resp = self.send(prep, **send_kwargs)
    543 
    544         return resp

E:\anacoda\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    675             # Redirect resolving generator.
    676             gen = self.resolve_redirects(r, request, **kwargs)
--> 677             history = [resp for resp in gen]
    678         else:
    679             history = []

E:\anacoda\lib\site-packages\requests\sessions.py in <listcomp>(.0)
    675             # Redirect resolving generator.
    676             gen = self.resolve_redirects(r, request, **kwargs)
--> 677             history = [resp for resp in gen]
    678         else:
    679             history = []

E:\anacoda\lib\site-packages\requests\sessions.py in resolve_redirects(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)
    235             else:
    236 
--> 237                 resp = self.send(
    238                     req,
    239                     stream=stream,

E:\anacoda\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    653 
    654         # Send the request
--> 655         r = adapter.send(request, **kwargs)
    656 
    657         # Total elapsed time of the request (approximately)

E:\anacoda\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='file.hankcs.workers.dev', port=443): Max retries exceeded with url: /hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024B8D6288E0>: Failed to establish a new connection: [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•ÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØïÂ§±Ë¥•„ÄÇ'))

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASEÊ®°ÂûãÂ§ÑÁêÜÊñáÊú¨Êó∂Êä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASEÊ®°ÂûãÂ§ÑÁêÜÊñáÊú¨Êó∂Êä•Èîô

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

model5 = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)
model5('ÂæàÂ•Ωbr ~%?‚Ä¶;# *‚Äô‚òÜ&‚ÑÉ$Ô∏ø‚òÖ?')
```

**Describe the current behavior**
A clear and concise description of what happened.
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-20-3ec54f8a3e83> in <module>
----> 1 model5('ÂæàÂ•Ωbr ~%?‚Ä¶;# *‚Äô‚òÜ&‚ÑÉ$Ô∏ø‚òÖ?')

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in __call__(self, data, **kwargs)
    769 
    770     def __call__(self, data, **kwargs) -> Document:
--> 771         return super().__call__(data, **kwargs)
    772 
    773     def __getitem__(self, task_name: str) -> Task:

/opt/app/conda/lib/python3.6/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---> 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in __call__(self, *args, **kwargs)
    636             **kwargs: Used in sub-classes.
    637         """"""
--> 638         return super().__call__(*args, **merge_dict(self.config, overwrite=True, **kwargs))

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in __call__(self, *args, **kwargs)
     34 
     35         """"""
---> 36         return self.predict(*args, **kwargs)

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict(self, data, tasks, skip_tasks, resolved_tasks, **kwargs)
    503             # Run the first task, let it make the initial batch for the successors
    504             output_dict = self.predict_task(first_task, first_task_name, batch, results, run_transform=True,
--> 505                                             cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
    506             # Run each task group in order
    507             for group_id, group in enumerate(target_tasks):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict_task(self, task, output_key, batch, results, output_dict, run_transform, cls_is_bos, sep_is_eos)
    593         output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
    594                                              results)
--> 595         self.decode_output(output_dict, batch, output_key)
    596         results[output_key].extend(task.prediction_to_result(output_dict[output_key]['prediction'], batch))
    597         return output_dict

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in decode_output(self, output_dict, batch, task_name)
    733                 output_per_task['mask'],
    734                 batch,
--> 735                 self.model.decoders[task_name])
    736 
    737     def update_metrics(self, batch: Dict[str, Any], output_dict: Dict[str, Any], metrics: MetricDict, task_name):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/tasks/tok/tag_tok.py in decode_output(self, output, mask, batch, decoder, **kwargs)
    122     def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],
    123                       mask: torch.BoolTensor, batch: Dict[str, Any], decoder, **kwargs) -> Union[Dict[str, Any], Any]:
--> 124         return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
    125 
    126     def update_metrics(self, batch: Dict[str, Any],

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in decode_output(self, logits, mask, batch, model)
    111             output = output.tolist()
    112         prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
--> 113         return self.tag_to_span(prediction, batch)
    114 
    115     def tag_to_span(self, batch_tags, batch: dict):

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in tag_to_span(self, batch_tags, batch)
    129                     for start, end, label in custom_words:
    130                         if end - start == 1:
--> 131                             tags[start] = S
    132                         else:
    133                             tags[start] = 'B'

IndexError: list assignment index out of range

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.6.11
- HanLP version: 2.1.0-beta.34

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6Ê®°ÂûãÂ§ÑÁêÜÊñáÊú¨Êó∂Êä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6Ê®°ÂûãÂ§ÑÁêÜÊñáÊú¨Êó∂Êä•Èîô

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

model4 = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6)
model4(""~(=^‚Ä•^)_ ~(=^‚Ä•^)_ ~(=^‚Ä•^)_"")
```

**Describe the current behavior**
A clear and concise description of what happened.
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-12-d6c0a38fb7b1> in <module>
----> 1 model4(""~(=^‚Ä•^)_ ~(=^‚Ä•^)_ ~(=^‚Ä•^)_"")

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in __call__(self, data, **kwargs)
    769 
    770     def __call__(self, data, **kwargs) -> Document:
--> 771         return super().__call__(data, **kwargs)
    772 
    773     def __getitem__(self, task_name: str) -> Task:

/opt/app/conda/lib/python3.6/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---> 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in __call__(self, *args, **kwargs)
    636             **kwargs: Used in sub-classes.
    637         """"""
--> 638         return super().__call__(*args, **merge_dict(self.config, overwrite=True, **kwargs))

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in __call__(self, *args, **kwargs)
     34 
     35         """"""
---> 36         return self.predict(*args, **kwargs)

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict(self, data, tasks, skip_tasks, resolved_tasks, **kwargs)
    503             # Run the first task, let it make the initial batch for the successors
    504             output_dict = self.predict_task(first_task, first_task_name, batch, results, run_transform=True,
--> 505                                             cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
    506             # Run each task group in order
    507             for group_id, group in enumerate(target_tasks):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in predict_task(self, task, output_key, batch, results, output_dict, run_transform, cls_is_bos, sep_is_eos)
    593         output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
    594                                              results)
--> 595         self.decode_output(output_dict, batch, output_key)
    596         results[output_key].extend(task.prediction_to_result(output_dict[output_key]['prediction'], batch))
    597         return output_dict

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py in decode_output(self, output_dict, batch, task_name)
    733                 output_per_task['mask'],
    734                 batch,
--> 735                 self.model.decoders[task_name])
    736 
    737     def update_metrics(self, batch: Dict[str, Any], output_dict: Dict[str, Any], metrics: MetricDict, task_name):

~/.local/lib/python3.6/site-packages/hanlp/components/mtl/tasks/tok/tag_tok.py in decode_output(self, output, mask, batch, decoder, **kwargs)
    122     def decode_output(self, output: Union[torch.Tensor, Dict[str, torch.Tensor], Iterable[torch.Tensor], Any],
    123                       mask: torch.BoolTensor, batch: Dict[str, Any], decoder, **kwargs) -> Union[Dict[str, Any], Any]:
--> 124         return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
    125 
    126     def update_metrics(self, batch: Dict[str, Any],

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in decode_output(self, logits, mask, batch, model)
    111             output = output.tolist()
    112         prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
--> 113         return self.tag_to_span(prediction, batch)
    114 
    115     def tag_to_span(self, batch_tags, batch: dict):

~/.local/lib/python3.6/site-packages/hanlp/components/tokenizers/transformer.py in tag_to_span(self, batch_tags, batch)
    129                     for start, end, label in custom_words:
    130                         if end - start == 1:
--> 131                             tags[start] = S
    132                         else:
    133                             tags[start] = 'B'

IndexError: list assignment index out of range

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: Python 3.6.11
- HanLP version: 2.1.0-beta.34

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Ëá™Â∑±ËÆ≠ÁªÉÁöÑpos.bin‰∏çËÉΩÂä†ËΩΩ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Ëá™Â∑±ËÆ≠ÁªÉÁöÑcrfÊ®°ÂûãÊñá‰ª∂Ôºåpos.bin‰∏çËÉΩÂä†ËΩΩÔºå‰ΩÜÊòØÂêåÊó∂ÁîüÊàêÁöÑpos.bin.txtÂ∞±ÂèØ‰ª•Âä†ËΩΩÊàêÂäü„ÄÇ


**Code to reproduce the issue**


```java
CRFPOSTagger tagger = new CRFPOSTagger(null); // ÂàõÂª∫Á©∫ÁôΩÊ†áÊ≥®Âô®
//        tagger = new CRFPOSTagger(PKU.POS_MODEL); // Âä†ËΩΩ
         tagger = new CRFPOSTagger(""/root/repo/hanlp-java/HanLP/data/test/pos.bin""); // Âä†ËΩΩ
        System.out.println(Arrays.toString(tagger.tag(""‰ªñ"", ""ÁöÑ"", ""Â∏åÊúõ"", ""ÊòØ"", ""Â∏åÊúõ"", ""‰∏äÂ≠¶""))); // È¢ÑÊµã
        AbstractLexicalAnalyzer analyzer = new AbstractLexicalAnalyzer(new PerceptronSegmenter(), tagger); // ÊûÑÈÄ†ËØçÊ≥ïÂàÜÊûêÂô®
        System.out.println(analyzer.analyze(""ÊùéÁãóËõãÁöÑÂ∏åÊúõÊòØÂ∏åÊúõ‰∏äÂ≠¶"")); // ÂàÜËØç+ËØçÊÄßÊ†áÊ≥®
```
Êä•ÈîôÂ¶Ç‰∏ãÔºö
java.lang.ArrayIndexOutOfBoundsException: 1677721600

	at com.hankcs.hanlp.model.perceptron.feature.FeatureMap.loadTagSet(FeatureMap.java:99)
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:92)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421)
	at com.hankcs.hanlp.model.crf.LogLinearModel.load(LogLinearModel.java:58)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388)
	at com.hankcs.hanlp.model.crf.LogLinearModel.<init>(LogLinearModel.java:83)
	at com.hankcs.hanlp.model.crf.CRFTagger.<init>(CRFTagger.java:41)
	at com.hankcs.hanlp.model.crf.CRFPOSTagger.<init>(CRFPOSTagger.java:45)
	at com.hankcs.hanlp.model.crf.CRFPOSTaggerTest.testTrain(CRFPOSTaggerTest.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:255)
	at junit.framework.TestSuite.run(TestSuite.java:250)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)

**Describe the current behavior**
100%Âá∫Èîô

**Expected behavior**
ËÉΩÊ≠£Â∏∏Âä†ËΩΩpos.bin

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- jkd version:jdk 1.8
- HanLP version: 1.7.5 and 1.8.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
CrfppTrainHanLPLoadÊâßË°åÊó∂Êä• ‚Äú[0/n ÊûÑÈÄ†ÂçïËØçÊó∂Â§±Ë¥•‚ÄùÁöÑÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
CrfppTrainHanLPLoadÁ®ãÂ∫èÂú®ÊâßË°åÊó∂ÔºåÂ¶ÇÊûúÊñáÊú¨‰∏≠Âá∫Áé∞""[0\n""ËøôÊ†∑ÁöÑÂ≠óÁ¨¶‰∏≤Ôºå‰ºöÊä•Á©∫ÊåáÈíàÈîôËØØÔºö


debugÂèëÁé∞ÔºåÂØπHanLP-1.8.3/src/main/java/com/hankcs/hanlp/model/perceptron/utility/IOUtility.javaÔºåÁ¨¨73Ë°åËøõË°å‰øÆÊîπ
```
# if (sentence.wordList.size() == 0) continue;
if (sentence==null || sentence.wordList.size() == 0) continue;
```
‰ΩÜÊòØÂèàÊä•Ôºö[0/n ÊûÑÈÄ†ÂçïËØçÊó∂Â§±Ë¥•
debugÂèëÁé∞Ôºö
/root/repo/1.8.3/HanLP-1.8.3/src/main/java/com/hankcs/hanlp/corpus/document/sentence/word/WordFactory.java

```java
public static IWord create(String param)
    {
        if (param == null) return null;
        if (param.startsWith(""["") && !param.startsWith(""[/""))
        {
            return CompoundWord.create(param);
        }
        else
        {
            return Word.create(param);
        }
    }
```
Ëøô‰∏™ÊÑèÊÄùÊòØÂ¶ÇÊûú‰ª•""[""ÂºÄÂ§¥ÁöÑËØùÔºåË∞ÉÁî®CompoundWord.createÔºåÁªßÁª≠debugÂèëÁé∞ÔºåÂú®CompoundWordÁ±ª‰∏≠ÁöÑcreateÊñπÊ≥ï‰∏≠ÁöÑËøôÊÆµ‰ª£Á†Å

```
public static CompoundWord create(String param)
    {
        if (param == null) return null;
        int cutIndex = param.lastIndexOf(']');
        if (cutIndex <= 2 || cutIndex == param.length() - 1) return null;
```
ÊàëÁêÜËß£Â¶ÇÊûúÂåÖÂê´‰∫ÜËøôÁßçÁöÑÂàÜËØçÁªìÊûúÔºö""[0/n""ÔºåÈÇ£ÊâßË°åÁöÑÁªìÊûúËÇØÂÆöÊòØnullÔºåËøôÈáå‰∏∫‰ªÄ‰πàËøôÊ†∑Â§ÑÁêÜÔºåËÉΩËß£Èáä‰∏Ä‰∏ã‰πàÔºüÊàëÊÉ≥Â∫îËØ•ÊòØÊúâÁêÜÁî±ÁöÑÔºå‰ΩÜÊòØËøôÈáåÊâßË°å‰∏äÊúâÈóÆÈ¢ò„ÄÇËÉΩËß£Èáä‰∏Ä‰∏ãÈùûÂ∏∏ÊÑüË∞¢ÔºÅÔºÅ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
public void testCwsTest() {

        
        str = ""Êìç‰Ωú/vn Êó∂Èó¥/n Ôºö/w 20210115/m 15/m :/w 19/m :/w Êìç‰Ωú‰∫∫/n Ôºö/w ÂàòX/nr (/w 1390000005/m )/w ÈáçÊñ∞Â§ÑÁêÜ/n "" +
                ""ËØ¥Êòé/v Ôºö/w ËßíËâ≤/n ÂÜÖ/f ‰∫ßÂìÅ/n ÊÄª/b Êï∞Èáè/n ‰∏∫/p [0/n ]/w ,/w ‰∏ç/d Êª°Ë∂≥/v ËßíËâ≤/n [/w 201908011652/m :/w :/w"" +
                "" IPTV/nz ËÆæÂ§á/n ‰∫ßÂìÅ/n ÂàÜÁªÑ/n ]/w ËÆæÂÆö/v ÁöÑ/u ÊúÄ/d Â∞èÂÄº/n ‰∏∫/v [/w 1/m ]/w ÁöÑ/u ÈôêÂà∂/vn Ôºå/w ËØ∑/v ÂØπ/p ÈîôËØØ/n"" +
                "" ÁïåÈù¢/n ËøõË°å/v Êà™Âõæ/vn Âπ∂/c ÂèëÈÄÅÁªô/n Á≥ªÁªüÁÆ°ÁêÜÂëò/nnt ;/w"";
        Sentence sentence;
        if (str == null) {
            System.out.println(""nok"");
        } else {
            str = str.trim();
            if (str.isEmpty()) {
                System.out.println(""nok"");
            } else {
                Pattern pattern = Pattern.compile(""(\\[(([^\\s\\]]+/[0-9a-zA-Z]+)\\s+)+?([^\\s\\]]+/[0-9a-zA-Z]+)]/?[0-9a-zA-Z]+)|([^\\s]+/[0-9a-zA-Z]+)"");
                Matcher matcher = pattern.matcher(str);
                List<IWord> wordList = new LinkedList();

                while (matcher.find()) {
                    String single = matcher.group();
                    IWord word = WordFactory.create(single);
                    if (word == null) {
                        System.out.println(""Âú®Áî® "" + single + "" ÊûÑÈÄ†ÂçïËØçÊó∂Â§±Ë¥•ÔºåÂè•Â≠êÊûÑÈÄ†ÂèÇÊï∞‰∏∫ "" + str);

                    }

                    wordList.add(word);
                }


            }

        }
    }
```

**Describe the current behavior**
100%ÈáçÁé∞ÈóÆÈ¢ò

**Expected behavior**
ÊâßË°åÊàêÂäü

**SystemX information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
- Python version:
- jdk version: jdk1.8
- HanLP version: 1.7.5 Âíå1.8.3

**Other info / logs**


* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
UD 2.10 Êï∞ÊçÆÈõÜ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊ®°ÂûãËÆ≠ÁªÉÊä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
UD 2.10 Êï∞ÊçÆÈõÜËÆ≠ÁªÉÊä•Èîô

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
ud = UniversalDependenciesParser()
ud.fit(
    trn_data=UD_210_MULTILINGUAL_TRAIN,
    dev_data=UD_210_MULTILINGUAL_DEV,
    save_dir=save_dir,
    transformer=ContextualWordEmbedding('token',
                            ""nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large"",
                            average_subwords=True,
                            max_sequence_length=256,
                            word_dropout=0.2),
    max_seq_len=256,
    sampler_builder=SortingSamplerBuilder(batch_size=16),
    epochs=10,
    seed=42,
    tree=True,
    dependencies='tok',
    mix_embedding=0,
    transform=NormalizeToken(dst=""token"",
                             src=""token"",
                             mapper={
                              ""-LRB-"": ""("",
                              ""-RRB-"": "")"",
                              ""-LCB-"": ""{"",
                              ""-RCB-"": ""}"",
                              ""-LSB-"": ""["",
                              ""-RSB-"": ""]"",
                              ""``"": ""\"""",
                              ""''"": ""\"""",
                              ""`"": ""'"",
                              ""¬´"": ""\"""",
                              ""¬ª"": ""\"""",
                              ""‚Äò"": ""'"",
                              ""‚Äô"": ""'"",
                              ""‚Äú"": ""\"""",
                              ""‚Äù"": ""\"""",
                              ""‚Äû"": ""\"""",
                              ""‚Äπ"": ""'"",
                              ""‚Ä∫"": ""'"",
                              ""‚Äì"": ""--"",
                              ""‚Äî"": ""--""}),
)
```

**Describe the current behavior**
A clear and concise description of what happened.

RuntimeError                              Traceback (most recent call last)
<ipython-input-6-d2a750e8a8b0> in <module>
     37                               ""‚Ä∫"": ""'"",
     38                               ""‚Äì"": ""--"",
---> 39                               ""‚Äî"": ""--""}),
     40 )

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in fit(self, trn_data, dev_data, save_dir, transformer, sampler_builder, mix_embedding, layer_dropout, n_mlp_arc, n_mlp_rel, mlp_dropout, lr, transformer_lr, patience, batch_size, epochs, gradient_accumulation, adam_epsilon, weight_decay, warmup_steps, grad_norm, tree, proj, punct, logger, verbose, devices, **kwargs)
    210             verbose=True,
    211             devices: Union[float, int, List[int]] = None, **kwargs):
--> 212         return super().fit(**merge_locals_kwargs(locals(), kwargs))
    213 
    214     def execute_training_loop(self, trn: DataLoader, dev: DataLoader, epochs, criterion, optimizer, metric, save_dir,

~/.local/lib/python3.6/site-packages/hanlp/common/torch_component.py in fit(self, trn_data, dev_data, save_dir, batch_size, epochs, devices, logger, seed, finetune, eval_trn, _device_placeholder, **kwargs)
    293                                                        dev_data=dev_data,
    294                                                        eval_trn=eval_trn,
--> 295                                                        overwrite=True))
    296 
    297     def build_logger(self, name, save_dir):

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in execute_training_loop(self, trn, dev, epochs, criterion, optimizer, metric, save_dir, logger, devices, ratio_width, patience, eval_trn, **kwargs)
    222             logger.info(f""[yellow]Epoch {epoch} / {epochs}:[/yellow]"")
    223             self.fit_dataloader(trn, criterion, optimizer, metric, logger, history=history, ratio_width=ratio_width,
--> 224                                 eval_trn=eval_trn, **self.config)
    225             loss, dev_metric = self.evaluate_dataloader(dev, criterion, metric, logger=logger, ratio_width=ratio_width)
    226             timer.update()

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in fit_dataloader(self, trn, criterion, optimizer, metric, logger, history, gradient_accumulation, grad_norm, ratio_width, eval_trn, **kwargs)
    271             total_loss += loss.item()
    272             if eval_trn:
--> 273                 self.decode_output(out, mask, batch)
    274                 self.update_metrics(metric, batch, out, mask)
    275             if history.step(gradient_accumulation):

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/ud/ud_parser.py in decode_output(self, outputs, mask, batch)
    285         arc_scores, rel_scores = outputs['class_probabilities']['deps']['s_arc'], \
    286                                  outputs['class_probabilities']['deps']['s_rel']
--> 287         arc_preds, rel_preds = BiaffineDependencyParser.decode(self, arc_scores, rel_scores, mask, batch)
    288         outputs['arc_preds'], outputs['rel_preds'] = arc_preds, rel_preds
    289         return outputs

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/biaffine/biaffine_dep.py in decode(self, arc_scores, rel_scores, mask, batch)
    544         tree, proj = self.config.tree, self.config.get('proj', False)
    545         if tree:
--> 546             arc_preds = decode_dep(arc_scores, mask, tree, proj)
    547         else:
    548             arc_preds = arc_scores.argmax(-1)

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in decode_dep(s_arc, mask, tree, proj)
    757             alg = mst
    758             s_arc.diagonal(0, 1, 2)[1:].fill_(float('-inf'))
--> 759         arc_preds[bad] = alg(s_arc[bad], mask[bad])
    760 
    761     return arc_preds

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in mst(scores, mask, multiroot)
    555                 s[:, 0] = float('-inf')
    556                 s[root, 0] = s_root[root]
--> 557                 t = chuliu_edmonds(s)
    558                 s_tree = s[1:].gather(1, t[1:].unsqueeze(-1)).sum()
    559                 if s_tree > s_best:

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    484 
    485     # y is the contracted tree
--> 486     y = chuliu_edmonds(s)
    487     # exclude head of cycle from y
    488     y, cycle_head = y[:-1], y[-1]

~/.local/lib/python3.6/site-packages/hanlp/components/parsers/alg.py in chuliu_edmonds(s)
    448     cycle = torch.tensor(cycle)
    449     # indices of noncycle in the original tree
--> 450     noncycle = torch.ones(len(s)).index_fill_(0, cycle, 0)
    451     noncycle = torch.where(noncycle.gt(0))[0]
    452 

RuntimeError: unknown parameter type

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.6 LTS
- Python version: 3.6.5
- HanLP version: 2.1.0b32

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
FINE_ELECTRA_SMALL_ZH Ëß£ÊûêÈÉ®ÂàÜÈù¢ÁßØÂçï‰ΩçÂá∫Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
tokÊ®°Âûã: **FINE_ELECTRA_SMALL_ZH**   (close_tok_fine_electra_small_20220612_114112.zip)
‰æã1 ËæìÂÖ•Ôºö ‰∏≠ÂõΩÈù¢ÁßØÊòØ960‰∏áÂπ≥ÊñπÂçÉÁ±≥
ÁªìÊûú: ""‰∏≠ÂõΩ"", ""Èù¢ÁßØ"",  ""ÊòØ"", **""960‰∏á"",  _""Âπ≥Êñπ"",  ""ÂçÉ"",  ""Á±≥""_** 
_ÈîôËØØ_Ôºö""Âπ≥ÊñπÂçÉÁ±≥"" Ë¢´ ÂàÜ‰∏∫ **_""Âπ≥Êñπ"",  ""ÂçÉ"", ""Á±≥""_**

‰æã2 ËæìÂÖ•Ôºö ‰∏≠ÂõΩÈù¢ÁßØÊòØ960‰∏áÂçÉÁ±≥¬≤
ÁªìÊûú: ""‰∏≠ÂõΩ"", ""Èù¢ÁßØ"",  ""ÊòØ"", **""960‰∏á"", _""ÂçÉ"",  ""Á±≥"",  ""¬≤""_** 
_ÈîôËØØ_: ""ÂçÉÁ±≥¬≤"" Ë¢´ ÂàÜ‰∏∫ **_""ÂçÉ"",  ""Á±≥"",  ""¬≤""_**

‰æã3 ËæìÂÖ•:  ‰∏≠ÂõΩÈù¢ÁßØÊòØ9600000km¬≤
ÁªìÊûú: ""‰∏≠ÂõΩ"", ""Èù¢ÁßØ"",  ""ÊòØ"", **""_9600000km¬≤""_** 
_ÈîôËØØ_: ""9600000km¬≤"" Â∫î ÂàÜ‰∏∫ **_""9600000"",  ""km¬≤""_**

**Current behavior**
ÊîπÁî®Ê®°Âûã CTB9_TOK_ELECTRA_BASE_CRF ÂêéÔºå‰æã1„ÄÅ‰æã2 ÁªìÊûúÊ≠£Á°ÆÔºå Á±ª‰ºº‰æãÂ≠êËøòÊúâ ""‰∏≠ÂõΩÈù¢ÁßØÊòØ9600000ÂÖ¨Èáå¬≤"", ""ÊïôÂÆ§Èù¢ÁßØÊòØ180Á±≥¬≤""
‰æã3ÁªìÊûúÂàô‰ªçÁÑ∂Áõ∏Âêå„ÄÇ

**Expected behavior**
ÂØπÁ±ª‰ººÂ∫¶ÈáèÂçï‰ΩçÂÆûÁé∞Ê≠£Á°ÆÂàÜËØç

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b31

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
m1Á≥ªÁªüÂÆâË£ÖhanlpÂá∫Èîô can't find Rust compiler,"ÂÆâË£ÖÂ§±Ë¥•

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac m1 
- Python version:3.8.9
- HanLP version:1.8.3

**Other info / logs**

```python
  Building wheel for tokenizers (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  √ó Building wheel for tokenizers (pyproject.toml) did not run successfully.
  ‚îÇ exit code: 1
  ‚ï∞‚îÄ> [51 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.14-arm64-cpython-38
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
      creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
      copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
      copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
      copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
      copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
      copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
      copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
      copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
      running build_ext
      running build_rust
      error: can't find Rust compiler
      
      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
      
      To update pip, run:
      
          pip install --upgrade pip
      
      and then retry package installation.
      
      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for tokenizers
Successfully built pyyaml
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects

```
* [x] I've completed this form and searched the web for solutions.
"
amrËß£ÊûêÊó•ÊúüÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
amrÊ®°Âûã: MRP2020_AMR_ZHO_MENGZI_BASE
ËæìÂÖ• [""1972Âπ¥5Êúà"", ""ÁÅ´Êòü"", ""ÂèëÁîü"", ""Âú∞Èúá"", ""„ÄÇ""]  
Ôºà‰ΩøÁî®ÂàÜËØçÊ®°Âûã http://download.hanlp.com/tok/extra/msra_crf_electra_base_20220507_113936.zipÔºâ

**Describe the current behavior**
ÊâßË°åÁªìÊûú‰∏≠Ôºå**1972Âπ¥5Êúà** ‰∏çËÉΩË¢´ÂÆåÂÖ®Ëß£Êûê
```
{
        ""id"": ""0"",
        ""input"": ""1972Âπ¥5Êúà ÁÅ´Êòü ÂèëÁîü Âú∞Èúá „ÄÇ"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""date-entity"",
                ""anchors"": []
            },
            {
                ""id"": 1,
                ""label"": ""1972"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 7
                    }
                ]
            },
            {
                ""id"": 2,
                ""label"": ""planet"",
                ""anchors"": []
            },
            {
                ""id"": 3,
                ""label"": ""name"",
                ""properties"": [
                    ""op1""
                ],
                ""values"": [
                    ""ÁÅ´Êòü""
                ],
                ""anchors"": [
                    {
                        ""from"": 8,
                        ""to"": 10
                    }
                ]
            },
            {
                ""id"": 4,
                ""label"": ""ÂèëÁîü-01"",
                ""anchors"": [
                    {
                        ""from"": 11,
                        ""to"": 13
                    }
                ]
            },
            {
                ""id"": 5,
                ""label"": ""Âú∞Èúá"",
                ""anchors"": [
                    {
                        ""from"": 14,
                        ""to"": 16
                    }
                ]
            }
        ],
        ""edges"": [
            {
                ""source"": 0,
                ""target"": 1,
                ""label"": ""year""
            },
            {
                ""source"": 2,
                ""target"": 3,
                ""label"": ""name""
            },
            {
                ""source"": 4,
                ""target"": 2,
                ""label"": ""arg1""
            },
            {
                ""source"": 4,
                ""target"": 5,
                ""label"": ""arg0""
            },
            {
                ""source"": 4,
                ""target"": 0,
                ""label"": ""time""
            }
        ],
        ""tops"": [
            4
        ],
        ""framework"": ""amr""
    }

```
ÂàÜËØçÊîπ‰∏∫  [""1972Âπ¥"", ""5Êúà"", ""ÁÅ´Êòü"", ""ÂèëÁîü"", ""Âú∞Èúá"", ""„ÄÇ""]
ÔºàÂàÜËØçÊ®°Âûã: http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_crf_20220426_161255.zipÔºâ
Êó•ÊúüÂèØ‰ª•Ê≠£Á°ÆËß£Êûê„ÄÇ

**Expected behavior**
ÊèêÈ´ò ÂØπÁ≤óÁ≤íÂ∫¶ÂàÜËØçÁªìÊûúÁöÑÊó•ÊúüËß£ÊûêÁ≤æÂ∫¶„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie#getState ÈïøÊó∂Èó¥Âç†Áî®Á∫øÁ®ã CPU99.9,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Â†ÜÊ†àÊòæÁ§∫ com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie#getState(AhoCorasickDoubleArrayTrie.java:402)ÈïøÊó∂Èó¥Âç†Áî®Á∫øÁ®ã(Êó∂Èó¥Ë∂ÖËøá‰∏ÄÂë®)ÔºåÁ∫øÁ®ãÁä∂ÊÄÅ‰∏ÄÁõ¥RUNNABLE,CPUÂç†Áî®Ëææ99.9.


**Code to reproduce the issue**
ÊöÇ‰∏çÊ∏ÖÊ•öÊÄé‰πàËß¶ÂèëÁöÑ

``JAVA``

**Describe the current behavior**
Êó†

**Expected behavior**
Êó†

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Python version:
- HanLP version: portable-1.8.2

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
Â∑≤ÊêúÁ¥¢
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
pypi‰∏äÊú™Êõ¥Êñ∞ CTB9_TOK_ELECTRA_BASE_CRF„ÄÅCTB9_TOK_ELECTRA_BASE„ÄÅMSR_TOK_ELECTRA_BASE_CRF,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Âä†ËΩΩ CTB9_TOK_ELECTRA_BASE_CRF„ÄÅCTB9_TOK_ELECTRA_BASE„ÄÅMSR_TOK_ELECTRA_BASE_CRF
Ê®°ÂûãÊä•Èîô

**Code to reproduce the issue**
ÊâßË°å‰ª£Á†Å

```python
tok = hanlp.load(hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE_CRF)
```

**Describe the current behavior**
Êä•Èîô‰ø°ÊÅØ:
`AttributeError: module 'hanlp.pretrained.tok' has no attribute 'CTB9_TOK_ELECTRA_BASE_CRF'`

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version: python3.9
- HanLP version: 2.1.0b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
amrÈÉ®ÂàÜnodeÁº∫Â§±anchors‰ø°ÊÅØ„ÄÅlabelÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
amrÊ®°Âûã:  MRP2020_AMR_ZHO_MENGZI_BASE

‰æã1 ËæìÂÖ•  [""Êàë"", ""‰∏ç"", ""ÂêÉÈ•≠""]
ÊâßË°åÁªìÊûú‰∏≠Ôºå ÂêÉÈ•≠ ÂØπÂ∫îÁöÑ  ""anchors"": []  

```
{
        ""id"": ""0"",
        ""input"": ""Êàë ‰∏ç ÂêÉÈ•≠"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""Êàë"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 1
                    }
                ]
            },
            {
                ""id"": 1,
                ""label"": ""-"",
                ""anchors"": [
                    {
                        ""from"": 2,
                        ""to"": 3
                    }
                ]
            },
            {
                ""id"": 2,
                ""label"": ""ÂêÉÈ•≠-01"",
                ""anchors"": []
            }
        ],
        ""edges"": [
            {
                ""source"": 2,
                ""target"": 1,
                ""label"": ""polarity""
            },
            {
                ""source"": 2,
                ""target"": 0,
                ""label"": ""arg0""
            }
        ],
        ""tops"": [
            2
        ],
        ""framework"": ""amr""
    }
```

‰æã2Ôºå ËæìÂÖ• [""Êàë"", ""ÂêÉÈ•≠""]
ÂêÉÈ•≠ ÂØπÂ∫îÁöÑ ""label"": ""**Ê≠ª-01**"",  ""anchors"": []
```
{
        ""id"": ""0"",
        ""input"": ""Êàë ÂêÉÈ•≠"",
        ""nodes"": [
            {
                ""id"": 0,
                ""label"": ""Êàë"",
                ""anchors"": [
                    {
                        ""from"": 0,
                        ""to"": 1
                    }
                ]
            },
            {
                ""id"": 1,
                ""label"": ""Ê≠ª-01"",
                ""anchors"": []
            }
        ],
        ""edges"": [
            {
                ""source"": 1,
                ""target"": 0,
                ""label"": ""arg0""
            }
        ],
        ""tops"": [
            1
        ],
        ""framework"": ""amr""
    }
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
‰∏äËø∞‰æãÂ≠êÂú®Á∫øÁâàÊµãËØïÊó∂labelÊ≤°ÊúâÈóÆÈ¢ò

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.9
- HanLP version: 2.1b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Failed to load https://file.hankcs.com/hanlp/tok/coarse_electra_small_20220220_013548.zip.,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•

**Code to reproduce the issue**
hanlpÂ∑≤ÂçáÁ∫ßÂà∞ÊúÄÊñ∞ÁâàÊú¨
```
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
```

**Describe the current behavior**
Ê®°ÂûãÂ∑≤‰∏ãËΩΩÂÆåÊàêÔºå‰ΩÜÂä†ËΩΩÂ§±Ë¥•

**Expected behavior**
Â∏åÊúõÂÆåÊàêÂä†ËΩΩÊ®°Âûã

**System information**
Êä•Èîô‰ø°ÊÅØ‰ªÖÊèê‰æõ‰∫Ü‰ª•‰∏ã‰∏âË°åÔºåÊó†È¢ùÂ§ñÂÖ∂‰ªñ‰ø°ÊÅØ
OS: macOS-10.15.5-x86_64-i386-64bit
Python: 3.8.12
PyTorch: 1.11.0
HanLP: 2.1.0-beta.27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)Êä•ÈîôÔºåÊó†Ê≥ïÂä†ËΩΩÊ®°Âûã,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
ÊàëËøêË°å‰∫Ü
`hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)`ÁÑ∂ÂêéÂá∫Áé∞Êä•ÈîôÔºåÊó†Ê≥ïÂä†ËΩΩÊ®°Âûã

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)`
```python
```
```
import hanlp
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
```
**Describe the current behavior**
A clear and concise description of what happened.
ÁõÆÂâçÔºåÂä†ËΩΩÊ®°ÂûãÂêéÊä•ÈîôÔºåÊàëÂàÜÂà´Âú®Ëá™Â∑±Êú¨Âú∞ÂíågoogleÁöÑcolab‰∏äÈÉΩ‰∏çË°åÔºåÂ∞ùËØïpip upgrade hanlp[full] -U‰πüÂêéÂêåÊ†∑‰ºöÊä•Èîô
**Expected behavior**
A clear and concise description of what you expected to happen.
ËÉΩÂ§üÊ≠£Â∏∏Âä†ËΩΩÊ®°ÂûãÔºåÂÆåÊàêner‰ªªÂä°
**System information**
- OS Platform and Distribution:Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
- Python version:3.7.13
- HanLP version:2.1.0-beta.27

**Other info / logs**


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20211227_114712.zip.
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.13
PyTorch: 1.11.0+cu113
TensorFlow: 2.8.0
HanLP: 2.1.0-beta.27

OSError                                   Traceback (most recent call last)
[<ipython-input-26-b5faac023b11>](https://localhost:8080/#) in <module>()
----> 1 hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

11 frames
[/usr/local/lib/python3.7/dist-packages/hanlp/__init__.py](https://localhost:8080/#) in load(save_dir, verbose, **kwargs)
     41         from hanlp_common.constant import HANLP_VERBOSE
     42         verbose = HANLP_VERBOSE
---> 43     return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
     44 
     45 

[/usr/local/lib/python3.7/dist-packages/hanlp/utils/component_util.py](https://localhost:8080/#) in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
    169         except:
    170             pass
--> 171         raise e from None
    172 
    173 

[/usr/local/lib/python3.7/dist-packages/hanlp/utils/component_util.py](https://localhost:8080/#) in load_from_meta_file(save_dir, meta_filename, transform_only, verbose, **kwargs)
     97             else:
     98                 if os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 99                     obj.load(save_dir, verbose=verbose, **kwargs)
    100                 else:
    101                     obj.load(metapath, **kwargs)

[/usr/local/lib/python3.7/dist-packages/hanlp/common/keras_component.py](https://localhost:8080/#) in load(self, save_dir, logger, **kwargs)
    212         self.load_config(save_dir)
    213         self.load_vocabs(save_dir)
--> 214         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    215         self.load_weights(save_dir, **kwargs)
    216         self.load_meta(save_dir)

[/usr/local/lib/python3.7/dist-packages/hanlp/common/keras_component.py](https://localhost:8080/#) in build(self, logger, **kwargs)
    223         self.transform.build_config()
    224         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
--> 225                                                    loss=kwargs.get('loss', None)))
    226         self.transform.lock_vocabs()
    227         optimizer = self.build_optimizer(**self.config)

[/usr/local/lib/python3.7/dist-packages/hanlp/components/taggers/transformers/transformer_tagger_tf.py](https://localhost:8080/#) in build_model(self, transformer, max_seq_length, **kwargs)
     32 
     33     def build_model(self, transformer, max_seq_length, **kwargs) -> tf.keras.Model:
---> 34         model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
     35         self.transform.tokenizer = tokenizer
     36         return model

[/usr/local/lib/python3.7/dist-packages/hanlp/layers/transformers/loader_tf.py](https://localhost:8080/#) in build_transformer(transformer, max_seq_length, num_labels, tagging, tokenizer_only)
      9 
     10 def build_transformer(transformer, max_seq_length, num_labels, tagging=True, tokenizer_only=False):
---> 11     tokenizer = AutoTokenizer_.from_pretrained(transformer)
     12     if tokenizer_only:
     13         return tokenizer

[/usr/local/lib/python3.7/dist-packages/hanlp/layers/transformers/pt_imports.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, use_fast, do_basic_tokenize)
     68         tokenizer = cls.from_pretrained(get_tokenizer_mirror(transformer), use_fast=use_fast,
     69                                         do_basic_tokenize=do_basic_tokenize,
---> 70                                         **additional_config)
     71         tokenizer.name_or_path = transformer
     72         return tokenizer

[/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    483             if not isinstance(config, PretrainedConfig):
    484                 config = AutoConfig.from_pretrained(
--> 485                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs
    486                 )
    487             config_tokenizer_class = config.tokenizer_class

[/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    650         kwargs[""name_or_path""] = pretrained_model_name_or_path
    651         trust_remote_code = kwargs.pop(""trust_remote_code"", False)
--> 652         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    653         if ""auto_map"" in config_dict and ""AutoConfig"" in config_dict[""auto_map""]:
    654             if not trust_remote_code:

[/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    546         original_kwargs = copy.deepcopy(kwargs)
    547         # Get config dict associated with the base config file
--> 548         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
    549 
    550         # That config file may point us toward another config file to use.

[/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py](https://localhost:8080/#) in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    635         except EnvironmentError:
    636             raise EnvironmentError(
--> 637                 f""Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from ""
    638                 ""'https://huggingface.co/models', make sure you don't have a local directory with the same name. ""
    639                 f""Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory ""

OSError: Can't load config for 'bert-base-chinese'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-chinese' is the correct path to a directory containing a config.json file
=================================ERROR LOG ENDS=================================
* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
TupleTrieDictÁöÑlen‰∏ç‰∏ÄËá¥,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
TupleTrieDict ÂàùÂßãÂåñÂâçÂêàÂπ∂Êï∞ÊçÆÔºå‰∏éÂàùÂßãÂåñÂêéÊõ¥Êñ∞Êï∞ÊçÆÔºålen(TupleTrieDict)ÁöÑÁªìÊûú‰∏ç‰∏ÄËá¥

**Code to reproduce the issue**
ÂàÜÊâπÊ∑ªÂä†Êï∞ÊçÆkeyÊúâ643142Ôºàdict1ÊñπÂºèÔºâÔºåÂêàÂπ∂ÂêékeyÁöÑÈáè‰∏∫632400Ôºàdict2ÊñπÂºèÔºâ„ÄÇ
Á®ãÂ∫èËøêË°åÊó∂ÔºåÂèëÁé∞ dict1ÊñπÂºè ËäÇÁúÅ‰∫ÜÊó∂Èó¥ÔºåËØ∑ÈóÆ TupleTrieDict ÁöÑÊ≠§ÁßçÁé∞Ë±°ÔºåÊòØÁâπÊÄßÔºåËøòÊòØbugÔºü

```python
    from hanlp_trie.dictionary import TupleTrieDict

    dict1 = TupleTrieDict({""aa"": 1})
    print(len(dict1), dict1[""aa""])  # ËæìÂá∫ 1 1

    dict1[""aa""] += 2
    print(len(dict1), dict1[""aa""])  # ËæìÂá∫ 2 3

    dict2 = TupleTrieDict({""aa"": 4})
    print(len(dict2), dict2[""aa""])  # ËæìÂá∫ 1 4
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.9.7
- HanLP version: 2.1.0b27

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ÂêØÁî®Áî®Êà∑ËØçÂÖ∏ÔºåÂàÜËØçÁªìÊûú‰∏≠ÁöÑÂùêÊ†áÊòØÈîôËØØÁöÑ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÂêØÁî®Áî®Êà∑ËØçÂÖ∏ÔºåÂàÜËØçÁªìÊûú‰∏≠ÁöÑÂùêÊ†áÊòØÈîôËØØÁöÑ„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
tok = hanlp.load(hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH)

tok.config.output_spans = True

tok.dict_force = None
tok.dict_combine = None
sent = 'ÊàëÂÖàÂéªÁúãÂåªÁîü'
print(tok(sent))

tok.dict_combine = set({'ÂÖàÂéª'})
print(tok(sent))

# ËæìÂá∫ÔºåÂÖàÂéª ÁöÑÂùêÊ†áÂ∫îËØ•ÊòØ 1,3Ôºö
[['Êàë', 0, 1], ['ÂÖà', 1, 2], ['Âéª', 2, 3], ['Áúã', 3, 4], ['ÂåªÁîü', 4, 6]]
[['Êàë', 0, 1], ['ÂÖàÂéª', 1, 2], ['Áúã', 2, 3], ['ÂåªÁîü', 3, 4]]
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  win10
- Python version: python3.9.7
- HanLP version: hanlp-2.1.0b26

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Âú®Á∫øÁâàÂàÜËØçÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
‰æãÔºöÈ©¨ÊñØÂÖãÁ™ÅÁÑ∂ÊîæËØùÊî∂Ë¥≠Êé®ÁâπÔºåÂá∫‰ª∑2700‰∫øËÆ©ÂÆÉÈÄÄÂ∏Ç„ÄÇ
![Â±èÂπïÂø´ÁÖß 2022-04-16 ‰∏ãÂçà1 35 30](https://user-images.githubusercontent.com/10105704/163663066-839e5542-25e5-47e9-8a6d-7e82bebffed1.png)

""**Êé®ÁâπÔºå**""   


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
‰∏çÂè™Ëøô‰∏Ä‰∏™Âè•Â≠êÔºåÂú®Á∫øÁâà‰∏äÂ∑≤ËßÅËøáÂ§öÊ¨° **ÊääËØç‰∏éÊ†áÁÇπ** ÂàÜÂú®‰∏ÄËµ∑ÁöÑÊÉÖÂÜµ„ÄÇ

---
‰ΩøÁî®Êú¨Âú∞Áâàhanlp-2.1.0b24Ôºå tokÊ®°Âûã **FINE_ELECTRA_SMALL_ZH**Ôºå ÂØπ‰æãÂè•ÂàÜËØçÁªìÊûú**Ê≠£Á°Æ**„ÄÇ

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1.0b24

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Âú®Á∫øÁâàÂàÜËØçÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
‰æãÔºöÈ©¨ÊñØÂÖãÁ™ÅÁÑ∂ÊîæËØùÊî∂Ë¥≠Êé®ÁâπÔºåÂá∫‰ª∑2700‰∫øËÆ©ÂÆÉÈÄÄÂ∏Ç„ÄÇ
![Â±èÂπïÂø´ÁÖß 2022-04-16 ‰∏ãÂçà1 35 30](https://user-images.githubusercontent.com/10105704/163663066-839e5542-25e5-47e9-8a6d-7e82bebffed1.png)

""**Êé®ÁâπÔºå**""   


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
‰∏çÂè™Ëøô‰∏Ä‰∏™Âè•Â≠êÔºåÂú®Á∫øÁâà‰∏äÂ∑≤ËßÅËøáÂ§öÊ¨° **ÊääËØç‰∏éÊ†áÁÇπ** ÂàÜÂú®‰∏ÄËµ∑ÁöÑÊÉÖÂÜµ„ÄÇ

---
‰ΩøÁî®Êú¨Âú∞Áâàhanlp-2.1.0b24Ôºå tokÊ®°Âûã **FINE_ELECTRA_SMALL_ZH**Ôºå ÂØπ‰æãÂè•ÂàÜËØçÁªìÊûú**Ê≠£Á°Æ**„ÄÇ

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1.0b24

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
amrËß£ÊûêÈÉ®ÂàÜÊï∞Â≠óÂá∫Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
‰æã1ÔºöÊàëÁªô‰∫Ü‰ªñ15‰∏áÂÖÉ„ÄÇ
amr Ëß£ÊûêÁªìÊûúÂ¶Ç‰∏ãÂõæ:
![bug](https://user-images.githubusercontent.com/10105704/163555082-0d6052d6-20c6-4a75-be86-26dc3a2035a3.png)
‚Äú**15‰∏á**‚Äù Êú™Ë¢´Ê≠£Á°ÆËß£Êûê

---
‰æã2:  ÊàëÁªô‰∫Ü‰ªñÂçÅ‰∫îÁÇπÂÖ´‰∏áÂÖÉ„ÄÇ
![bug2](https://user-images.githubusercontent.com/10105704/163556640-02c5c566-6d1a-401b-b102-c2982947f36f.png)

‚Äú**ÂçÅ‰∫îÁÇπÂÖ´‰∏á**‚Äù Êú™Ë¢´Ê≠£Á°ÆËß£Êûê

---
‰æã3: ÊàëÁªô‰∫Ü‰ªñÂçÅÂÖÉ‰∏âËßíÂÖ´ÂàÜÈí±„ÄÇ
![Â±èÂπïÂø´ÁÖß 2022-04-15 ‰∏ãÂçà5 54 34](https://user-images.githubusercontent.com/10105704/163557085-a1f0bb31-15d9-43cf-8af9-430a8dae10f8.png)
‚Äú**ÂçÅÂÖÉ‰∏âËßíÂÖ´ÂàÜ**‚Äù Êú™Ë¢´Ê≠£Á°ÆËß£Êûê

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
**Â∞Ü‚Äú15‰∏á‚ÄùÊîπ‰∏∫‚ÄúÂçÅ‰∫î‰∏á‚ÄùÂêéÔºåÂèØËß£Êûê‰∏∫ ‚Äú150000‚Äù**
ÈîôËØØÂ∫îÂá∫Ëá™Êï∞Â≠óËΩ¨Êç¢ÁöÑËøáÁ®ã„ÄÇ ÂèØ‰ª•ÂèÇËÄÉ https://github.com/microsoft/Recognizers-Text 

**Expected behavior**
ËÉΩÊ≠£Á°ÆÊòæÁ§∫ label„ÄÇ
ÂΩìÁÑ∂‰∫ÜÔºåËæìÂá∫Êï∞ÊçÆÈáåÁöÑ anchors Ê†áËÆ∞‰∫ÜÂéüÊñá‰ΩçÁΩÆÔºåÊâÄ‰ª•ÈóÆÈ¢ò‰πü‰∏çÊòØÁâπÂà´ÁöÑÂ§ßüòÑ


Áúã‰∫Ü‰∏ãËæìÂá∫ÁöÑÊï∞ÊçÆÔºåanchorsÊòØ‰øùÁïô‰∫ÜÂéüÊñáÁöÑ‰ΩçÁΩÆÔºåÊâÄ‰ª•ÈóÆÈ¢ò‰πü‰∏çÊòØÁâπÂà´ÁöÑÂ§ß„ÄÇ

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1b23

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
HanLP1.x ÊñáÊú¨Êé®Ëçê ÈÄªËæëÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Âú® Suggester Á±ªÁöÑsuggestÊñπÊ≥ï‰∏≠ÂØπ‰∫é‰∏çÂêåËØÑ‰ª∑Âô®Ê±ÇÂíåÂá∫ÈîôÔºåSuggester ‰Ωç‰∫épackage com.hankcs.hanlp.suggest;
 ` scoreMap.put(entry.getKey(), score / max + entry.getValue() * scorer.boost);`
ÂÖ∂‰∏≠maxË°®Á§∫ÂΩìÂâçËØÑ‰ª∑Âô®ÁöÑÊúÄ‰ºòÂæóÂàÜÔºåË¢´ÈîôËØØÁöÑÈô§Âú®‰∫Ü‰πãÂâçËØÑ‰ª∑Âô®ÂæóÂàÜ‰πãÂíå‰∏äÔºåÂØºËá¥ÊúÄÁªàÊé®ËçêÁªìÊûú‰∏çÂáÜÁ°Æ
Â∫îÊîπ‰∏∫
` scoreMap.put(entry.getKey(), score  + entry.getValue() * scorer.boost/ max);`
**Code to reproduce the issue**
ÊµãËØï‰ª£Á†Å
```public class TEST {
   public static void main(String[] args)
    {
        Suggester suggester = new Suggester();
        String[] titleArray =
            (
                ""wuqi\n"" +""ÊúçÂä°Âô®""
            ).split(""\\n"");
        for (String title : titleArray)
        {
            suggester.addSentence(title);
        }
        System.out.println(suggester.suggest(""Âä°Âô®"", 1));    
    }
}
```
ÂÆûÈôÖËøêË°åÁªìÊûúÊé®Ëçê‰∏∫wuqi

**Describe the current behavior**
Âõ†‰∏∫ÂØπ‰∫éËØÑ‰ª∑Âô®ÂàÜÊï∞Ê±ÇÂíåÈîôËØØÂØºËá¥‰∫ÜÊ®°ÂûãÂæóÂàÜÂèëÁîüÂÅèÂ∑ÆÔºåÊõ¥ÂÄæÂêë‰∫éÈÄâÊã©ÊãºÈü≥Ê£ÄÊü•ÂæóÂàÜÈ´òÁöÑÂè•Â≠ê


**Expected behavior**
ÊúüÊúõËøêË°åÁªìÊûúÂ∫îÂ¶Ç‰∏ãÊâÄÁ§∫ÔºåÊØèËΩÆÂæóÂà∞ÁöÑËØÑÂàÜÂ∫îËØ•‰Ωç‰∫éÔºà0,1Ôºâ‰πãÈó¥ÔºåÊúÄÂêéÊµãËØïÁªìÊûúÂ∫îËØ•‰∏∫ÊúçÂä°Âô®
```
---IdVectorScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 1.0 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.1457421786266213E-10 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.1457421786266213E-10 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 1.0
---EditDistanceScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 2.0 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 0.5 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 0.5 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 1.0
ÂΩìÂâçÊÄªÂæóÂàÜ 0.4 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 0.2 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 0.5 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ wuqi Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.4
---PinyinScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 2.7 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.1666666666666665 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.6666666666666665 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.7000000000000002
ÂΩìÂâçÊÄªÂæóÂàÜ 1.4 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.6666666666666665 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.6666666666666665 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ wuqi Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.9999999999999999
```
**System information**
- Windows
- jdk11.0.5
- HanLP version:1.x

**Other info / logs**
ÂÆûÈôÖËøêË°åÁªìÊûúÔºåÂá∫Áé∞‰∫ÜËΩÆÊ¨°ËØÑÂàÜÂ§ß‰∫é1ÁöÑÊÉÖÂÜµ„ÄÇËØÑ‰ª∑Âô®ËØÑÂàÜÂè†Âä†ÂºÇÂ∏∏
```
---IdVectorScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 1.1457421786266213E-10 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.1457421786266213E-10 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.1457421786266213E-10 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 1.1457421786266213E-10
---EditDistanceScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 0.5000000002291485 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 0.5 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 0.5 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.5000000001145742
ÂΩìÂâçÊÄªÂæóÂàÜ 0.2 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 0.2 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 0.5 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ wuqi Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.2
---PinyinScorer------
ÂΩìÂâçÊÄªÂæóÂàÜ 1.4666666668041557 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.1666666666666665 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.6666666666666665 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ ÊúçÂä°Âô® Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 0.9666666665750072
ÂΩìÂâçÊÄªÂæóÂàÜ 1.7866666666666666 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜ 1.6666666666666665 ÂΩìÂâçËØÑ‰ª∑Âô®ÁªôÂá∫ÂæóÂàÜÊúÄÂ§ßÂÄº 1.6666666666666665 ÂÄôÈÄâÂè•Â≠êÂÜÖÂÆπ wuqi Êú¨ËΩÆÂ¢ûÈïøÂàÜÊï∞ 1.5866666666666667
```
* [x] I've completed this form and searched the web for solutions.
"
fix typo in document,
fix typo in document,
ViterbiSegment Ê∑ªÂä†ÊòØÂê¶ËøõË°å Normalize ÁöÑÈÖçÁΩÆÊñπÊ≥ï,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
ÁõÆÂâçÂè™Êúâ‰∏ÄÁßçÊñπÂºèÂºÄÂÖ≥ NormalizationÔºå‰∏îÂΩ±ÂìçÂÖ®Â±ÄÔºå‰∏çËÉΩ‰ª•Âçï‰∏™ÂØπË±°ÊéßÂà∂„ÄÇ 
`HanLP.Config.Normalization = true`
 
**Will this change the current api? How?**
Â∏åÊúõ ViterbiSegment Ê∑ªÂä†ÈÖçÁΩÆÊñπÊ≥ïÔºåËÉΩÂ§üÂçïÁã¨ÈÖçÁΩÆÊòØÂê¶ËøõË°å Normalize„ÄÇeg:  
`new ViterbiSegment().enableNormalize(bool)`

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ÊâßË°ådemo 404,"**Describe the bug**
<img width=""1322"" alt=""image"" src=""https://user-images.githubusercontent.com/6836030/155309692-da5a3604-55d7-43c2-b875-d8f38749e538.png"">

**Code to reproduce the issue**
```python
# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2021-04-29 11:06
import hanlp
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.utils.io_util import get_resource

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)
ner: TaggingNamedEntityRecognition = HanLP['ner/msra']
ner.dict_whitelist = {'ÂçàÈ•≠Âêé': 'TIME'}
doc = HanLP('2021Âπ¥ÊµãËØïÈ´òË°ÄÂéãÊòØ138ÔºåÊó∂Èó¥ÊòØÂçàÈ•≠Âêé2ÁÇπ45Ôºå‰ΩéË°ÄÂéãÊòØ44', tasks='ner/msra')
doc.pretty_print()
print(doc['ner/msra'])

ner.dict_tags = {('ÂêçÂ≠ó', 'Âè´', 'ÈáëÂçé'): ('O', 'O', 'S-PERSON')}
HanLP('‰ªñÂú®ÊµôÊ±üÈáëÂçéÂá∫ÁîüÔºå‰ªñÁöÑÂêçÂ≠óÂè´ÈáëÂçé„ÄÇ', tasks='ner/msra').pretty_print()

# HanLP.save(get_resource(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH))

# ÈúÄË¶ÅÁÆóÊ≥ïÂü∫Á°ÄÊâçËÉΩÁêÜËß£ÔºåÂàùÂ≠¶ËÄÖÂèØÂèÇËÄÉ http://nlp.hankcs.com/book.php
# See https://hanlp.hankcs.com/docs/api/hanlp/components/mtl/tasks/ner/tag_ner.html
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC
- Python version: 3.9
- HanLP version: 2.1.0B16

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ViterbiSegment Ëá™ÂÆö‰πâ CustomDictionaryForcing Êó∂ÂêØÁî® enableCustomDictionaryForcing ‰∏çÁîüÊïà,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ViterbiSegment ‰ΩøÁî® DynamicCustomDictionary Êó∂ÔºåÂêåÊó∂ÂêØÁî® enableCustomDictionaryForcing ‰∏çÁîüÊïà„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
val dictionary = DynamicCustomDictionary()
dictionary.insert(""fuck"", ""ZH_CN_COMMON"")

val segment = ViterbiSegment()
    .enableCustomDictionaryForcing(true)
    .enableCustomDictionary(dictionary)
val text = ""fucking,fuckkkkkk,fuck123""

//Ëá™ÂÆö‰πâDictionaryÊú™ÁîüÊïà
println(segment.seg(text))

//ÂÖ®Â±ÄDictionaryÊâç‰ºöÁîüÊïà
CustomDictionary.add(""fuck"")
println(segment.seg(text))
```

ËæìÂá∫Ôºö
[fucking/nx, ,/w, fuckkkkkk/nx, ,/w, fuck/ZH_CN_COMMON, 123/m]
[fuck/ZH_CN_COMMON, ing/nx, ,/w, fuck/ZH_CN_COMMON, kkkkk/nx, ,/w, fuck/ZH_CN_COMMON, 123/m]

**Describe the current behavior**
Ëá™ÂÆö‰πâDynamicCustomDictionary‰∏çÁîüÊïàÔºå‰ΩøÁî®ÂÖ®Â±ÄÁöÑCustomDictionaryÊâç‰ºöÁîüÊïà„ÄÇ

**Expected behavior**
Ëá™ÂÆö‰πâDynamicCustomDictionaryÁîüÊïà„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
- Python version:
- HanLP version: <img width=""219"" alt=""image"" src=""https://user-images.githubusercontent.com/10527522/154885763-bd0172d4-ac0f-4811-b279-a83dcb7a2410.png"">


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
py38ÁéØÂ¢É‰∏ãÊó†Ê≥ïÂä†ËΩΩÊ®°Âûã,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
py38ÁéØÂ¢É‰∏ãÊó†Ê≥ïÂä†ËΩΩÊ®°Âûã„ÄÇ‰ΩÜpy36ÂèØ‰ª•Âä†ËΩΩÔºåÂπ∂‰∏î‰πãÂâçÊ®°ÂûãÂ∑≤ÁªèÂú®Êú¨Âú∞‰∏ãËΩΩÂ•Ω‰∫Ü„ÄÇ
ÊåâÁÖßÊèêÁ§∫ÔºåÂ∞ÜÂÖ∂ÂçáÁ∫ßÂà∞15ÁâàÊú¨Ôºå‰æùÁÑ∂Âá∫Áé∞ÂêåÊ†∑ÁöÑÈóÆÈ¢ò

**Code to reproduce the issue**
```python
model= hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows-10-10.0.19042-SP0
- Python version:  3.8.12
- HanLP version:  2.1.0-beta.14

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip.
Please upgrade HanLP with:

	pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
================================ERROR LOG BEGINS================================
OS: Windows-10-10.0.19042-SP0
Python: 3.8.12
PyTorch: 1.10.1+cpu
HanLP: 2.1.0-beta.14
Traceback (most recent call last):
  File ""D:/data/10 gitlab/information_extract/src/model_quant.py"", line 202, in <module>
    obj = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\utils\component_util.py"", line 170, in load_from_meta_file
    raise e from None
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\utils\component_util.py"", line 91, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp_common\reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp_common\reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 27, in <module>
    from hanlp.components.mtl.tasks import Task
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\components\mtl\tasks\__init__.py"", line 23, in <module>
    from hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\transform\transformer_tokenizer.py"", line 9, in <module>
    from hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, PretrainedConfig, AutoTokenizer_
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\hanlp\layers\transformers\pt_imports.py"", line 13, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, \
  File ""<frozen importlib._bootstrap>"", line 1039, in _handle_fromlist
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\__init__.py"", line 2709, in __getattr__
    return super().__getattr__(name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\file_utils.py"", line 1822, in __getattr__
    value = getattr(module, name)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\file_utils.py"", line 1821, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\auto\__init__.py"", line 202, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\auto\tokenization_auto.py"", line 36, in <module>
    from ..flaubert.tokenization_flaubert import FlaubertTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\flaubert\tokenization_flaubert.py"", line 23, in <module>
    from ..xlm.tokenization_xlm import XLMTokenizer
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\transformers\models\xlm\tokenization_xlm.py"", line 25, in <module>
    import sacremoses as sm
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\__init__.py"", line 2, in <module>
    from sacremoses.tokenize import *
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\tokenize.py"", line 10, in <module>
    from sacremoses.util import is_cjk
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\sacremoses\util.py"", line 11, in <module>
    from joblib import Parallel, delayed
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\__init__.py"", line 119, in <module>
    from .parallel import Parallel
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\parallel.py"", line 28, in <module>
    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\_parallel_backends.py"", line 22, in <module>
    from .executor import get_memmapping_executor
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\executor.py"", line 14, in <module>
    from .externals.loky.reusable_executor import get_reusable_executor
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\loky\__init__.py"", line 12, in <module>
    from .backend.reduction import set_loky_pickler
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 125, in <module>
    from joblib.externals import cloudpickle  # noqa: F401
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\__init__.py"", line 3, in <module>
    from .cloudpickle import *
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 152, in <module>
    _cell_set_template_code = _make_cell_set_template_code()
  File ""D:\software\anaconda3\anaconda\envs\py38\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 133, in _make_cell_set_template_code
    return types.CodeType(
TypeError: an integer is required (got type bytes)
=================================ERROR LOG ENDS=================================

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
[linux] fail to pip install hanlp ,"**Describe the bug**
I install hanlp on windows successfully but fail to install hanlp on linux.

**Code to reproduce the issue**
pip install hanlp

**Describe the current behavior**
installation take hours and finally failed. see logs below.

**Expected behavior**
install successfully.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux 8e9e71981ec3 5.3.0-51-generic #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Python version:
3.10
- HanLP version: 2.1

**Other info / logs**
INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.
  Downloading regex-2021.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 763 kB 114.8 MB/s 
  Downloading regex-2021.10.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 763 kB 93.0 MB/s 
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking

ERROR: Exception:
Traceback (most recent call last):
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/cli/base_command.py"", line 173, in _main
    status = self.run(options, args)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/cli/req_command.py"", line 203, in wrapper
    return func(self, options, args)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/commands/install.py"", line 315, in run
    requirement_set = resolver.resolve(
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 94, in resolve
    result = self._result = resolver.resolve(
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 472, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""/opt/conda/envs/dict/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 383, in resolve
    raise ResolutionTooDeep(max_rounds)
pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 2000000

* [x] I've completed this form and searched the web for solutions.
https://stackoverflow.com/questions/66008760/pips-dependency-resolver-takes-way-too-long-to-solve-the-conflict
‰ΩøÁî®pip install hanlp --use-deprecated=legacy-resolverÂèØ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò
Ê≠§Â§ñÔºåË¶ÅÂÖàÂÆâË£Öpytorch>=1.6ÔºåÂÜçÊâßË°å‰∏äËø∞ÂëΩ‰ª§„ÄÇ

ËôΩÁÑ∂ÈÄöËøá‰∏äËø∞ÂäûÊ≥ïËß£ÂÜ≥‰∫ÜÈóÆÈ¢òÔºå‰ΩÜÂ∏åÊúõËÉΩÂ§üÊèê‰æõÊñπ‰æøÁöÑÂÆâË£ÖÔºå‰ª•ÂèäËØ¥ÊòéÈúÄË¶Å‰ªÄ‰πà‰æùËµñ„ÄÇ"
mtl.evaluateÊä•Âá∫ AssertionError: No samples loaded,"**Describe the bug**
mtl.evaluateÊä•Âá∫ AssertionError: No samples loaded


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import os
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.common.transform import NormalizeCharacter
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.components.mtl.tasks.pos import TransformerTagging
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding
from hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder
from hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON
from hanlp.utils.log_util import cprint


root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))


def cdroot():
    os.chdir(root)


F_LEARN_RATE = 1e-3
N_EPOCH = 1
N_BATCH_SIZE = 16
N_MAX_SEQ_LEN = 510

CTB8_CWS_TRAIN = r""/home/_tmp/data/ctb8_cn/tasks/cws/train.txt""
CTB8_CWS_DEV = r""/home/_tmp/data/ctb8_cn/tasks/cws/dev.txt""
CTB8_CWS_TEST = r""/home/_tmp/data/ctb8_cn/tasks/cws/test.txt""

CTB8_POS_TRAIN = r""/home/_tmp/data/ctb8_cn/tasks/pos/train.txt""
CTB8_POS_DEV = r""/home/_tmp/data/ctb8_cn/tasks/pos/dev.txt""
CTB8_POS_TEST = r""/home/_tmp/data/ctb8_cn/tasks/pos/test.txt""

S_NER_DATA_DIR = r""/home/_tmp/data/msra_ner_token_level_cn/""
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN = os.path.join(S_NER_DATA_DIR, ""word_level.train.tsv"")
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV = os.path.join(S_NER_DATA_DIR, ""word_level.dev.tsv"")
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST = os.path.join(S_NER_DATA_DIR, ""word_level.test.tsv"")

S_SAV_MODEL_DIR = '/home/_tmp/data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small'
S_PERTRAINED_DIR = r""/home/data_sync/pretrain_model/hfl__chinese-electra-180g-small-discriminator""

print(CTB8_CWS_TRAIN)
print(CTB8_CWS_DEV)
print(CTB8_CWS_TEST)
print()
print(CTB8_POS_TRAIN)
print(CTB8_POS_DEV)
print(CTB8_POS_TEST)
print()
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN)
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV)
print(MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST)

tasks = {
    'tok': TaggingTokenization(  # ÂàÜËØç
        CTB8_CWS_TRAIN,
        CTB8_CWS_DEV,
        CTB8_CWS_TEST,
        SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
        max_seq_len=N_MAX_SEQ_LEN,
        hard_constraint=True,
        char_level=True,
        tagging_scheme='BMES',
        lr=F_LEARN_RATE,
        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),
    ),
    # 'pos': TransformerTagging(  # ËØçÊÄß
    #     CTB8_POS_TRAIN,
    #     CTB8_POS_DEV,
    #     CTB8_POS_TEST,
    #     SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
    #     hard_constraint=True,
    #     max_seq_len=N_MAX_SEQ_LEN,
    #     char_level=True,
    #     dependencies='tok',
    #     lr=1e-3,
    # ),
    'ner': TaggingNamedEntityRecognition(  # ÂÆû‰Ωì  mat multi mat erro
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,
        SortingSamplerBuilder(batch_size=N_BATCH_SIZE),
        lr=F_LEARN_RATE,
        # secondary_encoder=RelativeTransformerEncoder(768, k_as_x=True),  # base
        secondary_encoder=RelativeTransformerEncoder(256, k_as_x=True),   # small
        dependencies='tok',
    ),
    # 'dep': BiaffineDependencyParsing(
    #     CTB8_SD330_TRAIN,
    #     CTB8_SD330_DEV,
    #     CTB8_SD330_TEST,
    #     SortingSamplerBuilder(batch_size=32),
    #     lr=1e-3,
    #     tree=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'con': CRFConstituencyParsing(
    #     CTB8_BRACKET_LINE_NOEC_TRAIN,
    #     CTB8_BRACKET_LINE_NOEC_DEV,
    #     CTB8_BRACKET_LINE_NOEC_TEST,
    #     SortingSamplerBuilder(batch_size=32),
    #     lr=1e-3,
    #     dependencies='tok',
    # )
}

mtl = MultiTaskLearning()

mtl.load(S_SAV_MODEL_DIR)
print(mtl('ÂçéÁ∫≥Èü≥‰πêÊóó‰∏ãÁöÑÊñ∞Âû£ÁªìË°£Âú®12Êúà21Êó•‰∫éÊó•Êú¨Ê≠¶ÈÅìÈ¶Ü‰∏æÂäûÊ≠åÊâãÂá∫ÈÅìÊ¥ªÂä®'))
for k, v in tasks.items():
    v.trn = tasks[k].trn
    v.dev = tasks[k].dev
    v.tst = tasks[k].tst
metric, *_ = mtl.evaluate(S_SAV_MODEL_DIR)
for k, v in tasks.items():
    print(metric[k], end=' ')
print()

```

**Describe the current behavior**
ËøêË°åÊä•Èîô
File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/dataset.py"", line 129, in __init__
    assert data, 'No samples loaded'
AssertionError: No samples loaded

**Expected behavior**
Á®ãÂ∫èÊ≠£Â∏∏ËæìÂá∫Ê†∑Êú¨ÈõÜ‰∏äP R F

**System information**
- centos-release-7-9.2009.1.el7.centos.x86_64:
- python=3.6.4:
- hanlp=2.1.0a65

**Other info / logs**
/home/_tmp/data/ctb8_cn/tasks/cws/train.txt
/home/_tmp/data/ctb8_cn/tasks/cws/dev.txt
/home/_tmp/data/ctb8_cn/tasks/cws/test.txt

/home/_tmp/data/ctb8_cn/tasks/pos/train.txt
/home/_tmp/data/ctb8_cn/tasks/pos/dev.txt
/home/_tmp/data/ctb8_cn/tasks/pos/test.txt

/home/_tmp/data/msra_ner_token_level_cn/word_level.train.tsv
/home/_tmp/data/msra_ner_token_level_cn/word_level.dev.tsv
/home/_tmp/data/msra_ner_token_level_cn/word_level.test.tsv
{
  ""tok"": [
    ""ÂçéÁ∫≥"",
    ""Èü≥‰πê"",
    ""Êóó‰∏ã"",
    ""ÁöÑ"",
    ""Êñ∞Âû£ÁªìË°£"",
    ""Âú®"",
    ""12Êúà"",
    ""21Êó•"",
    ""‰∫é"",
    ""Êó•Êú¨"",
    ""Ê≠¶ÈÅìÈ¶Ü"",
    ""‰∏æÂäû"",
    ""Ê≠åÊâã"",
    ""Âá∫ÈÅì"",
    ""Ê¥ªÂä®""
  ],
  ""ner"": [
    [""ÂçéÁ∫≥Èü≥‰πê"", ""ORGANIZATION"", 0, 2],
    [""Êñ∞Âû£ÁªìË°£"", ""PERSON"", 4, 5],
    [""12Êúà"", ""DATE"", 6, 7],
    [""21Êó•"", ""DATE"", 7, 8],
    [""Êó•Êú¨"", ""LOCATION"", 9, 10],
    [""Ê≠¶ÈÅìÈ¶Ü"", ""LOCATION"", 10, 11]
  ]
}
1 / 2 Building tst dataset for ner ...
Traceback (most recent call last):
  File ""hanlp_train_cn.py"", line 134, in <module>
    metric, *_ = mtl.evaluate(S_SAV_MODEL_DIR)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 753, in evaluate
    rets = super().evaluate('tst', save_dir, logger, batch_size, output, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 469, in evaluate
    device=self.devices[0], logger=logger, overwrite=True))
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 156, in build_dataloader
    cache=isinstance(data, str), **config)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py"", line 123, in build_dataloader
    dataset = self.build_dataset(data, cache=cache, transform=transform, **args)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/ner/transformer_ner.py"", line 216, in build_dataset
    dataset = super().build_dataset(data, transform, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 170, in build_dataset
    return TSVTaggingDataset(data, transform=transform, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/datasets/ner/tsv.py"", line 45, in __init__
    super().__init__(data, transform, cache, generate_idx)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/dataset.py"", line 129, in __init__
    assert data, 'No samples loaded'
AssertionError: No samples loaded


* [x] I've completed this form and searched the web for solutions.
"
"open_base.pyËÆ≠ÁªÉÊúâbug , mat1 and mat2 shapes cannot be multiplied","**Describe the bug**
open_base.pyËÆ≠ÁªÉÊúâbug

 mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)

**Code to reproduce the issue**

```
# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2020-12-03 14:24

import os
from hanlp.common.dataset import SortingSamplerBuilder
from hanlp.common.transform import NormalizeCharacter
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing
from hanlp.components.mtl.tasks.dep import BiaffineDependencyParsing
from hanlp.components.mtl.tasks.ner.tag_ner import TaggingNamedEntityRecognition
from hanlp.components.mtl.tasks.pos import TransformerTagging
from hanlp.components.mtl.tasks.sdp import BiaffineSemanticDependencyParsing
from hanlp.components.mtl.tasks.srl.bio_srl import SpanBIOSemanticRoleLabeling
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from hanlp.datasets.ner.msra import MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, \
    MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST
from hanlp.datasets.parsing.ctb8 import CTB8_POS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_SD330_TEST, CTB8_SD330_DEV, \
    CTB8_SD330_TRAIN, CTB8_CWS_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_BRACKET_LINE_NOEC_TRAIN, \
    CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TEST
from hanlp.datasets.parsing.semeval16 import SEMEVAL2016_TEXT_TRAIN_CONLLU, SEMEVAL2016_TEXT_TEST_CONLLU, \
    SEMEVAL2016_TEXT_DEV_CONLLU
# from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_DEV, \
#     ONTONOTES5_CONLL12_CHINESE_TRAIN
from hanlp.layers.embeddings.contextual_word_embedding import ContextualWordEmbedding
from hanlp.layers.transformers.relative_transformer import RelativeTransformerEncoder
from hanlp.utils.lang.zh.char_table import HANLP_CHAR_TABLE_JSON
from hanlp.utils.log_util import cprint


root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))


def cdroot():
    """"""
    cd to project root, so models are saved in the root folder
    """"""
    os.chdir(root)


n_batch_size = 8


tasks = {
    'tok': TaggingTokenization(  # ÂàÜËØç
        CTB8_CWS_TRAIN,
        CTB8_CWS_DEV,
        CTB8_CWS_TEST,
        SortingSamplerBuilder(batch_size=n_batch_size),
        max_seq_len=510,
        hard_constraint=True,
        char_level=True,
        tagging_scheme='BMES',
        lr=1e-3,
        transform=NormalizeCharacter(HANLP_CHAR_TABLE_JSON, 'token'),
    ),
    # 'pos': TransformerTagging(  # ËØçÊÄß
    #     CTB8_POS_TRAIN,
    #     CTB8_POS_DEV,
    #     CTB8_POS_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     hard_constraint=True,
    #     max_seq_len=510,
    #     char_level=True,
    #     dependencies='tok',
    #     lr=1e-3,
    # ),
    'ner': TaggingNamedEntityRecognition(  # ÂÆû‰Ωì  mat multi mat erro
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV,
        MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST,
        SortingSamplerBuilder(batch_size=n_batch_size),
        lr=1e-3,
        secondary_encoder=RelativeTransformerEncoder(768, k_as_x=True),
        dependencies='tok',
    ),
    # 'srl': SpanBIOSemanticRoleLabeling(  # ‰æùÂ≠òÂè•Ê≥ï  download error
    #     ONTONOTES5_CONLL12_CHINESE_TRAIN,
    #     ONTONOTES5_CONLL12_CHINESE_DEV,
    #     ONTONOTES5_CONLL12_CHINESE_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size, batch_max_tokens=2048),
    #     lr=1e-3,
    #     crf=True,
    #     dependencies='tok',
    # ),
    # 'dep': BiaffineDependencyParsing(  # ÊàêÂàÜÂè•Ê≥ï
    #     CTB8_SD330_TRAIN,
    #     CTB8_SD330_DEV,
    #     CTB8_SD330_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     tree=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'sdp': BiaffineSemanticDependencyParsing(  # ËØ≠‰πâ‰æùÂ≠ò
    #     SEMEVAL2016_TEXT_TRAIN_CONLLU,
    #     SEMEVAL2016_TEXT_DEV_CONLLU,
    #     SEMEVAL2016_TEXT_TEST_CONLLU,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     apply_constraint=True,
    #     punct=True,
    #     dependencies='tok',
    # ),
    # 'con': CRFConstituencyParsing(  # ËØ≠‰πâËßíËâ≤ memory out
    #     CTB8_BRACKET_LINE_NOEC_TRAIN,
    #     CTB8_BRACKET_LINE_NOEC_DEV,
    #     CTB8_BRACKET_LINE_NOEC_TEST,
    #     SortingSamplerBuilder(batch_size=n_batch_size),
    #     lr=1e-3,
    #     dependencies='tok',
    # )
}

mtl = MultiTaskLearning()
save_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small'
# save_dir = 'data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base'
# save_dir = 'data/model/mtl/open_tok_pos_ner_bert_base'
mtl.fit(
    ContextualWordEmbedding('token',
                            # ""bert-base-chinese"",
                            ""hfl/chinese-electra-180g-small-discriminator"",
                            # ""hfl/chinese-electra-180g-base-discriminator"",
                            average_subwords=True,
                            max_sequence_length=510,
                            word_dropout=.1),
    tasks,
    save_dir,
    1,  # 30
    lr=1e-3,
    encoder_lr=5e-5,
    grad_norm=1,
    gradient_accumulation=2,
    eval_trn=False,
)
cprint(f'Model saved in [cyan]{save_dir}[/cyan]')
mtl.load(save_dir)
for k, v in tasks.items():
    v.trn = tasks[k].trn
    v.dev = tasks[k].dev
    v.tst = tasks[k].tst
metric, *_ = mtl.evaluate(save_dir)
for k, v in tasks.items():
    print(metric[k], end=' ')
print()
print(mtl('ÂçéÁ∫≥Èü≥‰πêÊóó‰∏ãÁöÑÊñ∞Âû£ÁªìË°£Âú®12Êúà21Êó•‰∫éÊó•Êú¨Ê≠¶ÈÅìÈ¶Ü‰∏æÂäûÊ≠åÊâãÂá∫ÈÅìÊ¥ªÂä®'))
```

**Describe the current behavior**
python open_base.py ËøêË°åÊä•Èîô
RuntimeError: mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)


**Expected behavior**
Á®ãÂ∫èÊ≠£Â∏∏ËÆ≠ÁªÉ


**System information**
- centos-release-7-9.2009.1.el7.centos.x86_64
- python=3.6.4
- hanlp=2.1.0a65

**Other info / logs**
Using GPUs: [0]
Epoch 1 / 1:
    1/24919 loss: 0.7001 ETA: 43 m 14 sTraceback (most recent call last):
  File ""hanlp_train.py"", line 135, in <module>
    eval_trn=False,
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 644, in fit
    **tasks)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 295, in fit
    overwrite=True))
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 287, in execute_training_loop
    **self.config)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 346, in fit_dataloader
    output_dict, _ = self.feed_batch(batch, task_name)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 687, in feed_batch
    decoder=self.model.decoders[task_name]),
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 182, in feed_batch
    return decoder(h, batch=batch, mask=mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py"", line 35, in forward
    contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 309, in forward
    x = layer(x, mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 263, in forward
    x = self.self_attn(x, mask)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/hanlp/layers/transformers/relative_transformer.py"", line 135, in forward
    qv = self.qv_linear(x)  # batch_size x max_len x d_model2
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/home/xy/miniconda3/envs/py364_xy/lib/python3.6/site-packages/torch/nn/functional.py"", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (800x256 and 768x1536)


* [x] I've completed this form and searched the web for solutions."
Update DoubleArrayTrie.java,"Âéü‰π¶page78
ÂØªÊâæ‰∏Ä‰∏™Ëµ∑Âßã‰∏ãÊ†ábÔºå‰ΩøÂæóÊâÄÊúâ<img src=""https://latex.codecogs.com/svg.image?check[base[b]&space;&plus;&space;s_{i}.code]&space;==&space;0"" title=""check[base[b] + s_{i}.code] == 0"" />
"
ÂÖ≥‰∫éconstituency treeÊï∞ÊçÆÊ†áÊ≥®,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
everyone
**Are you willing to contribute it (Yes/No):**
Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- Python version: Python3.*
- HanLP version: master branch

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->



‰Ω†Â•ΩÔºåÂÖ≥‰∫éconstituency treeÊ†áÊ≥®Ôºå‰ªéÊ†áÊ≥®‰∏äÊù•ÁúãÔºö
```
                      TOP                 
                       |                   
                     IP-HLN               
                 ______|________________   
              IP-TPC              |     | 
     ___________|______           |     |  
    |                  VP         |     | 
    |            ______|_____     |     |  
    |         PP-DIR         |    |     | 
    |       ____|______      |    |     |  
NP-PN-SBJ  |           NP    VP NP-SBJ  VP
    |      |           |     |    |     |  
    NR     P           NN    VV   NN    VV
    |      |           |     |    |     |  
    ÂπøË•ø     ÂØπ           Â§ñ     ÂºÄÊîæ   ÊàêÁª©    ÊñêÁÑ∂

```
ÂÆÉÁöÑÂΩ¢ÂºèË≤å‰ººÂíå‰∫∫Ê≠£Â∏∏ÁêÜËß£ÊòØÁõ∏ÂèçÁöÑÔºåÊØîÂ¶Ç‰∏ãÈù¢ÂèØËÉΩ‰ºöÊõ¥Â•ΩÔºö


![123321](https://user-images.githubusercontent.com/11239531/143535834-28e93b8a-7589-4fd5-bf0c-2345661c9328.png)


ÊàëÁöÑÈóÆÈ¢òÂú®‰∫éÔºö

ÊúâÊ≤°ÊúâÁé∞ÊàêÁöÑÊ†áÊ≥®Â∑•ÂÖ∑ÂèØ‰ª•ÂÆûÁé∞Á±ª‰ºº‰∏äÂõæÊïàÊûúÔºåÊàñËÄÖËØ¥ÊúâÊ≤°ÊúâÁõ∏ÂÖ≥ÁöÑÊ†áÊ≥®Â∑•ÂÖ∑Êé®Ëçê„ÄÇ

ÈùûÂ∏∏ÊÑüË∞¢ÔºÅÔºÅÔºÅ

"
crfÊ®°ÂûãÂú®Â§ÑÁêÜemojiÊó∂Êä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**ÈóÆÈ¢òÊèèËø∞:**
crfÊ®°ÂûãÂú®Â§ÑÁêÜemojiÊó∂Êä•Èîô(wordbasedsegmenter‰∏ç‰ºöÊä•Èîô), Ê†∑‰æã‰ª£Á†ÅÂ¶Ç‰∏ã:


**Â§çÁé∞‰ª£Á†Å**

```
from jpype import java

from pyhanlp import *

text = 'üò±üò±üò±‰Ω†Â•ΩÔºåÊ¨¢ËøéÂú®üò±Pythoüò±üò±n‰∏≠üò±Ë∞ÉÁî®HanLPÁöÑAPI   üò±üò±üò±üò±üò±üò±üò±üò±üò±üò±'

String = java.lang.String
java_string = String(text.encode(), 'UTF8')
string_from_java = str(java_string).encode('utf-16', errors='surrogatepass').decode('utf-16')
print('ÂéüÊñáÔºö' + string_from_java)
print('HanLPÂàÜËØçÁªìÊûú')
crflex = HanLP.newSegment(""crf"")
for term in crflex.seg(java_string):
    print(str(term.word).encode('utf-16', errors='surrogatepass').decode('utf-16'))
```

**Êä•Èîô‰ø°ÊÅØ**
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte


**System information**
Á≥ªÁªü: MAC Big Sur, I7.
python 3.6.8
HanLP Jar version: 1.8.2
pyhanlp version: 0.1.79

**ÂÖ∂‰ªñ‰ø°ÊÅØ**
crfÊ®°ÂûãÂú®ËæìÂÖ•ÂÖ´Â≠óËäÇÁöÑÂçïemojiÊó∂, ‰ºöÊãÜÊàê‰∏§‰∏™4Â≠óËäÇÁöÑÂ≠óËøîÂõû
![image](https://user-images.githubusercontent.com/16161025/141046032-da8161a6-d3a0-4db0-ab28-f3f32c729024.png)

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
failed to load a pretrained model,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Failed to load a pretrained model.

**Code to reproduce the issue**
(first do _pip install hanlp[full]_)
```python
import hanlp
hanlp.load(hanlp.pretrained.tok.CTB6_CONVSEG)
```

**Describe the current behavior**
```
2021-11-07 23:34:22.992023: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers
Failed to load https://file.hankcs.com/hanlp/tok/ctb6_convseg_nowe_nocrf_20200110_004046.zip.
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below.
```

**Expected behavior**
Successful loading.

**System information**
OS: Windows-10-10.0.22000-SP0
Python: 3.9.7
PyTorch: 1.9.1
HanLP: 2.1.0-alpha.63

**Other info / logs**
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 126, in load_from_meta_file
    raise e from None
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\utils\component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp_common\reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp_common\reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""C:\Programming\miniconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 680, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 850, in exec_module
  File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\components\tok_tf.py"", line 9, in <module>
    from hanlp.common.keras_component import KerasComponent
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\common\keras_component.py"", line 15, in <module>
    from hanlp.callbacks.fine_csv_logger import FineCSVLogger
  File ""C:\Programming\miniconda3\lib\site-packages\hanlp\callbacks\fine_csv_logger.py"", line 33, in <module>
    class FineCSVLogger(tf.keras.callbacks.History):
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""C:\Programming\miniconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Programming\miniconda3\lib\site-packages\keras\__init__.py"", line 25, in <module>
    from keras import models
  File ""C:\Programming\miniconda3\lib\site-packages\keras\models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""C:\Programming\miniconda3\lib\site-packages\keras\metrics.py"", line 26, in <module>
    from keras import activations
  File ""C:\Programming\miniconda3\lib\site-packages\keras\activations.py"", line 20, in <module>
    from keras.layers import advanced_activations
  File ""C:\Programming\miniconda3\lib\site-packages\keras\layers\__init__.py"", line 23, in <module>
    from keras.engine.input_layer import Input
  File ""C:\Programming\miniconda3\lib\site-packages\keras\engine\input_layer.py"", line 21, in <module>
    from keras.engine import base_layer
  File ""C:\Programming\miniconda3\lib\site-packages\keras\engine\base_layer.py"", line 43, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""C:\Programming\miniconda3\lib\site-packages\keras\mixed_precision\loss_scale_optimizer.py"", line 18, in <module>
    from keras import optimizers
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizers.py"", line 26, in <module>
    from keras.optimizer_v2 import adadelta as adadelta_v2
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizer_v2\adadelta.py"", line 22, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""C:\Programming\miniconda3\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 36, in <module>
    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\eager\monitoring.py"", line 360, in __init__
    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
  File ""C:\Programming\miniconda3\lib\site-packages\tensorflow\python\eager\monitoring.py"", line 135, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.
```

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
func 'input_is_single_sample()' has a bug ,"**Describe the bug**
In 'components/taggers/transformers/transform_tf.py' file, the function in line 117 maybe wrong.

def input_is_single_sample(self, input: Union[List[str], List[List[str]]]) -> bool:
        return isinstance(input[0], str)

**Code to reproduce the issue**

import hanlp
import unittest
ner1 = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

class TestNER(unittest.TestCase):
    def test_ner_multi_sent(self):
        texts = [
            'Âåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Êµ∑Ê∑ÄÂ§ßË°óÂ§©‰ΩøÂ§ßÂé¶5Â±ÇÂ∞èÈÇÆÂ±ÄÔºå Êî∂‰ª∂‰∫∫ÁéãÂ∞è‰∫å',
            'Âåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Ê∏ÖÂçéÂ§ßÂ≠¶ÊòéÂæ∑Ê•ºÔºå Êî∂‰ª∂‰∫∫ÁéãÂ∞è‰∫å']
        doc = ner1(texts)
        print(doc1)

if __name__ == '__main__':
    unittest.main()


**Describe the current behavior**
with the current behavior, the func 'input_is_single_sample()' will return True for the input, which is not expected.  
As a result, the NER result is wrong.
[('Âåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Êµ∑Ê∑ÄÂ§ßË°óÂ§©‰ΩøÂ§ßÂé¶5Â±ÇÂ∞èÈÇÆÂ±ÄÔºå Êî∂‰ª∂‰∫∫ÁéãÂ∞è‰∫å', 'NS', 0, 1), ('Âåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Ê∏ÖÂçéÂ§ßÂ≠¶ÊòéÂæ∑Ê•ºÔºå Êî∂‰ª∂‰∫∫ÁéãÂ∞è‰∫å', 'NS', 1, 2)]


**Expected behavior**
The func 'input_is_single_sample()' should  return False. And NER result should be like this:
[[('Âåó‰∫¨Â∏Ç', 'NS', 0, 3), ('Êµ∑Ê∑ÄÂå∫', 'NS', 3, 6), ('Êµ∑Ê∑ÄÂ§ßË°ó', 'NS', 6, 10), ('Â§©‰ΩøÂ§ßÂé¶', 'NS', 10, 14), ('ÁéãÂ∞è‰∫å', 'NR', 24, 27)], [('Âåó‰∫¨Â∏Ç', 'NS', 0, 3), ('Êµ∑Ê∑ÄÂå∫', 'NS', 3, 6), ('Ê∏ÖÂçéÂ§ßÂ≠¶', 'NT', 6, 10), ('ÊòéÂæ∑Ê•º', 'NS', 10, 13), ('ÁéãÂ∞è‰∫å', 'NR', 18, 21)]]

**System information**
No Need.
The bug is in Master brach and doc-zh brach.

**Other info / logs**
No Need.

* [x] I've completed this form and searched the web for solutions."
hanlp ËøêË°åÊä•Èîô,"
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)


**Describe the current behavior**
A clear and concise description of what happened.
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip.


**System information**
OS: Darwin-17.6.0-x86_64-i386-64bit
Python: 3.7.2
PyTorch: 1.1.0
HanLP: 2.1.0-alpha.61

**Other info / logs**

Exception has occurred: ImportError (note: full exception trace is shown but execution is paused at: )
cannot import name 'IterableDataset' from 'torch.utils.data.dataset' (/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataset.py)

* [x] I've completed this form and searched the web for solutions."
batch size Êó†Ê≥ïËÆæÁΩÆ,"**Describe the bug**
Â¶ÇÈ¢òÔºåbs Êó†Ê≥ïËÆæÁΩÆÔºå‰∏ÄÁõ¥‰∏∫ÈªòËÆ§ÁöÑ 32ÔºåÂØºËá¥ÊòæÂ≠òÂç†Áî®ÂíåÂà©Áî®ÁéáÊó†Ê≥ï‰∏äÂçá



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
from hanlp.components.mtl.tasks.constituency import CRFConstituencyParsing
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning

# hanlp.load()
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)  # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
res = HanLP(['ÁªìÊûÑ‰∏∫XP+Âú∞ÁöÑÁü≠ËØÑÔºåÁî®‰∫é‰øÆÈ•∞Âä®ËØçÁü≠ËØ≠VP'] * 128, batch_size=64, tasks=['con'])
```

debug ÂèØ‰ª•ÁúãÂà∞
![image](https://user-images.githubusercontent.com/25661058/136928619-6904a18a-9b71-4c7f-841b-0be312f249bc.png)

![image](https://user-images.githubusercontent.com/25661058/136928645-a11ee331-3c24-4a40-bf77-61ebe6416bd2.png)



**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- HanLP version: '2.1.0-alpha.61'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Correct the order of the returned P/R values,
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

class Node method 

def _add_child(self, char, value, overwrite=False):
        child = self._children.get(char)
        if child is None:
            child = Node(value)
            self._children[char] = child
        elif overwrite:
            child._value = value
        return child

error for ""elif overwrite:""
should be  ""if overwrite""

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py```python
```

**Describe the current behavior**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

**Expected behavior**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: all
- HanLP version: all

**Other info / logs**
https://github.com/hankcs/HanLP/blob/doc-zh/plugins/hanlp_trie/hanlp_trie/trie.py

* [x] I've completed this form and searched the web for solutions.
"
Correct the seeding behavior for `seed=0`,
Correct the seeding behavior for `seed=0`,
srlËæìÂá∫Êºè‰∫Ü‰∏Ä‰∏™ÂàÜËØçÁªìÊûúÔºåÊàñËÄÖÊòØÊúâ‰∏Ä‰∏™ÂêàÂπ∂ËØçÁöÑËµ∑Âßã‰ΩçÁΩÆÈîô‰∫Ü,"**Describe the bug**
A clear and concise description of what the bug is.
Âú®ÂÅöÁ§∫‰æãÁ®ãÂ∫èÈáåËæìÂÖ•ÊüêËØ≠Âè•ÂêéÂæóÂà∞ÁöÑËøîÂõûÁªìÊûúÈáåÔºåsrlÊåáÁ§∫‰ΩçÁªÑÂêàËµ∑Êù•ÊØîÂéüËØ≠Âè•Â∞ë
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, devices=0) # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
r3 = HanLP.predict([""""""‰ªäÂ§©ÁªßÁª≠Áé©Êñ∞ÁîüÊ®°ÊãüÂô®ÔºåËøô‰∏™Â≠êÂºπÂ§ßÂÆ∂ÈÉΩÁü•ÈÅìÂêßÔºü
                ÊääÂ≠êÂºπÊï∞ÈáèÊîπÊàê‰∫åÂçÅÂÖ≠ÔºåÁÑ∂ÂêéÂí±‰ª¨ÂÜçÂè¨Âî§Â∞±‰ºöÂè¨Âî§Âá∫‰∏ÄÁæ§ÊàòÊñóÊú∫„ÄÇ
                Êõ¥Â•ΩÁé©ÁöÑÊòØÔºåÂ¶ÇÊûúÂí±‰ª¨Âú®ÁûÑÂáÜÊ®°Âºè‰∏ãÂÜçÂè¨Âî§ÁöÑËØùÔºåÂ∞±‰ºöÂè¨Âî§Âá∫‰∏ÄÊû∂ÂèØ‰ª•È©æÈ©∂ÁöÑÊàòÊñó„ÄÇ
                ÂèØÂà´Â∞èÁúãËøôÊû∂ÊàòÊñóÊú∫ÔºåÊúÄÈ´òÈÄüÂ∫¶‰∫îÁßíÂ∞±ÂèØ‰ª•Ë∑®Ë∂äÂ§™Âπ≥Ê¥ãÁßíÊùÄÁõÆÂâç‰∫∫Á±ªÂà∂ÈÄ†ÁöÑÊâÄÊúâÈ£ûË°åÂô®„ÄÇ
                Âø´Êù•Êñ∞Âª∫Ê®°ÊãüÂô®ÔºåÂ∞ùËØï‰∏Ä‰∏ãÂêß„ÄÇ""""""])
print(r3)
```

**Describe the current behavior**
""srl"": [
    [[[""‰ªäÂ§©"", ""ARGM-TMP"", 0, 1], [""ÁªßÁª≠"", ""PRED"", 1, 2], [""Áé©Êñ∞ÁîüÊ®°ÊãüÂô®"", ""ARG1"", 2, 5]], **_[[""‰ªäÂ§©"", ""ARGM-TMP"", 0, 1], [""Áé©"", ""PRED"", 2, 3],_** [""Êñ∞ÁîüÊ®°ÊãüÂô®"", ""ARG1"", 3, 5]], [[""Â§ßÂÆ∂"", ""ARG0"", 8, 9], [""ÈÉΩ"", ""ARGM-ADV"", 9, 10], [""Áü•ÈÅì"", ""PRED"", 10, 11]], [[""Â≠êÂºπÊï∞Èáè"", ""ARG1"", 14, 16], [""Êîπ"", ""PRED"", 16, 17], [""‰∫åÂçÅÂÖ≠"", ""ARG2"", 18, 19]], [[""ÁÑ∂Âêé"", ""ARGM-DIS"", 20, 21], [""Âí±‰ª¨"", ""ARG0"", 21, 22], [""ÂÜç"", ""ARGM-ADV"", 22, 23], [""Âè¨Âî§"", ""PRED"", 23, 24]], [[""Êõ¥"", ""ARGM-ADV"", 32, 33], [""Â•ΩÁé©"", ""PRED"", 33, 34]], [[""Êõ¥Â•ΩÁé©ÁöÑ"", ""ARG0"", 32, 35], [""ÊòØ"", ""PRED"", 35, 36], [""Â¶ÇÊûúÂí±‰ª¨Âú®ÁûÑÂáÜÊ®°Âºè‰∏ãÂÜçÂè¨Âî§ÁöÑËØùÔºåÂ∞±‰ºöÂè¨Âî§Âá∫‰∏ÄÊû∂ÂèØ‰ª•È©æÈ©∂ÁöÑÊàòÊñó"", ""ARG1"", 37, 57]], [[""Â¶ÇÊûú"", ""ARGM-DIS"", 37, 38], [""Âí±‰ª¨"", ""ARG0"", 38, 39], [""Âú®ÁûÑÂáÜÊ®°Âºè‰∏ã"", ""ARGM-LOC"", 39, 43], [""ÂÜç"", ""ARGM-ADV"", 43, 44], [""Âè¨Âî§"", ""PRED"", 44, 45]], [[""È©æÈ©∂"", ""PRED"", 54, 55], [""ÊàòÊñó"", ""ARG1"", 56, 57]], [[""ÂèØ"", ""ARGM-ADV"", 58, 59], [""Âà´"", ""ARGM-ADV"", 59, 60], [""Â∞èÁúã"", ""PRED"", 60, 61], [""ËøôÊû∂ÊàòÊñóÊú∫"", ""ARG1"", 61, 64]], [[""Ë∑®Ë∂ä"", ""PRED"", 71, 72], [""Â§™Âπ≥Ê¥ã"", ""ARG1"", 72, 73]], [[""ÁßíÊùÄ"", ""PRED"", 73, 74], [""ÁõÆÂâç‰∫∫Á±ªÂà∂ÈÄ†ÁöÑÊâÄÊúâÈ£ûË°åÂô®"", ""ARG1"", 74, 80]], [[""ÁõÆÂâç"", ""ARGM-TMP"", 74, 75], [""‰∫∫Á±ª"", ""ARG0"", 75, 76], [""Âà∂ÈÄ†"", ""PRED"", 76, 77]], [[""Âø´"", ""ARGM-ADV"", 81, 82], [""Êù•"", ""PRED"", 82, 83], [""Êñ∞Âª∫Ê®°ÊãüÂô®"", ""ARG1"", 83, 85]], [[""Âø´"", ""ARGM-ADV"", 81, 82], [""Êñ∞Âª∫"", ""PRED"", 83, 84], [""Ê®°ÊãüÂô®"", ""ARG1"", 84, 85]], [[""Â∞ùËØï"", ""PRED"", 86, 87], [""‰∏Ä‰∏ã"", ""ARGM-ADV"", 87, 88]]]

**Expected behavior**
""srl"": [
    [[[""‰ªäÂ§©"", ""ARGM-TMP"", 0, 1], [""ÁªßÁª≠"", ""PRED"", 1, 2], [""Áé©Êñ∞ÁîüÊ®°ÊãüÂô®"", ""ARG1"", 2, 5]], [[""‰ªäÂ§©"", ""ARGM-TMP"", 0, 1], **_[""Áé©"", ""PRED"", 1, 3]_**, [""Êñ∞ÁîüÊ®°ÊãüÂô®"", ""ARG1"", 3, 5]], [[""Â§ßÂÆ∂"", ""ARG0"", 8, 9], [""ÈÉΩ"", ""ARGM-ADV"", 9, 10], [""Áü•ÈÅì"", ""PRED"", 10, 11]], [[""Â≠êÂºπÊï∞Èáè"", ""ARG1"", 14, 16], [""Êîπ"", ""PRED"", 16, 17], [""‰∫åÂçÅÂÖ≠"", ""ARG2"", 18, 19]], [[""ÁÑ∂Âêé"", ""ARGM-DIS"", 20, 21], [""Âí±‰ª¨"", ""ARG0"", 21, 22], [""ÂÜç"", ""ARGM-ADV"", 22, 23], [""Âè¨Âî§"", ""PRED"", 23, 24]], [[""Êõ¥"", ""ARGM-ADV"", 32, 33], [""Â•ΩÁé©"", ""PRED"", 33, 34]], [[""Êõ¥Â•ΩÁé©ÁöÑ"", ""ARG0"", 32, 35], [""ÊòØ"", ""PRED"", 35, 36], [""Â¶ÇÊûúÂí±‰ª¨Âú®ÁûÑÂáÜÊ®°Âºè‰∏ãÂÜçÂè¨Âî§ÁöÑËØùÔºåÂ∞±‰ºöÂè¨Âî§Âá∫‰∏ÄÊû∂ÂèØ‰ª•È©æÈ©∂ÁöÑÊàòÊñó"", ""ARG1"", 37, 57]], [[""Â¶ÇÊûú"", ""ARGM-DIS"", 37, 38], [""Âí±‰ª¨"", ""ARG0"", 38, 39], [""Âú®ÁûÑÂáÜÊ®°Âºè‰∏ã"", ""ARGM-LOC"", 39, 43], [""ÂÜç"", ""ARGM-ADV"", 43, 44], [""Âè¨Âî§"", ""PRED"", 44, 45]], [[""È©æÈ©∂"", ""PRED"", 54, 55], [""ÊàòÊñó"", ""ARG1"", 56, 57]], [[""ÂèØ"", ""ARGM-ADV"", 58, 59], [""Âà´"", ""ARGM-ADV"", 59, 60], [""Â∞èÁúã"", ""PRED"", 60, 61], [""ËøôÊû∂ÊàòÊñóÊú∫"", ""ARG1"", 61, 64]], [[""Ë∑®Ë∂ä"", ""PRED"", 71, 72], [""Â§™Âπ≥Ê¥ã"", ""ARG1"", 72, 73]], [[""ÁßíÊùÄ"", ""PRED"", 73, 74], [""ÁõÆÂâç‰∫∫Á±ªÂà∂ÈÄ†ÁöÑÊâÄÊúâÈ£ûË°åÂô®"", ""ARG1"", 74, 80]], [[""ÁõÆÂâç"", ""ARGM-TMP"", 74, 75], [""‰∫∫Á±ª"", ""ARG0"", 75, 76], [""Âà∂ÈÄ†"", ""PRED"", 76, 77]], [[""Âø´"", ""ARGM-ADV"", 81, 82], [""Êù•"", ""PRED"", 82, 83], [""Êñ∞Âª∫Ê®°ÊãüÂô®"", ""ARG1"", 83, 85]], [[""Âø´"", ""ARGM-ADV"", 81, 82], [""Êñ∞Âª∫"", ""PRED"", 83, 84], [""Ê®°ÊãüÂô®"", ""ARG1"", 84, 85]], [[""Â∞ùËØï"", ""PRED"", 86, 87], [""‰∏Ä‰∏ã"", ""ARGM-ADV"", 87, 88]]]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Python version:  3.8.10
- HanLP version:  latest(v2.1.0.a)

**Other info / logs**
nothing.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ÊúâÁöÑÊ®°ÂûãÂú®windowsÊó†Ê≥ï‰ΩøÁî®ÔºåÂõ†‰∏∫windows‰∏çÂÖÅËÆ∏ÂàõÂª∫Âêç‰∏∫conÁöÑÁõÆÂΩï,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÊúâÁöÑÊ®°ÂûãÂú®windowsÊó†Ê≥ï‰ΩøÁî®ÔºåÂõ†‰∏∫windows‰∏çÂÖÅËÆ∏ÂàõÂª∫Âêç‰∏∫conÁöÑÁõÆÂΩï„ÄÇ

ËøôÊòØwindowsÊó†Ê≥ï‰ΩøÁî®ÁöÑÁõÆÂΩïÂêçÂàóË°®Ôºö
CON, PRN, AUX, NUL, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, and LPT9.

Áî±‰∫é NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA Ê®°Âûã‰ºöËØïÂõæÂàõÂª∫‰∏Ä‰∏™ con ÁõÆÂΩïÔºåÂõ†Ê≠§‰ºöÂá∫Áé∞pythonÈîôËØØÔºö
NotADirectoryError: [WinError 267] ÁõÆÂΩïÂêçÁß∞Êó†Êïà„ÄÇ:

**Code to reproduce the issue**
Âú®windowsÊâßË°å‰∏ãÂàóÂëΩ‰ª§ÔºåÂú®‰∏ãËΩΩÂÆåÊØïÂêéÔºåÂ∞±‰ºöÁúãÂà∞‰∏äËø∞ÈîôËØØ„ÄÇ
```python
HanLP = hanlp.load(hanlp.pretrained.mtl.NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA)
```

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
‰ª£Á†ÅÂêàÂπ∂,ÊúÄÊñ∞‰ª£Á†ÅÂêàÂπ∂
DoubleArrayTrieÈáåÁöÑLongestSearcherÁöÑnextÊñπÊ≥ïÈúÄË¶ÅËøõË°åÂº∫ÂåñÔºåÂΩì‰º†ÂÖ•ÁöÑtreemapÁöÑvalue‰∏∫nullÊó∂‚Ä¶,
DoubleArrayTrieÈáåÁöÑLongestSearcherÁöÑnextÊñπÊ≥ïÔºåÂΩì‰º†ÂÖ•ÁöÑtreemapÁöÑvalue‰∏∫nullÊó∂Ôºå‰ºöÂºïÂèëbug,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**

DoubleArrayTrieÈáåÁöÑLongestSearcherÁöÑnextÊñπÊ≥ïÔºåÂΩì‰º†ÂÖ•ÁöÑtreemapÁöÑvalue‰∏∫nullÊó∂Ôºå‰ºöÂºïÂèëbug
![image](https://user-images.githubusercontent.com/13550295/130592417-32ae4155-0ea3-4c7a-ad17-9fd2c67e92a5.png)


**Code to reproduce the issue**

```java
 public void testLongestSearcherWithNullValue() {
        TreeMap<String, String> buildFrom = new TreeMap<String, String>();
        TreeMap<String, String> buildFromValueNull = new TreeMap<String, String>();
        String[] keys = new String[]{""he"", ""her"", ""his""};
        for (String key : keys) {
            buildFrom.put(key, key);
            buildFromValueNull.put(key, null);
        }
        DoubleArrayTrie<String> trie = new DoubleArrayTrie<String>(buildFrom);
        DoubleArrayTrie<String> trieValueNull = new DoubleArrayTrie<String>(buildFromValueNull);

        String text = ""her3he6his-hers! "";

        DoubleArrayTrie<String>.LongestSearcher searcher = trie.getLongestSearcher(text.toCharArray(), 0);
        DoubleArrayTrie<String>.LongestSearcher searcherValueNull = trieValueNull.getLongestSearcher(text.toCharArray(), 0);

        while (true) {
            boolean next = searcher.next();
            boolean nextValueNull = searcherValueNull.next();

            if (next && nextValueNull) {
                assertTrue(searcher.begin == searcherValueNull.begin && searcher.length == searcherValueNull.length);
            } else if (next || nextValueNull) {
                assert false;
                break;
            } else {
                break;
            }
        }
    }
```

**Describe the current behavior**
Áî±‰∫émap‰º†ÂÖ•ÁöÑvalue‰∏∫nullÔºåÂç≥‰ΩøÂ≠òÂú®Ôºå‰πüÊü•‰∏çÂà∞

**Expected behavior**
A clear and concise description of what you expected to happen.
Ê†πÊçÆlengthÊàñËÄÖindexËøõË°åÂà§Êñ≠
‰øÆÂ§çÂèÇËßÅpull request (https://github.com/hankcs/HanLP/pull/1674)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: xxx
- HanLP version: 1.8.2

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
import hanlp bug on hanlp-2.1.0a55,"import hanlpÁöÑÊó∂ÂÄôÔºåÂá∫Áé∞AttributeError 
---------------------------------------------------------------------------
AttributeError       Traceback (most recent call last)
/var/folders/31/7mbjndl966d92drq1zvt9t3c0000gn/T/ipykernel_30433/3724283222.py in <module>
----> 1 import hanlp #Successfully installed hanlp-2.1.0a55

~/anaconda3/lib/python3.7/site-packages/hanlp/__init__.py in <module>
      8 from hanlp.version import __version__
      9 
---> 10 hanlp.utils.ls_resource_in_module(hanlp.pretrained)
     11 
     12 
AttributeError: module 'hanlp' has no attribute 'utils' 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS montery 
- Python version: Python 3.7.3
- HanLP version:hanlp-2.1.0a55

**Other info / logs** 

* [x] I've completed this form and searched the web for solutions. "
pos/ctbËØçÊÄßËØÜÂà´Â∑ÆÂºÇËæÉÂ§ß,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
„ÄêÊï¥ÁÆ±„ÄëÈáëÈæôÈ±º‰∫îÂ∏∏Á®ªËä±È¶ôÂéüÈ¶ôÁ®ªÂ§ßÁ±≥2.5kg*4/ÁÆ±Êñ∞Á±≥

‰∏äÈù¢ËøôÊù°ËØ≠Âè•‰∏≠2.5kg pos/ctbËØçÊÄßÊ†áÊ≥®Êó∂ÔºåË¢´Ê†áÊ≥®Êàê‰∫ÜurlÔºåÊÑüËßâÂ∑ÆÂºÇÊúâÁÇπÂ§ßÔºåÂπ∂‰∏îÈÄöËøá *Êµ∑ÈáèÁ∫ßnative API* ÁöÑpythonËØ≠Ë®ÄËøõË°åËØÜÂà´Ôºå‰ªçÁÑ∂Ë¢´ËØÜÂà´Êàê‰∫ÜURL
![ÂõæÁâá](https://user-images.githubusercontent.com/56599784/129319120-4df90514-9747-4f02-9beb-a0ff7ac1bdee.png)
![ÂõæÁâá](https://user-images.githubusercontent.com/56599784/129319786-9ba92cb4-e99f-4f28-a63a-6e225e19d661.png)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
test = ['„ÄêÊï¥ÁÆ±„ÄëÈáëÈæôÈ±º‰∫îÂ∏∏Á®ªËä±È¶ôÂéüÈ¶ôÁ®ªÂ§ßÁ±≥2.5kg*4/ÁÆ±Êñ∞Á±≥']
HanLP: MultiTaskLearning = hanlp.load(
    hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)
lp = HanLP(test)
print(lp)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: 3.8.10
- HanLP version: test = ['„ÄêÊï¥ÁÆ±„ÄëÈáëÈæôÈ±º‰∫îÂ∏∏Á®ªËä±È¶ôÂéüÈ¶ôÁ®ªÂ§ßÁ±≥2.5kg*4/ÁÆ±Êñ∞Á±≥']

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ÂÖ≥‰∫éÂ§öÈü≥Â≠ó‚ÄúËéé‚ÄùÁöÑÊãºÈü≥Ëß£Êûê‰ºòÂåñÂª∫ËÆÆ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
**Ëéé**Ëøô‰∏™Â§öÈü≥Â≠ó‰∏ÄËà¨Â∏∏ËßÅ‰∫é‰∫∫ÂêçÔºåÊØîÂ¶ÇÔºö`ËééÂ£´ÊØî‰∫ö`Ôºå`‰∏ΩËéé`Á≠âÔºåÂΩìÂâçÂèëÁé∞Âú®`ËééËçâ`Ëøô‰∏™ËØçÊàñËÄÖ‰∏Ä‰∫õÂè§Êñá‰∏≠ËØª‰Ωú`suo`„ÄÇ
ÁõÆÂâçÁöÑÊãºÈü≥ËØçÂÖ∏ÈáåÂ∞Ü`suo`ÊéíÂú®‰∫Ü`sha`ÂâçÈù¢ÔºåÂØºËá¥ÊâÄÊúâ‰∫∫Âêç„ÄÅÂú∞ÂêçÁöÑÊãºÈü≥ËΩ¨Êç¢ÂÖ®ÈÉΩ‰∏çÊ≠£Á°ÆÔºåÊòØÂê¶Â∞Ü`sha`‰Ωú‰∏∫ËØçÂÖ∏ÁöÑÁ¨¨‰∏ÄÈ°∫‰ΩçËß£ÊûêÊØîËæÉÂêàÈÄÇÂë¢Ôºü

**Will this change the current api? How?**
ÂΩìÂâçÁöÑAPI‰∏çÈúÄË¶ÅË∞ÉÊï¥ÔºåÂè™ÈúÄË¶ÅË∞ÉÊï¥ËØçÂÖ∏Âç≥ÂèØÔºåÂΩìÁÑ∂Áî®Êà∑ÂèØ‰ª•Ëá™Â∑±Ë∞ÉÊï¥ÊãºÈü≥ËØçÂÖ∏Ôºå‰ΩÜÊòØËøô‰∏™Â§öÈü≥Â≠óÁ°ÆÂÆûÊòØ`sha1`Áî®ÁöÑÊØîËæÉÈ¢ëÁπÅÔºåÂ∫îÁî®Êõ¥ÂπøÊ≥õÔºåÂõ†Ê≠§ÊâçÊÅ≥ËØ∑Âú®È°πÁõÆ‰∏≠Ë∞ÉÊï¥„ÄÇ
‰øÆÊîπËØçÂÖ∏Êñá‰ª∂[data/dictionary/pinyin/pinyin.txt](https://github.com/hankcs/HanLP/blob/1.x/data/dictionary/pinyin/pinyin.txt#L21325)ÔºåÂ∞Ü`Ëéé=suo1,sha1`Ë∞ÉÊï¥Êàê`Ëéé=sha1,suo1`ÔºåÂêåÊó∂Ê∑ªÂä†ËØçÂÖ∏`Ë∏èËééË°å=ta4,suo1,xing2`„ÄÇ

**Who will benefit with this feature?**
Âü∫Êú¨ÊâÄÊúâ‰∫∫ÈÉΩ‰ºöÂèóÁõä‰∫éËøôÊ¨°ÊîπÂä®ÔºåÊØïÁ´ü`ËééËçâ`‰πüÂ∑≤ÁªèÂÆö‰πâÂà∞ËØçÂÖ∏‰∏≠‰∫ÜÔºåÂâ©‰ΩôÊÉÖÂÜµÂü∫Êú¨ÈÉΩÊòØËß£ÈáäÊàê`sha`„ÄÇ

**Are you willing to contribute it (Yes/No):**
Â¶ÇÊûúÊúâÂøÖË¶ÅÁöÑËØùÔºåÊàëÂèØ‰ª•Êèê‰∫§PR„ÄÇÂΩìÁÑ∂‰ΩúËÄÖÁõ¥Êé•Ë∞ÉÊï¥‰ºöÊõ¥Âø´‰∫õ„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: Java 8
- HanLP version:
```xml
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.8.2</version>
</dependency>
```

**Any other info**
‰ª£Á†ÅÊÆµÔºö
```java
System.out.println(HanLP.convertToPinyinString(""„ÄäÁΩóÂØÜÊ¨ß‰∏éÊú±‰∏ΩÂè∂„ÄãÊòØËééÂ£´ÊØî‰∫öÂàõ‰ΩúÁöÑ„ÄÇ"", ""_"", false));
```
ËæìÂá∫ÁªìÊûúÔºö
```text
„Ää_luo_mi_ou_yu_zhu_li_ye_„Äã_shi_suo_shi_bi_ya_chuang_zuo_de_„ÄÇ
```

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
pip install hanlp[full] ÁöÑÊó∂ÂÄôÊúâ‰æùËµñÂÜ≤Á™Å,"
**Describe the bug**
ËøõË°åfullÁöÑhanlpÂÆâË£ÖÊó∂ÔºåÊúâ2.1Âíå2.0‰∏§‰∏™ÁâàÊú¨ÂêåÊó∂ÂÆâË£ÖËÄå‰∏î‰æùËµñÁöÑtfÁâàÊú¨‰∏çÂêåÔºåÊó†Ê≥ïÂÆâË£Ö„ÄÇ

WARNING: hanlp 2.0.0a5 does not provide the extra 'full'
  Using cached hanlp-2.0.0a4.tar.gz (872 kB)
WARNING: hanlp 2.0.0a4 does not provide the extra 'full'
  Using cached hanlp-2.0.0a3.tar.gz (867 kB)
WARNING: hanlp 2.0.0a3 does not provide the extra 'full'
ERROR: Cannot install hanlp[full]==2.0.0a10, hanlp[full]==2.0.0a11, hanlp[full]==2.0.0a12, hanlp[full]==2.0.0a13, hanlp[full]==2.0.0a14, hanlp[full]==2.0.0a15, hanlp[full]==2.0.0a16, h
anlp[full]==2.0.0a17, hanlp[full]==2.0.0a18, hanlp[full]==2.0.0a19, hanlp[full]==2.0.0a20, hanlp[full]==2.0.0a21, hanlp[full]==2.0.0a22, hanlp[full]==2.0.0a24, hanlp[full]==2.0.0a25, h
anlp[full]==2.0.0a26, hanlp[full]==2.0.0a27, hanlp[full]==2.0.0a28, hanlp[full]==2.0.0a29, hanlp[full]==2.0.0a3, hanlp[full]==2.0.0a30, hanlp[full]==2.0.0a31, hanlp[full]==2.0.0a32, ha
nlp[full]==2.0.0a33, hanlp[full]==2.0.0a34, hanlp[full]==2.0.0a35, hanlp[full]==2.0.0a36, hanlp[full]==2.0.0a37, hanlp[full]==2.0.0a38, hanlp[full]==2.0.0a39, hanlp[full]==2.0.0a4, han
lp[full]==2.0.0a40, hanlp[full]==2.0.0a41, hanlp[full]==2.0.0a42, hanlp[full]==2.0.0a43, hanlp[full]==2.0.0a44, hanlp[full]==2.0.0a45, hanlp[full]==2.0.0a46, hanlp[full]==2.0.0a47, han
lp[full]==2.0.0a48, hanlp[full]==2.0.0a49, hanlp[full]==2.0.0a5, hanlp[full]==2.0.0a50, hanlp[full]==2.0.0a51, hanlp[full]==2.0.0a52, hanlp[full]==2.0.0a53, hanlp[full]==2.0.0a54, hanl
p[full]==2.0.0a55, hanlp[full]==2.0.0a56, hanlp[full]==2.0.0a57, hanlp[full]==2.0.0a58, hanlp[full]==2.0.0a59, hanlp[full]==2.0.0a6, hanlp[full]==2.0.0a60, hanlp[full]==2.0.0a61, hanlp
[full]==2.0.0a62, hanlp[full]==2.0.0a63, hanlp[full]==2.0.0a64, hanlp[full]==2.0.0a65, hanlp[full]==2.0.0a66, hanlp[full]==2.0.0a67, hanlp[full]==2.0.0a68, hanlp[full]==2.0.0a69, hanlp
[full]==2.0.0a8, hanlp[full]==2.0.0a9, hanlp[full]==2.1.0a0, hanlp[full]==2.1.0a1, hanlp[full]==2.1.0a10, hanlp[full]==2.1.0a11, hanlp[full]==2.1.0a12, hanlp[full]==2.1.0a13, hanlp[ful
l]==2.1.0a14, hanlp[full]==2.1.0a15, hanlp[full]==2.1.0a16, hanlp[full]==2.1.0a17, hanlp[full]==2.1.0a18, hanlp[full]==2.1.0a19, hanlp[full]==2.1.0a2, hanlp[full]==2.1.0a20, hanlp[full
]==2.1.0a21, hanlp[full]==2.1.0a22, hanlp[full]==2.1.0a23, hanlp[full]==2.1.0a24, hanlp[full]==2.1.0a25, hanlp[full]==2.1.0a26, hanlp[full]==2.1.0a27, hanlp[full]==2.1.0a28, hanlp[full
]==2.1.0a29, hanlp[full]==2.1.0a3, hanlp[full]==2.1.0a30, hanlp[full]==2.1.0a31, hanlp[full]==2.1.0a32, hanlp[full]==2.1.0a33, hanlp[full]==2.1.0a34, hanlp[full]==2.1.0a35, hanlp[full]
==2.1.0a36, hanlp[full]==2.1.0a37, hanlp[full]==2.1.0a38, hanlp[full]==2.1.0a4, hanlp[full]==2.1.0a41, hanlp[full]==2.1.0a42, hanlp[full]==2.1.0a43, hanlp[full]==2.1.0a44, hanlp[full]=
=2.1.0a45, hanlp[full]==2.1.0a46, hanlp[full]==2.1.0a47, hanlp[full]==2.1.0a48, hanlp[full]==2.1.0a5, hanlp[full]==2.1.0a50, hanlp[full]==2.1.0a51, hanlp[full]==2.1.0a52, hanlp[full]==
2.1.0a53, hanlp[full]==2.1.0a6, hanlp[full]==2.1.0a7, hanlp[full]==2.1.0a8 and hanlp[full]==2.1.0a9 because these package versions have conflicting dependencies.

The conflict is caused by:
    hanlp[full] 2.1.0a53 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a52 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a51 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a50 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a48 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a47 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a46 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a45 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a44 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a43 depends on tensorflow==2.3.0; extra == ""full""
    hanlp[full] 2.1.0a42 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a41 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a38 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a37 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a36 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a35 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a34 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a33 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a32 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a31 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a30 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a29 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a28 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a27 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a26 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a25 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a24 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a23 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a22 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a21 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a20 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a19 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a18 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a17 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a16 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a15 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a14 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a13 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a12 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a11 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a10 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a9 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a8 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a7 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a6 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a5 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a4 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a3 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a2 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a1 depends on tensorflow==2.3.0
    hanlp[full] 2.1.0a0 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a69 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a68 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a67 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a66 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a65 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a64 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a63 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a62 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a61 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a60 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a59 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a58 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a57 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a56 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a55 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a54 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a53 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a52 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a51 depends on tensorflow==2.3.0
    hanlp[full] 2.0.0a50 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a49 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a48 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a47 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a46 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a45 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a44 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a43 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a42 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a41 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a40 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a39 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a38 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a37 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a36 depends on tensorflow==2.1.0
    hanlp[full] 2.0.0a35 depends on tensorflow==2.1.0
    ...


**Code to reproduce the issue**

```python
pip install hanlp[full] 
```

**Describe the current behavior**
pipÊèêÁ§∫ÂÜ≤Á™ÅÔºåÊó†Ê≥ïÂÆâË£Ö

**Expected behavior**
Ê≠£Â∏∏ÂÆâË£Ö
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Python version: 3.9.5
- HanLP version: full

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ImportError: cannot import name 'TFAutoModel',"**Describe the bug**
ImportError: cannot import name 'TFAutoModel'

**Code to reproduce the issue**
from transformers import BertTokenizer, BertConfig, PretrainedConfig, TFAutoModel, \

**Describe the current behavior**
ImportError: cannot import name 'TFAutoModel'

**Expected behavior**
ImportError: cannot import name 'TFAutoModel'

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Darwin-19.6.0-x86_64-i386-64bit
- Python version:3.6.9
- HanLP version:2.1.0-alpha.53

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/tok/pku98_6m_conv_ngram_20200110_134736.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp_common/reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp_common/reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/components/tok_tf.py"", line 12, in <module>
    from hanlp.components.taggers.transformers.transformer_tagger_tf import TransformerTaggerTF
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger_tf.py"", line 11, in <module>
    from hanlp.layers.transformers.loader_tf import build_transformer
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/layers/transformers/loader_tf.py"", line 12, in <module>
    from hanlp.layers.transformers.tf_imports import zh_albert_models_google, bert_models_google
  File ""/Users/yuanxiao/.pyenv/versions/3.6.9/lib/python3.6/site-packages/hanlp/layers/transformers/tf_imports.py"", line 5, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, TFAutoModel, \
ImportError: cannot import name 'TFAutoModel'
=================================ERROR LOG ENDS=================================
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above and the system info below.
OS: Darwin-19.6.0-x86_64-i386-64bit
Python: 3.6.9
PyTorch: 1.9.0
HanLP: 2.1.0-alpha.53

* [x] I've completed this form and searched the web for solutions."
cannot import name 'albert_models_tfhub' from 'bert' ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
bertÂåÖÈáåÊ≤°Êúâalbert_models_tfhub
  File ""C:\ProgramData\Anaconda3\envs\nlp\lib\site-packages\hanlp\layers\transformers\loader_tf.py"", line 9, in <module>
    from bert import albert_models_tfhub, fetch_tfhub_albert_model, load_stock_weights
ImportError: cannot import name 'albert_models_tfhub' from 'bert' (C:\ProgramData\Anaconda3\envs\nlp\lib\site-packages\bert\__init__.py)

**Code to reproduce the issue**
import hanlp
import bert
from hanlp_restful import HanLPClient

test1 = 'ËëõÈπ§ÂÜõÂÖàÁîü:ÂçöÂ£´Â≠¶‰Ωç„ÄÇ2008Âπ¥Ëá≥2011Âπ¥Â∞±ËÅå‰∫é‰∏≠ËØö‰ø°ËØÅÂà∏ËØÑ‰º∞ÊúâÈôêÂÖ¨Âè∏„ÄÅ‰∏≠ËØö‰ø°ÂõΩÈôÖ‰ø°Áî®ËØÑÁ∫ßÊúâÈôêÂÖ¨Âè∏„ÄÇ2011Âπ¥11ÊúàÂä†ÁõüÈì∂ÂçéÂü∫ÈáëÁÆ°ÁêÜÊúâÈôêÂÖ¨Âè∏,ÂéÜ‰ªªÁ†îÁ©∂Âëò„ÄÅÂü∫ÈáëÁªèÁêÜÂä©ÁêÜ„ÄÇÁé∞‰ªªÊäïËµÑÁÆ°ÁêÜ‰∏âÈÉ®Âü∫ÈáëÁªèÁêÜ„ÄÇËá™2014Âπ¥10Êúà8Êó•Ëµ∑ÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏≠ËØÅ‰∏≠Á•®50ÊåáÊï∞ÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë(LOF)‚Äù‚ÄúÈì∂Âçé‰ø°Áî®ÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë(LOF)‚ÄùÁöÑÂü∫ÈáëÁªèÁêÜ;Ëá™2014Âπ¥10Êúà8Êó•Ëµ∑Ëá≥2016Âπ¥4Êúà25Êó•ÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏≠ËØÅÊàêÈïøËÇ°ÂÄ∫ÊÅíÂÆöÁªÑÂêà30/70ÊåáÊï∞ËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2015Âπ¥4Êúà24Êó•Ëµ∑ÊãÖ‰ªª‚ÄúÈì∂ÂçéÊ≥∞Âà©ÁÅµÊ¥ªÈÖçÁΩÆÊ∑∑ÂêàÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2015Âπ¥5Êúà6Êó•Ëµ∑ÊãÖ‰ªª‚ÄúÈì∂ÂçéÊÅíÂà©ÁÅµÊ¥ªÈÖçÁΩÆÊ∑∑ÂêàÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2015Âπ¥6Êúà17Êó•Ëµ∑ÊãÖ‰ªª‚ÄúÈì∂Âçé‰ø°Áî®ÂèåÂà©ÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2016Âπ¥8Êúà5Êó•Ëµ∑ÊãÖ‰ªª‚ÄúÈì∂ÂçéÈÄöÂà©ÁÅµÊ¥ªÈÖçÁΩÆÊ∑∑ÂêàÂûãËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2016Âπ¥12Êúà5Êó•ÂºÄÂßãÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏äËØÅ10Âπ¥ÊúüÂõΩÂÄ∫ÊåáÊï∞ËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2016Âπ¥12Êúà5Êó•ÂºÄÂßãÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏äËØÅ5Âπ¥ÊúüÂõΩÂÄ∫ÊåáÊï∞ËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2017Âπ¥4Êúà17Êó•ÂºÄÂßãÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏≠ÂÄ∫-10Âπ¥ÊúüÂõΩÂÄ∫ÊúüË¥ßÊúüÈôêÂåπÈÖçÈáëËûçÂÄ∫ÊåáÊï∞ËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ;Ëá™2017Âπ¥4Êúà17Êó•ÂºÄÂßãÊãÖ‰ªª‚ÄúÈì∂Âçé‰∏≠ÂÄ∫-5Âπ¥ÊúüÂõΩÂÄ∫ÊúüË¥ßÊúüÈôêÂåπÈÖçÈáëËûçÂÄ∫ÊåáÊï∞ËØÅÂà∏ÊäïËµÑÂü∫Èáë‚ÄùÂü∫ÈáëÁªèÁêÜ„ÄÇ2018Âπ¥7ÊúàÂä†ÂÖ•ÈïøÁõõÂü∫ÈáëÁÆ°ÁêÜÊúâÈôêÂÖ¨Âè∏,Áé∞‰ªªÂõ∫ÂÆöÊî∂ÁõäÈÉ®ÊâßË°åÊÄªÁõë,Ëá™2018Âπ¥12Êúà6Êó•Ëµ∑‰ªªÈïøÁõõÁõõÁê™‰∏ÄÂπ¥ÊúüÂÆöÊúüÂºÄÊîæÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫ÈáëÂü∫ÈáëÁªèÁêÜ„ÄÇÊãü‰ªªÈïøÁõõÂÆâÈë´‰∏≠Áü≠ÂÄ∫ÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫ÈáëÂü∫ÈáëÁªèÁêÜ„ÄÇ2020Âπ¥9Êúà29Êó•Ëµ∑ÊãÖ‰ªªÈïøÁõõÁõõË£ïÁ∫ØÂÄ∫ÂÄ∫Âà∏ÂûãËØÅÂà∏ÊäïËµÑÂü∫ÈáëÂü∫ÈáëÁªèÁêÜ„ÄÇ'

HanLP = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)
HanLP(test1)

```python
```

**Describe the current behavior**
ÂêØÂä®‰∏ç‰∫Ü

**Expected behavior**
Ê≠£Â∏∏ÂêØÂä®

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
centos Âä†ËΩΩ‰æùÂ≠òÂè•Ê≥ïÊä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÂÖ∂‰ªñÂäüËÉΩOKÔºå‰ΩÜÊòØÂä†ËΩΩ‰æùÂ≠òÂè•Ê≥ïÁöÑÊó∂ÂÄôÂá∫Èîô‰∫Ü

**Code to reproduce the issue**
Êä•ÈîôÂÜÖÂÆπ
dependency_info = HanLP.parseDependency(sentence)
jpype._jclass.ExceptionInInitializerError: None
```python
```

**Describe the current behavior**
Áõ¥Êé•Êä•ÈîôÈÄÄÂá∫

**Expected behavior**
winÁ≥ªÁªüÈÉΩÂ•ΩÂ•ΩÁöÑÔºåÈÉ®ÁΩ≤Âà∞linuxÂ∞±Ë∑™‰∫ÜÔºåÂ∏åÊúõËÉΩËøêË°åËµ∑Êù•Âêß

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  centos7
- Python version: 3.6.8
- HanLP version: 1.7.5
- JDK versionÔºö1.8.0_252

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Failed to load model,"
**Describe the bug**
Failed to load model

**Code to reproduce the issue**
HanLP = hanlp.load(hanlp.pretrained.classifiers.CHNSENTICORP_BERT_BASE_ZH)

**Describe the current behavior**
Failed to load model

**Expected behavior**
Failed to load model

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows-10-10.0.19041-SP0
- Python version:3.8.3
- HanLP version:2.1.0-alpha.50

**Other info / logs**
Failed to load https://file.hankcs.com/hanlp/classification/chnsenticorp_bert_base_20200104_164655.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\utils\component_util.py"", line 81, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\keras_component.py"", line 214, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\keras_component.py"", line 223, in build
    self.transform.build_config()
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\common\transform_tf.py"", line 54, in build_config
    self.output_types, self.output_shapes, self.padding_values = self.create_types_shapes_values()
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp\components\classifiers\transformer_classifier_tf.py"", line 78, in create_types_shapes_values
    shapes = ([max_length], [max_length], [max_length]), [None, ] if self.config.multi_label else []
  File ""c:\users\admin\envs\work\lib\site-packages\hanlp_common\structure.py"", line 95, in __getattr__
    return self.__getitem__(key)
KeyError: 'multi_label'
=================================ERROR LOG ENDS=================================
Please upgrade hanlp with:
pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above and the system info below.
OS: Windows-10-10.0.19041-SP0
Python: 3.8.3
PyTorch: 1.8.1+cu102
HanLP: 2.1.0-alpha.50
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

* [x] I've completed this form and searched the web for solutions."
Â¢ûÂä†Ëá™ÂÆö‰πâËØçÂÖ∏binÊòØÂê¶‰∏ªÂä®Ê†πÊçÆÊñá‰ª∂ÂèòÊõ¥Êó∂Èó¥Êõ¥Êñ∞ÁöÑÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫true,Â¢ûÂä†ÈÖçÁΩÆCustomDictionaryBinUpdateÔºåÁî®‰∫éÂà§Êñ≠ÊòØÂê¶‰∏ªÂä®Ê†πÊçÆÊñá‰ª∂Êó∂Èó¥ÂèòÈù©Êù•Êõ¥Êñ∞CustomDictionary.txt.binÔºåÈªòËÆ§‰∏∫true
ËÉΩÂê¶Â∞ÜËá™ÂÆö‰πâËØçÂÖ∏binÊõ¥Êñ∞Âä†ËΩΩÊîπ‰∏∫ÂèØÈÖçÁΩÆ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Âõ†‰∏∫Âú®ÂºÄÂèëelasticsearchÁõ∏ÂÖ≥hanlpÊèí‰ª∂Êó∂ÔºåelasticsearchÂú®7.11.0‰πãÂêéÈôêÂà∂‰∫ÜÊèí‰ª∂ÁöÑÊùÉÈôêÔºåÂè™ÊúâËØªÂèñÊñá‰ª∂ÊùÉÈôêÔºå‰ΩÜHanLPÂú®1.8.0‰πãÂêéÂ¢ûÂä†‰∫ÜËØçÂÖ∏ÁÉ≠Êõ¥Êñ∞ÔºåÂú®ËØªÂèñcustom dictionaryÊó∂Ôºå‰ºöÂà§Êñ≠binÊñá‰ª∂ÊòØÂê¶ÊòØÊúÄÊñ∞Ôºå‰ªéËÄåÂºÄÂêØÁÉ≠Âä†ËΩΩÔºàÂà†Èô§Âπ∂ÈáçÊñ∞ÊûÑÂª∫ÔºâÔºå‰ΩÜelasticsearchÊèí‰ª∂ÂÆâË£ÖÊó∂ÔºåËØçÂÖ∏Êñá‰ª∂ÁöÑÊúÄÂêéÊõ¥Êñ∞Êó∂Èó¥‰∏çÂÆöÔºå‰ªéËÄåÂØºËá¥ÁÉ≠Âä†ËΩΩÔºåÂõ†ÊùÉÈôêÈóÆÈ¢òÁÉ≠Âä†ËΩΩÂ§±Ë¥•

**Will this change the current api? How?**
ËÉΩÂê¶Â∞ÜËØçÂÖ∏ÁÉ≠Âä†ËΩΩÊîπ‰∏∫ÂèØÈÖçÁΩÆÔºåÂ¢ûÂä†ÂºÄÂÖ≥

**Who will benefit with this feature?**
elasticsearchÂÖ≥‰∫éhanlpÁöÑÂàÜËØçÊèí‰ª∂

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version: >= 1.8.0

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Âü∑Ë°åÂàÜË©ûÊúÉÁî¢Áîü RuntimeError: cannot perform reduction function argmax on a tensor with no elements because the operation does not have an identity,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÊüêÂÄãÂ≠ó‰∏≤‰ΩøÁî®Âü∑Ë°åÂàÜË©ûÊúÉÁî¢Áîü exception„ÄÇ

**Code to reproduce the issue**
```python
import hanlp
import re


HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ÈÄôÂÄã‰∏çÊúÉÊúâ exception
#HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH) # ÈÄôÂÄãÊúÉÊúâ exception


def segmenter_for_report(sentence):
    import re

    regex_patterns = {}
    regex_patterns['patter1'] = re.compile('\s+')
    regex_patterns['patter2'] = re.compile('[^\s]+')

    tokens = []

    s = sentence
    cur_pos = 0
    token_start_pos_base = cur_pos
    while True:
        max_matched_len = 0
        next_token = None
        token_type = None

        # greedy search for the longest pattern from the beginning of 's'
        print('Segmenting sentence: %s' % s)
        for k, p in regex_patterns.items():
            # return matchobject if matches
            m = p.match(s)
            if m:
                print('„Äê%s„Äë matched pattern: %s, start: %d, end: %d' %(s, k, m.start(0), m.end(0)))
                cur_matched_len = m.end(0) - m.start(0)
                if max_matched_len < cur_matched_len:
                    max_matched_len = cur_matched_len
                    next_token = m.group(0)
                    token_type = k

        if token_type:
            print('longest matched pattern type: %s, token: %s' % (token_type, next_token))
        else:
            print('Cannot find the matched pattern. sentence: %s' % s)

        # split off the longest token we found.
        if next_token:
            string_left = s[max_matched_len:]
            string_left_trimmed = s[max_matched_len:].lstrip()

            # process the token we found
            token_start_pos = token_start_pos_base
            token_end_pos = token_start_pos_base + max_matched_len
            tokens.append(next_token)

            s = string_left_trimmed
            token_start_pos_base = token_end_pos + (
                        len(string_left) - len(string_left_trimmed))  # the next base
                
        else:
            if len(s.strip()) > 0:
                print('Unprocessed substring: %s' % s)
                break

        if len(s) == 0:
            break
    return tokens


test_cases = [
    '( Õ°¬∞ Õú ñ Õ° ¬∞)',
]    

for c in test_cases:
    seg_phrases = segmenter_for_report(c)
    for seg_text in seg_phrases:
        tok_results = HanLP(seg_text, tasks=['tok/fine', 'pos/ctb'])
        print('result: %s' % tok_results)
```

**Describe the current behavior**
ËÄå‰∏î‰ª•‰∏äÁãÄÊ≥ÅÂè™Êúâ‰ΩøÁî® hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH ÊâçÊúÉÁôºÁîü
‰ΩøÁî® hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH ‰∏çÊúÉÁôºÁîü

**Expected behavior**
Ëá≥Â∞ëÊ≠£Â∏∏ËøîÂõû„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Python version: Python 3.7.3
- HanLP version: 2.1.0a38

**Other info / logs**
```
Segmenting sentence: ( Õ°¬∞ Õú ñ Õ° ¬∞)
„Äê( Õ°¬∞ Õú ñ Õ° ¬∞)„Äë matched pattern: patter2, start: 0, end: 1
longest matched pattern type: patter2, token: (
Segmenting sentence: Õ°¬∞ Õú ñ Õ° ¬∞)
„ÄêÕ°¬∞ Õú ñ Õ° ¬∞)„Äë matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: Õ°¬∞
Segmenting sentence: Õú ñ Õ° ¬∞)
„ÄêÕú ñ Õ° ¬∞)„Äë matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: Õú ñ
Segmenting sentence: Õ° ¬∞)
„ÄêÕ° ¬∞)„Äë matched pattern: patter2, start: 0, end: 1
longest matched pattern type: patter2, token: Õ°
Segmenting sentence: ¬∞)
„Äê¬∞)„Äë matched pattern: patter2, start: 0, end: 2
longest matched pattern type: patter2, token: ¬∞)
result: {
  ""tok/fine"": [
    ""(""
  ],
  ""pos/ctb"": [
    ""PU""
  ]
}
result: {
  ""tok/fine"": [
    ""¬∞""
  ],
  ""pos/ctb"": [
    ""PU""
  ]
}
result: {
  ""tok/fine"": [
    "" ñ""
  ],
  ""pos/ctb"": [
    ""NN""
  ]
}
Traceback (most recent call last):
  File ""tests\report_hanlp.py"", line 75, in <module>
    tok_results = HanLP(seg_text, tasks=['tok/fine', 'pos/ctb'])
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\torch\autograd\grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\common\torch_component.py"", line 633, in __call__
    **kwargs))
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\common\component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 505, in predict
    cls_is_bos=cls_is_bos, sep_is_eos=sep_is_eos)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 592, in predict_task
    self.decode_output(output_dict, batch, output_key)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\multi_task_learning.py"", line 732, in decode_output
    self.model.decoders[task_name])
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\mtl\tasks\tok\tag_tok.py"", line 123, in decode_output
    return TransformerTaggingTokenizer.decode_output(self, output, mask, batch, decoder)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\tokenizers\transformer.py"", line 97, in decode_output
    output = super().decode_output(logits, mask, batch, model)
  File ""D:\MyProjects\boxservice\env\lib\site-packages\hanlp\components\taggers\tagger.py"", line 64, in decode_output
    return logits.argmax(-1)
RuntimeError: cannot perform reduction function argmax on a tensor with no elements because the operation does not have an identity
```

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Êúâ‰∫õ‰∏≠ÊñáÂ≠óÊúÉË™§Âà§ÁÇ∫Ê®ôÈªûÁ¨¶ËôüÔºàctb: PU),"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Êúâ‰∫õ‰∏≠ÊñáÂ≠óÊúÉË™§Âà§ÁÇ∫Ê®ôÈªûÁ¨¶Ëôü.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)

test_cases = [
        'ÂóØ..Âïä..ÊàëÂÄëÊ≤íÊúâÊî∂Âà∞Ê™¢ËàâÂïä',
        'ÊôÆ‰∏ÅÂ∞±Â§ßÈáèÁöÑ‰æÜÊ±ôË°äÂ∏åÊãâËïä',
        'ÂéªÊ¥óÈÇ£ÂÄãÈπº ÂéªÊ¥óÈÇ£ÂÄãÈπΩ',
        'Á¶èÂ≥∂ÈÇ£ÂÄãÊòØÁàÜÁÇ∏ÁöÑÂïèÈ°å Ê†∏ÈõªÂª†ÈÇ£ÂÄãÊ∞öÊ¥ó‰∏çÊéâÁöÑ',
        'ÂÉèÂÆÉÁöÑÊ®°ÂºèÂèØ‰ª•ÊîπÊàê15ÂàÜÈêòÊàñËÄÖÊòØÊ∞∏..Ê∞∏‰πÖ',
        'Á¶øÊ∫úÁ™© ÂìàÊâπÊ∫ú Ê≠êÊôÆÊêú',
    ]   

for c in test_cases:
    HanLP(c, tasks=['tok/fine', 'pos/ctb'])

```

**Describe the current behavior**
ÈÄôÂπæÂÄãÂ≠óÊúÉË™§Âà§ÁÇ∫ Ê®ôÈªûÁ¨¶Ëôü (PUÔºâ: 

ÂóØ..ÔºàPUÔºâÔºöÂóØ..Âïä..ÊàëÂÄëÊ≤íÊúâÊî∂Âà∞Ê™¢ËàâÂïä
Âïä..ÔºàPUÔºâÔºöÂóØ..Âïä..ÊàëÂÄëÊ≤íÊúâÊî∂Âà∞Ê™¢ËàâÂïä
Ë°äÔºàPUÔºâÔºöÊôÆ‰∏ÅÂ∞±Â§ßÈáèÁöÑ‰æÜÊ±ôË°äÂ∏åÊãâËïä
ÈπºÔºàPUÔºâÔºö ÂéªÊ¥óÈÇ£ÂÄãÈπº
Ê∞öÔºàPUÔºâÔºö Á¶èÂ≥∂ÈÇ£ÂÄãÊòØÁàÜÁÇ∏ÁöÑÂïèÈ°å Ê†∏ÈõªÂª†ÈÇ£ÂÄãÊ∞öÊ¥ó‰∏çÊéâÁöÑ
Ê∞∏ÔºàPUÔºâÔºöÂÉèÂÆÉÁöÑÊ®°ÂºèÂèØ‰ª•ÊîπÊàê15ÂàÜÈêòÊàñËÄÖÊòØÊ∞∏..Ê∞∏‰πÖ
Ê∫úÔºàPUÔºâÔºöÁ¶øÊ∫úÁ™© ÂìàÊâπÊ∫ú Ê≠êÊôÆÊêú

**Expected behavior**
ÊáâË©≤Ëá≥Â∞ëË¶ÅÁü•ÈÅìÊòØ‰∏≠Êñá„ÄÇ‰∏çË´ñË©ûÊÄßÊ≠£Á¢∫ËàáÂê¶

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- Python version: Python 3.7.3 
- HanLP version:  2.1.0a38

**Other info / logs**
```
2021-05-14 09:42:45,673 - unittest - INFO - {
  ""tok/fine"": [
    ""ÂóØ.."",
    ""Âïä.."",
    ""ÊàëÂÄë"",
    ""Ê≤íÊúâ"",
    ""Êî∂Âà∞"",
    ""Ê™¢Ëàâ"",
    ""Âïä""
  ],
  ""pos/ctb"": [
    ""PU"",
    ""PU"",
    ""PN"",
    ""VV"",
    ""VV"",
    ""NN"",
    ""SP""
  ]
}

2021-05-14 09:42:45,694 - unittest - INFO - {
  ""tok/fine"": [
    ""ÊôÆ‰∏Å"",
    ""Â∞±"",
    ""Â§ßÈáè"",
    ""ÁöÑ"",
    ""‰æÜÊ±ô"",
    ""Ë°ä"",
    ""Â∏åÊãâËïä""
  ],
  ""pos/ctb"": [
    ""NR"",
    ""AD"",
    ""CD"",
    ""DEV"",
    ""VV"",
    ""PU"",
    ""NR""
  ]
}

2021-05-14 09:42:45,712 - unittest - INFO - {
  ""tok/fine"": [
    ""Âéª"",
    ""Ê¥ó"",
    ""ÈÇ£"",
    ""ÂÄã"",
    ""Èπº""
  ],
  ""pos/ctb"": [
    ""VV"",
    ""VV"",
    ""DT"",
    ""M"",
    ""PU""
  ]
}

2021-05-14 09:42:45,772 - unittest - INFO - {
  ""tok/fine"": [
    ""Ê†∏ÈõªÂª†"",
    ""ÈÇ£"",
    ""ÂÄã"",
    ""Ê∞ö"",
    ""Ê¥ó"",
    ""‰∏ç"",
    ""Êéâ"",
    ""ÁöÑ""
  ],
  ""pos/ctb"": [
    ""NN"",
    ""DT"",
    ""M"",
    ""PU"",
    ""VV"",
    ""AD"",
    ""VV"",
    ""SP""
  ]
}

2021-05-14 09:42:45,792 - unittest - INFO - {
  ""tok/fine"": [
    ""ÂÉè"",
    ""ÂÆÉ"",
    ""ÁöÑ"",
    ""Ê®°Âºè"",
    ""ÂèØ‰ª•"",
    ""ÊîπÊàê"",
    ""15"",
    ""ÂàÜÈêò"",
    ""ÊàñËÄÖ"",
    ""ÊòØ"",
    ""Ê∞∏.."",
    ""Ê∞∏‰πÖ""
  ],
  ""pos/ctb"": [
    ""P"",
    ""PN"",
    ""DEG"",
    ""NN"",
    ""VV"",
    ""VV"",
    ""CD"",
    ""M"",
    ""CC"",
    ""VC"",
    ""PU"",
    ""VA""
  ]
}


2021-05-14 09:42:45,830 - unittest - INFO - {
  ""tok/fine"": [
    ""ÂìàÊâπ"",
    ""Ê∫ú""
  ],
  ""pos/ctb"": [
    ""IJ"",
    ""PU""
  ]
}

2021-05-14 09:42:45,830 - unittest - INFO - {
  ""tok/fine"": [
    ""ÂìàÊâπ"",
    ""Ê∫ú""
  ],
  ""pos/ctb"": [
    ""IJ"",
    ""PU""
  ]
}
```


* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ValueError: invalid literal for int() with base 8: 'uild_ten',"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Run failed with simple code after hanlp pip installed.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
HanLP(['2021Âπ¥HanLPv2.1‰∏∫Áîü‰∫ßÁéØÂ¢ÉÂ∏¶Êù•Ê¨°‰∏ñ‰ª£ÊúÄÂÖàËøõÁöÑÂ§öËØ≠ÁßçNLPÊäÄÊúØ„ÄÇ', 'ÈòøÂ©Ü‰∏ªÊù•Âà∞Âåó‰∫¨Á´ãÊñπÂ∫≠ÂèÇËßÇËá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏„ÄÇ'])
```

**Describe the current behavior**
Run failed with exceptions.

**Expected behavior**
Run success with ouputs as readme.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Python version: python 3.6.9
- HanLP version: 2.1.0a38 (pip install)

**Other info / logs**
```
Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip
100% 114.3 MiB   2.7 MiB/s ETA:  0 s [=============================================================]
Decompressing /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to /home/zhangguanqun/.hanlp/mtl
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 627/627 [00:00<00:00, 409kB/s]
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110k/110k [00:00<00:00, 151kB/s]  
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 269k/269k [00:00<00:00, 277kB/s] 
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.00/2.00 [00:00<00:00, 1.53kB/s]
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:00<00:00, 81.4kB/s]
Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.0/19.0 [00:00<00:00, 15.1kB/s]
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 189, in nti
    n = int(s.strip() or ""0"", 8)
ValueError: invalid literal for int() with base 8: 'uild_ten'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 2299, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File ""/usr/lib/python3.6/tarfile.py"", line 1093, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File ""/usr/lib/python3.6/tarfile.py"", line 1035, in frombuf
    chksum = nti(buf[148:156])
  File ""/usr/lib/python3.6/tarfile.py"", line 191, in nti
    raise InvalidHeaderError(""invalid header"")
tarfile.InvalidHeaderError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 555, in _load
    return legacy_load(f)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 466, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File ""/usr/lib/python3.6/tarfile.py"", line 1591, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1621, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1484, in __init__
    self.firstmember = self.next()
  File ""/usr/lib/python3.6/tarfile.py"", line 2311, in next
    raise ReadError(str(e))
tarfile.ReadError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 81, in load_from_meta_file
    obj.load(save_dir, verbose=verbose, **kwargs)
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 182, in load
    self.load_weights(save_dir, **kwargs)
  File ""/home/zhangguanqun/venv_tfnightly_2.5_210325/lib/python3.6/site-packages/hanlp/common/torch_component.py"", line 100, in load_weights
    self.model_.load_state_dict(torch.load(filename, map_location='cpu'), strict=False)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 559, in _load
    raise RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name))
RuntimeError: /home/zhangguanqun/.hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/model.pt is a zip archive (did you mean to use torch.jit.load()?)
=================================ERROR LOG ENDS=================================
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```


* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
pyhanlpÂ§öËøõÁ®ãÈóÆÈ¢ò,"**Describe the bug**
pyhanlpÂ§öËøõÁ®ãÂºÇÂ∏∏. ‰∏çËÉΩÂÖÖÂàÜÂà©Áî®cpu,ËÄå‰∏îÊÑüËßâ  ‰ª£Á†ÅÂÅúÊ≠¢/""Âç°‰Ωè""

**Code to reproduce the issue**
```
!pip3 install pyhanlp
from multiprocessing import Pool
from tqdm import tqdm
from pyhanlp import HanLP
print(HanLP.segment('hello'))

def test_process(tmp: int):
    for i in range(10000):
        HanLP.segment(""ÂïÜÂìÅÂíåÊúçÂä°"")

pool_ = Pool(2)
result = pool_.map(test_process, tqdm(range(10)))
pool_.close()
pool_.join()
# print(result)
print('END')
```

**Describe the current behavior**
ÂêåÊ†∑ÁöÑÂ§öËøõÁ®ã‰ª£Á†Å,Â∞±ÂçïÁ∫ØÁöÑÂàÜËØç‰ª£Á†ÅÊîπÊàêÂÖ∂ÂÆÉÂàÜËØçÂ∑•ÂÖ∑ÊòØÊ≤°ÊúâÈóÆÈ¢òÁöÑ
```HanLP.segment``` -> ```jieba.cut```
‰ΩÜÊòØhanlpËøêË°åÁöÑÊó∂ÂÄôcpu‰ΩøÁî®ÁéáÂú®130%Â∑¶Âè≥(Êú∫Âô®ÊòØ2È¢ó  E5-2620 v4,ÊØèÈ¢óÊòØ8Ê†∏16Á∫øÁ®ã,ÂÜÖÂ≠òÂâ©‰Ωô30G)
Êàë‰∏çÁü•ÈÅìÊòØÁúüÁöÑÂç°‰ΩèËøòÊòØ,ÈÄüÂ∫¶ÊÖ¢.



**Expected behavior**
ÊàëÂ∏åÊúõËÉΩÂ§üÂ§öËøõÁ®ã,È´òÈÄüËøêË°åhanlpÂàÜËØç


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core) 
- Python version:3.6.5
- HanLP version:
hanlp                       2.1.0a36
hanlp-common                0.0.6
hanlp-downloader            0.0.20
hanlp-trie                  0.0.2
pyhanlp                     0.1.77

**Other info / logs**
ËøôÊòØ‰ΩøÁî®
https://play.hanlp.ml/run/hanlp-zh
ËøêË°åÁöÑÁªìÊûú

cpu‰ΩøÁî®Áéá‰æùÊóßÊòØ0,Á≠â‰∫ÜÂ•Ω‰πÖ,‰∏ÄÁõ¥Âú®ËΩ¨.
ÂõæÁâáÈìæÊé•
https://sm.ms/image/acSxlnBwpG49ehJ

‰ª£Á†ÅÊîπËá™**https://github.com/hankcs/HanLP/issues/1625**

Âú®ÁΩë‰∏äÊâæÂà∞ÁöÑÂèØËÉΩÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò
https://bbs.hankcs.com/t/topic/2128

* [x] I've completed this form and searched the web for solutions.
"
Pinyin.java ‰∏≠ ÈÉ®ÂàÜÈüµÊØçÈîôËØØ,"**Describe the bug**
Hello, hanlp developers, I found some bugs in the file Pinyin.java

in commit 6b60684f447d4c9f4ad68016fd1b443ef50e9bb4

lve3 's yunmu is ve Ôºå but lve4 's yunmu is ue .

Source Code: 
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L702
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L698
https://github.com/hankcs/HanLP/blob/1.x/src/main/java/com/hankcs/hanlp/dictionary/py/Pinyin.java#L841

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
from pyhanlp import *
from multiprocessing import Pool, cpu_count
Pinyin = JClass(""com.hankcs.hanlp.dictionary.py.Pinyin"")
# hanlp ÊµãËØï
text = ""È©¥Â≠êÔºåÁï•Â∏¶ÔºåÁñüÁñæ""       
pinyin_list = HanLP.convertToPinyinList(text)
for p in pinyin_list:
    print(p.__str__(), end="" "")
print()
for pinyin in pinyin_list:
    print(""%s,"" % pinyin.getYunmu(), end="" "")
```

**Describe the current behavior**
```text
lv2 zi5 none5 lve4 dai4 none5 nve4 ji2 
u, i, none, ue, ai, none, ue, i, 
```

**Expected behavior**
```text
lv2 zi5 none5 lve4 dai4 none5 nve4 ji2 
ve, i, none, ve, ai, none, ve, i, 
```

**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04
- Python version: 3.6.12
- HanLP version: 1.8.1

**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
"
golang support,hanlp restful golang support
Ë∞ÉÊï¥golang ËØ¥Êòé Áõ∏ÂØπ‰ΩçÁΩÆ,ÂÖ∂‰∏≠java‰∏≠Ë∑üÂø´ÈÄü‰∏äÊâãÊÑüËßâÊòØËÅîÂêàËµ∑Êù•ÁöÑ„ÄÇÊâÄ‰ª•Ë∞ÉÊï¥‰∫Ü‰∏Ä‰∏ãÁõ∏ÂØπ‰ΩçÁΩÆ
golang support,golang restful support
golang support,golang restful support
Ë∞ÉÁî®CustomDictionary.reload()ËøîÂõûfalseÔºåÊó†Ê≥ïÊõ¥Êñ∞ËØçÂÖ∏,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
from hanlp import CustomDictionary ÂêéÔºåË∞ÉÁî®CustomDictionary.reload()ËøîÂõûfalseÔºåÊó†Ê≥ïÊõ¥Êñ∞ËØçÂÖ∏ÔºåÈÄöËøáÊâìÂç∞ÈÉ®ÂàÜÊó•ÂøóÂêé‰ªçÁÑ∂Ê≤°ÊúâËß£ÂÜ≥ÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/70429720/112116759-92bc6280-8bf5-11eb-9518-2d554eccc8ae.png)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

>>> from pyhanlp import CustomDictionary, HanLP
>>> print(HanLP.Config.CustomDictionaryPath)
('/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/CustomDictionary.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txt ns', '/usr/local/python3/lib/python3.6/site-packages/pyhanlp/static/data/dictionary/person/nrf.txt nrf')
>>> print(CustomDictionary.DEFAULT.path)
()
>>> print(CustomDictionary.reload())
False


**Describe the current behavior**
A clear and concise description of what happened.
reloadÊ∞∏ËøúËøîÂõûfalseÔºåÊöÇÊó∂Êó†Ê≥ïÂÆö‰ΩçÂÖ∑‰ΩìÂéüÂõ†

**Expected behavior**
A clear and concise description of what you expected to happen.
ÊúüÊúõreloadÂæóÂà∞TrueÁöÑËøîÂõûÁªìÊûúÔºåÊàêÂäüÊõ¥Êñ∞

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- Python version: 3.6.8
- HanLP version: 0.1.63


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ËßÅ‰∏äËæπÁöÑÊà™Âõæ‰∏≠

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
convertToPinyinListÊñπÊ≥ïÔºåËøîÂõû‰∫ÜÊâÄÊúâÂêåÈü≥Â≠óÁöÑÊãºÈü≥„ÄÇ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
convertToPinyinListÊñπÊ≥ïÔºåËøîÂõû‰∫ÜÊâÄÊúâÂêåÈü≥Â≠óÁöÑÊãºÈü≥„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Java
System.out.println(HanLP.convertToPinyinString(""‰Ω†Â•Ω"", "" "", false));
// ni hao hao
```

**Describe the current behavior**
convertToPinyinListÊñπÊ≥ïËøîÂõû‰∫ÜÂ§öÈü≥Â≠óÊâÄÊúâÁöÑÊãºÈü≥ÔºåÂØºËá¥ËøîÂõûÁöÑÊãºÈü≥Â≠óÁ¨¶‰∏≤ÊúâÂ§ö‰ΩôÁöÑÂ§öÈü≥Â≠óÔºåÂêåÊó∂ÂΩìÈúÄË¶ÅËΩ¨ÊãºÈü≥Â≠óÁ¨¶‰∏≤Êúâ‰∏≠Ëã±Ê∑∑ÊùÇÊó∂Ôºå‰ºöÂá∫Áé∞Êä•ÈîôIndexOutOfBoundaryException„ÄÇ1.7.8Êó†Ê≠§ÈóÆÈ¢ò„ÄÇ

**Expected behavior**
‚Äú‰Ω†Â•Ω‚ÄùËøîÂõû‚Äúni hao‚ÄùËÄå‰∏çÊòØ‚Äúni hao hao‚Äù„ÄÇ

**System information**
- Linux„ÄÅWindows
- Python version:
- HanLP version:1.8.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
FileNotFoundError: The identifier cc.en.300.bin resolves to a non-exist meta file config.json,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
FileNotFoundError when try to load word2vec pre-trainned model

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
fst = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN)
```

**Describe the current behavior**
load FASTTEXT_CC_300_EN failed with FileNotFoundError

**Expected behavior**
load FASTTEXT_CC_300_EN model should success 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Python version: Python 3.8.8
- HanLP version: Version: 2.1.0a36

**Other info / logs**
```
(hanlp) ACTIONTRUSTNAME:~ pe.li$ python
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
>>> fst = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/pe.li/miniconda3/envs/hanlp/lib/python3.8/site-packages/hanlp/__init__.py"", line 43, in load
    return load_from_meta_file(save_dir, 'meta.json', verbose=verbose, **kwargs)
  File ""/Users/pe.li/miniconda3/envs/hanlp/lib/python3.8/site-packages/hanlp/utils/component_util.py"", line 53, in load_from_meta_file
    raise FileNotFoundError(f'The identifier {save_dir} resolves to a non-exist meta file {metapath}. {tips}')
FileNotFoundError: The identifier /Users/pe.li/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin resolves to a non-exist meta file /Users/pe.li/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin/config.json.
```

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
demoËÆ≠ÁªÉÊï∞ÊçÆ‰Ωé‰∏ã,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ËøêË°åÁªôÂá∫Êù•ÁöÑ ËÆ≠ÁªÉdemoÔºågpu Âà©Áî®ÁéáÂæà‰ΩéÔºå

**Code to reproduce the issue**
https://github.com/hankcs/HanLP/blob/master/plugins/hanlp_demo/hanlp_demo/zh/demo_custom_dict.py

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
![cuda](https://user-images.githubusercontent.com/22882285/109949146-00235480-7d16-11eb-89bd-32da15eff0f9.jpg)
![hanlp](https://user-images.githubusercontent.com/22882285/109949170-05809f00-7d16-11eb-8709-6b1aed23e42f.jpg)
![hanlp1](https://user-images.githubusercontent.com/22882285/109949193-0a455300-7d16-11eb-9f18-6653c562014c.jpg)


**System information**
- OS Platform and Distribution (e.g., win 10):
- Python version: 3.6
- HanLP version: 2.1
- torch version:1.7.0
- torchvision: 0.8.1

**Other info / logs**
ËØ∑Á°Æ‰∫∫ÊòØ‰∏çÊòØ‰Ω†‰ª¨ÈÇ£ËæπÊ≤°ÊúâÂØπwin‰Ωú‰ºòÂåñÔºåÂØºËá¥ Âú®win 10‰∏ãÊõ¥‰∏ç‰∏ä linux ÁöÑËÆ≠ÁªÉÈÄüÂ∫¶

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
hjj ,"
![hanlp1](https://user-images.githubusercontent.com/22882285/109948619-796e7780-7d15-11eb-8e43-6dac7856ab63.jpg)
<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
demo ËÆ≠ÁªÉÁºìÊÖ¢Ôºågpu‰ΩøÁî®ÁéáÂá†‰πé‰∏∫Èõ∂

**Code to reproduce the issue**
import hanlp
from hanlp.components.mtl.multi_task_learning import MultiTaskLearning
from hanlp.components.mtl.tasks.tok.tag_tok import TaggingTokenization
from tests import cdroot

cdroot()
HanLP: MultiTaskLearning = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
tok: TaggingTokenization = HanLP['tok/fine']

# tok.dict_force = tok.dict_combine = None
# print(f'‰∏çÊåÇËØçÂÖ∏:\n{HanLP(""ÂïÜÂìÅÂíåÊúçÂä°Ë°å‰∏ö"")[""tok/fine""]}')
#
# tok.dict_force = {'ÂíåÊúç', 'ÊúçÂä°Ë°å‰∏ö'}
# print(f'Âº∫Âà∂Ê®°Âºè:\n{HanLP(""ÂïÜÂìÅÂíåÊúçÂä°Ë°å‰∏ö"")[""tok/fine""]}')  # ÊÖéÁî®ÔºåËØ¶ËßÅ„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÁ¨¨‰∫åÁ´†
#
# tok.dict_force = {'ÂíåÊúçÂä°': ['Âíå', 'ÊúçÂä°']}
# print(f'Âº∫Âà∂Ê†°Ê≠£:\n{HanLP(""Ê≠£ÂêëÂåπÈÖçÂïÜÂìÅÂíåÊúçÂä°„ÄÅ‰ªª‰ΩïÂíåÊúçÂä°ÂøÖÊåâ‰∏äËø∞ÂàáÂàÜ"")[""tok/fine""]}')

tok.dict_force = None
tok.dict_combine = {'ÂíåÊúç', 'ÊúçÂä°Ë°å‰∏ö'}
print(f'ÂêàÂπ∂Ê®°Âºè:\n{HanLP(""ÂïÜÂìÅÂíåÊúçÂä°Ë°å‰∏ö"")[""tok/fine""]}')

# ÈúÄË¶ÅÁÆóÊ≥ïÂü∫Á°ÄÊâçËÉΩÁêÜËß£ÔºåÂàùÂ≠¶ËÄÖÂèØÂèÇËÄÉ http://nlp.hankcs.com/book.php
# See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
![cuda](https://user-images.githubusercontent.com/22882285/109948589-72476980-7d15-11eb-93b5-73c71bc857d7.jpg)
![hanlp](https://user-images.githubusercontent.com/22882285/109948605-770c1d80-7d15-11eb-8cee-a51abec776e9.jpg)
![hanlp1](https://user-images.githubusercontent.com/22882285/109948680-8723fd00-7d15-11eb-9010-17ac0c7988ce.jpg)


**System information**
- OS Platform and Distribution (e.g., win 10):
- Python version:3.6
- HanLP version:2.1

**Other info / logs**
 Âú®Âä†ËΩΩÂÆåÊàêÊ®°Âûã‰πãÂêéÔºåÊ≤°ÊúâÁúãÂà∞gpuË¥üËΩΩÊåáÊ†áÔºåËøô‰∏™ÊòØ‰∏çÊ≠£Â∏∏ÁöÑ

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
pyhanlpÂ§öËøõÁ®ãÈóÆÈ¢ò,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
‰ΩøÁî®pyhanlpÂú®Â§öÁ∫øÁ®ã‰∏ãÊµãËØïÊó†ÈóÆÈ¢òÔºåÂú®Â§öËøõÁ®ã‰∏ã‰ºöÂá∫Áé∞ÂºÇÂ∏∏„ÄÇ


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```from multiprocessing import Pool
from tqdm import tqdm
from bert4keras.snippets import parallel_apply
import argparse
import pandas as pd
from pyhanlp import HanLP, SafeJClass
from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM

def test_process(tmp: int):
    print(str(tmp) + '\n')
    print(HanLP.segment(""ÂïÜÂìÅÂíåÊúçÂä°""))
    return str(tmp)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('-mode', '--mode', type=str, default='pool', help='')
    # Ëé∑ÂèñÁõ∏ÂÖ≥ÂèÇÊï∞
    args = parser.parse_args()
    mode = args.mode
    if mode == 'apply':
        result = parallel_apply(func=test_process, iterable=tqdm(range(20)),
                                workers=20, max_queue_size=1500, callback=None, )
        print(result)
        print('END')
    elif mode == 'pool':
        pool_ = Pool(20)
        result = pool_.map(test_process, tqdm(range(10)))
        pool_.close()
        pool_.join()
        print(result)
        print('END')
```



**Describe the current behavior**
A clear and concise description of what happened.

HanLP segment ->: 
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f994b134449, pid=79026, tid=0x00007f9960d80700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29
#
# Core dump written. Default location: /data1/jiangxl/bert4keras-master/pretraining/core or core.79026
#
# Can not save log file, dump to screen..
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f994b134449, pid=79026, tid=0x00007f9960d80700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29
#
# Core dump written. Default location: /data1/jiangxl/bert4keras-master/pretraining/core or core.79026
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread is native thread

siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000000

Registers:
RAX=0x0000000001f26eb0, RBX=0x0000000000000000, RCX=0x0000000000000000, RDX=0x00000000008ddd00
RSP=0x00007f9960d7dc60, RBP=0x0000000001f26eb0, RSI=0x0000000000000000, RDI=0x00000000008ddd00
R8 =0x0000000000000000, R9 =0x000000000000000c, R10=0x0000000000000002, R11=0x0000000000a16e00
R12=0x00007f981c001910, R13=0x0000000000000005, R14=0x00007f9960d7dde0, R15=0x00007f9960d7dd60
RIP=0x00007f994b134449, EFLAGS=0x0000000000010202, CSGSFS=0x0000000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f9960d7dc60)
0x00007f9960d7dc60:   00007f9960d7dca0 00007f981c001910
0x00007f9960d7dc70:   0000000000000005 00007f9960d7dca0
0x00007f9960d7dc80:   0000000001de9df8 00007f994b142f2f
0x00007f9960d7dc90:   00007f9960d7dcbf 00007f990c0d80f8
0x00007f9960d7dca0:   00007f994b3805d0 00007f981c0018f0
0x00007f9960d7dcb0:   0000000000000005 0000000000512e89
0x00007f9960d7dcc0:   0000000000000000 0000000000000001
0x00007f9960d7dcd0:   0000000000000001 00007f994b13f6d8
0x00007f9960d7dce0:   0000000000000000 0000000000000001
0x00007f9960d7dcf0:   0000000000000008 0000000000000000
0x00007f9960d7dd00:   00007f9960d7dde0 0000000000000001
0x00007f9960d7dd10:   0000000001de9df8 00007f9960d7dd5f
0x00007f9960d7dd20:   00007f9960d7dde0 0000000000000001
0x00007f9960d7dd30:   00007f990c0cce10 00007f994b140b03
0x00007f9960d7dd40:   00007f9960d7de00 00007f9960d7dde0
0x00007f9960d7dd50:   0000000000000000 00007f9960d7dde0
0x00007f9960d7dd60:   00007f981c001670 00007f981c0018d0
0x00007f9960d7dd70:   00007f9960d7de00 00007f99bdff9ad8
0x00007f9960d7dd80:   0000000000000001 00007f981c0018d0
0x00007f9960d7dd90:   00007f9960d7de00 00007f994b1674a2
0x00007f9960d7dda0:   00007f9960d7ddd0 00007f9960d7dde0
0x00007f9960d7ddb0:   00007f9960d7ddcf 00007f994b0e8f00
0x00007f9960d7ddc0:   00007f994b0e3188 0000000000474a23
0x00007f9960d7ddd0:   00007f981c0018d0 00007f99c53dd630
0x00007f9960d7dde0:   00007f981c0015b0 00007f981c0015b8
0x00007f9960d7ddf0:   00007f981c0015b8 00007f9960d7de40
0x00007f9960d7de00:   00007f994b380590 00007f981c001600
0x00007f9960d7de10:   00007f981c001608 00007f981c001608
0x00007f9960d7de20:   0000000000000004 00007f994b0e8f00
0x00007f9960d7de30:   00007f994b167380 0000000001f26eb0
0x00007f9960d7de40:   00007f994b0e8f00 0000000000000000
0x00007f9960d7de50:   00007f990c0cce10 0000000000437e60 

Instructions: (pc=0x00007f994b134449)
0x00007f994b134429:   f4 55 53 48 83 ec 08 e8 4b d3 fe ff 48 89 c3 e8
0x00007f994b134439:   93 c9 fe ff 48 8b 10 48 89 c7 ff 52 30 48 89 c5
0x00007f994b134449:   48 8b 03 44 89 ea 48 89 df 4c 89 e6 ff 90 18 05
0x00007f994b134459:   00 00 48 89 c3 e8 6d c9 fe ff 48 89 c7 48 8b 00 

Register to memory mapping:

RAX=0x0000000001f26eb0 is an unknown value
RBX=0x0000000000000000 is an unknown value
RCX=0x0000000000000000 is an unknown value
RDX=0x00000000008ddd00: <offset 0x4ddd00> in python3 at 0x0000000000400000
RSP=0x00007f9960d7dc60 is an unknown value
RBP=0x0000000001f26eb0 is an unknown value
RSI=0x0000000000000000 is an unknown value
RDI=0x00000000008ddd00: <offset 0x4ddd00> in python3 at 0x0000000000400000
R8 =0x0000000000000000 is an unknown value
R9 =0x000000000000000c is an unknown value
R10=0x0000000000000002 is an unknown value
R11=0x0000000000a16e00 is an unknown value
R12=0x00007f981c001910 is an unknown value
R13=0x0000000000000005 is an unknown value
R14=0x00007f9960d7dde0 is an unknown value
R15=0x00007f9960d7dd60 is an unknown value


Stack: [0x00007f9960581000,0x00007f9960d81000],  sp=0x00007f9960d7dc60,  free space=8179k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [_jpype.cpython-35m-x86_64-linux-gnu.so+0x35449]  JPJavaEnv::NewString(unsigned short const*, int)+0x29


---------------  P R O C E S S  ---------------

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap:
 PSYoungGen      total 305664K, used 124003K [0x00000000d5580000, 0x00000000eaa80000, 0x0000000100000000)
  eden space 262144K, 47% used [0x00000000d5580000,0x00000000dce98e28,0x00000000e5580000)
  from space 43520K, 0% used [0x00000000e8000000,0x00000000e8000000,0x00000000eaa80000)
  to   space 43520K, 0% used [0x00000000e5580000,0x00000000e5580000,0x00000000e8000000)
 ParOldGen       total 699392K, used 0K [0x0000000080000000, 0x00000000aab00000, 0x00000000d5580000)
  object space 699392K, 0% used [0x0000000080000000,0x0000000080000000,0x00000000aab00000)
 Metaspace       used 4217K, capacity 5120K, committed 5376K, reserved 1056768K
  class space    used 466K, capacity 496K, committed 512K, reserved 1048576K

Card table byte_map: [0x00007f9939e9c000,0x00007f993a29d000] byte_map_base: 0x00007f9939a9c000

Marking Bits: (ParMarkBitMap*) 0x00007f994b091c80
 Begin Bits: [0x00007f9914000000, 0x00007f9916000000)
 End Bits:   [0x00007f9916000000, 0x00007f9918000000)

Polling page: 0x00007f99c5444000

CodeCache: size=245760Kb used=4071Kb max_used=4086Kb free=241688Kb
 bounds [0x00007f993a65d000, 0x00007f993aa6d000, 0x00007f994965d000]
 total_blobs=565 nmethods=309 adapters=166
 compilation: enabled

Compilation events (10 events):
Event: 0.787 Thread 0x0000000001d60800  305       4       com.hankcs.hanlp.model.perceptron.model.LinearModel::load (165 bytes)
Event: 0.790 Thread 0x0000000001d69000  306       1       java.nio.Buffer::position (5 bytes)
Event: 0.790 Thread 0x0000000001d69000 nmethod 306 0x00007f993aa368d0 code [0x00007f993aa36a20, 0x00007f993aa36b30]
Event: 0.820 Thread 0x0000000001d6b800  307       1       java.nio.channels.spi.AbstractInterruptibleChannel::isOpen (5 bytes)
Event: 0.820 Thread 0x0000000001d6b800 nmethod 307 0x00007f993aa25590 code [0x00007f993aa256e0, 0x00007f993aa257f0]
Event: 0.825 Thread 0x0000000001d60800 nmethod 305 0x00007f993aa5b190 code [0x00007f993aa5b4c0, 0x00007f993aa5cba8]
Event: 0.872 Thread 0x0000000001d67000  308       3       java.lang.ThreadLocal::getMap (5 bytes)
Event: 0.872 Thread 0x0000000001d67000 nmethod 308 0x00007f993aa3e410 code [0x00007f993aa3e580, 0x00007f993aa3e6f0]
Event: 0.883 Thread 0x0000000001d6d800  309       3       java.lang.ThreadLocal::get (38 bytes)
Event: 0.883 Thread 0x0000000001d6d800 nmethod 309 0x00007f993aa5aa50 code [0x00007f993aa5abe0, 0x00007f993aa5af98]

GC Heap History (0 events):
No events

Deoptimization events (10 events):
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.542 Thread 0x0000000001780000 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f993aa18bf8 method=java.util.AbstractCollection.toArray([Ljava/lang/Object;)[Ljava/lang/Object; @ 119
Event: 0.641 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa0d7a0 method=java.util.TreeMap.getEntry(Ljava/lang/Object;)Ljava/util/TreeMap$Entry; @ 36
Event: 0.653 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa2cc90 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.663 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa37664 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.692 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa40950 method=com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(Lcom/hankcs/hanlp/corpus/io/ByteArray;)Z @ 31
Event: 0.702 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa20240 method=com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(I)V @ 10
Event: 0.745 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa3f010 method=com.hankcs.hanlp.model.perceptron.model.LinearModel.load(Lcom/hankcs/hanlp/corpus/io/ByteArray;)Z @ 106
Event: 0.750 Thread 0x0000000001780000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f993aa14e28 method=java.util.ArrayList.grow(I)V @ 15

Internal exceptions (7 events):
Event: 0.061 Thread 0x0000000001780000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.defineClass(Ljava/lang/String;[BII)Ljava/lang/Class; name or signature does not match> (0x00000000d5587ca8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/sh
Event: 0.061 Thread 0x0000000001780000 Exception <a 'java/lang/NoSuchMethodError': Method sun.misc.Unsafe.prefetchRead(Ljava/lang/Object;J)V name or signature does not match> (0x00000000d5587f90) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jni.c
Event: 0.209 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56adce0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.209 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56adef0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.210 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56b01c8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.210 Thread 0x0000000001780000 Exception <a 'java/security/PrivilegedActionException'> (0x00000000d56b03d8) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/prims/jvm.cpp, line 1390]
Event: 0.255 Thread 0x0000000001780000 Exception <a 'java/lang/NoClassDefFoundError': Ljava/lang;> (0x00000000d576d2c0) thrown at [/HUDSON3/workspace/8-2-build-linux-amd64/jdk8u121/8372/hotspot/src/share/vm/classfile/systemDictionary.cpp, line 199]




**Expected behavior**
A clear and concise description of what you expected to happen.
Ê≠£Â∏∏ÂàÜËØçËæìÂá∫

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

python3.5
pyhanlp 0.1.44
JPype1 0.6.3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
"IndexError: The shape of the mask [86, 39, 39] at index 1 does not match the shape of the indexed tensor [86, 41, 41, 19] at index 1","<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Error when parsing 33082 english sentences.

**Code to reproduce the issue**
data here:  
[qtitles.zip](https://github.com/hankcs/HanLP/files/6056254/qtitles.zip)



```python
import hanlp
han = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MT5_SMALL) # 'mt5 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD and OntoNotes5 corpus.'

with open('/home1/qtitles','r')as ifile:
    corpus = ifile.read().split('\n')

ps=han(corpus)
```

**Describe the current behavior**
Error & Exit with the following output:

```
Traceback (most recent call last):
  File ""parse2.py"", line 14, in <module>
    ps=han(corpus)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/torch_component.py"", line 629, in __call__
    return super().__call__(data, **merge_dict(self.config, overwrite=True,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 512, in predict
    output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 590, in predict_task
    output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 682, in feed_batch
    'output': task.feed_batch(h,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 182, in feed_batch
    return decoder(h, batch=batch, mask=mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/ner/biaffine_ner/biaffine_ner_model.py"", line 82, in forward
    return self.decode(contextualized_embeddings, gold_starts, gold_ends, gold_labels, mask,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/ner/biaffine_ner/biaffine_ner_model.py"", line 116, in decode
    candidate_ner_scores = candidate_ner_scores[candidate_scores_mask]
IndexError: The shape of the mask [86, 39, 39] at index 1 does not match the shape of the indexed tensor [86, 41, 41, 19] at index 1

```

**Expected behavior**
no output

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.6.1810 (Core) 
- Python version: 3.8.3
- HanLP version: '2.1.0-alpha.25'


**Other info / logs**

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Get the index of a token or a ner for example in the input text,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
I've been looking into hanlp source code and documentation to find a way to get the index of a token or a ner in the original input text. I was not able to find a solution to this problem (i.e I only get the index of a word in the tokens list).  
Here is an example:  
```
import hanlp
model = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE, devices=0)
ner = model(""My name is John Smith. I am 19 and a student in college."")
print(ner)
```

The output is:  
`{'tok': ['My', 'name', 'is', 'John', 'Smith', '.', 'I', 'am', '19', 'and', 'a', 'student', 'in', 'college', '.'], 'ner': [('John Smith', 'PERSON', 3, 5), ('19', 'DATE', 8, 9)], 'srl': [[('My name', 'ARG1', 0, 2), ('is', 'PRED', 2, 3), ('John Smith', 'ARG2', 3, 5)], [('I', 'ARG1', 6, 7), ('am', 'PRED', 7, 8), ('19 and a student in college', 'ARG2', 8, 14)]], 'sdp/dm': [[], [(1, 'poss'), (3, 'ARG1')], [(1, 'orphan')], [(1, 'orphan')], [(3, 'ARG2'), (4, 'compound')], [(1, 'orphan')], [(8, 'ARG1')], [], [(8, 'ARG2')], [(1, 'orphan')], [(1, 'orphan')], [(9, '_and_c'), (11, 'BV'), (13, 'ARG1')], [(1, 'orphan')], [(13, 'ARG2')], [(1, 'orphan')]], 'sdp/pas': [[], [(1, 'det_ARG1'), (3, 'verb_ARG1')], [(1, 'orphan')], [(1, 'orphan')], [(3, 'verb_ARG2'), (4, 'noun_ARG1')], [(1, 'orphan')], [(8, 'verb_ARG1')], [(6, 'conj_ARG2')], [(10, 'coord_ARG1')], [(8, 'verb_ARG2')], [(1, 'orphan')], [(10, 'coord_ARG2'), (11, 'det_ARG1'), (13, 'prep_ARG1')], [(1, 'orphan')], [(13, 'prep_ARG2')], [(1, 'orphan')]], 'sdp/psd': [[(2, 'APP')], [(3, 'ACT-arg')], [(6, 'CONJ.member')], [(5, 'NE')], [(3, 'PAT-arg')], [], [(8, 'ACT-arg')], [(6, 'CONJ.member'), (10, 'CONJ.member')], [(8, 'PAT-arg')], [(6, 'CONJ.member')], [(6, 'orphan')], [(8, 'PAT-arg'), (10, 'CONJ.member')], [(6, 'orphan')], [(12, 'LOC')], [(6, 'orphan')]], 'con': ['TOP', [['S', [['S', [['NP', [['PRON', ['My']], ['NOUN', ['name']]]], ['VP', [['AUX', ['is']], ['NP', [['PROPN', ['John']], ['PROPN', ['Smith']]]]]]]], ['PUNCT', ['.']], ['S', [['NP', [['PRON', ['I']]]], ['VP', [['AUX', ['am']], ['NP', [['NP', [['NUM', ['19']]]], ['CCONJ', ['and']], ['NP', [['NP', [['DET', ['a']], ['NOUN', ['student']]]], ['PP', [['ADP', ['in']], ['NP', [['NOUN', ['college']]]]]]]]]]]]]], ['PUNCT', ['.']]]]]], 'lem': ['my', 'name', 'be', 'John', 'Smith', '.', 'I', 'be', '19', 'and', 'a', 'student', 'in', 'college', '.'], 'pos': ['PRON', 'NOUN', 'AUX', 'PROPN', 'PROPN', 'PUNCT', 'PRON', 'AUX', 'NUM', 'CCONJ', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT'], 'fea': ['Number=Sing|Person=1|Poss=Yes|PronType=Prs', 'Number=Sing', 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin', 'Number=Sing', 'Number=Sing', '_', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', 'Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin', 'NumType=Card', '_', 'Definite=Ind|PronType=Art', 'Number=Sing', '_', 'Number=Sing', '_'], 'dep': [(2, 'nmod:poss'), (4, 'nsubj'), (5, 'cop'), (0, 'root'), (4, 'flat'), (4, 'punct'), (9, 'nsubj'), (9, 'cop'), (4, 'parataxis'), (12, 'cc'), (12, 'det'), (9, 'conj'), (14, 'case'), (12, 'nmod'), (9, 'punct')]}`

For example instead/in addition of getting `('John Smith', 'PERSON', 3, 5)` is it possible to get `('John Smith', 'PERSON', 11, 21)` where **11** and **21** are the start and end indexes of 'John Smith' in the original input text.
**Will this change the current api? How?**
It is not necessary to change the current API, it is possible to add it as an option.
**Who will benefit with this feature?**
Everyone who uses hanlp
**Are you willing to contribute it (Yes/No):**
No.
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Python version: 3.6
- HanLP version: `hanlp==2.1.0a12`

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
RecursionError: maximum recursion depth exceeded in comparison,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
‰ΩøÁî®readme.mdÁ§∫‰æãÁöÑ‰ª£Á†ÅÂéªËß£Êûê20‰∏áË°åËá™ÁÑ∂ËØ≠Ë®ÄÂè•Â≠êÊä•Ê≠§ÈîôËØØ„ÄÇÊØè‰∏™Âè•Â≠êÁöÑlengthÈôêÂà∂Âú®40‰ª•ÂÜÖ„ÄÇ


**Code to reproduce the issue**
Êï∞ÊçÆËßÅ 
[sg_phone40.zip](https://github.com/hankcs/HanLP/files/6042006/sg_phone40.zip)


```python
import hanlp
from tqdm import tqdm
han = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
#p=han(['2021Âπ¥HanLPv2.1‰∏∫Áîü‰∫ßÁéØÂ¢ÉÂ∏¶Êù•Ê¨°‰∏ñ‰ª£ÊúÄÂÖàËøõÁöÑÂ§öËØ≠ÁßçNLPÊäÄÊúØ„ÄÇ', 'ÈòøÂ©Ü‰∏ªÊù•Âà∞Âåó‰∫¨Á´ãÊñπÂ∫≠ÂèÇËßÇËá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏„ÄÇ'])
#print(type(p))#<class 'hanlp_common.document.Document'>'
#print(p)


with open('sg_phone40','r')as ifile:
    corpus = ifile.read().split('\n')     #Áõ¥Êé•ËæìÂÖ•corpusÔºå‰∏çÁî®‰∏ãÈù¢ÁöÑkÂÅöbatching‰πü‰ºöÊä•ÂêåÊ†∑ÁöÑÈîôÔºå‰ΩÜÊòØË¶ÅË∑ëÂà∞Â§ßÁ∫¶7‰∏áÂ§öË°åÁöÑÊó∂ÂÄôÊâçÂá∫ÈîôÔºåË¶ÅÁ≠âÂæà‰πÖÔºå‰∏çÊñπ‰æødebug

err=73072
corpus=corpus[err-2000 : err+2000]

n=len(corpus)
k=1000
ps=[]

batch=[]
for s in tqdm(corpus):
    if len(batch)<k:
        batch.append(s)
    else:
        ptmp=han(batch)
        ps.append(ptmp)
        batch=[]

if len(batch)>0:
    ptmp=han(batch)
    ps.append(ptmp)

import pickle

pickle.dump( ps, open( ""phone_hanlp_pipeline20210225_2.pkl"", ""wb"" ) )

```


**Describe the current behavior**
Êä•ÈîôÈÄÄÂá∫
```
(base) [cc@localhost sg_phone]$ python parse2.py 
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 2001/4000 [00:17<00:17, 111.90it/s]
Traceback (most recent call last):
  File ""parse2.py"", line 24, in <module>
    ptmp=han(batch)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 768, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/torch_component.py"", line 629, in __call__
    return super().__call__(data, **merge_dict(self.config, overwrite=True,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 512, in predict
    output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 592, in predict_task
    self.decode_output(output_dict, batch, output_key)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 728, in decode_output
    output_per_task['prediction'] = self.tasks[task_name].decode_output(
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/mtl/tasks/constituency.py"", line 135, in decode_output
    return CRFConstituencyParser.decode_output(self, out, mask, batch, output.get('span_probs', None),
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/constituency/crf_constituency_parser.py"", line 127, in decode_output
    chart_preds = decoder.decode(s_span, s_label, mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/constituency/crf_constituency_model.py"", line 189, in decode
    span_preds = cky(s_span, mask)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 313, in cky
    trees = [backtrack(p[i], 0, length) for i, length in enumerate(lens.tolist())]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 313, in <listcomp>
    trees = [backtrack(p[i], 0, length) for i, length in enumerate(lens.tolist())]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 308, in backtrack
    ltree = backtrack(p, i, split)
  [Previous line repeated 982 more times]
  File ""/home1/data/cc/opt/anaconda3/lib/python3.8/site-packages/hanlp/components/parsers/alg.py"", line 305, in backtrack
    if j == i + 1:
RecursionError: maximum recursion depth exceeded in comparison
```

**Expected behavior**
Ê≠£Â∏∏Ë∑ëÂÆå


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Python version: 3.8.3
- HanLP version: 2.1.0-alpha.23

**Other info / logs**
Â§çÁé∞ÈîôËØØÁöÑ‰ª£Á†Å‰∏çËÆæÁΩÆ`k`ÂÅöbatchingÔºåÁõ¥Êé•Ë∑ë‰πü‰ºöÂá∫ÈîôÁöÑ„ÄÇ

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
‰æùÂ≠òÊ†ëÁªìÊûÑÈîôËØØÔºåÊüê‰∫õhead indexÊ†πÊú¨‰∏çÂ≠òÂú®ÔºàÂêåissue#1499Ôºâ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Â§çÁé∞bug‰ª£Á†ÅÔºåÊµãËØïÊï∞ÊçÆËßÅhttps://github.com/hankcs/HanLP/issues/1499#
ÂΩìÊó∂ÊòØËß£ÂÜ≥‰∫ÜÔºåÊàëÂÆâË£Ö‰∫ÜÊúÄÊñ∞ÁöÑ''2.1.0-alpha.23'ÁâàÊú¨ÂèàÂá∫Áé∞ÂêåÊ†∑ÁöÑÈîôËØØÔºöÊüê‰∫õhead indexÂ§ß‰∫émax possible token index

**Code to reproduce the issue**
Â§çÁé∞bug‰ª£Á†ÅÔºåÊµãËØïÊï∞ÊçÆÂêåhttps://github.com/hankcs/HanLP/issues/1499#

```python
import hanlp

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL16_NEWS_BIAFFINE_ZH)
print(semantic_parser([('Ëú°ÁÉõ', 'NN'), ('‰∏§', 'CD'), ('Â§¥', 'NN'), ('ÁÉß', 'VV')]))

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='part_of_speech_tags') \
    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies') \
    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')


with open('hanlp_phones','r')as ifile:
    corpus = ifile.read()

print(corpus[:200])

print('begin pipeline...')
p1=pipeline(corpus)
type(p1)  # hanlp.common.document.Document

print('pipeline done...')

import pickle

pickle.dump( p1, open( ""hanlp_pipeline0702.pkl"", ""wb"" ) )
```

Ê£ÄÊü•head idËøáÂ§ßÁöÑÈîôËØØÔºö
```python
print('checking pickle')
import pickle

p2 = pickle.load(open(""hanlp_pipeline0702.pkl"", ""rb""))


def check2(s,n):
    maxv=0
    for w in s:
        vid=w['id']
        maxv=max(maxv,vid)
    for w in s:
        head_id=w['head']
        if head_id>maxv:
            print(f'{n}:{p2[""sentences""][n]}\n')
            print(str(s)+'\n\n')
            break

n=0
for sentence in p2[""syntactic_dependencies""]:
    check2(sentence, n)
    n+=1
```

ÂæóÂà∞ÈîôËØØËæìÂá∫Ôºö
```
20:nekenÂ∞ºÂáØÊÅ©ÊâãÊú∫en8cÊÄéÊ†∑ÂºÄÊú∫

1       nekenÂ∞ºÂáØÊÅ©     _       NR      _       _       3       nn      _       _
2       ÊâãÊú∫    _       NN      _       _       3       nn      _       _
3       en8c    _       NN      _       _       6       assmod  _       _
4       ÊÄéÊ†∑    _       AD      _       _       3       assm    _       _
5       ÂºÄÊú∫    _       VV      _       _       6       nn      _       _


21:neken/Â∞ºÂáØÊÅ© en3‰∏âÈò≤ÊâãÊú∫ ÊÄé‰πàÊ†∑

1       neken/Â∞ºÂáØÊÅ©    _       NR      _       _       2       nn      _       _
2               _       CD      _       _       8       nsubj   _       _
3       en3     _       NR      _       _       8       advmod  _       _
4       ‰∏âÈò≤    _       JJ      _       _       8       ba      _       _
5       ÊâãÊú∫    _       NN      _       _       7       assmod  _       _
6               _       CD      _       _       5       assm    _       _
7       ÊÄé‰πàÊ†∑  _       AD      _       _       8       nsubj   _       _


24:gigasetÊâãÊú∫Ê≠ªÊú∫ÂºÄ‰∏ç‰∫ÜÊú∫?

1       gigaset _       NR      _       _       4       nn      _       _
2       ÊâãÊú∫    _       NN      _       _       4       nn      _       _
3       Ê≠ªÊú∫    _       NN      _       _       4       nn      _       _
4       ÂºÄ      _       VV      _       _       5       top     _       _
5       ‰∏ç      _       AD      _       _       0       root    _       _
6       ‰∫Ü      _       VV      _       _       5       dobj    _       _
7       Êú∫      _       VV      _       _       9       cop     _       _
8       ?       _       PU      _       _       9       advmod  _       _
```

**Describe the current behavior**
When doing dependency parsing,  some head id > max possible head id. 
**Expected behavior**
When doing dependency parsing,  all head id <= max possible head id. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)  
- Python version: 3.8.3 
- HanLP version: 2.1.0-alpha.23

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
HanLP2.1ÂàÜËØçÊä•Êñ∞ÁöÑÈîôÔºöFailed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

ÊÇ®Â•ΩÔºåÊÑüË∞¢‰ΩúËÄÖÂèäÊó∂ÁöÑÂõûÂ§ç‰∏ä‰∏Ä‰∏™bugÔºÅ

‰ΩÜÊàëÂàöÂàöÈáçÊñ∞pip install hanlp -U ‰πãÂêéÔºåÂπ∂ÂêåÊó∂Â∞ùËØïÊú¨Âú∞git pullÔºåÂÜçËøêË°åÂêåÊ†∑ÁöÑ‰ª£Á†ÅÔºå‰ºöÊä•Â¶Ç‰∏ãÊñ∞ÁöÑÈîôËØØÔºåËØ¥ÊòØÊó†Ê≥ïloadÊ®°ÂûãÔºå‰ΩÜÊàë‰πãÂâçÂ∑≤Áªè‰∏ãËΩΩËøáËøô‰∏™Ê®°ÂûãÂà∞ÊàëÊú¨Âú∞‰∫ÜÔºö

(base) river@riverdeMacBook-Pro mtl % ll
total 0
drwxr-xr-x 3 river staff 96 2 23 01:21 ./
drwxr-xr-x 4 river staff 128 2 22 21:36 ../
drwxr-xr-x 13 river staff 416 2 22 22:10 close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


ÂÆåÊï¥‰ª£Á†ÅÂ¶Ç‰∏ã


```python

import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
tok = HanLP[‚Äòtok/coarse‚Äô]

# ËØªÂèñËá™ÂÆö‰πâËØçË°®
beauty_vocab = set()
with open(‚Äô‚Ä¶/data/userdict.txt‚Äô, ‚Äòr‚Äô, encoding=‚Äòutf-8‚Äô) as fin:
    for line in fin:
        beauty_vocab.add(line.strip())
tok.dict_combine = beauty_vocab

text = ‚ÄòÊàëÊúÄÂøÉÊ∞¥ÁöÑÈÇ£‰∫õ Á∫¢ÔºöÂØπÂè£Á∫¢ËøôÁßç‰∏úË•øÁúüÁöÑÊòØÊØ´Êó†ÊäµÊäóÂäõÂïä ÊØèÊ¨°ÂÖ•‰∫ÜÊñ∞Ëâ≤Â∞±Âú®ËìùÊúãÂèãÊâãËáÇ‰∏äËØï ‰ªñÊÄª‰ºöËØ¥ Â•≥ÁîüÂà∞Â∫ïÈúÄË¶ÅÊúâÂ§öÂ∞ëÂè£Á∫¢Âïä Ë∫´‰∏∫‰∏Ä‰∏™Áõ¥Áî∑ÂèØËÉΩÁúüÁöÑ‰∏çÊáÇ Á∫¢Êúâ‰∏ÄÁôæÁßç ÂìàÂìàÂìàÂìà Ë®ÄÂΩíÊ≠£‰º† ÈòøÁéõÂ∞ºÁ∫¢ÁÆ°501 503 504 ÈòøÁéõÂ∞ºÁ∫¢ÁÆ°ÁúüÁöÑÊòØÊàëÊúÄÁà±ÁöÑÂîáÈáâ‰∫Ü ÂÖ•ÁöÑÁ¨¨‰∏ÄÊîØÊòØ504 Á¨¨‰∏ÄÊ¨°Áî®Â∞±Ë¢´ÊÉäËâ≥Âà∞‰∫Ü Á´üÁÑ∂ÊúâËøô‰πà‰∏ùÊªëÁöÑÂîáÈáâ ÂÆåÂÖ®ÊòØÂ•∂Ê≤πÊÖïÊñØË¥®Âú∞ ËøòÂëàÁé∞ÂæàÈ´òÁ∫ßÁöÑÂìëÂÖâÊÑü ÁúüÁöÑÊòØË∂ÖÁà± Áà±‰∏çÈáäÊâã ÂêéÊù•ÂèàÁõ∏ÁªßÊî∂‰∫Ü501Âíå503 ‰ª•Âêé‰∏ÄÂÆöËøò‰ºöÁªßÁª≠Êî∂ ÂñúÁà±Á®ãÂ∫¶ yslÊñπÁÆ°17 13 17ÊòØÊàëÊò•Â§èÁî®ÁöÑÊúÄÂ§öÊ¨°ÁöÑ‰∏ÄÊîØ Âá†‰πéÊØèÂ§©ÈÉΩÂú®Áî® ‰∏çÁü•ÈÅì‰ªäÂ§©Êì¶‰ªÄ‰πàÁöÑÊó∂ÂÄôÂ∞±Áî®ÂÆÉ ÂáÜÊ≤°Èîô ‰∏Ä‰∏™Â§èÂ§©ËøáÂéªÂ∞±‰∏ãÂéª‰∫Ü‰∏ÄÂ§ßÊà™ ÂñúÁà±Á®ãÂ∫¶ 13ÊòØÂêéÊúüÂÖ•ÁöÑ Áõ∏ÊØî17Âà©Áî®ÁéáÂ∞±Ê≤°ÊúâÈÇ£‰πàÈ´ò ÊÄé‰πàËØ¥Âë¢ 13ËøòÊòØÊúâ‰∫õÊåëÁöÆÁöÑ Á¥†È¢úÊ∂ÇËÇØÂÆöÊòØÂúüÁàÜ‰∫Ü ÂñúÁà±Á®ãÂ∫¶ macÂ≠êÂºπÂ§¥ see sheer‰πüÊòØÊàëÁöÑÂøÉÂ§¥Áà± Âàö‰π∞ÁöÑÊó∂ÂÄôÊØèÂ§©ÈÉΩÊîæÂåÖÈáå ÈöèÊó∂Ë°• Ë∂ÖÁ∫ßÊªãÊ∂¶ È¢úËâ≤ÁÇíÈ∏°Êó•Â∏∏ ‰∏äÁè≠Ê∂ÇÂÆåÂÖ®‰∏ç‰ºöÊòæÂæóÁ™ÅÂÖÄ Ë∂ÖÁà± ÂñúÁà±Á®ãÂ∫¶ chillÂ∞èËæ£Ê§í ‰πüÊòØË∂ÖÁà±ÁöÑ‰∏ÄÊîØ ÊÄé‰πàÊ∂ÇÈÉΩÂ•ΩÁúã ÊØ´‰∏çÂ§∏Âº† ÁßãÂÜ¨‰∏çÁü•ÈÅìÊ∂ÇÂï•Êó∂ Êì¶ÂÆÉÊÄª‰∏ç‰ºöÂá∫Èîô ÂñúÁà±Á®ãÂ∫¶ ÈõÖËØóÂÖ∞ÈªõloveÁ≥ªÂàó 300Ê©òÁ∫¢Ëâ≤ 310Ê¢ÖÂ≠êËâ≤ ÈÉΩÊòØÊàëÁà±ÁöÑÈ¢úËâ≤ ÂæàÊòæÁôΩ ÂñúÁà±Á®ãÂ∫¶ ÈòøÁéõÂ∞ºÂ∞èËÉñ‰∏Å504 Â§ßÂêçÈºéÈºéÁöÑÂ•∂Ê≤πÊ©ò ÂêÑÁßçË¢´Â§∏ Êâõ‰∏ç‰ΩèÈ£éÁßçËçâÁöÑ ‰ΩÜÊòØÔºÅÁúüÁöÑÊ≤°ÊúâÈÇ£‰πàÂ•Ω Â•ΩÂêó Êó†ËÆ∫ÂêéÊ∂ÇËñÑÊ∂ÇÊÑüËßâÈÉΩ‰∏çÊòØÂæàÊª°ÊÑè ÊàëÈÄöÂ∏∏ÊòØÂè†Âä†ÂìëÂÖâÂÖ∂‰ªñÂè£Á∫¢‰∏äÈù¢Áî® ÂñúÁà±Á®ãÂ∫¶ tf16 È£éËÄÅÂ§ßÁöÑÁï™ËåÑËâ≤ Âøç‰∏ç‰ΩèÂÖ•ÁöÑ ‰ΩÜÊòØ‰ΩøÁî®ÁéáÂæà‰Ωé ‰∏çÁü•ÈÅì‰∏∫Âï• ÂèØËÉΩ‰∏çÂ§üÊó•Â∏∏ ‰πüÂèØËÉΩÊòØ‰∏çËàçÂæóÁî® ÂÅ∑Á¨ëR ÂñúÁà±Á®ãÂ∫¶ È¶ôÂ•àÂÑø58 ‰πüÊòØË∑üÈ£éÂÖ•ÁöÑ ÊçÆËØ¥ÂêÑÁßçÈÄÇÂêàÁßãÂÜ¨ ÈÄÇÂêàÈªÑÁöÆ ÊØ´‰∏çÁäπË±´Êãø‰∏ã butÊúâÁÇπÊñØÊúõ Âπ∂‰∏çÊòØÂæàÂ•ΩÈ©æÈ©≠Âïä ËÄå‰∏îÊÑüËßâ‰∏çÊòæÁôΩÔºÅ ÂñúÁà±Á®ãÂ∫¶ yslÈªëÁÆ°402 ËøôÂè™ÊòØÁõ≤ÈÄâÁöÑ ÈÄÇÂêàÊò•Â§è ÊÑüËßâ‰∏ÄËà¨ ‰∏≠ËßÑ‰∏≠Áü© ÂñúÁà±Á®ãÂ∫¶ yslÈïúÈù¢ÂîáÈáâ09 Ê∞¥Á∫¢Ëâ≤ È¢úËâ≤‰∏çÈîô ‰ΩÜÊòØ‰ΩÜÊòØ Ëøô‰∏™ÂîáÈáâÁöÑË¥®Âú∞ÊàëÁúüÁöÑÊòØÁà±‰∏çËµ∑Êù• ‰∏çËÉΩÊäøÂò¥ Á≤òÁ≤òÁöÑ Âæà‰∏çÂ•Ω‰∏äÂùáÂåÄ ÂñúÁà±Á®ãÂ∫¶ 3ce ÂçóÁìúËâ≤116 Âπ≤ÔºÅÂ∑®Âπ≤ÔºÅÁÇíÈ∏°Âπ≤ÔºÅÊ†πÊú¨‰∏çËÉΩÁî® ‰π∞‰πãÂâçË¢´È¢úËâ≤Âê∏Âºï ÂæàÂ§ö‰∫∫ÈÉΩËØ¥ÂÆÉÂæàÂπ≤ ÊàëÂøÉÊÉ≥ ÂÜçÂπ≤ËÉΩÂπ≤ÊàêÂï•Ê†∑ Ââç‰∏ÄÂ§©Êôö‰∏äÂÅöÂîáËÜú Êó©Êô®ÂÖàÁî®ÂîáÈÉ®ÊâìÂ∫ïÂÜçÊ∂Ç ‰∏äÁè≠‰∏çÂà∞‰∏§Â∞èÊó∂ Âò¥Â∞±Âπ≤Âà∞Ëµ∑ÁöÆ ÂçóÁìúËâ≤Âè£Á∫¢ÂæàÂ§ö ÂçÉ‰∏áÂà´‰π∞Ëøô‰∏ÄÊîØ ÂñúÁà±Á®ãÂ∫¶ ÊúÄÂêéËØ¥‰∏ÄÂè• Êú¨‰∫∫ÈªÑÁöÆ ÂîáËâ≤ÊµÖ Âπ≤Ëµ∑ÁöÆÂîáÁ∫πÈáç ÂÄüËìùÁ•®ÁöÑËÉ≥ËÜäËØïËâ≤ ‰ªÖ‰æõÂèÇËÄÉ Á¨¨‰∏ÄÊ¨°ÂÜôËøô‰πàÈïøÁöÑÁ¨îËÆ∞ ÊâãÊåáÊñ≠‰∫Ü Â•Ω‰∫ÜÂ∞±ÈÖ± ÂèπÊ∞îR‚Äô

cut_res = HanLP(text, tasks=‚Äòtok/coarse‚Äô)
print(cut_res)
```

**Describe the current behavior**
A clear and concise description of what happened.

ÊàëÂàöÂàöÈáçÊñ∞pip install hanlp -U ‰πãÂêéÔºåÂπ∂ÂêåÊó∂Â∞ùËØïÊú¨Âú∞git pullÔºåÂÜçËøêË°åÂêåÊ†∑ÁöÑ‰ª£Á†ÅÔºå‰ºöÊä•Â¶Ç‰∏ãÊñ∞ÁöÑÈîôËØØÔºåËØ¥ÊòØÊó†Ê≥ïloadÊ®°ÂûãÔºå‰ΩÜÊàë‰πãÂâçÂ∑≤Áªè‰∏ãËΩΩËøáËøô‰∏™Ê®°ÂûãÂà∞ÊàëÊú¨Âú∞‰∫ÜÔºö

(base) river@riverdeMacBook-Pro mtl % ll
total 0
drwxr-xr-x 3 river staff 96 2 23 01:21 ./
drwxr-xr-x 4 river staff 128 2 22 21:36 ../
drwxr-xr-x 13 river staff 416 2 22 22:10 close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159/



**Expected behavior**
A clear and concise description of what you expected to happen.


1. ËøòËØ∑ÊïôÂéü‰ΩúËÄÖÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ„ÄÇ‰∏áÂàÜÊÑüË∞¢ÔºÅ
2. Âè¶Â§ñËøòÊúâ‰∏™ÈóÆÈ¢òÔºöÊòØÂê¶‰øÆÂ§ç‰πãÂêé„ÄÅ[Ëá™ÂÆö‰πâËØçË°®]ÂèØ‰ª•Áî®‰∫éopenÊ®°Âûã‰ª•ÂèäcloseÊ®°ÂûãÔºüËøô‰∏§ÁßçÊ®°ÂûãÁöÑ‰∏ªË¶ÅÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü Âõ†‰∏∫‰πãÂâçËØïÊñáÊú¨ÂàÜËØçÁöÑÊó∂ÂÄôÊúâ‰∫õÊ†∑Êú¨Á¢∞Âà∞tok/fine‰ºöÊä•ÈîôÔºåÊúâ‰∫õÁ¢∞Âà∞tok/coarse‰ºöÊä•Èîô„ÄÇ„ÄÇ„ÄÇopenÊ®°ÂûãÂè™Êúâ'tok'Ëøô‰∏™keyÔºåËÄåcloseÊ®°ÂûãÊúâ'tok/fine'Âíå'tok/coarse'‰∏§ÁßçÔºåÊòØÂê¶Ëá™ÂÆö‰πâËØçË°®‰πüÈÉΩÂèØ‰ª•Áî®‰∫éËøô2ÁßçÊ®°ÂºèÔºüÊàëÁöÑ‰∏öÂä°Âú∫ÊôØÈúÄË¶ÅÁ≤íÂ∫¶ÊØîËæÉÁªÜÁöÑ„ÄÅËØ≠‰πâ‰ø°ÊÅØÊõ¥‰∏∞ÂØåÁöÑ(ÂàÜÂâ≤ÊàêÊõ¥ÈïøÁöÑ)Áü≠ËØ≠ÔºåÊòØ‰∏çÊòØcloseÊ®°ÂûãÁöÑ'tok/coarse'ÁöÑÊïàÊûú‰ºöÊúÄÂ•ΩÔºü

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac OS 10.15.7

- Python version:
python 3.9.2

- HanLP version:
hanlp==2.1.0a20
hanlp-common==0.0.6
hanlp-trie==0.0.2
torch==1.7.1



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ÂÆåÊï¥Êä•Èîô‰ø°ÊÅØÂ¶Ç‰∏ãÔºö

Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/component_util.py"", line 81, in load_from_meta_file
obj.load(save_dir, verbose=verbose, **kwargs)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py"", line 183, in load
self.to(devices)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py"", line 519, in to
devices = cuda_devices(devices)
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/torch_util.py"", line 73, in cuda_devices
query = gpus_available()
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/utils/torch_util.py"", line 46, in gpus_available
nvmlShutdown()
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pynvml/nvml.py"", line 772, in nvmlShutdown
fn = get_func_pointer(""nvmlShutdown"")
File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pynvml/nvml.py"", line 387, in get_func_pointer
raise NVMLError(NVML_ERROR_UNINITIALIZED)
pynvml.nvml.NVMLError_Uninitialized: Uninitialized
=================================ERROR LOG ENDS=================================
Please upgrade hanlp with:
pip install --upgrade hanlp

If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.

* [x] I've completed this form and searched the web for solutions.
"
HanLP2.1ÂàÜËØçÊó∂Â§ßÈáèÊñáÊú¨‰ºöÊä•ÈîôÔºöRuntimeError: expected scalar type Float but found Long,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ÊàëÂú®Áî®HanLP2.1ÂàÜËØçÊó∂‰ºöÈÅáÂà∞Â§ßÈáèÊñáÊú¨Êä•ÈîôÔºöRuntimeError: expected scalar type Float but found Long
ÂæàÂ§öÈïøÊñáÊú¨ÂàÜËØçÊó∂‰ºöÊä•Ëøô‰∏™Èîô„ÄÇ„ÄÇ
Ë∞∑Ê≠åÊêú‰∫ÜÂçäÂ§©ÔºåÈÉΩËØ¥ÊòØpytorchÈáåÁöÑ‰ª£Á†ÅÈóÆÈ¢òÔºå‰∏çÁ°ÆÂÆöÊòØ‰∏çÊòØÊ∫êÁ†ÅÁöÑÈóÆÈ¢òÔºåËøòÊòØÊàëËøôËæπÁöÑÂéüÂõ†ÂØºËá¥„ÄÇ„ÄÇ

**Code to reproduce the issue**
ÂÆåÊï¥‰ª£Á†ÅÂ¶Ç‰∏ã

```python
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ‰∏ñÁïåÊúÄÂ§ß‰∏≠ÊñáËØ≠ÊñôÂ∫ì
tok = HanLP[‚Äòtok/coarse‚Äô]

# ËØªÂèñËá™ÂÆö‰πâËØçË°®
beauty_vocab = set()
with open(‚Äô‚Ä¶/data/userdict.txt‚Äô, ‚Äòr‚Äô, encoding=‚Äòutf-8‚Äô) as fin:
    for line in fin:
        beauty_vocab.add(line.strip())
tok.dict_combine = beauty_vocab

text = ‚ÄòÊàëÊúÄÂøÉÊ∞¥ÁöÑÈÇ£‰∫õ Á∫¢ÔºöÂØπÂè£Á∫¢ËøôÁßç‰∏úË•øÁúüÁöÑÊòØÊØ´Êó†ÊäµÊäóÂäõÂïä ÊØèÊ¨°ÂÖ•‰∫ÜÊñ∞Ëâ≤Â∞±Âú®ËìùÊúãÂèãÊâãËáÇ‰∏äËØï ‰ªñÊÄª‰ºöËØ¥ Â•≥ÁîüÂà∞Â∫ïÈúÄË¶ÅÊúâÂ§öÂ∞ëÂè£Á∫¢Âïä Ë∫´‰∏∫‰∏Ä‰∏™Áõ¥Áî∑ÂèØËÉΩÁúüÁöÑ‰∏çÊáÇ Á∫¢Êúâ‰∏ÄÁôæÁßç ÂìàÂìàÂìàÂìà Ë®ÄÂΩíÊ≠£‰º† ÈòøÁéõÂ∞ºÁ∫¢ÁÆ°501 503 504 ÈòøÁéõÂ∞ºÁ∫¢ÁÆ°ÁúüÁöÑÊòØÊàëÊúÄÁà±ÁöÑÂîáÈáâ‰∫Ü ÂÖ•ÁöÑÁ¨¨‰∏ÄÊîØÊòØ504 Á¨¨‰∏ÄÊ¨°Áî®Â∞±Ë¢´ÊÉäËâ≥Âà∞‰∫Ü Á´üÁÑ∂ÊúâËøô‰πà‰∏ùÊªëÁöÑÂîáÈáâ ÂÆåÂÖ®ÊòØÂ•∂Ê≤πÊÖïÊñØË¥®Âú∞ ËøòÂëàÁé∞ÂæàÈ´òÁ∫ßÁöÑÂìëÂÖâÊÑü ÁúüÁöÑÊòØË∂ÖÁà± Áà±‰∏çÈáäÊâã ÂêéÊù•ÂèàÁõ∏ÁªßÊî∂‰∫Ü501Âíå503 ‰ª•Âêé‰∏ÄÂÆöËøò‰ºöÁªßÁª≠Êî∂ ÂñúÁà±Á®ãÂ∫¶ yslÊñπÁÆ°17 13 17ÊòØÊàëÊò•Â§èÁî®ÁöÑÊúÄÂ§öÊ¨°ÁöÑ‰∏ÄÊîØ Âá†‰πéÊØèÂ§©ÈÉΩÂú®Áî® ‰∏çÁü•ÈÅì‰ªäÂ§©Êì¶‰ªÄ‰πàÁöÑÊó∂ÂÄôÂ∞±Áî®ÂÆÉ ÂáÜÊ≤°Èîô ‰∏Ä‰∏™Â§èÂ§©ËøáÂéªÂ∞±‰∏ãÂéª‰∫Ü‰∏ÄÂ§ßÊà™ ÂñúÁà±Á®ãÂ∫¶ 13ÊòØÂêéÊúüÂÖ•ÁöÑ Áõ∏ÊØî17Âà©Áî®ÁéáÂ∞±Ê≤°ÊúâÈÇ£‰πàÈ´ò ÊÄé‰πàËØ¥Âë¢ 13ËøòÊòØÊúâ‰∫õÊåëÁöÆÁöÑ Á¥†È¢úÊ∂ÇËÇØÂÆöÊòØÂúüÁàÜ‰∫Ü ÂñúÁà±Á®ãÂ∫¶ macÂ≠êÂºπÂ§¥ see sheer‰πüÊòØÊàëÁöÑÂøÉÂ§¥Áà± Âàö‰π∞ÁöÑÊó∂ÂÄôÊØèÂ§©ÈÉΩÊîæÂåÖÈáå ÈöèÊó∂Ë°• Ë∂ÖÁ∫ßÊªãÊ∂¶ È¢úËâ≤ÁÇíÈ∏°Êó•Â∏∏ ‰∏äÁè≠Ê∂ÇÂÆåÂÖ®‰∏ç‰ºöÊòæÂæóÁ™ÅÂÖÄ Ë∂ÖÁà± ÂñúÁà±Á®ãÂ∫¶ chillÂ∞èËæ£Ê§í ‰πüÊòØË∂ÖÁà±ÁöÑ‰∏ÄÊîØ ÊÄé‰πàÊ∂ÇÈÉΩÂ•ΩÁúã ÊØ´‰∏çÂ§∏Âº† ÁßãÂÜ¨‰∏çÁü•ÈÅìÊ∂ÇÂï•Êó∂ Êì¶ÂÆÉÊÄª‰∏ç‰ºöÂá∫Èîô ÂñúÁà±Á®ãÂ∫¶ ÈõÖËØóÂÖ∞ÈªõloveÁ≥ªÂàó 300Ê©òÁ∫¢Ëâ≤ 310Ê¢ÖÂ≠êËâ≤ ÈÉΩÊòØÊàëÁà±ÁöÑÈ¢úËâ≤ ÂæàÊòæÁôΩ ÂñúÁà±Á®ãÂ∫¶ ÈòøÁéõÂ∞ºÂ∞èËÉñ‰∏Å504 Â§ßÂêçÈºéÈºéÁöÑÂ•∂Ê≤πÊ©ò ÂêÑÁßçË¢´Â§∏ Êâõ‰∏ç‰ΩèÈ£éÁßçËçâÁöÑ ‰ΩÜÊòØÔºÅÁúüÁöÑÊ≤°ÊúâÈÇ£‰πàÂ•Ω Â•ΩÂêó Êó†ËÆ∫ÂêéÊ∂ÇËñÑÊ∂ÇÊÑüËßâÈÉΩ‰∏çÊòØÂæàÊª°ÊÑè ÊàëÈÄöÂ∏∏ÊòØÂè†Âä†ÂìëÂÖâÂÖ∂‰ªñÂè£Á∫¢‰∏äÈù¢Áî® ÂñúÁà±Á®ãÂ∫¶ tf16 È£éËÄÅÂ§ßÁöÑÁï™ËåÑËâ≤ Âøç‰∏ç‰ΩèÂÖ•ÁöÑ ‰ΩÜÊòØ‰ΩøÁî®ÁéáÂæà‰Ωé ‰∏çÁü•ÈÅì‰∏∫Âï• ÂèØËÉΩ‰∏çÂ§üÊó•Â∏∏ ‰πüÂèØËÉΩÊòØ‰∏çËàçÂæóÁî® ÂÅ∑Á¨ëR ÂñúÁà±Á®ãÂ∫¶ È¶ôÂ•àÂÑø58 ‰πüÊòØË∑üÈ£éÂÖ•ÁöÑ ÊçÆËØ¥ÂêÑÁßçÈÄÇÂêàÁßãÂÜ¨ ÈÄÇÂêàÈªÑÁöÆ ÊØ´‰∏çÁäπË±´Êãø‰∏ã butÊúâÁÇπÊñØÊúõ Âπ∂‰∏çÊòØÂæàÂ•ΩÈ©æÈ©≠Âïä ËÄå‰∏îÊÑüËßâ‰∏çÊòæÁôΩÔºÅ ÂñúÁà±Á®ãÂ∫¶ yslÈªëÁÆ°402 ËøôÂè™ÊòØÁõ≤ÈÄâÁöÑ ÈÄÇÂêàÊò•Â§è ÊÑüËßâ‰∏ÄËà¨ ‰∏≠ËßÑ‰∏≠Áü© ÂñúÁà±Á®ãÂ∫¶ yslÈïúÈù¢ÂîáÈáâ09 Ê∞¥Á∫¢Ëâ≤ È¢úËâ≤‰∏çÈîô ‰ΩÜÊòØ‰ΩÜÊòØ Ëøô‰∏™ÂîáÈáâÁöÑË¥®Âú∞ÊàëÁúüÁöÑÊòØÁà±‰∏çËµ∑Êù• ‰∏çËÉΩÊäøÂò¥ Á≤òÁ≤òÁöÑ Âæà‰∏çÂ•Ω‰∏äÂùáÂåÄ ÂñúÁà±Á®ãÂ∫¶ 3ce ÂçóÁìúËâ≤116 Âπ≤ÔºÅÂ∑®Âπ≤ÔºÅÁÇíÈ∏°Âπ≤ÔºÅÊ†πÊú¨‰∏çËÉΩÁî® ‰π∞‰πãÂâçË¢´È¢úËâ≤Âê∏Âºï ÂæàÂ§ö‰∫∫ÈÉΩËØ¥ÂÆÉÂæàÂπ≤ ÊàëÂøÉÊÉ≥ ÂÜçÂπ≤ËÉΩÂπ≤ÊàêÂï•Ê†∑ Ââç‰∏ÄÂ§©Êôö‰∏äÂÅöÂîáËÜú Êó©Êô®ÂÖàÁî®ÂîáÈÉ®ÊâìÂ∫ïÂÜçÊ∂Ç ‰∏äÁè≠‰∏çÂà∞‰∏§Â∞èÊó∂ Âò¥Â∞±Âπ≤Âà∞Ëµ∑ÁöÆ ÂçóÁìúËâ≤Âè£Á∫¢ÂæàÂ§ö ÂçÉ‰∏áÂà´‰π∞Ëøô‰∏ÄÊîØ ÂñúÁà±Á®ãÂ∫¶ ÊúÄÂêéËØ¥‰∏ÄÂè• Êú¨‰∫∫ÈªÑÁöÆ ÂîáËâ≤ÊµÖ Âπ≤Ëµ∑ÁöÆÂîáÁ∫πÈáç ÂÄüËìùÁ•®ÁöÑËÉ≥ËÜäËØïËâ≤ ‰ªÖ‰æõÂèÇËÄÉ Á¨¨‰∏ÄÊ¨°ÂÜôËøô‰πàÈïøÁöÑÁ¨îËÆ∞ ÊâãÊåáÊñ≠‰∫Ü Â•Ω‰∫ÜÂ∞±ÈÖ± ÂèπÊ∞îR‚Äô

cut_res = HanLP([text])[‚Äòtok/coarse‚Äô][0]
print(cut_res)
```

**Describe the current behavior**
‰ª£Á†ÅÂ¶Ç‰∏äÔºåÁ¢∞Âà∞ÈùûÂ∏∏Â§öÂ¶Ç‰∏äÁöÑtextÊñáÊú¨ÔºåÂàÜËØçÂ∞±‰ºöÊä•Èîô

**Expected behavior**
ËøòËØ∑ÊïôÂéü‰ΩúËÄÖÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ„ÄÇ‰∏áÂàÜÊÑüË∞¢ÔºÅ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac OS 10.15.7

- Python version:
python 3.9.2

- HanLP version:
hanlp==2.1.0a20
hanlp-common==0.0.6
hanlp-trie==0.0.2
torch==1.7.1

**Other info / logs**
ÂÆåÊï¥ÁöÑÊä•Èîô‰ø°ÊÅØÂ¶Ç‰∏ãÔºö

Traceback (most recent call last):
File ‚Äú/Users/river/Desktop/1-tag-recall/note-tag/optimization/src/preprocess.py‚Äù, line 27, in
cut_res = HanLP([text])
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py‚Äù, line 768, in call
return super().call(data, batch_size, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py‚Äù, line 26, in decorate_context
return func(*args, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.py‚Äù, line 629, in call
return super().call(data, **merge_dict(self.config, overwrite=True,
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/component.py‚Äù, line 36, in call
return self.predict(data, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py‚Äù, line 512, in predict
output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py‚Äù, line 590, in predict_task
output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.py‚Äù, line 682, in feed_batch
‚Äòoutput‚Äô: task.feed_batch(h,
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/init.py‚Äù, line 182, in feed_batch
return decoder(h, batch=batch, mask=mask)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py‚Äù, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.py‚Äù, line 35, in forward
contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py‚Äù, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py‚Äù, line 310, in forward
x = layer(x, mask)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py‚Äù, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py‚Äù, line 264, in forward
x = self.self_attn(x, mask)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py‚Äù, line 727, in call_impl
result = self.forward(*input, **kwargs)
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.py‚Äù, line 150, in forward
D = torch.einsum(‚Äònd,ld->nl‚Äô, self.r_w_bias, pos_embed)[None, :, None] # head x 2max_len, ÊØè‰∏™headÂØπ‰ΩçÁΩÆÁöÑbias
File ‚Äú/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/functional.py‚Äù, line 344, in einsum
return _VF.einsum(equation, operands) # type: ignore
RuntimeError: expected scalar type Float but found Long

* [x] I've completed this form and searched the web for solutions.
"
CharTabel ÂΩí‰∏ÄÂåñÈÉ®ÂàÜÂ≠óÁ¨¶Â≠òÂú®ÈîôËØØ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
1. Êúâ‰∏™issueÂÖ≥‰∫éË∞ÉÁî®CharTabelÔºåÊää‚ÄúÂπ∫‚ÄùÊîπ‰∏∫‚Äú‰πà‚Äù‰∏çÂêàÁêÜportable‰øÆÂ§ç‰∫ÜÔºå‰ΩÜÊòØ‰∏ãËΩΩ1.7.5 zipÂåÖÊúâÈóÆÈ¢òÔºåÂêéÂèëÁé∞CharTable.txt.bin md5‰∏ç‰∏ÄËá¥
2. ‰ª•‰∏ãÂ≠óÁ¨¶ÊúâÈóÆÈ¢òÔºöÂÖ∂‰∏≠Á¨¨‰∏ÄÂàóÊòØÂéüÂßãÂ≠óÁ¨¶ÔºåÁ¨¨‰∫åÂàóÊòØÂΩí‰∏ÄÂåñÂêéÂ≠óÁ¨¶ÔºåÊã¨Âè∑Ë°®Á§∫ Âª∫ËÆÆÂèØ‰ª•ËÄÉËôëÊã¨Âè∑ÂÜÖÂ≠óÁ¨¶ÊõøÊç¢ÂéüÊúâÂΩí‰∏ÄÂåñÂÜÖÂÆπ
Áåõ Âãê
Ëú∫ Èúì
ËÑä Âµ¥
È™º ËÉ≥
Êãæ ÂçÅ
Âäà Âôº
Ê∫ú ÁÜò
Âë± Âìå
ÊÄµ ÊÜ∑
Á≥∏ Á∫üÔºà‰∏ùÔºâ
‰πæ Âπ≤
Ëâ∏ ËâπÔºàËçâÔºâ
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
public void testCharTable() {
        Map<String, String> normalizationBadCase = new HashMap<>();
        normalizationBadCase.put(""Áåõ"", ""Áåõ"");
        normalizationBadCase.put(""Ëú∫"", ""Ëú∫"");
        normalizationBadCase.put(""ËÑä"", ""ËÑä"");
        normalizationBadCase.put(""È™º"", ""È™º"");
        normalizationBadCase.put(""Êãæ"", ""Êãæ"");
        normalizationBadCase.put(""Âäà"", ""Âäà"");
        normalizationBadCase.put(""Ê∫ú"", ""Ê∫ú"");
        normalizationBadCase.put(""Âë±"", ""Âë±"");
        normalizationBadCase.put(""ÊÄµ"", ""ÊÄµ"");
        normalizationBadCase.put(""Á≥∏"", ""‰∏ù"");
        normalizationBadCase.put(""‰πæ"", ""‰πæ"");
        normalizationBadCase.put(""Ëâ∏"", ""Ëçâ"");
        for (Map.Entry<String, String> entry : normalizationBadCase.entrySet()) {
            assert CharTable.convert(entry.getKey()).equals(entry.getValue());
        }
    }
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version:
- HanLP version: 1.8.0

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Translate documents to Chinese,
Ëá™ÂÆö‰πâËØçÂÖ∏ÂØπKBeamArcEagerDependencyParserÊó†Êïà,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Ê∑ªÂä†Êñ∞ËØçËøõÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÂêéÔºåKBeamArcEagerDependencyParser‰ªçÁÑ∂Êó†Ê≥ïÂØπËØ•ËØçËøõË°åÊ≠£Á°ÆÂàÜËØç„ÄÇ

**Code to reproduce the issue**
Ê∑ªÂä†""Áî≥ËØ∑Á¨î n 100""ËøõÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏
```scala
val parser = new KBeamArcEagerDependencyParser()
parser.parse(""ÂèàÊúâÊñ∞ÁöÑÁî≥ËØ∑Á¨îÂèØ‰ª•Êãø‰∫Ü"")
```
```
1      Âèà      Âèà      A       AD      _       2       advmod  _       _
2       Êúâ      Êúâ      V       VE      _       0       ROOT    _       _
3       Êñ∞      Êñ∞      V       VA      _       6       rcmod   _       _
4       ÁöÑ      ÁöÑ      D       DEC     _       3       cpm     _       _
5       Áî≥ËØ∑    Áî≥ËØ∑    N       NN      _       6       nn      _       _
6       Á¨î      Á¨î      N       NN      _       8       nsubj   _       _
7       ÂèØ‰ª•    ÂèØ‰ª•    V       VV      _       8       mmod    _       _
8       Êãø      Êãø      V       VV      _       2       dep     _       _
9       ‰∫Ü      ‰∫Ü      S       SP      _       2       dep     _       _
```

**Describe the current behavior**
Âç≥‰ΩøÊ∑ªÂä†‰∫ÜÊñ∞ËØçÔºå‰ªçÁÑ∂Êó†Ê≥ïÂØπÊñ∞ËØçËøõË°åÊ≠£Á°ÆÂàÜËØçÔºåËÄåÂè¶‰∏Ä‰∏™Êé•Âè£HanLP.parseDependencyÂèØ‰ª•Ê†πÊçÆÊñ∞Âä†ÂÖ•ÁöÑËØçËøõË°åÊ≠£Á°ÆÂàÜËØç

**Expected behavior**
ÊúüÂæÖËÉΩÈÄöËøáÂä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåKBeamArcEagerDependencyParserËÉΩÂ§üÂÖàÊ≠£Á°ÆÂàÜËØçÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äËøîÂõûÊ≠£Á°ÆÁªìÊûú
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux
- Scala version:2.12.2
- HanLP version:1.7.8

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
CoreStopWordDictionary.dictionary.clear()Á©∫ÊåáÈíà,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

Ë∞ÉÁî®`CoreStopWordDictionary.reload()`ÂêéË∞ÉÁî®`CoreStopWordDictionary.dictionary.clear()`‰ºöÁ©∫ÊåáÈíà„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
CoreStopWordDictionary.reload()
CoreStopWordDictionary.dictionary.clear()
```

**Describe the current behavior**
A clear and concise description of what happened.

ÊâßË°å`CoreStopWordDictionary.dictionary.clear()`Êó∂`equivalenceClassMDAGNodeHashMap`Á©∫ÊåáÈíàÂºÇÂ∏∏„ÄÇ

**Expected behavior**
A clear and concise description of what you expected to happen.

Ê≠£Á°ÆÊ∏ÖÁ©∫ÂÅúÁî®ËØç„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Big Sur 11.0.1 20B29 x86_64.
- Python version:ÈùûpythonÁâàÊú¨
- HanLP version:1.7.8

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ÂàÜÊûêÂéüÂõ†‰∏∫`StopWordDictionary.reload()`Êó∂ÊâßË°å‰∫Ü`MDAG.simplify()`ÔºåÂÖ∂‰∏≠Êúâ`equivalenceClassMDAGNodeHashMap = null;`

`MDAGSet.clear()`ÊñπÊ≥ï‰∏≠`equivalenceClassMDAGNodeHashMap.clear();`Ê≤°ÊúâÂà§Á©∫„ÄÇ

Áõ∏ÂÖ≥Ôºö[https://github.com/hankcs/HanLP/commit/a57645c739b403b5fe8ae7cba0fb44d0dd992317](https://github.com/hankcs/HanLP/commit/a57645c739b403b5fe8ae7cba0fb44d0dd992317)

* [x] I've completed this form and searched the web for solutions.
"
RuntimeError: Already borrowed using mtl model in Hanlp2.1.x  with multiple threads,"**Describe the bug**
The multiple task learning model in Hanlp2.1.x can not support multiple threads, a ```RuntimeError``` with ```Already borrowed``` is raised and can reproduce stably. Seem as this is a bug in transformers >3.5.0 with a fast tokenizer.

**Code to reproduce the issue**
```
from multiprocessing.dummy import Pool as ThreadPool
import hanlp

class Hanlp(object):
    def __init__(self):
        self.model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    def get_batch(self, sentences):
        return self.model(sentences)

model = Hanlp()
def tokenizer_test(text):
    print(model.get_batch(text)['tok/fine'])

pool = ThreadPool(10)  
data_list = ['ÊµãËØïËøôÊòØÊµãËØï'] * 100
pool.map(tokenizer_test, data_list)
pool.close()
pool.join()
```

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""mul_test.py"", line 17, in <module>
    pool.map(tokenizer_test, data_list)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 290, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 683, in get
    raise self._value
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/multiprocessing/pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""mul_test.py"", line 12, in tokenizer_test
    print(model.get_batch(text)['tok/fine'])
  File ""mul_test.py"", line 8, in get_batch
    return self.model(sentences)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 748, in __call__
    return super().__call__(data, batch_size, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/torch_component.py"", line 631, in __call__
    **kwargs))
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/component.py"", line 36, in __call__
    return self.predict(data, **kwargs)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 483, in predict
    for batch in dataloader:
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 429, in __iter__
    for raw_batch in super(PadSequenceDataLoader, self).__iter__():
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 435, in __next__
    data = self._next_data()
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 206, in __getitem__
    sample = self.transform_sample(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/dataset.py"", line 105, in transform_sample
    sample = self.transform(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/common/transform.py"", line 319, in __call__
    sample = t(sample)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 309, in __call__
    input_tokens, input_ids, subtoken_offsets = tokenize_str(input_tokens, add_special_tokens=True)
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 248, in tokenize_str
    add_special_tokens=add_special_tokens).encodings[0]
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 2462, in encode_plus
    **kwargs,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 465, in _encode_plus
    **kwargs,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 372, in _batch_encode_plus
    pad_to_multiple_of=pad_to_multiple_of,
  File ""/home/anaconda3/envs/tensorflow2-cpu/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 325, in set_truncation_and_padding
    self._tokenizer.no_truncation()
RuntimeError: Already borrowed
```

After search with Google, it seems as this is a bug in transformers >3.5.0 with a fast tokenizer.
https://github.com/huggingface/tokenizers/issues/537
I tried init an instance of the model for each thread as followed, the error is also occured, so confused. Whether there is bug in hanlp?
```
from multiprocessing.dummy import Pool as ThreadPool
import hanlp
from numpy import array

class Hanlp(object):
    def __init__(self):
        self.model = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    def get_batch(self, sentences):
        return self.model(sentences)

def tokenizer_test(model, text):
    print(model.get_batch(text)['tok/fine'])

pool = ThreadPool(10)
data_list = ['ÊµãËØïËøôÊòØÊµãËØï'] * 100
models = list(array([Hanlp() for i in range(10)]).repeat(10))
print(models)
data = zip(models, data_list)
pool.starmap(tokenizer_test, data)
pool.close()
pool.join()
```
If move model init in single thread as followsÔºåthere is no error...but this solution is not effective because each data will init a model...
```
def tokenizer_test(text):
    model = Hanlp()
    print(model.get_batch(text)['tok/fine'])
```

**Expected behavior**
no error

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Python version: 3.7.1
- HanLP version: 2.1.0
- transformer version: 3.5.1

**Other info / logs**
* [x] I've completed this form and searched the web for solutions."
"install hanlp bug,demo run error","

**Describe the bug**
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
dony222:test dony$ python3 testHanLP.py
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Failed to load https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_zh_20201222_130611.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 74, in load_from_meta_file
    obj: Component = object_from_classpath(cls)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp_common/reflection.py"", line 27, in object_from_classpath
    classpath = str_to_type(classpath)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp_common/reflection.py"", line 44, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/components/mtl/multi_task_learning.py"", line 23, in <module>
    from hanlp.components.mtl.tasks import Task
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/components/mtl/tasks/__init__.py"", line 22, in <module>
    from hanlp.transform.transformer_tokenizer import TransformerSequenceTokenizer
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/transform/transformer_tokenizer.py"", line 9, in <module>
    from hanlp.layers.transformers.pt_imports import PreTrainedTokenizer, PretrainedConfig, AutoTokenizer
  File ""/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/hanlp/layers/transformers/pt_imports.py"", line 10, in <module>
    from transformers import BertTokenizer, BertConfig, PretrainedConfig, \
ImportError: cannot import name 'BertTokenizerFast' from 'transformers' (/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/transformers/__init__.py)
=================================ERROR LOG ENDS=================================

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
HanLP(['2021Âπ¥HanLPv2.1‰∏∫Áîü‰∫ßÁéØÂ¢ÉÂ∏¶Êù•Ê¨°‰∏ñ‰ª£ÊúÄÂÖàËøõÁöÑÂ§öËØ≠ÁßçNLPÊäÄÊúØ„ÄÇ', 'ÈòøÂ©Ü‰∏ªÊù•Âà∞Âåó‰∫¨Á´ãÊñπÂ∫≠ÂèÇËßÇËá™ÁÑ∂ËØ≠‰πâÁßëÊäÄÂÖ¨Âè∏„ÄÇ']).pretty_print()
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.7
- HanLP version:2.1.0a5 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
TableTransform‰∏≠ÔºåË∞ÉÁî®read_cellsÊó∂ÊâÄÁî®ÁöÑÂèÇÊï∞ÂêçÔºåÂíåÂáΩÊï∞ÂÆö‰πâ‰∏≠ÁöÑÂèÇÊï∞Âêç‰∏ç‰∏ÄËá¥,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
read_cellsÂáΩÊï∞ÔºöÂú®TableTransform‰∏≠ÔºåË∞ÉÁî®read_cellsÊó∂ÊâÄÁî®ÁöÑÂèÇÊï∞ÂêçÔºåÂíåÂáΩÊï∞ÂÆö‰πâ‰∏≠ÁöÑÂèÇÊï∞Âêç‰∏ç‰∏ÄËá¥

**Code to reproduce the issue**
Ë∞ÉÁî®ÁöÑ‰ΩçÁΩÆÔºö
class TableTransform
def file_to_inputs
for cells in read_cells(filepath, **skip_header**=self.config.skip_header, delimiter=self.config.delimiter)

read_cellsÂáΩÊï∞Âú®io_utils.py‰∏≠ÁöÑÂÆö‰πâ‰∏∫Ôºö
def read_cells(filepath: str, delimiter='auto', strip=True, **skip_first_line**=False)

**Describe the current behavior**
Êó†Ê≥ïË∞ÉÁî®read_cells

**Expected behavior**
None

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): not related
- Python version: not related
- HanLP version: 2.1.0a0

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
1.x,Êõ¥Êñ∞‰ª£Á†Å
Fix evaluation and other minor issues to adapt to multi-label classification,
HanLP 1.x Âä†ËΩΩ bin ÁöÑÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ËøûÁª≠Âä†ËΩΩ‰∏§Ê¨°Ê®°ÂûãÊó∂ÔºåÁ¨¨‰∫åÊ¨°Ê≤°ÊúâÂä†ËΩΩËÄåÊòØÁõ¥Êé•‰ΩøÁî®Á¨¨‰∏ÄÊ¨°Âä†ËΩΩÁöÑÊ®°Âûã

**Code to reproduce the issue**

```java
    public static void main(String[] args) {
        // Á¨¨‰∏ÄÊ¨°ÊâßË°å‰ºöÊä•Âá∫Ë≠¶ÂëäÔºåÁÑ∂Âêé‰ºöËΩ¨Êç¢ txt Êñá‰ª∂‰∏∫ bin Êñá‰ª∂ÔºåÂêéÈù¢Â∞±‰∏çÂÜçÊä•Èîô
        // ËøûÁª≠ÊâßË°å‰∏§Ê¨°Êó∂ÔºåÁ¨¨‰∫åÊ¨°‰ΩøÁî®ÁöÑÊòØÁ¨¨‰∏ÄÊ¨°ËΩΩÂÖ•ÁöÑ bin Êñá‰ª∂
        // ‰∫§Êç¢‰ª•‰∏ãÁé∞Âú∫‰ª£Á†ÅÁöÑÈ°∫Â∫èÂ∞±ÂèØ‰ª•ÁúãÂà∞Âå∫Âà´
        show_subtitle(""my_cws_model"");
        trainBigram(MY_CWS_CORPUS_PATH, MY_MODEL_PATH);
        loadBigram(MY_MODEL_PATH);
        show_subtitle(""msr_ngram"");
        trainBigram(MSR_TRAIN_PATH, MSR_MODEL_PATH);
        loadBigram(MSR_MODEL_PATH);
    }
```

ËØ¶ÊÉÖÂèØ‰ª•ÂèÇËÄÉÔºöhttps://github.com/zhuyuanxiang/Hanlp-Books-Examples/blob/main/src/main/java/ch03/sec03/DemoNgramSegment.java

**Describe the current behavior**
--------------->my_cws_model<---------------
„ÄåÂïÜÂìÅ„ÄçÁöÑËØçÈ¢ëÔºö2
„ÄåÂïÜÂìÅ@Âíå„ÄçÁöÑÈ¢ëÊ¨°Ôºö1
[ÂïÜÂìÅ, Âíå, ÊúçÂä°]
[Ë¥ßÂ∏Å, Âíå, ÊúçÂä°]
--------------->msr_ngram<---------------
„ÄåÂïÜÂìÅ„ÄçÁöÑËØçÈ¢ëÔºö2
„ÄåÂïÜÂìÅ@Âíå„ÄçÁöÑÈ¢ëÊ¨°Ôºö1
[ÂïÜÂìÅ, Âíå, ÊúçÂä°]
[Ë¥ßÂ∏Å, Âíå, ÊúçÂä°]

**Expected behavior**
--------------->my_cws_model<---------------
„ÄåÂïÜÂìÅ„ÄçÁöÑËØçÈ¢ëÔºö2
„ÄåÂïÜÂìÅ@Âíå„ÄçÁöÑÈ¢ëÊ¨°Ôºö1
[ÂïÜÂìÅ, Âíå, ÊúçÂä°]
[Ë¥ßÂ∏Å, Âíå, ÊúçÂä°]
--------------->msr_ngram<---------------
„ÄåÂïÜÂìÅ„ÄçÁöÑËØçÈ¢ëÔºö1
„ÄåÂïÜÂìÅ@Âíå„ÄçÁöÑÈ¢ëÊ¨°Ôºö0
[ÂïÜÂìÅ, Âíå, ÊúçÂä°]
[Ë¥ßÂ∏Å, Âíå, ÊúçÂä°]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version:
- Java version: 8.0
- HanLP version: 1.7.8

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
added support for multi-label classification,
unable to use mixed precision,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
Unable to use automatic mixed precision

**Code to reproduce the issue**

```
if self.config.use_amp:
     policy = mixed_precision.Policy('mixed_float16')
     mixed_precision.set_policy(policy)
```

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/__main__.py"", line 45, in <module>
    cli.main()
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py"", line 430, in main
    run()
  File ""/Users/lei/.vscode/extensions/ms-python.python-2020.11.371526539/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py"", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str(""__main__""))
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/lei/Documents/Projects/NLP/Êñ∞ÂçéÁ§æNLP/ÂàÜÁ±ª/hanlp_classification.py"", line 42, in <module>
    history = classifier.fit(TRANIN_SET, DEV_SET, save_dir, **kwargs)
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/components/classifiers/transformer_classifier.py"", line 119, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/common/component.py"", line 341, in fit
    model, optimizer, loss, metrics = self.build(**merge_dict(self.config, logger=logger, training=True))
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/common/component.py"", line 255, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/components/classifiers/transformer_classifier.py"", line 174, in build_model
    model, self.transform.tokenizer = build_transformer(transformer, max_length, len(self.transform.label_vocab),
  File ""/Users/lei/Documents/Projects/NLP/HanLP/hanlp/layers/transformers/loader.py"", line 66, in build_transformer
    output = l_bert([l_input_ids, l_token_type_ids], mask=l_mask_ids)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 258, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /usr/local/lib/python3.8/site-packages/bert/model.py:79 call  *
        embedding_output = self.embeddings_layer(inputs, mask=mask, training=training)
    /usr/local/lib/python3.8/site-packages/bert/embeddings.py:226 call  *
        embedding_output += tf.reshape(pos_embeddings, broadcast_shape)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1141 binary_op_wrapper
        raise e
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1125 binary_op_wrapper
        return func(x, y, name=name)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1447 _add_dispatch
        return gen_math_ops.add_v2(x, y, name=name)
    /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:495 add_v2
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /usr/local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:503 _apply_op_helper
        raise TypeError(

    TypeError: Input 'y' of 'AddV2' Op has type float16 that does not match type float32 of argument 'x'.
```

**Expected behavior**
amp mode enabled

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Python version: 3.8
- HanLP version: LATEST

**Other info / logs**


* [x] I've completed this form and searched the web for solutions.
"
Added support for multi-label classification,
tokenizer.predict() ÂçäÂ∞èÊó∂ Â∑¶Âè≥ÂºÄÂßãÊä•Èîô,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
```python
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
t= tokenizer.predict(content,batch_size=900)
```

**Describe the current behavior**

tokenizer.predict Âçä‰∏™Â∞èÊó∂Â∑¶Âè≥ÂºÄÂßãÊä•Èîô

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Python version:3.8.5
- HanLP version:2.0.0a67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```python

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13499' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13500' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13501' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13502' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13503' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13504' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13505' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13506' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13507' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13508' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13509' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13510' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13511' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed





```

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
TypeError: build_callbacks() got multiple values for argument 'logger',"

**Describe the bug**
TypeError: build_callbacks() got multiple values for argument 'logger'

**Code to reproduce the issue**
```
import hanlp
from hanlp.datasets.classification.sentiment import CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_TEST, CHNSENTICORP_ERNIE_VALID

classifier = hanlp.load('LARGE_ALBERT_BASE')

# Train classifer
classifier.fit(CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_VALID, save_dir, transformer='albert_large_zh')

```

**Describe the current behavior**
TypeError: build_callbacks() got multiple values for argument 'logger'

**Expected behavior**
Model got trained

**System information**

**Other info / logs**
In `components.py`L326, it is trying to create a redundant logger, which caused this problem. Please take a look 

* [x] I've completed this form and searched the web for solutions."
module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects',"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.


Êä•Èîô Ôºö module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
```

**Describe the current behavior**
ÂÆâË£Ö‰∫Ügraphviz ‰πãÂêé ÂÜçËøêË°å ‰ª£Á†ÅÂºÄÂßãÊä•ÈîôÔºåÂç∏ËΩΩÊéâgraphviz ÈáçÊñ∞ÂÆâË£Ö tensorflowÂØπÂ∫îÁâàÊú¨‰æùÁÑ∂Êä•Èîô„ÄÇ

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 18.4
- Python version:3.6
- HanLP version:2.0.0a67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Ë∞ÉÁî®‰∫ÜÂàÜËØçÁöÑÊé•Âè£ÔºåÂèëÁé∞ÊàëÁöÑÁ®ãÂ∫èÊó†Ê≥ïÂÜôÊñá‰ª∂‰∫Ü,"
**Describe the bug**
Ë∞ÉÁî®‰∫ÜÂàÜËØçÁöÑÊé•Âè£ÔºåÂèëÁé∞ÊàëÁöÑÁ®ãÂ∫èÊó†Ê≥ïÂÜôÊñá‰ª∂‰∫Ü

**Code to reproduce the issue**

token = load('LARGE_ALBERT_BASE')
output = token('ÂïÜÂìÅÂíåÊúçÂä°')
fd = open('output.txt','w')

for item in output:
fd.write(item+'\n')

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version:3.6.8
- HanLP version:2.0.0-alpha.67

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
"KBeamArcEagerDependencyParser Âä†ËΩΩÊ®°ÂûãÊó∂Ë∑ØÂæÑÂê´""data""‰ºöÂØºËá¥ÂºÇÂ∏∏","**Describe the bug**
KBeamArcEagerDependencyParser Âä†ËΩΩÊ®°ÂûãÊó∂Ë∑ØÂæÑÂê´""data""‰ºöÂØºËá¥ÂºÇÂ∏∏

**Code to reproduce the issue**

```python
PerceptronPOSModelPath='G:/pythonProject/wikidata_django/env/Lib/site-packages/pyhanlp/static/data/model/perceptron/pku1998/pos.bin'
# Ë∞ÉÁî®
parser = KBeamArcEagerDependencyParser()
```

**Describe the current behavior**
ËΩΩÂÖ•Ê®°ÂûãÊó∂Áõ¥Êé•Â∞Ü`PerceptronPOSModelPath`ÁöÑË∑ØÂæÑ‰ªéidata_djangoÂºÄÂßãÊõøÊç¢‰∏∫`G:/pythonProject/wikidata/model/perceptron/ctb/pos.bin`

**Expected behavior**
‰ªé`pyhanlp/static/data`ÂºÄÂßãÊõøÊç¢

**System information**
- OS Platform and Distribution: Windows 10 20H2
- Python version: 3.8.6
- HanLP version: 1.7.8

**Other info / logs**
```java
public KBeamArcEagerDependencyParser(String modelPath) throws IOException, ClassNotFoundException
{
    this(new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                                       HanLP.Config.PerceptronPOSModelPath.replaceFirst(""data.*?.bin"", ""data/model/perceptron/ctb/pos.bin"")
    ).enableCustomDictionary(false), new KBeamArcEagerParser(modelPath));
}
```
`com.hankcs.hanlp.dependency.perceptron.parser.KBeamArcEagerDependencyParser` 59Ë°å replaceFirstÂØºËá¥
* [x] I've completed this form and searched the web for solutions."
ËØ∑Ê±ÇÂ¢ûÂä†‰∏ªÂä®ÂàùÂßãÂåñÂäüËÉΩ,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî®ÂàÜËØçÔºöNLPTokenizer.segment(word)ÔºåËÄóÊó∂5sÂ∑¶Âè≥
**Will this change the current api? How?**
no
**Who will benefit with this feature?**
quoter
**Are you willing to contribute it (Yes/No):**
no

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- Python version:
- HanLP version: 1.7.5

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
ForkËøõÁ®ã‰∏≠Âä†ËΩΩÊ®°ÂûãÂ§±Ë¥•,"
**Describe the bug**
A clear and concise description of what the bug is.
```bash
[2020-11-07 20:23:18,313: WARNING/ForkPoolWorker-2] Failed to load https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip. See traceback below:
[2020-11-07 20:23:18,313: WARNING/ForkPoolWorker-2] ================================ERROR LOG BEGINS================================
[2020-11-07 20:23:18,316: WARNING/ForkPoolWorker-2] Traceback (most recent call last):
[2020-11-07 20:23:18,316: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 49, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 75, in build_transformer
    with stdout_redirected(to=os.devnull):
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 559, in stdout_redirected
    stdout_fd = fileno(stdout)
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 540, in fileno
    raise ValueError(""Expected a file (`.fileno()`) or a file descriptor"")
[2020-11-07 20:23:18,317: WARNING/ForkPoolWorker-2] ValueError: Expected a file (`.fileno()`) or a file descriptor
[2020-11-07 20:23:18,318: WARNING/ForkPoolWorker-2] =================================ERROR LOG ENDS=================================
[2020-11-07 20:23:18,811: WARNING/ForkPoolWorker-2] If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
def attr_extract(txt):
    map_path = os.path.join(os.path.dirname(__file__), ""map.txt"")
    regulation = load_map(map_path)
    sentenceStr = txt
    tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
    wordList = tokenizer(sentenceStr)
    tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN)
    labelList = tagger(wordList)

```

**Describe the current behavior**
A clear and concise description of what happened.
```
celeryÂêéÂè∞‰ªªÂä°Â§ÑÁêÜÂàÜËØçÊó∂Âä†ËΩΩÊ®°ÂûãÂ§±Ë¥•
```
**Expected behavior**
A clear and concise description of what you expected to happen.
```
ÊúüÊúõÊ≠£Â∏∏ËøêË°å
```
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS
- Python version: Python3.6.12
- HanLP version: hanlp2.0.0a66

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
ÊÑüÁü•Êú∫ËØçÊÄßÊ†áÊ≥®Âá∫Áé∞java.lang.NullPointerExceptionÂºÇÂ∏∏,"**Describe the bug**
ÊåâÁÖßhttps://github.com/hankcs/HanLP/wiki/%E7%BB%93%E6%9E%84%E5%8C%96%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A0%87%E6%B3%A8%E6%A1%86%E6%9E%B6
ËÆ≠ÁªÉÔºåÁ¨¨‰∫åÊ≠•ËØçÊÄßÊ†áÊ≥®ÔºåÂá∫Áé∞Á©∫ÊåáÈíàÂºÇÂ∏∏

**Code to reproduce the issue**

```java
String posModelFile = ""D:\\Develop\\hanlp\\icwb2-data\\training\\model\\perceptron\\msr_training.utf8\\pos.bin"";
        PerceptronTrainer trainer = new POSTrainer();
        trainer.train(
                ""D:\\Develop\\hanlp\\icwb2-data\\training\\msr_training.utf8"",
                posModelFile
        );
```

**Describe the current behavior**
ËÆ≠ÁªÉÁöÑÈ¢ÑÊñôÊòØÔºömsr_training.utf8ÔºåÊÑüÁü•Êú∫ËØçÊÄßÊ†áÊ≥®ËÆ≠ÁªÉÂá∫Áé∞Á©∫ÊåáÈíàÂºÇÂ∏∏

**Expected behavior**
Ê≠£Â∏∏ËøêË°å

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version:
- HanLP version: portable-1.7.5

**Other info / logs**
`
ÂºÄÂßãÂä†ËΩΩËÆ≠ÁªÉÈõÜ...

java.lang.NullPointerException
	at java.util.TreeMap.getEntry(TreeMap.java:347)
	at java.util.TreeMap.get(TreeMap.java:278)
	at com.hankcs.hanlp.model.perceptron.tagset.TagSet.add(TagSet.java:44)
	at com.hankcs.hanlp.model.perceptron.instance.POSInstance.<init>(POSInstance.java:43)
	at com.hankcs.hanlp.model.perceptron.instance.POSInstance.create(POSInstance.java:262)
	at com.hankcs.hanlp.model.perceptron.POSTrainer.createInstance(POSTrainer.java:36)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer$1.process(PerceptronTrainer.java:288)
	at com.hankcs.hanlp.model.perceptron.utility.IOUtility.loadInstance(IOUtility.java:81)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.loadTrainInstances(PerceptronTrainer.java:282)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.train(PerceptronTrainer.java:122)
	at com.hankcs.hanlp.model.perceptron.POSTrainer.train(POSTrainer.java:43)
	at com.hankcs.hanlp.model.perceptron.PerceptronTrainer.train(PerceptronTrainer.java:320)
`

* [x] I've completed this form and searched the web for solutions.
"
È∫ªÁÉ¶ÁªôCRFSegmenterÊ∑ªÂä†ÊµÅÁöÑËØªÂèñÊñπÂºèÔºåÊñπ‰æøÁõ¥Êé•‰ªéjarÂåÖÈáåÁõ¥Êé•ËØªÂèñbinÊñá‰ª∂,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Are you willing to contribute it (Yes/No):**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
1.x,Â¢ûÂä†ÊµÅÁöÑËØªÂèñÊñπÂºè
spark‰∏≠‰ΩøÁî®,"<!--
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ
ÊèêÈóÆËØ∑‰∏äËÆ∫ÂùõÔºå‰∏çË¶ÅÂèëËøôÈáåÔºÅ

‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
spark‰∏≠Âè™ËÉΩÊú¨Âú∞‰ΩøÁî®hanlpÁöÑ‰æùÂ≠òÂè•Ê≥ïÂäüËÉΩÔºåÂú®udfÊàñÂàÜÂå∫‰∏≠Êó†Ê≥ï‰ΩøÁî®„ÄÇ

**Code to reproduce the issue**
Êä•Èîô‰ø°ÊÅØÔºö`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`

**Describe the current behavior**
ÊàëËØïÁùÄÂú®spark‰∏≠Âä†ËΩΩportableÂåÖ‚Äôhanlp-portable-1.7.8.jar‚ÄôÔºåÂπ∂‰∏îÂ∞ÜdataÊîæÂà∞hdfs‰∏äÔºåÈÖçÁΩÆ‰∫Ühanlp.propertiesÂíåioadapter„ÄÇ
Êú¨Âú∞ËøõË°å‰æùÂ≠òÂè•Ê≥ïÊòØÂèØ‰ª•ÊâßË°åÁöÑ„ÄÇ‰ΩÜÊòØËøêÁî®udfÊàñÂàÜÂ∏ÉÂºèÊâßË°åÂ∞±‰ºöÊä•ÈîôÔºö`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`

**Expected behavior**
Â∏åÊúõ‰∏çË¶ÅÊä•ÈîôÔºåudfÈ°∫Âà©ÊâßË°å

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Spark version: 2.4.5
- HanLP version:1.7.8

* [x] I've completed this form and searched the web for solutions.
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->
<!-- ÂèëË°®ÂâçÂÖàÊêúÁ¥¢ÔºåÊ≠§Â§Ñ‰∏ÄÂÆöË¶ÅÂãæÈÄâÔºÅ -->"
Âä†ËΩΩÊ®°Âûãhanlp.pretrained.cws.LARGE_ALBERT_BASEÊó∂Âá∫Èîô,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
Âä†ËΩΩcwsÊ®°ÂûãalbertÊó∂Âá∫Èîô
```bash
Downloading https://file.hankcs.com/hanlp/embeddings/albert_base_zh.tar.gz to /home/yihuazhou/.hanlp/embeddings/albert_base_zh.tar.gz

97.69%, 37.0 MB/37.9 MB, 92 KB/s, ETA 10 s      Failed to load https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 201, in download
    urlretrieve(url, tmp_path, reporthook)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/urllib/request.py"", line 286, in urlretrieve
    raise ContentTooShortError(
urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 38807364 out of 39731739 bytes>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 254, in build
    self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 34, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/layers/transformers/loader.py"", line 39, in build_transformer
    bert_dir = get_resource(model_url)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 340, in get_resource
    path = download(url=path, save_path=realpath)
  File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 214, in download
    installed_version, latest_version, latest_version_str = check_outdated()
ValueError: not enough values to unpack (expected 3, got 2)
=================================ERROR LOG ENDS=================================
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
```

**Describe the current behavior**
A clear and concise description of what happened.
ÁõÆÂâç‰∏ãËΩΩÂÆåalbertÊ®°ÂûãÂêéÂä†ËΩΩÂá∫Èîô

**Expected behavior**
A clear and concise description of what you expected to happen.
Â∏åÊúõËÉΩÊ≠£Â∏∏Âä†ËΩΩALbertÊ®°Âûã

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Python version: 3.8.3
- HanLP version: '2.0.0-alpha.61'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Which word segmentation is faster in python or java  when using spark,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
I want to try HanLP in spark (in scala, i can directly use HanLP Java version 1.x) or pyspark(HanLP 2.x). The datasets are billion level, so i want to know which one is better
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
No
**Are you willing to contribute it (Yes/No):**
No
**System information**
- OS Platform and Distribution :Linux Ubuntu 16.04
- Python version: 3.7
- HanLP version: 2.x in Python, 1.x in Java
- Scala version: 2.11.x
- Java version: 1.8
**Any other info**
thank you for your support, i tried to find the post button in its bbs but couldn't find it. BTW, hava a happy national day!

* [x] I've carefully completed this form.
"
Cannot process Supplementary Character in Java / Êó†Ê≥ïÂú®Java‰∏≠Â§ÑÁêÜSupplementaryÂ≠óÁ¨¶,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

When handling supplementary character, HanLP(I tested with pinyin and word segmentation) couldn't handle the supplementary character properly. For short. Java represent a unicode character > 0xFFFF as two sepeprate char, thus HanLP treat them as two seperate Chinese character when getting Pinyin on it. However, those chars not assigned to any validate charset, so the pinyin result would be two 'none', rather than one 'none'.

Word segmentation cannot recognize it, but would always keep them as a word.

Â§ÑÁêÜ[SupplementaryÂ≠óÁ¨¶](https://www.oracle.com/technical-resources/articles/javase/supplementary.html)Êó∂ÔºåHanLPÔºàÊàëÊµãËØï‰∫ÜÊãºÈü≥Ê†áÊ≥®ÂíåÂàÜËØçÔºâ‰ºº‰πéÊ≤°Ê≥ïÊÅ∞ÂΩìÁöÑÂ§ÑÁêÜSupplementaryÂ≠óÁ¨¶„ÄÇÁÆÄÂçïÂú∞ËØ¥ÔºåJavaÂ∞Ü0xFFFF‰ª•‰∏äÁöÑUnicodeÂ≠óÁ¨¶Ë°®Á§∫‰∏∫‰∏§‰∏™charÔºåÂõ†Ê≠§HanLPÂú®Ê†áÊ≥®ÊãºÈü≥ÁöÑÊó∂ÂÄô‰ºöÂ∞ÜÂÖ∂ËßÜ‰∏∫‰∏§‰∏™Áã¨Á´ãÁöÑÊ±âÂ≠ó„ÄÇÁÑ∂ËÄåËøô‰∫õcharÁöÑÂÄºÁâπÊÑèÁöÑÊ≤°ÊúâÊåáÂÆöÁªô‰ªªÊÑèÊúâÊïàÁöÑÂ≠óÁ¨¶ÈõÜÔºåÂõ†Ê≠§ÊãºÈü≥Ê†áÊ≥®ÁöÑÁªìÊûúÊòØ‰∏§‰∏™'none'ÔºåËÄå‰∏çÊòØ‰∏Ä‰∏™'none'

ÂàÜËØç‰πüÂπ∂‰∏çËÉΩËØÜÂà´ËøôÁßçÂ≠óÁ¨¶Ôºå‰ΩÜÊòØÂàÜËØçÊÄª‰ºöÁ°Æ‰øùËøô‰∫õÂ≠óÁ¨¶ÊòØ‰∏Ä‰∏™ËØçÔºåÁªìÊûú‰∏≠‰∏ç‰ºö‰∫ßÁîüÁ†¥Á¢éÁöÑchar„ÄÇ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```kotlin
println(HanLP.convertToPinyinList(""Ø®õ"").map { it.pinyinWithToneMark })
```

**Describe the current behavior**
A clear and concise description of what happened.

Get: `[none, none]`
`Ø®õ` would be represent as `\uD87E\uDE1B` in Java, none of them is validate Chinese character. So got 2 `none`.
`Ø®õ`Âú®Java‰∏≠Ë¢´Ë°®Á§∫‰∏∫ `\uD87E\uDE1B`ÔºåÊØè‰∏Ä‰∏™ÂçïÁã¨ÁöÑcharÈÉΩ‰∏çÊòØÊúâÊïàÁöÑ‰∏≠ÊñáÂ≠óÁ¨¶ÔºåÂõ†Ê≠§ÂæóÂà∞‰∫Ü‰∏§‰∏™`none`„ÄÇ

**Expected behavior**
A clear and concise description of what you expected to happen.

Should get: `[f√©n]`, or at least a `[none]`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004
- Python version: Not available
- HanLP version: 1.7.8
- Java Version: Java 11

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
ËØçÊÄßÊ†áÊ≥®ËΩΩÂÖ•Ê®°ÂûãÊó∂ÂèëÁîüÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ËΩΩÂÖ•ËØçÊÄßÊ†áÊ≥®Ê®°Âûãctb5_pos_rnn_fasttext_20191230_202639.zip Êó∂ÂèëÁîüÈîôËØØÔºåÊèêÁ§∫ÁâàÊú¨‰∏çÂåπÈÖç„ÄÇ
ÂâçÂá†‰∏™Êúà‰ΩøÁî®‰∏ÄÁõ¥Ê≠£Â∏∏Ôºå‰πãÂâçÁöÑhanlpÁâàÊú¨ÊòØ2.0.0a44„ÄÇ‰ªäÂ§©‰ΩøÁî®ÁöÑÊó∂ÂÄôÁ™ÅÁÑ∂Êä•ÈîôÔºåÊèêÁ§∫Êõ¥Êñ∞ÂêéÂÜçËØï„ÄÇ
‰ΩÜÊòØÊõ¥Êñ∞Âà∞2.0.0a60Âêé‰æùÁÑ∂Êä•Èîô„ÄÇ

**Code to reproduce the issue**
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)

**Describe the current behavior**
ËΩΩÂÖ•Êó∂Êä•ÈîôÔºöMemoryError: bad allocationÔºå
ÊèêÁ§∫Ôºöhttps://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running a lower version: 2.0.0-alpha.60. Please upgrade hanlp with:
pip install --upgrade hanlp
upgrade‰πãÂêé‰ªçÁÑ∂Â≠òÂú®ËØ•ÈîôËØØÔºåÂ∑≤ÊòØÊúÄÊñ∞ÁâàÊú¨„ÄÇ

**Expected behavior**
ËΩΩÂÖ•Ê®°ÂûãÊàêÂäüÔºå‰∏çÊä•Èîô

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 ‰∏ì‰∏öÁâà 18363.1016
- Python version:3.6.5
- HanLP version:2.0.0a60

**Other info / logs**
Êä•Èîô‰ø°ÊÅØÔºö
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\components\taggers\rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\layers\embeddings\__init__.py"", line 34, in build_embedding
    custom_objects=tf.keras.utils.get_custom_objects())
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 360, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 697, in from_config
    return cls(**config)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\hanlp\layers\embeddings\fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\fasttext\FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""D:\Study\NLP\pyNLP\venv\lib\site-packages\fasttext\FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
MemoryError: bad allocation
=================================ERROR LOG ENDS=================================
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running a lower version: 2.0.0-alpha.60. 
Please upgrade hanlp with:
pip install --upgrade hanlp
If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
When reporting an issue, make sure to paste the FULL ERROR LOG above.

* [x] I've completed this form and searched the web for solutions.
"
Translate documents to Chinese,
"HanLP 2.0ËØçÊÄßÊ†áÊ≥®ÈõÜÔºà‰∏≠Ëã±ÊñáÔºâ‰ΩïÊó∂ÂèØ‰ª•ÂÖ¨Â∏ÉÊñáÊ°£ÔºüÊÉ≥Áü•ÈÅìDEG, PU, CDÁ≠âÊ†áÊ≥®ÂàÜÂà´ÂØπÂ∫î‰ªÄ‰πàËØçÊÄß","<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Ê±ÇÂä©ÔºåHanLP 2.0ËØçÊÄßÊ†áÊ≥®ÈõÜÔºà‰∏≠Ëã±ÊñáÔºâÂì™ÈáåÂèØ‰ª•ÊâæÂà∞Ôºü
2.0ÁöÑËØçÊÄßÊ†áÊ≥®ÈÉΩÊç¢Êàê‰∫ÜÂ§ßÂÜôÂ≠óÊØç: CC, VV, DEG, PU, CDÁ≠âÁ≠âÔºåÊàëÊÉ≥Áü•ÈÅìÊØè‰∏Ä‰∏™ÂØπÂ∫î‰ªÄ‰πàËØçÊÄß

Êâæ‰∫Ü‰∏ÄÂ§ßÂúàÊ≤°ÊúâÊâæÂà∞2.0ÂØπÂ∫îÁöÑÊ†áÊ≥®ÈõÜ„ÄÇ„ÄÇÂè™ÊêúÂà∞‰∫Ü1.xÁöÑÔºöhttp://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8

‰∏áÂàÜÊÑüË∞¢ÔºÅ

**Will this change the current api? How?**
‰∏ç‰ºöÂΩ±ÂìçapiÔºå‰ΩÜÊòØ‰∏∫‰∫Ü‰æø‰∫é‰ΩøÁî®ÁöÑËØùÔºåËøòÊòØÂæàÈúÄË¶ÅËøô‰∏™Ê†áÊ≥®ÈõÜ„ÄÇ„ÄÇ

**Who will benefit with this feature?**
ÊâÄÊúâ‰ΩøÁî®Êñπ

**Are you willing to contribute it (Yes/No):**
Yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:3.7
- HanLP version:2.0

**Any other info**
ÊÑüË∞¢‰ΩúËÄÖÁöÑÂÖ±‰∫´ÔºÅ
* [x] I've carefully completed this form.
"
Albert_tokenination.py Áº∫Â§±‰∏Ä‰∏™import ,Áº∫Â§± import sentencepiece as spm
"Âú®ËøêË°å""HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py""‰ª£Á†ÅÊó∂Êä•Èîô„ÄÇ","<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
Âú®ËøêË°å""HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py""‰ª£Á†ÅÊó∂Êä•Èîô„ÄÇ
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
ËøêË°åËøô‰∏™HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.pyËÑöÊú¨„ÄÇ
```python
```

**Describe the current behavior**
A clear and concise description of what happened.
Êä•Èîô‰ø°ÊÅØTypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'
**Expected behavior**
A clear and concise description of what you expected to happen.
Ê≠£Â∏∏ËÆ≠ÁªÉ
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:python3.7
- HanLP version:HanLP 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:/Users/Administrator/Desktop/HanLP-doc-zh/HanLP-doc-zh/tests/train/zh/train_msra_ner_albert.py"", line 14, in <module>
    metrics='f1')
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\ner.py"", line 83, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\taggers\transformers\transformer_tagger.py"", line 57, in fit
    return super().fit(**merge_locals_kwargs(locals(), kwargs))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\common\component.py"", line 352, in fit
    metrics=metrics, overwrite=True))
  File ""C:\Users\Administrator\Desktop\HanLP-doc-zh\HanLP-doc-zh\hanlp\components\taggers\transformers\transformer_tagger.py"", line 93, in train_loop
    validation_steps=dev_steps,
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 806, in train_function
    return step_function(self, iterator)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 796, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1211, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 2585, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 2945, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\autograph\impl\api.py"", line 275, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 789, in run_step
    outputs = model.train_step(data)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 757, in train_step
    self.trainable_variables)
  File ""C:\Users\Administrator\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 2745, in _minimize
    experimental_aggregate_gradients=False)
TypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'
* [x] I've completed this form and searched the web for solutions.
"
hanlpÈúÄË¶ÅÂ¢ûÂä†hdfsË∑ØÂæÑËØªÂèñÂäüËÉΩ,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
hanlpÈúÄË¶ÅÂ¢ûÂä†hdfsË∑ØÂæÑËØªÂèñÂäüËÉΩ
**Will this change the current api? How?**
 Â¢ûÂä†hdfs IOAdapter 
**Who will benefit with this feature?**
Â¢ûÂº∫ÂäüËÉΩÁöÑÈÄÇÁî®ÊÄßÔºå‰∏çÁÑ∂ÊØèÊ¨°‰øÆÊîπËØçÂÖ∏ÔºåËøòÈúÄË¶ÅÈáçÊñ∞ÊâìÂåÖÔºõÂú®ÂÖ¨Âè∏Â§ßÊï∞ÊçÆÁéØÂ¢É‰∏ãÈáåÊòØ‰∏çÂêàÈÄÇÁöÑ
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   linux „ÄÅ Distribution
- Python version: none
- HanLP version:  portable1.7.8

**Any other info**
hanlp javaÁâàÊú¨ÔºåÈúÄË¶ÅÂ¢ûÂä†hdfs ÁöÑÊìç‰Ωú„ÄÇÂ¶ÇÊûúÂè™ÊòØÊääÂ≠óÂÖ∏ÔºåÊàñËÄÖÊ®°ÂûãÊîæÂú® jarÈáåÂ§™Â§ß‰∫Ü„ÄÇ
Â∏åÊúõÂèØ‰ª•Êéà‰∫àÂàÜÊîØÊùÉÈôêÔºåÊàëÂ∑≤ÁªèÂÜôÂÆå‰ª£Á†Å‰∫Ü„ÄÇËÆ©ÊàëÊèê‰∫§‰∏ãÂàÜÊîØÂêß„ÄÇË∞¢Ë∞¢„ÄÇüôè
* [x] I've carefully completed this form.
"
ËØ∑Ê±ÇÊ∑ªÂä†Âà§Êñ≠Â≠óÁ¨¶ÊòØÂê¶‰∏∫Ê±âÂ≠óÁöÑ API,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
‰πãÂâçÂú®Áî® TinyPinyin Â∫ìËøõË°åÊãºÈü≥ËΩ¨Êç¢Ôºå‰ΩÜÊòØÂèëÁé∞‰∫ÜËØ•Â∫ìÁöÑÂäüËÉΩÊõ¥‰∏∫ÂÆåÂñÑÂº∫Â§ßÔºåÈÅÇ‰ΩøÁî®ËØ•Â∫ìÈáçÊûÑ‰πãÂâçÁöÑ‰ª£Á†ÅÔºå‰ΩÜÊòØÂÖ∂‰∏≠‰ΩøÁî®‰∫ÜÂà§Êñ≠Â≠óÁ¨¶ÊòØÂê¶‰∏∫Ê±âÂ≠óÁöÑ APIÔºåËÄåÂú®ËØ•Â∫ì‰∏≠Ê≤°ÊúâÊâæÂà∞Á±ª‰ººÁöÑÂÖ¨ÂºÄ APIÔºå‰∏çËÉΩÂÆåÂÖ®Ê∂àÈô§ÂØπ TinyPinyin Â∫ìÁöÑ‰æùËµñÔºåÊâÄ‰ª•ÊÉ≥ËØ∑‰ΩúËÄÖÊ∑ªÂä†Á±ª‰ºº `HanLP.isHan('Ê±â')` ÁöÑ API„ÄÇ

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Everyone. ËØ•Â∫ì‰∏ìÊ≥®‰∫éÊ±âËØ≠È¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÂäüËÉΩÔºåÊâÄ‰ª•ÊàëËßâÂæóÊ≠§ÂäüËÉΩÁöÑÂèó‰ºó‰πüÊå∫ÂπøÁöÑ„ÄÇ

**Are you willing to contribute it (Yes/No):**
No.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows [ÁâàÊú¨ 10.0.17763.1039]
- Python version: None
- HanLP version: Java com.hankcs:hanlp:portable-1.7.8

**Any other info**
TinyPinyin Â∫ìÂà§Êñ≠Â≠óÁ¨¶ÊòØÂê¶‰∏∫Ê±âÂ≠óÁöÑ API `Pinyin.isChinese('Ê±â')`: https://github.com/promeG/TinyPinyin/blob/master/lib/src/main/java/com/github/promeg/pinyinhelper/Pinyin.java#L146

* [x] I've carefully completed this form.
"
Update setup.py,Update sentencepiece version to 0.1.86
URLTokenizer ‰∏çËÉΩËØÜÂà´‰∏≠ÊñáÈ°∂Á∫ßÂüüÂêç,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÊÅï‰∏çÂèóÁêÜ„ÄÇ
-->

**Describe the bug**
ËøêË°åÊµãËØï Demo https://github.com/hankcs/HanLP/blob/1.x/src/test/java/com/hankcs/demo/DemoURLRecognition.java ÔºåÂèëÁé∞‰∏çËÉΩËØÜÂà´Âá∫Â∏¶‰∏≠ÊñáÈ°∂Á∫ßÂüüÂêçÁöÑÁΩëÂùÄ„ÄÇ

**Code to reproduce the issue**

```python
        String text =
                ""HanLPÁöÑÈ°πÁõÆÂú∞ÂùÄÊòØhttps://github.com/hankcs/HanLPÔºå"" +
                        ""ÂèëÂ∏ÉÂú∞ÂùÄÊòØhttps://github.com/hankcs/HanLP/releasesÔºå"" +
                        ""ÊàëÊúâÊó∂ÂÄô‰ºöÂú®www.hankcs.com‰∏äÈù¢ÂèëÂ∏É‰∏Ä‰∫õÊ∂àÊÅØÔºå"" +
                        ""ÊàëÁöÑÂæÆÂçöÊòØhttp://weibo.com/hankcs/Ôºå‰ºöÂêåÊ≠•Êé®ÈÄÅhankcs.comÁöÑÊñ∞Èóª„ÄÇ"" +
                        ""Âê¨ËØ¥.‰∏≠ÂõΩÂüüÂêçÂºÄÊîæÁî≥ËØ∑‰∫Ü,‰ΩÜÊàëÂπ∂Ê≤°ÊúâÁî≥ËØ∑hankcs.‰∏≠ÂõΩ,Âõ†‰∏∫Á©∑‚Ä¶‚Ä¶"";
        List<Term> termList = URLTokenizer.segment(text);
        System.out.println(termList);
        for (Term term : termList)
        {
            if (term.nature == Nature.xu)
                System.out.println(term.word);
        }
```
ËæìÂá∫Ôºö
```
[HanLP/nx, ÁöÑ/uj, È°πÁõÆ/n, Âú∞ÂùÄ/n, ÊòØ/v, https://github.com/hankcs/HanLP/xu, Ôºå/w, ÂèëÂ∏É/v, Âú∞ÂùÄ/n, ÊòØ/v, https://github.com/hankcs/HanLP/releases/xu, Ôºå/w, Êàë/r, ÊúâÊó∂ÂÄô/d, ‰ºö/v, Âú®/p, www/nx, ./w, hankcs/nrf, ./w, com/nx, ‰∏äÈù¢/f, ÂèëÂ∏É/v, ‰∏Ä‰∫õ/m, Ê∂àÊÅØ/n, Ôºå/w, Êàë/r, ÁöÑ/uj, ÂæÆÂçö/n, ÊòØ/v, http://weibo.com/hankcs/xu, //w, Ôºå/w, ‰ºö/v, ÂêåÊ≠•/vd, Êé®ÈÄÅ/nz, hankcs/nrf, ./w, com/nx, ÁöÑ/uj, Êñ∞Èóª/n, „ÄÇ/w, Âê¨ËØ¥/v, ./w, ‰∏≠ÂõΩ/ns, ÂüüÂêç/n, ÂºÄÊîæ/v, Áî≥ËØ∑/v, ‰∫Ü/ul, ,/w, ‰ΩÜ/c, Êàë/r, Âπ∂/c, Ê≤°Êúâ/v, Áî≥ËØ∑/v, hankcs/nrf, ./w, ‰∏≠ÂõΩ/ns, ,/w, Âõ†‰∏∫/c, Á©∑/a, ‚Ä¶‚Ä¶/w]
https://github.com/hankcs/HanLP
https://github.com/hankcs/HanLP/releases
http://weibo.com/hankcs
```

**Describe the current behavior**
‰∏çËÉΩËØÜÂà´Âá∫Â∏¶‰∏≠ÊñáÈ°∂Á∫ßÂüüÂêçÁöÑÁΩëÂùÄ„ÄÇ

**Expected behavior**
ËÉΩËØÜÂà´Âá∫Â∏¶‰∏≠ÊñáÈ°∂Á∫ßÂüüÂêçÁöÑÁΩëÂùÄ„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Update setup.py,Update params-flow==0.8.2
Ê®°Âûã‰ºòÂåñÈóÆÈ¢ò,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
ÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ãÊòØÂê¶ÂèØ‰ª•ËÄÉËôë‰ΩøÁî®tensorflow-liteÊàñËÄÖopenvino‰Ωú‰∏∫Êé®ÁêÜÂêéÁ´ØÔºåËøôÊ†∑ÁöÑËØùÂèØ‰ª•ÈÅøÂÖç‰æùËµñÊï¥‰∏™tensorflowÊàñpytorchÂ∫ìÔºå‰πüËÉΩÊèêÈ´òÂä†ËΩΩÈÄüÂ∫¶ÔºåÂØπ‰∫égpuËµÑÊ∫ê‰∏çÊòØÂæà‰∏∞ÂØåÁöÑÁî®Êà∑‰πüÊõ¥Âä†ÂèãÂ•ΩÔºåËÉΩÂ§üÊèêÂçáÊ°åÈù¢cpuÁ´ØÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ
Â¶ÇÊûú‰ΩøÁî®transformerÁöÑËØùÔºåÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®Ê®°ÂûãËí∏È¶èÂáèÂ∞ëÂ†ÜÂè†Â±ÇÊï∞Ôºà‰æãÂ¶Ç, DistilBertÔºâÔºåÂêåÊ†∑ËÉΩÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
everyone who use this

**Are you willing to contribute it (Yes/No):**
ÂèØ‰ª•ÂÅöÔºå‰ΩÜÊòØÂèØËÉΩÊ≤°ÊúâË∂≥Â§üÁöÑËµÑÊ∫êÂÅö

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version: v2.0.0-alpha.0      

**Any other info**
no

* [x] I've carefully completed this form.
"
ImportError: cannot import name 'BertModelLayer',"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
hanlp2.0ÂÆâË£Ö‰ΩøÁî®ÈîôËØØ
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import hanlp
```python
```

**Describe the current behavior**
A clear and concise description of what happened.
ImportError: cannot import name 'BertModelLayer'
**Expected behavior**
A clear and concise description of what you expected to happen.
Ê≠£Â∏∏ÂØºÂÖ•
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Python version:3.6.9
- HanLP version:hanlp-2.0.0a46

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/__init__.py"", line 17, in <module>
    import hanlp.components
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/__init__.py"", line 5, in <module>
    from . import tok
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/tok.py"", line 12, in <module>
    from hanlp.components.taggers.transformers.transformer_tagger import TransformerTagger
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 13, in <module>
    from hanlp.layers.transformers.loader import build_transformer
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 9, in <module>
    from bert import BertModelLayer, albert_models_tfhub, fetch_tfhub_albert_model
* [x] I've completed this form and searched the web for solutions.
yes"
HMM-FirstOrderHiddenMarkovModelTestÊñá‰ª∂bugÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
Âú®ÊàëË∑üË∏™‰ΩøÁî®hmmÂØπÁé©ÂÖ∑ÂåªÁñóÊ®°ÂûãÂàÜÊûêÊó∂ÔºåÂæóÂá∫probÂπ∂Èùû0.015ÔºåÂàÜÊûêÈóÆÈ¢òÂèëÁé∞bugÂ¶Ç‰∏ã
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
 for(int cur_s = 0; cur_s < max_s; ++cur_s) {
            score[cur_s] = this.start_probability[cur_s] + this.emission_probability[cur_s][observation[0]];
        }
Â¶ÇËøôÈáåÊúÄÂêéobservation[0]Âú®t=2Êó∂ÂàªÊó∂ÊòØ‰ΩìÂØíÁä∂ÊÄÅÔºåobservation[0]ÂÄº‰∏∫2 Ôºå ÁÑ∂ËÄå.emission_probability[cur_s][2]‰∏≠Âç¥ÊòØÁ¨¨‰∏â‰∏™Áä∂ÊÄÅÁöÑÂÄºÔºåÈóÆÈ¢òÂéüÂõ†Âú®‰∫é0‰∏ãÊ†á  ÂØºËá¥2‰∏ãÊ†áÊåáÂêë‰∫ÜÁ¨¨‰∏â‰∏™Áä∂ÊÄÅÂÄº „ÄÇ

/**
     * ÊòæÁä∂ÊÄÅ
     */
    enum Feel
    {
        normal,
        dizzy,
        cold,
    }
ÊàëÂú®FeelÂ£∞ÊòéÊó∂Ë∞ÉÊç¢‰∫Ü dizzyÂíåcoldÈ°∫Â∫è ‰ΩøÂÖ∂‰∏éemission_probabilityÁä∂ÊÄÅÁü©ÈòµÂØπÂ∫îÔºåÂæóÂá∫probÊ¶ÇÁéáÂÄº‰∏∫0.015(‰πãÂâçÊ¶ÇÁéáÁªìÊûú‰∏∫0.0097199995Ôºâ
```

**Describe the current behavior**
A clear and concise description of what happened.
‰∏Ä‰∏™Â∞èÂ∞èÁöÑBUG
**Expected behavior**
A clear and concise description of what you expected to happen.
Êèê‰∫§
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows
- Python version:3.6
- HanLP version:1.78

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
testPredict
* [x] I've completed this form and searched the web for solutions.
"
ÊâãÂä®ÂÆâË£ÖÁöÑpyhanlpÊó†tests.test_utilityÊ®°ÁªÑ,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Ëá™Âä®ÂÆâË£ÖÂ§±Ë¥•Ôºå‰ΩøÁî®ÊâãÂä®ÂÆâË£ÖÔºåfrom pyhanlp import *ÂèØ‰ª•ÊàêÂäüË∞ÉÁî®hanlp
**Will this change the current api? How?**
‰ΩÜÊòØÂêéÁª≠Â≠¶‰π†ÈÅáÂà∞‰∫ÜËøô‰∏™‰ª£Á†Å‚Äî‚Äîfrom tests.test_utility import ensure_data
**Who will benefit with this feature?**
ÊèêÁ§∫‰∏çÂ≠òÂú®Ëøô‰∏™Ê®°ÁªÑÔºåËØ∑ÈóÆÂ¶Ç‰ΩïËß£ÂÜ≥Ôºü
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
ËæìÂÖ•‰∏Ä‰∏™ËØçËØ≠ÔºåË¶ÅÊ±ÇËøîÂõû‰∏éËøô‰∏™ËØçÁõ∏ÂÖ≥ÁöÑËØçËØ≠„ÄÇÈô§‰∫ÜËØçÂêëÈáèËÅöÁ±ªËøòÊúâÂà´ÁöÑÊñπÊ≥ïÂêó,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
Ê±Ç‰∏ÄÁßçËÉΩÂ§üÂ§ÑÁêÜÊ†áÈ¢òÊâÄËØ¥‰ªªÂä°ÁöÑÊñπÊ≥ïÔºåÂè™ÈúÄË¶ÅÊÄùË∑ØÂ∞±ÂèØ‰ª•„ÄÇ
**Will this change the current api? How?**
Â¶ÇÊûúÊàëÂèØ‰ª•ÂÆåÊàêËøô‰∏™ÂäüËÉΩÔºå‰ºöÂºÄÊ∫ê‰ª£Á†ÅÂπ∂Ë¥°ÁåÆÁªôËøô‰∏™È°πÁõÆ
**Who will benefit with this feature?**
We
**Are you willing to contribute it (Yes/No):**
Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
hanlp2.0Ê∫êÁ†ÅÂ∞èÈîôËØØ,"ÊúÄËøëÁúãhanlp2.0ÁöÑÊ∫êÁ†ÅÔºåÂèëÁé∞‰∏Ä‰∏™‰∏çÂΩ±ÂìçhanlpÂäüËÉΩÁöÑÂ∞èÊâãËØØÔºàÂèØËÉΩÔºâ„ÄÇ
‰ª£Á†Å‰ΩçÁΩÆÔºöhanlp/common/transform‰∏≠264Ë°å„ÄÇ
ÂáΩÊï∞ÂêçÁß∞‰∏∫input_to_inputs„ÄÇ
Â∞èÈîôËØØ‰∏∫ËØ•ÂáΩÊï∞ÁöÑËøîÂõûÂÄºÁ±ªÂûãTuple[bool, Any]
ÁªìÂêàËØ•ÂáΩÊï∞ÂÆûÈôÖËøîÂõûÂÄºÁ±ªÂûã‰∏éÂÖ∂‰ªñÂú∞ÊñπË∞ÉÁî®ËØ•ÂáΩÊï∞ËøîÂõûÂÄº‰ΩøÁî®ÊÉÖÂÜµÔºåËøîÂõûÂÄºÁ±ªÂûãÂ∫îËØ•‰∏∫Tuple[Any, bool]„ÄÇ
‰∏çÂΩ±Âìç‰ΩøÁî®ÔºåÂ∞±ÊòØÁºñËæëÂô®pycharm‰ºöÊä•warning„ÄÇ"
ÂàÜËØç‰πãÂêéËØçÂíåÂéüÊñá‰∏ç‰∏ÄËá¥ÔºåÂàÜËØç‰πãÂêéÊúâÁöÑÊ±âÂ≠óÂíåÂéüÊñá‰∏≠‰∏ç‰∏ÄËá¥,"**Describe the bug**

ÂàÜËØç‰πãÂêéËØçÂíåÂéüÊñá‰∏ç‰∏ÄËá¥ÔºåÂàÜËØç‰πãÂêéÊúâÁöÑÊ±âÂ≠óÂíåÂéüÊñá‰∏≠‰∏ç‰∏ÄËá¥Á§∫‰æãÊ∫êÁ†ÅÂ¶Ç‰∏ã„ÄÇ

**Code to reproduce the issue**

```java
String paragraph =""ÂÆ¢ËßÇÊù•ËÆ≤ÔºåËøô‰∏Ä‰ª∑Ê†º‰∏éÁ¶èÁâπÁåõÁ¶ΩÁõ∏ÊØîÊúâÁùÄ‰∏çÂ∞è‰ºòÂäøÔºå‰ΩÜÂΩìÁñ´ÊÉÖÂàöÂàöËøáÂéªÔºåÂõΩ‰∫∫‰ºöËä±20‰∏áÂÖÉ‰π∞‰∏ÄËæÜÁöÆÂç°ÂêóÔºü"";
Segment  segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableIndexMode(true).enableMultithreading(true);
  List<Term> termList = segment.seg(paragraph);
```

**Describe the current behavior**

 ÂàÜËØç‰πãÂêétermList ‰∏≠ ‚Äò‚ÄúÁåõÁ¶Ω‚Äù  ÂèòÊàê‰∫Ü  ‚ÄúÂãêÁ¶Ω‚Äù

**Expected behavior**

Â∏åÊúõÂàÜËØç‰πãÂêéÂíåÂéüÂè•‰∏≠ÁöÑËØç‰øùÊåÅ‰∏ÄËá¥
Âç≥termList ‰æùÁÑ∂ÊòØ ‚Äò‚ÄúÁåõÁ¶Ω‚Äù ‰∏§‰∏™Â≠ó

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7
- java version: 1.8 
- HanLP version: 1.7.3

**Other info / logs**
ÊúÄÂêéÁöÑÂàÜËØçÁªìÊûúÔºåÂú®‰∏ãÈù¢ÁöÑÊà™Âõæ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞„ÄÇ
![ÂàÜËØç](https://user-images.githubusercontent.com/30866940/87658023-35326500-c78e-11ea-9379-f84a10beaab2.png)

* [x] I've completed this form and searched the web for solutions.
"
Ê∑ªÂä†ÊñáÊú¨Ê†áÁÇπ,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
    Âú®‰∏ÄÊÆµÊó†Ê†áÁÇπÊñáÊú¨‰∏≠ÔºåÊ∑ªÂä†Ê†áÁÇπÊñ≠Âè•
**Will this change the current api? How?**
  ‰∏ç‰ºö
**Who will benefit with this feature?**
    ÂèØÁî®‰∫éËØ≠Èü≥ËØÜÂà´ÊâÄÁîüÊàêÁöÑÊñáÊú¨ÔºåÂ∞ÜÊó†Ê†áÁÇπÁöÑÊñáÊú¨Êñ≠Âè•
**Are you willing to contribute it (Yes/No):**
   Yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**
  No
* [x] I've carefully completed this form.
   
"
‰æùÂ≠òÊ†ëÁªìÊûÑÈîôËØØÔºåÊüê‰∫õhead indexÊ†πÊú¨‰∏çÂ≠òÂú®,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
‰æùÂ≠òÊ†ëÁªìÊûÑÈîôËØØÔºåÊüê‰∫õhead indexÊ†πÊú¨‰∏çÂ≠òÂú®

**Code to reproduce the issue**

```python
>>> syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
>>> print(syntactic_parser([(Â∞ºÂáØÊÅ©, 'NR'), ('ÊâãÊú∫', 'NN'), ('ÊÄé‰πàÊ†∑', 'AD'), ('Ôºü', 'PU')])) #ËøôÊòØhanlpÂÆòÊñπdemo pipelineËæìÂá∫ÁöÑÂàÜËØçÁªìÊûú
1       Â∞ºÂáØÊÅ©  _       NR      _       _       2       nn      _       _
2       ÊâãÊú∫    _       NN      _       _       4       assmod  _       _
3       ÊÄé‰πàÊ†∑  _       AD      _       _       2       assm    _       _
4       Ôºü      _       PU      _       _       7       nsubj   _       _
```

**Describe the current behavior**
‰∏äËø∞Ê†∑‰æã‰∏≠Ôºå‚ÄòÔºü‚ÄôÁöÑheadËäÇÁÇπ7ÂÆåÂÖ®‰∏çÂ≠òÂú®

**Expected behavior**
head index <=  mac vertex index


**System information**
- OS Platform and Distribution :centos 7
- Python version: 3.7.6
- HanLP version: hanlp-2.0.0a44

**Other info / logs**
Â•ΩÂ§öÈÉΩÊúâÁªìÊûÑÈîôËØØÔºåËØ¶ËßÅ  [bug_syn.txt](https://github.com/hankcs/HanLP/files/4857936/bug_syn.txt)
```
Â∞ºÂáØÊÅ©ÊâãÊú∫ÊÄé‰πàÊ†∑Ôºü
1       Â∞ºÂáØÊÅ©  _       NR      _       _       2       nn      _       _
2       ÊâãÊú∫    _       NN      _       _       4       assmod  _       _
3       ÊÄé‰πàÊ†∑  _       AD      _       _       2       assm    _       _
4       Ôºü      _       PU      _       _       7       nsubj   _       _

Â∞ºÂáØÊÅ©ËÄÅ‰∫∫Êú∫ÊâãÊú∫Â•ΩÁî®Âêó
1       Â∞ºÂáØÊÅ©  _       NR      _       _       3       nn      _       _
2       ËÄÅ‰∫∫    _       NN      _       _       3       nummod  _       _
3       Êú∫ÊâãÊú∫  _       NN      _       _       5       nn      _       _
4       Â•Ω      _       AD      _       _       5       nn      _       _
5       Áî®      _       VV      _       _       7       nsubj   _       _
6       Âêó      _       SP      _       _       7       mmod    _       _

Â§©ËØ≠ÊâãÊú∫Âà∞Â∫ïÊÄé‰πàÊ†∑Ôºü
1       Â§©ËØ≠    _       NN      _       _       4       dep     _       _
2       ÊâãÊú∫    _       NN      _       _       4       punct   _       _
3       Âà∞Â∫ï    _       AD      _       _       4       nummod  _       _
4       ÊÄé‰πàÊ†∑  _       AD      _       _       6       nsubj   _       _
5       Ôºü      _       PU      _       _       6       nsubj   _       _

Â§©ËØ≠Êô∫ËÉΩÊâãÊú∫ÊÄé‰πàÊ†∑?
1       Â§©ËØ≠    _       NN      _       _       2       nn      _       _
2       Êô∫ËÉΩ    _       NN      _       _       6       nn      _       _
3       ÊâãÊú∫    _       NN      _       _       6       nn      _       _
4       ÊÄé‰πàÊ†∑  _       AD      _       _       5       nn      _       _
5       ?       _       PU      _       _       6       nn      _       _


```


* [x] I've completed this form and searched the web for solutions.
"
fix errors for combineNER,fix errors when a compound word consists of two words and appears at the end of a sentence
jvm.dllÊñá‰ª∂Âú®ÂØπÂ∫îË∑ØÂæÑ‰∏ãÂ≠òÂú®‰ΩÜstartJvmÊâæ‰∏çÂà∞,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**

pyhanlpÂíåJPype1ÈÉΩÂÆâË£ÖÂ•Ω‰∫ÜÂØπÂ∫îÁöÑÁéØÂ¢ÉÔºåÂπ∂‰∏îJAVAÁéØÂ¢É‰πüÈÖçÁΩÆÂ•Ω‰∫ÜÔºåcmdËøêË°åjava‰πüÊòØokÁöÑ„ÄÇ
‰ΩÜÊòØËøêË°å‰ª£Á†ÅÊó∂Ôºåjvm.dllÊñá‰ª∂Âú®ÂØπÂ∫îË∑ØÂæÑ‰∏ãÂ≠òÂú®‰ΩÜstartJvmÊâæ‰∏çÂà∞

**Code to reproduce the issue**

import pyhanlp
# Êú∫ÊûÑÂêçËØÜÂà´,Ê†áÊ≥®‰∏∫nt
def organization_recognize(sentence):
    segment = HanLP.newSegment().enableOrganizationRecognize(True)
    return segment.seg(sentence)
print(organization_recognize('dawdaw'))

```python
```

**Describe the current behavior**
Á®ãÂÆáËøêË°åÊó∂Êä•ÈîôÊó†Ê≥ïÊâæÂà∞jvm.dllÊñá‰ª∂Ôºå‰ΩÜËØ•Êñá‰ª∂Âú®ÂØπÂ∫îË∑ØÂæÑ‰∏ãÂ≠òÂú®

**Expected behavior**

Ê≠£Â∏∏ÊâßË°å

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- Python version: python3.8
- HanLP version: jpype1-0.7.0 pyhanlp-0.1.66

**Other info / logs**
ÂÖ∑‰Ωì‰ø°ÊÅØÂ¶ÇÂõæÔºö

![1](https://user-images.githubusercontent.com/65661595/85613616-a51c6680-b68c-11ea-8a74-b6f92f0f42c5.png)
![2](https://user-images.githubusercontent.com/65661595/85613620-a64d9380-b68c-11ea-9561-2402145f5bde.png)
![3](https://user-images.githubusercontent.com/65661595/85613630-a8afed80-b68c-11ea-8a35-91441ead366e.png)



* [ ] I've completed this form and searched the web for solutions.
Âπ∂Êú™ÊâæÂà∞Ëß£ÂÜ≥ÊñπÊ°à"
Java Memory Leak NLPÂàÜËØçÂä†ËΩΩÊ®°ÂûãÂç†Áî®ÁöÑÂÜÖÂ≠ò‰∏çÈáäÊîæ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
JavaÈ°πÁõÆÂêØÂä®ÂêéÂä†ËΩΩÊâßË°åÂ¶Ç‰∏ã‰ª£Á†ÅÔºö
NLPTokenizer.segment(""ÂΩìÂπ¥Êú´‰∏™‰∫∫Â≠òÊ¨æ‰ΩôÈ¢ù"")
‰ª£Á†ÅÂêéÔºåÊ†πÊçÆÁõëÊéßÂ∑•ÂÖ∑ÊèêÁ§∫Âä†ËΩΩÊ®°ÂûãÂç†Áî®500Â§öMÁöÑÂÜÖÂ≠òÔºåÂ•ΩÂÉèÈïøÊó∂Èó¥‰∏çÈáäÊîæÔºåCMSÂûÉÂúæÂõûÊî∂Âô®‰∏ÄÁõ¥Âú®ÂõûÊî∂‰ΩÜÊòØÂõûÊî∂‰∏çÊéâÔºåÊúâ‰ªÄ‰πàÊñπÊ≥ïÂèØ‰ª•ÈáäÊîæÂÜÖÂ≠òÂêóÔºå‰ΩøÁî®MATÂ∑•ÂÖ∑ÊèêÁ§∫ÂÜÖÂ≠òÂèØËÉΩÊ≥ÑÊºèÔºåÊàñËÄÖÂáèÂ∞ëÊ®°ÂûãÂç†Áî®ÁöÑÂÜÖÂ≠òÂ§ßÂ∞è

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
Class Name                                                                                           | Shallow Heap | Retained Heap | Percentage
-------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                                     |              |               |           
org.apache.catalina.loader.WebappClassLoader @ 0x769990000                                           |          208 |   546,208,656 |     72.14%
|- class com.hankcs.hanlp.tokenizer.NLPTokenizer @ 0x76acbe470                                       |            8 |   489,747,520 |     64.68%
|  '- com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer @ 0x76a791710                      |           32 |   489,747,512 |     64.68%
|     |- com.hankcs.hanlp.model.perceptron.PerceptronSegmenter @ 0x76fed02f8                         |           24 |   278,039,544 |     36.72%
|     |  |- com.hankcs.hanlp.model.perceptron.model.StructuredPerceptron @ 0x76ff1bec8               |           24 |   278,038,960 |     36.72%
|     |  |  |- com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap @ 0x76c1575d8       |           24 |   149,699,096 |     19.77%
|     |  |  |  '- com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger @ 0x7702d8b30|           32 |   149,699,072 |     19.77%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.IntArrayList @ 0x76c7c8ce8              |           40 |    74,849,512 |      9.89%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.IntArrayList @ 0x76c7c8d10              |           40 |    74,849,512 |      9.89%
|     |  |  |     |- com.hankcs.hanlp.collection.trie.datrie.Utf8CharacterMapping @ 0x77031ead8      |           16 |            16 |      0.00%
|     |  |  |     '- Total: 3 entries                                                                |              |               |           
|     |  |  |- float[32084956] @ 0x7770ecf40                                                         |  128,339,840 |   128,339,840 |     16.95%
|     |  |  '- Total: 2 entries                                                                      |              |               |           
|     |  |- com.hankcs.hanlp.model.perceptron.tagset.CWSTagSet @ 0x770326b38                         |           48 |           560 |      0.00%
|     |  '- Total: 2 entries                                                                         |              |               |           
|     |- com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger @ 0x76ff1bee0                         |           16 |   164,834,720 |     21.77%
|     |- com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer @ 0x76ff1e5f8                      |           24 |    46,873,184 |      6.19%
|     |- com.hankcs.hanlp.seg.Config @ 0x76b798570                                                   |           32 |            32 |      0.00%
|     '- Total: 4 entries                                                                            |              |               |           
-------------------------------------------------------------------------------------------------------------------------------------------------

```

**Describe the current behavior**

ÂèëÁé∞‰∏çÂ§™ÈáäÊîæÂÜÖÂ≠òÔºåÂÜÖÂ≠òÂç†Áî®ËøáÂ§öÔºåÊòØ‰∏çÊòØ‰ΩøÁî®ÈªòËÆ§ÁöÑÊ®°ÂûãÂ∫ìÊñá‰ª∂Â§™Â§ßÔºåÂä†ËΩΩ‰∫ÜËøáÂ§öÁöÑÁºìÂ≠ò

**Expected behavior**

ÊúüÂæÖÂèØ‰ª•ÂÆöÊúüÈáäÊîæÊàñÊúâÂäûÊ≥ïÂ§ÑÁêÜÈôç‰ΩéÂç†Áî®ÂÜÖÂ≠òÂ§ßÂ∞èÔºåÊàñËÄÖÂÜÖÁΩÆÁöÑÊ®°ÂûãÂ∫ìÂ¶Ç‰ΩïÂáèÂ∞è

**System information**
- Mac OS10.14
- Java1.8
- HanLP1.7.5

**Other info / logs**
ÊöÇÊó†

* [x] I've completed this form and searched the web for solutions."
java portable-1.7.7 data/dictionary/placeÁº∫Â∞ëÊñá‰ª∂ns.txt.bin,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
java portable-1.7.7ÁâàÊú¨Ôºå data/dictionary/placeÁº∫Â∞ëÊñá‰ª∂ns.txt.bin

**Code to reproduce the issue**
1. ÂºïÂÖ•maven‰æùËµñ
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.7.7</version>
</dependency>

2. ËøêË°å‚ÄúÂú∞ÂùÄËØÜÂà´‚ÄùÂÆòÊñπdemo
```java
String[] testCase = new String[]{
        ""Ê≠¶ËÉúÂéøÊñ∞Â≠¶‰π°ÊîøÂ∫úÂ§ßÊ•ºÈó®ÂâçÈî£ÈºìÂñßÂ§©"",
        ""ËìùÁøîÁªôÂÆÅÂ§èÂõ∫ÂéüÂ∏ÇÂΩ≠Èò≥ÂéøÁ∫¢Ê≤≥ÈïáÈªëÁâõÊ≤üÊùëÊçêËµ†‰∫ÜÊåñÊéòÊú∫"",
};
Segment segment = HanLP.newSegment().enablePlaceRecognize(true);
for (String sentence : testCase)
{
    List<Term> termList = segment.seg(sentence);
    System.out.println(termList);
}
```


**Describe the current behavior**
Êä•ÈîôÔºåÊèêÁ§∫data/dictionary/placeÁº∫Â∞ëÊñá‰ª∂ns.txt.bin„ÄÇ
Êü•Áúã‰æùËµñÂåÖ‰∏≠ÔºåÁº∫Â∞ëÊ≠§Êñá‰ª∂


**Expected behavior**
Ê≠£Â∏∏ËøêË°å

**System information**
- Ubuntu 16
- Java version: 11
- HanLP version: portable-1.7.7

* [ ] I've completed this form and searched the web for solutions.
"
segment.seg wrong,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
ÂΩìÊàë‰ΩøÁî®‰ª•‰∏ãËøôÊÆµ‰ª£Á†ÅÊó∂Ôºö
segment = HanLP.newSegment().enableNameRecognize(True)
result = segment.seg(""Êüê‰∏ÄÊÆµËØù"")

Ê≠§Êó∂ÊàëÂú®ÊàëÁöÑUbuntuÁ≥ªÁªü‰∏ãÂπ∂‰∏ç‰ºöÊä•ÈîôÔºåÂπ∂‰ºöËøîÂõûÁªìÊûú
‰ΩÜÂΩìÊàëÂú®WindowsÁ≥ªÁªü‰∏ãÂç¥Êä•ÈîôÔºö
TypeError: Ambiguous overloads found for com.hankcs.hanlp.seg.Segment.seg(str) between:
	public java.util.List com.hankcs.hanlp.seg.Segment.seg(char[])
	public java.util.List com.hankcs.hanlp.seg.Segment.seg(java.lang.String)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
Ubuntu‰∏ãÊ≠£Â∏∏ËøîÂõûÁªìÊûúÔºåWindows‰∏ãÊ≤°Êúâ

**Expected behavior**
Â∫îËØ•Ê≠£Â∏∏ËøîÂõûÁªìÊûú

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04 and Windows10
- Python version: Python3.8
- HanLP version:  HanLp1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [ ] I've completed this form and searched the web for solutions.
"
Ëá™ÂÆö‰πâIOAdapterÊó∂com.hankcs.hanlp.dictionary.other.CharType Â≠òÂú®BUG,"

**Describe the bug**
A clear and concise description of what the bug is.
Âú®‰ΩøÁî®hanlp ‰Ωú‰∏∫ ÈòøÈáåÁöÑelasicsearch‰∫ëÂàÜËØçÊèí‰ª∂Êó∂ÔºåÈòøÈáå‰∫ëÈôêÂà∂‰∫ÜÊèí‰ª∂ÂÜôÊñá‰ª∂ÊùÉÈôêÔºåÂè™ËÉΩËøúÁ®ãËÆøÈóÆhanlpÂ≠óÂÖ∏Êñá‰ª∂ÔºåÂõ†Ê≠§Ëá™ÂÆö‰πâ IOAdapterÔºå‰ΩÜCharType Á±ª‰øùÂ≠ò data/dictionary/other/CharType.bin Êñá‰ª∂Êó∂ÔºåÊú™‰ΩøÁî®IOUtilÁ±ªÔºåÁõ¥Êé•‰ΩøÁî®‰∫ÜFileOutputStream Á±ª‰øùÂ≠òÔºåÂØºËá¥Êó†Ê≥ïÂÜôÂÖ•Êñá‰ª∂ÂºÇÂ∏∏
**Code to reproduce the issue**
`CharType.java`
```java
 DataOutputStream out = new DataOutputStream(new FileOutputStream(HanLP.Config.CharTypePath));
```

"
www.hanlp.comÁôª‰∏ç‰∏äÂéª‰∫ÜÔºüÊúâ‰∫∫ËÉΩÁúã‰∏Ä‰∏ãÂêóÔºü,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [ ] I've completed this form and searched the web for solutions.
"
‰∏ãËΩΩDataÂ§±Ë¥•,"---
name: üêõBug report
about: Create a report to help us improve
title: 'ImportÊó∂‰∏çËÉΩ‰∏ãËΩΩdata'
labels: bug
assignees: hankcs

---
**Describe the bug**
ÂΩìÊàë‰ΩøÁî®jupyter notebookËøêË°åimport pyhanlp‰ºö‰∏ãËΩΩdata-1.7.zipÔºå‰∏ÄÁõ¥‰ºöÂá∫Áé∞timeoutÔºåËÄåÂΩìÊàëÊâãÂ∑•‰∏ãËΩΩdata-1.7.zipÂπ∂Â∞ÜÂ§çÂà∂Âà∞/opt/anaconda3/lib/python3.7/site-packages/pyhanlp/static/ÔºåÂπ∂Â∞ÜÂÖ∂ÈáçÂëΩÂêç‰∏∫data-1.7.7.zipÊó∂Ôºå‰∏ãÊ¨°ËøêË°åimport pyhanlpËøòÊòØ‰ºöËá™Âä®‰∏ãËΩΩËØ•Êñá‰ª∂„ÄÇ

ËÄåÂΩìÊàë‰ΩøÁî®IPythonÂëΩ‰ª§Ë°åËøêË°åImport pyhanlpÊó∂ÔºåÂèà‰ºöËá™Âä®‰∏ãËΩΩdata-1.7.5.zipÊñá‰ª∂ÔºåËôΩÁÑ∂Êñá‰ª∂Â§π‰πüÂ∑≤ÁªèÊúâËØ•Êñá‰ª∂ÔºàÊàëÊâãÂ∑•‰∏ãËΩΩÂêéÊîæÂÖ•Ôºâ„ÄÇËØ∑ÈóÆÊòØ‰ΩïÂõû‰∫ãÂë¢Ôºü

**Code to reproduce the issue**

```import pyhanlp
```

**Describe the current behavior**
‰∏çËÉΩÂÆåÊàê‰∏ãËΩΩpyhanlpÊâÄÈúÄÁöÑdataÊñá‰ª∂Ôºå‰πüÊó†Ê≥ïËØÜÂà´ÊâãÂ∑•‰∏ãËΩΩÁöÑdataÊñá‰ª∂„ÄÇ

**Expected behavior**
ÂÆåÊàê‰∏ãËΩΩ

**System information**
- MacOS 10.15
- Anaconda python3.7
- HanLP version:latest

**Other info / logs**
screenshot included

* [ ] I've completed this form and searched the web for solutions.


![hanlp](https://user-images.githubusercontent.com/62363087/82623297-6705d000-9c12-11ea-90d6-515ecd1a012b.png)
![hanlp2](https://user-images.githubusercontent.com/62363087/82623310-708f3800-9c12-11ea-87af-c22cc5546346.png)

"
ËÉΩÂê¶Â¢ûÂä†Â§öËØ≠Ë®ÄÊîØÊåÅÔºüÊØîÂ¶ÇÈü©Êñá Êó•Êñá ‰∏≠Êñá,"<!--
Thank you for suggesting an idea to make HanLP better.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the feature and the current behavior/state.**
ÊîØÊåÅÂ§öËØ≠Ë®ÄÂàÜËØç,ÊàñÂàÜËØçÂ§öËØ≠Ë®ÄÂåñ
**Will this change the current api? How?**
ÂèØËÉΩÈúÄË¶ÅÊõ¥Êñ∞
**Who will benefit with this feature?**
ÊúâÂõΩÈôÖ‰∏öÂä°ÁöÑÊàñË∑®ÂõΩ,ÊÉ≥ÂØπ‰∫éÊó•Êñ∞ÊúàÂºÇÁöÑ‰∏≠ÂõΩ ÂæàÊòØÈúÄË¶Å
**Are you willing to contribute it (Yes/No):**
yes
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Any other info**

* [x] I've carefully completed this form.
"
ÂÆòÁΩëË¢´ÈòªÊñ≠‰∫Ü,ÂÆòÁΩëË¢´ÈòªÊñ≠‰∫ÜÔºåÂéªÂ§áÊ°à‰øÆÂ§ç‰∏ãÂêß„ÄÇ
semantic parserÁªìÊûúÊó†Ê≥ï‰ΩøÁî®pickleËøõË°åÊåÅ‰πÖÂåñ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
Èâ¥‰∫éÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÈÄüÂ∫¶ÈóÆÈ¢ò‰ª•ÂèäÂ∑≤Áü•ÁöÑtf2ÂÜÖÂ≠òÊ≥ÑÈú≤ÈóÆÈ¢òÔºå ‰∏Ä‰∏™Ëß£ÂÜ≥ÂäûÊ≥ïÊòØ‰ΩøÁî®Á¶ªÁ∫ø‰ªªÂä°Â§ÑÁêÜÊï∞ÊçÆÔºå ÁÑ∂ÂêéÊåÅ‰πÖÂåñÊï∞ÊçÆÂêéÂÜçÊé•ÂÖ•‰∏ãÊ∏∏‰ªªÂä°„ÄÇ ÁÑ∂ËÄåÔºå Âú®ÊµãËØïÊåÅ‰πÖÂåñËøáÁ®ã‰∏≠Ôºå ÂèëÁé∞‰ºº‰πéÁî±‰∫é`SerializableDict`Á±ªÊ≤°Êúâimplement `__getstate__` method, ÂØºËá¥semantic analysis ÁªìÊûúÊó†Ê≥ïË¢´pickleÊåÅ‰πÖÂåñ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ""-1""
import hanlp
import pickle
import traceback
tokenizer = hanlp.load(hanlp.pretrained.cws.PKU_NAME_MERGED_SIX_MONTHS_CONVSEG)
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL16_NEWS_BIAFFINE_ZH)
text = """"""HanLPÊòØ‰∏ÄÁ≥ªÂàóÊ®°Âûã‰∏éÁÆóÊ≥ïÁªÑÊàêÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂåÖÔºåÁõÆÊ†áÊòØÊôÆÂèäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
HanLPÂÖ∑Â§áÂäüËÉΩÂÆåÂñÑ„ÄÅÊÄßËÉΩÈ´òÊïà„ÄÅÊû∂ÊûÑÊ∏ÖÊô∞„ÄÅËØ≠ÊñôÊó∂Êñ∞„ÄÅÂèØËá™ÂÆö‰πâÁöÑÁâπÁÇπ„ÄÇÂÜÖÈÉ®ÁÆóÊ≥ïÁªèËøáÂ∑•‰∏öÁïåÂíåÂ≠¶ÊúØÁïåËÄÉÈ™åÔºåÈÖçÂ•ó‰π¶Á±ç„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÂ∑≤ÁªèÂá∫Áâà„ÄÇ""""""

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='part_of_speech_tags') \
    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')
res = pipeline(text)

x = res['semantic_dependencies'][0]

print(""direct dumping result...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(res, f)
except Exception as e:
    traceback.print_exc()

print(""dumping dict(result) with semantic...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(dict(res), f)
    print(""dump succeeded"")
except Exception as e:
    traceback.print_exc()

print(""dumping semantic result...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(x[0], f)
except Exception as e:
    traceback.print_exc()

print(""direct dumping result without semantic...."")
del res['semantic_dependencies']
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(res, f)
except Exception as e:
    traceback.print_exc()

print(""dumping dict(result) without semantic...."")
try:
    with open('tests/test_hanlp.pk', 'wb') as f:
        pickle.dump(dict(res), f)
    print(""dump succeeded"")
except Exception as e:
    traceback.print_exc()
```

**Describe the current behavior**
5‰∏™ÂÆûÈ™åÔºö
- Áõ¥Êé•pickle, Â§±Ë¥•
- Áªôpipeline result ÂâçÈù¢Â∞ÅË£Ö‰∏ÄÂ±ÇdictÔºå Â§±Ë¥•
- Âè™pickle semantic parser resultÔºå Â§±Ë¥•
- Âà†ÊéâresultÁöÑsemantic parserÈÉ®ÂàÜÔºå Áõ¥Êé•dumpÔºå Â§±Ë¥•
- ÁªôÂà†Êéâsemantic parserÈÉ®ÂàÜÁöÑresultÂ∞ÅË£Ö‰∏ÄÂ±ÇdictÔºå ÊàêÂäü


**Expected behavior**
Áªôresult Â∞ÅË£Ö‰∏ÄÂ±ÇdictÂêéÂ∫îËØ•ÈÉΩÂèØ‰ª•Áî®pickleËøõË°åÂ∫èÂàóÂåñÂπ∂dump?


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7
- HanLP version: 2.0.0-alpha.43


**Other info / logs**
```
direct dumping result....
Traceback (most recent call last):
  File ""<input>"", line 6, in <module>
TypeError: 'list' object is not callable
dumping dict(result) with semantic....
Traceback (most recent call last):
  File ""<input>"", line 13, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf2_gpu\lib\site-packages\hanlp\common\structure.py"", line 92, in __getattr__
    return self.__getitem__(key)
KeyError: '__getstate__'
dumping semantic result....
Traceback (most recent call last):
  File ""<input>"", line 21, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf2_gpu\lib\site-packages\hanlp\common\structure.py"", line 92, in __getattr__
    return self.__getitem__(key)
KeyError: '__getstate__'
direct dumping result without semantic....
Traceback (most recent call last):
  File ""<input>"", line 29, in <module>
TypeError: 'list' object is not callable
dumping dict(result) without semantic....
dump succeeded
```

* [x] I've completed this form and searched the web for solutions.
"
hanlp-2.0.0a43Âä†ËΩΩnerÊ®°ÂûãÊä•ÈîôÔºåhanlp‰∏éÊ®°ÂûãÁâàÊú¨‰∏çÂåπÈÖç,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
hanlpÁâàÊú¨ÂíånerÊ®°ÂûãÁâàÊú¨‰∏çÂåπÈÖç


**Code to reproduce the issue**
hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform: Linux Ubuntu 16.04
- Python version: Python 3.6
- HanLP version:hanlp-2.0.0a43

**Other info / logs**
ÈîôËØØÊèêÁ§∫Ôºö
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.43. Try to upgrade hanlp with
pip install --upgrade hanlp
‰ΩÜÊòØÂÆâË£Ö‰∫Ühanlp-2.0.0-alpha.5ÁâàÊú¨ÁöÑÊèêÁ§∫
Failed to download https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20191230_205748.zip due to timeout('The read operation timed out',). Please download it to /home/ubuntu/.hanlp/ner/ner_bert_base_msra_20191230_205748.zip by yourself. Or consider upgrading pip install -U hanlp

"
ÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÊúâ‰∏Ä‰∫õÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->
- Java Code: `String simplified=HanLP.convertToSimplifiedChinese(tradition);`
- HanLP version: 1.7.7

ÊØîÂ¶Ç‚ÄúÈô∑Èò±‚Äù Ë¢´ ËΩ¨Êç¢Êàê ‚ÄúÁå´ËÖª‚Äù
‚ÄúÁåõÁÉà‚ÄùË¢´ËΩ¨Êç¢Êàê‚ÄúÂãêÁÉà""
‚ÄùÈ°∫Âè£Ê∫ú‚ÄúË¢´ËΩ¨Êç¢Êàê‚ÄùÈ°∫Âè£ÁÜò""
""ËÑäÊ¢Å""Ë¢´ËΩ¨Êç¢Êàê‚ÄúÂµ¥Ê¢Å‚Äù
‚ÄúÈÄöÈÅì‚ÄùË¢´ËΩ¨Êç¢Êàê‚Äú‰ø°ÈÅì‚Äù
Ëøô‰∫õËΩ¨Êç¢ÈÉΩÊ≤°ÊúâÂøÖË¶ÅÔºåËΩ¨Êç¢ÂâçÂêéÂπ∂‰∏çÊòØÁÆÄ‰Ωì‰∏éÁπÅ‰ΩìÁöÑÂÖ≥Á≥ª„ÄÇ"
limie_evalÂèÇÊï∞ÂÄºÂ§™Â∞èÊó∂ÔºåÊä•Á©∫ÊåáÈíà,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
limie_evalÂèÇÊï∞ÂÄºÂ§™Â∞èÊó∂ÔºåÊä•Á©∫ÊåáÈíà.
‰∏çÂêåÁöÑÊµãËØïÂ≠óÁ¨¶‰∏≤ÊÉÖÂÜµ‰∏ãÔºåÊúâÁöÑÊòØÂú®0.2ÁöÑÊó∂ÂÄôÊä•Ëøô‰∏™ÈîôÔºåÊúâÁöÑ0.1ÁöÑÊó∂ÂÄôÊä•Ëøô‰∏™Èîô
ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:242)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```scala
object HanLPTest {
  def main(args: Array[String]): Unit = {
    val analyzer: ClusterAnalyzer[String] = new ClusterAnalyzer[String]()
    val strs: List[(String, String)] = List[(String, String)](
      (""1"",""‰∏≠ÂõΩËæΩÂÆÅÊ≤àÈò≥Â§ß‰∏úÂå∫ÊúõËä±ÂåóË°ó„ÄêÊúõËä±‰∏≠Ë°ó111-8Âè∑‰∏áÂüé6Âè∑Ê•º2ÂçïÂÖÉ20-4Âè∑„Äë (Èë´Ê∑ºÊµ∑È≤úÈ¶ÜÈôÑËøë)""),
      (""2"",""‰∏≠ÂõΩËæΩÂÆÅÊ≤àÈò≥Â§ß‰∏úÂå∫„Äê‰∏áÂüé6Âè∑Ê•º2ÂçïÂÖÉ20-4Âè∑„Äë""),
      (""3"",""ËæΩÂÆÅÊ≤àÈò≥Â∏ÇÂ§ß‰∏úÂå∫‰∏âÁéØÂÜÖÊúõËä±‰∏≠Ë°ó111-8Âè∑‰∏áÂüé6Âè∑Ê•º2ÂçïÂÖÉ20-4Âè∑""))
    for ((id, str) <- strs) {
      analyzer.addDocument(id, str)
    }
    println(analyzer.repeatedBisection(0.01))
  }
}
```

**Describe the current behavior**
A clear and concise description of what happened.

Ë∞ÉÊï¥Âà∞ËæÉÈ´òÁöÑÂèÇÊï∞Âêé‰∏çÊä•ÈîôÔºåÂ¶Ç0.3„ÄÅ0.5‰πãÁ±ªÁöÑÔºåÂ∞±‰∏çÊä•Èîô

**Expected behavior**
A clear and concise description of what you expected to happen.

ÊòØÂê¶ÂèØ‰ª•Â¢ûÂä†Á©∫ÊåáÈíàÈ™åËØÅ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10 ÈùûÂàÜÂ∏É
- Python version: scala2.11
- HanLP version: 1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:242)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:206)
	at HanLPTest$.main(HanLPTest.scala:31)
	at HanLPTest.main(HanLPTest.scala)

* [ ] I've completed this form and searched the web for solutions.
"
"pyhanlp Âíå Hanlp ÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥, ËØçÊÄß‰∏ç‰∏ÄËá¥","<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
pyhanlp Âíå java ÁâàÁöÑHanlpÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥
ËØçÊÄß‰πü‰∏ç‰∏ÄËá¥


**Code to reproduce the issue**

pyhanlp,  0.1.63
```python
from pyhanlp import *
print(HanLP.segment('‰Ω†ËØ¥ÁöÑËøôÂè•ËØùÊ≤°ÊúâÊ†πÊçÆ,Ê†πÊçÆ‰ª•ÂæÄÁöÑÁªèÈ™å,ÊàëÊòØÂØπÁöÑ.'))
[‰Ω†/rr, ËØ¥/v, ÁöÑ/ude1, Ëøô/rzv, Âè•ËØù/q, Ê≤°Êúâ/v, Ê†πÊçÆ/p, ,/w, Ê†πÊçÆ/p, ‰ª•ÂæÄ/t, ÁöÑ/ude1, ÁªèÈ™å/n, ,/w, Êàë/rr, ÊòØ/vshi, ÂØπ/p, ÁöÑ/ude1, ./w]
```
com.hankcs:hanlp:portable-1.7.5
```java
String text = ""‰Ω†ËØ¥ÁöÑËøôÂè•ËØùÊ≤°ÊúâÊ†πÊçÆ,Ê†πÊçÆ‰ª•ÂæÄÁöÑÁªèÈ™å,ÊàëÊòØÂØπÁöÑ."";
System.out.println(HanLP.segment(text));
[‰Ω†/r, ËØ¥/v, ÁöÑ/uj, Ëøô/r, Âè•ËØù/q, Ê≤°/d, Êúâ/v, Ê†πÊçÆ/p, ,/w, Ê†πÊçÆ/p, ‰ª•ÂæÄ/t, ÁöÑ/uj, ÁªèÈ™å/n, ,/w, Êàë/r, ÊòØ/v, ÂØπ/p, ÁöÑ/uj, ./w]
```

**Describe the current behavior**
pyhanlp ÁöÑÂàÜËØç Ê≤°Êúâ/v,  ËØçÊÄß: ‰Ω†/rr
Hanlp ÂàÜËØç: Ê≤°/d, Êúâ/v,  ËØçÊÄß: ‰Ω†/r

**Expected behavior**
Ë°®Áé∞‰∏ÄËá¥

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.62
- HanLP version: portable-1.7.5

**Other info / logs**
None

* [x] I've completed this form and searched the web for solutions.
"
HanLP 2.0 ‰∏≠ÁöÑ pipeline ‰∏≠Â¶Ç‰ΩïÂú®ÊúÄÂêé‰∏ÄÊ≠•ËøõË°åÊ†πÊçÆËá™ÂÆö‰πâËØçÂÖ∏ÂØπÊ®°ÂûãÂàÜËØçÁªìÊûúÂêàÂπ∂,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
ÊÉ≥Âú® pipeline ‰∏≠‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏Âêé‰∫éÊ®°ÂûãÁîüÊïàÔºåÁî®‰∫éÂØπÊ®°ÂûãÂàÜËØçÁªìÊûúÁöÑÂêéÂêàÂπ∂ÔºåÂ¶Ç HanLP 1.x ÁâàÊú¨ÈÇ£Ê†∑
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from hanlp.common.trie import Trie

import hanlp

tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
text = 'ÊàëÂõΩÊñ∞ËÉΩÊ∫êÊ±ΩËΩ¶‰∫ßÈáèÁ™ÅÁ†¥2‰∫øÂè∞‰∫ßÈáèÔºåÊòØ‰∏çÊòØËÅ™Êòé‰∫∫ÔºüNLPÁªüËÆ°Ê®°ÂûãÊ≤°ÊúâÂä†ËßÑÂàôÔºåËÅ™Êòé‰∫∫Áü•ÈÅìËá™Â∑±Âä†„ÄÇËã±Êñá„ÄÅÊï∞Â≠ó„ÄÅËá™ÂÆö‰πâËØçÂÖ∏ÁªüÁªüÈÉΩÊòØËßÑÂàô„ÄÇ'
print(tokenizer(text))

trie = Trie()
trie.update({'Ëá™ÂÆö‰πâ': 'custom', 'ËØçÂÖ∏': 'dict', 'ËÅ™Êòé‰∫∫': 'smart', 'ÂõΩÊñ∞ËÉΩÊ∫ê': 'stock'})


def split_sents(text: str, trie: Trie):
    words = trie.parse_longest(text)
    sents = []
    pre_start = 0
    offsets = []
    for word, value, start, end in words:
        if pre_start != start:
            sents.append(text[pre_start: start])
            offsets.append(pre_start)
        pre_start = end
    if pre_start != len(text):
        sents.append(text[pre_start:])
        offsets.append(pre_start)
    return sents, offsets, words


print(split_sents(text, trie))


def merge_parts(parts, offsets, words):
    items = [(i, p) for (i, p) in zip(offsets, parts)]
    #items += [(start, [word]) for (word, value, start, end) in words]
    # In case you need the tag, use the following line instead
    items += [(start, [(word, value)]) for (word, value, start, end) in words]
    return [each for x in sorted(items) for each in x[1]]


tokenizer = hanlp.pipeline() \
    .append(split_sents, output_key=('parts', 'offsets', 'words'), trie=trie) \
    .append(tokenizer, input_key='parts', output_key='tokens') \
    .append(merge_parts, input_key=('tokens', 'offsets', 'words'), output_key='merged')

print(tokenizer(text))
```

**Describe the current behavior**
A clear and concise description of what happened.
""merged"": [
    ""Êàë"",
    [""ÂõΩÊñ∞ËÉΩÊ∫ê"", ""stock""],
    ""Ê±ΩËΩ¶"",
    ""‰∫ßÈáè"",
    ""Á™ÅÁ†¥"",
    ""2‰∫ø"",
    ""Âè∞"",
    ""‰∫ßÈáè"",
    ""Ôºå"",
    ""ÊòØ"",
    ""‰∏ç"",
    ""ÊòØ"",
    [""ËÅ™Êòé‰∫∫"", ""smart""],
**Expected behavior**
A clear and concise description of what you expected to happen.
‚ÄúÂõΩÊñ∞ËÉΩÊ∫ê‚Äù Ëøô‰∏™ËØçÂ∫îËØ•‰∏çÁîüÊïà
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Python version: 3.6
- HanLP version: 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
hanlp+jupyterÁöÑdockerÈïúÂÉè,"**Describe the feature and the current behavior/state.**
ÁõÆÂâçÂÆòÊñπÊñáÊ°£ÈáåÊ≤°ÊúâÊèê‰æõÊõ¥Âø´‰∏äÊâãHanLP ÁöÑÊñπÂºèÔºåÊâÄ‰ª•ÊàëÂÅö‰∫Ü‰∏Ä‰∏™HanLP + Jupyter ÁöÑDockerÈïúÂÉèÔºåÂèØ‰ª•Â∏ÆÂä©ÊÑüÂÖ¥Ë∂£ÁöÑ‰∫∫Êõ¥Âø´‰∏äÊâã‰ΩìÈ™å„ÄÇ
walterinsh/hanlp:2.0.0a41-jupyter

[https://github.com/WalterInSH/hanlp-jupyter-docker](https://github.com/WalterInSH/hanlp-jupyter-docker)

Â¶ÇÊûúÊª°Ë∂≥‰Ω†‰ª¨ÁöÑÊúüÊúõÔºåÂèØ‰ª•Âä†Âú®ÊñáÊ°£Èáå„ÄÇ

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
‰ºö‰ΩøÁî®dockerÔºåÊúüÊúõÂø´ÈÄüÂ∞ùËØïÁöÑ‰∫∫

**Are you willing to contribute it (Yes/No):**
yes

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian
- Python version: 3.6
- HanLP version: 2.0.0a41

**Any other info**

* [x] I've carefully completed this form.
"
NERÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
Âä†ËΩΩ NER Ê®°ÂûãÂ§±Ë¥•Ôºå‰ª•‰∏ãÊòØÊä•Èîô‰ø°ÊÅØÔºö

Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip. See stack trace below
Traceback (most recent call last):
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/hanlp/layers/transformers/loader.py"", line 115, in build_transformer
    l_bert = bert.BertModelLayer.from_params(bert_params, name='albert' if albert else ""bert"")
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/params/with_params.py"", line 67, in from_params
    instance = cls(*args, **kwargs)
  File ""/home/t-xif/anaconda3/envs/tf2/lib/python3.7/site-packages/params/with_params.py"", line 47, in __init__
    self._construct(*args, **other_args)
TypeError: _construct() got an unexpected keyword argument 'name'
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.40. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
```

**Describe the current behavior**
Âá†Â§©ÂâçË∞ÉÁî®Ê®°ÂûãÊòØÊ≠£Â∏∏ÁöÑÔºåÊ®°ÂûãÂíåË∞ÉÁî®ÊñπÂºèÈÉΩÊ≤°ÊúâÊîπÂèòÔºå‰ªäÂ§©Ë∞ÉÊ®°ÂûãÁ™ÅÁÑ∂Â§±Ë¥•‰∫ÜÔºåÊåâÁÖßÊèêÁ§∫Â∞Ü hanlp ÂçáÁ∫ßÂà∞ a5 ÁâàÊú¨‰πü‰ºöÊåÇ„ÄÇÊòØ‰∏çÊòØÂÜÖÈÉ® API Êîπ‰∫Ü‰ªÄ‰πàÂú∞ÊñπÂØºËá¥Ë∞ÉÁî®Â§±Ë¥•ÁöÑÔºü

**Expected behavior**
Ê®°ÂûãÂä†ËΩΩÊàêÂäü„ÄÇ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Python version: 3.7
- HanLP version: 2.0.0-alpha.40. (ËØï‰∫Ü 2.0.0-alpha.5 ‰πü‰∏çË°å)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Âü∫‰∫ébertËá™ÂÆö‰πâÂàÜËØçËÆ≠ÁªÉÂá∫Èîô,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
tests/train/zh/cws/train_msr_cws_bert.py ËøêË°åÂá∫Èîô
TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ((tf.int32, tf.int32, tf.int32), tf.int32), but the yielded element was (['""', '‰∫∫', '‰ª¨', 'Â∏∏', 'ËØ¥', 'Áîü', 'Ê¥ª', 'ÊòØ', '‰∏Ä', 'ÈÉ®', 'Êïô', 'Áßë', '‰π¶', ',', 'ËÄå', 'Ë°Ä', '‰∏é', 'ÁÅ´', 'ÁöÑ', 'Êàò', '‰∫â', 'Êõ¥', 'ÊòØ', '‰∏ç', 'ÂèØ', 'Â§ö', 'Âæó', 'ÁöÑ', 'Êïô', 'Áßë', '‰π¶', ',', 'Â•π', 'Á°Æ', 'ÂÆû', 'ÊòØ', 'Âêç', 'ÂâØ', 'ÂÖ∂', 'ÂÆû', 'ÁöÑ', '‚Äò', 'Êàë', 'ÁöÑ', 'Â§ß', 'Â≠¶', '‚Äô', '„ÄÇ'], ['S', 'B', 'E', 'S', 'S', 'B', 'E', 'S', 'S', 'S', 'B', 'M', 'E', 'S', 'S', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S', 'B', 'M', 'M', 'E', 'S', 'B', 'M', 'E', 'S', 'S', 'B', 'E', 'S', 'B', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S']).
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```# -*- coding:utf-8 -*-
# Author: hankcs
# Date: 2019-12-21 15:39
from hanlp.components.tok import TransformerTokenizer
from hanlp.datasets.cws.sighan2005.msr import SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, SIGHAN2005_MSR_TEST
from tests import cdroot
from bert.loader import bert_models_google
bert_models_google['bert-base-chinese']='/usr/cws/bert-base-chinese'
cdroot()
tokenizer = TransformerTokenizer()
save_dir = 'data/model/cws_bert_base_msra'
tokenizer.fit(SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_VALID, save_dir, transformer='bert-base-chinese',
              metrics='f1')
# tagger.load(save_dir)
print(tokenizer.predict(['‰∏≠Â§ÆÊ∞ëÊóè‰πêÂõ¢Á¶ªÂºÄÂåó‰∫¨ÂâçÂæÄÁª¥‰πüÁ∫≥', 'ÂïÜÂìÅÂíåÊúçÂä°']))
tokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)
print(f'Model saved in {save_dir}')

```

**Describe the current behavior**
Ê®°ÂûãËÉΩÂ§üÂä†ËΩΩÁöÑÔºå‰ΩÜÊòØdatasetÈáåÈÉΩÊòØ‰∏≠ÊñáÔºåÂ∫îËØ•ÊòØ‰∏≠ÊñáÁöÑËØçËΩ¨ÊàêinputidsÊúâÈîôËØØÔºåÊâÄ‰ª•Êä•‰∫Ü`generator` yielded an element that did not match the expected structure. ÂÆö‰ΩçÂà∞ÁöÑÊòØhanlp/common/transform.py ÈáåÁöÑsamples = self.inputs_to_samples(inputs, gold)Ôºåinputs_to_samples Ê≤°ÊúâË∞ÉÁî®Á±ª‰ººconvert_examples_to_featuresËøôÁßçÊñπÊ≥ïÔºåÊâÄ‰ª•ÂÖ®ÊòØ‰∏≠Êñá„ÄÇ
ÊàëÊòØÈÄöËøápip install hanlp ÂÆâË£ÖÁöÑ
**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: 3.6.8
- HanLP version: hanlp-2.0.0a39

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
"was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38.","Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip. See stack trace below
Traceback (most recent call last):
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 70, in get_tensor
    self, compat.as_bytes(tensor_str))
RuntimeError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\components\taggers\transformers\transformer_tagger.py"", line 36, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\layers\transformers\loader.py"", line 141, in build_transformer
    skipped_weight_value_tuples = bert.load_bert_weights(l_bert, ckpt)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\bert_for_tf2-0.12.7-py3.6.egg\bert\loader.py"", line 216, in load_stock_weights
    ckpt_value = ckpt_reader.get_tensor(stock_name)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 74, in get_tensor
    error_translator(e)
  File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 48, in error_translator
    raise errors_impl.OpError(None, None, error_message, errors_impl.UNKNOWN)
tensorflow.python.framework.errors_impl.OpError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
* [ ] I've completed this form and searched the web for solutions.
"
windows10ÂÆâË£ÖÂêéÂØºÂÖ•Â§±Ë¥•,"Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
Traceback (most recent call last):
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó„ÄÇ

During handling of the above exception, another exception occurred:

<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version:
- HanLP version:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂä†ËΩΩÊ®°ÂûãÂá∫Èîô,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
‰ªé`hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH`Âä†ËΩΩ‰æùÂ≠òÂàÜÊûêÊ®°ÂûãÂá∫Èîô

**Code to reproduce the issue**
```python
import hanlp
hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
```

**Describe the current behavior**
‰ªé`hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH`Âä†ËΩΩ‰æùÂ≠òÂàÜÊûêÊ®°ÂûãÂá∫Èîô

**Expected behavior**
ÊàêÂäüÂä†ËΩΩ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Python version: Linux xxx 4.15.0-72-generic #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
- HanLP version: 2.0.0a39

**Other info / logs**
```bash
Failed to load https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip. See stack trace below
Traceback (most recent call last):
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/common/component.py"", line 269, in build
    self.model(sample_inputs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/hanlp/components/parsers/biaffine/model.py"", line 92, in call
    x = self.lstm(embed, mask=mask)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 281, in call
    outputs = layer(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 543, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 657, in call
    initial_state=forward_state, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 644, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1144, in call
    **cudnn_lstm_kwargs)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1362, in cudnn_lstm
    time_major=time_major)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py"", line 1906, in cudnn_rnnv3
    ctx=_ctx)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py"", line 2002, in cudnn_rnnv3_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)
  File ""/data/humeng/projs/extract_spo/.venv/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: Fail to find the dnn implementation. [Op:CudnnRNNV3]
https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.39. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
```

* [x] I've completed this form and searched the web for solutions.
"
Pyhanlp‰∏éjava HanlpÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
ÊàëÁî®Python‰∏ãËΩΩ‰∫Üpyhanlp, version update Âà∞‰∫Ü1.7.7ÔºåÂêåÊ†∑ÔºåÂú®java maven projectÊàëÁî®‰∫Ühanlp-portable-1.7.7, ‰ΩÜÊòØÊàëÂèëÁé∞ÂàÜËØçÁªìÊûúÁ´üÁÑ∂‰∏ç‰∏ÄËá¥„ÄÇ
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
test case: ‚ÄúÂÜÖÊ≤≥ËøêËæì‚Äù
```python
```
Hanlp.segment(‚ÄúÂÜÖÊ≤≥ËøêËæì‚Äù)

**Describe the current behavior**
A clear and concise description of what happened.
pyhanlp‰∏≠Hanlp.segmentÁªìÊûúÔºö‚ÄúÂÜÖÊ≤≥ËøêËæì‚Äù
Java‰∏≠Hanlp.segmentÁªìÊûúÔºö‚ÄúÂÜÖÊ≤≥ ËøêËæì‚Äù
**Expected behavior**
A clear and concise description of what you expected to happen.
pyhanlp‰∏éjava‰∏≠Hanlp.segmentÁªìÊûúÂ∫îËØ•‰∏ÄËá¥ÊâçÂØπ„ÄÇ
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.4
- Python version:3.6.2
- HanLP version:1.7.7

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
non
* [x] I've completed this form and searched the web for solutions.
"
Typo Fix,"### Ê≥®ÊÑè‰∫ãÈ°π

- ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
- ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
- ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
- ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
- [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

### Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

fixÊó•ÂøóÊâìÂç∞ÈîôËØØÈóÆÈ¢ò"
Typo fix,Typo fix
pipelineÊ∑ªÂä†Ëá™ÂÆö‰πâfunctionÂêéÔºå Âæ™ÁéØ‰ΩøÁî®ÂÜÖÂ≠òÊ∫¢Âá∫,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
ÊàëÈúÄË¶ÅÂú®tokenizerÂíåtaggerÂêéÔºå Áõ¥Êé•ÂèñÂà∞‰∏Ä‰∏™""token/tag""ÊãºÊé•ÁöÑËØç„ÄÇÊïÖÊ≠§Ê∑ªÂä†‰∫Ü‰∏Ä‰∏™`hanlp_get_tokens`ÁöÑfunction appendÂú®piplineÈáå„ÄÇ ËØ¶ÊÉÖËßÅ‰∏ãÊñπ‰ª£Á†Å„ÄÇ

ÂÆûÈôÖ‰ªªÂä°ÈúÄË¶ÅÈ¢ÑÊµã‰∏Ä‰∏áÁØáÊñáÁ´†ÁöÑÂàÜËØçÂíåtagÔºõ ÁÑ∂ËÄåÂú®gpuÁéØÂ¢É‰∏ãË∑ëÂ§ßÊ¶Ç1/10ÁöÑloopÊó∂oopÔºõ
disable‰∫ÜGPUÔºå Áî®`tracemalloc`Êü•ÁúãÂèëÁé∞`transform\txt.py`ÂÜÖÂ≠òÊ∫¢Âá∫„ÄÇ


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import pandas as pd
from tqdm import tqdm
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
import hanlp
import tensorflow as tf
# Set CPU as available physical device
# tf.config.set_visible_devices([], 'GPU')
tf.config.list_physical_devices()
tokenizer = hanlp.load('PKU_NAME_MERGED_SIX_MONTHS_CONVSEG')
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
def hanlp_get_tokens(tokens, tags, target_tag_list=[]):
    res = []
    for tokens, post in zip(tokens, tags):
        for i, j in zip(tokens, post):
            if i.strip():
                if not target_tag_list:
                    res.append(i + '/' + j)
                else:
                    if j in target_tag_list:
                        res.append(i + '/' + j)
    return res

pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(tokenizer, output_key='tokens') \
    .append(tagger, output_key='tags') \
    .append(hanlp_get_tokens, input_key=('tokens', 'tags'), output_key='target_token')

import tracemalloc
import linecache

def display_top(snapshot, key_type='lineno', limit=3):
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))

# Ëé∑ÂèñÊï∞ÊçÆ
text = ""‰ªäÂ§©ÊàëÂõûÂÆ∂‰π∞‰∫Ü‰∏™Â§ßË•øÁìú;""*100
# data_df = data_df.loc[data_df['doc_id'] >= 5150]
tracemalloc.start()
for index, i in enumerate(tqdm([text]*20000)):
    tokenized = {}
    # title_counters = Counter(title_token)
    res = pipeline(i)['target_token']
    # content_counters = Counter(content_token)
    snapshot = tracemalloc.take_snapshot()
    display_top(snapshot)
```

**Describe the current behavior**
ÂèØ‰ª•ÁúãÂà∞ÁöÑÊòØÔºå ÈöèÁùÄiterationÊï∞ÁõÆÂ¢ûÂä†Ôºå `word += c`ËøôÈáåÁöÑÂÜÖÂ≠ò‰πü‰∏çÊñ≠Â¢ûÂä†„ÄÇ„ÄÇ„ÄÇ‰ΩÜÊòØÊàëÁøªÊù•Ë¶ÜÂéªÁúãÈÇ£ÊÆµ‰ª£Á†ÅÂÆåÂÖ®Ê≤°ÁúãÂá∫ÈóÆÈ¢òÔºå ÊïÖÊ≠§Âè™ËÉΩÊèêÂá∫issueÔºå ÊúüÂæÖ‰ΩïËÄÅÂ∏àÁúãÁúã„ÄÇÂÖ∑‰ΩìlogÂ∑≤ÁªèÈôÑ‰∏ä„ÄÇ

**Expected behavior**
loop prediction‰∏ç‰ºöÂ¢ûÂä†ÂÜÖÂ≠ò‰ΩøÁî®

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Python version: 3.7
- HanLP version: 2.0.0-alpha.39

**Other info / logs**
[log gist](https://gist.github.com/luoy2/ee8256e6f90289e068695dcf661a14a6)


* [x] I've completed this form and searched the web for solutions.
"
batch‰∏∫32Êó∂Êä•ÈîôÔºåÂÖ∂‰ªñÊï∞Â≠ó‰∏çÊä•Èîô,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
Áî®ÁöÑÊòØpython,cpuË∑ëÁöÑ‰ª£Á†Å,Áî®hanlp.pipeline()ËøõË°åÂàÜÊûêÔºåËæìÂÖ•batch‰∏∫32Êó∂Êä•ÈîôÔºåÂÖ∂‰ªñÊï∞Â≠ó‰∏çÊä•Èîô

‰∏ãÈù¢ÊòØÊä•Âá∫ÁöÑÈîôËØØ‰ø°ÊÅØÔºö

UnknownError                              Traceback (most recent call last)
<ipython-input-88-3f7e6c8a5769> in <module>
      9 for i in tqdm(rand_loader):
     10     print(i)
---> 11     doc = pipeline(i)
     12     #print(doc)
     13 end=time.time()

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\pipeline.py in __call__(self, doc, **kwargs)
     95     def __call__(self, doc: Document, **kwargs) -> Document:
     96         for component in self:
---> 97             doc = component(doc)
     98         return doc
     99 

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in __call__(self, data, **kwargs)
     49 
     50     def __call__(self, data, **kwargs):
---> 51         return self.predict(data, **kwargs)
     52 
     53     @staticmethod

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\pipeline.py in predict(self, doc, **kwargs)
     48         if unpack:
     49             kwargs['_hanlp_unpack'] = True
---> 50         output = self.component(input, **kwargs)
     51         if isinstance(output, types.GeneratorType):
     52             output = list(output)

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in __call__(self, data, **kwargs)
     49 
     50     def __call__(self, data, **kwargs):
---> 51         return self.predict(data, **kwargs)
     52 
     53     @staticmethod

D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\common\component.py in predict(self, data, batch_size, **kwargs)
    453         num_samples = 0
    454         data_is_list = isinstance(data, list)
--> 455         for idx, batch in enumerate(dataset):
    456             samples_in_batch = tf.shape(batch[-1] if isinstance(batch[-1], tf.Tensor) else batch[-1][0])[0]
    457             if data_is_list:

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in __next__(self)
    628 
    629   def __next__(self):  # For Python 3 compatibility
--> 630     return self.next()
    631 
    632   def _next_internal(self):

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in next(self)
    672     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    673     try:
--> 674       return self._next_internal()
    675     except errors.OutOfRangeError:
    676       raise StopIteration

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in _next_internal(self)
    663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:
--> 665         return structure.from_compatible_tensor_list(self._element_spec, ret)
    666 
    667   @property

D:\Anaconda3\envs\pytorch\lib\contextlib.py in __exit__(self, type, value, traceback)
    128                 value = type()
    129             try:
--> 130                 self.gen.throw(type, value, traceback)
    131             except StopIteration as exc:
    132                 # Suppress StopIteration *unless* it's the same exception that

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\eager\context.py in execution_mode(mode)
   1898   finally:
   1899     ctx.executor = executor_old
-> 1900     executor_new.wait()
   1901 
   1902 

D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\eager\executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

UnknownError: AssertionError: unable to assign 22 datapoints to 32 clusters
Traceback (most recent call last):

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\ops\script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\parsers\conll.py"", line 239, in generator
    buckets = dict(zip(*kmeans(lengths, n_buckets)))

  File ""D:\Anaconda3\envs\pytorch\lib\site-packages\hanlp\components\parsers\alg.py"", line 52, in kmeans
    assert len(d) >= k, f""unable to assign {len(d)} datapoints to {k} clusters""

AssertionError: unable to assign 22 datapoints to 32 clusters


	 [[{{node PyFunc}}]]


"
2.0.0a38ÁöÑÁ±ªÂ•ΩÂÉèÊúâ‰∏™ÊñπÊ≥ï‰∏çÂØπ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**

ÊàëÂú®Áúãhanlp\common\structure.pyÈáåÈù¢ÁöÑSerializableÁ±ª‰ª£Á†Å,ÂèëÁé∞loadÊñπÊ≥ï‰∏≠Êó†ËÆ∫Êñá‰ª∂ÊòØ‰∏çÊòØjson,ÈÉΩ‰ΩøÁî®load_jsonÂéªËØªÂèñÊñá‰ª∂ ,
(Âú®saveÊó∂‰ºö‰øùÂ≠ò‰∏∫jsonÊàñpickleÊñá‰ª∂,)
loadÊó∂ÂØπ‰∫éjsonÂ§ñÁöÑÊ†ºÂºèÊåâÁêÜËØ¥Â∫îËØ•‰ΩøÁî®load_pickleËÄå‰∏çÊòØload_json
(Á±ªÈáåÈù¢ÊúâÂÆö‰πâload_pickleÂáΩÊï∞,‰ΩÜÊòØÊ≤°ÊúâÁî®Âà∞)

Áñë‰ººÊúâÈóÆÈ¢òÁöÑ‰ª£Á†Å,ËßÅ‰∏ãÈù¢ÁöÑÊ≥®Èáä

**Code to reproduce the issue**
‰∏ãÈù¢ÊòØSerializableÁ±ªÁöÑÈÉ®ÂàÜ‰ª£Á†Å:

```python
class Serializable(object):
    """"""
    A super class for save/load operations.
    """"""

    def save(self, path, fmt=None):
        if not fmt:
            if filename_is_json(path):
                self.save_json(path)
            else:
                self.save_pickle(path)
        elif fmt in ['json', 'jsonl']:
            self.save_json(path)
        else:
            self.save_pickle(path)

    def load(self, path, fmt=None):
        if not fmt:
            if filename_is_json(path):
                self.load_json(path)
            else:
                self.load_json(path)   ######Ëøô‰∏ÄË°åÊàëËßâÂæóÊúâÈóÆÈ¢ò,ÊÑüËßâÂ∫îËØ•ÊòØload_pickle
        elif fmt in ['json', 'jsonl']:
            self.load_json(path)
        else:
            self.load(path)
```

* [x] I've completed this form and searched the web for solutions.
"
support getting all tags,
Ë∞ÉÁî®CharTabelÔºåÊää‚ÄúÂπ∫‚ÄùÊîπ‰∏∫‚Äú‰πà‚Äù‰∏çÂêàÁêÜ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.

Ë∞ÉÁî®CharTabelÔºåÊää‚ÄúÂπ∫‚ÄùÊîπ‰∏∫‚Äú‰πà‚Äù‰∏çÂêàÁêÜÔºåËôΩÁÑ∂ ‰πà ‰πüÊúâ1ÁöÑÊÑèÊÄùÔºå‰∏î‰πüÊúâÂèëÈü≥‰∏∫yaoÁöÑÔºå‰ΩÜÊòØ‰πàÈÄöÂ∏∏‰∏ç‰ª£Ë°®Âπ∫ÁöÑÊÑèÊÄùÔºå‰∏îÂπ∫Â≠óÂ∑≤ÁªèÊòØÊ≠£ÂàôÂåñÁöÑÔºåÊ≤°ÊúâÂøÖË¶ÅËøõ‰∏ÄÊ≠•ÊîπÂèòÔºåÈúÄË¶ÅÂéªÊéâCharTable.txtÈáåÈù¢Âπ∫=‰πà

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
 System.out.println(CharTable.convert(""Âπ∫Â¶πÁöÑÊâãÊú∫Âè∑Á†ÅÊòØÂπ∫‰∏â‰∫åÂºÄÂ§¥ÁöÑ""));
```

**Describe the current behavior**
A clear and concise description of what happened.

‰πàÂ¶πÁöÑÊâãÊú∫Âè∑Á†ÅÊòØ‰πà‰∏â‰∫åÂºÄÂ§¥ÁöÑ

**Expected behavior**
A clear and concise description of what you expected to happen.

Âπ∫Â¶πÁöÑÊâãÊú∫Âè∑Á†ÅÊòØÂπ∫‰∏â‰∫åÂºÄÂ§¥ÁöÑ

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: java
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
1.x,ÂêåÊ≠•Êõ¥Êñ∞
AbstractClassifier‰∏≠ÁöÑenableProbabilityÊñπÊ≥ï‰ºº‰πéÊúâBUG,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
Áî®Êà∑Ë∞ÉÁî®enableProbability(boolean enable)ÂêéÔºå‰∏çÁÆ°‰º†ÂÖ•ÁöÑÂÄºÊòØtrueËøòÊòØfalseÔºåÈÉΩ‰∏ç‰ºöÂΩ±ÂìçÂà∞configProbabilityEnabledÁöÑÂÄºÔºå‰ªéËÄåÂØºËá¥ if (configProbabilityEnabled) MathUtility.normalizeExp(predictionScores); Ê∞∏ËøúÊâßË°å„ÄÇ

Âª∫ËÆÆÂ∞Ü
````java
    @Override
    public IClassifier enableProbability(boolean enable)
    {
        return this;
    }
````
‰øÆÊîπ‰∏∫
````java
    @Override
    public IClassifier enableProbability(boolean enable)
    {
        configProbabilityEnabled = enable;
        return this;
    }
````

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```java
    public Map<String, Double> test() throws IOException {
        IClassifier iClassifier = new NaiveBayesClassifier(trainOrLoadModel()).enableProbability(false);;
        return iClassifier.predict(""test java demo""); 
}
```

**Describe the current behavior**
enableProbabilityÊñπÊ≥ïÂ§±Êïà„ÄÇ

**Expected behavior**
enableProbabilityÊñπÊ≥ïÁîüÊïà„ÄÇ

**System information**
- Windows 0
- Java 1.8
- HanLP 1.7.5

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
ËØçÁΩë‰øùËØÅ‰ªéËµ∑ÁÇπÂá∫ÂèëÁöÑ‚ÄúÊâÄÊúâ‚ÄùË∑ØÂæÑÈÉΩ‰ºöËøûÈÄöÂà∞ÁªàÁÇπÁöÑÈóÆÈ¢ò,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
java 1.7.6ÁâàÊú¨ÔºåÊûÑÂª∫ËØçÁΩëÔºåÊó†Ê≥ï‰øùËØÅ‰ªéËµ∑ÁÇπÂá∫ÂèëÁöÑÊâÄÊúâË∑ØÂæÑÈÉΩ‰ºöËøûÈÄöÂà∞ÁªàÁÇπ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# -- coding: utf-8 --
from jpype import JString
from pyhanlp import *
from tests.book.ch03.msr import msr_model
from tests.test_utility import test_data_path


NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')
CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')
WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')
Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')
CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
Nature = JClass('com.hankcs.hanlp.corpus.tag.Nature')


def my_cws_corpus():
    data_root = test_data_path()
    corpus_path = os.path.join(data_root, 'my_cws_corpus1.txt')
    if not os.path.isfile(corpus_path):
        with open(corpus_path, 'w', encoding='utf-8') as out:
            out.write('''ÂïÜÂìÅ ÂíåÊúç Âä°
ÂïÜÂìÅ Âíå Ë¥ßÂ∏Å''')
    return corpus_path



def train_bigram(corpus_path, model_path):
    sents = CorpusLoader.convert2SentenceList(corpus_path)
    for sent in sents:
        for word in sent:
            if word.label is None:
                word.setLabel(""n"")
    maker = NatureDictionaryMaker()
    maker.compute(sents)
    maker.saveTxtTo(model_path)  # tests/data/my_cws_model.txt


def load_bigram(model_path, verbose=True):
    HanLP.Config.CoreDictionaryPath = model_path + "".txt""  # unigram
    HanLP.Config.BiGramDictionaryPath = model_path + "".ngram.txt""  # bigram
    # ‰ª•‰∏ãÈÉ®ÂàÜ‰∏∫ÂÖºÂÆπÊñ∞Ê†áÊ≥®ÈõÜÔºå‰∏çÊÑüÂÖ¥Ë∂£ÂèØ‰ª•Ë∑≥Ëøá
    HanLP.Config.CoreDictionaryTransformMatrixDictionaryPath = model_path + "".tr.txt""  # ËØçÊÄßËΩ¨ÁßªÁü©ÈòµÔºåÂàÜËØçÊó∂ÂèØÂøΩÁï•
    if model_path != msr_model:
        with open(HanLP.Config.CoreDictionaryTransformMatrixDictionaryPath) as src:
            for tag in src.readline().strip().split(',')[1:]:
                Nature.create(tag)
    CoreBiGramTableDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary')
    CoreDictionary.getTermFrequency(""ÂïÜÂìÅ"")
    # ÂÖºÂÆπ‰ª£Á†ÅÁªìÊùü
    if verbose:
        sent = 'ÂïÜÂìÅÂíåÊúçÂä°'
        wordnet = generate_wordnet(sent, CoreDictionary.trie)
        print(wordnet)
    return 0


def generate_wordnet(sent, trie):
    """"""
    ÁîüÊàêËØçÁΩë
    :param sent: Âè•Â≠ê
    :param trie: ËØçÂÖ∏ÔºàunigramÔºâ
    :return: ËØçÁΩë
    """"""
    searcher = trie.getSearcher(JString(sent), 0)
    wordnet = WordNet(sent)
    while searcher.next():
        wordnet.add(searcher.begin + 1,
                    Vertex(sent[searcher.begin:searcher.begin + searcher.length], searcher.value, searcher.index))
    # ÂéüÂ≠êÂàÜËØçÔºå‰øùËØÅÂõæËøûÈÄö
    vertexes = wordnet.getVertexes()
    i = 0
    while i < len(vertexes):
        if len(vertexes[i]) == 0:  # Á©∫ÁôΩË°å
            j = i + 1
            for j in range(i + 1, len(vertexes) - 1):  # ÂØªÊâæÁ¨¨‰∏Ä‰∏™ÈùûÁ©∫Ë°å j
                if len(vertexes[j]):
                    break
            wordnet.add(i, Vertex.newPunctuationInstance(sent[i - 1: j - 1]))  # Â°´ÂÖÖ[i, j)‰πãÈó¥ÁöÑÁ©∫ÁôΩË°å
            i = j
        else:
            # print(vertexes[i][-1].realWord)
            i += len(vertexes[i][-1].realWord)

    return wordnet



if __name__ == '__main__':
    corpus_path = my_cws_corpus()
    model_path = os.path.join(test_data_path(), 'my_cws_model1')
    train_bigram(corpus_path, model_path)
    load_bigram(model_path)


```

**Describe the current behavior**
A clear and concise description of what happened.
ÂÅáÂ¶ÇËØçÂÖ∏‰∏≠Êúâ[""ÂïÜÂìÅ‚ÄùÔºå‚ÄúÂíåÊúç‚ÄùÔºå‚ÄúÂíå‚ÄùÔºå‚ÄúÂä°‚Äù]Ëøô‰∫õËØçÔºåÈÇ£‰πà""ÂïÜÂìÅÂíåÊúçÂä°‚ÄúËøôÂè•ËØùÁöÑËØçÁΩëËæìÂá∫‰∏∫
0:[ ]
1:[ÂïÜÂìÅ]
2:[]
3:[Âíå, ÂíåÊúç]
4:[]
5:[Âä°]
6:[ ]
**Expected behavior**
A clear and concise description of what you expected to happen.
0:[ ]
1:[ÂïÜÂìÅ]
2:[]
3:[Âíå, ÂíåÊúç]
4:[Êúç]
5:[Âä°]
6:[ ]
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Python version: 3.7
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
HanLP.segmentÂÖ≥‰∫é‰∏é‚Äú‰∏Ä‚ÄùÊúâÂÖ≥ÁöÑÂàÜËØçÈîôËØØ,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
java 1.7.6ÁâàÊú¨ÔºåË∞ÉÁî®HanLP.segmentÂêéÔºå‚ÄúÂçÅ‰∏Ä‰ªãÁªç‚ÄùÔºåÂàÜËØç‰∏∫[ÂçÅ/m, ‰∏Ä‰ªã/nz, Áªç/nz]Ôºå‚ÄúÂçÅ‰∏Ä‰∏≠ÂõΩÊîæÂÅáÂêó‚ÄùÂàÜËØç‰∏∫[ÂçÅ/m, ‰∏Ä‰∏≠/j, ÂõΩ/n, ÊîæÂÅá/vi, Âêó/y]ÔºåÁ±ª‰ººËøôÁßç‰∏é‚Äú‰∏Ä‚ÄùÁõ∏ÂÖ≥ÁöÑÂàÜËØçÂ≠òÂú®ÈîôËØØ
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
HanLP.segment(""ÂçÅ‰∏Ä‰∏≠ÂõΩÊîæÂÅáÂêó"")
```

**Describe the current behavior**
A clear and concise description of what happened.
‚ÄúÂçÅ‰∏Ä‰ªãÁªç‚ÄùÔºåÂàÜËØç‰∏∫[ÂçÅ/m, ‰∏Ä‰ªã/nz, Áªç/nz]
**Expected behavior**
A clear and concise description of what you expected to happen.
[ÂçÅ‰∏Ä, ‰ªãÁªç]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- Python version: java
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ÊâÄ‰ª•ÂæàÂ§ö ""‰∏ÄA@*""ËøôÁßçÊ®°ÂºèÔºåÂÖ∂‰∏≠A‰ª£Ë°®‰∏Ä‰∏™Â≠óÔºå*‰ª£Ë°®‰ªªÊÑèÂ≠óÔºåÁÑ∂ÂêéÊàëÊää‰ª•AÂºÄÂ§¥ÁöÑËØçÂÖ®ÈÉ®ÊãøÂá∫Êù•ÔºåÂπ∂‰ª•È¢ëÁéáÊéíÂ∫èÔºåÂèëÁé∞ÊúâÂæàÂ§öËøôÊ†∑ÁöÑÈóÆÈ¢òÔºåÂéüÂõ†Â∫îËØ•ÊòØ  ‰∏Ä‰∏≠„ÄÅ‰∏ÄÂèë„ÄÅ‰∏ÄÈÄöÁ≠âËøõË°åËΩ¨Áßª‰∏çÂ§™ÂêàÈÄÇÔºå‰∏ä‰º†Êñá‰ª∂Ôºå‰∏çÁü•ÊúâÊó†ÂÖ¥Ë∂£Âº∫Âåñ
[hanLPÂÖ≥‰∫é‰∏ÄÁöÑÂàÜËØçÈîôËØØÊñá‰ª∂.txt](https://github.com/hankcs/HanLP/files/4196279/hanLP.txt)


‰∏≠ÂõΩ=39573  ‰æãÂ≠êÔºöÂçÅ‰∏Ä‰∏≠ÂõΩÊîæÂÅáÂêó
ÂèëÂ±ï=20444  ‰æãÂ≠êÔºöÂçÅ‰∏ÄÂèëÂ±ïËÆ°Âàí
Êó∂Èó¥=17679
ÈÄöËøá=16921  ‰æãÂ≠êÔºöÂçÅ‰∏ÄÈÄöËøáÂ±±Êµ∑ÂÖ≥Âêó
ÂèëÁé∞=16201  
ÁªèÊµé=14634
‰ºöËÆÆ=13344
ÂêåÊó∂=13137
ÁîüÊ¥ª=12721
ÂØπ‰∫é=10684
ÂèëÁîü=10588
‰∏≠Â§Æ=10431
‰ªãÁªç=9492
‰ª∑Ê†º=9121
‰∏ÄÁõ¥=8928
Ë°å‰∏∫=8549
‰∏≠ÂøÉ=8440
‰∏ÄËµ∑=8280
ÁéØÂ¢É=7994
Â§ßÂÆ∂=7830
ÊúüÈó¥=7734
‰∏ñÁïå=7554
‰ª£Ë°®=7412
Â§ßÂ≠¶=7294
ÂèëÂ∏É=7279
Êó∂ÂÄô=7149
È¢ÜÂØº=7055
ÊâãÊú∫=6656


* [x] I've completed this form and searched the web for solutions.
"
module 'hanlp' has no attribute 'load',"
**Describe the bug**
AttributeError: module 'hanlp' has no attribute 'load'

**System information**
- OS Platform and Distribution (e.g., Centos7):
- Python version:3.7
- HanLP version:2.0
"
loadÊï∞ÊçÆ‰∏≠ÁöÑerror,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter.  
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
ËøáÁ®ãÊòØËøôÊ†∑ÁöÑÔºö
ÈÄöËøáhanlp.pretrained.ALL  ÂèñÂæó‰∫ÜloadÂú®ÂèòÈáèÂêéËøõË°åÊâπÈáèloadÔºåÂèëÁé∞Âá∫ÈîôÂæàÂ§öÔºö


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
A clear and concise description of what happened.
hanlp.load('GLOVE_6B_ROOT')
hanlp.load('GLOVE_6B_50D')
hanlp.load('GLOVE_6B_100D')
hanlp.load('GLOVE_6B_200D')
hanlp.load('GLOVE_6B_300D')
hanlp.load('GLOVE_840B_300D')
hanlp.load('PTB_POS_RNN_FASTTEXT_EN')
hanlp.load('FLAIR_LM_FW_WMT11_EN')
hanlp.load('FLAIR_LM_BW_WMT11_EN')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_PKU')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_MSR')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_CHAR')
hanlp.load('SEMEVAL16_EMBEDDINGS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_NEWS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_TEXT_CN')
hanlp.load('CTB5_FASTTEXT_300_CN')
hanlp.load('TENCENT_AI_LAB_EMBEDDING')
hanlp.load('RADICAL_CHAR_EMBEDDING_100')

hanlp.load()   Êó∂ÂèëÁîüÈîôËØØ„ÄÇ


**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 prof  X64
- Python version:  Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32
- HanLP version: hanlp==2.0.0a36


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
ËØ¶ÁªÜÁöÑ‰ø°ÊÅØÂ¶Ç‰∏ãÔºö
******Á¨¨‰∏Ä:******
hanlp.load('GLOVE_6B_ROOT')
hanlp.load('GLOVE_6B_50D')
hanlp.load('GLOVE_6B_100D')
hanlp.load('GLOVE_6B_200D')
hanlp.load('GLOVE_6B_300D')
hanlp.load('GLOVE_840B_300D')
hanlp.load('PTB_POS_RNN_FASTTEXT_EN')
hanlp.load('FLAIR_LM_FW_WMT11_EN')
hanlp.load('FLAIR_LM_BW_WMT11_EN')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_PKU')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_WORD_MSR')
hanlp.load('CONVSEG_W2V_NEWS_TENSITE_CHAR')
hanlp.load('SEMEVAL16_EMBEDDINGS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_NEWS_CN')
hanlp.load('SEMEVAL16_EMBEDDINGS_300_TEXT_CN')
hanlp.load('CTB5_FASTTEXT_300_CN')
hanlp.load('TENCENT_AI_LAB_EMBEDDING')
hanlp.load('RADICAL_CHAR_EMBEDDING_100')

ËøôÂá†‰∏™‰∏ãËΩΩÊä•Á±ª‰ººËøôÊ†∑Âú®ÈîôËØØÔºö
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python36\lib\site-packages\hanlp\__init__.py"", line 51, in load
    return load_from_meta_file(save_dir, meta_filename, transform_only=transform_only, load_kwargs=load_kwargs, **kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 34, in load_from_meta_file
    raise FileNotFoundError(f'The identifier {save_dir} resolves to a non-exist meta file {metapath}. {tips}')
FileNotFoundError: The identifier C:\Users\Roottan\AppData\Roaming\hanlp\hanlp\lm\flair_lm_wmt11_en_20191229_033714\flair_lm_bw_wmt11_en resolves to a non-exist meta file C:\Users\****\AppData\Roaming\hanlp\hanlp\lm\flair_lm_wmt11_en_20191229_033714\flair_lm_bw_wmt11_en\meta.json.

Â∞±ÊòØÊä•Ôºömeta.json Êñá‰ª∂Êâæ‰∏çÂà∞„ÄÇ

******Á¨¨‰∫å:********
hanlp.load('MSRA_NER_BERT_BASE_ZH')
hanlp.load('MSRA_NER_ALBERT_BASE_ZH')
hanlp.load('CONLL03_NER_BERT_BASE_UNCASED_EN')
hanlp.load('CHNSENTICORP_BERT_BASE_ZH')
Êä•Á±ª‰ºº‰ª•‰∏ãÁöÑÈîôÔºö
Done loading 197 BERT weights from: C:\Users\****\AppData\Roaming\hanlp\thirdparty\storage.googleapis.com\bert_models\2018_11_03\chinese_L-12_H-768_A-12\bert_model.ckpt into <bert.model.BertModelLayer object at 0x0000020C43D25828> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]
Unused weights from checkpoint:
        bert/pooler/dense/bias
        bert/pooler/dense/kernel
        cls/predictions/output_bias
        cls/predictions/transform/LayerNorm/beta
        cls/predictions/transform/LayerNorm/gamma
        cls/predictions/transform/dense/bias
        cls/predictions/transform/dense/kernel
        cls/seq_relationship/output_bias
        cls/seq_relationship/output_weights

Â∞±ÊòØÊä•ÔºöUnused weights from checkpoint:


******Á¨¨‰∏â:********
hanlp.load('SST2_BERT_BASE_EN')
Failed to load https://file.hankcs.com/hanlp/classification/sst2_bert_base_uncased_en_20200104_175422.zip. See stack trace below
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 39, in load_from_meta_file
    obj: Component = object_from_class_path(cls, **kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\utils\reflection.py"", line 24, in object_from_class_path
    class_path = str_to_type(class_path)
  File ""C:\Python36\lib\site-packages\hanlp\utils\reflection.py"", line 37, in str_to_type
    cls = getattr(importlib.import_module(module_name), class_name)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'hanlp.components.classifiers.bert_text_classifier'
https://file.hankcs.com/hanlp/classification/sst2_bert_base_uncased_en_20200104_175422.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.36. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

ËØ¥ÊòéÔºöpip install --upgrade hanlp   ‰∏çËÉΩËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ


******Á¨¨Âõõ:********
hanlp.load('SST2_ALBERT_BASE_EN')
>>> hanlp.load('SST2_ALBERT_BASE_EN')
Fetching ALBERT model: albert_base version: 2
albert_base.tar.gz: 0.00B [00:00, ?B/s]
Failed to load https://file.hankcs.com/hanlp/classification/sst2_albert_base_20200122_205915.zip. See stack trace below
Traceback (most recent call last):

hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_BASE_EN')
Fetching ALBERT model: albert_base version: 2
albert_base.tar.gz: 0.00B [00:00, ?B/s]
Failed to load https://file.hankcs.com/hanlp/classification/empathetic_dialogues_situation_albert_base_20200122_212250.zip. See stack trace below
Traceback (most recent call last):
  File ""C:\Python36\lib\urllib\request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""C:\Python36\lib\http\client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""C:\Python36\lib\http\client.py"", line 1026, in _send_output
    self.send(msg)
  File ""C:\Python36\lib\http\client.py"", line 964, in send
    self.connect()
  File ""C:\Python36\lib\http\client.py"", line 1392, in connect
    super().connect()
  File ""C:\Python36\lib\http\client.py"", line 936, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""C:\Python36\lib\socket.py"", line 724, in create_connection
    raise err
  File ""C:\Python36\lib\socket.py"", line 713, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•ÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØïÂ§±Ë¥•„ÄÇ

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""C:\Python36\lib\site-packages\hanlp\common\component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""C:\Python36\lib\site-packages\hanlp\common\component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""C:\Python36\lib\site-packages\hanlp\components\classifiers\transformer_classifier.py"", line 153, in build_model
    tagging=False)
  File ""C:\Python36\lib\site-packages\hanlp\layers\transformers\loader.py"", line 84, in build_transformer
    transformer))
  File ""C:\Python36\lib\site-packages\bert\loader_albert.py"", line 179, in fetch_tfhub_albert_model
    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir, local_file_name=local_file_name)
  File ""C:\Python36\lib\site-packages\params_flow\utils\fetch_unpack.py"", line 48, in fetch_url
    urllib.request.urlretrieve(url, local_path, report_hook, data=None)
  File ""C:\Python36\lib\urllib\request.py"", line 248, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""C:\Python36\lib\urllib\request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Python36\lib\urllib\request.py"", line 526, in open
    response = self._open(req, data)
  File ""C:\Python36\lib\urllib\request.py"", line 544, in _open
    '_open', req)
  File ""C:\Python36\lib\urllib\request.py"", line 504, in _call_chain
    result = func(*args)
  File ""C:\Python36\lib\urllib\request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""C:\Python36\lib\urllib\request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•ÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØï Â§±Ë¥•„ÄÇ>
https://file.hankcs.com/hanlp/classification/empathetic_dialogues_situation_albert_base_20200122_212250.zip was created with hanlp-2.0.0-alpha.27, while you are running 2.0.0-alpha.36. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .

hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_LARGE_EN')

ËøôÂá†‰∏™ÁªèÊ£ÄÊü•ÊòØhttps://tfhub.dev ‰∏ä‰∏çÂéªÁöÑÂéüÂõ†ÔºåÁî®‰ª£ÁêÜÂêéËß£ÂÜ≥ÔºåÂú®Ê≠§‰∏çÂæó‰∏çÂêêÊßΩ‰∏Ä‰∏ãÔºåÂõΩÂÜÖ‰ΩúÁ®ãÂ∫èÁ†îÁ©∂Ôºådouble hitÔºö
albert_models_tfhub = {
    ""albert_base"":    ""https://tfhub.dev/google/albert_base/{version}?tf-hub-format=compressed"",
    ""albert_large"":   ""https://tfhub.dev/google/albert_large/{version}?tf-hub-format=compressed"",
    ""albert_xlarge"":  ""https://tfhub.dev/google/albert_xlarge/{version}?tf-hub-format=compressed"",
    ""albert_xxlarge"": ""https://tfhub.dev/google/albert_xxlarge/{version}?tf-hub-format=compressed"",
}

******Á¨¨‰∫î:********
tokenizer = hanlp.load('SST2_ALBERT_BASE_EN')
>>> tokenizer = hanlp.load('EMPATHETIC_DIALOGUES_SITUATION_ALBERT_LARGE_EN')
Fetching ALBERT model: albert_large version: 2
Already  fetched:  albert_large.tar.gz
already unpacked at: C:\Users\Roottan\AppData\Roaming\hanlp\thirdparty\tfhub.dev\google\albert_large\albert_large
Done loading 23 BERT weights from: C:\Users\****\AppData\Roaming\hanlp\thirdparty\tfhub.dev\google\albert_large\albert_large into <bert.model.BertModelLayer object at 0x000001DCDF95B828> (prefix:albert_2). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]
Unused weights from saved model:
        bert/pooler/dense/bias
        bert/pooler/dense/kernel
        cls/predictions/output_bias
        cls/predictions/transform/LayerNorm/beta
        cls/predictions/transform/LayerNorm/gamma
        cls/predictions/transform/dense/bias
        cls/predictions/transform/dense/kernel

‰∏çÁü•ÈÅìÊòØ‰∏™Ê°àËøòÊòØÂ§ßÂÆ∂‰ºöÈÅáÂà∞ÁöÑÈÄöÁóÖ




* [x] I've completed this form and searched the web for solutions.
"
pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN don't work.,"**Describe the bug**
version: hanlp 2.0.0-alpha.34
because of the lack of downloading, I downloaded ""ner_conll03_bert_base_uncased_en_20200104_194352.zip"" manually, place it into /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip

**Code to reproduce the issue**
```python
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)
```

**Describe the current behavior**
if don't unzip the .zip file by myself, got msg as below:
FileNotFoundError: The identifier /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip resolves to a non-exist meta file /xxx/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip/meta.json.

Then, I unzip it and retry.

**Expected behavior**
https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.34. Try to upgrade hanlp with
pip install --upgrade hanlp

how to get alpha.5? which source/image mirror?

**System information**
-CentOS Linux release 7.7.1908 (Core)
- Python version: Python 3.7.4
- HanLP version: 2.0.0-alpha.34

"
hanlp 2.0.0a33 pipeline Ëá™ÂÆö‰πâ output_key ÂêéÔºåjson Â∫èÂàóÂåñÁªìÊûúÊÆãÁïôsentences „ÄÅ tokens„ÄÅpart_of_speech_tags Á≠âÂ≠óÊÆµ,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
hanlp 2.0.0a33 pipeline Ëá™ÂÆö‰πâ output_key ÂêéÔºåjson Â∫èÂàóÂåñÁªìÊûúÊÆãÁïô**sentences „ÄÅ tokens„ÄÅpart_of_speech_tags„ÄÅnamed_entities„ÄÅsyntactic_dependencies„ÄÅsemantic_dependencies** Á≠âÂ≠óÊÆµ

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

print('Ê≠£Âú®Âä†ËΩΩ')

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')
semantic_parser = hanlp.load('SEMEVAL16_TEXT_BIAFFINE_ZH')

# ‰∏ãÈù¢Ëá™ÂÆö‰πâ‰∫Ü ouput_key
pipeline = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='s') \
    .append(tokenizer, output_key='t') \
    .append(tagger, output_key='p') \
    .append(syntactic_parser, input_key=('t', 'p'), output_key='syn', conll=False) \
    .append(semantic_parser, input_key=('t', 'p'), output_key='sem', conll=False)
print(pipeline)

text = '''HanLPÊòØ‰∏ÄÁ≥ªÂàóÊ®°Âûã‰∏éÁÆóÊ≥ïÁªÑÊàêÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂåÖÔºåÁõÆÊ†áÊòØÊôÆÂèäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ
HanLPÂÖ∑Â§áÂäüËÉΩÂÆåÂñÑ„ÄÅÊÄßËÉΩÈ´òÊïà„ÄÅÊû∂ÊûÑÊ∏ÖÊô∞„ÄÅËØ≠ÊñôÊó∂Êñ∞„ÄÅÂèØËá™ÂÆö‰πâÁöÑÁâπÁÇπ„ÄÇ
ÂÜÖÈÉ®ÁÆóÊ≥ïÁªèËøáÂ∑•‰∏öÁïåÂíåÂ≠¶ÊúØÁïåËÄÉÈ™åÔºåÈÖçÂ•ó‰π¶Á±ç„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÂ∑≤ÁªèÂá∫Áâà„ÄÇ
'''

doc = pipeline(text)
print(doc)  # ÊâìÂç∞ÁªìÊûúÊ≠£Â∏∏ÔºåÊ≤°ÊúâÊÆãÁïôÂ≠óÊÆµ

with open('output.json',""w"") as f:
    json.dump(doc, f, indent=4, ensure_ascii=False)    # Â∫èÂàóÂåñÁöÑÁªìÊûúÊúâÊÆãÁïôÂ≠óÊÆµ
```

**Describe the current behavior**
json Â∫èÂàóÂåñÂêéÁöÑÁªìÊûúÔºåÂ≠òÂú®‰∏ãÈù¢‰∏Ä‰∫õ‚Äú**Êú™ÂÆö‰πâÂ≠óÊÆµ**‚Äù
{
    ""sentences"": [],
    ""tokens"": [],
    ""part_of_speech_tags"": [],
    ""named_entities"": [],
    ""syntactic_dependencies"": [],
    ""semantic_dependencies"": [],
    ""s"": [
        ""HanLPÊòØ‰∏ÄÁ≥ªÂàóÊ®°Âûã‰∏éÁÆóÊ≥ïÁªÑÊàêÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂåÖÔºåÁõÆÊ†áÊòØÊôÆÂèäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÂ∫îÁî®„ÄÇ"",
        ""HanLPÂÖ∑Â§áÂäüËÉΩÂÆåÂñÑ„ÄÅÊÄßËÉΩÈ´òÊïà„ÄÅÊû∂ÊûÑÊ∏ÖÊô∞„ÄÅËØ≠ÊñôÊó∂Êñ∞„ÄÅÂèØËá™ÂÆö‰πâÁöÑÁâπÁÇπ„ÄÇ"",
        ""ÂÜÖÈÉ®ÁÆóÊ≥ïÁªèËøáÂ∑•‰∏öÁïåÂíåÂ≠¶ÊúØÁïåËÄÉÈ™åÔºåÈÖçÂ•ó‰π¶Á±ç„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÂ∑≤ÁªèÂá∫Áâà„ÄÇ""
    ],
...

**Expected behavior**
**Êú™ÂÆö‰πâÂ≠óÊÆµ‰∏çÂá∫Áé∞**

**System information**
- OS Platform and Distribution:18.04
- Python version:3.6
- HanLP version:hanlp 2.0.0a33

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
pip install hanlp‰ºöËá™Âä®‰∏ãËΩΩtensorflow2.1,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
pip install hanlp‰ºöËá™Âä®‰∏ãËΩΩtensorflow2.1ÔºåÂπ∂Ë¶ÜÁõñÂ∑≤ÊúâÁöÑtensorflow-gpu2.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```bash
pip install hanlp
```

**Describe the current behavior**
```bash
Collecting hanlp
  Using cached hanlp-2.0.0a33.tar.gz (130 kB)
Collecting tensorflow==2.1.0
  Using cached tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)
```


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04.1-Ubuntu
- Python version: 3.6
- HanLP version: hanlp-2.0.0a33

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0a31 CoNLLWord JSON Â∫èÂàóÂåñÁªìÊûú ‰∏é ÊñáÊ°£ËØ¥Êòé‰∏ç‰∏ÄËá¥,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
CoNLLWord JSON Â∫èÂàóÂåñÁªìÊûú ‰∏é ÊñáÊ°£ËØ¥Êòé‰∏ç‰∏ÄËá¥

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')

text = 'ÂÜÖÈÉ®ÁÆóÊ≥ïÁªèËøáÂ∑•‰∏öÁïåÂíåÂ≠¶ÊúØÁïåËÄÉÈ™åÔºåÈÖçÂ•ó‰π¶Á±ç„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÂ∑≤ÁªèÂá∫Áâà„ÄÇ'

pipeline = hanlp.pipeline() \
            .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
            .append(tokenizer, output_key='tokens') \
            .append(tagger, output_key='part_of_speech_tags')
            .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies')

output = pipeline(text)

print(output)  

with open('output.json',""w"") as f:
    json.dump(output, f, indent=4, ensure_ascii=False)
```

**Describe the current behavior**
# ÂΩìÂâçËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ã
```
...
""syntactic_dependencies"": [
    [{""id"": 1, ""form"": ""ÂÜÖÈÉ®"", ""cpos"": ""NN"", ""pos"": null, ""head"": 2, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 2, ""form"": ""ÁÆóÊ≥ï"", ""cpos"": ""NN"", ""pos"": null, ""head"": 18, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 3, ""form"": ""ÁªèËøá"", ""cpos"": ""P"", ""pos"": null, ""head"": 18, ""deprel"": ""prep"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 4, ""form"": ""Â∑•‰∏öÁïå"", ""cpos"": ""NN"", ""pos"": null, ""head"": 6, ""deprel"": ""conj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 5, ""form"": ""Âíå"", ""cpos"": ""CC"", ""pos"": null, ""head"": 6, ""deprel"": ""cc"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 6, ""form"": ""Â≠¶ÊúØÁïå"", ""cpos"": ""NN"", ""pos"": null, ""head"": 7, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 7, ""form"": ""ËÄÉÈ™å"", ""cpos"": ""NN"", ""pos"": null, ""head"": 3, ""deprel"": ""pobj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 8, ""form"": ""Ôºå"", ""cpos"": ""PU"", ""pos"": null, ""head"": 18, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 9, ""form"": ""ÈÖçÂ•ó"", ""cpos"": ""NN"", ""pos"": null, ""head"": 10, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 10, ""form"": ""‰π¶Á±ç"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 11, ""form"": ""„Ää"", ""cpos"": ""PU"", ""pos"": null, ""head"": 14, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 12, ""form"": ""Ëá™ÁÑ∂"", ""cpos"": ""NN"", ""pos"": null, ""head"": 13, ""deprel"": ""nn"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 13, ""form"": ""ËØ≠Ë®Ä"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""nsubj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 14, ""form"": ""Â§ÑÁêÜ"", ""cpos"": ""VV"", ""pos"": null, ""head"": 18, ""deprel"": ""dep"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 15, ""form"": ""ÂÖ•Èó®"", ""cpos"": ""NN"", ""pos"": null, ""head"": 14, ""deprel"": ""dobj"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 16, ""form"": ""„Äã"", ""cpos"": ""PU"", ""pos"": null, ""head"": 14, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 17, ""form"": ""Â∑≤Áªè"", ""cpos"": ""AD"", ""pos"": null, ""head"": 18, ""deprel"": ""advmod"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 18, ""form"": ""Âá∫Áâà"", ""cpos"": ""VV"", ""pos"": null, ""head"": 0, ""deprel"": ""root"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}, {""id"": 19, ""form"": ""„ÄÇ"", ""cpos"": ""PU"", ""pos"": null, ""head"": 18, ""deprel"": ""punct"", ""lemma"": null, ""feats"": null, ""phead"": null, ""pdeprel"": null}]
  ],
...
```

**Expected behavior**
```
...
""syntactic_dependencies"": [
[[2, ""nn""], [18, ""nsubj""], [18, ""prep""], [6, ""conj""], [6, ""cc""], [7, ""nn""], [3, ""pobj""], [18, ""punct""], [10, ""rcmod""], [15, ""nn""], [15, ""punct""], [15, ""nn""], [15, ""nn""], [15, ""nn""], [18, ""nsubj""], [15, ""punct""], [18, ""advmod""], [0, ""root""], [18, ""punct""]]
]
...
```

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Python version: 3.6
- HanLP version: hanlp 2.0.0a31

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
pipeline Object of type 'CoNLLWord' is not JSON serializable,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
pipeline  syntactic_parser  ËæìÂá∫ÁöÑÁªìÊûú ÈÄöËøá json dump Â§±Ë¥•    .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
import json

tokenizer = hanlp.load('CTB6_CONVSEG')
tagger = hanlp.load('CTB5_POS_RNN')
syntactic_parser = hanlp.load('CTB7_BIAFFINE_DEP_ZH')

text = 'ÊµãËØï‰∏Ä‰∏ãËøô‰∏™ÊñπÊ≥ïËÉΩÂê¶‰ΩøÁî®„ÄÇ'

pipeline = hanlp.pipeline() \
            .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
            .append(tokenizer, output_key='tokens') \
            .append(tagger, output_key='part_of_speech_tags')
            .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies')

output = pipeline(text)

print(output)    # **ÁªìÊûúËÉΩÊ≠£Â∏∏ÊâìÂç∞**

with open('output.json',""w"") as f:
    json.dump(output, f, indent=4, ensure_ascii=False)
```

**Describe the current behavior**
json.dump Â§±Ë¥•

**Expected behavior**
ËÉΩÊ≠£Â∏∏ JSON serializable

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Python version: 3.6
- HanLP version: hanlp 2.0.0a30

**Other info / logs**
Traceback (most recent call last):
  File ""down.py"", line 33, in <module>
    json.dump(output, f, indent=4, ensure_ascii=False)
  File ""/usr/lib/python3.6/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/usr/lib/python3.6/json/encoder.py"", line 430, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/usr/lib/python3.6/json/encoder.py"", line 437, in _iterencode
    o = _default(o)
  File ""/usr/lib/python3.6/json/encoder.py"", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'CoNLLWord' is not JSON serializable

* [x] I've completed this form and searched the web for solutions.
"
ÊãºÈü≥ËΩ¨Êç¢ ‚ÄúÁúãÁùÄ‚Äù Â∫îËØ•ÊòØ kanzhe,Â¶ÇÈ¢ò
ÊÄé‰πàÊü•ÁúãGPUÊòØÂê¶ÊàêÂäü‰ΩøÁî®,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
pipÂÆâË£Ö‰πãÂêéÔºåÂèØ‰ª•ÊàêÂäüËøêË°åNERÔºå‰ΩÜÊòØÂ¢ûÂä†batchÂêéGPUÁöÑÊòæÂ≠òÂßãÁªàÂÅúÁïôÂú®100Â§öMBÔºåÊÄé‰πàÊü•ÁúãGPUÊòØÂê¶ÊàêÂäü‰ΩøÁî®Ôºü




* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0a26 ‰ΩøÁî® CTB5_POS_RNN Êä•Èîô,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
hanlp 2.0.0a26 ‰ΩøÁî® CTB5_POS_RNN Êä•Èîô .

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load('CTB5_POS_RNN')
(tokenizer('ÂïÜÂìÅÂíåÊúçÂä°')
```

**Describe the current behavior**
ËøêË°å‰∏äËø∞‰ª£Á†ÅÂá∫Èîô

**Expected behavior**
ËÉΩÂ§üËøêË°åÊµãËØï‰ª£Á†Å

**System information**
- Ubuntu 18.04
- Python version: 3.6
- HanLP version: 2.0.0a26

**Other info / logs**
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mode
    yield
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterator_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 825, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""load-sem.py"", line 5, in <module>
    tokenizer('ÂïÜÂìÅÂíåÊúçÂä°')
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 51, in __call__
    return self.predict(data, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 47, in predict
    return super().predict(sents, batch_size)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 454, in predict
    for idx, batch in enumerate(dataset):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __next__
    return self.next()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mode
    executor_new.wait()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 825, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape () where an element of shape (None,) was expected.


	 [[{{node PyFunc}}]]
```

* [x] I've completed this form and searched the web for solutions.
"
2.0.0-alpha.25 Âä†ËΩΩ CTB5_POS_RNN Âá∫Èîô,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
2.0.0-alpha.25 Âä†ËΩΩ CTB5_POS_RNN Âá∫Èîô

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load('CTB5_POS_RNN')
print(tokenizer('ÂïÜÂìÅÂíåÊúçÂä°'))
```

**Describe the current behavior**
Âä†ËΩΩ CTB5_POS_RNN Âá∫Èîô

ËØï‰∫ÜÂá†Ê¨°ÔºåÊ≠§Â§ÑÂá∫Èîô
Downloading **https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt** to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
**100.00%, 4 KB/4 KB, 14.1 MB/s, ETA 0 s**     
......
**FileNotFoundError**: [Errno 2] No such file or directory: '/root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'

**Expected behavior**
ËÉΩÂ§üËøêË°åÊµãËØï‰ª£Á†Å

**System information**
- Ubuntu 18.04
- Python version: 3.6
- HanLP version: 2.0.0-alpha.25

**Other info / logs**
Downloading https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip to /root/.hanlp/pos/ctb5_pos_rnn_20191229_015325.zip
100.00%, 52.4 MB/52.4 MB, 4.2 MB/s, ETA 0 s        
Extracting /root/.hanlp/pos/ctb5_pos_rnn_20191229_015325.zip to /root/.hanlp/pos
Downloading https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
**100.00%, 4 KB/4 KB, 14.1 MB/s, ETA 0 s**      
Extracting /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip to /root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/__init__.py"", line 53, in build_embedding
    trainable=config.get('embedding_trainable', False))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/word2vec.py"", line 22, in __init__
    self.vocab, self.array_np = self._load(path, vocab, normalize)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/word2vec.py"", line 50, in _load
    word2vec, dim = load_word2vec(path)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 420, in load_word2vec
    with open(realpath, encoding='utf-8', errors='ignore') as f:
**FileNotFoundError**: [Errno 2] No such file or directory: '/root/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_20191229_015325.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.25. Try to upgrade hanlp with
pip install --upgrade hanlp

* [x] I've completed this form and searched the web for solutions.
"
hanlp 2.0.0-alpha.25 Âä†ËΩΩ hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH Âá∫Èîô,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
hanlp 2.0.0-alpha.25 Âä†ËΩΩ hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH Âá∫Èîô

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
tagger(['Êàë', 'ÁöÑ', 'Â∏åÊúõ', 'ÊòØ', 'Â∏åÊúõ', 'ÂíåÂπ≥'])
```

**Describe the current behavior**
Âä†ËΩΩ hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH Êä•Èîô

**Expected behavior**
ËÉΩÂ§üËøêË°åÊµãËØï‰ª£Á†Å
**‰∏ç‰ºöÊòØÂÜÖÂ≠ò‰∏çÂ§üÂêßÔºü**

**System information**
- Ubuntu 18.04 
- Python version: 3.6
- HanLP version: 2.0.0-alpha.25

**Other info / logs**
Downloading https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip to /root/.hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip
100.00%, 1.4 MB/1.4 MB, 677 KB/s, ETA 0 s      
Extracting /root/.hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip to /root/.hanlp/pos
Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin to /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip
1.68%, 53.6 MB/3.1 GB, 9.6 MB/s, ETA 5 m 27 s   
100.00%, 3.1 GB/3.1 GB, 8.0 MB/s, ETA 0 s      
Extracting /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip to /root/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh
Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/embeddings/fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
**MemoryError: std::bad_alloc**
**https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip** was created with **hanlp-2.0.0, while you are running 2.0.0-alpha.25.** Try to upgrade hanlp with
pip install --upgrade hanlp

* [x] I've completed this form and searched the web for solutions.
"
Ê†πÊçÆÂú∞ÂùÄÊâãÂä®‰∏ãËΩΩÊ®°ÂûãÂêéÔºåÂä†ËΩΩÊä•Èîô,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
A clear and concise description of what the bug is.
ÊâßË°å‰ª£Á†ÅÔºö
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)

Âõ†‰∏∫‰∏ãËΩΩÂ§™ÊÖ¢

Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin to /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip

Please download it to /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip

ÊàëÊâãÂä®‰∏ãËΩΩÂêéÔºåÂà∞ /Users/kiwi/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip ÂêéÔºåËøêË°å‰ª£Á†ÅÊä•Èîô

Failed to load https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip. See stack trace below
Traceback (most recent call last):
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/components/taggers/rnn_tagger.py"", line 34, in build_model
    embeddings = build_embedding(embeddings, self.transform.word_vocab, self.transform)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/hanlp/layers/embeddings/fast_text.py"", line 35, in __init__
    self.model = fasttext.load_model(filepath)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/fasttext/FastText.py"", line 350, in load_model
    return _FastText(model_path=path)
  File ""/Users/kiwi/anaconda/python.app/Contents/lib/python3.6/site-packages/fasttext/FastText.py"", line 43, in __init__
    self.f.loadModel(model_path)
RuntimeError: Caught an unknown exception!
https://file.hankcs.com/hanlp/pos/ctb5_pos_rnn_fasttext_20191230_202639.zip was created with hanlp-2.0.0, while you are running 2.0.0-alpha.24. Try to upgrade hanlp with
pip install --upgrade hanlp



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
```

**Describe the current behavior**
A clear and concise description of what happened.

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os
- Python version: 3.69
- HanLP version: 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions."
"Error in component_util.py file. Should be sys.exit(), but instead it is exit(). Also need to import sys ","<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
I am trying to run NER with hanlp, but it is giving me an error. I am running the following comman in Google Colab with GPU

recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)

**Code to reproduce the issue**
Following were my Cell values in Colab
!pip install --hanlp
import hanlp
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)


```python
```

**Describe the current behavior**
When I run the said code, I get the follwoing error and point to this file ""component_util.py""
NameError: name 'exit' is not defined on line 56

**Expected behavior**
The command should have ran successfully allowing me to Find Named Entity in  sentence

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab with GPU
- Python version: 3.6
- HanLP version: 2.0.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
Named Entity Recognition Doesnt work,"I am running this project in Google Colab with GPU, which I run this command
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)

It gives following error,
Downloading https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip to /root/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip
92.72%, 356.6 MB/384.6 MB, 7.3 MB/s, ETA 4 s      Executing op Range in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Downloading https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
100.00%, 388.8 MB/388.8 MB, 7.9 MB/s, ETA 0 s      
Extracting /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18
Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op TruncatedNormal in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0

Failed to load https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip. See stack trace below
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
    obj.load(save_dir, **load_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
    loss=kwargs.get('loss', None)))
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 35, in build_model
    model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 108, in build_transformer
    with stdout_redirected(to=os.devnull):
  File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 520, in stdout_redirected
    stdout_fd = fileno(stdout)
  File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 501, in fileno
    fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()
io.UnsupportedOperation: fileno
https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.20. Try to upgrade hanlp with
pip install --upgrade hanlp

---------------------------------------------------------------------------

UnsupportedOperation                      Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
     42                     load_kwargs = {}
---> 43                 obj.load(save_dir, **load_kwargs)
     44             obj.meta['load_path'] = load_path

9 frames

UnsupportedOperation: fileno


During handling of the above exception, another exception occurred:

NameError                                 Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
     54                 f'Try to upgrade hanlp with\n'
     55                 f'pip install --upgrade hanlp')
---> 56         exit(1)
     57 
     58 

NameError: name 'exit' is not defined

Any help will be appreciated!
Thank in advance"
ËÅöÁ±ªÁÆóÊ≥ï-‰º†ÂÖ•Á∞áÂèÇÊï∞Â§ß‰∫éÊñáÊ°£‰∏™Êï∞Êó∂Êä•Á©∫ÊåáÈíà,"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
‰º†ÂÖ•Á∞áÂèÇÊï∞Â§ß‰∫éÊñáÊ°£‰∏™Êï∞Êó∂Êä•Á©∫ÊåáÈíà

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
public static void main(String[] args) {
		String a = ""select * from table;"";
		String b = ""select * from table;"";
		String c = ""select * from table where id = 100;"";
		String d = ""select * from table;"";
		String e = ""delete from table;"";
		String f = ""update table set age = 1 where id = 9"";
		String g = ""update table set age = 1 where id = 88"";
		String h = ""update table set age = 1 where id = 10"";

		Set<Integer> set = new HashSet<>();
		List<String> list = new ArrayList<>();
		list.add(a);
		list.add(b);
		list.add(c);
		list.add(d);
		list.add(e);
		list.add(f);
		list.add(g);
		list.add(h);
		String[] array = list.toArray(new String[0]);
		set.add(0);
		set.add(1);
		set.add(3);
		set.add(5);
		set.add(6);
		set.add(7);
		System.out.println(""================ËÅöÁ±ªÁÆóÊ≥ï=============="");
		ClusterAnalyzer<String> analyzer = new ClusterAnalyzer<>();
		for (Integer s: set) {
			analyzer.addDocument(String.valueOf(s), array[s]);
		}
		// kÂ§ß‰∫ésetÈõÜÂêàÂ§ßÂ∞è
		int k = 10;
		System.out.println(analyzer.kmeans(k));
		System.out.println();

		System.out.println(analyzer.repeatedBisection(k));
		System.out.println(analyzer.repeatedBisection(1.0));

	}
```

**Describe the current behavior**
```
kmeans:
================ËÅöÁ±ªÁÆóÊ≥ï==============
Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.refine_clusters(ClusterAnalyzer.java:263)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.kmeans(ClusterAnalyzer.java:147)
```
```
repeatedBisection:
================ËÅöÁ±ªÁÆóÊ≥ï==============

Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:222)
	at com.hankcs.hanlp.mining.cluster.ClusterAnalyzer.repeatedBisection(ClusterAnalyzer.java:180)
```
**Expected behavior**
Á∞áÁ±ªÂèÇÊï∞‰º†ÁöÑÂÄºË∂äÁïåÊó∂ÔºåÁ®ãÂ∫èÂ∫îÂΩìÂèãÂ•ΩÂ§ÑÁêÜ‰∏∫ÊñáÊ°£‰∏™Êï∞
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Python version:
- HanLP version: 1.7.6

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
"
ModuleNotFoundError: No module named 'regex',"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**

**ModuleNotFoundError: No module named 'regex'**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
Python 3.7.5 (default, Jan  6 2020, 17:18:04)
[Clang 11.0.0 (clang-1100.0.33.16)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import hanlp
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/__init__.py"", line 6, in <module>
    import hanlp.common
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/__init__.py"", line 4, in <module>
    from . import component
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/component.py"", line 17, in <module>
    from hanlp.common.structure import SerializableDict
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/structure.py"", line 6, in <module>
    from hanlp.utils.io_util import save_json, save_pickle, load_pickle, load_json, filename_is_json
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/__init__.py"", line 5, in <module>
    from . import rules
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/rules.py"", line 3, in <module>
    from hanlp.utils.english_tokenizer import tokenize_english
  File ""/Users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/english_tokenizer.py"", line 12, in <module>
    from regex import compile, DOTALL, UNICODE, VERBOSE
ModuleNotFoundError: No module named 'regex'
```

**Describe the current behavior**
ModuleNotFoundError: No module named 'regex'

**Expected behavior**
No error, no warning.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin MacBook 19.2.0
- Python version: 3.7.5
- HanLP version: 2.0.0a10

**Other info / logs**

**Solved**

```
pip install regex
```"
‰∏ãËΩΩÂè•Ê≥ïËß£ÊûêÊó∂Âá∫Èîô,"

**Describe the bug**
‰∏ãËΩΩÂè•Ê≥ïËß£ÊûêÊó∂Âá∫ÈîôÔºåÊèêÁ§∫https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.12. T

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import hanlp
syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)
```

**Describe the current behavior**
```
Downloading https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip to /Users/wuhaixu/.hanlp/dep/biaffine_ctb7_20191229_130325.zip
100.00%, 65.8 MB/65.8 MB, 219 KB/s, ETA 0 s          
Extracting /Users/wuhaixu/.hanlp/dep/biaffine_ctb7_20191229_130325.zip to /Users/wuhaixu/.hanlp/dep
Downloading https://github.com/SUDA-LA/CIP/archive/master.zip#BPNN/data/embed.txt to /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip
100.00%, 0 KB/0 KB, 0 KB/s, ETA 0 s      
Extracting /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master.zip to /Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive
Failed to load https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip. See stack trace below
Traceback (most recent call last):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/utils/component_util.py"", line 36, in load_from_meta_file
    obj.load(save_dir)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 231, in load
    self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 242, in build
    loss=kwargs.get('loss', None)))
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/parsers/biaffine_parser.py"", line 35, in build_model
    self.transform) if pretrained_embed else None
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/layers/embeddings/__init__.py"", line 33, in build_embedding
    layer: tf.keras.layers.Embedding = tf.keras.utils.deserialize_keras_object(embeddings)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 305, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 519, in from_config
    return cls(**config)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/layers/embeddings/word2vec.py"", line 119, in __init__
    word2vec, _output_dim = load_word2vec(filepath)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/utils/io_util.py"", line 410, in load_word2vec
    with open(realpath, encoding='utf-8', errors='ignore') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/wuhaixu/.hanlp/thirdparty/github.com/SUDA-LA/CIP/archive/master/BPNN/data/embed.txt'
https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20191229_130325.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.12. Try to upgrade hanlp with
pip install --upgrade hanlp
```


**System information**
- Mac OS
- Python 3.6.9
- HanLP version: 2.0.0-alpha.12



* [x] I've completed this form and searched the web for solutions.
"
ÂÆû‰ΩìËØÜÂà´AttributeError: 'FullTokenizer' object has no attribute 'unk_token',"<!--
Please carefully fill out this form to bypass our spam filter. Please make sure that this is a bug. We only address bugs and feature requests issues on GitHub. Other questions should be posted on stackoverflow or https://bbs.hankcs.com/
‰ª•‰∏ãÂøÖÂ°´ÔºåÂê¶ÂàôÁõ¥Êé•ÂÖ≥Èó≠„ÄÇ
-->

**Describe the bug**
ÂÆû‰ΩìËØÜÂà´ËæìÂÖ•‰ª•‰∏ãÊñáÊú¨Êó∂Ôºå‰ºöÂá∫Áé∞AttributeError: 'FullTokenizer' object has no attribute 'unk_token'

**Code to reproduce the issue**
```python
import hanlp

recognizer([list('Â≠ΩÂÄ∫ Ôºà‰∏äÊµ∑ËØùÔºâ')])
recognizer([list('ÂÖ≥‰∫éÁà±ÁöÑÂõõÁØáÂ∞èÂ∞èËØ¥„ÄäÈ±ºÂ§¥„ÄãÂÑøÂ≠ê‰ªéÂ∞èÂ∞±ËÆ∞ÂæóÔºå‰ΩÜÂá°ÂÆ∂ÈáåÂêÉÈ±ºÔºåÂ¶àÂ¶àÊÄªÊòØÊääÈ±ºÂ§¥Â§πËá™Â∑±Á¢óÈáåÔºåÂõ†‰∏∫È±ºÂ§¥ËÇâÂ∞ë„ÄÇÂêéÊù•ÊØèÊ¨°Áà∏Áà∏ÈÉΩÊääÈ±ºÂ§¥Â§πÁªôÂ•π„ÄÇÁõ¥Âà∞Â¶àÂ¶àËÄÅ‰∫ÜÔºå‰æùÊóßÂ¶ÇÊ≠§„ÄÇÂêéÊù•ÔºåÂ¶àÂ¶àÁóÖÈáçÔºå‰∏ÄÂ§©ÔºåÂÆ∂ÈáåÂèàÂêÉÈ±º‰∫ÜÔºåÂ¶àÂ¶àË∑ü‰ªñÂíåÁà∏Áà∏ËØ¥ÔºöÊàëÂêÉ‰∫Ü‰∏ÄËæàÂ≠êÈ±ºÂ§¥ÔºåÊàëË¶ÅÊ≠ª‰∫ÜÔºåËÆ©Êàë‰πüÂêÉ‰∏ÄÊ¨°È±ºËÇâÂêß„ÄÇÁà∏Áà∏ÁóõÂì≠Ôºö‚ÄúÊàëËøôËæàÂ≠êÊúÄÂñúÊ¨¢ÁöÑÂ∞±ÊòØÈ±ºÂ§¥Â∞±ÈÖíÔºåÊàë‰ª•‰∏∫‰Ω†ÁúüÁöÑÁà±ÂêÉÔºåÊâçÊØè Ê¨°ÂøçÁùÄÂè£Ê∞¥ÊääÈ±ºÂ§¥ËÆ©Áªô‰Ω†„ÄÇ‚Äù„Ää‰∏ãÈõ®„Äã‰∏ÄÂú∫ËΩ¶Á•∏ÔºåÂ§∫Ëµ∞‰∫ÜÂ•≥Â≠©ÂæàÂ§ö‰∏úË•øÔºåÂ•≥Â≠©ÂèòÂæóÂèàËÅãÂèàÁûéÔºåÁî∑ÊúãÂèã‰πüÁ¶ªÂ•πËÄåÂéªÔºåÂè™ÊúâÂ¶àÂ¶à‰∏ÄÁõ¥Èô™Âú®Â•πË∫´Ëæπ„ÄÇÂ•≥Â≠©‰ªéÂ∞èÂ∞±ÂñúÊ¨¢‰∏ãÈõ®Â§©ÔºåÂ•πÂñúÊ¨¢Âú®ÁªÜÈõ®‰∏≠Êº´Ê≠•ÁöÑÊÑü Ëßâ„ÄÇÂ¶àÂ¶àÁü•ÈÅìÂ•πÂñúÊ¨¢Èõ®ÔºåÂ∞±Â∏∏Êâ∂Â•≥Â≠©Âá∫ÂéªÔºåËÆ©Â•πÊÑüÂèóÈõ®Ê∞¥ÊãçÊâìÂèåÊâã„ÄÇÊâÄÂπ∏ÁöÑÊòØ‰ªäÂπ¥Èõ®ÁâπÂà´Â§ö„ÄÇ‰∏ÄÂ§©ÔºåÂèà‰∏ãÈõ®‰∫ÜÔºåÂ¶àÂ¶àÊâ∂Â•≥ÂÑøÂá∫Âéª„ÄÇ‚ÄúÂ¶àÔºåÊàëÊÉ≥ÂõûÂéª‰∫Ü„ÄÇ‚Äù""Â•Ω„ÄÇ‚ÄùÂõûÂéªÊó∂Â•≥ÂÑøÁöÑËÄ≥ÊúµÁ™ÅÁÑ∂Êúâ‰∫ÜÂâßÁÉàÁöÑÁñºÁóõÂíåÂèçÂ∫îÔºåÊ®°Á≥ä‰∏≠ÈöêÁ∫¶Âê¨Âà∞Ôºö‚ÄúÁúãÔºåÂèàÊòØÈÇ£‰∏§‰∏™‰∫∫ÔºåÂ§ßÊô¥Â§©ÁöÑÈõá‰∫∫Ê¥íÊ∞¥ÔºåÁúüÂèØÁ¨ëÔºåÂèà‰∏çÊòØÊãçÊàè......‚ÄùÊ≠§ÂàªÔºåÂ•πÂº∫ÂøçÊ≥™Ê∞¥Ôºå‰∏çËÆ©Â¶àÂ¶àÁúãËßÅ„ÄÇ¬†„ÄäËÄÅ‰º¥„ÄãÈÄÄ‰ºëÂú®ÂÆ∂ÂêéÔºåËÄÅ‰º¥ÊúÄÁà±‰ªéÊó©Âà∞ÊôöÊï∞ËêΩÊàëÂèà ËÄÅÂèàËÉñÂ•ΩÂêÉÊáíÂÅö„ÄÇ‰ªäÊó©Ëµ∑Â∫äÊàëÁ™ÅÁÑ∂Âí≥ÂóΩÂπ∂ÂêêÂá∫‰∏ÄÂè£È≤úË°ÄÔºå‰ªñÁúãÂà∞ÂêéÊï¥‰∏™‰∏äÂçàÊ≤°ÊúâËØ¥Âá∫‰∏ÄÂè•ËØùÔºåÈó∑Èó∑ÁöÑÊäΩÁÉü„ÄÇ‰∏≠ÂçàÊãâÊàëÂéª‰∫ÜÂåªÈô¢ÔºåÊúÄÂêéÂæóÁü•ÈÇ£ÊòØÊàëÁâôÈæàÂèëÁÇéÂè£ËÖîÂá∫ÁöÑË°ÄÔºå‰ªñÁ´ãÈ©¨Â∞±Á´ôÂú®ÂåªÈô¢ÊÄíÈ™ÇÊàëÔºö‚Äù‰Ω†Ëøô‰∏™Ê≤°Áî®ÁöÑËÉñËÄÅÂ§™Â©Ü‚Ä¶‚Ä¶‚ÄùÂè™ÊòØÊ≤°È™ÇÂÆåÔºå‰ªñÁöÑÁúºÁú∂Â∑≤Êª°ÊòØÊ≥™Ê∞¥......„ÄäÂ≠òÊäò„ÄãËÄÅ‰∫∫ÂæóËÇ∫Áôå‰ΩèÈô¢ÔºåÂÆ∂ÈáåÂá†‰πéÊéèÁ©∫‰∫ÜÔºåÊúÄÂêéÂÆûÂú®Ê≤°ÂäûÊ≥ïÔºåÂÆ∂‰∫∫Âè™ËÉΩÁúºÁùÅÁùÅÁúãÁùÄËÄÅ‰∫∫ÂèóÁùÄÁóÖÁóõÁöÑÊäòÁ£®„ÄÇËÄÅ‰∫∫‰∏¥Ëµ∞ÂâçÁöÑ‰∏ÄÊôöÔºåÂ∞ÜÂ≠ôÂ≠êÂè´Âà∞Â∫äÂâçÔºåÂ∞èÂøÉÂú∞‰ªéÊÄÄ‰∏≠ÊéèÂá∫‰∏ÄÊú¨Â≠òÊäòÔºåÂ∞èÂ£∞ÂØπÂ≠ôÂ≠êËØ¥ÔºöËøôÊòØÊàëÁïôÁöÑ‰∏ÄÁÇπÁßÅÊàøÈí±ÔºåÂ∞±Áü•ÈÅìÈÇ£ÂÇªËÄÅÂ©ÜÂ≠ê‰ºöÊääËá™Â∑±ÁöÑÂÖªËÄÅÈí±ÊãøÂá∫Êù•ÁªôÊàëÁúãÁóÖÔºåËøôÈí±ÊòØÊàëÂÅ∑ÂÅ∑ËóèÁöÑÔºåÁ≠âÊàëËµ∞ÂêéÂ∞±ÁïôÁªôÂ•πÂÖªËÄÅÂêß‚Ä¶‚Ä¶Â∞èÂ£∞ÂØπÂ≠ôÂ≠êËØ¥ÔºöËøôÊòØÊàëÁïôÁöÑ‰∏ÄÁÇπÁßÅÊàøÈí±ÔºåÂ∞±Áü•ÈÅìÈÇ£ÂÇªËÄÅÂ©ÜÂ≠ê‰ºöÊääËá™Â∑±ÁöÑÂÖªËÄÅÈí±ÊãøÂá∫Êù•ÁªôÊàëÁúãÁóÖÔºåËøôÈí±ÊòØÊàëÂÅ∑ÂÅ∑ËóèÁöÑÔºåÁ≠âÊàëËµ∞ÂêéÂ∞±ÁïôÁªôÂ•πÂÖªËÄÅÂêß‚Ä¶‚Ä¶')])
```

**Describe the current behavior**
Traceback (most recent call last):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mode
    yield
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterator_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FullTokenizer' object has no attribute 'unk_token'
Traceback (most recent call last):

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/transform.py"", line 155, in generator
    yield from samples

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_transform.py"", line 85, in inputs_to_samples
    pad_token_label_id=pad_label_idx)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/utils.py"", line 43, in convert_examples_to_features
    word_tokens = [x] * len(word)

AttributeError: 'FullTokenizer' object has no attribute 'unk_token'


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 51, in __call__
    return self.predict(data, **kwargs)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/component.py"", line 441, in predict
    for idx, batch in enumerate(dataset):
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __next__
    return self.next()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mode
    executor_new.wait()
  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FullTokenizer' object has no attribute 'unk_token'
Traceback (most recent call last):

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 236, in __call__
    ret = func(*args)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 789, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/common/transform.py"", line 155, in generator
    yield from samples

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_transform.py"", line 85, in inputs_to_samples
    pad_token_label_id=pad_label_idx)

  File ""/Users/wuhaixu/anaconda3/lib/python3.6/site-packages/hanlp/components/taggers/transformers/utils.py"", line 43, in convert_examples_to_features
    word_tokens = [tokenizer.unk_token] * len(word)

AttributeError: 'FullTokenizer' object has no attribute 'unk_token'


	 [[{{node PyFunc}}]]


**Expected behavior**
ÊúüÊúõÈ°∫Âà©ËøîÂõûÂÆû‰ΩìËØÜÂà´ÁªìÊûú

**System information**
- MacOS 10.14.6
- python3.6.9
- 2.0.0-alpha.10



* [x] I've completed this form and searched the web for solutions.
"
Export models for serving. Explore the scalable way to serve efficiently,tensorflow serving consumes memory greedily. Will find a way to fit all the pipelines into one GPU.
Train a LM on wiki + weibo + qa + news + danmaku + reviews + ...,"What makes HanLP different than the majority of OSS projects?
One of the most important factors would be the large scale professional corpora, and the correct way to make use of them.
To have some unique pretrained LM before releasing the beta version would be a cool idea. Don't you think so?"
Unittest and CI integration,"Shall we use CI? I think so, as the project grows fast, CI makes it easier to releases stable codes. The downside would be, it requires lots of effort to write unit tests.  "
Deploy on the production server. Invite users for beta test,"Only a handful models are able to serve. Those subclass models are not likely to fit with tensorflow serving. Need more time to investigate.

Will need an authentication service to identify users for test purpose and rate limiting. I don't have much computation resource for all the users. Might invite 10 lucky users. "
RESTful API in Python,"Implement a RESTful API in Python, then release it to pypi."
RESTful API in Java,Planning to implement a RESTful API in Java. Then release it to maven.
Documentation,Planning to fully document all the codes and set up a documentation service.
Êñá‰ª∂Âä†ËΩΩÊä•Èîô,"https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip was created with hanlp-2.0.0, but you are running 2.0.0-alpha.10. Try to upgrade hanlp with"
ÂëΩ‰ª§Ë°åËæìÂÖ•HanlpÂêéÂá∫Èîô,"Traceback (most recent call last):
  File ""e:\coding\annoconda\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""e:\coding\annoconda\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""E:\coding\annoconda\Scripts\hanlp.exe\__main__.py"", line 5, in <module>
  File ""e:\coding\annoconda\lib\site-packages\pyhanlp\__init__.py"", line 145, in <module>
    _start_jvm_for_hanlp()
  File ""e:\coding\annoconda\lib\site-packages\pyhanlp\__init__.py"", line 133, in _start_jvm_for_hanlp
    HANLP_JVM_XMX, convertStrings=True)
  File ""e:\coding\annoconda\lib\site-packages\jpype\_core.py"", line 219, in startJVM
    _jpype.startup(jvmpath, tuple(args), ignoreUnrecognized, convertStrings)
jpype._jclass.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0"
‰∏ãËΩΩÊï∞ÊçÆÊñá‰ª∂Âá∫Èîô„ÄÇË≤å‰ººÊòØpythons3ÁöÑÂØπurlsÊ†ºÂºèÁöÑÈìæÊé•ÁöÑËØªÂèñÂÆâÂÖ®ÈóÆÈ¢ò,"![U9L7}ZG)Y2{ %V3D29$UD7B](https://user-images.githubusercontent.com/44012390/71877670-c03dfd00-3164-11ea-8637-b658cfe518c1.png)
"
hanlpÂ¶Ç‰ΩïÂØºÂÖ•Âà∞luke‰∏≠Âë¢Ôºü,"https://github.com/DmitryKey/luke
‰∏§‰∏™È°πÁõÆÁöÑissueÈáåÊàëÈÉΩÊ≤°ÊêúÂà∞Ëß£ÂÜ≥ÂäûÊ≥ïÔºåÊúâÂ§ß‰Ω¨Áü•ÈÅìÂêóÔºü"
È¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏ãËΩΩÂ§™ÊÖ¢ÔºåÂ∞ùËØï‰∫ÜÂæàÂ§öÊ¨°ÈÉΩÂ§±Ë¥•‰∫Ü,"recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)

Downloading:   0%|          | 939k/478M [07:50<2643:19:23, 50.2B/s]

ËØ∑ÈóÆÈÄöËøáÂÖ∂ÂÆÉÂ∑•ÂÖ∑‰∏ãËΩΩÂÆåÊàêÔºåÊîæÂú®Âì™‰∏™ÁõÆÂΩï‰∏ãÔºüÊàëÂú®Êó•ÂøóÈáåÁúãÂà∞‰ªé./cache/*** ÈáåÂä†ËΩΩ
"
ËØçÊÄßÂàÜÊûêÁöÑËøáÁ®ã‰∏≠Êä•Èîô,"python ÁâàÊú¨:3.7.4
hanlp  ÁâàÊú¨:2.0.0a5
Êä•Èîô‰ø°ÊÅØ:
`
Traceback (most recent call last):
  File ""/home/cy/liunx_work_26/hanlp_eg.py"", line 15, in <module>
    print(tagger.predict(['Êàë', 'ÁöÑ', 'Â∏åÊúõ', 'ÊòØ', 'Â∏åÊúõ', 'ÂíåÂπ≥']))
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/components/taggers/rnn_tagger.py"", line 47, in predict
    return super().predict(sents, batch_size)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/common/component.py"", line 445, in predict
    for output in self.predict_batch(batch, inputs=inputs):
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/hanlp/common/component.py"", line 455, in predict_batch
    Y = self.model.predict_on_batch(X)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1240, in predict_on_batch
    return training_v2_utils.predict_on_batch(self, x, standalone=True)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 556, in predict_on_batch
    return predict_on_batch_fn(inputs)  # pylint: disable=not-callable
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 267, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 717, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 891, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 543, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/wrappers.py"", line 657, in call
    initial_state=forward_state, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 644, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1147, in call
    **normal_lstm_kwargs)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py"", line 1281, in standard_lstm
    if sequence_lengths is not None else timesteps)
  File ""/home/cy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 3897, in rnn
    if mask.dtype != dtypes_module.bool:
TypeError: data type not understood
`"
Âä†ËΩΩMSRA NER È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊó∂ÂÄôÊä•Èîô,"ÁéØÂ¢É Ubuntu 18.04, Python 3.6
```
In [2]: recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_CN)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-736be980464f> in <module>
----> 1 recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_CN)

~/.local/lib/python3.6/site-packages/hanlp/__init__.py in load(save_dir, meta_filename, **kwargs)
     40     save_dir = hanlp.pretrained.ALL.get(save_dir, save_dir)
     41     from hanlp.utils.component_util import load_from_meta_file
---> 42     return load_from_meta_file(save_dir, meta_filename, **kwargs)
     43
     44

~/.local/lib/python3.6/site-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, **kwargs)
     30     obj: Component = object_from_class_path(cls, **kwargs)
     31     if hasattr(obj, 'load') and os.path.isfile(os.path.join(save_dir, 'config.json')):
---> 32         obj.load(save_dir)
     33     obj.meta['load_path'] = load_path
     34     return obj

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in load(self, save_dir, logger, **kwargs)
    225         self.load_config(save_dir)
    226         self.load_vocabs(save_dir)
--> 227         self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
    228         self.load_weights(save_dir)
    229         self.load_meta(save_dir)

~/.local/lib/python3.6/site-packages/hanlp/common/component.py in build(self, logger, **kwargs)
    236         self.transform.build_config()
    237         self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
--> 238                                                    loss=kwargs.get('loss', None)))
    239         self.transform.lock_vocabs()
    240         optimizer = self.build_optimizer(**self.config)

~/.local/lib/python3.6/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py in build_model(self, transformer, max_seq_length, **kwargs)
     37         tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(transformer)
     38         self.transform.tokenizer = tokenizer
---> 39         transformer: TFPreTrainedModel = TFAutoModel.from_pretrained(transformer, name=os.path.basename(transformer))
     40         self.transform.transformer_config = transformer.config
     41

~/.local/lib/python3.6/site-packages/transformers/modeling_tf_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    219             return TFRobertaModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
    220         elif 'bert' in pretrained_model_name_or_path:
--> 221             return TFBertModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
    222         elif 'openai-gpt' in pretrained_model_name_or_path:
    223             return TFOpenAIGPTModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)

~/.local/lib/python3.6/site-packages/transformers/modeling_tf_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    311         ret = model(model.dummy_inputs, training=False)  # build the network with dummy inputs
    312
--> 313         assert os.path.isfile(resolved_archive_file), ""Error retrieving file {}"".format(resolved_archive_file)
    314         # 'by_name' allow us to do transfer learning by skipping/adding layers
    315         # see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357

AssertionError: Error retrieving file /home/smtech/.cache/torch/transformers/987cd265ea1aa9cd7e884caf8dd86c2e764e5114ee9a14a67686c1fe05f7a26c.h5
```"
hanlpÊîØÊåÅÊÅ∂ÊÑèËØÑËÆ∫Ê£ÄÊµãÂêóÔºü,
Ëøô‰∏™ÁâàÊú¨Ê≤°ÊúâzipÂåÖÂïäÔºåÁ®ãÂ∫èÈáå‰∏ãËΩΩÊä•Èîô,"![image](https://user-images.githubusercontent.com/11661959/71723130-71dbe600-2e66-11ea-9a8d-a34ae0beb1e6.png)
"
java Áâà‰∏çÂêå module ‰∏ã‰ΩøÁî®‰∏çÂêåÁöÑÈÖçÁΩÆ,‰ΩúËÄÖ‰Ω†Â•ΩÔºå ÊàëÂú®‰∏§‰∏™‰∏çÂêåÁöÑ module ‰∏ãÈÖçÁΩÆ‰∏çÂêåÁöÑ hanlp.properties Ôºå Âú®Á¨¨‰∏â‰∏™ module  ‰∏ãË∞ÉÁî®‰∏§‰∏™ module Ôºå ÂèëÁé∞Áî®ÁöÑÈÉΩÊòØÂêå‰∏Ä‰ªΩÈÖçÁΩÆÔºå Êúâ‰ªÄ‰πàÂäûÊ≥ïËÆ© ‰∏çÂêåÁöÑ module Áî®‰∏çÂêåÁöÑ   hanlp.properties ÈÖçÁΩÆÂêóÔºü Ë∞¢Ë∞¢
java‰ª£Á†Å‰ª•ÂêéÁª¥Êä§ÂêóÔºü,‰Ω†Â•Ω‰ΩúËÄÖÔºåÊàëÂàöÂàö‰π∞‰∫Ü‰Ω†‰ª¨ÁöÑ‰π¶ÔºåÂèëÁé∞‰ªäÂ§©Ê∫êÁ†ÅÈÉΩÊç¢Êàêpy‰∫ÜÔºåËØ∑ÈóÆ‰ª•ÂêéjavaÈÉ®ÂàÜËøò‰ºöÂêåÊ≠•Êõ¥Êñ∞ÂêóÔºü
pyhanlpÂíåhanlp,"‰Ω†Â•ΩÔºå
ËØ∑ÈóÆpyhanlpÂíåhanlpÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÂë¢ÔºüÁÑ∂ÂêépyhanlpÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºåÊîØÊåÅËæìÂÖ•ÂàÜÂ•ΩËØçÁöÑÂè•Â≠êÂòõÔºüË∞¢Ë∞¢ÂõûÂ§ç ~
Êñ∞Âπ¥Âø´‰πê"
pyhanlpÈÖçÁΩÆÈóÆÈ¢òÔºöHANLP_JAR_PATHÂ∑≤ÈÖçÁΩÆÔºåÊä•Èîô‰∏çÊòØjarÊñá‰ª∂ÔºåÂ∑≤‰ΩøÁî® '/',"ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
È¶ñÈ°µÊñáÊ°£
wiki
Â∏∏ËßÅÈóÆÈ¢ò
ÊàëÂ∑≤ÁªèÈÄöËøáGoogleÂíåissueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
 ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï
ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.6

ÊàëÁöÑÈóÆÈ¢ò

HANLP_JAR_PATHÂíåHANLP_ROOT_PATHÂ∑≤ÈÖçÁΩÆ‰∏∫static
ËøêË°åfrom pyhanlp import *ÔºåÊä•ÈîôValueError: ÈÖçÁΩÆÈîôËØØ: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static ‰∏çÊòØjarÊñá‰ª∂
ÊàëÂ∑≤ÁªèÂ∞ÜÁéØÂ¢ÉÂèòÈáèHANLP_JAR_PATHÂíåHANLP_ROOT_PATHÁöÑË∑ØÂæÑÊîπ‰∏∫D:/anaconda/Lib/site-packages/pyhanlp/staticÔºåpropertiesÊñá‰ª∂ÁöÑrootË∑ØÂæÑ‰πü‰øÆÊîπ‰∫Ü"
congratsÔºÅ hanlp2.0.0 releaseÔºÅbase on pure py!,as title
pyhanlpÈÖçÁΩÆÈóÆÈ¢òÔºöHANLP_JAR_PATHÂ∑≤ÈÖçÁΩÆÔºåÊä•Èîô‰∏çÊòØjarÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
pyhanlpÂ∑≤ÈÄöËøápip install pyhanlpÂÆâË£Ö
Â∑≤‰ªéÂÆòÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜdataÊñá‰ª∂Âíåhanlp-1.7.6-release.zipÊñá‰ª∂ÔºåÂπ∂Ëß£ÂéãËá≥‰∫ÜstaticÊñá‰ª∂Â§π‰∏ã
HANLP_JAR_PATHÂíåHANLP_ROOT_PATHÂ∑≤ÈÖçÁΩÆ‰∏∫static
ËøêË°åfrom pyhanlp import *ÔºåÊä•ÈîôValueError: ÈÖçÁΩÆÈîôËØØ: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static ‰∏çÊòØjarÊñá‰ª∂"
pyhanlpÈÖçÁΩÆÈóÆÈ¢òÔºöHANLP_JAR_PATHÂ∑≤ÈÖçÁΩÆÔºåÊä•Èîô‰∏çÊòØjarÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [√ó] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

pyhanlpÂ∑≤ÈÄöËøápip install pyhanlpÂÆâË£Ö
Â∑≤‰ªéÂÆòÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜdataÊñá‰ª∂Âíåhanlp-1.7.6-release.zipÊñá‰ª∂ÔºåÂπ∂Ëß£ÂéãËá≥‰∫ÜstaticÊñá‰ª∂Â§π‰∏ã
HANLP_JAR_PATHÂíåHANLP_ROOT_PATHÂ∑≤ÈÖçÁΩÆ‰∏∫static

ËøêË°åfrom pyhanlp import *ÔºåÊä•ÈîôValueError: ÈÖçÁΩÆÈîôËØØ: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static ‰∏çÊòØjarÊñá‰ª∂"
pyhanlpÁöÑÈÖçÁΩÆÈóÆÈ¢òÔºöHANLP_JAR_PATHÔºå‰∏çÊòØjarÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
pyhanlpÂ∑≤ÈÄöËøápip install pyhanlpÂÆâË£Ö
Â∑≤‰ªéÂÆòÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜdataÊñá‰ª∂Âíåhanlp-1.7.6-release.zipÊñá‰ª∂ÔºåÂπ∂Ëß£ÂéãËá≥‰∫ÜstaticÊñá‰ª∂Â§π‰∏ã
HANLP_JAR_PATHÂíåHANLP_ROOT_PATHÂ∑≤ÈÖçÁΩÆ‰∏∫static

ËøêË°åfrom pyhanlp import *ÔºåÊä•ÈîôValueError: ÈÖçÁΩÆÈîôËØØ: HANLP_JAR_PATH=D:\anaconda\Lib\site-packages\pyhanlp\static ‰∏çÊòØjarÊñá‰ª∂
 

"
Áü≠ËØ≠ÊèêÂèñÔºöÂΩìÂ∑¶ÁÜµÊàñÂè≥ÁÜµÈÉΩÊòØ0Êó∂Ôºåscore‰∏∫NaN,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂØπÊüêÁØáÊñáÁ´†ËøõË°åÂÖ≥ÈîÆÁü≠ËØ≠ÊèêÂèñÊó∂ÔºåÂèëÁé∞Áü≠ËØ≠ÁöÑscoreÈÉΩÊòØNaNÔºåË∑üËøõÂèëÁé∞ÊòØËØçËØ≠ÁöÑÂ∑¶ÁÜµÊàñÂè≥ÁÜµÈÉΩÊòØ0ÂØºËá¥ÁöÑ

### Ëß¶Âèë‰ª£Á†Å
‰ªé MutualInformationEntropyPhraseExtractor.extractPhrase(text, size)  ->  occurrence.compute()
```
package com.hankcs.hanlp.corpus.occurrence;
public class Occurrence
{
...
    /**
     * ËæìÂÖ•Êï∞ÊçÆÂÆåÊØïÔºåÊâßË°åËÆ°ÁÆó
     */
    public void compute()
    {
        entrySetPair = triePair.entrySet();
        double total_mi = 0;
        double total_le = 0;
        double total_re = 0;
        for (Map.Entry<String, PairFrequency> entry : entrySetPair)
        {
            PairFrequency value = entry.getValue();
            value.mi = computeMutualInformation(value);
            value.le = computeLeftEntropy(value);
            value.re = computeRightEntropy(value);
            total_mi += value.mi;
            total_le += value.le;
            total_re += value.re;
        }

        for (Map.Entry<String, PairFrequency> entry : entrySetPair)
        {
            PairFrequency value = entry.getValue();
            // ÈóÆÈ¢òÂá∫Âú®‰∏ãÈù¢ËøôÂè•ÔºåÂΩìtotal_leÊàñtotal_re‰∏∫0Êó∂Ôºåscore‰∏∫NaN
            // Âõ†ÂØπÂ∑¶Âè≥‰ø°ÊÅØÁÜµ‰∏çÂ§™‰∫ÜËß£Ôºå‰∏çÁ°ÆÂÆö‰∏ãÈù¢ÁöÑÂ§ÑÁêÜÊñπÂºèÊòØÂê¶ÂèØË°åÔºö
            // ÁªôÂàÜÊØçÂä†‰∏Ä‰∏™Ë∂≥Â§üÂ∞èÁöÑÊï∞Ôºå‰æãÂ¶ÇÔºövalue.score = value.mi / total_mi + value.le / (total_le+0.0001)+ value.re / (total_re+0.0001);
            value.score = value.mi / total_mi + value.le / total_le+ value.re / total_re;   // ÂΩí‰∏ÄÂåñ
            value.score *= entrySetPair.size();
        }
    }
}
```
"
Nature is not concurrent safe. Change TreeMap to ConcurrentHashMap,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
Nature Ëøô‰∏™Á±ª‰∏çÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÔºå‰ºöÂú®Âπ∂ÂèëË∞ÉÁî®Êó∂‰∫ßÁîüÊúçÂä°Halt ÁöÑÈóÆÈ¢ò„ÄÇ
Â∞ÜNature ‰∏≠ÁöÑ TreeMap ÊîπÊàê‰∫Ü ConcurrentHashMap ÔºåËß£ÂÜ≥ÊéâËøô‰∏™ÈóÆÈ¢ò

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Which kind of model is better for keyword-set classification?,"There exists a similar task that is named text classification.

But I want to find a kind of model that the inputs are keyword set. And the keyword set is not from a sentence.

For example:
```
input [""apple"", ""pear"", ""water melon""] --> target class ""fruit""
input [""tomato"", ""potato""] --> target class ""vegetable""
```

Another example:
```
input [""apple"", ""Peking"", ""in summer""]  -->  target class ""Chinese fruit""
input [""tomato"", ""New York"", ""in winter""]  -->  target class ""American vegetable""
input [""apple"", ""Peking"", ""in winter""]  -->  target class ""Chinese fruit""
input [""tomato"", ""Peking"", ""in winter""]  -->  target class ""Chinese vegetable""
```
Thank you."
ÊåâÁÖß„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÈöè‰π¶‰ª£Á†ÅÂ§çÁé∞Êó∂ÔºåÂèëÁé∞Ëá™ÂÆö‰πâËØçÂÖ∏‰∏çËÉΩÂÆåÂÖ®ÁîüÊïà„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÊåâÁÖß„ÄäËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®„ÄãÁöÑÈöè‰π¶‰ª£Á†ÅËøõË°å3.4ËäÇÂÜÖÂÆπÁöÑÂ§çÁé∞ÔºåÂú®3.4.1ÁöÑ‚ÄúÈ¢ÑÊµã‚Äù‰∏ÄËäÇ‰∏≠Ôºå‰ΩøÁî®ËåÉ‰æã‰ª£Á†ÅË∞ÉÁî®ÂÖàÂâçÁîüÊàêÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂπ∂ÁîüÊàêÂØπÂ∫îÁöÑËØçÈ¢ëÊó∂Ôºå‰ªÖ1-gramÊ®°ÂûãÁöÑËØçÈ¢ëÁªüËÆ°ÊúâÊïàÔºångramÔºàÂÆûÈôÖ‰∏äÊòØ2gramÔºâÊ®°ÂûãÁöÑËØçÈ¢ëÁªüËÆ°Êó†Êïà„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§

1. Âà†Èô§ÂêÑËØçÂÖ∏ÁºìÂ≠òbinÊñá‰ª∂
2. ÊääÈöè‰π¶ÂØπÂ∫îÁ´†ËäÇÁöÑËåÉ‰æã‰ª£Á†ÅÂÜôÂÖ•Jupyter NotebookÔºåÊúüÈó¥Êú™Âá∫Áé∞Êä•Èîô„ÄÇ
3. ‰ª£Á†ÅÂÖ∑‰ΩìÂÜÖÂÆπ‰∏∫ÔºöÂÖàÂä†ËΩΩËá™ÂÆö‰πâÁöÑÁÆÄÊòìËØ≠ÊñôÂ∫ìÔºåÁÑ∂ÂêéÂàõÂª∫NatureDictionaryMakerÂØπË±°ÔºåËÆ≠ÁªÉÂæóÂà∞Âπ∂‰øùÂ≠ò1gram‰∏é2gramÊ®°Âûã‰∏∫ËØçÂÖ∏ÂΩ¢Âºè„ÄÇÁÑ∂ÂêéÈáçÊñ∞Ë∞ÉÁî®Ëøô‰∫õÊ®°ÂûãÊó∂ÔºåËæìÂá∫Âç¥‰∏éÂÆûÈôÖ‰∏çÁ¨¶Ôºå‰πü‰∏çÂêå‰∫éÂéü‰π¶‰∏≠ÁöÑÂ∫îÊúâËæìÂá∫„ÄÇ
      
### Ëß¶Âèë‰ª£Á†Å

```
import zipfile
import os
from pyhanlp import *
from pyhanlp.static import download, remove_file, HANLP_DATA_PATH


def test_data_path():
    """"""
    Ëé∑ÂèñÊµãËØïÊï∞ÊçÆË∑ØÂæÑÔºå‰Ωç‰∫é$root/data/testÔºåÊ†πÁõÆÂΩïÁî±ÈÖçÁΩÆÊñá‰ª∂ÊåáÂÆö„ÄÇ
    :return:
    """"""
    data_path = os.path.join(HANLP_DATA_PATH, 'test')
    if not os.path.isdir(data_path):
        os.mkdir(data_path)
    return data_path


def ensure_data(data_name, data_url):
    root_path = test_data_path()
    dest_path = os.path.join(root_path, data_name)
    if os.path.exists(dest_path):
        return dest_path
    if data_url.endswith('.zip'):
        dest_path += '.zip'
    download(data_url, dest_path)
    if data_url.endswith('.zip'):
        with zipfile.ZipFile(dest_path, ""r"") as archive:
            archive.extractall(root_path)
        remove_file(dest_path)
        dest_path = dest_path[:-len('.zip')]
    return dest_path

def my_cws_corpus():
    data_root = test_data_path()
    corpus_path = os.path.join(data_root, 'my_cws_corpus.txt')
    if not os.path.isfile(corpus_path):
        with open(corpus_path, 'w', encoding='utf8') as out:
            out.write('''ÂïÜÂìÅ\tÂíå\tÊúçÂä°
ÂïÜÂìÅ\tÂíåÊúç\tÁâ©Áæé‰ª∑Âªâ
ÊúçÂä°\tÂíå\tË¥ßÂ∏Å''')
    return corpus_path

#CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')

def train_bigram(corpus_path, model_path):
    sents = CorpusLoader.convert2SentenceList(corpus_path) #Âä†ËΩΩËØ≠ÊñôÂ∫ì
    for sent in sents:
        for word in sent:
            word.setLabel('n') #Ê†áÊ≥®
    maker = NatureDictionaryMaker()
    maker.compute(sents)
    maker.saveTxtTo(model_path)
    
def load_bigram(model_path):
    HanLP.Config.CoreDictionaryPath = model_path + '.txt' #1gramÊ®°Âûã
    HanLP.Config.BiGramDictionaryPath = model_path + '.ngram.txt' #ngramÊ®°Âûã
    CoreDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
    CoreBiGramTableDictionary = SafeJClass('com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary')
    print(CoreDictionary.getTermFrequency(""ÂïÜÂìÅ"")) # Ëé∑ÂèñÂØπÂ∫îËØçËØ≠ÁöÑËØçÈ¢ë
    print(CoreBiGramTableDictionary.getBiFrequency(""ÂïÜÂìÅ"", ""Âíå"")) #Ëé∑Âèñ‰∫åÂÖÉËØ≠Ê≥ïÁöÑÈ¢ëÊ¨°

NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')
CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')
WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')
Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')
ViterbiSegment = JClass('com.hankcs.hanlp.seg.Viterbi.ViterbiSegment')
DijkstraSegment = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')
CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')
Nature = JClass('com.hankcs.hanlp.corpus.tag.Nature')

corpus_path = my_cws_corpus()
model_path = os.path.join(test_data_path(), 'my_cws_model')
train_bigram(corpus_path, model_path)
load_bigram(model_path)
```

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
2
1
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
2
0
```

Á¨¨‰∫åË°åÁöÑËæìÂá∫È¢ëÊï∞‰∏éÂÆûÈôÖ‰∏çÁ¨¶ÔºåÂõ†‰∏∫ÂØπÂ∫îÁöÑËØçÂÖ∏my_cws_model.ngram.txt‰∏≠ÁöÑÁ°ÆËÆ∞Ëø∞‰∫ÜÔºö
ÂïÜÂìÅ@Âíå 1

Âõ†Ê≠§ËæìÂá∫ÁöÑÈ¢ëÊï∞ÁªüËÆ°ÁªìÊûúÂ∫îÂΩì‰∏∫1ËÄå‰∏çÊòØ0ÔºåËøô‰∏§‰∏™ËØç‰πüÂùáÂú®1gramÊ®°ÂûãËØçÂÖ∏‰∏≠ÊúâÂá∫Áé∞ÔºåÂ∫îÂΩì‰∏çÊòØËøôÊñπÈù¢ÁöÑÂéüÂõ†„ÄÇ

## ÂÖ∂‰ªñ‰ø°ÊÅØ

‰ΩøÁî®ÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÈÉΩÊòØÁî±maker.saveTxtTo(model_path)ÂæóÂà∞ÁöÑÔºåÊú™ÂÅö‰ªª‰ΩïÊîπÂä®„ÄÇ
Â¶ÇÊûúngramÊ®°ÂûãÂπ∂‰∏çË∞ÉÁî®Ëøô‰∏™Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåËÄåÊòØ‰ΩøÁî®Ëá™Â∏¶ÁöÑËØçÂÖ∏ÔºåÂàôËØçÈ¢ëÁªüËÆ°Âπ∂Êó†ÈóÆÈ¢ò
Â¶ÇÊûúngramËØçÂÖ∏‰∏≠Âá∫Áé∞‰∫Ü1gram‰∏çÊõæÂá∫Áé∞ÁöÑËØçÔºåÂàôÂÖ∂ËØçÈ¢ëÁªüËÆ°‰πü‰ºöÊòØ0Ôºå‰ΩÜÊòØÊàëÈÅáÂà∞ÁöÑÊÉÖÂÜµ‰∏çÂ±û‰∫éÊ≠§Á±ª„ÄÇ
ÊàëÁöÑÁ≥ªÁªüÊòØWindows10ÔºåËØ≠Ë®ÄÁéØÂ¢É‰∏∫Python 3.7.4 anacondaÁéØÂ¢É
ngramÊ®°ÂûãÁîüÊàêÁöÑÁºìÂ≠òÊñá‰ª∂Ôºömy_cws_model.ngram.txt.table.binÂ§ßÂ∞è‰ªÖÊúâ1kbÔºåÊàë‰∏çÊ∏ÖÊ•öËøôÊòØÂê¶Ê≠£Â∏∏

"
hanlp elasticsearchÊèí‰ª∂,"Â∏åÊúõÂèØ‰ª•ÂºÄÊ∫ê‰∏Ä‰∏™ealsticsearchÊèí‰ª∂ÔºåelasticsearchÁöÑÂàÜËØçÊèí‰ª∂ik,Êó†Ê≥ïÊîØÊåÅÂàÜÂ∏ÉÂºèÁöÑÂä®ÊÄÅ‰øÆÊîπËØçÂ∫ìÔºåÂ∏åÊúõÂèØ‰ª•Êúâ‰∏ÄÂ•óÂü∫‰∫éhanlpÁöÑelasticsearchÊèí‰ª∂ÔºåÂπ∂‰∏îÊîØÊåÅÂàÜÂ∏ÉÂºè„ÄÇ"
Âä†ËΩΩCoreDictionaryPathÊòæÁ§∫SSLÂíåURLËØÅ‰π¶È™åËØÅÂ§±Ë¥•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®ÊúÄÊñ∞Áâà‰π¶Á±çP35È°µ‰ª£Á†ÅÂä†ËΩΩHanLPÁöÑÊ†∏ÂøÉËØçÂÖ∏Â∫ìÔºåÂá∫Áé∞SSLÔºåÂèäURLËØÅ‰π¶È™åËØÅÂ§±Ë¥•„ÄÇ‰ª£Á†ÅÊù•Ëá™ch02ÁöÑutitil.py.
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)>

ËØïÁùÄ‰ΩøÁî®ÂÖ®Â±Ä‰∏çÊ£ÄÊµãSSLÂèäURlÊú™ËÉΩËß£ÂÜ≥

## Â§çÁé∞ÈóÆÈ¢ò
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
Ê∑ªÂä†‰ª£Á†Å‰ª•Êó†ËßÜSSLËØÅ‰π¶Ê£ÄÊµãÔºåÊó†Êïà



### ‰ª£Á†ÅÂ§çÁé∞

```
  from pyhanlp import *
# ÂÖ®Â±ÄÂèñÊ∂àËØÅ‰π¶È™åËØÅ

import ssl
ssl._create_default_https_context = ssl._create_unverified_context



def load_dictionary():
    '''
    Âä†ËΩΩHanLp‰∏≠ÁöÑminiËØçÂ∫ì
    :return: ‰∏Ä‰∏™setÂΩ¢ÂºèÁöÑËØçÂ∫ì
    '''
    IOUtil=JClass('com.hankcs.hanlp.corpus.io.IOUtil')
    path = HanLP.Config.CoreDictionaryPath.replace('.txt', '.mini.txt')
    dic = IOUtil.loadDictionary([path])
    return set(dic.keySet())

if __name__ == '__main__':
    dic = load_dictionary()
    print(len(dic))
    print(list(dic)[0])
```
### ÊúüÊúõËæìÂá∫

ËøîÂõû‰∏Ä‰∏™setÁöÑÂΩ¢ÂºèÁöÑËØçÂ∫ì



## ÂÖ∂‰ªñ‰ø°ÊÅØ

MAC Python3.7, Pycharm

"
tfidfÔºåidfÁöÑÊï∞ÊçÆÂèØ‰ª•ÈÄöËøáÂä†ËΩΩidfÊñá‰ª∂ÂæóÂà∞,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
tfidfÔºåidfÁöÑÊï∞ÊçÆÂèØ‰ª•ÈÄöËøáÂä†ËΩΩidfÊñá‰ª∂ÂæóÂà∞„ÄÇ

```
    /**
     * Âä†ËΩΩËá™ÂÆö‰πâidfÊñá‰ª∂
     *
     * @param idfPath
     */
    public void loadIdfFile(String idfPath){
        String line = null;
        boolean first = true;
        try
        {
            idf  = new HashMap<String, Double>();
            BufferedReader bw = new BufferedReader(new InputStreamReader(IOUtil.newInputStream(idfPath), ""UTF-8""));
            while ((line = bw.readLine()) != null)
            {
                if (first)
                {
                    first = false;
                    if (!line.isEmpty() && line.charAt(0) == '\uFEFF')
                        line = line.substring(1);
                }
                String lineValue[] = line.split("" "");
                idf.put(lineValue[0],Double.valueOf( lineValue[1]));
            }
            bw.close();
        }
        catch (Exception e)
        {
            logger.warning(""Âä†ËΩΩ"" + idfPath + ""Â§±Ë¥•Ôºå"" + e);
            throw new RuntimeException(""ËΩΩÂÖ•ÂèçÊñáÊ°£ËØçÈ¢ëÊñá‰ª∂"" + idfPath + ""Â§±Ë¥•"");
        }

    }
```


"
Ê¨¢ËøéÂèÇÂä†2019 HanLPÊäÄÊúØ‰∫§ÊµÅ‰ºö,"Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑHanLP2.0Â∞ÜÂú®Á¨¨‰∫åÂ±äÈóÆÈÅìÂ¥ÇÂ±±¬∑‰∫∫Â∑•Êô∫ËÉΩ‰∏éÂ§ßÊï∞ÊçÆÈ´òÂ≥∞ËÆ∫Âùõ‰∏äÊ≠£ÂºèÂèëÂ∏ÉÔºåHanLP2.0Â∞ÜÊúâÂ§öÈ°πÈù©ÂëΩÊÄßÁöÑÁ™ÅÁ†¥ÔºåËØöÈÇÄÂêÑ‰ΩçÂºÄÂèëËÄÖËéÖ‰∏¥ËÆ∫Âùõ‰∫§ÊµÅÂøÉÂæó‰∏é‰Ωì‰ºö„ÄÇ

ËÆ∫ÂùõÂ∞Ü‰∫é:2019Âπ¥12Êúà27Êó•‰∏äÂçà9:00ÔºåÂú®ÈùíÂ≤õÂ¥ÇÂ±±Âå∫Êµ∑Â§©Â§ßÂâßÈô¢ÈÖíÂ∫óÂè¨ÂºÄ„ÄÇ

ËØ¶ÁªÜÂú∞ÂùÄ:ÈùíÂ≤õÂ∏ÇÂ¥ÇÂ±±Âå∫‰∫ëÂ≤≠Ë∑Ø5Âè∑ÔºåÊµ∑Â§©Â§ßÂâßÈô¢ÈÖíÂ∫ó„ÄÇ

ÈúÄË¶ÅÂú®‰ºö‰∏äÊºîËÆ≤ÂíåÂÆâÊéí‰ΩèÂÆøÁöÑ‰∫∫ÂëòËØ∑‰∏éËÆ∫ÂùõÁªÑÂßî‰ºöËÅîÁ≥ª

ËÅîÁ≥ª‰∫∫:ËñõÊå£(ÁîµËØù:18605325520)

Èíü‰Ω©‰Ω©(150-6688-0317)

ËØ¶ÁªÜ‰ªãÁªçÔºöhttp://www.hdb-cdn4.cn/party/7pnca.html‚Äã"
pyhanlp/tests/book/ch02/dat.pyÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

python: 3.7.3
jpype: 0.7.0
ËøêË°åpyhanlp/tests/book/ch02/dat.pyÂá∫Áé∞ÈîôËØØ„ÄÇ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![image](https://user-images.githubusercontent.com/22760903/71138277-e8150180-2245-11ea-9ac3-9f5dda41fc87.png)

"
test‰∏≠demoÁöÑ‰æãÂ≠êÂú®pycharmÊä•Èîô ,"Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.5.jar   Âü∫‰∫épython3.7ÁöÑanaconda
12Êúà17Êó• ÈÄöËøápip install pyhanlpÂåÖ
Â¶ÇÊà™ÂõæÊâÄÁ§∫ÔºåÂú®copyÂ≠¶‰π† pyhanlp demo ‰ª£Á†Å‰∏äÔºåpycharmÊä•Èîô
![image](https://user-images.githubusercontent.com/17291290/70996821-8f8f1880-210e-11ea-8cc3-bd014d172aad.png)

ËøêË°åÁöÑÊòØdemo‰∏≠ÁöÑ‰æãË°å‰ª£Á†Åhttps://github.com/hankcs/pyhanlp/blob/master/tests/demos/demo_sentiment_analysis.py
## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.5.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.5.jar  

## ÊàëÁöÑÈóÆÈ¢ò
Â¶ÇÊà™ÂõæÊâÄÁ§∫ÔºåÂú®copyÂ≠¶‰π† pyhanlp demo ‰ª£Á†Å‰∏äÔºåpycharmÊä•Èîô

## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
CoreStopWordDictionary.add() Â¶Ç‰ΩïÊåÅ‰πÖÂåñ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

CoreStopWordDictionary.add() Â¶Ç‰ΩïÂ∞ÜÂÅúÁî®ËØçÂÜôÂÖ•stopwords.txtÔºü
ËøòÊòØËØ¥Âè™ËÉΩÊâãÂä®‰øÆÊîπstopword.txtÊàñËÄÖÁî®Êñá‰ª∂ÊµÅÁöÑÊñπÂºè„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
  public class DemoStopWord
{
    public static void main(String[] args) throws Exception {
        System.out.println(CoreStopWordDictionary.contains(""‰∏ÄËµ∑""));
        stopwords();
        add();
        stopwords();
    }

    public static void add() {
        boolean add = CoreStopWordDictionary.add(""‰∏ÄËµ∑"");
        CoreStopWordDictionary.reload();
        System.out.println(""add = "" + add);
    }
    public static void stopwords() throws Exception {
        String con = ""Êàë‰ª¨‰∏ÄËµ∑ÂéªÈÄõË∂ÖÂ∏ÇÔºåÊàë‰ª¨ÂÖàÂéªÂüéË•øÈì∂Ê≥∞ÔºåÊàë‰ª¨ÂÜçÂéªÂüéÂçóÈì∂Ê≥∞„ÄÇÁÑ∂ÂêéÊàë‰ª¨ÂÜç‰∏ÄËµ∑ÂõûÂÆ∂"";
        List<String> strings = HanLP.extractKeyword(con, 5);
        System.out.println(""strings = "" + strings);
    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
ÊàëÊúüÊúõÊòØË∞ÉÁî®CoreStopWordDictionary.add(""‰∏ÄËµ∑"");ÂêéÔºåÂ∞ÜÂÅúÁî®ËØç""‰∏ÄËµ∑""Âä†ÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏„ÄÇÁÑ∂ÂêéË∞ÉÁî®CoreStopWordDictionary.reload();ÈáçÊñ∞Âä†ËΩΩÂ≠óÂÖ∏ÂíåÁºìÂ≠òËææÂà∞Âä®ÊÄÅ‰øÆÊîπÂÅúÁî®ËØçËØçÂÖ∏ÁöÑÂäüËÉΩ„ÄÇ
```
false
strings = [‰∏ÄËµ∑, Èì∂Ê≥∞, Âüé, ÂÜç, Ë•ø]
add = true
strings = [Âüé, Ë•ø, Èì∂Ê≥∞, ÂÖà, ÂÜç]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
CoreStopWordDictionary.add(""‰∏ÄËµ∑"");Âπ∂Ê≤°ÊúâÂ∞Ü‚Äú‰∏ÄËµ∑‚ÄùÂÜôÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏ÔºåÂè™ÊòØÂú®ÂÜÖÂ≠ò‰∏≠„ÄÇË∞ÉÁî®CoreStopWordDictionary.reload();‰ª•ÂêéÂÅúÁî®ËØçÂ≠óÂÖ∏ÈáåÊ≤°Êúâ‚Äú‰∏ÄËµ∑‚Äù
```
false
strings = [‰∏ÄËµ∑, Èì∂Ê≥∞, Âüé, ÂÜç, Ë•ø]
add = true
strings = [‰∏ÄËµ∑, Èì∂Ê≥∞, Âüé, ÂÜç, Ë•ø]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÊâÄ‰ª•ËØ¥Áé∞Âú®ËøòÊòØË¶ÅÊâãÂä®‰øÆÊîπstopword.txtÊàñËÄÖÊòØ‰ΩøÁî®Êñá‰ª∂ÊµÅÁöÑÂΩ¢ÂºèÊù•‰øÆÊîπÂØπ‰πàÔºüÈÇ£CoreStopWordDictionary.add()ÂíåCoreStopWordDictionary.remove()Ëøô‰∏§‰∏™ÊñπÊ≥ïÂ∫îËØ•ÊÄé‰πàÁî®Ôºü
"
hanlp segment,"ÂÆâË£ÖÊàêÂäüÂêéÔºåËøêË°å hanlp segment 

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f649eb21b6e, pid=40945, tid=140070366816064
#
# JRE version: OpenJDK Runtime Environment (7.0_91) (build 1.7.0_91-mockbuild_2015_11_20_16_53-b00)
# Java VM: OpenJDK 64-Bit Server VM (24.91-b01 mixed mode linux-amd64 compressed oops)
# Derivative: IcedTea 2.6.2
# Distribution: CentOS Linux release 7.1.1503 (Core) , package rhel-2.6.2.3.el7-x86_64 u91-b00
# Problematic frame:
# V  [libjvm.so+0x616b6e]
#
# Core dump written. Default location: /root/UPA-Python-1.0.0-1/core or core.40945
#
# An error report file with more information is saved as:
# /tmp/jvm-40945/hs_error.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
#"
ÊéíÈáçÂ∫ì,‰Ω†Â•ΩÔºåËøôËæπËÄÉËôëËøáÊéíÈáçÂ∫ìÁöÑÈúÄÊ±ÇÂòõÔºåÁ±ª‰ºº‰∫éÊØï‰∏öËÆ∫ÊñáÁöÑÈÇ£ÁßçÁõ∏‰ººÂ∫¶ÁöÑËÆ°ÁÆóÔºåÂª∫ËÆÆ‰∏™‰∏Ä‰∏™Â∫ìÔºåÁÑ∂ÂêéÊääÁõ∏‰ººÂ∫¶È´òÁöÑÂàóÂá∫Êù•ÔºåÊàñËÄÖjavaÂÆûÁé∞Êúâ‰ªÄ‰πàÂ•ΩÁöÑÂÆûÁé∞ÊñπÊ°àÂòõÔºü
ÂÖ≥‰∫éËØ≠‰πâ‰æùÂ≠òÂí®ËØ¢,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
ÂÖ≥‰∫éÊñ∞‰π¶Âí®ËØ¢
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Â§ß‰Ω¨‰Ω†Â•ΩÔºåÂàöÂÖ•Êâã‰∫Ü‰Ω†ÁöÑÊñ∞‰π¶ÔºåÈÇ£‰∏™ÊÄùÁª¥ÂØºÂõæÁâπÂà´Ê£íÔºåÊÄùÁª¥ÂØºÂõæ‰∏≠ÂØπËØ≠‰πâ‰æùÂ≠òÈáå‰ªãÁªçÁöÑÁÆóÊ≥ïÂú®Âì™ÂÑøÊúâ‰ªãÁªçÂêóÔºåÊòØÂê¶ÊúâÂºÄÊ∫êÁöÑÂèØ‰ª•ÊãúËØªÂ≠¶‰π†Ôºü
"
 raise ValueError( ValueError: ÈÖçÁΩÆÈîôËØØ: Êï∞ÊçÆÂåÖ C:/Users/15761/Anaconda3/envs/python38/Lib/site-packages/pyhanlp/static\data ‰∏çÂ≠òÂú®ÔºåËØ∑‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑroot,"## ÊàëÁöÑÈóÆÈ¢ò
Êä•Èîô‰ø°ÊÅØÔºö
(python38) C:\Users\15761>hanlp
Traceback (most recent call last):
  File ""c:\users\15761\anaconda3\envs\python38\lib\runpy.py"", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\15761\anaconda3\envs\python38\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\15761\Anaconda3\envs\python38\Scripts\hanlp.exe\__main__.py"", line 4, in <module>
  File ""c:\users\15761\anaconda3\envs\python38\lib\site-packages\pyhanlp\__init__.py"", line 145, in <module>
    _start_jvm_for_hanlp()
  File ""c:\users\15761\anaconda3\envs\python38\lib\site-packages\pyhanlp\__init__.py"", line 75, in _start_jvm_for_hanlp
    raise ValueError(
ValueError: ÈÖçÁΩÆÈîôËØØ: Êï∞ÊçÆÂåÖ C:/Users/15761/Anaconda3/envs/python38/Lib/site-packages/pyhanlp/static\data ‰∏çÂ≠òÂú®ÔºåËØ∑‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑroot

ÊàëÂ∑≤ÁªèÊåâ‰∏ãÈù¢ÈìæÊé•ÈáåÁöÑÂÅö‰∫Ü,Êää`data`
https://github.com/hankcs/pyhanlp/wiki/%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AE
![image](https://user-images.githubusercontent.com/39977054/70859711-b7d21800-1f52-11ea-88f3-062572949e2c.png)

‰ΩÜÊòØËøòÊòØ‰∏çË°å
![image](https://user-images.githubusercontent.com/39977054/70859720-d6381380-1f52-11ea-8fd5-6d8243eaebde.png)



"
ÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÂ≠óÂÖ∏t2s.txt‰∏≠ ËäùÂ£´=‰πæÈÖ™ÊòØ‰∏çÊòØÈîôÁöÑÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂØπ‰∫éËá™ËÆ≠ÁªÉÊ®°ÂûãCRFNERecognizerÁöÑrecognize ÊñπÊ≥ï‰∏çËÉΩËØÜÂà´Âá∫Ê≠£Á°ÆÁöÑnertagsÔºå‰ΩÜÊòØPerceptronNERecognizerÂç¥ÂèØ‰ª•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
È¶ñÂÖàÈùûÂ∏∏ÊÑüË∞¢ÊÇ®Ëøô‰∏™È°πÁõÆÔºåÊàëÂú®Áî®CRFÁöÑÊñπÂºèËÆ≠ÁªÉÊ®°ÂûãÁöÑÊó∂ÂÄôÔºåÂΩìË∞ÉÁî®CRFNERecognizerÁöÑrecognize ÊñπÊ≥ï‰∏çËÉΩËØÜÂà´Âá∫Â§çÂêàËØçÊ≠£Á°ÆÁöÑnertagsÔºåËØÜÂà´Âá∫Êù•ÁöÑÈÉΩÊòØOÔºåÂÆûÈôÖ‰∏äÊàëÂ∑≤ÁªèÊåâÁÖß‰∫∫Ê∞ëÊó•Êä•ÁöÑÊ†ºÂºèÁî®‰∏≠Êã¨Âè∑Ê†áËÆ∞Âá∫Êù•‰∫ÜÔºåÂèØÂΩìÊàëÁî®ÊÑüÁü•Êú∫ÁöÑÊñπÂºèÔºåÂç≥PerceptronNERecognizer Ëøô‰∏™ÂØπË±°  ÊòØÂèØ‰ª•ËØÜÂà´Âá∫ÊàëÁöÑÂ§çÂêàËØçÁöÑÊ≠£Á°ÆnertagsÁöÑÔºåÁ®çÂêéÊàëÊääCRFÁöÑ‰ª£Á†ÅÂàóÂá∫Êù•ÔºåËØ∑ÊÇ®Áúã‰∏ãÊòØÂê¶ÊúâÈóÆÈ¢ò„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà Áúã‰∏ãÊàëÁöÑÊ†áËÆ∞ËØ≠Êñô Â¶Ç‰∏ãÔºö
[ÊΩòÂ©∑/nr Ê∞®Âü∫ÈÖ∏/n Ê¥ó/v Êä§/v Â•óË£Ö/n ‰π≥Ê∂≤/n ‰øÆÊä§/v Ê¥óÂèë/v Ê∞¥/n]/baj 500/m ml/q Êê≠ÈÖç/v [3/m ÂàÜÈíü/q Â•áËøπ/n Êä§ÂèëÁ¥†/nz]/baj 70/m ml Êñ∞Êóß/q ÂåÖË£Ö/n ÈöèÊú∫/d ÂèëË¥ß/v Ôºå/w ‰ª∑Ê†º/n Ôø•/v 59.90/m Ôºå/w ÈÄÇÂêà/v Â§¥ÁöÆ/n Ôºö/w Âπ≤ÊÄß/n Ôºå/w ÂäüÊïà/n Ôºö/w Ê∞¥/n Ê∂¶/v „ÄÇ/w
ËøôÊòØËÆ≠ÁªÉËØçÊÄßÊ®°ÂûãÂíånerÊ®°ÂûãÁöÑËØ≠ÊñôÔºåÂàÜËØçÊ®°ÂûãÁöÑËØ≠Êñô ÊòØÂéªÊéâ ËØçÊÄßÊ†áËÆ∞Âíå‰∏≠Êã¨Âè∑‰πãÂêéÁöÑ ÂçïËØç
2. ÁÑ∂Âêé Áî®hanlpÊèê‰æõÔºàCRFSegmenterÔºåCRFPOSTaggerÔºåCRFNERecognizer‰∏â‰∏™Êé•Âè£ÂØπË±°ÔºâÁöÑCRFÊé•Âè£ÂàÜÂà´ËÆ≠ÁªÉÂàÜËØçÊ®°ÂûãÔºåËØçÊÄßÊ®°Âûã‰ª•ÂèäNERÊ®°Âûã
3. Êé•ÁùÄ Áî®CRFNERecognizer ÁöÑrecognize  ÊñπÊ≥ï ÊúüÊúõËé∑ÂèñÊØè‰∏™ÂçïËØçÂØπÂ∫îÁöÑnertag

### Ëß¶Âèë‰ª£Á†Å

```
    str1 = 'ÊΩòÂ©∑Ê∞®Âü∫ÈÖ∏Ê¥óÊä§Â•óË£Ö‰π≥Ê∂≤‰øÆÊä§Ê¥óÂèëÊ∞¥500mlÊê≠ÈÖç3ÂàÜÈíüÂ•áËøπÊä§ÂèëÁ¥†70ml Êñ∞ÊóßÂåÖË£ÖÈöèÊú∫ÂèëË¥ßÔºå‰ª∑Ê†ºÔø•59.90ÔºåÈÄÇÂêàÂ§¥ÁöÆÔºöÂπ≤ÊÄßÔºåÂäüÊïàÔºöÊ∞¥Ê∂¶„ÄÇ'
    seg = CRFSegmenter(cws_path)
    wordList = seg.segment(str1)
    tagger = CRFPOSTagger(pos_path)
    pos_tags = tagger.tag(wordList)
    customNerTags = ['baj']
    ner = CRFNERecognizer(train_model+"".txt"",customNerTags)
    arrNer = ner.recognize(list(wordList),list(pos_tags))
    ner_tags = list([p for p in list(arrNer)])
    print(ner_tags)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
['B-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'M-baj', 'E-baj', 'O', 'O', 'O', 'B-baj', 'M-baj', 'M-baj', 'E-baj', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

‰ª•‰∏äÊúüÊúõËæìÂá∫ÁöÑÁªìÊûúÊòØÊàëÁî®ÊÑüÁü•Êú∫ÁöÑÁõ∏ÂÖ≥ÂØπË±°ÔºàPerceptronSegmenterÔºåPerceptronPOSTaggerÔºåPerceptronNERecognizerÔºâËæìÂá∫ÁöÑ"
1.6.2ÁâàÊú¨ÁöÑÂ∞ùËØï‰∏ãËΩΩÂæàÂ§öÊ¨°‰∫ÜÔºåÂ∞±ÊòØ‰∏ãËΩΩ‰∏ç‰∫ÜÔºåËØ∑ÈóÆÊúâ‰∏ãËΩΩËøáÁöÑÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êúâbert Áõ∏ÂÖ≥ÁöÑÁ±ªÂ∫ì‰πà,ÊúâÊääbert Êï¥ÂêàÂà∞hanlpÈáåÈù¢ÁöÑËÆ°Âàí‰πà(*_*)
jsp‰∏≠Ë∞ÉÁî®ÂàÜËØçÂô®ÊèêÁ§∫ HanLP cannot be resolved,"v1.7.5
Âú®jsp‰∏≠Êó†Ê≥ïË∞ÉÁî®ÂàÜËØçÂô®Ôºå‰ºöÊèêÁ§∫ Ôºöorg.apache.jasper.JasperException: Unable to compile class for JSP: 
...
HanLP cannot be resolved
<%@ page contentType=""text/html;charset=UTF-8"" language=""java"" %>
<%@ page import=""ne.aaa"" %>
<%@ page import=""java.util.List"" %>
<%@ page import=""java.util.ArrayList"" %>
<%@ page import=""com.hankcs.hanlp.HanLP"" %>
<%
    List list = new ArrayList();  
   list = HanLP.segment(""***"");

%>
<html>
<head>
    <title>Title</title>
</head>
<body>
</body>
</html>


Âú®Ë∞ÉÁî®HanLP.segment(""***"");ÁöÑÊó∂ÂÄôÁõ¥Êé•Êä•Èîô"
‰øÆÂ§çÔºöÂä†ËΩΩËá™ÂÆö‰πâÂÅúÁî®ËØçÊñá‰ª∂Êó†Êïà,"- ‰øÆÂ§çÔºöÂä†ËΩΩËá™ÂÆö‰πâÂÅúÁî®ËØçÊñá‰ª∂Êó†Êïà

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
ÊâìÂåÖÂêéÁºñËØëÂêéÂ≠óÂÖ∏ÊÄé‰πàÂä†ËΩΩÔºüË∑ØÂæÑÊÄé‰πàËÆæÁΩÆÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö<version>portable-1.7.5</version>

## ÊàëÁöÑÈóÆÈ¢ò

      ÊâìÂåÖÂêéÁºñËØëÂêéÂ≠óÂÖ∏ÊÄé‰πàÂä†ËΩΩÔºüË∑ØÂæÑÊÄé‰πàËÆæÁΩÆÔºü
      Âõ†‰∏∫Âú®Êú¨Âú∞ideaÊµãËØïÁöÑÊó∂ÂÄôÔºåÂèØ‰ª•Â∞ÜrootËÆæÁΩÆÊàêÂØπÂ∫îÁöÑË∑ØÂæÑÔºåËÄå‰∏îÊòØÂèØ‰ª•ËøêË°åÊàêÂäüÁöÑÔºå‰ΩÜÊòØÊâìÂåÖÁºñËØëÂêéÊÄé‰πàËøõË°åË∑ØÂæÑÁöÑËÆæÁΩÆÂë¢Ôºü




"
Âú®Á∫øÊºîÁ§∫‰∏é‰ª£Á†ÅÂè•Ê≥ïÂàÜÊûêÁªìÊûúÂ≠òÂú®Â∑ÆÂºÇ„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->
hanlp-1.7.5ÔºåmasterÂùáÂ≠òÂú®Ê≠§ÈóÆÈ¢ò
## ÊàëÁöÑÈóÆÈ¢ò
Âú®Á∫øÊºîÁ§∫‚Äúhttp://www.hanlp.com/?sentence=ÊâìÂºÄÁ©∫Ë∞ÉË∞ÉÈ´òÊ∏©Â∫¶‚Äù
‰∏éÊ∫êÁ†ÅÁªìÊûú‰∏çÂêå
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂØπÊØîÂú®Á∫øÊºîÁ§∫ÂíåmasterÔºåhanlp-1.7.5‰∏≠ÁöÑDemoDependencyParserÁöÑ‰æùÂ≠òÂàÜÊûêÁªìÊûúÔºåËß£ÊûêÂè•Â≠ê‚ÄúÊâìÂºÄÁ©∫Ë∞ÉË∞ÉÈ´òÊ∏©Â∫¶‚ÄùÔºåÂæóÂà∞ÁöÑÁªìÊûúÊòØ‰∏çÂêåÁöÑÔºåÊòØ‰ΩøÁî®‰∫Ü‰∏çÂêåÊ®°ÂûãÂêó
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Ê≤°Êúâ‰øÆÊîπ‰ªª‰Ωï
master‰∏éjarÂåÖÁªìÊûú„ÄÇ
1	ÊâìÂºÄ	ÊâìÂºÄ	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
2	Á©∫Ë∞É	Á©∫Ë∞É	n	n	_	1	Âä®ÂÆæÂÖ≥Á≥ª	_	_
3	Ë∞ÉÈ´ò	Ë∞ÉÈ´ò	v	v	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	Ê∏©Â∫¶	Ê∏©Â∫¶	n	n	_	1	Âä®ÂÆæÂÖ≥Á≥ª	_	_
Âú®Á∫øÊºîÁ§∫ÁªìÊûú
![ÂõæÁâá20191207150048](https://user-images.githubusercontent.com/27552782/70370454-7ca46900-1902-11ea-8551-723f55489536.png)
"
import pyhanlpÊó∂Êä•ÈîôÔºàImportError: numpy.core.multiarray failed to importÔºâ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö0.1.57
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö0.1.57

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®Ê†πÊçÆ‰π¶‰∏äÊ≠•È™§ÂÆâË£ÖÂ•Ω‰πãÂêéÔºåimport pyhanlpÊó∂Êä•Èîô
C:\Users\ZzT>python
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyhanlp
D:\Anaconda3\lib\site-packages\numpy\core\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:
D:\Anaconda3\lib\site-packages\numpy\.libs\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll
D:\Anaconda3\lib\site-packages\numpy\.libs\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll
  stacklevel=1)
ImportError: numpy.core.multiarray failed to import
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 12, in <module>
    from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM, java, \
  File ""D:\Anaconda3\lib\site-packages\jpype\__init__.py"", line 17, in <module>
    from ._jpackage import *
  File ""D:\Anaconda3\lib\site-packages\jpype\_jpackage.py"", line 19, in <module>
    import _jpype
ImportError: numpy.core.multiarray failed to import


ÁªèÊü•ËØ¢ÁôæÂ∫¶ÔºåÂàùÊ≠•Âà§ÂÆöÊòØnumpyÁâàÊú¨‰∏çÂåπÈÖçÈÄ†ÊàêÁöÑÔºå‰ΩÜÊòØÂ∞ùËØï‰∫Ünumpy1.16.0,1.16.5,1.17.4 ‰πãÂêéÔºåÂùáÊ≤°ÊúâËß£ÂÜ≥ÔºåÊ±ÇÊôóÂì•Ëß£Á≠îÔºåÈùûÂ∏∏ÊÑüË∞¢





 

"
javaÁéØÂ¢ÉÂ∑≤ÁªèÈÖçÁΩÆÂ•ΩÔºåËøêË°åhanlpÊó∂Â∞±ÊòØÊâæ‰∏çÂà∞java,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
javaÁéØÂ¢ÉÂ∫îËØ•ÊòØÈÖçÁΩÆÂ•Ω‰∫ÜÔºå JDK 13
C:\Users\ÊòüÂõûÂçÅ‰πù>java -version
java version ""13.0.1"" 2019-10-15
Java(TM) SE Runtime Environment (build 13.0.1+9)
Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)

Âú®‰ΩøÁî®pyhanlpÊó∂Â∞±Âá∫Áé∞‰∫Ü‰∏ãÈù¢ÁöÑÈóÆÈ¢ò
>>> from pyhanlp import *
Êâæ‰∏çÂà∞JavaÔºåËØ∑ÂÆâË£ÖJDK8Ôºöhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
ÊòØÂê¶ÂâçÂæÄ https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html Ôºü(y/n)

ËøôÊòØ‰ªÄ‰πàÂéüÂõ†ÂïäÔºü


"
Ê∑ªÂä†ÂàÜÊûêÂ≠óÂÖ∏Êó∂ÔºåËØçÈ¢ëËæìÂÖ•È°πÂ∫îÂ¶Ç‰ΩïÁêÜËß£Âπ∂ÂÆûÈôÖÂ∫îÁî®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
È¶ñÈ°µÊºîÁ§∫‰ª£Á†ÅÔºö
8. Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ 
‰∏≠ÁöÑÊºîÁ§∫ËØ≠Âè•Ôºö
  System.out.println(CustomDictionary.add(""ÂçïË∫´Áãó"", ""nz 1024 n 1""));
ÂÖ∂‰∏≠ 1024 Ëøô‰∏™ËØçÈ¢ëÔºåÂ∫îËØ•Â¶Ç‰ΩïÁêÜËß£Âèä‰ΩøÁî®Ôºü

ÂÅáËÆæÂÆûÈôÖ‰∏≠ÊúâÂ§ßÊñáÊú¨ÊÆµ txtÔºåÂõ†‰∏∫‰∏öÂä°È¢ÜÂüüÈóÆÈ¢òÔºåÊàëÁõÆÂâçÊúâ‰∏Ä‰∏™‰∏ìÊúâÂêçËØç wordÔºåÈúÄË¶ÅÊ∑ªÂä†ÂÖ•Âä®ÊÄÅÂ≠óÂÖ∏„ÄÇÈÇ£ÊàëÂΩìÁÑ∂ÂèØ‰ª•‰∫∫‰∏∫ÁöÑÂÆö‰πâ‰∏Ä‰∏™ÂÆÉÁöÑËØçÊÄßÔºå‰ΩÜËØçÈ¢ëÊàëÂ¶Ç‰ΩïËÉΩÁü•ÈÅìÂë¢ÔºüËÄåËøô‰∏™Â≠óÂÖ∏ÔºåÊàëÊòØÊãøÂéªÂàÜÊûêtxtÔºåÂåÖÊã¨‰ª•ÂêéÂèØËÉΩÂá∫Áé∞ÁöÑ txt ÁöÑ„ÄÇÂ¶ÇÊûúÊåâÁÖßËØçÈ¢ëÁöÑÊ±âËØ≠ÊÑèÊÄùÔºåÂ∞±ÊòØËØçÂá∫Áé∞ÁöÑÊ¨°Êï∞„ÄÇÂÆûÈôÖ‰∏≠ÊàëËøûÊñáÊú¨ÈÉΩÂú®‰∏öÂä°Á≥ªÁªü‰∏ÄÁõ¥Âä®ÊÄÅÂ¢ûÂä†ÁùÄÔºåÊàëÊòæÁÑ∂‰∏çÂèØËÉΩÁü•ÈÅìÂÆÉ‰ºöÂá∫Áé∞Â§öÂ∞ëÊ¨°Âêß„ÄÇ

ÊïÖËÄåÔºåÊúâÂ¶Ç‰∏äÈóÆÈ¢ò„ÄÇ


## ÂÖ∂‰ªñ‰ø°ÊÅØ

‰∏çÁü•ÈÅìÊòØÂê¶ËÉΩËÅîÁ≥ªÂà∞ ‚ÄúËù¥Ëù∂ÊïàÂ∫î‚Äù Á´ôÁÇπÁöÑÁÆ°ÁêÜÂëòÔºåÂêêÊßΩ‰∏Ä‰∏ãÈÇ£‰∏™Á´ôÁÇπ„ÄÇ
1Ôºå‰∏çÊòéÂéüÂõ†ÁöÑ‰∏™‰∫∫Êó†Ê≥ïÊ≥®ÂÜåÔºåÊèêÁ§∫ ÔºöipÂú∞ÂùÄÂ∑≤ÁªèÊ≥®ÂÜåÔºåËÅîÁ≥ªÁÆ°ÁêÜÂëò„ÄÇ‰∏≠ÂõΩÊúâÁã¨Á´ãIPÁöÑ‰∏™‰∫∫ÊúâÂá†‰∏™ÂïäÔºåÊàë‰ª¨ÂÖ¨Âè∏ÂØπÂ§ñÈÉΩÊ≤°ÊúâÁã¨Á´ãIPÂêßÔºåËøô‰∏™Âà§ÂÆöÈôêÂà∂Ôºå‰Ωú‰∏∫‰∏™‰∫∫ÔºåÊàëÈÉΩ‰∏çÁü•ÈÅìÊÄéÊ†∑Â§ÑÁêÜ„ÄÇÊòØËÅîÁ≥ªbbsÁÆ°ÁêÜÂëòÂë¢ÔºåËøòÊòØËÅîÁ≥ªÊåÅÊúâËøô‰∏™IPÁöÑÁÆ°ÁêÜÂëòÔºåËøòÊòØÊàë‰ª¨ËøôÊ†ãÂ§ßÂé¶ÁöÑÁâ©‰∏öÂõ¢Èòü„ÄÇ
2Ôºågithub ‰ºº‰πéÂèØ‰ª•ÊéàÊùÉÁôªÂΩïÔºå‰ΩÜ‰πüÂæàÂ•áÊÄ™ÔºåÊàëÂ∑≤ÁªèÂºπÂá∫Á™óÂè£ÊéàÊùÉÊàêÂäü‰∫ÜÔºå‰ΩÜÊ≤°ÊúâÁõ¥Êé•ÁôªÂΩï‰∏äÂéªÔºåÂºπÂá∫‰∏Ä‰∏™ÁôªÂΩï/Ê≥®ÂÜåÁöÑÂ±ÇÔºåÊàë‰πü‰∏çÁü•ÈÅìÁÇπÂì™‰∏™ÔºåÂèçÊ≠£ÁÇπÁôªÂΩïÔºåÁôªÂΩï‰∏ç‰∫ÜÔºåÁÇπÊ≥®ÂÜåÔºåÂèàÂõûÂà∞ÂêêÊßΩ1ÊèèËø∞ÁöÑÊÉÖÂÜµ„ÄÇÂìéÔºåÈöæÂäû„ÄÇ

"
"ÂõõÂ∑ùÂèëÊñáÂèñÁºîÂÖ®ÈÉ®‰∏çÂêàËßÑp2p, ‰∏çËÉΩÂàÜÂºÄ""ÂêàËßÑ""Âíåp2p","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        String words = ""ÂõõÂ∑ùÂèëÊñáÂèñÁºîÂÖ®ÈÉ®‰∏çÂêàËßÑP2P"";
        System.out.println(NLPTokenizer.segment(words));
```
### ÊúüÊúõËæìÂá∫
```
ÂõõÂ∑ù ÂèëÊñá ÂèñÁºî ÂÖ®ÈÉ® ‰∏ç ÂêàËßÑ P2P
```

### ÂÆûÈôÖËæìÂá∫
```
[ÂõõÂ∑ù/n, ÂèëÊñá/v, ÂèñÁºî/v, ÂÖ®ÈÉ®/n, ‰∏ç/d, ÂêàËßÑp2p/a]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
CoreNatureDictionary.ngram.txt‰∏≠Âä†ÂÖ•‰∫Ü‰∫åÂÖÉÂè•Ê≥ïÔºö
ÂêàËßÑ@p2p 200
ÂêàËßÑ@P2P 200
‰ªç‰∏çËÉΩË¢´ÂàÜÂºÄÔºå‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÂõ†‰∏∫ËÆ≠ÁªÉËØ≠ÊñôÊ≤°ÊúâËøôÊ†∑ÁöÑÊï∞ÊçÆÂØºËá¥(ÊêúÁ¥¢‰∫Üdata/test/‰∏ãÁöÑÊñá‰ª∂Ê≤°ÊêúÂà∞ÔºÇp2p"")Ôºé

ÊÉ≥Áü•ÈÅìÊúâ‰ªÄ‰πàÂäûÊ≥ïËß£ÂÜ≥Ôºé


"
HanLPÂ§öÂÆû‰æãÈ≠îÊîπ‰∏≠,"Áî±‰∫éÊó©ÊúüËÆæËÆ°Â±ÄÈôêÔºåÁõÆÂâçHanLPÁöÑ`CustomDictionary`„ÄÅ`CoreDictionary`„ÄÅ`CoreBiGramTableDictionary`Á≠âÈÉΩÊòØÈùôÊÄÅËµÑÊ∫êÁ±ª„ÄÇËÄå‰∏Ä‰∫õÂ∫îÁî®Âú∫ÊôØË¶ÅÊ±ÇÂä†ËΩΩ‰∏çÂêåÁöÑËØçÂÖ∏ÔºåÊØîÂ¶ÇÂêå‰∏Ä‰∏™JVM‰∏≠‰∏çÂêåÁî®Êà∑ÂÆû‰æãÔºåÊàñËÄÖ‰∏çÂêåÈ¢ÜÂüü‰∏ãÂä†ËΩΩ‰∏çÂêåÁöÑbigramÊ®°Âûã„ÄÇÁî±‰∫é‰∏™‰∫∫Êó∂Èó¥ÊúâÈôêÔºåËøô‰∏™ÂäüËÉΩËÆ©Â§ßÂÆ∂‰πÖÁ≠â‰∫Ü„ÄÇ

Áé∞Âú®ÔºåÊâÄÊúâÈùôÊÄÅËµÑÊ∫êÁ±ªÊ≠£Âú®ÈÄêÊ≠•ÊîπÈÄ†‰∏≠„ÄÇÁõÆÂâçÁöÑËøõÂ∫¶Â¶Ç‰∏ãÔºö

- [x] `CustomDictionary`ÈáçÊûÑÂÆåÊØï
  - Â¶ÇÊûú‰Ω†‰∏çÈúÄË¶ÅÂ§öÂÆû‰æãÔºåÊó†ÈúÄ‰ªª‰ΩïÊîπÂä®Ôºå1.x‰øùÊåÅÂâçÂêëÂÖºÂÆπ
  - Â¶ÇÊûú‰Ω†ÈúÄË¶ÅÂ§öÂÆû‰æãÔºåÂèØ‰ª•‰∏∫ÂàÜËØçÂô®`segment`Êàñ`analyzer`ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ`DynamicCustomDictionary`ÂÆû‰æãÔºåÂπ∂‰∏îË∞ÉÁî®ËØ•ÂÆû‰æãÁöÑ`insert`ÊñπÊ≥ï„ÄÇ
    - Âç≥`segment.customDictionary = new DynamicCustomDictionary(""ËØçÂÖ∏1.txt"", ""ËØçÂÖ∏2.txt"")`
    - ÁÑ∂Âêé`segment.customDictionary.insert`
    - ÂèÇËÄÉ[demo](https://github.com/hankcs/HanLP/blob/74e6d7457b02ab872aa24c8476bf0b4449d8650e/src/test/java/com/hankcs/demo/DemoCustomDictionary.java#L70)

- [ ] `CoreDictionary`ÈáçÊûÑ‰∏≠
- [ ] `CoreBiGramTableDictionary`ÈáçÊûÑ‰∏≠

"
‰øÆÊîπÔºöÂä†ËΩΩËá™ÂÆö‰πâÂÅúÁî®ËØçÊñá‰ª∂Êó†Êïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âä†ËΩΩËá™ÂÆö‰πâÂÅúÁî®ËØçÊñá‰ª∂Êó†Êïà
```
CoreStopWordDictionary.load(""data/dictionary/custom/CustomStopWords.txt"", false);
```

### Ëß¶Âèë‰ª£Á†Å

```
    /**
     * Âä†ËΩΩÂè¶‰∏ÄÈÉ®ÂÅúÁî®ËØçËØçÂÖ∏
     * @param coreStopWordDictionaryPath ËØçÂÖ∏Ë∑ØÂæÑ
     * @param loadCacheIfPossible ÊòØÂê¶‰ºòÂÖàÂä†ËΩΩÁºìÂ≠òÔºàÈÄüÂ∫¶Êõ¥Âø´Ôºâ
     */
    public static void load(String coreStopWordDictionaryPath, boolean loadCacheIfPossible)
    {
        ByteArray byteArray = loadCacheIfPossible ? ByteArray.createByteArray(coreStopWordDictionaryPath + Predefine.BIN_EXT) : null;
        if (byteArray == null)
        {
            try
            {
                dictionary = new StopWordDictionary(HanLP.Config.CoreStopWordDictionaryPath);
                DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(HanLP.Config.CoreStopWordDictionaryPath + Predefine.BIN_EXT)));
                dictionary.save(out);
                out.close();
            }
            catch (Exception e)
            {
                logger.severe(""ËΩΩÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏"" + HanLP.Config.CoreStopWordDictionaryPath + ""Â§±Ë¥•""  + TextUtility.exceptionToString(e));
                throw new RuntimeException(""ËΩΩÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏"" + HanLP.Config.CoreStopWordDictionaryPath + ""Â§±Ë¥•"");
            }
        }
        else
        {
            dictionary = new StopWordDictionary();
            dictionary.load(byteArray);
        }
    }

```
### ‰øÆÊîπÂêéÁöÑ‰ª£Á†Å


```
    /**
     * Âä†ËΩΩÂè¶‰∏ÄÈÉ®ÂÅúÁî®ËØçËØçÂÖ∏
     * @param coreStopWordDictionaryPath ËØçÂÖ∏Ë∑ØÂæÑ
     * @param loadCacheIfPossible ÊòØÂê¶‰ºòÂÖàÂä†ËΩΩÁºìÂ≠òÔºàÈÄüÂ∫¶Êõ¥Âø´Ôºâ
     */
    public static void load(String coreStopWordDictionaryPath, boolean loadCacheIfPossible)
    {
        ByteArray byteArray = loadCacheIfPossible ? ByteArray.createByteArray(coreStopWordDictionaryPath + Predefine.BIN_EXT) : null;
        if (byteArray == null)
        {
            try
            {
                // HanLP.Config.CoreStopWordDictionaryPath Êîπ‰∏∫ coreStopWordDictionaryPath
                dictionary = new StopWordDictionary(coreStopWordDictionaryPath);
                DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(coreStopWordDictionaryPath + Predefine.BIN_EXT)));
                dictionary.save(out);
                out.close();
            }
            catch (Exception e)
            {
                logger.severe(""ËΩΩÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏"" + coreStopWordDictionaryPath + ""Â§±Ë¥•""  + TextUtility.exceptionToString(e));
                throw new RuntimeException(""ËΩΩÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏"" + coreStopWordDictionaryPath + ""Â§±Ë¥•"");
            }
        }
        else
        {
            dictionary = new StopWordDictionary();
            dictionary.load(byteArray);
        }
    }

```
"
ÊÇ®Â•ΩÔºåÊàëÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÂÖ≥‰∫é‰ªéÁΩëÁªú‰∏äÂÖ¨ÂºÄÁöÑ98Âπ¥‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ìÂà∞HanLPËá™Â∏¶ËØçÂÖ∏ÁöÑÂ§ÑÁêÜËøáÁ®ã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1„ÄÅÂú®ÁΩëÁªú‰∏äÊúâÂÖ¨ÂºÄÁöÑ98Âπ¥‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ìÔºåÊúâÂ∑≤ÁªèÊ†áÊ≥®ÁöÑÂíåÊú™Ê†áÊ≥®ÁöÑÔºåÊàëÊü•Áúã‰πãÂâçÁöÑlssues,Êúâ‰∫∫ËØ¥ËøáÁΩë‰∏ä98Âπ¥ÁöÑ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÊòØÁî±ÂåóÂ§ß‰∫∫Â∑•ËÅîÂêàÊ†áÊ≥®ÁöÑÔºåËØ∑ÈóÆÔºöÊúÄÂéüÂßãÁöÑÊñ∞ÈóªËØ≠ÊñôÊòØÊñáÁ´†ÔºåÊï¥‰∏™Ê†áÊ≥®ËøáÁ®ãÊòØÊÄéÊ†∑ÁöÑÔºüÂ∞ÜÊâÄÊúâÁöÑÊñ∞ÈóªÊñáÁ´†Êî∂ÈõÜËµ∑Êù•ÔºåÁÑ∂ÂêéËøõË°å‰∫∫Â∑•ÂàÜËØçÂπ∂‰∏îÊ†áÊ≥®ËØçÊÄßÔºåÁÑ∂ÂêéÊåâÁÖßËßÑËåÉÊï¥ÁêÜÂêéÂèëÂ∏ÉÔºåÂ∞±ÂèòÊàêÊàë‰ª¨ÁúãÂà∞ÁöÑÁΩë‰∏äÁöÑÂ∏¶ÊúâÊ†áÊ≥®ÁöÑÂÖ¨ÂºÄËØ≠ÊñôÂ∫ì‰∫ÜÔºü
2„ÄÅÂ¶ÇÊûúÁ¨¨‰∏Ä‰∏™ÈóÆÈ¢òÊòØ‰∫∫Â∑•ÂàÜËØçÂπ∂Ê†áÊ≥®ÁöÑÔºåÈÇ£Áé∞Âú®Êàë‰ª¨Â∑≤ÁªèÊúâ‰∫ÜË¢´‰∫∫Â∑•ÂàÜËØçÂπ∂Ê†áÊ≥®Â•Ω‰∫ÜÁöÑÊñ∞ÁöÑ98Âπ¥‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ìÔºà**‰∏ãÈù¢Á¨¨‰∏ÄÂº†Âõæ**Ôºâ
![image](https://user-images.githubusercontent.com/43544445/70107064-71420b00-1680-11ea-95f6-51168be49915.png)
ÁªèËøáÊÄéÊ†∑ÁöÑÂ§ÑÁêÜÂæóÂà∞HanLP‰∏≠ÊâÄÁî®Âà∞ÁöÑtxtËØçÂÖ∏Ôºå‰æãÂ¶Çmini.txtÔºà**‰∏ãÈù¢Á¨¨‰∫åÂº†Âõæ**Ôºâ
![image](https://user-images.githubusercontent.com/43544445/70107614-f4b02c00-1681-11ea-80f9-c06f8627534f.png)
Â∞±ÊòØÁî±Ë¢´‰∫∫Â∑•ÂàÜËØçÂπ∂Ê†áÊ≥®Â•Ω‰∫ÜÁöÑÊñ∞ÁöÑ98Âπ¥‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ì  Âà∞  HanLP‰∏≠ÊâÄÁî®Âà∞ÁöÑtxtËØçÂÖ∏Ôºå‰æãÂ¶Çmini.txtÊàñHanLP‰∏≠ÂÖ∂‰ªñÁöÑËØçÂÖ∏     ‰πãÈó¥ÁªèËøá‰∫ÜÊÄéÊ†∑ÁöÑÂ§ÑÁêÜËøáÁ®ãÔºåÊòØËÑöÊú¨Á®ãÂ∫èÂêóÔºüÂ¶ÇÊûúÊòØËÑöÊú¨Á®ãÂ∫èÁöÑËØùÔºåÂú®HanLPÂåÖ‰∏≠ÊúâÊ∫êÁ†ÅÂêóÔºüËã•Ê≤°ÊúâÁöÑËØùÂèØ‰ª•ÁªôÊàëËÆ≤‰∏Ä‰∏ãÊÄùË∑ØÂêóÔºü HanLP‰∏≠ÊâÄÁî®Âà∞ÁöÑtxtËØçÂÖ∏‰∏≠ÁöÑËØçÈ¢ëÊòØÊÄéÊ†∑Êù•ÁöÑÔºåËÑöÊú¨Á®ãÂ∫èÂú®ÊúÄÂºÄÂßãÊÄéÊ†∑ÁªüËÆ°ÊØè‰∏™ÂçïËØçÁöÑËØçÈ¢ëÁöÑÔºüÂú®ÊúÄÊúÄÂºÄÂßãÂ∞±ÊòØÁî®TF-IDFÁÆóÊ≥ïËøõË°åÁªüËÆ°ÂêóÔºü
Êç¢Âè•ËØùËØ¥ÔºåÊàëÊÉ≥ÈóÆÔºå‰ªéÊúÄÂéüÂßãÁöÑÊ≤°ÊúâÁªèËøá‰ªª‰ΩïÂä†Â∑•ÁöÑÁîüËØ≠ÊñôÔºà‰πüÂ∞±ÊòØÂéüÊñáÁ´†ÔºâÁªèËøáÊÄéÊ†∑ÁöÑÂ§ÑÁêÜÊâçÂèØ‰ª•ÂèòÊàêÂèØ‰ª•Ë¢´HanLP‰ΩøÁî®ÁöÑËØçÂÖ∏Êñá‰ª∂ÔºåÊàñËÄÖÊàëÊÉ≥Ë¶ÅËá™Â∑±Âà∂‰Ωú‰∏ìÁî®ËØ≠ÊñôÂ∫ìÔºå‰ªéÊúÄÂºÄÂßãÁöÑ‰∏ìÁî®È¢ÜÂüüÊñáÁ´†ÊñáÊú¨ÁªèËøá‰∫∫Â∑•ÂàÜËØçÊ†áÊ≥®Âêé**ÂÜç**ÁªèËøáÊÄé**Ê†∑ÁöÑÂ§ÑÁêÜ**ÂèØ‰ª•ÂæóÂà∞ËÉΩÂ§üË¢´HanLP‰ΩøÁî®ÁöÑËØçÂÖ∏Êñá‰ª∂ÔºåÂπ∂‰∏îÂèØ‰ª•Â∞ÜÊàëËá™Â∑±ÁöÑ‰∏ìÁî®ËØ≠ÊñôÂ∫ìÂØπÂ∫îÁöÑËØçÂÖ∏ÊõøÊç¢ÊéâHanLPËá™Â∏¶ËØçÂÖ∏„ÄÇ
ËØ∑Ê±ÇÂ§ßÁ•ûËß£Á≠îÔºÅÂçÅÂàÜÊÑüË∞¢
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®Á∫øÊºîÁ§∫ÂíåÊú¨Âú∞ËøêË°åÂàÜËØçËØçÊÄß‰∏ç‰∏ÄËá¥,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

## ÊàëÁöÑÈóÆÈ¢ò
Êú¨Âú∞ÂàÜËØçÁöÑËØçÊÄßÔºàÔºÇÈúÄË¶ÅÔºÇËøô‰∏™ËØçÔºâÂíåÂú®Á∫øÊºîÁ§∫ÁªìÊûú‰∏ç‰∏ÄÊ†∑:
Êú¨Âú∞ÂàÜÊàê‰∫Ün,Á∫ø‰∏äÂàÜÊàê‰∫Üvn(Á¨¨‰∏Ä‰∏™""ÈúÄË¶Å""),vÔºàÁ¨¨‰∫å‰∏™""ÈúÄË¶Å""Ôºâ

## Â§çÁé∞ÈóÆÈ¢ò
### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å
        String words = ""ÂΩìÂâçÈò∂ÊÆµÁöÑÈÖçÁΩÆÈúÄË¶ÅÁ´ãË∂≥Èò≤Âæ°ÔºåÂêåÊó∂ÈúÄË¶ÅË∞®Èò≤‰ªäÂπ¥Âº∫ÂäøÊùøÂùóÂú®Ë°•Ë∑åÈ£éÈô©"";
        System.out.println(NLPTokenizer.segment(words));
### ÊúüÊúõËæìÂá∫
[ÂΩìÂâç/n, Èò∂ÊÆµ/n, ÁöÑ/u, ÈÖçÁΩÆ/n, ÈúÄË¶Å/v, Á´ãË∂≥/n, Èò≤Âæ°/n, Ôºå/w, ÂêåÊó∂/n, ÈúÄË¶Å/v, Ë∞®Èò≤/v, ‰ªäÂπ¥/t, Âº∫Âäø/n, ÊùøÂùó/n, Âú®/p, Ë°•/v, Ë∑å/v, È£éÈô©/n]

### ÂÆûÈôÖËæìÂá∫
[ÂΩìÂâç/n, Èò∂ÊÆµ/n, ÁöÑ/u, ÈÖçÁΩÆ/n, ÈúÄË¶Å/n, Á´ãË∂≥/n, Èò≤Âæ°/n, Ôºå/w, ÂêåÊó∂/n, ÈúÄË¶Å/n, Ë∞®Èò≤/v, ‰ªäÂπ¥/t, Âº∫Âäø/n, ÊùøÂùó/n, Âú®/p, Ë°•/v, Ë∑å/v, È£éÈô©/n]

## Á∫ø‰∏äÊºîÁ§∫ËæìÂá∫Ôºö
![Ê∑±Â∫¶Êà™Âõæ_ÈÄâÊã©Âå∫Âüü_20191203095146](https://user-images.githubusercontent.com/22389800/70013524-f101a400-15b2-11ea-8957-8bce0238f73c.png)




"
Â≠óËäÇË∑≥Âä®Ë¢´ÂàÜÂºÄ‰∫ÜÔºåÂìàÂìàÂìà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
import com.hankcs.hanlp.corpus.MSR;,1.7.5 ‰ª£Á†ÅÈáåÈù¢Ê≤°ÊúâÂÆö‰πâÁõ∏ÂÖ≥ÁöÑjavaÊñá‰ª∂‰∫Ü - compile demoÂíåbook example ÊúâÁõ∏ÂêåÈîôËØØÔºåÊúâÁ¢∞Âà∞ÂêåÊ†∑ÈóÆÈ¢òÁöÑÂêóÔºü
HanLPÂèØ‰ª•‰ΩøÁî®OpenNLPËÆ≠ÁªÉÂæóÊ®°ÂûãÂ∫ìÂêó?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"doc2vecËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶Êó∂,ÊÄé‰πàÊâçËÉΩËøîÂõûÂÖ®ÈÉ®ÔºåÈªòËÆ§ËøîÂõû10‰∏™","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
git bash‰∏ãÂëΩ‰ª§Êó†Êïà,Âú®cmdÂëΩ‰ª§Ë°å‰∏ãÂèØ‰ª•Ê≠£Â∏∏Ë∞ÉÁî®Âπ∂‰ΩøÁî®ÂäüËÉΩhanlp segmentÔºå‰ΩÜÂú®git bash‰∏ãÔºåËæìÂÖ•Âè•Â≠êÂπ∂ÂõûËΩ¶ÂêéÊ≤°Êúâ‰ªª‰ΩïÂèçÂ∫î
ËØçÊÄßÊ†áÊ≥®‰∏çÂáÜÁ°ÆÔºå‚ÄòÊòéÁ°ÆÊèêÂá∫ÁªôÁöÑÊ†áÊ≥®ÊòØÂêçËØç‚Äô,"<img width=""855"" alt=""‰ºÅ‰∏öÂæÆ‰ø°Êà™Âõæ_15745009854716"" src=""https://user-images.githubusercontent.com/49904623/69476574-7e583200-0e16-11ea-9fe3-ec9d0c7f39a1.png"">
"
import pyhanlpÁΩëÁªúÊä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫é‰ΩøÁî®NaiveBayesClassifierËøõË°åÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ª,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
LinuxÁéØÂ¢ÉÂºïÁî®dataÊï∞ÊçÆÂåÖÊó∂Êä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®linuxÈÉ®ÁΩ≤ÔºåÂú®ÂºïÁî®dataÊï∞ÊçÆÂåÖÁöÑÊó∂ÂÄôÂèëÁîü‰∏ãËΩΩÈîôËØØÔºåÈÅáÂà∞‰∏§‰∏™ÈóÆÈ¢òÔºö
1„ÄÅÊàëÊäädataÊï∞ÊçÆÂåÖÊòØÊîæÁΩÆ‰∫é/eyas/templete/ ÁõÆÂΩï‰∏ãÔºà/eyas‰∏∫Á°¨ÁõòÂàÜÂå∫ÔºâÔºåËÄå‰∏îhanlp.properties‰∏≠ÁöÑËÆæÁΩÆÊòØ root=/eyas/templete/Ôºå‰ΩÜÊòØÂç¥ÂºïÁî®‰∏çÂà∞Êï∞ÊçÆÂåÖ„ÄÇ
2„ÄÅÂü∫‰∫éÈóÆÈ¢ò1ÔºåÁ®ãÂ∫èËß¶ÂèëÈáçÊñ∞‰∏ãËΩΩÊìç‰ΩúÔºåÂç¥‰∏ãËΩΩ‰∏ç‰∫ÜÊï∞ÊçÆÂåÖÔºåÊä•ÈîôÂÜÖÂÆπÔºöjava.io.IOException: No file to download. Server replied HTTP code: 302

<i>Êú¨‰∫∫ÊòØÂ∞èÁôΩÔºåÂ¶ÇÊûúÈóÆÁöÑÈóÆÈ¢òÂ§™lowÔºåËØ∑ËßÅË∞Ö„ÄÅÁ®çÂæÆËÄêÂøÉÊåáÂØº‰∏ãÂìà„ÄÇË∞¢Ë∞¢ÔºÅ</i>

"
ÂàÜÂè•ÈóÆÈ¢ò,"ÊÉ≥Ë¶ÅÊääÂè•Â≠ê
 ‚ÄùÁÑ∂ÂêéË∑üÊÇ®ËÆ≤Ëß£‰∏Ä‰∏ãËΩ¶ÂÜµÂíåÂΩìÂú∞ÁöÑ‰ºòÊÉ†ÊîøÁ≠ñÂÖàÁîüÊÇ®ÁúãÂ∞æÂè∑5920ÊòØÊÇ®Êú¨‰∫∫ÁöÑÊâãÊú∫Âè∑Âêß‚Äú
ÂàÜÊàê‰∏ãÈù¢‰ø©ÊÆµÂè•Â≠êÊúâ‰ªÄ‰πàÂäûÊ≥ïÂêó
 ÁÑ∂ÂêéË∑üÊÇ®ËÆ≤Ëß£‰∏Ä‰∏ãËΩ¶ÂÜµÂíåÂΩìÂú∞ÁöÑ‰ºòÊÉ†ÊîøÁ≠ñ
 ÂÖàÁîüÊÇ®ÁúãÂ∞æÂè∑5920ÊòØÊÇ®Êú¨‰∫∫ÁöÑÊâãÊú∫Âè∑Âêß
"
NLPÂàÜËØçÂØπÂ≠òÂú®Á©∫Ê†ºÁöÑÂè•Â≠êËøõË°åÂàÜËØçÊó∂Â≠òÂú®ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
NLPÂàÜËØçÂØπÂ≠òÂú®Á©∫Ê†ºÁöÑÂè•Â≠êËøõË°åÂàÜËØçÊó∂Â≠òÂú®ÈóÆÈ¢òÔºå‰ºöÂ∞ÜÁ©∫Ê†ºÂêéÁöÑÊâÄÊúâËØçÂàÜÊàê‰∏Ä‰∏™ËØç„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""ÈÉ≠Âæ∑Á∫≤ ÂàòÂæ∑Âçé guodegang liudehua""
term_list = NLPTokenizer.segment(text)
print(term_list)    
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÈÉ≠Âæ∑Á∫≤/nr, ÂàòÂæ∑Âçé/nr, guodegang/nx, liudehua/nx]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÈÉ≠Âæ∑Á∫≤ ÂàòÂæ∑Âçé guodegang liudehua/nr]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ª£Á†ÅÊ†ºÂºèÂåñ,@hankcs  ‰ª£Á†ÅÂèØ‰ª•Ê†ºÂºèÂåñ‰∏ãÂêó
ÊâÄÊúâÁöÑÂàÜËØçÂô®ËøêË°å‰∏ÄÊÆµÊó∂Èó¥ÂêéÔºåÂ∞ÜÊï∞Â≠óËØÜÂà´Êàênz,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

PerceptronLexicalAnalyzer   ViterbiSegment Á≠âÂàÜËØçÂô®Âú®ËøêË°å‰∏ÄÊÆµÊó∂Èó¥ÂêéÔºåÂ∞ÜÊï∞Â≠óËØÜÂà´Êàênz

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà ‰ª•ViterbiSegmentÂàÜËØçÂô®‰∏∫‰æã

### Ëß¶Âèë‰ª£Á†Å

```
    private static Segment segment = HanLP.newSegment().enableOffset(true);

    public List<EntityResult> resolve(String sentence) {
        List<EntityResult> entities = new ArrayList<>();
        List<Term> termList = segment.seg(sentence);
        log.info(""QuantityResolver termList={}"",termList);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
termList=[1/m, Âä†/v, 2/m]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
termList=[1/nz, Âä†/v, 2/nz]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Ê≠§resolve(String sentence)Êé•Âè£1sÊâßË°å1-5Ê¨°Ôºå1‰∏™Â∞èÊó∂ÂêéÂ∞±‰ºöËØÜÂà´ÈîôËØØ

"
Infinite recursivon ?,https://github.com/hankcs/HanLP/blob/56058d68290578931267fd31cdfad88c31d75fa2/src/main/java/com/hankcs/hanlp/mining/cluster/Cluster.java#L140-L150
Êä•Èîôjava.lang.NoClassDefFoundError: com/hankcs/hanlp/HanLP,Â¶ÇÈ¢òÔºåÊàëÂÆâË£Ö‰∫ÜpyhanlpÂêéÔºåËøêË°åHanLP = JClass('com.hankcs.hanlp.HanLP')Â∞±Êä•È¢òÁõÆÁöÑÈîôËØØÔºåËØ∑ÈóÆËøô‰∏™ÈîôÊòØ‰ªÄ‰πàÂéüÂõ†ÔºåË¶ÅÊÄé‰πàËß£ÂÜ≥Âë¢ÔºüÂ∞èÁôΩ‰∏ÄÊûöÔºåËØ∑Â§ßÂÆ∂Ê£íÊ£íÂøôËß£Á≠î‰∏Ä‰∏ã
"Ch03ÁöÑngram_segment.pyÊñπÊ≥ïprint(CoreBiGramTableDictionary.getBiFrequency(""ÂïÜÂìÅ"", ""Âíå""))ËøîÂõûÈ¢ëÊ¨°‰∏çÂØπ","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

Áõ¥Êé•git cloneÂà∞Êú¨Âú∞ÁöÑpyhanlpÔºåËøêË°åCh03ÁöÑngram_segment.pyÔºåËøîÂõûÁöÑ1-gramÈ¢ëÊ¨°‰∏∫2Ôºå2-gramÈ¢ëÊ¨°‰∏∫0ÔºåJavaÁâàÁöÑËæìÂá∫ÊòØÊ≠£Á°ÆÁöÑÔºåËøîÂõû„ÄêÂïÜÂìÅ„ÄëÁöÑËØçÈ¢ëÔºö2Ôºå„ÄêÂïÜÂìÅ@Âíå„ÄëÁöÑÈ¢ëÊ¨°Ôºö1

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Êú™ÂØπ‰ª£Á†ÅÂÅö‰øÆÊîπ

### Ê≠•È™§

1. Áõ¥Êé•Âú®Pycharm‰∏≠ËøêË°åÁöÑngram_segment.py

### Ëß¶Âèë‰ª£Á†Å

```
   print(CoreDictionary.getTermFrequency(""ÂïÜÂìÅ""))
   print(CoreBiGramTableDictionary.getBiFrequency(""ÂïÜÂìÅ"", ""Âíå""))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫1-gramÈ¢ëÊ¨°‰∏∫2Ôºå2-gramÈ¢ëÊ¨°‰∏∫1
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫1-gramÈ¢ëÊ¨°‰∏∫2Ôºå2-gramÈ¢ëÊ¨°‰∏∫0
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![ÂæÆ‰ø°Êà™Âõæ_20191108212057](https://user-images.githubusercontent.com/54297119/68480206-37dbd280-026f-11ea-92f3-10fb5df66af3.png)
![ÂæÆ‰ø°Êà™Âõæ_20191108212350](https://user-images.githubusercontent.com/54297119/68480210-39a59600-026f-11ea-89e6-5ccc0ab56201.png)

"
ÂÆâË£ÖËøáÁ®ãÂá∫Áé∞ÁöÑÈóÆÈ¢òÔºå‰ª•ÂèäÊúÄÂêéÂÆâË£ÖÊàêÂäüÁöÑÁâàÊú¨Âè∑ÔºåÊ±áÊä•‰∏Ä‰∏ã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp 0.1.50
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp 0.1.47

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÁéØÂ¢ÉÂèÇÊï∞Ôºö
* win10
* jdk 1.8
* python 3.7

Â§±Ë¥•ÂÆâË£ÖËøáÁ®ãÔºö
* Á¨¨‰∏ÄÊ¨°ÔºöÁõ¥Êé•`pip install pyhanlp` ÔºàÁâàÊú¨0.1.50ÔºâÔºåËá™Âä®ÂÆâË£Ö‰æùËµñjpype1ÔºàÁâàÊú¨0.7.0ÔºâÔºå‰∏ãËΩΩdataÊñá‰ª∂ÊâßË°å`hanlp`Êä•ÈîôÔºö`startJVM() got an unexpected keyword argument 'convertStrings'`
* Á¨¨‰∫åÊ¨°Ôºö`pip install pyhanlp=0.1.47`Ôºå`pip install jpype1=0.6.2`ÔºåÊâßË°åÊä•ÈîôÔºö`AttributeError: module '_jpype' has no attribute 'setResource'`

ÊàêÂäüËøáÁ®ãÔºö
* `pip install pyhanlp=0.1.47`
* `pip install jpype1=0.6.3`
"
HanLPÂä†ËΩΩËá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑBug„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. Âú®Â≠òÂú®hanlp.propertiesÈÖçÁΩÆÊñá‰ª∂ÁöÑÊÉÖÂÜµ‰∏ã,ËÆæÁΩÆÂ§ö‰∏™Â≠óÂÖ∏Âπ∂Áî®;ÂàÜÂâ≤.
‰æãÂ¶Ç:CustomDictionaryPath=a.txt;b.txt;c.txt;
    hanlp‰ºöÂêØÂä®ÁªùÂØπË∑ØÂæÑÊêúÁ¥¢ÁöÑÊñπÂºèÂä†ËΩΩËá™ÂÆö‰πâÂ≠óÂÖ∏.
2. Áî±‰∫éhanlp‰ºöÂ∞ùËØï‰ªéÁ¨¨‰∏Ä‰∏™mainPath(ÂØªÊâæa.txt.bin),Âπ∂Âú®Âä†ËΩΩÊàêÂäüÁöÑÊÉÖÂÜµ‰∏ã,Á´ãÂàªËøîÂõû.
    ÂØºËá¥b.txt;c.txt‰∏ç‰ºöË¢´Âä†ËΩΩ.

### Ëß¶Âèë‰ª£Á†Å

```
Âú®CustomDictionary::public static boolean loadMainDictionary(String mainPath, String path[], DoubleArrayTrie<CoreDictionary.Attribute> dat, boolean isCache)ÊñπÊ≥ï‰∏≠.

logger.info(""Ëá™ÂÆö‰πâËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:"" + mainPath);
        if (loadDat(mainPath, dat)) return true;
```

"
ËøêË°åÊú¨‰π¶ch02 evaluate_cws ‰ºöÊä•Èîô,"ËøêË°åÂêéÊä•Èîô ÈîôËØØ‰∏∫Ôºö
jpype._jclass.NoClassDefFoundError: java.lang.NoClassDefFoundError: com/hankcs/hanlp/HanLP"
Ê≤°ÊúâÊâæÂà∞pythonÁâàÊú¨ÁöÑ‰ª£Á†ÅÔºåÊ≤°ÊúâÊâæÂà∞Âú®‰π¶33È°µ‰∏≠ÊèêÂèäÁöÑaipf_law.pyÊñá‰ª∂,"<!--
1.7.5ÁâàÊú¨ÁöÑÔºåÊ≤°ÊúâÊâæÂà∞‰π¶‰∏≠ÊèêÂèäÁöÑpy‰ª£Á†Å
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
     1.7.5ÁâàÊú¨ÁöÑÔºåÊ≤°ÊúâÊâæÂà∞‰π¶‰∏≠ÊèêÂèäÁöÑpy‰ª£Á†Å
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
anacondaÁéØÂ¢É‰∏ãÂÆâË£ÖpyhanlpÁöÑÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpython3.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpython3.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## Âá∫Áé∞‰∫ÜÂ¶Ç‰∏ãÊâÄÁ§∫ÁöÑÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/53120545/68183221-b36f2280-ffd6-11e9-91d0-8b9824b05c2c.png)

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®python‰∏≠ÊúâÊèê‰æõÂ∞Üdoc2vec‰øùÂ≠òÂπ∂Âä†ËΩΩÁöÑÊñπÊ≥ïÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®python‰∏≠ÔºåÈÄöËøáword2vecÂä†ËΩΩÈ¢ÑÂÖàËÆ≠ÁªÉÂ•ΩÁöÑËØçÂêëÈáèÊ®°ÂûãÔºåÁÑ∂ÂêéÂä†ËΩΩÂæÖÊü•ËØ¢ÁöÑÊñáÊ°£ÂêéÔºåÁîüÊàêÊñáÊ°£ÂêëÈáèdoc2vecÔºåËØ•ÂØπË±°ÊúâÊèê‰æõ‰øùÂ≠òÂíåÂä†ËΩΩÁöÑÊñπÊ≥ïÂêóÔºüËã•ÂèØ‰ª•ÔºåÂú®‰πãÂêéÁöÑ‰ΩøÁî®‰∏≠ÔºåÂèØ‰ª•‰∏çÂä†ËΩΩword2vecÔºåÂè™Âä†ËΩΩdoc2vecÔºåÂ∞±ÂèØ‰ª•ËøõË°åÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÂêóÔºü

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = 'E:/data/word2vec.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)
docs = [""doc1"",  ""doc2"", ""doc3"",  ""doc4"", ""doc5""]
for idx, doc in enumerate(docs):
    doc2vec.addDocument(idx, doc)

ËøêË°å‰∏äËø∞‰ª£Á†ÅÂêéÔºåÊúâÊèê‰æõdoc2vecÂØπË±°‰øùÂ≠òÁöÑÊñπÊ≥ïÂêóÔºüÔºåËã•ÂèØ‰ª•ÔºåÂú®‰πãÂêéÁöÑ‰ΩøÁî®‰∏≠ÔºåÂèØ‰ª•‰∏çÂä†ËΩΩword2vecÔºåÂè™Âä†ËΩΩdoc2vecÔºåÂ∞±ÂèØ‰ª•ËøõË°åÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÂêóÔºü
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúâÊèê‰æõdoc2vecÂØπË±°‰øùÂ≠òÂíåÂä†ËΩΩÁöÑÊñπÊ≥ïÂêóÔºü
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Create arrayList,"import java.util.Scanner;

public class RuangKelas {
    public static void main(String[] args) {

        // Membuat Array dan Scanner
        String[][] meja = new String[2][3];
        Scanner scan = new Scanner(System.in);

        // mengisi setiap meja
        for(int bar = 0; bar < meja.length; bar++){
            for(int kol = 0; kol < meja[bar].length; kol++){
                System.out.format(""Siapa yang akan duduk di meja (%d,%d): "", bar, kol);
                meja[bar][kol] = scan.nextLine();
            }
        }

        // menampilkan isi Array
        System.out.println(""-------------------------"");
        for(int bar = 0; bar < meja.length; bar++){
            for(int kol = 0; kol < meja[bar].length; kol++){
                System.out.format(""| %s | \t"", meja[bar][kol]);
            }
            System.out.println("""");
        }
        System.out.println(""-------------------------"");
    }
}"
Prevent compile fail after JDK8,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

‰ΩøÁî® `buffer` ÊõøÊç¢‰∫Ü‰ª£Á†Å‰∏≠ÁöÑ `_`ÔºåÂê¶Âàô `JDK8` ‰πãÂêéÁöÑ `JDK` ‰ºöÁºñËØëÂ§±Ë¥•ÔºåÂà†Èô§‰∫Ü‰∏Ä‰∏™Ê≤°ÊúâÁî®Âà∞‰∏î‰ºöÂºïËµ∑ÁºñËØëÂ§±Ë¥•ÁöÑÊñá‰ª∂„ÄÇ

"
ËØ∑ÈóÆÂêå‰πâËØçÂ≠óÂÖ∏Âú®lucene‰∏≠Â¶Ç‰Ωï‰ΩøÁî®Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
http://www.hankcs.com/program/java/lucene-synonymfilterfactory.html
Âú®ÁΩë‰∏äÁúãÂà∞ËøôÁØáÊñáÁ´†Ôºå‰ΩÜÊòØÂ§™ËÄÅ‰∫ÜÔºåËÄå‰∏îÁé∞Âú®ÁöÑÂêå‰πâËØçÂ≠óÂÖ∏Ê†ºÂºè‰πü‰∏ç‰∏ÄÊ†∑‰∫Ü„ÄÇÈ∫ªÁÉ¶‰ªãÁªç‰∏Ä‰∏ãhanLp‰∏≠ÁöÑÂêå‰πâËØçÂ¶Ç‰Ωï‰∏éluceneÁªìÂêà‰ΩøÁî®ÔºüË∞¢Ë∞¢„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
jpype._jclass.IllegalArgumentException: java.lang.IllegalArgumentException: ÈîôËØØÁöÑÊ®°ÂûãÁ±ªÂûã: ‰º†ÂÖ•ÁöÑ‰∏çÊòØÂàÜËØçÊ®°ÂûãÔºåËÄåÊòØ POS Ê®°Âûã,"ÈóÆÈ¢òÔºö
   pyhanlp ÊåâÁÖßÊïôÁ®ã ÂØºÂÖ•Ëá™Â∑±ËÆ≠ÁªÉÁöÑcwsÊä•ÈîôÔºåÊòØ‰ªÄ‰πàÂéüÂõ†Âë¢Ôºü
‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
CRFLexicalAnalyzer = JClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
analyzer = CRFLexicalAnalyzer(r""C:\Users\gy\Desktop\pku98\cws.txt"")
print(analyzer.analyze(""17kbav995%""))"
python Â¶Ç‰ΩïÂØºÂÖ•ËØçÂÖ∏ËÆ≠ÁªÉËá™ÂÆö‰πâcrfÊ®°Âûã„ÄÇ,Â§ßÁ•û‰ª¨Â•ΩÔºåÂº±Âº±ÈóÆ‰∏ãÔºåpython Â¶Ç‰ΩïÂØºÂÖ•ËØçÂÖ∏ËÆ≠ÁªÉËá™ÂÆö‰πâcrfÊ®°ÂûãÔºüËÆ≠ÁªÉÂÆå‰πãÂêéÂ¶Ç‰ΩïÂØºÂÖ•Âë¢ÔºüÊ±ÇdemoË∞¢Ë∞¢~
ÊÉ≥ÈóÆ‰∏ãÂêéÁª≠‰ºöÊúâÈîôÂ≠óÁ∫†ÈîôËÉΩÂäõ,ÊÉ≥ÈóÆ‰∏ãÂêéÁª≠‰ºöÊúâÈîôÂ≠óÁ∫†ÈîôËÉΩÂäõ
CRFLexicalAnalyzerÂàÜËØçÁ¥¢ÂºïÊ®°ÂºèÂíåÈùûÁ¥¢ÂºïÊ®°ÂºèÁªìÊûú‰∏ÄÊ†∑,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
CRFLexicalAnalyzerÂàÜËØçÁ¥¢ÂºïÊ®°Âºè‰∏ãÂàÜËØçÁªìÊûúÂíåÈùûÁ¥¢ÂºïÊ®°ÂºèÁªìÊûú‰∏ÄËá¥
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
Â∞ÜIndexTokenizerÁ±ª‰∏≠
public static final Segment SEGMENT = HanLP.newSegment().enableIndexMode(true);
‰øÆÊîπ‰∏∫
public static final Segment SEGMENT = HanLP.newSegment(""crf"").enableIndexMode(true);
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
ÊâßË°åDemoIndexSegmentÁöÑmainÊñπÊ≥ï
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   List<Term> termList = IndexTokenizer.segment(""‰∏ªÂâØÈ£üÂìÅ"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

        System.out.println(""\nÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö"");
        IndexTokenizer.SEGMENT.enableIndexMode(1);
        termList = IndexTokenizer.segment(""‰∏ªÂâØÈ£üÂìÅ"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
‰∏ªÂâØÈ£üÂìÅ/n [0:4]
‰∏ªÂâØÈ£ü/j [0:3]
ÂâØÈ£üÂìÅ/n [1:4]
ÂâØÈ£ü/n [1:3]
È£üÂìÅ/n [2:4]

ÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö
‰∏ªÂâØÈ£üÂìÅ/n [0:4]
‰∏ªÂâØÈ£ü/j [0:3]
‰∏ª/ag [0:1]
ÂâØÈ£üÂìÅ/n [1:4]
ÂâØÈ£ü/n [1:3]
ÂâØ/b [1:2]
È£üÂìÅ/n [2:4]
È£ü/v [2:3]
ÂìÅ/ng [3:4]
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
‰∏ªÂâØÈ£üÂìÅ/n [0:4]

ÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö
‰∏ªÂâØÈ£üÂìÅ/n [0:4]

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
 ÂàÜÁ±ªËÆ≠ÁªÉÁöÑÁõÆÂΩïÁªìÊûÑËÉΩÂê¶ÊîØÊåÅ‰∏Ä‰∏™Êñá‰ª∂ÈáåÂ§¥ÂåÖÂê´ÂæàÂ§ö‰∏™ÊñáÁ´†ÁöÑ?," /**
     * Áî®UTF-8ÁºñÁ†ÅÁöÑËØ≠ÊñôËÆ≠ÁªÉÊ®°Âûã
     *
     * @param folderPath  Áî®UTF-8ÁºñÁ†ÅÁöÑÂàÜÁ±ªËØ≠ÊñôÁöÑÊ†πÁõÆÂΩï.ÁõÆÂΩïÂøÖÈ°ªÊª°Ë∂≥Â¶Ç‰∏ãÁªìÊûÑ:<br>
     *                    Ê†πÁõÆÂΩï<br>
     *                    ‚îú‚îÄ‚îÄ ÂàÜÁ±ªA<br>
     *                    ‚îÇ   ‚îî‚îÄ‚îÄ 1.txt<br>
     *                    ‚îÇ   ‚îî‚îÄ‚îÄ 2.txt<br>
     *                    ‚îÇ   ‚îî‚îÄ‚îÄ 3.txt<br>
     *                    ‚îú‚îÄ‚îÄ ÂàÜÁ±ªB<br>
     *                    ‚îÇ   ‚îî‚îÄ‚îÄ 1.txt<br>
     *                    ‚îÇ   ‚îî‚îÄ‚îÄ ...<br>
     *                    ‚îî‚îÄ‚îÄ ...<br>
     *                    Êñá‰ª∂‰∏ç‰∏ÄÂÆöÈúÄË¶ÅÁî®Êï∞Â≠óÂëΩÂêç,‰πü‰∏çÈúÄË¶Å‰ª•txt‰Ωú‰∏∫ÂêéÁºÄÂêç,‰ΩÜ‰∏ÄÂÆöÈúÄË¶ÅÊòØÊñáÊú¨Êñá‰ª∂.


ÂéüÊîØÊåÅÊ†ºÂºèÂ¶Ç‰∏ä

‰ΩÜÊòØÂÅáËÆæÊñáÊú¨ÊúâÂá†ÂçÅ‰∏á‰∏™ÁöÑËØù,Êñá‰ª∂Á≥ªÁªüÂéãÂäõÂæàÂ§ß. 

ÊâÄ‰ª•ËÉΩ‰∏çËÉΩÊîØÊåÅ‰∏Ä‰∏ã
‰∏Ä‰∏™ÁõÆÂΩïÊúâÂá†‰∏™Êñá‰ª∂  Êñá‰ª∂Âêç‰ª£Ë°®ÂàÜÁ±ª,  Êñá‰ª∂ÈáåÂ§¥ÊØè‰∏ÄË°åÂ∞±ÊòØ‰∏Ä‰∏™ÊñáÁ´†.

ÁªèÊµé.1
ÁªèÊµé.2

‰ΩìËÇ≤.1
‰ΩìËÇ≤.2

ÂÜõ‰∫ã.1
ÂÜõ‰∫ã.2
ÂÜõ‰∫ã.3

Á±ª‰ºº‰ª•‰∏ä

‰∏áÂàÜÊÑüË∞¢
"
java.lang.OutOfMemoryError: Java heap space,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö0.1.50
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö0.1.50*[pipÂÆâË£Ö]* (hanlp-1.7.5.jar,data-for-1.7.5)

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®PythonÁâàÊú¨ÁöÑhanlpÂàÜÊûêÁ∫¢Ê•ºÊ¢¶ÊñáÊú¨Êó∂ÔºàÊèêÂèñÊëòË¶ÅÊìç‰ΩúÔºâÊèêÁ§∫ÈîôËØØÔºö
`Traceback (most recent call last):
  File ""D:\Files\python\MachineLearning\Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ\hanlpÂàÜÊûêÁ∫¢Ê•ºÊ¢¶.py"", line 22, in <module>
    print(HanLP.extractSummary(text, 5))
jpype._jclass.java.lang.OutOfMemoryError: java.lang.OutOfMemoryError: Java heap space`

ÊâÄÁî®ÊñáÊú¨Êñá‰ª∂Âú®https://github.com/ouening/MLPractice/blob/master/Red_Mansions_Anasoft.txt

## ÂÖ∂‰ªñ‰ø°ÊÅØ
OS: win10 64bit
python: 3.7.2

"
word2vec Ê®°ÂûãÊúâ1‰∏™GÔºåÊó†ËÆ∫ËÆæÁΩÆÂ§öÂ∞ëÂÜÖÂ≠òÁªôÊ®°ÂûãÔºåÈÉΩÊÑüËßâ‰∏çÊòØ‰∏ÄËà¨ÁöÑÊÖ¢ÔºåÊó†Ê≥ï‰ΩøÁî®„ÄÇ,ÊàëÁöÑword2vec Ê®°ÂûãÊúâ1‰∏™GÔºåÂèëÁé∞ËøêË°ådocVectorModel.nearest ÊñπÊ≥ïÊòØÂú®Â§™ÊÖ¢ÔºåÊàëÂ∫îËØ•Â¶Ç‰ΩïÊèêÈ´òÊ®°ÂûãËÆ≠ÁªÉÂ•ΩÁöÑÔºåÈ¢ÑÊµãÈÄüÂ∫¶ÔºüÁé∞Âú®Â§ß‰∫é 20Áßí‰ª•‰∏äÔºåÁúüÁóõÂøÉÔºåÂéãÊ†πÊ≤°Ê≥ïÁî®Âë¢
hanlp.properties.in‰ΩúÁî®,"‰øÆÊîπ‰∫Ühanlp.propertiesÂêéÔºåhanlp.properties.inÁöÑÁõ∏ÂØπÂ∫îÂ≠óÊÆµË¶ÅÊîπÂêóÔºü
Ëøô‰∏™.inÊñá‰ª∂‰ΩúÁî®ÊòØÂï•ÔºüÊòØÂê¶ÊòØÂ§á‰ªΩÊñá‰ª∂Ôºü"
ÂàÜËØçÊó∂ÊØè‰∏™ËØçÊÄßÁöÑÂê´‰πâÊòØ‰ªÄ‰πà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

##‰æãÂ¶ÇÔºöÂ∞èÁôΩÂæàÁÆÄÂçïÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰æãÂ¶ÇÂàÜÊ¨°ÁªìÊûúÔºö [Âº†‰∫Æ/nr, ÂºÄ/v, ‰∫Ü/ule, ‰∏Ä‰∏™/mq, È∫ªËæ£ÁÉ´/nf, Â∫ó/n]ÔºåÈáåÈù¢ÁöÑnr„ÄÅule„ÄÅmq‰ª£Ë°®‰ªÄ‰πàËØçÊÄßÂê´‰πâÔºåÊúâÊ≤°Êúâ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂØπÂ∫îÂÖ≥Á≥ªÁöÑËØçÂÖ∏„ÄÇÂú®ÁΩë‰∏äÊ≤°ÊúâÊâæÂà∞Áªü‰∏ÄÁöÑËØçÊÄßË°®Á§∫Ë°®ÂèØÂèÇËÄÉ„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
‰∏≠ÊñáÊñá‰ª∂ÂêçÁöÑËá™ÂÆö‰πâËØçÂ∫ìÂä†ËΩΩÂ§±Ë¥•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
hanlp-1.7.4-release

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

**‰∏≠ÊñáÊñá‰ª∂Âêç**ÁöÑËá™ÂÆö‰πâËØçÂ∫ìÂä†ËΩΩÂ§±Ë¥•Ôºà**ËØ¶ÊÉÖËßÅÊà™Âõæ**Ôºâ
**Ëã±ÊñáÂêç**ÁöÑËØçÂ∫ìÂèØ‰ª•Âä†ËΩΩÊàêÂäüÔºåÊØîÂ¶ÇÔºöCustomDictionary.txtÂèØ‰ª•Âä†ËΩΩÊàêÂäü„ÄÇ
ÁÑ∂ÂêéÊàëÂ∞Ü**Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt**ÊîπÊàê**hdhybcck.txt**Â∞±ÂèØ‰ª•Âä†ËΩΩÊàêÂäü„ÄÇ

Á≥ªÁªüÔºöcentos6.10
Á≥ªÁªüÁöÑËØ≠Ë®Ä‰ΩøÁî®ÁöÑÊòØ zh_CN.UTF-8
ÁºñÁ®ãËØ≠Ë®Ä‰ΩøÁî®ÁöÑÊòØpythonÔºöpyhanlp
javaÔºöopenjdk version ""1.8.0_222""

hanlp.properties
root=/usr/local/python3/lib/python3.7/site-packages/pyhanlp/static/
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt ns;data/dictionary/person/nrf.txt nrf;

## Â§çÁé∞ÈóÆÈ¢ò



## ÂÖ∂‰ªñ‰ø°ÊÅØ
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->


======================
Êë∏Á¥¢‰∫ÜÂçäÂ§©ÈóÆÈ¢òÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü„ÄÇ
**ÂéüÂõ†ÊòØÔºöÊàëÂú®Êú¨Âú∞Ëß£Âéã‰∫Üdata-for-1.7.4.zipÂÜç‰∏ä‰º†Âà∞ÊúçÂä°Âô®ÂØºËá¥ÁöÑ~**


"
‰æùÂ≠òÂè•Ê≥ïÊ®°ÂùóÊòØÂê¶ÊúâÊñπÊ≥ïÊîπËøõ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºåÊàëÂèëÁé∞‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊ®°ÂùóÂ¶Ç‰∏ã‰∏§‰∏™ÈóÆÈ¢ò„ÄÇÈíàÂØπÁ¨¨‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊòØÂê¶ÊàëÂèØ‰ª•Ëá™Ë°åÂÅö‰øÆÊîπÊàñËÆ≠ÁªÉÊàñÊîπËøõÔºüÈíàÂØπÁ¨¨‰∫å‰∏™ÈóÆÈ¢òÔºåÊÉ≥ËØ∑ÈóÆÂú®Á∫øÁâàÊú¨ÁöÑÈÖçÁΩÆÊòØÊÄé‰πàÊ†∑ÁöÑ„ÄÇ

### 1. Âú®Á∫øÂíåÁ¶ªÁ∫øÈÉΩÂåÖÂê´ÈîôËØØ
```
hanlp parse <<< ""ÊùéÂÖãÂº∫‰∏ªÊåÅÂè¨ÂºÄÁªèÊµéÂΩ¢ÂäøÂ∫ßË∞à‰ºö""
1	ÊùéÂÖãÂº∫	ÊùéÂÖãÂº∫	nh	nr	_	2	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	‰∏ªÊåÅ	‰∏ªÊåÅ	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	Âè¨ÂºÄ	Âè¨ÂºÄ	v	v	_	2	Âπ∂ÂàóÂÖ≥Á≥ª	_	_
4	ÁªèÊµé	ÁªèÊµé	n	n	_	5	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
5	ÂΩ¢Âäø	ÂΩ¢Âäø	n	n	_	3	Âä®ÂÆæÂÖ≥Á≥ª	_	_
6	Â∫ßË∞à‰ºö	Â∫ßË∞à‰ºö	n	n	_	3	Âä®ÂÆæÂÖ≥Á≥ª	_	_
```
Á¨¨5‰∏™‚ÄúÂΩ¢Âäø‚ÄùÁöÑÁà∂ËäÇÁÇπÂ∫îËØ•ÊòØ6ËÄå‰∏çÊòØ3ÔºåÂÖ≥Á≥ªÂ∫îËØ•ÊòØÂÆö‰∏≠

![case1.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypnuk99bj208v029wek.jpg)

Êü•Áúã‰∫ÜltpÁöÑÁªìÊûúÊòØÊ≠£Á°ÆÁöÑÔºö
![case2_2.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypyc36m5j20cj02pglo.jpg)

### 2. Âú®Á∫øÊºîÁ§∫Ê≠£Á°ÆÔºåÁ¶ªÁ∫øÁâàÊú¨ÊúâÈîôËØØ

```
hanlp parse <<< ""ÊõπÂõΩËØ∑ËæûÈü©ÂõΩÊ≥ïÂä°ÈÉ®ÈïøÂÆò""
1	ÊõπÂõΩ	ÊõπÂõΩ	nh	nr	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
2	ËØ∑Ëæû	ËØ∑Ëæû	v	v	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
3	Èü©ÂõΩÊ≥ïÂä°ÈÉ®	Èü©ÂõΩÊ≥ïÂä°ÈÉ®	ni	nt	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	ÈïøÂÆò	ÈïøÂÆò	n	n	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
```
Á¨¨‰∏Ä‰∏™""ÊõπÂõΩ""ÊòæÁ§∫ÊòØÂÆö‰∏≠ÂÖ≥Á≥ª(ÈîôËØØ)ÔºåÂú®Á∫øÁâàÊú¨Â¶Ç‰∏ãÂõæÔºåÊòæÁ§∫ÊòØ‰∏ªË∞ìÂÖ≥Á≥ª(Ê≠£Á°Æ)
![case2.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7ypuq7zp3j207u02awei.jpg)

ÊÑüË∞¢~









"
ÈîôËØØÂàÜËØç‚Äú‰ºûÊä§‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

sentence= `ÊâÆÊºî‚ÄúÂ§ß‰ºûÊä§Â∞è‰ºû‚ÄùËßíËâ≤ÁöÑÂéøÂßî‰π¶ËÆ∞Ë¢´ÈÄÆÊçï`

![badcase.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7yo7qainsj20d603caag.jpg)

ËØ¥Êòé
1. Âú®Á∫øÊºîÁ§∫ÂíåÊú¨Âú∞ÁâàÊú¨ÈÉΩÂ≠òÂú®Ëøô‰∏™badcase
2. Ê£ÄÊü•‰∫Ü‰∏ãÊåÇËΩΩÁöÑÂêÑ‰∏™Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÊ≤°ÊúâÂá∫Áé∞Ëøô‰∏™ËØç
"
ÊÑüÁü•Êú∫ÂàÜËØçÂô®ÁöÑÁªìÊûúÂíåÂú®Á∫øÊºîÁ§∫‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÁõÆÂâçÂú®ÊµãËØïHanLPÁöÑÊÑüÁü•Êú∫ÂàÜËØçÔºåÂèëÁé∞ÈªòËÆ§ÈÖçÁΩÆ‰∏ãÔºå‰∏Ä‰∫õÁªìÊûú‰∏éÂú®Á∫øÊºîÁ§∫ÁöÑÁªìÊûú‰∏ç‰∏ÄËá¥ÔºåËÄå‰∏îÊïàÊûú‰∏çÂ¶ÇÂú®Á∫øÊºîÁ§∫ÁöÑÁªìÊûúÔºåËØ∑ÈóÆÊòØÂê¶Âú®Á∫øÊºîÁ§∫Áî®ÁöÑÂàÜËØçÊ®°ÂûãÊúâ‰ºòÂåñÊàñËÄÖÊõ¥Êñ∞ÔºåÊòØÂê¶‰ºöÂú®ËøëÊúüÁöÑÊñ∞ÁâàÊú¨‰∏≠ÂèëÂ∏É„ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§
1. ÊµãËØïÂè•ÔºöËÉΩ‰∏çËÉΩËøõÂâçÂçÅ
Âú®Á∫øÊºîÁ§∫ÁªìÊûúÔºö
![B8C02F9288DA606A512811A552D1C861](https://user-images.githubusercontent.com/980318/66748351-8b017600-eeb9-11e9-953a-9bdb16a951ab.jpg)

1.7.4ÊµãËØïÁªìÊûú
`ËÉΩ‰∏çËÉΩ/i ËøõÂâç/v ÂçÅ/m`

2.ÊµãËØïÂè•ÔºöÂπøÂú∫‰∏äÊúâÂæàÂ§ö‰∫∫Âú®Ë∑≥Ëàû
Âú®Á∫øÊºîÁ§∫ÁªìÊûúÔºö
![843B0807FDE9AEBBD8A725B04B84C7B3](https://user-images.githubusercontent.com/980318/66748564-0cf19f00-eeba-11e9-838b-76f38f864ddc.jpg)

1.7.4ÊµãËØïÁªìÊûú
`ÂπøÂú∫/n ‰∏äÊúâ/v ÂæàÂ§ö/m ‰∫∫/n Âú®/p Ë∑≥Ëàû/v`



## ÂÖ∂‰ªñ‰ø°ÊÅØ
Â¶ÇÊûú‰∏ç‰ºöÊúâÊñ∞ÁâàÊ®°ÂûãÂèëÂ∏ÉÔºåÊúâÊ≤°Êúâ‰ªÄ‰πàÂäûÊ≥ï‰øÆÊ≠£ÂàÜËØç„ÄÇ
Â∑≤ÁªèÂ∞ùËØïÂú®Á∫øÂ≠¶‰π†Ôºå‰ΩÜÂèëÁé∞Âπ∂‰∏ç‰∏ÄÂÆöËÉΩÂΩ±ÂìçÂàÜËØçÁöÑÁªìÊûú„ÄÇ
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"ËØ∑ÈóÆËÉΩÂê¶ÈÖçÁΩÆÂ§ö‰∏™Áî®Êà∑ËØçÂÖ∏, ÂàÜËØç‰ºòÂÖàÁ∫ßÈ°∫Â∫èÔºåÁî®Êà∑ËØçÂÖ∏1>Ê†∏ÂøÉËØçÂÖ∏>Áî®Êà∑ËØçÂÖ∏2","ÊÇ®Â•Ω,ÊÑüË∞¢ÂàÜ‰∫´
      ËØ∑ÈóÆËÉΩÂê¶ÈÖçÁΩÆÂ§ö‰∏™Áî®Êà∑ËØçÂÖ∏,ÂàÜËØç‰ºòÂÖàÁ∫ßÈ°∫Â∫èÔºåÁî®Êà∑ËØçÂÖ∏1>Ê†∏ÂøÉËØçÂÖ∏>Áî®Êà∑ËØçÂÖ∏2 ?
      Â¶ÇÊûúÈÖçÁΩÆ‰∏çËÉΩÂÆûÁé∞, ‰ª£Á†ÅÂ±ÇÈù¢ÊúâÂèØËÉΩÂÆûÁé∞Âêó? Â¶ÇÊûúÊúâÂèØËÉΩÂÆûÁé∞ÔºåÈÇ£ÊàëÊÉ≥Ëá™Â∑±ËØïÁùÄÊîπ‰ª£Á†Å„ÄÇÂ¶ÇÊûúËÉΩÂæóÂà∞‰∫õËÆ∏ÊåáÂØºÔºåÂçÅÂàÜÊÑüË∞¢~"
ÊÇ®Â•ΩÔºåËØ∑ÈóÆÊúâËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÂäüËÉΩ‰πàÔºåSRL,
ÊÄßËÉΩÊµãËØï DoubleArrayTrie ÂÜÖÂ≠ò‰ΩøÁî® > AhoCorasickDoubleArrayTrie >  Bintrie,"## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

## ÊàëÁöÑÈóÆÈ¢ò
ÊåâÁÖßÂéüÁêÜ 
   ÂØπDAT „ÄÅ ACDat „ÄÅBintrie ËøõË°åÊÄßËÉΩÊµãËØï

ÊµãËØïÊñáÊú¨ : 

""ÊúÄËøëÊ≠£Âú®ÂÅö‰∏Ä‰∏™Ëá™Â∑±ÁöÑNLPÂ∫ìÔºåÂàöËµ∑Ê≠•ÁöÑÁ¨¨‰∏Ä‰∏™ÈóÆÈ¢òÂ∞±ÊòØÂ≠óÂÖ∏ÁöÑÂÇ®Â≠ò‰∏éÊü•ËØ¢„ÄÇÊØ´Êó†ÁñëÈóÆÔºåÊúÄ‰Ω≥ÁöÑÊï∞ÊçÆÁªìÊûÑÊòØTrieÊ†ëÔºåÂêåÊó∂‰∏∫‰∫ÜÂπ≥Ë°°ÊïàÁéáÂíåÁ©∫Èó¥ÔºåÂÜ≥ÂÆö‰ΩøÁî®ÂèåÊï∞ÁªÑTrieÊ†ë„ÄÇÁé∞Âú®ÁöÑÈóÆÈ¢òÊòØÔºåÂèåÊï∞ÁªÑTrieÊ†ëÊòØ‰∏Ä‰∏™ÂéãÁº©ÁöÑTrieÊ†ë"";

ÊµãËØïËØçÂÖ∏: 
CoreNatureDictionary.txt

JVM Â†ÜËÆæÁΩÆ: -Xms2g Xmx2g 

ÊÄßËÉΩÂàÜÊûêÂ∑•ÂÖ∑: JConsole  

ÊµãËØïÁªìÊûú : 
ÊåâÁÖßÂéüÁêÜ DAT ÁöÑÂÜÖÂ≠ò‰ΩøÁî®Â∫îËØ•ÊòØ‰∏âËÄÖÊúÄÂ∞èÁöÑ
‰ΩÜÁõÆÂâçÊµãÂà∞ÁöÑÁªìÊûú DAT > ACDAT > Bintrie 
ÂèçËÄå Bintrie Â†ÜÂÜÖÂ≠ò‰ΩøÁî®ÊúÄÂ∞è

ÊÄßËÉΩÊñπÈù¢ : DAT ÊÄßËÉΩÁ∫¶Á≠â‰∫é ACDAT > Bintrie 

### Ëß¶Âèë‰ª£Á†Å

```
        String text = ""ÊúÄËøëÊ≠£Âú®ÂÅö‰∏Ä‰∏™Ëá™Â∑±ÁöÑNLPÂ∫ìÔºåÂàöËµ∑Ê≠•ÁöÑÁ¨¨‰∏Ä‰∏™ÈóÆÈ¢òÂ∞±ÊòØÂ≠óÂÖ∏ÁöÑÂÇ®Â≠ò‰∏éÊü•ËØ¢„ÄÇÊØ´Êó†ÁñëÈóÆÔºåÊúÄ‰Ω≥ÁöÑÊï∞ÊçÆÁªìÊûÑÊòØTrieÊ†ëÔºåÂêåÊó∂‰∏∫‰∫ÜÂπ≥Ë°°ÊïàÁéáÂíåÁ©∫Èó¥ÔºåÂÜ≥ÂÆö‰ΩøÁî®ÂèåÊï∞ÁªÑTrieÊ†ë„ÄÇÁé∞Âú®ÁöÑÈóÆÈ¢òÊòØÔºåÂèåÊï∞ÁªÑTrieÊ†ëÊòØ‰∏Ä‰∏™ÂéãÁº©ÁöÑTrieÊ†ë"";

public void testACDAT() throws InterruptedException {
        Thread.sleep(10000);
        Runtime.getRuntime().gc();
        TreeMap<String, CoreDictionary.Attribute> map = new TreeMap<String, CoreDictionary.Attribute>();
        IOUtil.LineIterator iterator = new IOUtil.LineIterator(""data/dictionary/CoreNatureDictionary.txt"");
        while (iterator.hasNext())
        {
            String line = iterator.next().split(""\\s"")[0];
            map.put(line, new CoreDictionary.Attribute(Nature.n));
        }
        Runtime.getRuntime().gc();
        System.out.println("" Map ÊûÑÂª∫ÊàêÂäü !!!!!"");
        Thread.sleep(10000);

        //AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute> act = new AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute>();
        //DoubleArrayTrie<CoreDictionary.Attribute> act = new DoubleArrayTrie<CoreDictionary.Attribute>();
        BinTrie<CoreDictionary.Attribute> act = new BinTrie<CoreDictionary.Attribute>();
        long timeMillis = System.currentTimeMillis();
        act.build(map);
        System.out.println(""ÊûÑÂª∫ËÄóÊó∂Ôºö"" + (System.currentTimeMillis() - timeMillis) + "" ms"");
        timeMillis = System.currentTimeMillis();
        for(int i = 0 ; i < 30000000; i ++){
            act.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>() {
                @Override
                public void hit(int begin, int end, CoreDictionary.Attribute value) {

                }
            });
        }
        System.out.println(""ÂàÜËØçËÄóÊó∂Ôºö"" + (System.currentTimeMillis() - timeMillis) + "" ms"");
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
hanlpÂíåpyhanlpÁöÑdependency parseÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

jar  1.7.4: lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/hanlp-1.7.4.jar
data 1.7.4: lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/data
config    : lib/python3.7/site-packages/pyhanlp-0.1.49-py3.7.egg/pyhanlp/static/hanlp.properties


<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

```
sent = ""‰∏≠ÂõΩ11ÂÆ∂‰∏ªÊµÅÂø´ÈÄíÂÖ¨Âè∏ÂÆ£Â∏ÉÂºÄÂêØÂ§áÊàòÂèå11""
from pyhanlp import HanLP, JClass
sentence = HanLP.parseDependency(sent)
print([w.LEMMA for w in sentence.iterator()])
```
ÂæóÂà∞ÁöÑÁªìÊûúÊòØ: ['‰∏≠ÂõΩ', '11', 'ÂÆ∂‰∏ª', 'ÊµÅ', 'Âø´ÈÄíÂÖ¨Âè∏', 'ÂÆ£Â∏É', 'ÂºÄÂêØ', 'Â§áÊàò', 'Âèå', '11']

‰ΩÜÊòØÔºåÁõ¥Êé•Âú®ÂëΩ‰ª§Ë°å‰∏≠‰ΩøÁî®`hanlp segment <<< ""‰∏≠ÂõΩ11ÂÆ∂‰∏ªÊµÅÂø´ÈÄíÂÖ¨Âè∏ÂÆ£Â∏ÉÂºÄÂêØÂ§áÊàòÂèå11""`(Êàñhanlp parse)ÂæóÂà∞ÁöÑÁªìÊûúÊòØ:
‰∏≠ÂõΩ/ns 11/m ÂÆ∂/q ‰∏ªÊµÅ/n Âø´ÈÄíÂÖ¨Âè∏/nis ÂÆ£Â∏É/v ÂºÄÂêØ/v Â§áÊàò/vi Âèå/q 11/m

ÊòéÊòæÁ¨¨‰∫å‰∏™ÊòØÂØπÁöÑÔºå‰πüÂ∞±ÊòØpyhanlpÁöÑÁªìÊûú‰∏∫‰ªÄ‰πàÂíåhanlp‰∏ç‰∏ÄËá¥Âë¢Ôºü


"
Ëá™Âª∫ËØçÂÖ∏Êó†Ê≥ïÁîüÊàê.txt.binÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â∞ÜËØçÂÖ∏ËΩ¨Êç¢‰∏∫csvÊñá‰ª∂ÂêéÔºåÊó†Ê≥ïÊ≠£Â∏∏ÂàÜËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âõ†‰∏∫ÊàëË¶ÅÂØπËã±ÊñáËøõË°åÂàÜËØçÔºåÊâÄ‰ª•ÊàëÂ∞ÜËØçÂÖ∏ËΩ¨Âèò‰∏∫csvÊ†ºÂºè„ÄÇ‰ΩÜÊòØËΩ¨Êç¢ÂÆå‰πãÂêéÔºåÊó†Ê≥ïÂàÜËØç„ÄÇËΩ¨Êç¢ÁöÑÂéüÊñá‰ª∂ÊòØ""198901.txt"" ,ËØ•Êñá‰ª∂ËΩ¨Êç¢ÂâçÔºåËøõË°åËÆ≠ÁªÉÁªìÊûúÊ≠£Â∏∏

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂ∞ÜtxtÊñá‰ª∂Êîπ‰∏∫csvÊñá‰ª∂

‰øÆÊîπÂêéÔºåÊØèË°åÊï∞ÊçÆÂ¶Ç‰∏ã
```
ÊÄÄ/Ng,Êè£/v,Ëøô/r,Â¶ÇÊ≥£Â¶ÇËØâ/i,ÁöÑ/u,ÂëµÊä§/vn,Ôºå/w
```
2. ÁÑ∂ÂêéÊåâÁÖß‰∏ãÂàó‰ª£Á†ÅËÆ≠ÁªÉÔºåÁªìÊûúÂ∞±‰∏çÂØπ


### Ëß¶Âèë‰ª£Á†Å

```
public class DemoTrainCWS{

    public static void main(String[] args) throws IOException {

        PerceptronTrainer trainer = new CWSTrainer();
        PerceptronTrainer.Result result = trainer.train(
                ""data/ordinaryData/addSpu/1008-origin.csv"",
                ""data/myDemo/cws.bin""
        );

        System.out.printf(""ÂáÜÁ°ÆÁéáF1:%.2f\n"", result.getAccuracy());
        PerceptronSegmenter segment = new PerceptronSegmenter(result.getModel());
        System.out.println(segment.segment(""ÂïÜÂìÅ‰∏éÊúçÂä°""));
    }


}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
['ÂïÜÂìÅ','Âíå','ÊúçÂä°']
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
['ÂïÜÂìÅÂíåÊúçÂä°']
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
jpypeË∞ÉÁî®ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
jpypeË∞ÉÁî®Êä•ÈîôÔºöjpype._jclass.ArrayIndexOutOfBoundsException: 1208
Êï¥‰ΩìÊä•ÈîôÔºö
Traceback (most recent call last):
  File ""entity_2.py"", line 123, in <module>
    main()
  File ""entity_2.py"", line 48, in main
    sentence = HanLP.parseDependency(line)
jpype._jclass.ArrayIndexOutOfBoundsException: 1208


## Â§çÁé∞ÈóÆÈ¢ò
Ê≤°ÊúâÈ¢ùÂ§ñÁöÑÊìç‰ΩúÔºåÂè™ÊòØË∞ÉÁî®‰∫ÜÂè•Ê≥ïÂàÜÊûêËøô‰∏™Êé•Âè£„ÄÇ


### Ëß¶Âèë‰ª£Á†Å
sentence = HanLP.parseDependency(line)


"
Â¶Ç‰ΩïÈÅøÂÖç ÈòøËé´Ë•øÊûó Ë¢´ÂàÜ‰∏∫ ‚ÄùÈòøËé´‚Äú ‚ÄùË•øÊûó‚ÄúÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö<version>portable-1.7.4</version>

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   HanLP.segment(sentence);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
  ËçØÂìÅ ÈòøËé´Ë•øÊûó
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
 ÂÆûÈôÖ‰ª£Á†ÅËæìÂá∫Ôºö ËçØÂìÅ ÈòøËé´ Ë•øÊûó 
  Âú®Á∫øhanlp.comÂèØ‰ª•ÊåâÊúüÊúõÂàÜËØçÔºå‰ΩÜÊòØÂ∫îÁî®Á®ãÂ∫èËæìÂá∫Â∞Ü ÈòøËé´Ë•øÊûó ÂàÜÂºÄ‰∫ÜÔºå  http://www.hanlp.com/?sentence=%E8%8D%AF%E5%93%81%E9%98%BF%E8%8E%AB%E8%A5%BF%E6%9E%97

 ËØ∑ÈóÆÂ∫îÁî®Á®ãÂ∫è‰∏≠ Â¶Ç‰ΩïËÆæÁΩÆÔºü

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊàëÂèØ‰ª•Áî®Ëá™Â∑±ÁöÑËÆ≠ÁªÉÈõÜËÆ≠ÁªÉ‰∏Ä‰∏™‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊ®°ÂûãÂêó,
ÂàÜËØçbadcase‚ÄúÂçÅ‰∏ÄÊé®ÈÄÅÂÜÖÂÆπ‚ÄùÔºå‰∏çÁü•Â¶Ç‰ΩïÁ∫†Ê≠£,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰∏ãËΩΩ‰∫Ü1.7.4ÁöÑÊï∞ÊçÆÂåÖ  HanLP.segmentÊù•ÂàÜËØç‚ÄúÂçÅ‰∏ÄÊé®ÈÄÅÂÜÖÂÆπ‚ÄùÔºåÂá∫Áé∞badcaseÔºå‰∏çÁü•Â¶Ç‰ΩïÁ∫†Ê≠£

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

HanLP.segment(""ÂçÅ‰∏ÄÊé®ÈÄÅÂÜÖÂÆπ"")

### Ëß¶Âèë‰ª£Á†Å


### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÂçÅ‰∏Ä/Êé®ÈÄÅ/ÂÜÖÂÆπ
```
### ÂÆûÈôÖËæìÂá∫


<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂçÅ/‰∏ÄÊé®/ÈÄÅ/ÂÜÖÂÆπ
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Á∫ø‰∏ädemoÁöÑÁªìÊûúÂíåÊú¨Âú∞ÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
hanlp-1.7.4.jar
python 3.6.9
mac osx 10.14

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊµãËØïËØ≠Âè•: `‰∏äÊµ∑ÂÜúÂïÜÈì∂Ë°å1600‰∏áËÇ°ËÇ°ÊùÉÂ∞ÜË¢´ÊãçÂçñ`
‰ªé[Âú®Á∫ødemo](http://hanlp.com/)ÂæóÂà∞ÁöÑÁªìÊûúÂ¶Ç‰∏ãÂõæ:
![demo.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7e589ip3vj20j30lfgn1.jpg)

‰ΩÜÊòØÂú®Êú¨Âú∞ÂæóÂà∞ÁöÑÁªìÊûúÂ¶Ç‰∏ã:
![local.png](http://ww1.sinaimg.cn/large/901f9a6fgy1g7e58nwuyij20gg05mjs6.jpg)

Â¶Ç‰ΩïÂæóÂà∞Âú®Á∫øÁöÑËøô‰∏™ÁªìÊûúÂë¢Ôºü"
do refactor in segment package,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->
Hi.
I do refactor in the ```com.hankcs.hanlp.seg``` package. 

- rename four abstract class with word ```Abstract``` ahead, and put them into the ```com.hankcs.hanlp.seg.base``` package;

- remove ```com.hankcs.hanlp.seg.common.Config``` class into ``com.hankcs.hanlp.seg.common``` package, package permission of this class has also been modified;

- some package permission of ```com.hankcs.hanlp.seg.base.AbstractSegment```'s method has been modified;

- mainly, I write a ```com.hankcs.hanlp.seg.Segment``` class with a nested class ```Builder``` , and a ```com.hankcs.hanlp.seg.common.SegmentTypeEnum``` class, to provide a easier way to know what seg method can be used.

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Âêå‰∏Ä‰∏™ËØçÂú®‰∏çÂêåÁöÑÂè•Â≠ê‰∏≠ÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö`<version>portable-1.7.4</version>`


<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âêå‰∏Ä‰∏™ËØçÂú®‰∏çÂêåÁöÑÂè•Â≠ê‰∏≠ÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥Ôºà‰∏ãÈù¢‰ª•""Â∞±ÊòØ""Ëøô‰∏™ËØç‰∏∫‰æã)

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

‰∏â‰∏™ÊµãËØïÂè•Â≠êÔºö""ÊàëÂ∞±ÊòØÊàë"", ""Â∞±ÊòØÊàë"", ""‰Ω†Â•ΩÂ∞±ÊòØ‰∫Ü""
ËæìÂá∫ÁªìÊûú: ""Êàë Â∞± ÊòØ Êàë"", ""Â∞± ÊòØ Êàë"", ""‰Ω†Â•Ω Â∞±ÊòØ ‰∫Ü""
"
ÂΩìÂú∞ÂêçÁ¨¨‰∫å‰ΩçÊòØÊñπ‰ΩçËØçÊó∂ ÂàÜËØç‰∏çÂáÜÁ°Æ,"* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.5.0

## ÊàëÁöÑÈóÆÈ¢ò
Âú®Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÊñáÊú¨‰∏≠ÈÖçÁΩÆ‰∫ÜÂú∞Âêç‚ÄúÊ∑ÆÂçó ns 100‚Äú 100ÊòØËæÉÂ§ßÁöÑËØçÈ¢ë ‰ΩÜÂΩìËæìÂÖ•‚Äù6Êúà28Êó•Ê∑ÆÂçó‚ÄúÊó∂Ôºå‰∏çËÉΩÂæóÂà∞‚ÄùÊ∑ÆÂçó‚Äú
ÂàÜËØçÁªìÊûú‰∏∫[6/m, Êúà/qt, 28/m, Êó•Ê∑ÆÂçó/ns]

## Â§çÁé∞ÈóÆÈ¢ò
Segment SEGMENT = HanLP.newSegment()
					.enableCustomDictionaryForcing(true)
					.enableNameRecognize(true)
					.enableTranslatedNameRecognize(true)
					.enableJapaneseNameRecognize(true)
					.enablePlaceRecognize(true)
					.enableOrganizationRecognize(true)
					.enablePartOfSpeechTagging(true);

### Ê≠•È™§
1. È¶ñÂÖàÂú®\data\dictionary\custom‰∏ãÊ∑ªÂä†‰∫ÜÊñ∞ÊñáÊú¨Êñá‰ª∂ ‰ΩøÁî®utf-8ÁºñÁ†Å crlfÊç¢Ë°å
2. ÁÑ∂ÂêéÂú®ÈÖçÁΩÆÊñá‰ª∂‰∏≠Ê∑ªÂä†""Ê∑ÆÂçó ns 100""

### ÊúüÊúõËæìÂá∫
ÊúüÊúõÂàÜËØçÁªìÊûú‰∏∫[6,Êúà,28,Êó•,Ê∑ÆÂçó]

### ÂÆûÈôÖËæìÂá∫
ÂÆûÈôÖÂàÜËØçÁªìÊûú‰∏∫[6,Êúà,28,Êó•Ê∑ÆÂçó]
Ê≤°ÊúâÂàÜÂá∫Âú∞ÂêçÊ∑ÆÂçó

## ÂÖ∂‰ªñ‰ø°ÊÅØ
‚ÄúËÇ•‰∏ú‚Äù„ÄÅ‚ÄúËÇ•Ë•ø‚ÄùÁ≠â Á¨¨‰∫å‰∏™Â≠óÊòØÊñπ‰ΩçËØçÁöÑ ‰∏çËëóÂêçÁöÑ Âú∞Âêç ÈÉΩ‰ºöÂàÜÊàê‚ÄúÊó•ËÇ•‰∏ú‚ÄùÔºå‚ÄúÊó•ËÇ•Ë•ø‚Äù"
Â¶Ç‰ΩïÊõ¥ÁªÜËá¥ÁöÑÂàÜÂâ≤Ëã±ÊñáÂ≠óÁ¨¶ÂíåÁ©∫Â≠óÁ¨¶ ‰ª•ÂèäÊ±âÂ≠óÂíåÊï∞Â≠óÁªÑÊàêÁöÑÊï∞ÈáèËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ËøêË°åÂ¶Ç‰∏ãÁ®ãÂ∫èÂêéÔºåÊñáÊú¨‰∏≠ÁöÑ ‚Äú3.‰∏Ä‚Äù‰ºöËØÜÂà´Êàê‰∏Ä‰∏™ËØçÔºåËã±ÊñáÂ≠óÁ¨¶ÁöÑÂè≥Êã¨Âè∑ÂíåÊç¢Ë°åÁ¨¶\r\nËØÜÂà´Êàê‰∏Ä‰∏™ËØçÔºåÂ¶Ç‰ΩïÊâçËÉΩÊää‰ªñ‰ª¨ÂàÜÂºÄÂë¢„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
  public static void main(String[] args) {
		
	String text = ""3.‰∏Ä‰ΩçÈ°πÁõÆÁªèÁêÜÂ∫îËØ•ÂÅö‰∏ãÂàóÂì™‰∏ÄÈ°πÔºü(C)\r\n"" ;
	    
	List<Term> term = HanLP.newSegment().enableOffset(true).enableIndexMode(true).enableIndexMode(1).seg(text);
		
	   System.out.println(term.toString());
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[3/m,./w,‰∏Ä/m, ‰Ωç/q, È°πÁõÆ/n, ÁªèÁêÜ/n, Â∫îËØ•/v, ÂÅö/v, ‰∏ãÂàó/b, Âì™/r, ‰∏Ä/m, È°π/q, Ôºü/w, (/w, C/nx, )/w, /w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[3.‰∏Ä/m, ‰Ωç/q, È°πÁõÆ/n, ÁªèÁêÜ/n, Â∫îËØ•/v, ÂÅö/v, ‰∏ãÂàó/b, Âì™/r, ‰∏Ä/m, È°π/q, Ôºü/w, (/w, C/nx, )
/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Spark‰ΩøÁî®HDFSËØçÂ∫ìÂàÜËØçÔºåÊä•java.lang.StackOverflowError,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->






## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.4

## ÊàëÁöÑÈóÆÈ¢ò

ËØçÂ∫ìdataÁõÆÂΩïÂ∑≤‰∏ä‰º†Âà∞hdfs‰∏äÔºõ

ÈÖçÁΩÆÊñá‰ª∂Â¶Ç‰∏ãÔºö
![image](https://user-images.githubusercontent.com/10821279/64748199-d97ec600-d543-11e9-8004-f8d410a365a2.png)

Â∑≤ÁªèÂÆûÁé∞‰∫ÜIOÈÄÇÈÖçÂô®‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
![image](https://user-images.githubusercontent.com/10821279/64747652-38434000-d542-11e9-9a7e-55a7889448b2.png)

## Â§çÁé∞ÈóÆÈ¢ò

sparkÈõÜÁæ§‰∏ä‰∏ÄÂè•ÁÆÄÂçïÁöÑÂàÜËØçÂá∫Áé∞ÈîôËØØÔºå‰ª£Á†ÅÂ¶Ç‰∏ãÔºö

![image](https://user-images.githubusercontent.com/10821279/64747583-fc0fdf80-d541-11e9-9f41-059c8c3583d7.png)

ÈîôËØØÊó•ÂøóÂ¶Ç‰∏ãÔºö

![image](https://user-images.githubusercontent.com/10821279/64747625-206bbc00-d542-11e9-9cd2-0cb391493602.png)

Â¶Ç‰Ωï‰øÆÊîπÔºü

"
CharType ÊòØ‰∏çÊòØÊúâÈóÆÈ¢òÔºüÂ≠óÊØçÈÉΩË¢´Ê†áËØÜ‰∏∫CT_SINGLE,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âª∫ËÆÆÊäädemoÊîæÂú®Â§ñÈù¢ ÔºåËÄå‰∏çÊòØtestÈáåÈù¢ÔºåÊñπ‰æøÁî®Êà∑Êü•Êâæ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ΩøÁî®geventÂ§öËøõÁ®ãÂêØÂä®flaskÊä•ÈîôError occurred during initialization of VM Could not reserve enough space for object heap,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰Ω†Â•ΩÔºåÂú®‰ΩøÁî®geventÂ§öËøõÁ®ãÂêØÂä®flaskÊúçÂä°Êó∂ÔºåÂú®ÊúçÂä°‰∏≠Êúâ‰∏Ä‰∏™Ê®°Âùó‰ΩøÁî®Âà∞hanlp‰∏≠ÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶ÂàÜÊûêÔºåË∞ÉÁî®ÊñπÊ≥ïÂ¶Ç‰∏ãËø∞‰ª£Á†ÅÔºåÂêØÂä®ÊúçÂä°Êó∂Êä•ÈîôError occurred during initialization of VM; Could not reserve enough space for object heapÔºåËã•ÊòØÂ∞ÜËØ•Ê®°Âùó‰ªéÊúçÂä°‰∏≠ÁßªÈô§ÔºåÈáçÊñ∞ÂêØÂä®ÊúçÂä°ÔºåÊúçÂä°ÂèØ‰ª•Ê≠£Â∏∏ÂêØÂä®„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import SafeJClass

@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))

class start_model_server:
    def __init__(self, port):
        self.port = port
    
    def start_new_server(self): 
        server = WSGIServer(('0.0.0.0', self.port), app)
        server.init_socket()
        server.start()
        server.start_accepting()
        server._stop_event.wait()
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    doc2vec = DocVectorModel(word2vec._proxy)
    from multiprocessing import Process
    ports = [6000,6001,6002,6003]
    for port in ports:
        s = start_model_server(port)
        p = Process(target=s.start_new_server)
        p.start()
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúçÂä°Ê≠£Â∏∏ÂêØÂä®
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Error occurred during initialization of VM
Could not reserve enough space for object heap
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âêå‰πâËØçÂ§öÊ¨°Âá∫Áé∞‰ºöË¢´Ë¶ÜÁõñ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Â¶ÇÊûúÂêå‰∏Ä‰∏™ËØçÂ§öÊ¨°Âá∫Áé∞Ôºå‰ºöË¢´ÊõøÊç¢Ôºå‰æãÂ¶ÇÔºöAa01A01=Âåó‰∫¨ Âåó‰∫¨‰∫∫   Aa01A02=Âåó‰∫¨ Âåó‰∫¨Â§©ÂÆâÈó®

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    for (Synonym synonym : synonymList)
                {
                    treeMap.put(synonym.realWord, new SynonymItem(synonym, synonymList, type));
                    // ËøôÈáåÁ®çÂæÆÂÅö‰∏™test
                    //assert synonym.getIdString().startsWith(line.split("" "")[0].substring(0, line.split("" "")[0].length() - 1)) : ""ËØçÂÖ∏ÊúâÈóÆÈ¢ò"" + line + synonym.toString();
                }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
ËøôÈáåÂèØ‰ª•Âà§Êñ≠ÊòØÂê¶Â≠òÂú®ÔºåÂ≠òÂú®ÂàôËøΩÂä†ÔºåÂèç‰πãÊñ∞Â¢û
```
Âåó‰∫¨ Âåó‰∫¨‰∫∫ Âåó‰∫¨Â§©ÂÆâÈó®
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Âåó‰∫¨ Âåó‰∫¨Â§©ÂÆâÈó®
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰∏ÄÊòØËøôÊòØËá™Â∑±ÁöÑËøΩÊ±Ç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
pythonÁâàÔºö
ÈÇ£ÂèØÊòØÂ§èÁöá‰∏ú‰ºØÂ∏ùÂêõÁöÑÂÆ∂Êóè
[ÈÇ£/rzv, ÂèØÊòØ/c, Â§èÁöá/nr, ‰∏ú/f, ‰ºØ/ng, Â∏ùÂêõ/n, ÁöÑ/ude1, ÂÆ∂Êóè/n]
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
 segment = HanLP.newSegment().enableNameRecognize(True)
term_list = segment.seg(sentence)
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éËØçÂÖ∏ÁºñÁ†ÅÁöÑÁñëÈóÆ,"Âêå‰πâËØçÁºñÁ†ÅË°® = # @ ÂàÜÂà´Ë°®Á§∫‰ªÄ‰πàÊÑèÊÄù
‰æãÂ¶ÇÔºö
Aa01C05@ ‰ºóÂ≠¶Áîü
Aa01C06# Â¶áÂ≠∫ Áà∂ËÄÅÂÖÑÂºü Áî∑Â•≥ËÄÅÂ∞ë Áî∑Â•≥ËÄÅÂπº
Aa01C07# ÂÖöÁæ§ Âπ≤Áæ§ ÂÜõÊ∞ë Â∑•ÂÜúÂÖµ Âä≥ËµÑ ‰∏ª‰ªÜ ÂÆæ‰∏ª ÂÉß‰øó Â∏àÂæí Â∏àÁîü Â∏àÁîüÂëòÂ∑• ÊïôËÅåÂëòÂ∑• Áæ§‰Ωì Áà±ÂõΩÂøóÂ£´ ÂÖöÂ§ñ‰∫∫Â£´ Ê∞ë‰∏ª‰∫∫Â£´ Áà±ÂõΩ‰∫∫Â£´ ÊîøÁæ§ ÂÖöÊîøÁæ§ ÈùûÂÖö‰∫∫Â£´ ‰∏öÂÜÖ‰∫∫Â£´ Â∑•ÂÜúÂàÜÂ≠ê ÂÜõË≠¶Ê∞ë ÂÖöÊîøÂÜõÊ∞ë
Aa01D01@ ËßíËâ≤
Aa02A01= Êàë Âí± ‰ø∫ ‰Ωô Âêæ ‰∫à ‰æ¨ Âí±ÂÆ∂ Êú¨‰∫∫ Ë∫´ ‰∏™‰∫∫ ‰∫∫ÂÆ∂ ÊñØ‰∫∫
ËØçÊÄßÂêéÈù¢ÁöÑÊï∞Â≠óÊòØ‰ªÄ‰πàÊÑèÊÄù
‰æãÂ¶ÇÔºö
ÈæôÂ≤ó A 14 B 6 X 1
‰∏Ä‰∏áÈÅç	nz	9
‰∏Ä‰∏çÂéã‰ºó	i	3
‰∏Ä‰∏çÊÄïËã¶	l	10"
PythonÁâàÂÆâË£ÖÂ§±Ë¥•,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
Python 3.7
pyhanlp v0.1.22
hanlp 1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂÆâË£ÖÂêéÂàùÊ¨°ËøêË°åÔºåÊèêÁ§∫‰∫Ü‰ª•‰∏ãÈîôËØØÔºö

`java.lang.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0
`

Âú®TerminalÊü•ÁúãhanlpÁä∂ÊÄÅÔºåÊèêÁ§∫Ôºö
`
Traceback (most recent call last):
  File ""/usr/local/bin/hanlp"", line 6, in <module>
    from pyhanlp.main import main
  File ""/usr/local/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 122, in <module>
    _start_jvm_for_hanlp()
  File ""/usr/local/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 119, in _start_jvm_for_hanlp
    HANLP_JVM_XMX, convertStrings=True)
  File ""/usr/local/lib/python3.7/site-packages/jpype/_core.py"", line 219, in startJVM
    _jpype.startup(jvmpath, tuple(args), ignoreUnrecognized, convertStrings)
jpype._jclass.UnsupportedClassVersionError: org/jpype/classloader/JPypeClassLoader : Unsupported major.minor version 52.0
`

ÊàëÂÆâË£Ö‰∫ÜÊúÄÊñ∞ÁöÑJDKÔºå‰ΩÜ‰ªçÊó†Ê≥ïËøêË°å„ÄÇËØ∑ÈóÆÊòØ‰ªÄ‰πàÂéüÂõ†ÔºüÊòØÂíå #1244 Âêå‰∏Ä‰∏™ÈóÆÈ¢òÂêóÔºü"
Ëá™ÂÆö‰πâËØçÊÄßÂíåËØ≠Ê≥ïÂàÜÊûêÂô®Ëß£ÊûêÁªìÊûú‰∏≠ÁöÑËØçÊÄß‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ



ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4 protable.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.6.8 protable.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà: Âú®customerdictionary.txt Ê∑ªÂä†Êñ∞ËØçÂíåËØçÊÄßÔºåÂà∑Êñ∞ÁºìÂ≠òÂêéÔºåË∞ÉÁî®hanlp.segÔºàÔºâÊñπÊ≥ï
ÂàÜËØçÊïàÊûúÂêàÁêÜ
2. ÁÑ∂ÂêéÔºåË∞ÉÁî®dependancyPraserËøõË°åËØ≠Ê≥ïÂàÜÊûêÔºåÂèëÁé∞posÁªìÊûúÂíå‰πãÂâç‰∏ç‰∏ÄËá¥
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

public static void main(String[] args)
    {

        CoNLLSentence sentence = HanLP.parseDependency(""ÂÖçË¥πÊàøËµ†ÈÄÅÁßØÂàÜÂêó"");
        System.out.println(HanLP.segment(""ÂÖçË¥πÊàøËµ†ÈÄÅÁßØÂàÜÂêó""));
        System.out.println(sentence);
}

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
[ÂÖçË¥πÊàø/n, Ëµ†ÈÄÅ/v, ÁßØÂàÜ/n, Âêó/y]
1	ÂÖçË¥πÊàø	ÂÖçË¥πÊàø	**n	n**	_	2	Áä∂‰∏≠ÁªìÊûÑ	_	_
2	Ëµ†ÈÄÅ	Ëµ†ÈÄÅ	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	ÁßØÂàÜ	ÁßØÂàÜ	n	n	_	2	Âä®ÂÆæÂÖ≥Á≥ª	_	_
4	Âêó	Âêó	e	y	_	2	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_

### ÂÆûÈôÖËæìÂá∫
[ÂÖçË¥πÊàø/n, Ëµ†ÈÄÅ/v, ÁßØÂàÜ/n, Âêó/y]
1	ÂÖçË¥πÊàø	ÂÖçË¥πÊàø	v	vd	_	2	Áä∂‰∏≠ÁªìÊûÑ	_	_
2	Ëµ†ÈÄÅ	Ëµ†ÈÄÅ	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	ÁßØÂàÜ	ÁßØÂàÜ	n	n	_	2	Âä®ÂÆæÂÖ≥Á≥ª	_	_
4	Âêó	Âêó	e	y	_	2	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_




## ÂÖ∂‰ªñ‰ø°ÊÅØ
txtÂÜÖÂÆπ
ÔºöÂÖçË¥πÊàø n 1
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊñáÊú¨ËÅöÁ±ªClusterAnalyzerÊä•ÈîôÔºöNullÊåáÈíà,"repeatedBisectionÂèÇÊï∞Ë∞ÉËäÇËøáÁ®ã‰∏≠Ôºåcom.hankcs.hanlp.mining.cluster.ClusterAnalyzer  207Ë°å‰ºöÊä•Á©∫ÊåáÈíàÈîôËØØ„ÄÇ
![image](https://user-images.githubusercontent.com/18278523/63823557-db168e80-c986-11e9-967f-8232c42cd3ab.png)
"
ÊúâJavaScript ÁâàÊú¨ÂêóÔºü,Â∏åÊúõÂá∫‰∏Ä‰∏™ JavaScript ÁâàÊú¨
ÂÄíË£ÖÂè•ÂàÜÊûê,"
‚ÄúÂêÉÈ•≠‰∫ÜÂêó‰Ω†‚Äù Â§ÑÁêÜÊàê ‚Äù‰Ω†ÂêÉÈ•≠‰∫ÜÂêó‚Äú

ËøôÁßçÂÄíË£ÖÂè•Â§ÑÁêÜÊúâ‰ªÄ‰πàÂª∫ËÆÆÂêó"
ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏ÔºåÂú®Á∫øÂ≠¶‰π† Â§çÂêàËØçÂèäËá™ÂÆö‰πâËØçÊÄßÊ†áÊ≥®ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÄªÁöÑÈúÄÊ±ÇÊòØËá™ÂÆö‰πâËØçÊÄßÂíåÂ§çÂêàËØçËÉΩÂ§üÁªìÂêàËµ∑Êù•
‰æãÂ¶Ç‰∏ãÈù¢‰∏ÄÊÆµËØùÔºö
String text = ""ËÖ∞ÈÖ∏„ÄÅ‰πèÂäõ„ÄÅÂè£Âπ≤„ÄÅÁõÆÊ∂©„ÄÅÊâãË∂≥ÂøÉÁÉ≠„ÄÅÂ§úÂ∞øÂ§ö„ÄÅËÑâÁªÜÊàñÂº±"";

‰ΩøÁî®ÁöÑÊñπÊ≥ïÈªòËÆ§ÁöÑÂàÜËØçÁªìÊûú‰∏∫Ôºö
ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w ÊâãË∂≥/n ÂøÉÁÉ≠/n „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

ÊàëÂ∏åÊúõÁöÑÊïàÊûúÔºö
ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w  [ÊâãË∂≥ÂøÉ/n ÁÉ≠/a] /clinic „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

ÊàëÂ∞ùËØï‰∫ÜÂú®Á∫øÂ≠¶‰π†ÂíåËá™ÂÆö‰πâËØçÂÖ∏‰∏§ÁßçÊñπÊ≥ïÔºåÂèëÁé∞ÈÉΩÊ≤°ÂäûÊ≥ïÂæóÂà∞È¢ÑÊÉ≥ÁöÑÊïàÊûú„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

1„ÄÅËá™ÂÆö‰πâËØçÂÖ∏Ôºö
ËØçÂÖ∏ÔºöÊâãË∂≥ÂøÉÁÉ≠ clinic 1000

String text = ""ËÖ∞ÈÖ∏„ÄÅ‰πèÂäõ„ÄÅÂè£Âπ≤„ÄÅÁõÆÊ∂©„ÄÅÊâãË∂≥ÂøÉÁÉ≠„ÄÅÂ§úÂ∞øÂ§ö„ÄÅËÑâÁªÜÊàñÂº±"";

        PerceptronLexicalAnalyzer analyzer = new 
                   PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);

        System.out.println(analyzer.analyze(text));

### ÊúüÊúõËæìÂá∫

ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w  [ÊâãË∂≥ÂøÉ/n ÁÉ≠/a] /clinic „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

### ÂÆûÈôÖËæìÂá∫

ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w ÊâãË∂≥ÂøÉÁÉ≠/clinic „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

ËøôÁßçÊñπÂºèÂàÜËØçÂíåËØçÊÄßÊ†áÊ≥®ÊòØÂØπÁöÑÔºå‰ΩÜÂç¥‰∏çÊòØÂ§çÂêàËØç

2„ÄÅÂú®Á∫øÂ≠¶‰π†ÁöÑÊñπÂºèÔºö
String text = ""ËÖ∞ÈÖ∏„ÄÅ‰πèÂäõ„ÄÅÂè£Âπ≤„ÄÅÁõÆÊ∂©„ÄÅÊâãË∂≥ÂøÉÁÉ≠„ÄÅÂ§úÂ∞øÂ§ö„ÄÅËÑâÁªÜÊàñÂº±"";

        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);

        analyzer.getNERTagSet().nerLabels.add(""clinic"");
        analyzer.learn(""[ÊâãË∂≥ÂøÉ/n  ÁÉ≠/a]/clinic"");
        System.out.println(analyzer.analyze(text));

### ÊúüÊúõËæìÂá∫

ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w  [ÊâãË∂≥ÂøÉ/n ÁÉ≠/a] /clinic „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

### ÂÆûÈôÖËæìÂá∫
ËÖ∞ÈÖ∏/n „ÄÅ/w ‰πèÂäõ/a „ÄÅ/w Âè£Âπ≤/n „ÄÅ/w ÁõÆÊ∂©/Ng „ÄÅ/w ÊâãË∂≥ÂøÉÁÉ≠/n „ÄÅ/w Â§úÂ∞øÂ§ö/j „ÄÅ/w ËÑâ/Ng ÁªÜ/a Êàñ/c Âº±/a

ËøôÁßçÊñπÂºèÂàÜËØçÊòØÂØπÁöÑÔºå‰ΩÜÊòØËØçÊÄßÊ†áÊ≥®ÊòØÈîôÁöÑÔºå‰πü‰∏çÊòØÂ§çÂêàËØç
"
NLPTokenizer ÂØπ‰∫é Âè•Â≠ê‰∏≠ ‚ÄúÂíå‚Äù Âíå ‚Äú‰∏é‚Äù ÁöÑËß£Êûê‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®‰ΩøÁî®NLPTokenizer ËøõË°åÂàÜÊàêÁöÑËøáÁ®ã‰∏≠ ÂØπ‰∫é‚ÄúÂíå‚Äù Âíå‚Äú‰∏éÁöÑËß£Èáä‰∏ç‰∏ÄÊ†∑‚Äù
‰ª£Á†ÅÔºö
System.out.println(NLPTokenizer.segment(""‰∏ªÂ∏≠‰∏éÁâπÊúóÊôÆÈÄöÁîµËØù""));
System.out.println(NLPTokenizer.segment(""‰∏ªÂ∏≠ÂíåÁâπÊúóÊôÆÈÄöÁîµËØù""));

ËæìÂá∫ÁöÑÁªìÊûúÔºö
[‰∏ªÂ∏≠/n, ‰∏é/c, ÁâπÊúóÊôÆ/nrf, ÈÄö/v, ÁîµËØù/n]
[‰∏ªÂ∏≠/n, Âíå/c, ÁâπÊúó/nr, ÊôÆÈÄö/a, ÁîµËØù/n]

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    System.out.println(NLPTokenizer.segment(""‰∏ªÂ∏≠‰∏éÁâπÊúóÊôÆÈÄöÁîµËØù""));
     System.out.println(NLPTokenizer.segment(""‰∏ªÂ∏≠ÂíåÁâπÊúóÊôÆÈÄöÁîµËØù""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
[‰∏ªÂ∏≠/n, ‰∏é/c, ÁâπÊúóÊôÆ/nrf, ÈÄö/v, ÁîµËØù/n]
[‰∏ªÂ∏≠/n, ‰∏é/c, ÁâπÊúóÊôÆ/nrf, ÈÄö/v, ÁîµËØù/n]

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
[‰∏ªÂ∏≠/n, ‰∏é/c, ÁâπÊúóÊôÆ/nrf, ÈÄö/v, ÁîµËØù/n]
[‰∏ªÂ∏≠/n, Âíå/c, ÁâπÊúó/nr, ÊôÆÈÄö/a, ÁîµËØù/n]
```
ÂÆûÈôÖËæìÂá∫
``
Âú®ÂÆòÁΩë‰∏äÊµãËØï‰πüÊòØÂêåÊ†∑ÁöÑÊïàÊûú„ÄÇ
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂëΩ‰ª§hanlpÊâæ‰∏çÂà∞ÔºåÈúÄË¶ÅÂ¶Ç‰ΩïÈÖçÁΩÆ?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

Ê†πÊçÆ https://github.com/hankcs/pyhanlp ÊèêÂà∞ ‚Äú‰ΩøÁî®ÂëΩ‰ª§hanlp segmentËøõÂÖ•‰∫§‰∫íÂàÜËØçÊ®°Âºè‚ÄùÔºåÁÑ∂ËÄåÔºö
[root@ec-n1-1d68-0005 ~]# hanlp
-bash: hanlp: command not found

ÊòØ‰∏çÊòØËøòÂ∑Æ‰ªÄ‰πàÈÖçÁΩÆÊ≤°ÊúâÂÅöÂ•ΩÔºü
"
java.io.IOException: No FileSystem for scheme: D,"* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.3
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ë≠¶Âëä: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®ÁºìÂ≠òÂä†ËΩΩÂ§±Ë¥•ÔºåÂéüÂõ†Â¶Ç‰∏ãÔºöjava.io.IOException: No FileSystem for scheme: D
ÈÖçÁΩÆÊñá‰ª∂
root=D:/A/B/C/hanlp/
ÁÑ∂ÂêéÊä•ÈîôÁ±ª‰ºº
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
ÂÖ´Êúà 20, 2019 11:57:38 ‰∏äÂçà com.hankcs.hanlp.dictionary.other.CharTable loadBin
Ë≠¶Âëä: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®ÁºìÂ≠òÂä†ËΩΩÂ§±Ë¥•ÔºåÂéüÂõ†Â¶Ç‰∏ãÔºöjava.io.IOException: No FileSystem for scheme: D

Exception in thread ""main"" java.lBang.ExceptionInInitializerError
	Ë≠¶Âëä: ËØªÂèñD:/A/B/C/hanlp/data/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.IOException: No FileSystem for scheme: D
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->



### Ëß¶Âèë‰ª£Á†Å

```
    val a  = hanlp.HanLP.extractSummary(""ËøôÊòØ‰∏Ä‰∏™ÊµãËØï"",5)
```


"
ÂàÜËØçÁªìÊûúÊòØÂê¶ÊîØÊåÅ‰∏çËøõË°åËØçÊÄßÊ†áÊ≥®Âë¢Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
```java
System.out.println(HanLP.segment(""‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®HanLPÊ±âËØ≠Â§ÑÁêÜÂåÖÔºÅ""));
```
ÁöÑËæìÂá∫ÁªìÊûú‰∏∫
```
[‰Ω†Â•Ω/l, Ôºå/w, Ê¨¢Ëøé/v, ‰ΩøÁî®/v, HanLP/nx, Ê±âËØ≠/nz, Â§ÑÁêÜ/v, ÂåÖ/v, ÔºÅ/w]
```
ÊàëÊÉ≥Áü•ÈÅìÔºåÊúâÊ≤°ÊúâÂèÇÊï∞ÊàñËÄÖÊúâÊ≤°ÊúâÂÖ∂‰ªñÊé•Âè£ËÉΩÂ§üÊîØÊåÅ‰∏çËøõË°åÂàÜËØçÂêéÁöÑËØçÊÄßÊ†áÊ≥®Ôºå‰æãÂ¶Ç
```
[‰Ω†Â•Ω, Ôºå, Ê¨¢Ëøé, ‰ΩøÁî®, HanLP, Ê±âËØ≠, Â§ÑÁêÜ, ÂåÖ, ÔºÅ]
```
"
jpype._jclass.OutOfMemoryError: Java heap space,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®ËøõË°åÂ§öËøõÁ®ãËÅöÁ±ªÁÆóÊ≥ïrepeateBisectionË∞ÉÁî®Êó∂ÂÄôÔºåÂá∫Áé∞Âç°‰ΩèÔºåËøõË°å‰∏ç‰∏ãÂéª„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp 0.1.45
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp 0.1.45

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
def myfunc(qaid):
        analyzer1 = ClusterAnalyzer()#ÂÅúÊ≠¢Âú®Ëøô‰∏ÄÂè•ÔºåËøõË°å‰∏ç‰∏ãÂéª„ÄÇ
results.append(pool.apply_async(myfunc, (msg, )))
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏ÂíåÊ†∏ÂøÉËØçÂÖ∏‰∏≠ÁöÑÈ¢ëÊ¨°ÈóÆÈ¢ò,"hankcs  ‰Ω†Â•Ω! ÂèØ‰ª•ËÆ≤‰∏Ä‰∏ãËá™ÂÆö‰πâËØçÂÖ∏ÂíåÊ†∏ÂøÉËØçÂÖ∏‰∏≠ÁöÑÈ¢ëÊ¨°ÁöÑ‰ΩúÁî®Âêó.ÊàëÁúã‰∫ÜÁõ∏ÂÖ≥ÁöÑIssuesÂπ∂Ê≤°ÊúâÂæóÂà∞Ëß£ÊÉë,Ë∞¢Ë∞¢."
java.lang.NullPointerException,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖvËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp 0.1.45
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp 0.1.45

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å
res=analyzertxt.repeatedBisection(0.1)
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Traceback (most recent call last):
  File ""test_cluster.py"", line 65, in <module>
    res=cluster(filename)
  File ""test_cluster.py"", line 60, in cluster
    res=analyzertxt.repeatedBisection(0.3)
jpype._jexception.NullPointerExceptionPyRaisable: java.lang.NullPointerException


<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Change method name 'convert' to 'createSynonymList',"This class is used to represent  CoreSynonymDictionary.  This method named 'convert' is to create a synonym list. Thus, the method name 'createSynonymList' is more intuitive than 'convert'."
doc2vecËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶Êó∂Ôºå‰∏Ä‰∫õÂ≠óÊØçÁªÑÊàêÁöÑÁâπÊÆäÂ≠óÁ¨¶‰∏çËµ∑‰ΩúÁî®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

doc2vecËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶Êó∂Ôºå‰∏Ä‰∫õÂ≠óÊØçÁªÑÊàêÁöÑÁâπÊÆäÂ≠óÁ¨¶‰∏çËµ∑‰ΩúÁî®Ôºå‰∏çÂèÇ‰∏éÁõ∏‰ººÂ∫¶ËÆ°ÁÆó

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = './data/word2vec.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)

docs = [""ddÁöÑÂ∑•‰ΩúËÅåË¥£"", ""dmÁöÑÂ∑•‰ΩúËÅåË¥£"", ""pmoÁöÑÂ∑•‰ΩúËÅåË¥£""]

for idx, doc in enumerate(docs):
       doc2vec.addDocument(idx, doc)

for res in doc2vec.nearest('ddÁöÑÂ∑•‰ΩúËÅåË¥£'):
    print('%s = %.2f' % (docs[res.getKey().intValue()], res.getValue().floatValue()))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ddÁöÑÂ∑•‰ΩúËÅåË¥£ = 1.00

dmÁöÑÂ∑•‰ΩúËÅåË¥£ÔºåpmoÁöÑÂ∑•‰ΩúËÅåË¥£ÁöÑÂæóÂàÜÂ∞è‰∫é1ÔºåÂπ∂‰∏îÊéíÂú®ddÁöÑÂ∑•‰ΩúËÅåË¥£‰πãÂêé
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
dmÁöÑÂ∑•‰ΩúËÅåË¥£ = 1.00
pmoÁöÑÂ∑•‰ΩúËÅåË¥£ = 1.00
ddÁöÑÂ∑•‰ΩúËÅåË¥£ = 1.00
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ΩøÁî®SafeJClassË∞ÉÁî®ÂÅúÁî®ËØçÂÖ∏Êó∂ÔºåËá™ÂÆö‰πâÂÅúÁî®ËØçËøáÊª§Âô®‰∏çÁîüÊïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®SafeJClassË∞ÉÁî®ÂÅúÁî®ËØçÂÖ∏Êó∂ÔºåËá™ÂÆö‰πâÂÅúÁî®ËØçËøáÊª§Âô®‰∏çÁîüÊïàÔºåËÄå‰ΩøÁî®JClassËøõË°åË∞ÉÁî®Êó∂ÔºåËá™ÂÆö‰πâÂÅúÁî®ËØçËøáÊª§Âô®ÁîüÊïà„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
MyFilter = JClass('MyFilter')  # Ëá™ÂÆö‰πâÂÅúÁî®ËØçËøáÊª§Âô®
CoreStopWordDictionary.FILTER = MyFilter()
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""ÊÄé‰πàÂäûÁêÜÂ∑•‰ΩúËØÅ""
term_list = NLPTokenizer.segment(text)
print(term_list)
ËæìÂá∫ÁªìÊûúÔºö[ÊÄé‰πà, ÂäûÁêÜ, Â∑•‰ΩúËØÅ]
CoreStopWordDictionary.apply(term_list)
print(term_list)
ËæìÂá∫ÁªìÊûúÔºö[ÊÄé‰πà, ÂäûÁêÜ, Â∑•‰ΩúËØÅ]

ÂΩì‰ΩøÁî®SafeJClassÊó∂Ôºö
CoreStopWordDictionary = SafeJClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
MyFilter = SafeJClass('MyFilter')  # Ëá™ÂÆö‰πâÂÅúÁî®ËØçËøáÊª§Âô®
CoreStopWordDictionary.FILTER = MyFilter()
NLPTokenizer = SafeJClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
text = ""ÊÄé‰πàÂäûÁêÜÂ∑•‰ΩúËØÅ""
term_list = NLPTokenizer.segment(text)
print(term_list)
ËæìÂá∫ÁªìÊûúÔºö[ÊÄé‰πà, ÂäûÁêÜ, Â∑•‰ΩúËØÅ]
CoreStopWordDictionary.apply(term_list)
print(term_list)
ËæìÂá∫ÁªìÊûúÔºö[ÂäûÁêÜ, Â∑•‰ΩúËØÅ]
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÊÄé‰πà, ÂäûÁêÜ, Â∑•‰ΩúËØÅ]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂäûÁêÜ, Â∑•‰ΩúËØÅ]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂØπÊó•ÊúüÂàÜËØçÔºåÂ•áÊÄ™ÁöÑÁªìÊûú,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ


## ÁâàÊú¨Âè∑ 1.72


ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.72

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπÊó•ÊúüËøõË°åÂàÜËØçÔºå‰∏çÂêåÊó•Êúü‰ºöÂæóÂà∞‰∏çÂêåÁöÑÁªìÊûú„ÄÇÊØîÂ¶ÇÔºö
2018Âπ¥2Êúà11Êó•  ÂàÜËØçÁªìÊûú‰∏∫ 2018Âπ¥2Êúà11Êó•/t
2018Âπ¥3Êúà11Êó•  ÂàÜËØçÁªìÊûú‰∏∫ 2018Âπ¥/t 3Êúà/t 11Êó•/t

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
‰ΩøÁî®NLPTokenizer.ANALYZERÂèØ‰ª•Â§çÁé∞ÔºåÂÖ∂‰ªñÊñπÂºèÊú™ÊµãËØï„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""2018Âπ¥2Êúà11Êó•""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
2018Âπ¥/t 2Êúà/t 11Êó•/t
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
2018Âπ¥2Êúà11Êó•/t
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
„ÄêÂª∫ËÆÆ„ÄëÊòØÂê¶‰ΩúËÄÖÂàÜÊ¥æ‰ªªÂä°ÔºåÂ§ö‰∫∫Ë¥°ÁåÆÔºå‰ΩúËÄÖÂÆ°Ê†∏,"   È¶ñÂÖàÈùûÂ∏∏ÊÑüËßâÊèê‰æõHanLPËøô‰πàÊ£íÁöÑÂºÄÊ∫êÈ°πÁõÆÔºåÊçÆÊàëÊâÄÁü•ÔºåÁé∞Âú®HanLPËøòÊòØÊÇ®‰∏Ä‰∏™‰∫∫Áª¥Êä§ÁöÑÔºå
ÊòØÂê¶ÂèØ‰ª•Áî±ÊÇ®ÔºåÊàñËÄÖÊÇ®ÁªÑÂª∫‰∏Ä‰∏™Â§ßÁâõÂõ¢ÈòüÔºåÊØîÂ¶ÇHanLPÂßîÂëò‰ºöQQÁæ§Êù•ÊääÂÖ≥ÔºåÁÑ∂ÂêéÂ∞ÜHanLPÁöÑ‰ªªÂä°ÂàÜËß£‰∏∫ÂÆπÊòìÂÆûÁé∞ÁöÑÊòéÁªÜÈ°πÔºåÁÑ∂ÂêéÁî±ÁÉ≠ÂøÉÁΩëÂèãÈ¢ÜÂèñ‰ªªÂä°ÂºÄÂèëÔºåÁÑ∂ÂêéÊèê‰∫§Ê±áÊÄª„ÄÇ
   ËøôÊ†∑Ôºå‰ΩúËÄÖÂ∞±ÂèØ‰ª•‰∏ìÊ≥®Ê†∏ÂøÉÈÉ®ÂàÜÔºå‰∏çÁî®Âπ≤ÈÇ£‰πàÂ§ö‰ΩìÂäõÊ¥ª‰∫Ü„ÄÇ
ÂΩìÁÑ∂ÔºåËøôÂè™ÊòØ‰∏Ä‰∏™Âª∫ËÆÆÔºå‰∏ç‰∏ÄÂÆöÂèØË°åÔºåÁúüÊ≠£ËêΩÂú∞ËøòÈúÄË¶ÅËÄÉËôëÂæàÂ§öÊñπÈù¢ÁöÑÁªÜÂàô"
Âà†Èô§stopwords.txtÊñá‰ª∂ÂÜÖÂÆπÂêéÈáçÂêØÔºåËá™Â∏¶ÂÅúÁî®ËØç‰ªªÁÑ∂ÁîüÊïàÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
 ÈùûÂ∏∏ÊÑüË∞¢ÔºåÁî®‰∫ÜÊÇ®ËøôËæπÊèê‰æõÁöÑË°•‰∏Å #1253 ÔºåÊúâ‰ª•‰∏ã‰∏§‰∏™ÈóÆÈ¢òÔºö
1„ÄÅÂàÜËØçÂô®Âè™ËÉΩÈÄâÊã©Âü∫Á±ªË¶ÅÊòØseg.segmentÁöÑÂêóÔºüÊàëÈÄâÊã©ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØçÂ∞±ÂèØ‰ª•ËøõË°åËØ≠‰πâÊü•ËØ¢Ôºå‰ΩÜÊòØÈÄâÊã©NLPTokenizer Â∞±‰ºöÊèêÁ§∫Êä•ÈîôÔºå
com.hankcs.hanlp.mining.word2vec.DocVectorModel(com.hankcs.hanlp.mining.word2vec.WordVectorModel,com.hankcs.hanlp.seg.Segment,boolean)Ôºõ
2„ÄÅÊàëÁöÑÁõÆÁöÑÊòØÂà†Èô§hanlpÈáåÈù¢Ëá™Â∏¶ÁöÑÂÅúÁî®ËØçÔºå‰ΩøËá™Â∏¶ÁöÑÂÅúÁî®ËØçÂú®ÊàëÁöÑÁ®ãÂ∫è‰∏≠‰∏çÁîüÊïàÔºåÁÑ∂ÂêéÊ∑ªÂä†Ëá™Â∑±ÁöÑÂÅúÁî®ËØçÔºå‰ΩÜÊòØÊúÄÂêéÁöÑÊµãËØïÁªìÊûúÊòØÔºåËá™Â∏¶ÁöÑÂÅúÁî®ËØçÂà†Èô§Êó†ÊïàÔºå‰∏çÁÆ°ÊòØÁî®ÈÇ£‰∏™ÂàÜËØçÂô®ÔºåÁÑ∂Âêé‰ΩøÁî®CoreStopWordDictionary.apply(term_list)ËøõË°åÂà†Èô§ÂÅúÁî®ËØçÊó∂ÔºåËá™Â∏¶ÁöÑÂÅúÁî®ËØçÂêéËøòÊòØ‰ºöËµ∑‰ΩúÁî®„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
text = ‚ÄòÂëòÂ∑•ÊÄé‰πàÂäûÁêÜÂ∑•‰ΩúËØÅ‚Äô
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
term_list = NLPTokenizer.segment(text)
print(term_list)

ËæìÂá∫ÁªìÊûúÔºö
[ÂëòÂ∑•/n,  ÊÄé‰πà/r, ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
CoreStopWordDictionary.apply(term_list)
print(term_list)

ËæìÂá∫ÁªìÊûúÔºö
[ÂëòÂ∑•/n,  ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]

Êé•ÁùÄÔºåÂà†Èô§Âú®data/dictionaryÁõÆÂΩï‰∏ãÔºåÂà†Èô§stopwords.txt.binÊñá‰ª∂ÔºåÂπ∂Â∞Üstopwords.txtÊñá‰ª∂Âà†Èô§‰∏∫Á©∫ÔºåÈáçÂêØÂêéÔºåÈáçÊñ∞ËøêË°åÁ®ãÂ∫è„ÄÇ

CoreStopWordDictionary = JClass(""com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary"")
CoreStopWordDictionary.apply(term_list)
print(term_list)

ËæìÂá∫ÁªìÊûúÔºö
[ÂëòÂ∑•/n,  ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]

ÈóÆÈ¢òÔºö‰∏∫‰ªÄ‰πàÂà†Èô§‰∫ÜÂÅúÁî®ËØçÂÖ∏ÔºåhanlpËá™Â∏¶ÁöÑÂéüÊúâÂÅúÁî®ËØç‰ªªÁÑ∂ËÉΩÂô®‰ΩúÁî®Ôºü

```
### ÊúüÊúõËæìÂá∫


```
[ÂëòÂ∑•/n,  ÊÄé‰πà/r, ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂëòÂ∑•/n,  ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âà†Èô§stopwords.txtÊñá‰ª∂ÂÜÖÂÆπÂêéÈáçÂêØÔºåËá™Â∏¶ÂÅúÁî®ËØç‰ªªÁÑ∂ÁîüÊïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®data/dictionaryÁõÆÂΩï‰∏ãÔºåÂà†Èô§stopwords.txt.binÊñá‰ª∂ÔºåÂπ∂Â∞Üstopwords.txtÊñá‰ª∂Âà†Èô§‰∏∫Á©∫ÔºåÈáçÂêØÂêéÔºåÈáçÊñ∞ËøêË°åÁ®ãÂ∫èÔºåÂèëÁé∞ÂéüÂÅúÁî®ËØçË°®‰∏≠ÁöÑÂÅúÁî®ËØç‰ªªÁÑ∂ÁîüÊïàÔºåÂç≥Âà†Èô§Êó†Êïà„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
text = 'ÂëòÂ∑•ÊÄé‰πàÂäûÁêÜÂ∑•‰ΩúËØÅ?'
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
print(NotionalTokenizer.segment(text)) 

ËæìÂá∫ÁªìÊûúÔºö
[ÂëòÂ∑•/n, ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]

Êé•ÁùÄÂú®data/dictionaryÁõÆÂΩï‰∏ãÔºåÂà†Èô§stopwords.txt.binÊñá‰ª∂ÔºåÂπ∂Â∞Üstopwords.txtÊñá‰ª∂Âà†Èô§‰∏∫Á©∫ÔºåÂπ∂Âú®stopwords.txt‰∏≠Ê∑ªÂä†ÂÅúÁî®ËØç‚ÄúÂëòÂ∑•‚ÄùÔºåÈáçÂêØÂêéÔºåÈáçÊñ∞ËøêË°åÁ®ãÂ∫è

text = 'ÂëòÂ∑•ÊÄé‰πàÂäûÁêÜÂ∑•‰ΩúËØÅ?'
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
print(NotionalTokenizer.segment(text)) 

ËæìÂá∫ÁªìÊûúÔºö
[ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]

Ëá™Â∑±Ê∑ªÂä†ÁöÑÂÅúÁî®ËØçËÉΩÁîüÊïàÔºå‰ΩÜÊòØhanlpËá™Â∏¶ÁöÑÂÅúÁî®ËØç‰ªªÁÑ∂ÁîüÊïàÔºåÂà†Èô§‰∏çËµ∑‰ΩúÁî®„ÄÇ
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫ÁªìÊûúÔºö
[ÊÄé‰πà/r, ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂäûÁêÜ/v, Â∑•‰ΩúËØÅ/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Add unit tests for com.hankcs.hanlp.utility.ByteUtil,"I've analysed your codebase and noticed that `com.hankcs.hanlp.utility.ByteUtil` is not fully tested.
I've written some tests for the methods in this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests will help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other classes that you consider important in a subsequent PR."
‰∏∫‰ªÄ‰πàÊâßË°åÊ≠§ËÑöÊú¨ÂêéÁõ¥Êé•Êä•sslÈîôËØØ,"![image](https://user-images.githubusercontent.com/41661610/61841888-da429680-aec8-11e9-8aa5-fae1d38f803e.png)
"
androidË∑ë‰æùÂ≠òÂè•Ê≥ïÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÊàëÊòØ‰ΩøÁî®ÁöÑandroidÊùøË∑ëÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºåÂÜÖÂ≠ò4GÔºåÂè™Êúâ‰∏Ä‰∏™Â∫îÁî®Á®ãÂ∫èÔºåÂú®Âä†ËΩΩ‰æùÂ≠òÂè•Ê≥ïÊ®°ÂûãÁöÑÊó∂ÂÄôÊÄªÊòØÊä•.OutOfMemoryError: Failed to allocate a 12 byte allocation with 3856 free bytes and 3KB until OOM
ÂºÇÂ∏∏ÔºåÂêéÊù•ÊàëÂ∞ÜÂ∫îÁî®ÁöÑÂÜÖÂ≠òË∞ÉÊï¥‰∏∫‰∏çÂèóÈôêÂà∂Ôºå‰æùÁÑ∂‰ºöÊä•ËØ•ÈîôËØØÔºå‰πãÂâçÁúãandroidÁöÑdemoÈáåÈù¢ËÆ≤Ëß£ÁöÑÂèØ‰ª•ÊîØÊåÅÂè•Ê≥ïÂàÜÊûêÁöÑÔºåÊâÄ‰ª•ÊÉ≥Ë¶ÅÊ±ÇËØÅÊòØÊàëÁöÑÈÖçÁΩÆÂá∫Áé∞ÈóÆÈ¢ò‰∫ÜÂêóÔºüÊàëÈô§‰∫Ü‰æùÂ≠òÂè•Ê≥ïÊúâÈóÆÈ¢ò‰πãÂ§ñÔºåÂÖ∂‰ªñÁöÑÂàÜËØçÔºåÊãºÈü≥ËΩ¨Êç¢ÈÉΩÊòØÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®ÁöÑÔºåË∞¢Ë∞¢„ÄÇ

### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CoNLLSentence sentence = HanLP.parseDependency(""ÂæêÂÖàÁîüËøòÂÖ∑‰ΩìÂ∏ÆÂä©‰ªñÁ°ÆÂÆö‰∫ÜÊääÁîªÈõÑÈπ∞„ÄÅÊùæÈº†ÂíåÈ∫ªÈõÄ‰Ωú‰∏∫‰∏ªÊîªÁõÆÊ†á„ÄÇ"");
        // ‰πüÂèØ‰ª•Áî®Âü∫‰∫éArcEagerËΩ¨ÁßªÁ≥ªÁªüÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®
//        IDependencyParser parser = new KBeamArcEagerDependencyParser();
//        CoNLLSentence sentence = parser.parse(""ÂæêÂÖàÁîüËøòÂÖ∑‰ΩìÂ∏ÆÂä©‰ªñÁ°ÆÂÆö‰∫ÜÊääÁîªÈõÑÈπ∞„ÄÅÊùæÈº†ÂíåÈ∫ªÈõÄ‰Ωú‰∏∫‰∏ªÊîªÁõÆÊ†á„ÄÇ"");
        Log.i(tag,""start=""+sentence);
        // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
        for (CoNLLWord word : sentence)
        {
            Log.i(tag,""1„ÄÅlemma=""+word.LEMMA+"";deprel=""+ word.DEPREL+"";HEAD=""+word.HEAD.LEMMA);
        }
        // ‰πüÂèØ‰ª•Áõ¥Êé•ÊãøÂà∞Êï∞ÁªÑÔºå‰ªªÊÑèÈ°∫Â∫èÊàñÈÄÜÂ∫èÈÅçÂéÜ
        CoNLLWord[] wordArray = sentence.getWordArray();
        for (int i = wordArray.length - 1; i >= 0; i--)
        {
            CoNLLWord word = wordArray[i];
            Log.i(tag,""2„ÄÅlemma=""+word.LEMMA+"";deprel=""+ word.DEPREL+"";HEAD=""+word.HEAD.LEMMA);
        }
        // ËøòÂèØ‰ª•Áõ¥Êé•ÈÅçÂéÜÂ≠êÊ†ëÔºå‰ªéÊüêÊ£µÂ≠êÊ†ëÁöÑÊüê‰∏™ËäÇÁÇπ‰∏ÄË∑ØÈÅçÂéÜÂà∞ËôöÊ†π
        CoNLLWord head = wordArray[12];
        while ((head = head.HEAD) != null)
        {
            if (head == CoNLLWord.ROOT) {
                Log.i(tag,""head.LEMMA=""+head.LEMMA);
            } else {
                Log.i(tag,""%s --(%s)--> ""+head.LEMMA+"";""+ head.DEPREL);
            }
        }
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®‰ΩøÁî®SuggesterÊó∂ÔºåËøîÂõûÂàóË°®‰∏∫Á©∫ÁöÑÈóÆÈ¢ò,"```
from pyhanlp import *


def demo_suggester():
    """""" ÊñáÊú¨Êé®Ëçê(Âè•Â≠êÁ∫ßÂà´Ôºå‰ªé‰∏ÄÁ≥ªÂàóÂè•Â≠ê‰∏≠ÊåëÂá∫‰∏éËæìÂÖ•Âè•Â≠êÊúÄÁõ∏‰ººÁöÑÈÇ£‰∏Ä‰∏™)
    >>> demo_suggester()
    [Â®ÅÂªâÁéãÂ≠êÂèëË°®ÊºîËØ¥ ÂëºÂêÅ‰øùÊä§ÈáéÁîüÂä®Áâ©, Ëã±Êä•ÂëäËØ¥Á©∫Ê∞îÊ±°ÊüìÂ∏¶Êù•‚ÄúÂÖ¨ÂÖ±ÂÅ•Â∫∑Âç±Êú∫‚Äù]
    [Ëã±Êä•ÂëäËØ¥Á©∫Ê∞îÊ±°ÊüìÂ∏¶Êù•‚ÄúÂÖ¨ÂÖ±ÂÅ•Â∫∑Âç±Êú∫‚Äù]
    [„ÄäÊó∂‰ª£„ÄãÂπ¥Â∫¶‰∫∫Áâ©ÊúÄÁªàÂÖ•Âõ¥ÂêçÂçïÂá∫ÁÇâ ÊôÆ‰∫¨È©¨‰∫ëÂÖ•ÈÄâ]
    [È≠ÖÊÉëÂ§©ÂêéËÆ∏‰Ω≥ÊÖß‰∏çÁà±‚ÄúÈ¢ÑË∞ã‚Äù Áã¨Âî±„ÄäËÆ∏ÊüêÊüê„Äã]
    """"""
    Suggester = JClass(""com.hankcs.hanlp.suggest.Suggester"")
    suggester = Suggester()
    title_array = [
        ""Â®ÅÂªâÁéãÂ≠êÂèëË°®ÊºîËØ¥ ÂëºÂêÅ‰øùÊä§ÈáéÁîüÂä®Áâ©"",
        ""È≠ÖÊÉëÂ§©ÂêéËÆ∏‰Ω≥ÊÖß‰∏çÁà±‚ÄúÈ¢ÑË∞ã‚Äù Áã¨Âî±„ÄäËÆ∏ÊüêÊüê„Äã"",
        ""„ÄäÊó∂‰ª£„ÄãÂπ¥Â∫¶‰∫∫Áâ©ÊúÄÁªàÂÖ•Âõ¥ÂêçÂçïÂá∫ÁÇâ ÊôÆ‰∫¨È©¨‰∫ëÂÖ•ÈÄâ"",
        ""‚ÄúÈªëÊ†ºÊØî‚ÄùÊ®™Êâ´Ëè≤ÔºöËè≤Âê∏Âèñ‚ÄúÊµ∑Ááï‚ÄùÁªèÈ™åÂèäÊó©ÁñèÊï£"",
        ""Êó•Êú¨‰øùÂØÜÊ≥ïÂ∞ÜÊ≠£ÂºèÁîüÊïà Êó•Â™íÊåáÂÖ∂ÊçüÂÆ≥ÂõΩÊ∞ëÁü•ÊÉÖÊùÉ"",
        ""Ëã±Êä•ÂëäËØ¥Á©∫Ê∞îÊ±°ÊüìÂ∏¶Êù•‚ÄúÂÖ¨ÂÖ±ÂÅ•Â∫∑Âç±Êú∫‚Äù""
    ]
    for title in title_array:
        suggester.addSentence(title)

    print(suggester.suggest(""ÈôàËø∞"", 2))      # ËØ≠‰πâ
    print(suggester.suggest(""Âç±Êú∫ÂÖ¨ÂÖ≥"", 1))  # Â≠óÁ¨¶
    print(suggester.suggest(""mayun"", 1))   # ÊãºÈü≥
    print(suggester.suggest(""ÂæêÂÆ∂Ê±á"", 1)) # ÊãºÈü≥


if __name__ == ""__main__"":
    import doctest
    doctest.testmod(verbose=True)
```


### **ÊàëÂú®ÊâßË°å‰ª•‰∏ä‰ª£Á†ÅÊó∂ÔºåÂ§±Ë¥•‰∫ÜÔºådataÂíåjarÈÉΩÂÆâË£Ö‰∫ÜÔºåÊ≤°ÊúâÈóÆÈ¢òÁöÑÔºåÂêå‰∫ãÂú®windows‰∏äÊâßË°åÊ≤°ÊúâÈóÆÈ¢òÔºåÊàëÁöÑÁéØÂ¢ÉÊòØubuntu 18.0ÔºåjavaÁéØÂ¢ÉÊòØ""1.8.0_212""Ôºåpython3.6**
### **‰∏çÊ∏ÖÊ•öÂéüÂõ†**
### **Âú®Âá†‰∏™ÊúçÂä°Âô®Âπ≥Âè∞ÔºåÂíåÂ§ö‰∫∫ÈÉΩÊµãËØïÁöÑÊÉÖÂÜµ‰∏ãÔºåÁªìÊûúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇÊàëËßâÂæóËøôÊòØ‰∏Ä‰∏™bug,,Âè™Êúâsuggester‰ºöËøîÂõûÁ©∫ÂàóË°®ÔºåÂÖ∂‰ªñÁöÑÂäüËÉΩÈÉΩËÉΩÊ≠£Â∏∏‰ΩøÁî®**
![image](https://user-images.githubusercontent.com/36363279/61576187-2d070180-ab09-11e9-9278-be663fd9672c.png)

"
ÂØªÊ±ÇËß£ÂÜ≥ÊñπÊ°àÔºö‰∫∫Âêç„ÄÅÁîµËØù„ÄÅÁúÅÂ∏ÇÂå∫ËØÜÂà´ÈóÆÈ¢ò‰∏≠ÁöÑÔºö‰∫∫ÂêçËØÜÂà´ÈóÆÈ¢ò,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.3


## ÊàëÁöÑÈóÆÈ¢ò

ÈúÄÊ±ÇÔºö‰ªé‰∏Ä‰∏≤ÊñáÊú¨‰∏≠ÊèêÂèñÁîµËØù„ÄÅÁúÅÂ∏ÇÂå∫ÔºàÂçïÁã¨ÊèêÂèñÔºâÔºå‰∫∫Âêç
ÂÆûÁé∞ÈÄªËæëÔºöÂª∫ËÆÆÂØπÂ∫îÁöÑÁúÅ„ÄÅÂ∏Ç„ÄÅÂå∫ËØçÂ∫ìÔºà‰ΩøÁî®ÁöÑÊòØCustomDictionary.insertÂº∫Ë°åÊèíÂÖ•Ôºâ „ÄÅ‰∫∫ÂêçÂçïÁã¨ÁöÑ‰ø©‰∏™ËØçÂÖ∏


## Â§çÁé∞ÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/9693917/61531356-2faa1e00-aa59-11e9-874d-048e126c79a0.png)

ÊâìÂç∞ËØçÊÄßÂ¶Ç‰∏ãÔºö
‰∏Ä‰∏≤ÊñáÊú¨‰∏≠ÔºöÈªÑËã±Áî∑ 12345328978 ÊπñÂçóÂ®ÑÂ∫ïÂèåÂ≥∞AË°óÈÅìÂçéÊ¥™Ê∞¥Â§ßÂé¶AÂ∫ß102

ËøôÁßçÊÉÖÂÜµ‰ºöÊääÔºöÂçéÊ¥™Ê∞¥ÂíåÈªÑËã±Áî∑ÈÉΩËßÜ‰∏∫‰∫∫ÂêçÔºåÊàëÁöÑÂ§ßÊ¶ÇÊÄùË∑Ø‰∏∫ÔºöÂ§ö‰∏™/nrÊÉÖÂÜµ‰ª•ÊúÄËøúË∑ùÁ¶ªÁöÑ‰∏∫ÂáÜÔºå‰ΩÜ‰∏çÁü•ÈÅìÊòØÂê¶ÊúâÂÖ∂‰ªñÂÆûÁé∞ÊÄùË∑ØÔºåÊàñËÄÖÂèØË°åapi

@hankcs Ëøô‰πàÂ§ö‰∫∫ÁöÑÈóÆÈ¢òÔºåÊâìÊâ∞‰∫Ü~~^V^
"
Pyhanlp ÁöÑUser Guide,"https://github.com/FontTian/pyhanlp_user_guide

ÂéªÂπ¥Ëá™Â∑±ÂÜô‰∫Ü‰∏™Pyhanlp ÁöÑuser guide„ÄÇÂøòËÆ∞Âíå‰Ω†ËØ¥‰∏Ä‰∏ã‰∫Ü„ÄÇ
ËøôÊòØCSDNÂçöÂÆ¢ÂàóË°®Ôºöhttps://blog.csdn.net/fontthrone/article/category/8073727"
word2vecÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÂíåËØçËØ≠È°∫Â∫èÊó†ÂÖ≥ÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Êàë‰∏ãËΩΩÁöÑhanlpÁöÑÊ®°ÂûãÔºåÁÑ∂ÂêéÊµãËØïËØ≠Âè•:‚ÄúÊàëÁà±‰∏≠ÂõΩ‚ÄúÂíå‚ÄùÊàëÁà±‰∏≠ÂõΩ‚ÄùÁõ∏‰ººÂ∫¶1.0000001192092896Ôºõ‚ÄùÊàëÁà±‰∏≠ÂõΩ‚ÄùÂíå‚Äù‰∏≠ÂõΩÁà±Êàë‚ÄúÁõ∏‰ººÂ∫¶‰πüÊòØ1.0000001192092896

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
String word2VecPath = HanlpPropeties.getWord2VecPath();
        WordVectorModel wordVectorModel = null;
        try {
            wordVectorModel = new WordVectorModel(word2VecPath);
        } catch (IOException e) {
            e.printStackTrace();
        }
        DocVectorModel docVectorModel=new DocVectorModel(wordVectorModel);
        double ruleProb1=docVectorModel.similarity(caseInfo,ruleInfo);
```
### ÊúüÊúõËæìÂá∫

ÊàëÊÉ≥Áü•ÈÅìÁõ∏‰ººÂ∫¶ÊØîËæÉËØçÁöÑÈ°∫Â∫èÊúâÂÖ≥Á≥ªÊ≤°ÔºüÂº†‰∏âÊâì‰∫ÜÊùéÂõõÔºõÂíåÊùéÂõõÊâì‰∫ÜÂº†‰∏âÁõ∏‰ººÂ∫¶ÊòØ‰∏çÊòØ1ÔºåËøòÊòØÊàëÂì™ÈáåÂÜôÈîô‰∫Ü


```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
AhoCorasickDoubleArrayTrieSegment ‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂàÜËØçÁªìÊûúËØçÊÄß‰∏∫Á©∫,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
portable-1.7.4

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.4



## ÊàëÁöÑÈóÆÈ¢ò

AhoCorasickDoubleArrayTrieSegment ‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂàÜËØçÁªìÊûúËØçÊÄß‰∏∫Á©∫

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂàõÂª∫ËØçÊÄßTreeMap
2. ÁÑ∂ÂêéÂàùÂßãÂåñAhoCorasickDoubleArrayTrieSegmentÂπ∂‰ΩøÁî®TreeMapÁöÑÂÖ≥ÈîÆËØç
3. Êé•ÁùÄËøõË°åAhoCorasickDoubleArrayTrieSegmentÂàÜËØç

### Ëß¶Âèë‰ª£Á†Å

```
  public static void main(String[] args) {
		TreeMap<String, CoreDictionary.Attribute> dictionary = new TreeMap<>();
		dictionary.put(""È´òÂÜ∞Áßç"", CoreDictionary.Attribute.create(""fcz 1""));
		AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment(dictionary);
		List<Term> termList = segment.seg(""È´òÂÜ∞ÁßçÊ≠£Èò≥Ëæ£ÁªøËê§ÁÅ´Ëô´ÊâãÈïØ"");
		termList.forEach(System.out::println);
	}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
È´òÂÜ∞Áßç/fcz
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
È´òÂÜ∞Áßç/null
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êó†Ê≥ïÂÆâË£ÖpythonÁâàÊú¨,"ÂÖ∑‰ΩìÊìç‰ΩúÂíåÊä•ÈîôÂ¶Ç‰∏ãÔºö
Last login: Mon Jul 15 20:16:21 on ttys001
MacBook-Pro-de-Chen:~ noah$ pip install pyhanlp
Collecting pyhanlp
Collecting jpype1>=0.7.0 (from pyhanlp)
  Using cached https://files.pythonhosted.org/packages/28/63/784834e8a24ec2e1ad7f703c3dc6c6fb372a77cc68a2fdff916e18a4449e/JPype1-0.7.0.tar.gz
Building wheels for collected packages: jpype1
  Building wheel for jpype1 (setup.py) ... error
  ERROR: Complete output from command /Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-wheel-1dve7lyv --python-tag cp37:
  ERROR: /Users/noah/anaconda3/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'use_scm_version'
    warnings.warn(msg)
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.7
  creating build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcollection.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcomparable.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_classpath.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jtypes.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_pykeywords.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jproxy.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_gui.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_darwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/nio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jstring.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_cygwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jboxed.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/types.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/beans.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jvmfinder.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/imports.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jcustomizer.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_core.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jinit.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_linux.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jarray.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jobject.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jclass.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_windows.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jexception.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/reflect.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  copying jpype/_jpackage.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
  running build_ext
  running build_java
  Using Jar cache
  creating build/lib
  creating build/lib/org
  creating build/lib/org/jpype
  creating build/lib/org/jpype/classloader
  copying native/jars/org/jpype/classloader/JPypeClassLoader.class -> build/lib/org/jpype/classloader
  copying native/jars/org.jpype.jar -> build/lib
  running build_thunk
  Building thunks
    including thunk build/lib/org/jpype/classloader/JPypeClassLoader.class
    including thunk build/lib/org.jpype.jar
  /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setupext/build_ext.py:85: FeatureNotice: Turned ON Numpy support for fast Java array access
    FeatureNotice)
  building '_jpype' extension
  creating build/temp.macosx-10.7-x86_64-3.7
  creating build/temp.macosx-10.7-x86_64-3.7/build
  creating build/temp.macosx-10.7-x86_64-3.7/build/src
  creating build/temp.macosx-10.7-x86_64-3.7/native
  creating build/temp.macosx-10.7-x86_64-3.7/native/python
  creating build/temp.macosx-10.7-x86_64-3.7/native/common
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/noah/anaconda3/include -arch x86_64 -I/Users/noah/anaconda3/include -arch x86_64 -DMACOSX=1 -DHAVE_NUMPY=1 -Inative/common/include -Inative/python/include -Ibuild/src -Inative/jni_include -I/Users/noah/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/noah/anaconda3/include/python3.7m -c build/src/jp_thunk.cpp -o build/temp.macosx-10.7-x86_64-3.7/build/src/jp_thunk.o -ggdb
  warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
  In file included from build/src/jp_thunk.cpp:1:
  In file included from build/src/jp_thunk.h:3:
  native/common/include/jpype.h:82:10: fatal error: 'map' file not found
  #include <map>
           ^~~~~
  1 warning and 1 error generated.
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for jpype1
  Running setup.py clean for jpype1
Failed to build jpype1
Installing collected packages: jpype1, pyhanlp
  Running setup.py install for jpype1 ... error
    ERROR: Complete output from command /Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-record-l51xr0fq/install-record.txt --single-version-externally-managed --compile:
    ERROR: /Users/noah/anaconda3/lib/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'use_scm_version'
      warnings.warn(msg)
    running install
    running build
    running build_py
    creating build/lib.macosx-10.7-x86_64-3.7
    creating build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcollection.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcomparable.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_classpath.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jtypes.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_pykeywords.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jproxy.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_gui.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_darwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/nio.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jstring.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_cygwin.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jboxed.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/types.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/beans.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jvmfinder.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/imports.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jcustomizer.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_core.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jinit.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_linux.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jarray.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jobject.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jclass.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_windows.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jexception.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/reflect.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    copying jpype/_jpackage.py -> build/lib.macosx-10.7-x86_64-3.7/jpype
    running build_ext
    running build_java
    Using Jar cache
    copying native/jars/org/jpype/classloader/JPypeClassLoader.class -> build/lib/org/jpype/classloader
    copying native/jars/org.jpype.jar -> build/lib
    running build_thunk
    Building thunks
      including thunk build/lib/org/jpype/classloader/JPypeClassLoader.class
      including thunk build/lib/org.jpype.jar
    /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setupext/build_ext.py:85: FeatureNotice: Turned ON Numpy support for fast Java array access
      FeatureNotice)
    building '_jpype' extension
    creating build/temp.macosx-10.7-x86_64-3.7
    creating build/temp.macosx-10.7-x86_64-3.7/build
    creating build/temp.macosx-10.7-x86_64-3.7/build/src
    creating build/temp.macosx-10.7-x86_64-3.7/native
    creating build/temp.macosx-10.7-x86_64-3.7/native/python
    creating build/temp.macosx-10.7-x86_64-3.7/native/common
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/noah/anaconda3/include -arch x86_64 -I/Users/noah/anaconda3/include -arch x86_64 -DMACOSX=1 -DHAVE_NUMPY=1 -Inative/common/include -Inative/python/include -Ibuild/src -Inative/jni_include -I/Users/noah/anaconda3/lib/python3.7/site-packages/numpy/core/include -I/Users/noah/anaconda3/include/python3.7m -c build/src/jp_thunk.cpp -o build/temp.macosx-10.7-x86_64-3.7/build/src/jp_thunk.o -ggdb
    warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]
    In file included from build/src/jp_thunk.cpp:1:
    In file included from build/src/jp_thunk.h:3:
    native/common/include/jpype.h:82:10: fatal error: 'map' file not found
    #include <map>
             ^~~~~
    1 warning and 1 error generated.
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command ""/Users/noah/anaconda3/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-record-l51xr0fq/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/bb/yzzgnhrj70q9s996rsfz6txw0000gn/T/pip-install-ynmh4yg5/jpype1/
MacBook-Pro-de-Chen:~ noah$ 

Ê±ÇÈóÆÂ¶Ç‰ΩïËß£ÂÜ≥"
ÂàÜËØçËØçÊÄßÊ†áÊ≥®ÊÑüËßâÊ¨†Â¶•„Äê‰∏â‰πù„Äë,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.4

## ÊàëÁöÑÈóÆÈ¢òÔºö ÊàëÊâÄÂú®ÂåªËçØÁõ∏ÂÖ≥Ë°å‰∏öÔºåÂ§ÑÁêÜ„ÄêÂçéÊ∂¶‰∏â‰πùÂåªËçØËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏„ÄëÊó∂Ôºå„Äê‰∏â‰πù„ÄëË¢´Ê†áÊ≥®‰∏∫Êó∂Èó¥ËØçÔºå ÊàëËÆ§‰∏∫„Äê‰∏â‰πù„Äë‰∏ÄËØçÂèØ‰ª•ÊòØÊï∞ÈáèËØçÔºåÂèØ‰ª•ÊòØÂêçËØç„ÄÇ‰ΩÜÊòØÊó†Ê≥ïÁêÜËß£ÊòØ‰∏Ä‰∏™Êó∂Èó¥ËØçÔºåËØ¥„Äê‰∏â‰πùÂ§©„ÄëÊòØ‰∏Ä‰∏™Êó∂Èó¥ËØçÊàëËÆ§Âêå„ÄÇ

### Ê≠•È™§
‰ΩøÁî®Ê†áÂáÜÂàÜËØçÂô®ÂàÜËØç„ÄêÂçéÊ∂¶‰∏â‰πùÂåªËçØËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏„Äë

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(HanLP.segment(‚ÄúÂçéÊ∂¶‰∏â‰πùÂåªËçØËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏‚Äù));
    }
```
### ÊúüÊúõËæìÂá∫

```
ÊúüÊúõ„Äê‰∏â‰πù„ÄëÊòØÊï∞ÈáèËØç
```

### ÂÆûÈôÖËæìÂá∫

```
„Äê‰∏â‰πù„ÄëÊòØÊó∂Èó¥ËØç
```

"
Â¶Ç‰ΩïÊåâ‰∏çÂêåËßíËâ≤ÂéªËØÜÂà´ÂÖ∂ÂØπÂ∫îÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

ÊÇ®Â•ΩÔºåÊàëÊñ∞ÊâãÊé•Ëß¶nlpÔºåÊé•Âà∞‰∏Ä‰∏™ÈúÄÊ±ÇÂéªÂú®ÊñáÊú¨‰∏≠ËØÜÂà´‰∏çÂêåËßíËâ≤ÁöÑË¥¶Âè∑ÔºåÊàëÁöÑÊÄùË∑ØÂ¶Ç‰∏ãÔºå‰∏çÁü•ÈÅìËøôÊ†∑ÂØπ‰∏çÂØπÔºåËØ∑ÊåáÊïôÔºö

ÊàëÁöÑÊñáÊú¨Á§∫‰æãÔºö
ÊºîÂëòÂë®ÊòüÈ©∞ÔºàÁîµËØùÂè∑Á†ÅÔºö12345678966ÔºâÂú®1998Âπ¥Âá∫ÊºîËøá„ÄäÂ§ßËØùË•øÊ∏∏„ÄãÔºåÂØºÊºîÊòØÁéãÊô∂ÔºàÁîµËØùÂè∑Á†ÅÔºö34567889922ÔºâÔºå‰∏ÄÂêåÂèÇÊºîËøôÈÉ®ÁîµÂΩ±ÁöÑËøòÊúâÊºîÂëòÂê¥Â≠üËææÔºàQQÂè∑Ôºö12342314ÔºâÔºåÂêåÊó∂ÁéãÊô∂ÂØºÊºîÂèàËØ∑‰∫ÜÂë®Â∏ÜÂØºÊºîÔºàÂæÆ‰ø°Âè∑Ôºöasfd1234234ÔºâÊù•‰∏ÄÂêåÊåáÂØº„ÄÇ
ÈÇìË∂ÖÔºåÁîµËØùÔºö12343453425ÔºåÂéªÂπ¥ÂèÇÊºî‰∫ÜÁîµÂΩ±„ÄäÂΩ±„ÄãÔºåËøôÈÉ®ÁîµÂΩ±ÁöÑÂØºÊºîÊòØÁéã‰∫åÈ∫ªÂ≠êÔºå‰ªñÁöÑÊâãÊú∫Âè∑ÊòØÔºö324523523534ÔºåÁéã‰∫åÈ∫ªÂ≠êËßâÂæóËá™Â∑±ÂØºÊºîÁöÑ‰∏çË°åÔºåÂèàËØ∑‰∫ÜÂº†Ëâ∫Ë∞ãÔºàÂæÆ‰ø°Âè∑Ôºö234532kjhkÔºâ‰∏ÄÂêåÊåáÂØº„ÄÇ

Êàë‰ΩøÁî®‰∫ÜcrfÂàÜËØçÂéªÊ†áÊ≥®ËøôÊÆµÊñáÊú¨Ôºö
ÊºîÂëòÂë®ÊòüÈ©∞/ACTORÔºà/W ÁîµËØùÂè∑Á†Å/TEL Ôºö/W 12345678966/ACT_TEL ÔºâW Âú®1998Âπ¥Âá∫ÊºîËøá/AA „Ää/W Â§ßËØùË•øÊ∏∏/MOVIE „Äã/W Ôºå/W ÂØºÊºîÊòØÁéãÊô∂/DIR Ôºà/W ÁîµËØùÂè∑Á†Å/TEL Ôºö/W 34567889922/DIR_TEL Ôºâ/W Ôºå/W ‰∏ÄÂêåÂèÇÊºîËøôÈÉ®ÁîµÂΩ±ÁöÑËøòÊúâ/AA ÊºîÂëòÂê¥Â≠üËææ/ACTOR Ôºà/W QQÂè∑/QQ Ôºö/W 12342314/ACT_QQ Ôºâ/W Ôºå/W ÂêåÊó∂/AA ÁéãÊô∂ÂØºÊºî/DIR ÂèàËØ∑‰∫Ü/AA Âë®Â∏ÜÂØºÊºî/DIR Ôºà/W ÂæÆ‰ø°Âè∑/WX Ôºö/W asfd1234234/DIR_WX Ôºâ/W Êù•‰∏ÄÂêåÊåáÂØº/AA „ÄÇ/W
ÈÇìË∂Ö/ACTOR Ôºå/W ÁîµËØù/TEL Ôºö/W 12343453425/ACT_TEL Ôºå/W ÂéªÂπ¥ÂèÇÊºî‰∫ÜÁîµÂΩ±/AA „Ää/W ÂΩ±/MOVIE „Äã/W Ôºå/W ËøôÈÉ®ÁîµÂΩ±ÁöÑ/AA ÂØºÊºîÊòØÁéã‰∫åÈ∫ªÂ≠ê/DIR Ôºå/W ‰ªñÁöÑÊâãÊú∫Âè∑ÊòØ/TEL Ôºö/W 324523523534/DIR_TEL Ôºå/W Áéã‰∫åÈ∫ªÂ≠ê/DIR ËßâÂæóËá™Â∑±ÂØºÊºîÁöÑ‰∏çË°å/AA Ôºå/W ÂèàËØ∑‰∫ÜÂº†Ëâ∫Ë∞ã/DIR Ôºà/W ÂæÆ‰ø°Âè∑/WX Ôºö/W 234532kjhk/DIR_TEL Ôºâ/W ‰∏ÄÂêåÊåáÂØº/AA „ÄÇW

ËØ≠ÊñôÊ†áËÆ∞Ê∂âÂèäÂà∞ÁöÑËØçÊÄßÔºö
/AA ÊòØÊó†ÂÖ≥‰ø°ÊÅØÔºå/WÊòØÊ†áÁÇπÁ¨¶Âè∑Ôºõ
/ACTOR,/ACT_TEL,/ACT_QQ,/ACT_WX (ÊºîÂëòÂèäÂÖ∂ÂêÑÁßçÂè∑Á†Å)Ôºõ
/DIR,/DIR_TEL,/DIR_QQ,/DIR_WX (ÂØºÊºîÂèäÂÖ∂ÂêÑÁßçÂè∑Á†Å)„ÄÇ

ÊàëÊúâ1‰∫øÂ§öÊù°ËøôÁßçÊï∞ÊçÆÔºåË¶ÅÂú®ÂÖ∂‰∏≠ËØÜÂà´Âá∫ÊºîÂëòÁöÑÂêÑÁßçË¥¶Âè∑Ôºö/ACT_TEL,/ACT_QQ,/ACT_WX,ÂíåÂØºÊºîÁöÑÂêÑÁßçË¥¶Âè∑Ôºö/DIR_TEL,/DIR_QQ,/DIR_TEL,

Ê†áËÆ∞‰∫ÜÂ§ßÊ¶Ç500Êù°ËøôÁßçÊï∞ÊçÆÔºå
ÁÑ∂ÂêéÁî® CRFSegmenter Âíå CRFPOSTagger ÂéªÂàÜÂà´ËÆ≠ÁªÉcws.bin Âíå pos.bin,
ÁÑ∂Âêé CRFLexicalAnalyzer crfLexicalAnalyzer = new CRFLexicalAnalyzer(‚Äúcws.bin‚ÄùÔºå‚Äúpos.bin‚Äù);
Áî® crfLexicalAnalyzer.analyze(""ÊàëÁöÑÊñáÊú¨"")ÔºåÂéªËé∑ÂèñÁõ∏ÂÖ≥ËØçÊÄßÁöÑÂè∑Á†ÅÔºåËøôÊ†∑ÊòØÂê¶Ê≠£Á°ÆÔºå
ÊàëÂú®ËÆ≠ÁªÉ‰∫ÜÂá†Ê¨°ÂêéÂèëÁé∞ÔºåÂêÑÁßçÂè∑Á†ÅÁöÑËØÜÂà´Ê≠£Á°ÆÁéáËøòË°åÔºå‰ΩÜÊòØËßíËâ≤ÁöÑÊ≠£Á°ÆÁéáÂç¥‰∏çÈ´òÔºåÊØîÂ¶Ç 12345678966 Â∫îËØ•ËØÜÂà´‰∏∫ /ACT_TEL(ÊºîÂëòÊâãÊú∫Âè∑Á†Å)Ôºå‰ΩÜÂç¥ËØÜÂà´Êàê‰∫Ü /DIR_TEL(ÂØºÊºîÊâãÊú∫Âè∑Á†Å)Ôºõ
ËøôÁßçÊÉÖÂÜµ‰∏ãÊòØË¶ÅÂä†Â§ßËÆ≠ÁªÉÈáèËøòÊòØË¶ÅÈÄöËøáÂà´ÁöÑ‰ªÄ‰πàÂäûÊ≥ïÊù•ÊèêÈ´òÊ≠£Á°ÆÁéáÔºåËØ∑ÊåáÊïôÔºåË∞¢Ë∞¢ÔºÅÔºÅÔºÅ"
Âè•Ê≥ï‰æùËµñÈîôËØØÈóÆÈ¢ò,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âè•Ê≥ïÂàÜÊûêÁöÑÁªìÊûúÊúâËØØ

## Â§çÁé∞ÈóÆÈ¢ò
Âú®ÂëΩ‰ª§Ë°åÔºåÁõ¥Êé•ËæìÂÖ•hanlp parse <<<  'ËÄÅÂπøÁöÑÂë≥ÈÅì‚Ö¢:Â±±Êµ∑'ÔºåÂç≥ÂèØÁúãÂà∞‚Äò‚Ö¢'Ë¢´ËØÜÂà´‰∏∫‚ÄúÊ†∏ÂøÉÂÖ≥Á≥ª‚ÄùÔºåËøô‰∏™ÁΩóÈ©¨Êï∞Â≠óÂ∫îËØ•‰∏çÁÆóÁöÑÂêßÔºü

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
CRFNERecognizerËÆ≠ÁªÉÂä†ËΩΩÊ®°ÂûãÈóÆÈ¢ò„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ΩøÁî®`com.hankcs.hanlp.model.crf.CRFNERecognizer`Á±ªÁöÑtrainÊñπÊ≥ïËÆ≠ÁªÉNERÊ®°ÂûãÂêéÔºåÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÔºö
// ËÆ≠ÁªÉÊ®°Âûã
CRFNERecognizer nerecognizer = new CRFNERecognizer(null);
nerecognizer.train(""D:\\project\\jupyter-notebook\\data\\ËÆ≠ÁªÉ.txt"", ""D:/hanlp_data/data/model/crf/pku199801/ner.bin"");
// ËÆ≠ÁªÉÂÆåÊàêÔºåÁîüÊàê‰∏§‰∏™Êñá‰ª∂Ôºöner.bin, ner.bin.txt
2. ÁÑ∂Âêé
// Âä†ËΩΩÊ®°Âûã
CRFNERecognizer(""D:/hanlp_data/data/model/crf/pku199801/ner.bin.txt"");
3. Êé•ÁùÄÔºåÂ∞±Êä•Èîô‰∫Ü
```
Exception in thread ""main"" java.lang.IllegalArgumentException: ÈîôËØØÁöÑÊ®°ÂûãÁ±ªÂûã: ‰º†ÂÖ•ÁöÑ‰∏çÊòØÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ê®°ÂûãÔºåËÄåÊòØ POS Ê®°Âûã
	at com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer.<init>(PerceptronNERecognizer.java:43)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:67)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:49)
	at com.andglf.main.main.main(main.java:21)
```
### Ëß¶Âèë‰ª£Á†Å

```
// ËÆ≠ÁªÉÊ®°Âûã
CRFNERecognizer nerecognizer = new CRFNERecognizer(null);
nerecognizer.train(""D:\\project\\jupyter-notebook\\data\\ËÆ≠ÁªÉ.txt"", ""D:/hanlp_data/data/model/crf/pku199801/ner.bin"");
// ËÆ≠ÁªÉÂÆåÊàêÔºåÁîüÊàê‰∏§‰∏™Êñá‰ª∂Ôºöner.bin, ner.bin.txt
// Ê≥®ÊÑèÔºåÁ®ãÂ∫èÂà∞ËøôÈáåÊòØÊ≠£Â∏∏ÊâßË°åÁöÑ„ÄÇ
// Âä†ËΩΩÊ®°Âûã
new CRFNERecognizer(""D:/hanlp_data/data/model/crf/pku199801/ner.bin.txt"");
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Ê≠£Â∏∏ËøêË°å‰∏çÊä•Èîô
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: ÈîôËØØÁöÑÊ®°ÂûãÁ±ªÂûã: ‰º†ÂÖ•ÁöÑ‰∏çÊòØÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ê®°ÂûãÔºåËÄåÊòØ POS Ê®°Âûã
	at com.hankcs.hanlp.model.perceptron.PerceptronNERecognizer.<init>(PerceptronNERecognizer.java:43)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:67)
	at com.hankcs.hanlp.model.crf.CRFNERecognizer.<init>(CRFNERecognizer.java:49)
	at com.andglf.main.main.main(main.java:21)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëã±ÊñáÊñáÊú¨ÂàÜÁ±ªÈÄüÂ∫¶Â•ΩÊÖ¢ÔºåÊÄé‰πàÂäû,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Êàë‰ªéË∞∑Ê≠å‰∏ä‰∏ãËΩΩ‰∫ÜÊñáÊú¨ÂàÜÁ±ªÁöÑËØ≠ÊñôÂ∫ìÔºå‰∏ÄÂÖ±Êúâ8000ÁØá10Â§ö‰∏™ÁßçÁ±ªÁöÑÊñáÁ´†ÔºåÊàëÂ∑≤ÁªèËÆ≠ÁªÉÊàê‰∫ÜÊ®°ÂûãÔºå‰ΩÜÊòØËøêË°åÂàÜÁ±ªÁöÑÊó∂ÂÄôÈÄüÂ∫¶ÁâπÂà´ÊÖ¢ÔºåÂ§ßÊ¶ÇË¶Å‰∏§ÂàÜÈíüÊâçËØÜÂà´Âá∫Êù•Ôºå‰∏≠ÊñáÁöÑÂ§ßÊ¶Ç‰∏§ÁßíÈíüÔºå‰∏∫‰ªÄ‰πàËã±ÊñáÂíå‰∏≠ÊñáÁöÑÂ∑ÆË∑ùÈÇ£‰πàÂ§ßÂë¢ÔºåÂì™ÈáåÂá∫ÁöÑÈóÆÈ¢òÂë¢ÔºåÊàëÂ§ßÊ¶ÇÁúã‰∫ÜÂú®Ëé∑ÂèñÊ®°ÂûãÁöÑÊó∂ÂÄôÊó∂Èó¥ÂæàÈïøNaiveBayesModel model = (NaiveBayesModel) IOUtil.readObjectFrom(MODEL_PATH);‰∏∫‰ªÄ‰πà‰ºöËøôÊ†∑Âë¢ÔºåËØ∑Â§ßÁ•ûÂ∏ÆÂøôÂàÜÊûê‰∏Ä‰∏ã„ÄÇ

ÊàëÁöÑËØ≠ÊñôÂ∫ìÂ§ßÊ¶ÇÈïøËøôÊ†∑
From: admiral@jhunix.hcf.jhu.edu (Steve C Liu)
Subject: Re: Bring on the O's
Organization: Homewood Academic Computing, Johns Hopkins University, Baltimore, Md, USA
Lines: 39
Distribution: world
Expires: 5/9/95
NNTP-Posting-Host: jhunix.hcf.jhu.edu
Summary: Root, root, root for the Orioles...

I heard that Eli is selling the team to a group in Cinninati. This would
help so that the O's could make some real free agent signings in the 
offseason. Training Camp reports that everything is pretty positive right
now. The backup catcher postion will be a showdown between Tackett and Parent
although I would prefer Parent. #1 Draft Pick Jeff Hammonds may be coming
up faster in the O's hierarchy of the minors faster than expected. Mike
Flanagan is trying for another comeback. Big Ben is being defended by
coaches saying that while the homers given up were an awful lot, most came
in the beginning of the season and he really improved the second half. This
may be Ben's year. 
	I feel that while this may not be Mussina's Cy Young year, he will
be able to pitch the entire season without periods of fatigue like last year
around August. I really hope Baines can provide the RF support the O's need.
Orsulak was decent but I had hoped that Chito Martinez could learn defense
better and play like he did in '91. The O's right now don't have many
left-handed hitters. Anderson proving last year was no fluke and Cal's return
to his averages would be big plusses in a drive for the pennant. The 
rotation should be Sutcliffe, Mussina, McDonald, Rhodes, ?????. Olson is an
interesting case. Will he strike out the side or load the bases and then get

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

NaiveBayesModel model = (NaiveBayesModel) IOUtil.readObjectFrom(MODEL_PATH);
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊâßË°åfrom pyhanlp import *  Êä•Èîô‚ÄùA fatal error has been detected by the Java Runtime Environment:‚Äú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->
 ÊâßË°å Ôºöfrom pyhanlp import *  ‚Äú ÂØºÂÖ•pyhanlpÊó∂Êä•Èîô
## ÊàëÁöÑÈóÆÈ¢ò
   Python 3.6.3 |Anaconda, Inc.| (default, Oct  6 2017, 12:04:38) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from pyhanlp import *


<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà Âú®macÁöÑÁªàÁ´ØTerminaterËæìÂÖ•‚Äùpython3‚Äú 
2. ÁÑ∂ÂêéÊâßË°å‚Äùfrom pyhanlp import *‚Äú

### Ëß¶Âèë‰ª£Á†Å

```
  from phhanlp import *
```
### ÊúüÊúõËæìÂá∫



```
Á®ãÂ∫èÊ≠£Â∏∏ÊâßË°åÔºåÊó†ÈîôËØØÊèêÁ§∫
```

### ÂÆûÈôÖËæìÂá∫
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000001124355c0, pid=20972, tid=775
#
# JRE version: Java(TM) SE Runtime Environment (7.0_79-b15) (build 1.7.0_79-b15)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.79-b02 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.dylib+0x30f5c0]  jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x1b
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /Users/www/hs_err_pid20972.log

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![image](https://user-images.githubusercontent.com/12984460/60889481-ab52e080-a28b-11e9-91db-42422d65c4aa.png)

"
ÊÉÖÊÑüÂàÜÊûêÊ≤°ÊúâÁúãÂà∞ÊúâËÆ≠ÁªÉÂá∫Êù•ÁöÑÊ®°ÂûãÊñá‰ª∂Âë¢ÔºüÂú®Âì™Èáå,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÊÉÖÊÑüÂàÜÊûêËÆ≠ÁªÉÊï∞ÊçÆ‰ºöËÆ≠ÁªÉÂ§ÑÊ®°ÂûãÊñá‰ª∂ÂêóÔºåÊàëÁúã‰ª£Á†ÅÊ≤°ÊúâÂë¢

"
perceptronLexicalAnalyzerÁöÑenableOrganizationRecognizeËÆæÁΩÆ‰∏∫falseÔºå‰ªçËÉΩËØÜÂà´Âá∫nt,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Ê†πÊçÆËá™Ë∫´‰ΩøÁî®ÊÉÖÂÜµÔºåÊÉ≥ÂÖ≥Èó≠Êú∫ÊûÑËØÜÂà´ÔºåperceptronLexicalAnalyzerÁöÑenableOrganizationRecognizeËÆæÁΩÆ‰∏∫falseÔºå‰ªçËÉΩËØÜÂà´Âá∫nt„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
analyze(""Âåó‰∫¨ÊïôËÇ≤Â±Ä‰∏æÂäûÁü•ËØÜÁ´ûËµõ"")Ôºõ
ÂÖ≥Èó≠Âêé‰ªç‰ºöËØÜÂà´Âá∫[[Âåó‰∫¨/city ÊïôËÇ≤Â±Ä/n]/nt, ‰∏æÂäû/v, Áü•ËØÜ/n, Á´ûËµõ/vn]
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊñáÊú¨ÂàÜÁ±ªËã±ÊñáËØ≠ÊñôÂ∫ì,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÂÅöÊñáÊú¨ÂàÜÁ±ªÊîØÊåÅËã±ÊñáÂêóÔºåÊòØ‰∏çÊòØÈúÄË¶ÅËã±ÊñáÁöÑËØ≠ÊñôÂ∫ìÔºåÊâæ‰∏çÂà∞Ëã±ÊñáÁöÑÈ¢ÑÊñôÂ∫ìÂë¢Ôºü‰Ω†‰ª¨Â§ßÂÆ∂Ë∞ÅÊúâÂë¢ÔºåÂàÜ‰∫´‰∏Ä‰∏™

"
hanlpÊ∫êÁ†ÅÈõÜÊàêÂà∞springBoot‰∏≠Ôºåwindwo‰∏ã‰∏ÄÂàáÊ≠£Â∏∏Ôºå‰ΩÜÊòØÂà∞linux‰∏äÊä•ÈîôÔºåÂ≠óÂÖ∏Ë∑ØÂæÑÊâæ‰∏çÂà∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§


1ÔºöÈ¶ñÈ°µÊàëÊäähanlpÊ∫êÁ†ÅÂ§çÂà∂Âà∞springbootÈ°πÁõÆÈáåÔºåÊ≤°ÊúâÁî®ÂÆòÁΩëÊèê‰æõÁöÑ‰∏§ÁßçÊñπÂºèÔºåËÄåÊòØÁõ¥Êé•ÂºïÂÖ•Ê∫êÁ†Å„ÄÇ
2ÔºöÂú®window‰∏ãÊµãËØï‰∏ÄÂàáÊ≠£Â∏∏
3Ôºö‰ΩÜÂú®linux‰∏äÈÉ®ÁΩ≤ÂêéÔºåÊä•ÈîôÔºåÊèêÁ§∫Êâæ‰∏çÂà∞Â≠óÂÖ∏Ë∑ØÂæÑ
4ÔºöÂ∏åÊúõ‰ΩúËÄÖÂ∞ΩÂø´ÁúãÂà∞ÂõûÂ§çÔºåÊÄ•Áî®ÔºÅÔºÅÊÑüÊøÄ‰∏çÂ∞ΩÔºÅÔºÅÔºÅÔºÅ

### Ëß¶Âèë‰ª£Á†Å

```
   È¶ñÂÖàÊäähanlpÊ∫êÁ†ÅÂ§çÂà∂Âà∞springBootÈ°πÁõÆÈáåÔºåÂú®window‰∏ãËøêË°åÊ≠£Â∏∏Ôºå‰∏ÄÈÉ®ÁΩ≤Âà∞linux‰∏äÊèêÁ§∫Êâæ‰∏çÂà∞Â≠óÂÖ∏Ë∑ØÂæÑ
		//Ë∞ÉÁî®ÂàÜËØçÁªÑ‰ª∂ÁîüÊàê ‰∫ßÂìÅÂêçÁß∞ÂÖ≥ÈîÆËØç
		List<ProductKeywordDO> keyList=new ArrayList<>();
		List<Term> gjzList=NotionalTokenizer.segment(product.getProdName());
		if(gjzList!=null&&gjzList.size()>0){
			for (int i = 0; i <gjzList.size() ; i++) {
				ProductKeywordDO pkd=new ProductKeywordDO();
				pkd.preInsert();
				pkd.setProductId(product.getId());
				pkd.setName(gjzList.get(i).word);
				pkd.setIsFc(""2"");
				keyList.add(pkd);
			}
		}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËÉΩÊ≠£Á°ÆÂàÜËØçÔºåÁÑ∂ËÄåÁé∞Âú®Êä•ÈîôÔºåÊèêÁ§∫Êâæ‰∏çÂà∞Â≠óÂÖ∏Ë∑ØÂæÑ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Áé∞Âú®Êä•ÈîôÔºåÊèêÁ§∫Êâæ‰∏çÂà∞Â≠óÂÖ∏Ë∑ØÂæÑÔºådataÁõÆÂΩïÂ∫îËØ•ÊîæÂú®linux‰∏äÁöÑ‰ªÄ‰πà‰ΩçÁΩÆÊâçËÉΩË¢´Ê≠£Á°ÆÊâæÂà∞Ë∑ØÂæÑ
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/CustomDictionary.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/CustomDictionary.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/CustomDictionary.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txt
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/person/nrf.txtËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: data/dictionary/person/nrf.txt (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Â§±Ë¥•Ôºödata/dictionary/person/nrf.txt
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Ê≤°ÊúâÂä†ËΩΩÂà∞‰ªª‰ΩïËØçÊù°
02:13:50 [http-nio-8080-exec-10] ERROR HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏data/dictionary/custom/CustomDictionary.txt‰∏çÂ≠òÂú®ÔºÅjava.io.FileNotFoundException: data/dictionary/custom/CustomDictionary.txt.bin (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Ëá™ÂÆö‰πâËØçÂÖ∏[data/dictionary/custom/CustomDictionary.txt, data/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt, data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns, data/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txt, data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt, data/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txt ns, data/dictionary/person/nrf.txt nrf]Âä†ËΩΩÂ§±Ë¥•
02:13:50 [http-nio-8080-exec-10] WARN HanLP - ËØªÂèñdata/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
02:13:50 [http-nio-8080-exec-10] WARN HanLP - Ê†∏ÂøÉËØçÂÖ∏data/dictionary/CoreNatureDictionary.txt‰∏çÂ≠òÂú®ÔºÅjava.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt (No such file or directory)

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Â∏åÊúõ‰ΩúËÄÖÂ∞ΩÂø´ÂõûÂ§ç"
doc2vecËøõË°åËØ≠‰πâÊü•ËØ¢Êó∂ÔºåÊòØÂê¶Â∑≤ÁªèÂÜÖÁΩÆ‰∫ÜÂéªÂÅúÁî®ËØçÁöÑÊìç‰ΩúÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

doc2vecËøõË°åËØ≠‰πâÊü•ËØ¢Êó∂ÔºåÊòØÂê¶Â∑≤ÁªèÂÜÖÁΩÆ‰∫ÜÂéªÂÅúÁî®ËØçÁöÑÊìç‰ΩúÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
fix a small mistake in writing,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
IndexTokenizerÁ¥¢ÂºïÂàÜËØçÂô®Ê≤°ÊúâÊãÜÂàÜÂá∫CustomDictionary.add()Âä®ÊÄÅÊ∑ªÂä†ÁöÑÊãÜÂàÜËØçÊù°,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®CustomDictionaryÂä®ÊÄÅÊ∑ªÂä†‰∫ÜËá™ÂÆö‰πâËØçÊù°ÔºåCustomDictionary.add();Ôºå
‰ΩÜÊòØ‰ΩøÁî®Á¥¢ÂºïÂàÜËØçIndexTokenizerÁöÑÊó∂ÂÄôÔºåÂéª‰∏çËÉΩÊãÜÂàÜÂá∫‰∏äÈù¢Ëá™ÂÆö‰πâÊ∑ªÂä†ÁöÑËØçËØ≠„ÄÇList<Term> segment = IndexTokenizer.segment(buffer);
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å
``
```
    //Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÊù°
				List<CustomWord> customList = wordFrequencyService.queryCustomList();
				List<Term> segment = IndexTokenizer.segment(buffer);
				Segment newSegment = HanLP.newSegment(""viterbi"");
				Segment segment3 = newSegment.enableIndexMode(2);
				
				segment3.enableCustomDictionary(true);
				
				for (CustomWord customWord : customList) {
					boolean add = CustomDictionary.add(customWord.getWord());
					System.out.println(add);
				}
				final char[] charArray = buffer.toCharArray();
				CustomDictionary.parseText(buffer, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
		        {
		            @Override
		            public void hit(int begin, int end, CoreDictionary.Attribute value)
		            {
		            	  System.out.printf(""[%d:%d]=%s %s\n"", begin, end, new String(charArray, begin, end - begin), value);
		            }
		        });
				System.out.println(newSegment.seg(buffer));
				System.out.println(segment3.seg(buffer));
				// Ëá™ÂÆö‰πâËØçÂÖ∏Âú®ÊâÄÊúâÂàÜËØçÂô®‰∏≠ÈÉΩÊúâÊïà
		        //List<Term> segment = HanLP.segment(buffer);
				/*
				 * IndexTokenizer tokenizer = new IndexTokenizer(); List<Term> segment = tokenizer.segment(buffer);
				 * 
				 * System.out.println( tokenizer.segment(buffer)); System.out.println(tokenizer.seg2sentence(buffer));
				 */
				
				System.out.println(segment);
		        CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
		        Segment segment2 = analyzer.enableIndexMode(1);
		        List<Term> seg = segment2.seg(buffer);
		        System.out.println(seg);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂàÜËØçÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.4


## ÊàëÁöÑÈóÆÈ¢ò

`ÈïøÂ§ß`ÂàÜËØçÈóÆÈ¢ò

## Â§çÁé∞ÈóÆÈ¢ò
ÂØπÈïøÂ§ßËøõË°åÂàÜËØçÔºåÊüê‰∫õÊÉÖÂÜµ‰∏ã‰ºöÂàÜÊàê`Èïø„ÄÅÂ§ß`
### Ê≠•È™§

1. ÂáÜÂ§áÂè•Â≠ê`Â∏åÊúõÈïøÂ§ß`
2. ‰ΩøÁî®ÂàÜËØç
3. Êü•ÁúãÁªìÊûú

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        HanLP.segment(""Â∏åÊúõÈïøÂ§ß"").forEach(term -> System.out.print(term.word+"",""));
    }
```
### ÊúüÊúõËæìÂá∫

ÂàÜËØçÊàê `Â∏åÊúõ`+`ÈïøÂ§ß`

```
Â∏åÊúõ,ÈïøÂ§ß,
```

### ÂÆûÈôÖËæìÂá∫

ÂàÜËØçÊàê `Â∏åÊúõ`+`Èïø`+`Â§ß`
```
Â∏åÊúõ,Èïø,Â§ß,
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
‰ΩøÁî®`ÈïøÂ§ß‰ª•Âêé`ËøõË°åÂàÜËØçÂ∞±Ê≠£Â∏∏ÔºåÂàÜËØçÁªìÊûúÊòØ
```
ÈïøÂ§ß,‰ª•Âêé,
```
"
pyhanlp‰∏≠ÔºåAnalyzerÁ±ªÂ•ΩÂÉè‰∏çËÉΩÂú®enableCustomDictionaryForcing(True)Âêé‰ΩøÁî®seg()Ôºå,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.4.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.4.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
1. ÊàëÊÉ≥Ë¶ÅÂú®ÂàÜËØçÂâçÂº∫Âà∂ÂêØÂä®Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂàÜËØçÁªìÊûú‰ΩøÁî®CoreStopWordDictionaryÁ±ªËøáÊª§ÂÅúÁî®ËØç„ÄÇTokenizerÁ±ªÊ≤°ÊúâenableCustomDictionaryForcingÊñπÊ≥ïÔºåÊâÄ‰ª•Âè™ËÉΩ‰ΩøÁî®AnalyzerÁ±ªÂàÜËØç„ÄÇAnalyzerÁ±ªÂÆû‰æãÂåñÂêéÊúâ‰∏§‰∏™ÂàÜËØçÊñπÊ≥ïÔºå‰∏Ä‰∏™seg()Ôºå‰∏Ä‰∏™segment()„ÄÇsegment()ÊñπÊ≥ï‰∫ßÁîüÁöÑÁªìÊûúÊòØLinkedListÁ±ªÔºåÂØπÂÆÉË∞ÉÁî®CoreStopWordDictionaryapply()‰ºöÊä•Èîô„ÄÇseg()ÊñπÊ≥ï‰∫ßÁîüÁöÑÁªìÊûúÊòØArrayListÁ±ªÔºåÂèØ‰ª•Ê≠£Â∏∏ËøáÊª§ÂÅúÁî®ËØç„ÄÇ‰ΩÜÊòØÂØπÂàÜËØçÂô®‰ΩøÁî®#enableCustomDictionaryForcingÂêéÔºå‰ΩøÁî®seg()ÊñπÊ≥ï‰ºöÁõ¥Êé•Êä•java.util.NoSuchElementException„ÄÇËØ∑ÈóÆËøô‰∏™ÊúâÂäûÊ≥ïËß£ÂÜ≥ÂêóÔºü
Â¶ÇÊûú‰∏äËø∞ÈóÆÈ¢òÊó†Ê≥ïËß£ÂÜ≥ÔºåÊúâ‰ªÄ‰πàÊñπÊ°àËÉΩÂú®pyhanlpÈáåÔºåÂêåÊó∂‰ΩøÁî®enableCustomDictionaryForcingÂíåCoreStopWordDictionaryÂêóÔºü

2. ÂØπÈªòËÆ§ÂàÜËØçÂô®ÁöÑÂàÜËØçÁªìÊûúÊâßË°åCoreStopWordDictionary.apply()ËøîÂõûÁöÑÁªìÊûú‰∏∫NoneÔºå1.7.3ÁâàÊú¨Ê≤°ÊúâËøô‰∏™ÈóÆÈ¢ò„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
![image](https://user-images.githubusercontent.com/20453470/60585884-5b73a580-9dc3-11e9-90a5-bad816737bf0.png)

![image](https://user-images.githubusercontent.com/20453470/60634009-d11d5700-9e3f-11e9-8728-94c9e24b6516.png)





"
"ËØ∑ÈóÆËøô‰∏™""data/test/crf/cws-template.txt""Ê®°ÊùøÂú®Âì™Èáå",
hanlp segmentÂàÜËØçÂ§±Ë¥•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.4.jar    
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.4.jar    

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
windowsÁ≥ªÁªücmdÂëΩ‰ª§Ë°åÊ®°Âºè‰∏ãÔºåËæìÂÖ•hanlp segmentÂàÜËØçÊä•ÈîôÔºåËæìÂÖ•hanlp serveÂú®Á∫øÊ≠£Â∏∏
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Â∞ÜdataÊñá‰ª∂Â§πÊîæÂÖ•D:\python35\Lib\site-packages\pyhanlp\staticÊñá‰ª∂Â§π‰∏ãÔºåÂêåÊó∂‰øÆÊîπ‰∫Ühanlp.properties‰∏≠ÁöÑrootË∑ØÂæÑ‰∏∫D:\python35\Lib\site-packages\pyhanlp\static
### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
Traceback (most recent call last):
  File ""d:\python35\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\python35\Scripts\hanlp.exe\__main__.py"", line 9, in <module>
  File ""d:\python35\lib\site-packages\pyhanlp\main.py"", line 99, in main
    print(' '.join(term.toString() for term in segmenter.seg(any2utf8(line))))
TypeError: sequence item 0: expected str instance, java.lang.String found
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
Ê≠£Á°ÆÁöÑÂàÜËØçÁªìÊûú
```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Traceback (most recent call last):
  File ""d:\python35\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\python35\Scripts\hanlp.exe\__main__.py"", line 9, in <module>
  File ""d:\python35\lib\site-packages\pyhanlp\main.py"", line 99, in main
    print(' '.join(term.toString() for term in segmenter.seg(any2utf8(line))))
TypeError: sequence item 0: expected str instance, java.lang.String found
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
crf++ÊïàÁéáÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv 1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv 1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

 ÂΩìnerËÆ≠ÁªÉËØ≠Êñô‰∏∫600M‰ª•‰∏ä,crf++ÁöÑËø≠‰ª£‰ºöÂæàÊÖ¢ÔºåÂèÇÊï∞f 2 Ôºåc 3.0 ËØ∑ÈóÆÊúâ‰ªÄ‰πà‰ºòÂåñÊñπÊ≥ïÂêóÔºüÊàñËÄÖÂÖ∂‰ªñÈ´òÊïàÁöÑÂÉèËøô‰πàÂ§ßÁöÑËÆ≠ÁªÉËØ≠ÊñôÁöÑnerËÆ≠ÁªÉÊñπÂºè

"
ÊÑüÁü•Êú∫Ê®°Âûã‰∫∫ÂêçËØÜÂà´ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp 0.1.45

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂØπÂè•Â≠ê ‚ÄúËøôÊó∂ÊàëÂ•≥ÂÑøÂá§ÈúûÊé®Èó®ËøõÊù•ÔºåÂèàÊëáÊëáÊôÉÊôÉÂú∞ÊääÈó®ÂÖ≥‰∏ä„ÄÇÂá§ÈúûÂ∞ñÂ£∞ÁªÜÊ∞îÂú∞ÂØπÊàëËØ¥Ôºö‚ÄùÂàÜËØçÔºå
‰ºöÂæóÂà∞‚Äú„ÄÇÂá§Èúû‚Äù‰πüÊòØ‰∏™‰∫∫ÂêçËøôÁßçÂå™Â§∑ÊâÄÊÄùÁöÑÁªìÊûú„ÄÇÊääÂè•Âè∑ÊîπÊàêÈÄóÂè∑ÔºåÂàÜËØçÁªìÊûúÂ∞±‰ºöÂèòÊ≠£Â∏∏„ÄÇ
ÊàëÂ∑≤Â∞Ü‚ÄúÂá§Èúû‚ÄùÂä†ÂÖ•ËØçÂÖ∏ÔºåÁªìÊûúÊòØÁõ∏ÂêåÁöÑ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```

txt = ""ËøôÊó∂ÊàëÂ•≥ÂÑøÂá§ÈúûÊé®Èó®ËøõÊù•ÔºåÂèàÊëáÊëáÊôÉÊôÉÂú∞ÊääÈó®ÂÖ≥‰∏äÔºåÂá§ÈúûÂ∞ñÂ£∞ÁªÜÊ∞îÂú∞ÂØπÊàëËØ¥Ôºö""
data_path = ""/home/dream/miniconda3/envs/py37/lib/python3.7/site-packages/pyhanlp/static/data/model/perceptron/large/cws.bin""
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer(data_path,
                                    HanLP.Config.PerceptronPOSModelPath,
                                    HanLP.Config.PerceptronNERModelPath)
print(analyzer.seg(txt))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ËøôÊó∂/r, Êàë/r, Â•≥ÂÑø/n, Âá§Èúû/nr, Êé®Èó®/v, ËøõÊù•/v, Ôºå/w, Âèà/d, ÊëáÊëáÊôÉÊôÉ/v, Âú∞/u, Êää/p, Èó®ÂÖ≥/n, ‰∏ä/f, „ÄÇ/w, Âá§Èúû/nr, Â∞ñÂ£∞/nz, ÁªÜÊ∞î/a, Âú∞/u, ÂØπ/p, Êàë/r, ËØ¥/v, Ôºö/w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ËøôÊó∂/r, Êàë/r, Â•≥ÂÑø/n, Âá§Èúû/nr, Êé®Èó®/v, ËøõÊù•/v, Ôºå/w, Âèà/d, ÊëáÊëáÊôÉÊôÉ/v, Âú∞/u, Êää/p, Èó®ÂÖ≥/n, ‰∏ä/f, „ÄÇ Âá§Èúû/nr, Â∞ñÂ£∞/nz, ÁªÜÊ∞î/a, Âú∞/u, ÂØπ/p, Êàë/r, ËØ¥/v, Ôºö/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
doc2vecËØ≠‰πâÊü•ËØ¢È´òÂπ∂ÂèëÊó∂ÔºåÊúçÂä°ÂÅúÊ≠¢,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âü∫‰∫éflaskÔºåÊääpyhanlp‰∏≠Ë∞ÉÁî®ÊñáÊ°£ÂêëÈáèÊ®°ÂûãËøõË°åËØ≠‰πâÊü•ËØ¢ÂÅöÊàê‰∏Ä‰∏™ÊúçÂä°ÔºåÈ´òÂπ∂ÂèëËØ∑Ê±ÇÁªìÊûúÊó∂ÔºåÊúçÂä°ÂÅúÊ≠¢„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']
    
    if not jpype.isThreadAttachedToJVM():
          jpype.attachThreadToJVM()

    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    doc2vec = DocVectorModel(word2vec._proxy)

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    init_log()
    start_model_server()

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âü∫‰∫éHanLPÁöÑElasticsearchÂàÜËØçÊèí‰ª∂,"Âü∫‰∫éHanLPÁöÑElasticsearchÂàÜËØçÊèí‰ª∂ÔºåÂÅö‰∫ÜÊå∫‰πÖÁöÑ‰∫ÜÔºå2.xÁâàÊú¨‰πüÊúâÔºå‰ΩÜÊòØÂü∫Êú¨‰∏ä‰ªé6.3.1ÁâàÊú¨‰πãÂêéÂíåESÂêåÊ≠•Êõ¥Êñ∞ÔºåÊúâÊó∂ÂèØËÉΩÊõ¥Êñ∞‰∏çÂèäÊó∂Ôºå‰ΩÜÊòØÂ¶ÇÊûúÊúâÊó∂Èó¥Ôºå‰∏ÄÁõ¥ÂÅö‰∏ãÂéªÔºåÂ¶ÇÊûúÊúâÊúâÂÖ¥Ë∂£ÁöÑÂêåÂ≠¶ÂèØ‰ª•‰∏ÄËµ∑ÂºÄÂèëÁª¥Êä§

È°πÁõÆÂú∞ÂùÄÔºö[https://github.com/KennFalcon/elasticsearch-analysis-hanlp](https://github.com/KennFalcon/elasticsearch-analysis-hanlp)

- ÊîØÊåÅ‰∫ÜHanLPÂ§ßÈÉ®ÂàÜÁöÑÂàÜËØçÊñπÂºè

- ÊîØÊåÅËØçÂÖ∏ÁÉ≠Êõ¥Êñ∞

- ÊîØÊåÅËøúÁ®ãËØçÂÖ∏ÂäüËÉΩ

@hankcs Â∏åÊúõÊî∂ÂΩï‰∏Ä‰∏ãÔºåË∞¢Ë∞¢"
hanlpÊúâÊèê‰æõsolrÁöÑpython apiÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

hanlpÊúâÊèê‰æõsolrÁöÑpython apiÂêóÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
CoNLLWordÁ±ªÁöÑÊ≥®Èáä‰∏≠Áñë‰ººÂá∫Áé∞ÊãºÂÜôÈîôËØØÔºü,"`CoNLLWord`Á±ªÁöÑÁ¨¨ 24Ë°å ÂØπÂèòÈáè`LEMMA`Ê≥®Èáä‰∏≠ÂÜôÈÅì

> ÂΩìÂâçËØçËØ≠ÔºàÊàñÊ†áÁÇπÔºâÁöÑÂéüÂûãÊàñËØçÂπ≤ÔºåÂú®‰∏≠Êñá‰∏≠ÔºåÊ≠§Âàó‰∏éFORMÁõ∏Âêå

‰ΩÜÊòØËØ•Êñá‰ª∂Ôºà‰ª•ÂèäÊï¥‰∏™È°πÁõÆÔºâ‰∏≠Âπ∂Ê≤°ÊúâÂØπ`FORM`ÂèòÈáèÁöÑÂÆö‰πâÔºåÊâÄ‰ª•ËøôÈáåÊòØ‰∏çÊòØÂá∫Áé∞‰∫ÜÊãºÂÜôÈîôËØØÔºåÂ∫îËØ•ÊòØ`NAME`ÂèòÈáè„ÄÇËøòÊòØÊàëÁêÜËß£ÁöÑÂÅèÂ∑ÆÔºü

Ê≥®ÔºöÂêåÊ†∑ÁöÑÈóÆÈ¢òËøòÂá∫Áé∞Âú®ËØ•Êñá‰ª∂ÁöÑÁ¨¨ 61„ÄÅ76 Ë°å„ÄÇ"
PythonË∞ÉÁî®Êé•Âè£ÂæóÂà∞ÁöÑËØçÊ≥ïÂàÜÊûêÁªìÊûú‰∏éÂÆòÁΩëÊòæÁ§∫ÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
PythonË∞ÉÁî®Êé•Âè£ÂæóÂà∞ÁöÑËØçÊ≥ïÂàÜÊûêÁªìÊûú‰∏éÂÆòÁΩëÊòæÁ§∫ÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
from pyhanlp import *
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(""‰Ω≥Â•îÊ±ΩËΩ¶Ë¥∏ÊòìÊúâÈôêÂÖ¨Âè∏Â§ßÂ≤óÊ±ΩËΩ¶Áî®ÂìÅÂàÜÂÖ¨Âè∏""))

ÂæóÂà∞ËæìÂá∫Ôºö
![image](https://user-images.githubusercontent.com/15197043/59669211-65ef4600-91ec-11e9-9997-d1c6737170ab.png)

ÂÆòÁΩëËØçÊ≥ïÂàÜÊûêËæìÂá∫Ôºö

![image](https://user-images.githubusercontent.com/15197043/59669295-89b28c00-91ec-11e9-983a-9b9bf60751c7.png)
"
Âü∫‰∫éflutter„ÄÅHanLPÁöÑNLPÂ≠¶‰π†App,‰ΩøÁî®flutterÂü∫‰∫éHanLPÂèëÂ∏É‰∫Ü‰∏Ä‰∏™AppÔºåÈ°πÁõÆÂú∞ÂùÄhttps://github.com/sppsun/nlp_starter ÔºåÊ¨¢ËøéÊãçÁ†ñÔΩûÔΩû
merge,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
"Âú®1.7.3/pyhanlpÈáåÈù¢,PerceptronLexicalAnalyzer ,Ê≤°ÊúâsegÊñπÊ≥ï‰∫Ü","Âú®1.7.3/pyhanlpÈáåÈù¢,PerceptronLexicalAnalyzer ,Ê≤°ÊúâsegÊñπÊ≥ï‰∫Ü,Ë∞ÉÁî®ÁöÑÊó∂ÂÄô,Âá∫Áé∞ÂºÇÂ∏∏:jpype._jexception.NoSuchElementExceptionPyRaisable: java.util.NoSuchElementException
ËÄåË∞ÉÁî®segmentÊñπÊ≥ïÂ∞±Ê≤°ÈóÆÈ¢ò.(ËÄåÊ≠§ÊñπÊ≥ï‰πü‰∏çÊòØÊâÄÊúâÁöÑÂàÜËØçÂô®ÈÉΩÊúâ)
‰ΩøÁî®HanLP.newSegment('crf')‰πü‰∏ÄÊ†∑,Ê≤°ÊúâsegÊñπÊ≥ï‰∫Ü,Âá∫ÂºÇÂ∏∏."
Âü∫‰∫éflaskÔºåÊääpyhanlpÂÅöÊàê‰∏Ä‰∏™ÊúçÂä°ÔºåË∞ÉÁî®Âá∫Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âü∫‰∫éflaskÔºåÊääpyhanlpÂÅöÊàê‰∏Ä‰∏™ÊúçÂä°ÔºåË∞ÉÁî®ÊñáÊ°£ÂêëÈáèÊ®°ÂûãËøõË°åËØ≠‰πâÊü•ËØ¢ÔºåÈÅáÂà∞‰ª•‰∏ãÈóÆÈ¢òÔºå‰ª•Á¨¨‰∏ÄÁßçÊñπÂºè‰ª£Á†ÅË∞ÉÁî®ÊúçÂä°ÔºåÁ≥ªÁªüÊ≤°ÊúâÊä•ÈîôÔºåÂèØÂä†ËΩΩÊ®°ÂûãÂêéÊúçÂä°Ëá™Âä®Ê≠ªÊéâÔºõ‰ª•Á¨¨‰∫åÁßç‰ª£Á†ÅË∞ÉÁî®ÊúçÂä°ÔºåÂ∞±ËÉΩÂ§üÊ≠£Â∏∏ËæìÂá∫ÁªìÊûúÔºåÊàëÊÉ≥‰∫ÜËß£ÁöÑÊòØÁ¨¨‰∏ÄÁßçË∞ÉÁî®ÊñπÂºèÁöÑÊä•ÈîôÂéüÂõ†ÊòØ‰ªÄ‰πàÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

Á¨¨‰∏ÄÁßçÔºö
```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    doc2vec = DocVectorModel(word2vec._proxy)

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    init_log()
    start_model_server()

```

Á¨¨‰∫åÁßçÔºö
```
@app.route('/api/nearest/parse', methods=['POST'])
def NearestParse():
    request_object = json.loads(request.get_data().decode('utf-8'))
    text = request_object['text']

    docs = []
    
    for idx, doc in enumerate(docs):
        doc2vec.addDocument(idx, doc)
        
    for res in interpreter.nearest(text):
        print(res.getKey().intValue(), round(res.getValue().floatValue(), 2))
        
if __name__ == '__main__':
    WordVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
    DocVectorModel = SafeJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
    word2vec = WordVectorModel(word2vec_path)
    doc2vec = DocVectorModel(word2vec._proxy)
    init_log()
    start_model_server()

```

### ÊúüÊúõËæìÂá∫

```
Á¨¨‰∏ÄÁßçËÉΩÂ§üÊ≠£Â∏∏ËæìÂá∫ÁªìÊûú
```

### ÂÆûÈôÖËæìÂá∫

```
ÊúçÂä°Ëá™Âä®Ê≠ªÊéâ
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pythonÊé•Âè£ÁöÑÂè•Ê≥ïËß£ÊûêÂô®parseDependencyÂèØÂê¶Ëß£ÊûêÂàÜËØçÁªìÊûú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3 pyhanlp

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂáÜÂ§áÁî®Âè•Ê≥ïËß£ÊûêÂô®ÂéªËß£ÊûêÂ∑≤ÁªèÁªèËøáÂàÜËØçÂ§ÑÁêÜÂêéÁöÑÁªìÊûú„ÄÇ
>['Êàë','Êù•Ëá™', 'ÁÅ´Êòü', '„ÄÇ']

Âõ†‰∏∫ÁâπÊÆäÈ¢ÜÂüüÁöÑÂàÜËØçËØçË°®Êú™Áü•ÔºåÂáÜÂ§áÁî®Âè¶Â§ñÁöÑÂàÜËØçÂ∑•ÂÖ∑Êù•È¢ÑÂ§ÑÁêÜ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
    from pyhanlp import *
    dep = HanLP.parseDependency(['Êàë','Êù•Ëá™', 'ÁÅ´Êòü', '„ÄÇ'])

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
dep = HanLP.parseDependency(['Êàë','Êù•Ëá™', 'ÁÅ´Êòü', '„ÄÇ'])
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
1	Êàë	Êàë	r	r	_	2	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	Êù•Ëá™	Êù•Ëá™	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	ÁÅ´Êòü	ÁÅ´Êòü	n	n	_	2	Âä®ÂÆæÂÖ≥Á≥ª	_	_
4	„ÄÇ	„ÄÇ	wp	w	_	2	Ê†áÁÇπÁ¨¶Âè∑	_	_
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-34-e4d45bcc3795> in <module>
----> 1 dep = HanLP.parseDependency(['Êàë','Êù•Ëá™', 'ÁÅ´Êòü', '„ÄÇ'])

RuntimeError: No matching overloads found for parseDependency in find. at native/common/jp_method.cpp:127
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Add unit tests for com.hankcs.hanlp.utility.MathUtilityTest,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

Hi,

I've analysed your code base and noticed that `com.hankcs.hanlp.utility.MathUtility` in the `hanlp` module is not fully tested.

I've written some tests that cover this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests should help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other particular classes that you consider important.
"
word2vecËÆ°ÁÆóÁü≠ÊñáÊú¨Áõ∏‰ººÂ∫¶ÊîØÊåÅËã±ÊñáÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

word2vecËÆ°ÁÆóÁü≠ÊñáÊú¨Áõ∏‰ººÂ∫¶ÊîØÊåÅËã±ÊñáÂêóÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *

WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')


word2vec = WordVectorModel(model_path)
doc2vec = DocVectorModel(word2vec)

docs = []

for idx, doc in enumerate(docs):
    doc2vec.addDocument(idx, doc)

for res in doc2vec.nearest(text):
    print('%s = %.2f' % (docs[res.getKey().intValue()], res.getValue().floatValue()))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçÂÖ∏Ê†πÊçÆ‰∏çÂêåÁî®Êà∑‰ΩøÁî®‰∏çÂêåËØçÂÖ∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ëá™ÂÆö‰πâËØçÂÖ∏CustomDictionaryËÉΩ‰∏çËÉΩÔºå‚ÄúÈÄâÊã©‰∏çÂêåÁöÑËØçÂÖ∏‚ÄúËøõË°åÂàÜËØçÔºü
Â∞±ÊòØÊ†πÊçÆ‰∏çÂêåÁöÑÁî®Êà∑Ôºå‰ΩøÁî®‰∏çÂêåÁöÑËá™ÂÆö‰πâËØçÂÖ∏ËøõË°åÂàÜËØç„ÄÇ

ÊàñËÄÖÊòØËÉΩÂê¶ÁªôCustomDictionaryÊ∑ªÂä†‰∏Ä‰∏™Á±ªÂûãÔºåÊ†πÊçÆ‰∏çÂêåÁ±ªÂûãËøõË°å Ê∑ªÂä†‚ÄúËØçÊ±á‚ÄùÔºå‰ª•ÂèäÂàÜËØçÔºå‰∏çÂêåÁ±ªÂûã‰∫í‰∏çÂΩ±Âìç


"
ÊØè‰∏™Áî®Êà∑‰ΩøÁî®ÂçïÁã¨ÁöÑËØçÂÖ∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Portable,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Add unit tests for com.hankcs.hanlp.algorithm.EditDistance,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

Hi,

I've analysed your code base and noticed that `com.hankcs.hanlp.algorithm.EditDistance` in the `hanlp` module is not fully tested.

I've written some tests that cover this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests should help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other particular classes that you consider important."
NERÂØπÂú∞ÂêçÁöÑËØÜÂà´ÂèóÈ´òÈ¢ëËØçÔºàÂ¶ÇÔºöÂ∏ÇÈïøÔºâÁöÑÂπ≤Êâ∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®‰ΩøÁî®NERËØÜÂà´‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÁªÑÁªáÂêçÊó∂ÔºåÁªèÂ∏∏‰ºöË¢´‚ÄôÂ∏ÇÈïø‚ÄòËøô‰∏™ËØçÊâÄÂπ≤Êâ∞„ÄÇÂ¶ÇÔºöÁªáÈÄ†ÊúâÈôêÂÖ¨Âè∏ÂÖ¨Âè∏‰Ωç‰∫éÊ±üÈò¥Â∏ÇÈïøÊ≥æÈïá ÁöÑNERÊèêÂèñÁªìÊûú‰∏∫ ÔºàÂè™ÂÖ≥Ê≥® Ê±üÈò¥Â∏ÇÈïøÊ≥æÈïá ËøôÂá†‰∏™Â≠óÔºâ

### ÂÆûÈôÖËæìÂá∫

Ê±üÈò¥/ns Â∏ÇÈïø/nnt Ê≥æÈïá/ns

### ÊúüÊúõËæìÂá∫

Ê±üÈò¥Â∏Ç/ns ÈïøÊ≥æÈïá/ns

ËØ∑ÈóÆÂ¶Ç‰Ωï‰øÆÊîπËØçÂÖ∏ÂèØ‰ª•Ëé∑ÂæóÊúüÊúõÁªìÊûúÔºü

ÊàëÂ∞ùËØï‰øÆÊîπcustomer dictionary„ÄÇÂØπ‰∫éÁΩë‰∏äÁöÑ‰æãÂ≠êÔºå‚ÄúÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´ÁãóÔºåËøéÂ®∂ÁôΩÂØåÁæéÔºåËµ∞‰∏ä‰∫∫ÁîüÂ∑ÖÂ≥∞‚ÄùËøôÂè•ËØùÊù•ËØ¥ÔºåÂä†‰∏äÂú®Â≠óÂÖ∏ÈáåÂÆö‰πâÊîªÂüéÁãÆÁ°ÆÂÆûÂèØ‰ª•ÊîπÂèòÂàÜËØçÁªìÊûúÔºå‰ΩÜÂØπÊàë‰∏äÈù¢ÁöÑ‰æãÂ≠êÊ≤°ÊúâÊïàÊûú 
####= Â§áÊ≥®
‰∏∫‰∫ÜÊâìÂºÄÂú∞ÂêçÂíåÁªÑÁªáÂêçÁöÑÊèêÂèñÔºåÊàëÂØπsegmentÂä†‰∏ä‰ª•‰∏ã‰∏§‰∏™ÂºÄÂÖ≥ÔºöenablePlaceRecognize(True).enableOrganizationRecognize(True)
"
ÂàÜËØç‰∏çÂáÜÁ°ÆÔºå‰∏çËÉΩÊåâÁÖßËá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØç‰∏çÂáÜÁ°ÆÔºåÂéüÂõ†ÊòØÂõ†‰∏∫Ëã±ÊñáÈÄóÂè∑ÂºïËµ∑ÁöÑ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) {
        String str = ""c,Êàë‰ª¨ÈÉΩÂñúÊ¨¢cËØ≠Ë®ÄÔºåcolor,c++,c#,"";
        CustomDictionary.add(""c"",""zdy"");
        CustomDictionary.add(""c++"",""zdy"");
        System.out.println(HanLP.segment(str));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[c/zdy, ,/w, Êàë‰ª¨/r, ÈÉΩ/d, ÂñúÊ¨¢/v, c/zdy, ËØ≠Ë®Ä/n, Ôºå/w, color/nx, ,/w, c++/zdy,  /w, ,/w, c#/nx, ,/w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[c/zdy, ,/w, Êàë‰ª¨/r, ÈÉΩ/d, ÂñúÊ¨¢/v, c/zdy, ËØ≠Ë®Ä/n, Ôºå/w, color/nx, ,/w, c/zdy, ++,/w, c#/nx, ,/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Python 3.6 ModuleNotFoundError: No module named 'pyhanlp.static',"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv5.0.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv5.0.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<pip install hanlp ÂêéÊâßË°åpython 3.6Êä•error ModuleNotFoundError: No module named 'pyhanlp.static' >

## Â§çÁé∞ÈóÆÈ¢ò
### Ê≠•È™§

1. È¶ñÂÖà pip install hanlp
2. ÁÑ∂ÂêéÊâßË°å‰ª•‰∏ã
# -*- coding: utf8 -*-

import hanlp
from pyhanlp import *

print(hanlp.segment('ÂïÜÂìÅÂíåÊúçÂä°'))

3. Êé•ÁùÄÂá∫Áé∞error 
ModuleNotFoundError: No module named 'pyhanlp.static'

### Ëß¶Âèë‰ª£Á†Å

```
Traceback (most recent call last):
  File ""D:/python/FunNLP/FunNLP/pyhanlp/test1.py"", line 5, in <module>
    import hanlp
  File ""D:\Program Files\Anaconda3\lib\site-packages\hanlp\__init__.py"", line 6, in <module>
    from pyhanlp.static import STATIC_ROOT
ModuleNotFoundError: No module named 'pyhanlp.static'

```
### ÊúüÊúõËæìÂá∫

```
ÊúüÊúõËæìÂá∫
```ËÉΩÂ§üÊàêÂäüÊâßË°å„ÄÇ

### ÂÆûÈôÖËæìÂá∫

```
ÂÆûÈôÖËæìÂá∫
``` errorÂ¶Ç‰∏ä„ÄÇÂÆûÈôÖ‰∏äÂú®D:\Program Files\Anaconda3\Lib\site-packages ÈáåÈù¢ÊòØÊúâÂØπÂ∫îÁöÑstatic„ÄÇ

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Êó†„ÄÇ

"
CRFNERecognizer‰ΩøÁî®CRF++ÂØºÂá∫ÁöÑÊ®°ÂûãÂíåCRF++ÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlp‰∏≠Â¶Ç‰ΩïÂÖ≥Èó≠‰∏≠Êñá‰æùÂ≠òËá™Âä®ËΩ¨Êç¢,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp v0.1.22
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp v0.1.22



<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

pyhanlp‰∏≠Â¶Ç‰ΩïÂÖ≥Èó≠‰∏≠Êñá‰æùÂ≠òËá™Âä®ËΩ¨Êç¢. ËæìÂá∫ÁöÑÊó∂ÂÄôÂè•Ê≥ï‰æùÂ≠òÊòØ‰∏≠Êñá„ÄÇÂ¶Ç‰ΩïËæìÂá∫Ëã±ÊñáÂë¢Ôºü

![image](https://user-images.githubusercontent.com/10768193/58684668-ecfe8a80-83b3-11e9-8494-30be8b7b4b05.png)


ÊàëÁúãÂà∞‰∫ÜÂçö‰∏ªÁî®javaÊó∂ÁöÑËÆæÁΩÆÊòØÂèØ‰ª•ÂÖ≥Èó≠ÁöÑÔºåhttp://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html„ÄÇ


![image](https://user-images.githubusercontent.com/10768193/58684736-38b13400-83b4-11e9-905c-b23369b08401.png)

‰ΩøÁî®pyhanlpÁöÑËØùÂèØ‰ª•Áõ¥Êé•ÂÖ≥Èó≠ÂêóÔºü 

"
URLTokenizer.segment(text);‰ºöÂØºËá¥Á®ãÂ∫èÂç°‰Ωè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®‰ΩøÁî®URLTokenizerÁöÑÊó∂ÂÄô‰ºöÈÅáÂà∞Á®ãÂ∫èÂç°‰ΩèÁöÑÊÉÖÂÜµÔºådubug‰∫Ü‰∏Ä‰∏ãÔºå‰ºö‰∏ÄÁõ¥Âç°Âú®matcher.find()
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ËßÅ‰∏ãÈù¢‰ª£Á†ÅÔºåÂøÖÂÆöËß¶Âèë

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue() throws Exception
    {
        String text = ""Èöè‰æøÂÜôÁÇπÂï•ÂêßÔºüabNfxbGRIAUQfGGgvesskbrhEfvCdOHyxfWBq"";
        List< Term > terms = URLTokenizer.segment(text);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËÉΩÂ§üÊ≠£Á°ÆÊâßË°åÂÆå
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖÁ®ãÂ∫è‰ºö‰∏ÄÁõ¥Âç°Âú®matcher.find()
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
textrankÂ¶Ç‰ΩïËé∑ÂèñÂÖ≥ÈîÆËØçÁöÑÊùÉÈáçÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÈòÖËØª‰∫ÜÊñáÊ°£ÔºåÂÖ≥‰∫étextrankÁöÑ‰ªãÁªçÈáåÂè™ÊúâÊñπÊ≥ïextractKeyword(String content, int size)ÔºåÂç≥ËæìÂÖ•ÊñáÊ°£‰∏éË¶ÅÊèêÂèñÁöÑÂÖ≥ÈîÆËØçÊï∞ÈáèÔºåËøîÂõûÁõ∏Â∫îÊï∞ÈáèÁöÑÂÖ≥ÈîÆËØç„ÄÇ‰ΩÜÊòØÂØπ‰∫é‰∏çÂêåÂÖ≥ÈîÆËØçÁöÑÊùÉÈáçÔºåÊòØÂê¶ÊúâÊé•Âè£ÂèØ‰ª•Ëé∑ÂèñÂë¢Ôºü

### Ëß¶Âèë‰ª£Á†Å

```
String content = ""Á®ãÂ∫èÂëò(Ëã±ÊñáProgrammer)ÊòØ‰ªé‰∫ãÁ®ãÂ∫èÂºÄÂèë„ÄÅÁª¥Êä§ÁöÑ‰∏ì‰∏ö‰∫∫Âëò„ÄÇ‰∏ÄËà¨Â∞ÜÁ®ãÂ∫èÂëòÂàÜ‰∏∫Á®ãÂ∫èËÆæËÆ°‰∫∫ÂëòÂíåÁ®ãÂ∫èÁºñÁ†Å‰∫∫ÂëòÔºå‰ΩÜ‰∏§ËÄÖÁöÑÁïåÈôêÂπ∂‰∏çÈùûÂ∏∏Ê∏ÖÊ•öÔºåÁâπÂà´ÊòØÂú®‰∏≠ÂõΩ„ÄÇËΩØ‰ª∂‰ªé‰∏ö‰∫∫ÂëòÂàÜ‰∏∫ÂàùÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÈ´òÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÁ≥ªÁªüÂàÜÊûêÂëòÂíåÈ°πÁõÆÁªèÁêÜÂõõÂ§ßÁ±ª„ÄÇ"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
ËÉΩÂê¶ÊúâÊé•Âè£ÂèØ‰ª•Ëé∑ÂèñÂà∞‰∏çÂêåÂÖ≥ÈîÆËØçÁöÑÊùÉÈáçÂë¢Ôºü

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
HanLPÂæÆÊúçÂä°Áâà,"Êàë‰ª¨Âà©Áî®ÂΩìÂâçÊµÅË°åÁöÑÂæÆÊúçÂä°Ê°ÜÊû∂ÔºåÂ∞ÅË£Ö‰∫ÜHanLPÂ§ßÈÉ®ÂàÜÊñπÊ≥ïÔºåÈÄöËøáRestfull APIÁöÑÊñπÂºèÊèê‰æõÂæÆÊúçÂä°Ë∞ÉÁî®ÔºåÈ°πÁõÆÂú∞ÂùÄhttps://github.com/sppsun/sca-best-practice ÔºåÊ¨¢ËøéÊãçÁ†ñÔΩû

"
Âêå‰πâËØçËØçÂÖ∏ÁöÑÂºïÂÖ•ÊòØÂê¶ÊúâÂä©‰∫éÊñáÊ°£‰ΩôÂº¶Áõ∏‰ººÂ∫¶ËÆ°ÁÆó,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ


ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3


## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÊòØÂàöÊé•Ëß¶HanLPÔºåÊúâ‰∏™ÈóÆÈ¢òÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÂêå‰πâËØçËØçÂÖ∏ÁöÑÂºïÂÖ•ÊòØÂê¶ÊúâÂä©‰∫éÊñáÊ°£‰ΩôÂº¶Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂ¶ÇÊûúÂèØ‰ª•ÔºåË¶ÅÂ¶Ç‰ΩïÈÖçÁΩÆÔºü
ÊØîÂ¶ÇÔºö System.out.println(wordVectorModel.similarity(""‰π∞Âçñ‰∫∫"",""ÂïÜ‰∫∫""));
""‰π∞Âçñ‰∫∫""Âíå""ÂïÜ‰∫∫""Âú®Âêå‰πâËØçËØçÂÖ∏‰∏≠ÊòØÂêå‰πâËØçÔºå‰ΩÜÊòØÈÄöËøáWordVectorModelÊ±ÇÁõ∏‰ººÂ∫¶ÊòØ-1„ÄÇ
"
ÁπÅËΩ¨ÁÆÄÈîôËØØÊØîËæÉÂ§ö,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂØπ‰∫éÂõΩÈôÖÂåñ‰∏öÂä°ÔºåqueryÈáåÈù¢‰ºöÊúâÁÆÄ‰Ωì„ÄÅÁπÅ‰Ωì„ÄÅËã±Êñá„ÄÅÊó•ËØ≠Á≠âÊÉÖÂÜµÔºåÁπÅËΩ¨ÁÆÄÈîôËØØÊØîËæÉÂ§ö„ÄÇ

## ÊµãËØïÊñπÊ≥ï
Êãø‰∫ÜÁ∫¶2gÁöÑÁôæÁßëËØ≠ÊñôÔºåÂØπÊØî‰∫ÜopenccÂíåhanlpÁöÑÁπÅËΩ¨ÁÆÄÁªìÊûúÔºåÁ¨¨‰∏ÄÂàó‰∏∫ËØ≠ÊñôÔºåÁî±‰∫éËØ≠ÊñôÊØîËæÉÈïøÔºåÁ™ÉÂèñ‰∫Ü‰ª•diff‰∏∫‰∏≠ÂøÉÁöÑÂâçÂêé10‰∏™Â≠óÔºåÁ¨¨‰∫åÂàó‰∏∫openccÁªìÊûúÔºåÁ¨¨‰∏âÂàó‰∏∫hanlpÁªìÊûú„ÄÇ


ÂéüÂßãÂè•Â≠ê(„Äê„ÄëÂÜÖÈÉ®‰∏∫diffÂÜÖÂÆπ) |	opencc |	hanlp
 |  ------------| -- | -- | 
ÔºåÊñüÈÖíÁöÑ‰∫∫ÁøªËøáÂ§ßÈáëÊñó„ÄêÁåõ„ÄëÂáª‰ª£ÂêõÔºå‰∏Ä‰∏ãÂ∞±Á†∏Ê≠ª |	Áåõ |   Âãê
Ê†°ÂèäÁßëÁ†îÂçï‰ΩçÊåÇÈí©ÔºåÂπ∂„ÄêÂª∫„ÄëÁ´ã‰∫ÜÈïøÊúüÁöÑÂçè‰ΩúÂÖ≥Á≥ª |	Âª∫ |	Âàõ
ÂØáÂ§´‰∫∫ ‰ªñËá™Êã£‰∏ÄÊê≠Èáë„ÄêÂ†¶„ÄëÊ≠ª„ÄÇ‚Äù‰∫¶ÁúÅ‰Ωú‚Äú ‚Å§|	Â†¶|	Èöé
ÁªºÂêàÂÖºÂÆπÊÄß „ÄÄ„ÄÄ‰∫å„ÄÅ„ÄêÂ§ß„Äë‰ºóÂ®±‰πêÊÄß „ÄÄ„ÄÄ‰∏â„ÄÅ|	Â§ß|	Á¶è
ÂêàÂÖºÂÆπÊÄß „ÄÄ„ÄÄ‰∫å„ÄÅÂ§ß„Äê‰ºó„ÄëÂ®±‰πêÊÄß „ÄÄ„ÄÄ‰∏â„ÄÅ‰∫í|	‰ºó|	ÊñØ
ËøõË°åÊúâÊïàÁöÑ‰º†Êí≠ÊéßÂà∂Âíå„ÄêÊï¥„ÄëÂêàÁÆ°ÁêÜ„ÄÇ2007Âπ¥|	Êï¥|	ÈõÜ
Ë°åÊúâÊïàÁöÑ‰º†Êí≠ÊéßÂà∂ÂíåÊï¥„ÄêÂêà„ÄëÁÆ°ÁêÜ„ÄÇ2007Âπ¥Ôºå|	Âêà|	Êàê
ÊúâÁâ©È•ÆÁ¢ßÊ∞¥ÔºåÈ´òÊûóÊåÇÈùí„ÄêËú∫„Äë„ÄÇ‚Äù"",""ts"":|	Ëú∫|	Èúì
Ë•øÂÆâÂ∏ÇËé≤ÊπñÂüéÂÜÖÔºåÂÖ±ËÆ°„ÄêÊàø„ÄëÂ±ã231Êà∑„ÄÇ"",""|	Êàø|	‰Ωè
ÔºõË°åÁ®ã‰∏áÈáåÁöÑ‚Äú‰∏ñÁïåÂ±ã„ÄêËÑä„ÄëÊ±ΩËΩ¶ÊåëÊàòËµõ‚ÄùÁ≠âÊàêÂäü|	ËÑä|	Âµ¥
Êàê‚ÄúÂÖ®ÂõΩÊÄß‚Äù„ÄÅ‚ÄúÂÖ®Á®ã„ÄêÂºè„Äë‚ÄùÁöÑÊäÄÊúØÂàõÊñ∞ÂÖ¨ÂÖ±Êúç|	Âºè|	Â∫è

Êõ¥Â§ödiffÂèÇËßÅÊñá‰ª∂ [**diff.txt**](https://github.com/hankcs/HanLP/files/3223791/diff.txt)Ôºå‰ª•tabÈîÆÂàÜÈöîÔºåÂæàÂ§öÁî±‰∫éÂéüÂßãquery‰∏éÁπÅËΩ¨ÁÆÄÂêéÂè•Â≠êÈïøÂ∫¶‰∏ç‰∏ÄËá¥ÔºåÂèØËÉΩ‰ºöÂá∫Áé∞„Äê„ÄëÂú®ÈùûËΩ¨Êç¢Â≠óÁöÑ‰∏ä„ÄÇ

"
ËØ∑Ê±ÇÂá∫‰∏Ä‰∏™serverÁâàÁöÑÔºå‰ª•‰æø‰∫éÂÖ∂‰ªñÂºÄÂèëËØ≠Ë®ÄÂèØ‰ª•ÈÄöËøáÊé•Âè£Ë∞ÉÁî®,ËØ∑Ê±ÇÂá∫‰∏Ä‰∏™serverÁâàÁöÑÔºå‰ª•‰æø‰∫éÂÖ∂‰ªñÂºÄÂèëËØ≠Ë®ÄÂèØ‰ª•Áõ¥Êé•ÈÄöËøáAPIÁöÑÂΩ¢ÂºèË∞ÉÁî®
‚ÄúÁõ¥Èù¢‰∫∫Áîü‚ÄùËøô‰∏™ËØçËΩ¨ÊàêÁπÅ‰ΩìÔºåÂØπ‰∫é‚ÄúÈù¢‚ÄùÁöÑËΩ¨Êç¢Âá∫Áé∞ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->



### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        String content = ""Áõ¥Èù¢Áé∞ÂÆû,Áõ¥Èù¢‰∫∫Áîü"";
		System.out.println(HanLP.s2hk(content));
		System.out.println(HanLP.s2tw(content));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Áõ¥Èù¢ÁèæÂØ¶,Áõ¥Èù¢‰∫∫Áîü
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Áõ¥Èù¢ÁèæÂØ¶,Áõ¥È∫™‰∫∫Áîü
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
‰∏∫‰ªÄ‰πàÊàëÊèêÂèñÂú∞ÂùÄÁöÑÊó∂ÂÄôÂíåDemo‰∏äÁöÑ‰∏ç‰∏ÄÊ†∑Âë¢,"ÂØπ‰∫éÂêå‰∏Ä‰∏™Âú∞ÂùÄÔºåDEMOÂíåÊàëÁöÑÁ®ãÂ∫èÂàÜÂá∫Êù•ÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºåÊàë‰ΩøÁî®ÁöÑÊòØ1.7.3
‰∏äÊµ∑‰∏äÊµ∑Â∏ÇÊµ¶‰∏úÊñ∞Âå∫ÈáëÊ°•ÈïáÈáëÈ´òË∑Ø2216ÂºÑ
DEMOÂàÜÂá∫Êù•ÊòØÔºö‰∏äÊµ∑ ‰∏äÊµ∑Â∏Ç Êµ¶‰∏úÊñ∞Âå∫ ÈáëÊ°•Èïá ÈáëÈ´ò Ë∑Ø 2216 ÂºÑ
ÊàëÁöÑÁ®ãÂ∫èÂàÜÂá∫Êù•ÊòØÔºö[‰∏äÊµ∑/ns, ‰∏äÊµ∑Â∏ÇÊµ¶‰∏úÊñ∞Âå∫/ns, ÈáëÊ°•Èïá/ns, ÈáëÈ´òË∑Ø/ns, 2216/m, ÂºÑ/v]
Êää‰∏äÊµ∑Â∏ÇÊµ¶‰∏úÊñ∞Âå∫Ê≤°ÊúâÂàÜÂºÄÊàê‰∏äÊµ∑Â∏Ç  Êµ¶‰∏úÊñ∞Âå∫
‰ª£Á†ÅÔºö
       Segment segment = HanLP.newSegment().enablePlaceRecognize(true);
        List<Term> termList = segment.seg(""‰∏äÊµ∑‰∏äÊµ∑Â∏ÇÊµ¶‰∏úÊñ∞Âå∫ÈáëÊ°•ÈïáÈáëÈ´òË∑Ø2216ÂºÑ"");
        System.out.println(termList);"
‰ΩøÁî®Ëá™Â∑±ÁöÑËØ≠ÊñôËÆ≠ÁªÉÂàÜËØçÊ®°ÂûãÊó∂ÔºåÊÄé‰πàÂä†ÂÖ•ÊïàÊûúÊØîËæÉÂ•ΩÁöÑ‰∫∫Âêç„ÄÅÊú∫ÊûÑÂêçÁöÑÂëΩÂêçËØÜÂà´,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.3 ; masterÂàÜÊîØ

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÊàë‰ΩøÁî®https://github.com/hankcs/HanLP/wiki/CRF%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90 ÁöÑcrfÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÊñáÊ°£ÁöÑÊñπÊ≥ïÔºåÂØπËá™Â∑±ÁöÑ‰∏ì‰∏öÈ¢ÜÂüüËØ≠ÊñôËøõË°å‰∫ÜËÆ≠ÁªÉ
2. ÁÑ∂ÂêéÊàëÂèëÁé∞ÂØπ‰∫∫ÂêçÊú∫ÊûÑÂêçÁöÑËØÜÂà´ÊïàÊûúÊúâÁÇπÁ≥üÁ≥ïÔºåÊàëÂú®ÊÉ≥ËÉΩÂê¶Â∞ÜËá™Â∑±ÊâÄÂæóÁöÑ‰∏ì‰∏öÈ¢ÜÂüüÂàÜËØçÂô®ÂíåÈªòËÆ§ÁöÑ‰∫∫ÂêçËØÜÂà´ÔºåÊú∫ÊûÑÂêçËØÜÂà´ÊïàÊûúÂ•ΩÁöÑÁªìÂêà‰ΩøÁî®


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰øÆÊîπ Java ËøêË°åÁéØÂ¢ÉÁâàÊú¨ÔºàHanlp1.7.3Ôºâ,"ËøêË°å‰ª£Á†ÅÊàñËÄÖÂëΩ‰ª§Ë°å`hanlp segment <<< ...`Êó∂ÔºåÊèêÁ§∫Êàë‰ΩøÁî®‰ΩøÁî®Java6ÔºåÊàëÂ∑≤ÁªèÊúâÊúÄÊñ∞ÁâàÊú¨‰∫ÜÔºàÈÄöËøáhomebrew ÂÆâË£ÖÔºâÔºåÂèØ‰∏çÂèØ‰ª•‰ΩøÁî®Ëøô‰∏™ÁâàÊú¨Ôºü

[~] java --version
openjdk 12.0.1 2019-04-16
OpenJDK Runtime Environment (build 12.0.1+12)
OpenJDK 64-Bit Server VM (build 12.0.1+12, mixed mode, sharing)"
Â¶ÇÊûúËØ≠ÊñôÈõÜ‰∏≠Êúâ‰∏≠Êã¨Âè∑ÂºÄÂ§¥ÁöÑËØçÔºåËÆ≠ÁªÉÊ®°ÂûãÊó∂‰ºöÊäõÂá∫ÂºÇÂ∏∏„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Â¶ÇÊûúËØ≠ÊñôÈõÜ‰∏≠Êúâ **‰∏≠Êã¨Âè∑** ÂºÄÂ§¥ÁöÑËØçÔºå‰æãÂ¶ÇÔºö **[2003]00072/nz**  ÔºåËÆ≠ÁªÉÊ®°ÂûãÊó∂‰ºöÊäõÂá∫ÂºÇÂ∏∏„ÄÇ
java.lang.IllegalArgumentException: /opt/corpus/1.txtËØªÂèñÂ§±Ë¥•

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. Áî®Ëá™Â∑±ËÆ≠ÁªÉÁöÑÊÑüÁü•Êú∫Ê®°ÂûãÂàÜËØç„ÄÅÊ†áÊ≥®ËØçÊÄß„ÄÇ

1.txt ÊñáÊú¨ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö
```
Â∑•Á®ãÈ°πÁõÆÂÖçÁ®éÁ°ÆËÆ§‰π¶ÔºàÁºñÂè∑Ôºö[2003]00072Ôºâ
```

ÂàÜËØçÊ†áÊ≥®ÁªìÊûúÔºö
```
Â∑•Á®ã/n È°πÁõÆ/n ÂÖçÁ®é/n Á°ÆËÆ§‰π¶/n Ôºà/w ÁºñÂè∑/nz Ôºö/w [2003]00072/nz Ôºâ/w
```

2. ÊèêÂèñËØ≠ÊñôÁîüÊàêËØçÂÖ∏ÊàñËÄÖËÆ≠ÁªÉÊ®°Âûã

### Ëß¶Âèë‰ª£Á†Å

```
    public void testLoadCoupus() {
        final DictionaryMaker dictionaryMaker = new DictionaryMaker();
        CorpusLoader.walk(""/opt/corpus/1.txt"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(com.hankcs.hanlp.corpus.document.Document document)
            {
                    for (IWord word : document.getWordList()) {
                        if (word.getLabel().equals(""n"")) {
                                System.out.println(word.getValue());
                                dictionaryMaker.add(word);
                        }
                   }
            }
        });
        dictionaryMaker.saveTxtTo(""/opt/n"");
    }
```
### ÊúüÊúõËæìÂá∫

ÊúüÊúõÂèØ‰ª•Ê≠£Â∏∏ÁîüÊàê n Êñá‰ª∂„ÄÇ

### ÂÆûÈôÖËæìÂá∫

/opt/corpus/1.txt2019-05-22 10:42:02.503  WARN 2618 --- [           main] HanLP                                    : ‰ΩøÁî® 2003ÂàõÂª∫Âçï‰∏™ÂçïËØçÂ§±Ë¥•
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : ‰ΩøÁî®ÂèÇÊï∞2003ÊûÑÈÄ†ÂçïËØçÊó∂ÂèëÁîüÈîôËØØ
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : Âú®Áî® [2003]00072/nz ÊûÑÈÄ†ÂçïËØçÊó∂Â§±Ë¥•ÔºåÂè•Â≠êÊûÑÈÄ†ÂèÇÊï∞‰∏∫ Â∑•Á®ã/n È°πÁõÆ/n ÂÖçÁ®é/n Á°ÆËÆ§‰π¶/n Ôºà/w ÁºñÂè∑/nz Ôºö/w [2003]00072/nz Ôºâ/w
2019-05-22 10:42:02.504  WARN 2618 --- [           main] HanLP                                    : ‰ΩøÁî® Â∑•Á®ã/n È°πÁõÆ/n ÂÖçÁ®é/n Á°ÆËÆ§‰π¶/n Ôºà/w ÁºñÂè∑/nz Ôºö/w [2003]00072/nz Ôºâ/w ÂàõÂª∫Âè•Â≠êÂ§±Ë¥•

java.lang.IllegalArgumentException: /opt/corpus/1.txtËØªÂèñÂ§±Ë¥•

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
python Â∞ÅË£ÖÁöÑhanlp-1.7.3 ËøêË°åÊä•Èîô,"‰∏ãÈù¢ÊòØÊä•Èîô‰ø°ÊÅØ„ÄÇ
Python3.7Ôºå macOS
hanlp-1.7.3

> 
> Using local /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/hanlp-1.7.3-release.zip, ignore https://github.com/hankcs/HanLP/releases/download/v1.7.3/hanlp-1.7.3-release.zip
> Traceback (most recent call last):
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/bin/hanlp"", line 6, in <module>
>     from pyhanlp.main import main
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 122, in <module>
>     _start_jvm_for_hanlp()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/__init__.py"", line 39, in _start_jvm_for_hanlp
>     from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/__init__.py"", line 309, in <module>
>     install_hanlp_jar()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/__init__.py"", line 200, in install_hanlp_jar
>     with zipfile.ZipFile(jar_zip, ""r"") as archive:
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py"", line 1222, in __init__
>     self._RealGetContents()
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py"", line 1289, in _RealGetContents
>     raise BadZipFile(""File is not a zip file"")
> zipfile.BadZipFile: File is not a zip file"
TextRankËø≠‰ª£ÁÆóÊ≥ïÁñëÈóÆ----ÊèêÈóÆÔºàÂ∑≤Ëß£ÂÜ≥Ôºâ,"<!--
TextRankËø≠‰ª£ÁÆóÊ≥ïÁñëÈóÆ----ÊèêÈóÆ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢òÔºàÂ∑≤Ëß£ÂÜ≥Ôºâ

 
‰ΩúËÄÖÂú®ÊèêÂèñÂÖ≥ÈîÆÂ≠óËÉΩÂäõTextRankKeyword.java‰∏≠ÔºåËø≠‰ª£ÂÖ¨ÂºèÁî®ÁöÑÊòØÂéüÂßãÁöÑPageRankÂêóÔºüË≤å‰ººÊ≤°ÊúâÁî®TextRankÁöÑËø≠‰ª£ÂÖ¨ÂºèÔºåËøòÊòØÊàëÁêÜËß£ÁöÑ‰∏çÂØπÔºåÊúõÊåáÊ≠£ÔºÅ

----‰∏äËø∞ÁêÜËß£Èîô‰∫ÜÔºåÂè¶Â§ñ‰∏Ä‰∏™ÊúâÊùÉÈáçÁöÑÂÖ¨ÂºèÔºåÊòØÁî®Êù•ÊèêÂèñÊëòË¶ÅÁî®ÁöÑ


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        for (int i = 0; i < max_iter; ++i)
        {
            Map<String, Float> m = new HashMap<String, Float>();
            float max_diff = 0;
            for (Map.Entry<String, Set<String>> entry : words.entrySet())
            {
                String key = entry.getKey();
                Set<String> value = entry.getValue();
                m.put(key, 1 - d);
                for (String element : value)
                {
                    int size = words.get(element).size();
                    if (key.equals(element) || size == 0) continue;
                    // ---------ËøôËæπÁöÑËø≠‰ª£ÂÖ¨Âºè-----------
                    m.put(key, m.get(key) + d / size * (score.get(element) == null ? 0 : score.get(element)));

                }
                max_diff = Math.max(max_diff, Math.abs(m.get(key) - (score.get(key) == null ? 0 : score.get(key))));
            }
            score = m;
            if (max_diff <= min_diff) break;
        }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êï∞ÊçÆÊ∫ê‰∏ãËΩΩw,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp  1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ÂΩìÂâçÁöÑÁõ¥Êé•‰∏ãËΩΩÈÄüÂ∫¶ÂæàÊÖ¢ËÉΩ‰∏çËÉΩÂ∞ÜÊï∞ÊçÆÊ∫ê‰∏ä‰º†‰∏Ä‰ªΩÂà∞‰∫ëÁõò -->

"
ÂÖ≥‰∫éÁªìÊûÑÂåñÊÑüÁü•Êú∫‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´Âô®Êó†Ê≥ïËæìÂá∫Ê≠£Á°ÆÁöÑËØçËØ≠ÊâÄÂ±ûÁöÑÂëΩÂêçÂÆû‰ΩìÊàêÂàÜ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âà©Áî®ÁªìÊûÑÂåñÊÑüÁü•Êú∫Ê†áÊ≥®Ê°ÜÊû∂‰∏≠ÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´Âô®ÂØπÊñ∞ÁöÑËØ≠Âè•ËøõË°åËØÜÂà´Ôºå‰ΩÜÊú™ËÉΩËæìÂá∫Ê≠£Á°ÆÁöÑÁªìÊûúÔºö
ÂéüÈ°µÈù¢Á§∫‰æãÔºö
recognize(""Âê¥Âø†Â∏Ç ‰π≥Âà∂ÂìÅ ÂÖ¨Âè∏ Ë∞≠Âà©Âçé Êù•Âà∞ Â∏ÉËææÊãâÂÆ´ ÂπøÂú∫"".split("" ""), ""ns n n nr p ns n"".split("" ""))));
ËæìÂá∫Ôºö
[B-nt, M-nt, E-nt, S, O, S, O]
‰ª•‰∏äÁ§∫‰æãÂú®Ëá™Â∑±ÁöÑÂ§çÁé∞‰ª£Á†Å‰∏≠Ê≤°ÊúâÈóÆÈ¢ò

‰ΩÜÊòØ Âú®Ë∞ÉÁî®ÊÑüÁü•Êú∫ÂàÜËØçÂØπÂÖ∂‰ªñËØ≠Âè•Ëé∑ÂèñÂàÜËØçÂíåËØçÊÄßÂπ∂ËæìÂÖ• Â¶Ç‰∏ãÔºö

## Â§çÁé∞ÈóÆÈ¢ò

‰ΩÜÊòØ Âú®Ë∞ÉÁî®ÊÑüÁü•Êú∫ÂàÜËØçÂØπÂÖ∂‰ªñËØ≠Âè•Ëé∑ÂèñÂàÜËØçÂíåËØçÊÄßÂπ∂ËæìÂÖ• Â¶Ç‰∏ãÔºö

### Ê≠•È™§

1. È¶ñÂÖàÂà©Áî®NLPTokenizer Ëé∑ÂèñÂàÜËØç‰ª•ÂèäÊ†áÊ≥®ÁªìÊûú
2. ÁÑ∂Âêé‰∏§‰∏™Êï∞ÁªÑÁªìÊûú‰º†ÂÖ•recognize(String[],String[])


### Ëß¶Âèë‰ª£Á†Å
    recognizer.recognize(""ÁæéÂõΩÁ∫ΩÁ∫¶ Âíå ‰∏≠ÂõΩ ‰∏äÊµ∑ ÈÉΩ Âæà Â•ΩÁé©"".split("" ""), ""ns c ns ns d d a"".split("" ""));

### ÊúüÊúõËæìÂá∫
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
```
[B-nt, O,M-nt, E-nt, O, O, O, O]//Â§ßÊ¶ÇÁåúÊµãÁöÑÁªìÊûúÔºå‰ΩÜËá≥Â∞ë‰ºöÊúâB-nt/M-nt/E-ntËøôÊ†∑ÁöÑÊ†áÊ≥®È°π
```

### ÂÆûÈôÖËæìÂá∫
[O ,O, O, O, O, O, O] 

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÊòØÂê¶‰∏éËæìÂÖ•ÁöÑÂàÜËØçÁªìÊûúÂíåËØçÊÄßÊ†áÊ≥®ÊúâÂÖ≥Ôºå
NLPTokenizerÁöÑÊ†áÊ≥®ÁªìÊûú‰∏çËÉΩÊúâÊïàËØÜÂà´Ôºü
"
NLPÊó†Ê≥ïËØÜÂà´Êó•ÊúüÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

‰ªéÁôæÂ∫¶ËøòÊúâstanfod coreNLPËøáÊù•ÁöÑÔºå‰ΩÜÊòØHanLPËøûÊó•Êúü‰πüÊó†Ê≥ïËØÜÂà´Ôºü
![ÂõæÁâá](https://user-images.githubusercontent.com/13548273/57267480-e5004300-70b2-11e9-8bc2-9ab1eba2b8f2.png)
"
Ëá™ÂÆö‰πâËØçÂÖ∏Â≠òÂú®ÈïøËØçÔºåËá™ÂÆö‰πâËØçÊÄß ‰ª•mÊàñx ÂºÄÂ§¥Ôºå‰ºöË¢´‰øÆÊîπËØçÊÄß,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ëá™ÂÆö‰πâËØçÂÖ∏Â≠òÂú®ÈïøËØçÔºåËá™ÂÆö‰πâËØçÊÄß ‰ª•mÊàñx ÂºÄÂ§¥Ôºå‰ºöË¢´‰øÆÊîπËØçÊÄß
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. Ê∑ªÂä†Â∏¶ÊúâËá™ÂÆö‰πâËØçÊÄßÁöÑËá™ÂÆö‰πâÂàÜËØçËØçÂÖ∏ÔºåËá™ÂÆö‰πâ.txt

ÊàëÁöÑÈ¢ùÂ∫¶ xyz 1000
ÊèêÈ´òÈ¢ùÂ∫¶ my 1000

2. ‰øÆÊîπhanlp.propertiesÔºåÂú®CustomDictionaryPathÊ∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏(Ëá™ÂÆö‰πâ.txt)
3. Ë∞ÉÁî®HanLP.segment(""ÊàëÁöÑÈ¢ùÂ∫¶‰∏çÂ§üÔºåÈúÄË¶ÅÊèêÈ´òÈ¢ùÂ∫¶"")

### Ëß¶Âèë‰ª£Á†Å

```

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÊàëÁöÑÈ¢ùÂ∫¶/xyz, ‰∏çÂ§ü/a, Ôºå/w, ÈúÄË¶Å/v, ÊèêÈ´òÈ¢ùÂ∫¶/my]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÊàëÁöÑÈ¢ùÂ∫¶/x, ‰∏çÂ§ü/a, Ôºå/w, ÈúÄË¶Å/v, ÊèêÈ´òÈ¢ùÂ∫¶/mq]
```
### ÂéüÂõ†ÂàÜÊûê
Ëá™ÂÆö‰πâÂàÜËØçÁöÑÊó∂ÂÄôÔºåÂú®ÊãºÊé•ËØçÁöÑÊó∂ÂÄôÔºåViterbiSegmentÁöÑcombineWordsÊñπÊ≥ïË∞ÉÁî®VertexÁöÑcompileRealWordÊñπÊ≥ïÔºåÂú®ËØçÊÄß‰ª•xÂíåmÂºÄÂ§¥ÁöÑÊó∂ÂÄôÔºåÂØºËá¥ËØçÊÄß‰øÆÊîπ

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØç Ê∑ªÂä†Âà∞ CustomDictionary ÈáåÈù¢Â∞±ÂèØ‰ª•Ë¢´ËØÜÂà´ÔºåËá™Â∑±Âä†ËΩΩËØçÂÖ∏Â∞±‰∏çË¢´ËØÜÂà´,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
FAQÁ¨¨‰∏ÄÊù°,"![ÂõæÁâá](https://user-images.githubusercontent.com/13548273/57191377-6c38a400-6f57-11e9-91f9-928b71247edd.png)
Êñá‰∏≠ÊèêÂà∞Êó†ËÆ∫ÊÄé‰πàÂàÜËØçÔºåÂïÜÂìÅ„ÄÅÂíåÊúç„ÄÅÊúçÂä°ÈÉΩ‰∏çÂèØËÉΩÂêåÊó∂Âá∫Áé∞ÔºåËøôÊòØ‰∏ç‰∏•Ë∞®ÁöÑ„ÄÇ
![ÂõæÁâá](https://user-images.githubusercontent.com/13548273/57191407-c76a9680-6f57-11e9-8567-cf0979b8f234.png)
"
ÂΩìÂ§öÁ∫øÁ®ãÂÅöÂè•Ê≥ï‰æùÂ≠òÂàÜÊûêÔºåÂΩìÁî®enableDeprelTranslator(False)Êó∂Ôºå‰ºöÊä•Âú∞ÂùÄËæπÁïåÈîôËØØ„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Êàë‰øÆÊîπ‰∫Üpyhanlp‰∏≠test/test_multithread.py Êñá‰ª∂ÔºåÊµãËØïÂ§öÁ∫øÁ®ãÂè•Ê≥ï‰æùÂ≠òÂàÜÊûê„ÄÇÂèëÁé∞ÂΩìÊàë‰ΩøÁî®Ëã±Êñá‰æùÂ≠òÂÖ≥Á≥ªÔºåÂç≥: enableDeprelTranslator(False)Êó∂Ôºå‰ºöÊä•Address boundary errorÈîôËØØ„ÄÇ‰ΩÜÂ¶ÇÊûúÁõ¥Êé•‰ΩøÁî®‰∏≠ÊñáÁöÑ‰æùÂ≠òÂÖ≥Á≥ªÔºåÁ®ãÂ∫èËøêË°åÊ≠£Â∏∏„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# ===============================================================================
#
# Copyright (c) 2017 <> All Rights Reserved
#
#
# File: /Users/hain/ai/pyhanlp/tests/test_multithread.py
# Author: Hai Liang Wang
# Date: 2018-03-23:17:18:30
#
# ===============================================================================
from __future__ import print_function
from __future__ import division

__copyright__ = ""Copyright (c) 2017 . All Rights Reserved""
__author__ = ""Hai Liang Wang""
__date__ = ""2018-03-23:17:18:30""

import os
import sys

curdir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(curdir, os.path.pardir))

if sys.version_info[0] < 3:
    reload(sys)
    sys.setdefaultencoding(""utf-8"")
    # raise ""Must be using Python 3""

import threading
import time
from pyhanlp import HanLP, SafeJClass

# Âú®Á∫øÁ®ã‰ΩìÂ§ñÈÉ®Áî®SafeJClassÁ∫øÁ®ãÂÆâÂÖ®Âú∞ÂºïÂÖ•Á±ªÂêç
parser_class = SafeJClass(""com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser"")


class MyThread(threading.Thread):
    def __init__(self, name, counter, analyzer):
        threading.Thread.__init__(self)
        self.thread_name = name
        self.counter = counter
        self.analyzer = analyzer

    def run(self):
        print(""Starting "" + self.thread_name)
        while self.counter:
            time.sleep(1)
            t = self.analyzer.parse(""‰ªñ‰ª¨ÁôΩÂèëËãçËãç‰ªñ‰ª¨Á≤æÁ•ûÁüçÈìÑ,"")
            for word in t.iterator():  # ÈÄöËøádir()ÂèØ‰ª•Êü•ÁúãsentenceÁöÑÊñπÊ≥ï
                print(""%d: %s --(%s)--> %d: %s (%s, %s)"" % (word.ID, word.LEMMA, word.DEPREL, word.HEAD.ID, word.HEAD.LEMMA, word.POSTAG, word.CPOSTAG))
            self.counter -= 1



def test():
    analyzer = parser_class().enableDeprelTranslator(False)

    thread1 = MyThread(""Thread-1"", 1, analyzer)
    thread2 = MyThread(""Thread-2"", 2, analyzer)

    thread1.start()
    thread2.start()

    print('waiting to finish the thread')

    thread1.join()
    thread2.join()

    print(""Exiting Main Thread"")


if __name__ == '__main__':
#     FLAGS([__file__, '--verbosity', '1'])  # DEBUG 1; INFO 0; WARNING -1
    test()

```
### ÊúüÊúõËæìÂá∫

```
Ëß£ÊûêËøôÂè•ËØù‰∏âÈÅç
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Starting Thread-1
Starting Thread-2
waiting to finish the thread
'python test_threading.py' terminated by signal SIGSEGV (Address boundary error)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂèØ‰ª•ÊääËá™ÂÆö‰πâÁöÑÂàÜËØçÁªìÊûúÁî®‰∫é‰æùÂ≠òÂàÜÊûêÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlpÂÜçËøõË°åÂ∞ÅË£ÖÂÆ¢Êà∑Á´ØÊó∂Êä•ÈîôÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.03
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.02

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
pythonÁ®ãÂ∫èÊ≤°ÊúâÂ∞ÅË£ÖÊàêÂÆ¢Êà∑Á´ØÊó∂ÔºåÂèØ‰ª•Ê≠£Â∏∏Ë∞ÉÁî®ÔºåÂ∞ÅË£ÖÂêéÂ∞±Âá∫Áé∞ÂêÑÁßçË∑ØÂæÑÈóÆÈ¢òÔºåÂ∫îËØ•ÊÄé‰πàËß£ÂÜ≥Ôºü
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫écorpus.dependency.CoNll.CoNLLWordÁ±ª‰∏≠POSTAGÂíåCPOSTAGÁöÑ‰∏Ä‰∫õÊ≠ß‰πâÊÄßÈóÆÈ¢ò„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØ 1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3
pyhanlp 0.1.45

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºåÊàëÊúâ‰∏™ÂÖ≥‰∫éÂè•Ê≥ï‰æùÂ≠òËß£ÊûêÁöÑÈóÆÈ¢ò„ÄÇÁúãÊñáÊ°£ÂèëÁé∞com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord Á±ªÊúâCPOSTAGÂíåPOSTAG‰∏§‰∏™Â±ûÊÄßÔºåÂÖ∂‰∏≠CPOSTAGÊòØÁ≤óÁ≤íÂ∫¶ËØçÊÄßÔºåPOSTAGÊòØÁªÜÁ≤íÂ∫¶ËØçÊÄß„ÄÇÁÑ∂ËÄåÊàëÂú®ÊµãËØïdemo‰∏≠ÈÇ£‰∏™‰æãÂ≠êÁöÑÊó∂ÂÄôÂèëÁé∞‰ºº‰πé‰∏çÂ§™Ê∏ÖÊ•ö„ÄÇ‰æãÂ≠êÊòØÔºö‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏Ëë£‰∫ãÈïøË∞≠Êó≠ÂÖâÂíåÁßò‰π¶ËÉ°Ëä±ËïäÊù•Âà∞ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶ÜÂèÇËßÇ„ÄÇ
ÂÖ∂‰∏≠ Ë∞≠Êó≠ÂÖâ Âíå ËÉ°Ëä±Ëïä ÁöÑPOSTAGÊòØnrÔºå CPOSTAGÊòØnh„ÄÇ Ëøô‰∏§‰∏™Â•ΩÂÉèÁ±ªÂà´Â∑ÆÂà´Â•ΩÂÉèÈùûÂ∏∏Â§ß„ÄÇ
ÊâÄ‰ª•ÊÉ≥ËØ∑ÈóÆÂÖ≥‰∫éËøô‰∏§‰∏™Â±ûÊÄßÁöÑÂÖ∑‰ΩìËß£Èáä„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
Áõ¥Êé•Ë∞ÉÁî®‰ª•‰∏ãËß¶Âèë‰ª£Á†Å

### Ëß¶Âèë‰ª£Á†Å

```
t = HanLP.parseDependency(""‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏Ëë£‰∫ãÈïøË∞≠Êó≠ÂÖâÂíåÁßò‰π¶ËÉ°Ëä±ËïäÊù•Âà∞ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶ÜÂèÇËßÇ"")
for word in t.iterator():  
    print(""%d: %s --(%s)--> %s (%s, %s)"" % (word.ID, word.LEMMA, word.DEPREL, word.HEAD.LEMMA, word.POSTAG, word.CPOSTAG))
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
1: ‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Ëë£‰∫ãÈïø (nt, ni)
2: Ëë£‰∫ãÈïø --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Ë∞≠Êó≠ÂÖâ (n, n)
3: Ë∞≠Êó≠ÂÖâ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> Êù•Âà∞ (nr, nh)
4: Âíå --(Â∑¶ÈôÑÂä†ÂÖ≥Á≥ª)--> ËÉ°Ëä±Ëïä (c, c)
5: Áßò‰π¶ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ËÉ°Ëä±Ëïä (n, n)
6: ËÉ°Ëä±Ëïä --(Âπ∂ÂàóÂÖ≥Á≥ª)--> Ë∞≠Êó≠ÂÖâ (nr, nh)
7: Êù•Âà∞ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ## (v, v)
8: ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶Ü --(Âä®ÂÆæÂÖ≥Á≥ª)--> Êù•Âà∞ (ns, ns)
9: ÂèÇËßÇ --(Âπ∂ÂàóÂÖ≥Á≥ª)--> Êù•Âà∞ (v, v)
```


"
pythonË∞ÉÁî®ËÅöÁ±ªÔºå ÊòæÁ§∫‚Äúpytho Â∑≤ÂÅúÊ≠¢Â∑•‰Ωú‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2
pyhanlp 0.1.45

## ÊàëÁöÑÈóÆÈ¢ò
ÊâßË°åClusterAnalyzer = SafeJClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')Ëøô‰∏ÄÂè•
ÂºπÂá∫ ‚Äúpytho Â∑≤ÂÅúÊ≠¢Â∑•‰Ωú‚ÄùÔºåÊçïÊçâ‰∏ç‰∫ÜÈîôËØØ

## Â§çÁé∞ÈóÆÈ¢ò
Áõ¥Êé•ÊâßË°å pip install pyhanlp==0.1.45 ÂÆâË£ÖÊàêÂäü
ÊâßË°åtests‰∏≠ÁöÑ‰æãÂ≠êÔºötest_multithread.pyÊàêÂäü
ÊâßË°å IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil') Ëøô‰∏ÄÂè•ÊàêÂäü

### Ê≠•È™§

1. python ClusterService.py

### Ëß¶Âèë‰ª£Á†Å

import os,math,time,datetime
from pyhanlp import *
from pyhanlp import SafeJClass

from jpype import JavaException
import pandas as pd
import re,json,uuid

class ClusterService:
    def __init__(self):
        try:           
            print(""1"")
            self.IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil')    
            print(""2"")
            self.ClusterAnalyzer = SafeJClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
            print(""3"")   
        except JavaException as e:
            print('ËÅöÁ±ªÂàùÂßãÂåñÂá∫ÈîôÔºö',e)
        except Exception as  ex:         
            print('ËÅöÁ±ªÂàùÂßãÂåñÂá∫ÈîôÔºö',ex)

 if __name__ == ""__main__"":  
        CS = ClusterService()

### ÊúüÊúõËæìÂá∫

""1""
""2""
""3""
```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

""1""
""2""

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÂºπÂá∫ ‚Äúpytho Â∑≤ÂÅúÊ≠¢Â∑•‰Ωú‚ÄùÔºåÊçïÊçâ‰∏ç‰∫ÜÈîôËØØ

"
‰∏∫‰ªÄ‰πàrepoÈáåÁöÑdataÊñá‰ª∂Â§πÈáåÈù¢Ê≤°ÊúâCharType.bin,"‰∏∫‰ªÄ‰πàrepoÈáåÁöÑdata/dictionary/other/Êñá‰ª∂Â§πÈáåÈù¢Ê≤°ÊúâCharType.bin
ËÄåÔºåÂ∫ìÈúÄË¶Å

java.lang.IllegalArgumentException: Â≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°® /data/dictionary/other/CharType.bin Âä†ËΩΩÂ§±Ë¥•Ôºö java.io.FileNotFoundException:dictionary/other/CharType.bin (No such file or directory)"
ÊúÄÊñ∞ÁöÑjar(1.7.3)ÂåÖÂú®pyhanlp‰∏ãÊúâÂÖºÂÆπÊÄßÈóÆÈ¢ò,"Â∞ÜpyhanlpÂ∫ìÁõÆÂΩï‰∏≠ÁöÑhanlp-1.7.2.jarÊç¢Êàê‰∫Ühanlp-1.7.3.jar,dataÁõÆÂΩï‰πüÊõ¥Êñ∞‰∏∫ÊúÄÊñ∞ÁâàÂêé,ÂèëÁé∞‰πãÂâçË∑ëÁöÑÂ•ΩÂ•ΩÁöÑÁ®ãÂ∫è,Áé∞Âú®Âá∫Áé∞ÂêÑÁßçÁº∫Â§±ÂÖÉÁ¥†ÊñπÊ≥ïÁöÑÂºÇÂ∏∏.
Áª¥ÁâπÊØîÂàÜËØçÂô®Â•Ω‰Ωø,ÊÑüÁü•Âô®ÂàÜËØçÂô®‰∏çÂ•Ω‰Ωø."
ÂÖ≥‰∫éÁÆÄÁπÅËΩ¨Êç¢ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.5

## ÊàëÁöÑÈóÆÈ¢ò

System.out.println(HanLP.convertToSimplifiedChinese(""„Äå‰ª•ÂæåÁ≠âÂ¶≥Áï∂‰∏äÁöáÂêéÔºåÂ∞±ËÉΩË≤∑Â£´Â§öÂï§Ê¢®ÊÖ∂Á•ù‰∫Ü„Äç""));
ËæìÂá∫ Ôºö ‚Äú‰ª•ÂêéÁ≠â‰Ω†ÂΩì‰∏äÁöáÂêéÔºåÂ∞±ËÉΩ‰π∞ËçâËéìÂ∫ÜÁ•ù‰∫Ü‚Äù
System.out.println(HanLP.convertToTraditionalChinese(""‚Äú‰ª•ÂêéÁ≠â‰Ω†ÂΩì‰∏äÁöáÂêéÔºåÂ∞±ËÉΩ‰π∞ËçâËéìÂ∫ÜÁ•ù‰∫Ü‚Äù""));
ËæìÂá∫ Ôºö ‚Äú‰ª•ÂæåÁ≠â‰Ω†Áï∂‰∏äÁöáÂêéÔºåÂ∞±ËÉΩË≤∑ËçâËéìÊÖ∂Á•ù‰∫Ü‚Äù
ÊúüÊúõËÉΩËæìÂá∫ ‚Äú‰ª•ÂæåÁ≠âÂ¶≥Áï∂‰∏äÁöáÂêéÔºåÂ∞±ËÉΩË≤∑Â£´Â§öÂï§Ê¢®ÊÖ∂Á•ù‰∫Ü‚ÄùÔºåÊòØÊúâ‰ªÄ‰πàÂú∞ÊñπÈúÄË¶ÅÈÖçÁΩÆÂêóÔºü

"
Á†ÅÂÜúÂéÇÁöÑÂõæÁâáÊó†Ê≥ïÊòæÁ§∫‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊÑüÁü•Êú∫ÂàÜËØçÊó∂ Á±ª‰ºº‚ÄúÂæàÂ•Ω‚ÄùÔºå‚ÄúÈ´òÁ´Ø‚Äù‰∏ÄÁ±ªÁöÑËØçË¢´ËØÜÂà´Êàê‰∫∫Âêç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÑüÁü•Êú∫ÂàÜËØçÊó∂ Á±ª‰ºº‚ÄúÂæàÂ•Ω‚ÄùÔºå‚ÄúÈ´òÁ´Ø‚ÄùÔºå‚ÄúËæÉÈ´ò‚ÄùÔºå‚ÄúÂçïÈÄâ‚ÄùÁ≠â‰∏ÄÁ±ªÁöÑÂ∏∏Áî®ËØçË¢´ËØÜÂà´Êàê‰∫∫Âêç
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {   
       // SentenceAnalyzer‰∏∫ÁÆÄÂçïÂ∞ÅË£ÖÁöÑÊÑüÁü•Êú∫ÂàÜËØç 
        SentenceAnalyzer sentenceAnalyzer = new SentenceAnalyzer();
        System.out.println(sentenceAnalyzer.analyze(""È´òÁ´Ø""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
È´òÁ´Ø /n
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
È´òÁ´Ø /nr
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
CollectionUtility.sortMapByValueÊñπÊ≥ïÂú®Â≠òÂú®bug,"v1.72ÁâàÊú¨Â≠òÂú®bugÔºåÊúÄÊñ∞Áâàmaster‰∏≠‰æùÁÑ∂Â≠òÂú®
com.hankcs.hanlp.classification.utilities.CollectionUtility‰∏≠
public static <K, V extends Comparable<V>> Map<K, V> sortMapByValue(Map<K, V> input, final boolean desc)ÊñπÊ≥ïÂ≠òÂú®bug
ArrayList<Map.Entry<K, V>> entryList = new ArrayList<Map.Entry<K, V>>(input.size());
‰∏≠ÁöÑinput.size()Â∫îËØ•Êîπ‰∏∫input.entrySet()"
ÈáçÊñ∞Âä†ËΩΩÂÅúÁî®ËØçÔºàÁÉ≠Êõ¥Êñ∞Ôºâ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®ÂèòÊõ¥Ëá™ÂÆö‰πâÂÅúÁî®ËØçÂÖ∏`stopwords.txt`Êñá‰ª∂ÂêéÔºåÂú®‰∏çÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÁöÑÊÉÖÂÜµ‰∏ãÔºåÊó†Ê≥ïËá™Âä®Âä†ËΩΩÊõ¥Êñ∞Ôºå‰∫¶Êó†ÊâãÂä®Êõ¥Êñ∞ÁöÑÊìç‰Ωú„ÄÇ
Âç≥‰ΩøÊâãÂä®Âà†Èô§`stopwords.txt.bin`ÁºìÂ≠òÊñá‰ª∂Ôºå
Âú®‰πãÂêéÁöÑÊìç‰Ωú‰∏≠ÔºàÂ¶ÇÔºöÊèêÂèñÂÖ≥ÈîÆËØç`HanLP.extractKeyword( )`ÔºâÔºå‰πü‰∏ç‰ºöËß¶Âèëstopwords.txtÁöÑÈáçÊñ∞ËΩΩÂÖ•„ÄÇ

‰ªé [issue:1136](https://github.com/hankcs/HanLP/issues/1136) ‰∏≠‰∫ÜËß£Âà∞ Ëá™ÂÆö‰πâËØçÂÖ∏ Êúâ`reload()`ÁöÑÊñπÊ≥ï ÂèØ‰ª•ÊâãÂä® ÈáçÊñ∞ËΩΩÂÖ• Ëá™ÂÆö‰πâËØçÂÖ∏



## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

Ôºà‰∏çÈÄÇÁî®Ê≠§issueÔºâ

### Ëß¶Âèë‰ª£Á†Å

Ôºà‰∏çÈÄÇÁî®Ê≠§issueÔºâ

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

Ë¢´ËøΩÂä†Âà∞`stopwords.txt`Êñá‰ª∂‰∏≠ÁöÑËØçÔºå‰∏çÂÜçÂá∫Áé∞Âú®‚ÄúÊèêÂèñÂÖ≥ÈîÆËØç‚ÄùÁöÑÁªìÊûú‰∏≠

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

‰∏é ‚ÄúÂèòÊõ¥`stopwords.txt`Êñá‰ª∂‚Äù‰πãÂâçÁöÑÁªìÊûú‰∏ÄÊ†∑

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

ÁõÆÂâç‰∏∫‰∫ÜÂÆûÁé∞‚ÄúÊâãÂä®ÈáçÊñ∞ËΩΩÂÖ•stopword‚Äù‰πãÁõÆÁöÑÔºå
Êü•Áúã‰∫Ü`CoreStopWordDictionary`Á±ªÁöÑÈùôÊÄÅ‰ª£Á†ÅÂùóÔºå‰ªøÁÖßÁùÄ

Âú®ÈúÄË¶Å reload ÂÅúÁî®ËØçÂÖ∏ ÁöÑÂú∞ÊñπÔºåÂ¶Ç‰∏ãÊìç‰ΩúÔºö
```
try
{
    // ÂèçÂ∞ÑÔºö
    // 1. Ëé∑ÂèñÂà∞CoreStopWordDictionary‰∏≠ÈùûpublicÁöÑdictionaryÊàêÂëòÂèòÈáè
    // 2. ÂÜçÂèòÊõ¥ÂÖ∂ ÂèØËØ∑ÈóÆÊùÉÈôê
    Field dictionaryField = CoreStopWordDictionary.class.getDeclaredField(""dictionary"");
    dictionaryFiled.setAccessible(true);

    // 3. ÈáçÊñ∞ÂàõÂª∫‰∏Ä‰∏™StopWordDictionaryÂØπË±°ÔºåË∞ÉÁî®ÂÖ∂ save()ÊñπÊ≥ïÔºåÂπ∂ÊõøÊç¢ÊóßÁöÑStopWordDictionaryÂØπË±°
    StopWordDictionary dictionary = new StopWordDictionary(HanLP.Config.CoreStopWordDictionaryPath);
    DataOutputStream out = new DataOutputStream(new BufferedOutputStream(IOUtil.newOutputStream(HanLP.Config.CoreStopWordDictionaryPath + Predefine.BIN_EXT)));
    dictionary.save(out);

    dictionaryFiled.set(dictionaryField, dictionary);
    out.close();
}
catch (Exception e)
{
    // 
}
```

ÊòæÁÑ∂ÔºåËøôÊ†∑ÂÅöÂçÅÂàÜ‰∏ç‰ºòÈõÖ„ÄÇÂ∏åÊúõËÉΩÂú®ÂÆòÊñπÁöÑÁâàÊú¨ËÉΩÂä†ÂÖ•‚ÄúÊâãÂä®ÈáçÊñ∞ËΩΩÂÖ• ÂÅúÁî®ËØçÂÖ∏‚ÄùÁöÑÂäüËÉΩ„ÄÇ

Áõ∏ÂÖ≥ issue: [#28](https://github.com/hankcs/HanLP/issues/28) „ÄÅ[#32](https://github.com/hankcs/HanLP/issues/32)

Âä®ÁîªÊºîÁ§∫Ôºöhttps://i.loli.net/2019/04/23/5cbf2741654a3.gif
ËØ¥ÊòéÔºöÂ¶ÇÂõæÁöÑDemoÔºåÂæ™ÁéØÂú∞ÊèêÂèñ‰∏ÄÊÆµÊñáÊú¨ÁöÑÂÖ≥ÈîÆËØç„ÄÇÔºàÊù•Ê∫êÔºöÁôæÂ∫¶ÁôæÁßëÁöÑgithubËØçÊù°Ôºâ

ÂàùÂßãÔºöÊèêÂèñÂà∞‚Äú‰ª£Á†Å‚Äù‰∏ÄËØçÔºõ

Â∞ÜÂÖ∂Ê∑ªÂä†Âà∞`stopword.txt`‰∏≠ÔºåCtrl+S‰øùÂ≠òÔºåÈáçÂêØÁ®ãÂ∫èÔºåÊú™ÁîüÊïà„ÄÇ
Âà†Èô§`stopword.txt.bin`‰πãÂêéÔºåÈáçÂêØÁ®ãÂ∫èÔºåÂ∑≤ÁîüÊïà„ÄÇÔºàÁªìËÆ∫‚ë†ÔºöÂ¶ÇÂêå`Readme.md`ÊâÄË®ÄÔºå‚ÄúÂ¶ÇÊûú‰Ω†‰øÆÊîπ‰∫Ü‰ªª‰ΩïËØçÂÖ∏ÔºåÂè™ÊúâÂà†Èô§ÁºìÂ≠òÊâçËÉΩÁîüÊïà‚ÄùÔºâ

ÂÜç Ê∑ªÂä†‚ÄúÊ¶ÇÂøµ‚Äù‰∏ÄËØçÔºå‰∏çÈáçÂêØÁ®ãÂ∫èÔºåËæìÂá∫‰ªçÊúâÊ≠§ËØçÔºå
Âà†Èô§‰∫Ü 3‰∏™`*.txt.bin`Êñá‰ª∂ÔºàÊú™ÈáçÂêØÁ®ãÂ∫èÔºâÔºåËæìÂá∫‰ªçÊúâÊ≠§ËØç„ÄÇÔºàÁªìËÆ∫‚ë°Ôºöstopword ‰∏çËÉΩÁÉ≠Êõ¥Êñ∞ Âä†ËΩΩÔºâ

[![StopWord.gif](https://i.loli.net/2019/04/23/5cbf2741654a3.gif)](https://i.loli.net/2019/04/23/5cbf2741654a3.gif)"
ÂçöÂÆ¢ÁöÑÂõæÂÖ®ÊåÇ‰∫Ü,ÂçöÂÆ¢ÁöÑÂõæÈÉΩÊåÇ‰∫ÜÔºåËÉΩ‰øÆÂ§ç‰∏Ä‰∏ãÂêóÔºåÊñáÁ´†ÂÜôÁöÑÊå∫Â•ΩÁöÑ
‰∏∫‰ªÄ‰πàNLPTokenizer.ANALYZER‰∏ãsegmentÂíåanalyzeÁªìÊûú‰∏çÂêåÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å
‰∏çÂ•ΩÊÑèÊÄù ÊàëÊòØÁî®pythonËøêË°åÁöÑ
```
a=JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
sentence=""Èò∂ÊÆµÊ≥®ÈááÊØî1.04""
seg_result1=a.ANALYZER.seg(sentence)
print(seg_result1)
seg_result1=a.ANALYZER.analyze(sentence)
print(seg_result1)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Èò∂ÊÆµ/n, Ê≥®ÈááÊØî/n, 1.04/m]
Èò∂ÊÆµ/n Ê≥®ÈááÊØî/n 1.04/m
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Èò∂ÊÆµ/n, Ê≥®ÈááÊØî/v, 1.04/m]
Èò∂ÊÆµ/n Ê≥®ÈááÊØî/n 1.04/m
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Ê≥®ÈááÊØî n ÊòØËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑ"
Âä®ÊÄÅËá™ÂÆö‰πâËØçÂÖ∏ÊåÅ‰πÖÂåñÈóÆÈ¢ò,"* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

## ÊàëÁöÑÈóÆÈ¢ò

ËØ∑ÈóÆÔºöÊàëÁé∞Âú®‰ΩøÁî®:CustomDictionary.insert(""hualiang"", ""YangL 1024"");

Ëá™ÂÆö‰πâËØçÂÖ∏Ôºå‰ΩÜÊòØÊó†Ê≥ï‚ÄúÊåÅ‰πÖÂåñ‚ÄùÔºåÂΩìÁ®ãÂ∫èÂÖ≥Èó≠‰πãÂêéÊ∑ªÂä†ÁöÑËØç‰æø‰ºöÊ∂àÂ§±Êéâ„ÄÇ

ËØ∑ÈóÆÂêÑ‰ΩçÊúâÊ≤°Êúâ‰ªÄ‰πàÊñπÊ≥ïÔºåÊ∑ªÂä†ËØçÁöÑÂêåÊó∂ÔºåÂ∞Ü ‚ÄúËØç‚Äù Â≠òÂÖ•ËØçÂÖ∏‰∏≠„ÄÇ

‰∏áÂàÜÊÑüË∞¢ÔºÅ



"
ÊÄéÊ†∑ËØÑ‰ª∑ÊñáÊú¨Ëá™Âä®ËÅöÁ±ªÊïàÊûúÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

```
ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
tag_analyzer = ClusterAnalyzer()
tag_hanlp_cluster = []

for index, row in zip(df.loc[:, ""LabelTwoTag""].index.tolist(), df['LabelTwoTag']):
    tag_analyzer.addDocument(index, row.toString()[1:-1])

for beta in TAG_BETA_HANLP:
    tag_hanlp_cluster = tag_analyzer.repeatedBisection(beta)

print(""ËÅöÁ±ªÊï∞Èáè:"", len(tag_hanlp_cluster))
```
ËØ∑ÈóÆÊúâÊé•Âè£ÂèØ‰ª•Êü•ÁúãÁ±ª‰ººÊó†ÁõëÁù£ËÅöÁ±ª‰πãÂêéÁöÑËΩÆÂªìÁ≥ªÊï∞‰πàÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

‰ΩøÁî®pyhanlpËøõË°åÊñáÊú¨Ëá™Âä®ËÅöÁ±ª

### Ëß¶Âèë‰ª£Á†Å

```
ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
tag_analyzer = ClusterAnalyzer()
tag_hanlp_cluster = []

for index, row in zip(df.loc[:, ""LabelTwoTag""].index.tolist(), df['LabelTwoTag']):
    tag_analyzer.addDocument(index, row.toString()[1:-1])

for beta in TAG_BETA_HANLP:
    tag_hanlp_cluster = tag_analyzer.repeatedBisection(beta)

print(""ËÅöÁ±ªÊï∞Èáè:"", len(tag_hanlp_cluster))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
ËÉΩÂ§üÈÄöËøáanalyzerÊèê‰æõËΩÆÂªìÁ≥ªÊï∞Êù•ÁÆÄÂçïËØÑ‰º∞Ëá™Âä®ËÅöÁ±ªÁöÑÊïàÊûú„ÄÇ

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
Ê≤°ËÉΩÊâæÂà∞Êèê‰æõËΩÆÂªìÁ≥ªÊï∞ÁöÑÊé•Âè£
```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Âú®pyhanlpÁöÑgithubÊ≤°Ê≥ïÊèêissueÔºåÊâÄ‰ª•Â∞±Ë∑ëÂà∞ËøôÈáåÊù•Êèê‰∫ÜÔºåÊú¨Ë¥®ÊòØÂÖ≥‰∫éhanlpÁöÑissue„ÄÇ"
ÊÑüÁü•Êú∫ËÆ≠ÁªÉÊ®°ÂûãÊèêÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ËØ∑ÈóÆÂú®ËÆ≠ÁªÉÊÑüÁü•Êú∫ÁöÑÂàÜËØçÂíåËØçÊÄßÊ®°ÂûãÊó∂ÔºåÂ¶ÇÊûúÊÉ≥Ê∑ªÂä†‰∏Ä‰∫õ‰∏ì‰∏öÈ¢ÜÂüüÁöÑËØçÊ±áÔºå‰ΩÜÊòØ‰∏çÊÉ≥Ê∑ªÂä†ËØçÂÖ∏ÁâπÂæÅÔºåÂõ†‰∏∫Áº∫Â∞ëÊ†áÊ≥®ÔºåËÉΩ‰∏çËÉΩÊää‰∏ÄÂ†Ü‰∏ì‰∏öÈ¢ÜÂüüËØçÊ±áÂÅöÊàê‰∫∫Ê∞ëÊó•Êä•È¢ÑÊñôÂ∫ìÁöÑÊ†ºÂºèÂä†ÂÖ•ËÆ≠ÁªÉÔºü
 `<È¢ÜÂüüËØçÊ±á>/<È¢ÜÂüüËØçÊÄß>    <È¢ÜÂüüËØçÊ±á>/<È¢ÜÂüüËØçÊÄß>`
‰∏çÁü•ÈÅìËøôÁßçÁêÜËÆ∫‰∏äÂèØË°å‰∏çÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
HanLP.parseDependency 1024ÈóÆÈ¢ò,"ÊÇ®Â•ΩÔºåsentence = HanLP.parseDependency(doc)‰ºöÊä•Â¶Ç‰∏ãÈîôËØØÔºö
jpype._jexception.ArrayIndexOutOfBoundsExceptionPyRaisable: java.lang.ArrayIndexOutOfBoundsException: Index 1032 out of bounds for length 1024

ËØ∑ÈóÆËÉΩÊúâÂäûÊ≥ïÊâ©Â±ïÈïøÂ∫¶‰πàÔºü"
ÊÑüÁü•Êú∫‰∏çÂêåÊé•Âè£Âá∫Êù•ÁöÑÊ†áÁÇπÁ¨¶Âè∑ËØçÊÄß‰∏ç‰∏ÄÊ†∑,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®Wiki‰∏≠ÁöÑÁ§∫‰æãÔºåÊÑüÁü•Êú∫ËØçÊ≥ïÂàÜÊûêÂô®ÂíåÊÑüÁü•Êú∫ËØçÊÄßÊ†áÊ≥®Ôºå‰∏çÂêåÊé•Âè£ÂØπÂêå‰∏ÄÂè•ËØùÔºåÊ†áÁÇπÁ¨¶Âè∑ËØçÊÄßÊ†áÊ≥®‰∏ç‰∏ÄËá¥ÔºåËØ∑ÈóÆÂú®‰∏ç‰æùËµñËØçÂÖ∏ÁöÑÂâçÊèê‰∏ãÊÄé‰πàÊâçËÉΩÂÅöÂà∞‰∏ÄËá¥Ôºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        // ËøôÈáåÂùáÊòØÈªòËÆ§Ê®°ÂûãÔºåÂàÜËØçÊ®°ÂûãÊòØlargeÁõÆÂΩï‰∏ãÁöÑÔºåposÊ®°ÂûãÊòØ98Âπ¥ÁõÆÂΩï‰∏ãÁöÑ
        PerceptronSegmenter segment = new PerceptronSegmenter();
        PerceptronPOSTagger pos = new PerceptronPOSTagger();
        PerceptronLexicalAnalyzer pla = new PerceptronLexicalAnalyzer();
        String text = ""Ê≠§Â§ñÔºåÊùéÂΩ¶ÂÆè„ÄÅÈôÜÂ•á„ÄÅDuerOSÁöÑÊôØÈ≤≤ÔºåAIÊäÄÊúØÂπ≥Âè∞ÁöÑÁéãÊµ∑Â≥∞ÔºåËøòÈÇÄËØ∑‰∫Ü‰∏äÁôæ‰Ωç‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑ‰∏ìÂÆ∂‰ª•ÂèäÁõ∏ÂÖ≥ÁöÑÂêà‰Ωú‰ºÅ‰∏ö„ÄÇ"";
        List<String> tokens = segment.segment(text);
        String[] tags = pos.tag(tokens);
        for (int i = 0; i < tags.length; ++i) {
            System.out.print(tokens.get(i) + '/' + tags[i] + "" "");
        }
        System.out.println();
        System.out.println(pla.analyze(text));
        System.out.println(pla.seg(text));
```
### ÊúüÊúõËæìÂá∫
ÊúüÊúõÊ†áÁÇπÁ¨¶Âè∑ÁöÑËØçÊÄßËæìÂá∫‰∏ÄËá¥Ôºå‰∏∫ `w`
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Ê†áÁÇπÁ¨¶Âè∑Âùá‰∏∫ w
```

### ÂÆûÈôÖËæìÂá∫
```
Ê≠§Â§ñ/c Ôºå/v ÊùéÂΩ¶ÂÆè/nr „ÄÅ/v ÈôÜÂ•á/nr „ÄÅ/v DuerOS/v ÁöÑ/u ÊôØÈ≤≤/n Ôºå/n AI/vn ÊäÄÊúØ/n Âπ≥Âè∞/n ÁöÑ/u ÁéãÊµ∑Â≥∞/nr Ôºå/n Ëøò/d ÈÇÄËØ∑/v ‰∫Ü/u ‰∏äÁôæ/m ‰Ωç/q ‰∫∫Â∑•Êô∫ËÉΩ/n È¢ÜÂüü/n ÁöÑ/u ‰∏ìÂÆ∂/n ‰ª•Âèä/c Áõ∏ÂÖ≥/v ÁöÑ/u Âêà‰Ωú/vn ‰ºÅ‰∏ö/n „ÄÇ/w 
Ê≠§Â§ñ/c Ôºå/w ÊùéÂΩ¶ÂÆè/nr „ÄÅ/w ÈôÜÂ•á/nr „ÄÅ/w DuerOS/nx ÁöÑ/u ÊôØÈ≤≤/n Ôºå/w AI/nx ÊäÄÊúØ/n Âπ≥Âè∞/n ÁöÑ/u ÁéãÊµ∑Â≥∞/nr Ôºå/w Ëøò/d ÈÇÄËØ∑/v ‰∫Ü/u ‰∏äÁôæ/j ‰Ωç/q ‰∫∫Â∑•Êô∫ËÉΩ/n È¢ÜÂüü/n ÁöÑ/u ‰∏ìÂÆ∂/n ‰ª•Âèä/c Áõ∏ÂÖ≥/v ÁöÑ/u Âêà‰Ωú/vn ‰ºÅ‰∏ö/n „ÄÇ/w
[Ê≠§Â§ñ/c, Ôºå/w, ÊùéÂΩ¶ÂÆè/nr, „ÄÅ/w, ÈôÜÂ•á/nr, „ÄÅ/w, DuerOS/nx, ÁöÑ/u, ÊôØÈ≤≤/n, Ôºå/w, AI/nx, ÊäÄÊúØ/n, Âπ≥Âè∞/n, ÁöÑ/u, ÁéãÊµ∑Â≥∞/nr, Ôºå/w, Ëøò/d, ÈÇÄËØ∑/v, ‰∫Ü/u, ‰∏äÁôæ/m, ‰Ωç/q, ‰∫∫Â∑•Êô∫ËÉΩ/n, È¢ÜÂüü/n, ÁöÑ/u, ‰∏ìÂÆ∂/n, ‰ª•Âèä/c, Áõ∏ÂÖ≥/v, ÁöÑ/u, Âêà‰Ωú/vn, ‰ºÅ‰∏ö/n, „ÄÇ/w]
```
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊïèÊÑü‰ø°ÊÅØËøáÊª§,‰Ω†Â•Ω‰ªÄ‰πàÊó∂ÂÄô‰∏äÁ∫øÊïèÊÑü‰ø°ÊÅØËØÜÂà´ÂäüËÉΩÔºü
[Mac OS 10.14.4]Python3.6 ‰∏ãpip install pyhanlp Â§±Ë¥•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

ÂÆâË£ÖÁéØÂ¢ÉÔºö
Mac OS 10.14.4 
Python 3.6.8 :: Anaconda, Inc.
gcc-8 (Homebrew GCC 8.3.0) 8.3.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

ÂÆâË£ÖÂëΩ‰ª§Ôºö
pip intall pyhanlp 

ÊèêÁ§∫Ôºöerror: command 'gcc' failed with exit status 1

logÂ¶Ç‰∏ãÔºö

[log.txt](https://github.com/hankcs/HanLP/files/3067924/log.txt)

"
ËØ≠Êñôpku199801ÈìæÊé•http://hanlp.linrunsoft.com/release/corpus/pku98.zipÊó†Ê≥ïÊâìÂºÄ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËÉΩÂê¶ËæìÂá∫ÊñáÊ°£ÂêëÈáèÊ®°ÂûãÔºå‰πüÂ∞±ÊòØÂ§öÁª¥Â∫¶ÁöÑÂêëÈáèË°®Á§∫‰∏Ä‰∏™ÊñáÊ°£ÁöÑtxtÊñáÊú¨Ê†ºÂºè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ËÉΩÂê¶ËæìÂá∫Á±ª‰ººËØçÂêëÈáèÊ®°ÂûãÁöÑÊñáÊ°£ÂêëÈáèÊ®°ÂûãÔºå‰πüÂ∞±ÊòØÂ§öÁª¥Â∫¶ÂêëÈáèË°®Á§∫ÊñáÊ°£ÁöÑtxtÊñáÊú¨Ê†ºÂºè

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§


### Ëß¶Âèë‰ª£Á†Å

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->





### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
 ÊÉ≥Áü•ÈÅìopennlpËÆ≠ÁªÉmaxEntÊ®°ÂûãÁöÑËÆ≠ÁªÉÂèÇÊï∞ÊòØ‰ªÄ‰πàÊ†∑Â≠êÁöÑ ÂàùÂ≠¶ËÄÖ Â∏åÊúõ‰∏çÂêùËµêÊïô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïËÆ°ÁÆó‰∏§‰∏™Â≠óÁ¨¶‰∏≤ÊàñÊñáÊú¨ÁöÑÁõ∏‰ººÂ∫¶Ôºü,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2 <br/>
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

## ÊàëÁöÑÈóÆÈ¢ò

Â¶Ç‰ΩïËÆ°ÁÆó‰∏§‰∏™Â≠óÁ¨¶‰∏≤ÊàñÊñáÊú¨ÁöÑÁõ∏‰ººÂ∫¶Ôºü

## Â§çÁé∞ÈóÆÈ¢ò


### Ê≠•È™§



### Ëß¶Âèë‰ª£Á†Å


### ÊúüÊúõËæìÂá∫

ËÆ°ÁÆó‰∏§‰∏™Â≠óÁ¨¶‰∏≤ÊàñÊñáÊú¨ÁöÑÁõ∏‰ººÂ∫¶



### ÂÆûÈôÖËæìÂá∫



## ÂÖ∂‰ªñ‰ø°ÊÅØ



"
Ê±ÇÊé®ËçêHanLP for NodeJS ËøòÂú®Ê¥ªË∑ÉÁöÑÁâàÊú¨,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Â¶ÇÈ¢òÔºåÊ±ÇÂ§ßÁ•ûÊé®ËçêÔºåË∞¢Ë∞¢"
1.7.2 ÁâàÊú¨‰∏≠ CustomDictionary.insert ÂØπ NLPTokenizer Êó†ÊïàÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

„ÄÄ„ÄÄÈùûÂ∏∏ÊÑüË∞¢Ëøô‰∏™È°πÁõÆÔºåÂØπNLPÁöÑÁêÜËß£Ê∑±ÂÖ•‰∫ÜÂæàÂ§öÔºåÊàë‰πãÂâçÁî®ÁöÑÁâàÊú¨ÊòØ1.6.4ÔºåÂü∫Êú¨Âè™Áî®‰∫ÜÊàëËÆ§‰∏∫ÊúÄÊ†∏ÂøÉÁ®≥ÂÆöÁöÑ‰ª•ÁªüËÆ°Ê®°ÂûãÔºàStandardTokenizer„ÄåViterbiSegment„ÄçÔºâ‰∏∫‰∏ªÔºåËßÑÂàôÔºàCustomDictionaryÔºâ‰∏∫ËæÖÁöÑ‰∏≠ÊñáÂàÜËØçÊúçÂä°„ÄÇ
„ÄÄ„ÄÄËøôÊ†∑ËôΩÁÑ∂È´òÊïà„ÄÅÁ®≥ÂÆöÔºå‰ΩÜÈöèÁùÄÈÅáÂà∞ÈóÆÈ¢òÂ§çÊùÇÊÄßÁöÑÊèêÈ´òÔºåÊàëÈÄêÊ∏êÂèëÁé∞Â∑≤‰∏çËÉΩÊª°Ë∂≥Êàënlp‰ªªÂä°‰∏ä‰∏Ä‰∫õÈúÄÊ±ÇÔºåÊØîÂ¶ÇÂØπÊú™ insert Âà∞ CustomDictionary ÁöÑ„ÄåÊú∫ÊûÑÂêç„Äç„ÄÅ„Äå‰∫∫Âêç„ÄçËØÜÂà´ËæÉÂ∑ÆÔºåÂÜçÊØîÂ¶ÇÊ≤°ËÉΩÂ•ΩÂ•ΩÂà©Áî®Âà∞ parseDependency„ÄÇ
„ÄÄ„ÄÄÂú®ÁúãÂà∞HanLP ÂÖ¨ÂºÄ‰∫Ü[Âú®Á∫øÊºîÁ§∫](http://hanlp.com)ÁöÑ1‰∫øÁ∫ßËØ≠ÊñôËÆ≠ÁªÉÁöÑÂàÜËØçÊ®°ÂûãÂêéÔºåÈùûÂ∏∏ÂÖ¥Â•ãÊÉ≥Ë¶ÅÂ•ΩÂ•ΩÁöÑÂà©Áî®Ëµ∑Êù•„ÄÇËôΩÁÑ∂ÊàëÂ∑≤ÁªèÁúãËøáÂ•ΩÂá†ÈÅçÈ¶ñÈ°µÂèäwiki„ÄÅFAQ„ÄÅ‰ª•ÂèäÁõ∏ÂÖ≥ issuesÔºå‰ΩÜÂèØËÉΩÁî±‰∫éÂü∫Á°ÄËæÉÂ∑ÆÔºåÂØπ HanLP ÁöÑËØ∏Â§ö„ÄåÁâπÊÄß„ÄçÁêÜËß£‰∏çÊ∑±Ôºå‰∏çÁü•ÈÅìËØ•Â¶Ç‰ΩïÁî®Â•ΩÔºåÊÄªÊÑüËßâÂêÑ‰∏™ÂäüËÉΩÈó¥ÊÄªÊòØ‚ÄúÈ±º‰∏éÁÜäÊéå‰∏çÂèØÂÖºÂæó‚Äù„ÄÇ
„ÄÄ„ÄÄ‰∫éÊòØÊÉ≥Âú®ËøôÈáåÈõÜ‰∏≠Êï¥ÁêÜ‰∏Ä‰∏ãÊàëÁöÑÈóÆÈ¢òÔºö

1. ÁâàÊú¨1.7.2 ‰∏é ÁâàÊú¨1.6.4ÔºåÂÆÉ‰ª¨ÁöÑÈªòËÆ§ HanLP.segmentÔºàStandardTokenizerÔºâ ÊïàÊûúÊòØ‰∏ÄÊ†∑ÁöÑÂêóÔºüÂÆÉ‰ª¨ÈÉΩÊòØÂü∫‰∫é98Âπ¥‰∫∫Ê∞ëÊó•Êä•Ê†áÊ≥®ËØ≠ÊñôÁöÑÁªüËÆ°Ê®°ÂûãÂàÜËØçÂêóÔºü
2. HanLP.segment ÂàÜËØçÁªìÊûú‰∏≠ÁöÑ‚ÄúËØçÊÄß‚ÄùÔºàÂåÖÊã¨‰∫∫ÂêçÂíåÂú∞ÂêçÔºâÊòØÊÄé‰πàÊù•ÔºüÈÉΩÊòØÊ†πÊçÆ CoreDictionary Âíå CustomDictionary ‰∏≠Á°ÆÂÆöÊù•ÁöÑÂêóÔºüÊâÄ‰ª•ËøôÈáåÂπ∂‰∏çÊ∂âÂèäÂà∞ HMM„ÄÅCRF ÂØπÂêóÔºüÊâÄ‰ª•ËôΩÁÑ∂‰∏Ä‰∏™ËØçÂèØËÉΩ‰ºöÊúâÂ§ö‰∏™ËØçÊÄßÔºå‰ΩÜÂú® HanLP.segment ÁªìÊûú‰∏≠ÊØè‰∏™ËØç‰∏ÄÂÆöÊòØ‰∏Ä‰∏™Âõ∫ÂÆöÁöÑËØçÊÄßÔºü
3. ÁâàÊú¨1.7.2 Âà©Áî®‰∫øÁ∫ßËØ≠ÊñôËÆ≠ÁªÉÁöÑÂàÜËØçÊ®°ÂûãÂè™ÊòØÂ∫îÁî®Âà∞‰∫Ü NLPTokenizer ‰∏äÂêóÔºüparseDependency ‰πüÊòØÂü∫‰∫éËøô‰∏™ËØ≠ÊñôÂêóÔºüËøòÊòØËØ¥ parseDependency ÊòØÂü∫‰∫é NLPTokenizer ÁöÑÁªìÊûúÔºü
4. ‚ÄúËá™ÂÆö‰πâËØçÂÖ∏Âú®ÊâÄÊúâÂàÜËØçÂô®‰∏≠ÈÉΩÊúâÊïà‚ÄùÔºå‰ΩÜÊàëÂèëÁé∞ 1.7.2 ÁâàÊú¨‰∏≠Âà©Áî® CustomDictionary.insert ÂêéÂπ∂Êú™ÁîüÊïàÔºå‰ΩÜ‰øÆÊîπ `dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt `ÂêéÁ°ÆÂÆû‰ºöÁîüÊïàÔºåÁõ∏ÂÖ≥‰ª£Á†ÅÂú®‰∏ãÈù¢„ÄÇ
5. Áõ∏ÂêåÁöÑÂè•Â≠êÂú®[Âú®Á∫øÊºîÁ§∫](http://hanlp.com/?sentence=ÊîØÊè¥Ëá∫ÁÅ£Ê≠£È´îÈ¶ôÊ∏ØÁπÅÈ´îÔºöÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥3Êúà1Êó•Áî±ÊØîÁàæ¬∑ËìãËå≤Âíå‰øùÁæÖ¬∑ËâæÂÄ´ÂâµÁ´ã)‰∏ä‰∏é1.7.2ÁâàÊú¨ÁöÑ NLPTokenizer ÁªìÊûú‰∏çÂêåÔºåÁõ∏ÂÖ≥‰ª£Á†ÅÂú®‰∏ãÈù¢„ÄÇ



## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        # ÈóÆÈ¢ò 4
        NLPTokenizer.ANALYZER.enableCustomDictionary(true);
        CustomDictionary.insert(""ÈíüÊ≠£"", ""nr 1"");
        CustomDictionary.insert(""Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂"", ""ntc 1"");
        System.out.println(HanLP.segment(""Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂Ëë£‰∫ãÈïø„ÄÅÈ¶ñÂ∏≠ÁªèÊµéÂ≠¶ÂÆ∂ÈíüÊ≠£ÁîüË°®Á§∫""));
        System.out.println(NLPTokenizer.analyze(""Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂Ëë£‰∫ãÈïø„ÄÅÈ¶ñÂ∏≠ÁªèÊµéÂ≠¶ÂÆ∂ÈíüÊ≠£ÁîüË°®Á§∫""));

        # ÈóÆÈ¢ò 5
        NLPTokenizer.ANALYZER.enableCustomDictionary(false);
        // Ê≥®ÊÑèËßÇÂØü‰∏ãÈù¢‰∏§‰∏™‚ÄúÂ∏åÊúõ‚ÄùÁöÑËØçÊÄß„ÄÅ‰∏§‰∏™‚ÄúÊôöÈúû‚ÄùÁöÑËØçÊÄß
        System.out.println(NLPTokenizer.analyze(""ÊîØÊè¥Ëá∫ÁÅ£Ê≠£È´îÈ¶ôÊ∏ØÁπÅÈ´îÔºöÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥3Êúà1Êó•Áî±ÊØîÁàæ¬∑ËìãËå≤Âíå‰øùÁæÖ¬∑ËâæÂÄ´ÂâµÁ´ã„ÄÇ""));

    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
# ÈóÆÈ¢ò 4
# Ê≠§ÈÉ®ÂàÜÊàëÁêÜËß£ ‚ÄúÈíü/n, Ê≠£Áîü/v‚Äù ÁöÑÂàáÂàÜÊñπÂºè
[Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂/ntc, Ëë£‰∫ãÈïø/nnt, „ÄÅ/w, È¶ñÂ∏≠/n, ÁªèÊµéÂ≠¶ÂÆ∂/nnt, Èíü/n, Ê≠£Áîü/v, Ë°®Á§∫/v]
# Ê≠§ÈÉ®ÂàÜÂ∫îËØ•ÊòØ‚ÄúË¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂/ntc‚Äù Âíå ‚Äú[Èíü/n, Ê≠£Áîü/v]/nr‚Äù
Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂/ntc Ëë£‰∫ãÈïø/nnt „ÄÅ/w È¶ñÂ∏≠/n ÁªèÊµéÂ≠¶ÂÆ∂/n [Èíü/n, Ê≠£Áîü/v]/nr Ë°®Á§∫/v

# ÈóÆÈ¢ò 5
# ‚ÄúÊØîÁàæ¬∑ËìãËå≤‚Äù Â∫îËØ•ÊòØ nr
ÊîØÊè¥/v [Ëá∫ÁÅ£/ns Ê≠£È´î/n È¶ôÊ∏Ø/ns ÁπÅÈ´î/n]/nt Ôºö/w [ÂæÆËΩØ/nt ÂÖ¨Âè∏/n]/nt Êñº/p 1975Âπ¥/t 3Êúà/t 1Êó•/t Áî±/p ÊØîÁàæ¬∑ËìãËå≤/nr Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/nr ÂâµÁ´ã/v „ÄÇ/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
# ÈóÆÈ¢ò 4
# Ê≠§ÈÉ®ÂàÜÊ≤°ÈóÆÈ¢ò
[Ë¥¢Êñ∞Êô∫Â∫ìËé´Â∞ºÂ°îÁ†îÁ©∂/ntc, Ëë£‰∫ãÈïø/nnt, „ÄÅ/w, È¶ñÂ∏≠/n, ÁªèÊµéÂ≠¶ÂÆ∂/nnt, Èíü/n, Ê≠£Áîü/v, Ë°®Á§∫/v]
# Ê≠§ÈÉ®ÂàÜ CustomDictionary.insert ÂêéÂØπ NLPTokenizer Êú™ÁîüÊïàÔºå ËÄå‰∏î‰∏∫‰ªÄ‰πà‚ÄúËë£‰∫ãÈïø‚ÄùËØçÊÄßÊòØ n ËÄå‰∏çÊòØ nntÔºüËÄå‰∏î‰∏∫‰ªÄ‰πà‰∏çÊòØ‚Äú[Èíü/n, Ê≠£Áîü/v]/nr‚Äù
[Ë¥¢Êñ∞/j Êô∫Â∫ì/n Ëé´Â∞ºÂ°î/n Á†îÁ©∂/vn Ëë£‰∫ãÈïø/n]/nt „ÄÅ/w È¶ñÂ∏≠/n ÁªèÊµéÂ≠¶ÂÆ∂/n ÈíüÊ≠£Áîü/nr Ë°®Á§∫/v

# ÈóÆÈ¢ò 5
# Ê≥®ÊÑè ‚ÄúÊØîÁàæ¬∑ËìãËå≤/nz‚Äù
ÊîØÊè¥/v [Ëá∫ÁÅ£/ns Ê≠£È´î/n È¶ôÊ∏Ø/ns ÁπÅÈ´î/n]/nt Ôºö/w [ÂæÆËΩØ/nt ÂÖ¨Âè∏/n]/nt Êñº/p 1975Âπ¥/t 3Êúà/t 1Êó•/t Áî±/p ÊØîÁàæ¬∑ËìãËå≤/nz Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/nr ÂâµÁ´ã/v „ÄÇ/w

ËÄåÂú®Á∫øÊºîÁ§∫‰∏≠ÊòØÊ≠£Á°ÆÁöÑ nr
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Áî®Êà∑Ëá™ÂÆö‰πâËØçÂú®NLPTokenizer.segment‰∏≠‰∏çËµ∑‰ΩúÁî®,"from pyhanlp import *

def add_dictionary():
CustomDictionary = JClass(""com.hankcs.hanlp.dictionary.CustomDictionary"")
CustomDictionary.add(""ÊîªÂüéÁãÆ"")

def keyword_extract():
NLPTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"") # NLPÊ†áÊ≥®
print(NLPTokenizer.segment(""ÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´ÁãóÔºåËøéÂ®∂ÁôΩÂØåÁæéÔºåËµ∞‰∏ä‰∫∫ÁîüÂ∑ÖÂ≥∞""))

if name == ""main"":
add_dictionary()
keyword_extract()

ÁªìÊûúÔºö[ÊîªÂüé/ns, ÁãÆ/Ng, ÈÄÜË¢≠/v, ÂçïË∫´/n, Áãó/n, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/nr]

Âú®pythonÁâàÊú¨Èáå‰∏çËµ∑‰ΩúÁî®Âë¢"
pyhanlpËá™Âä®ÂÆâË£ÖÂ§±Ë¥•ÔºåÊâãÂä®‰∏ãËΩΩreleaseÂíådataÊñá‰ª∂ÂêéÔºåÊâãÂä®ÈÖçÁΩÆ‰ªçÁÑ∂Áº∫Â∞ëjarÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->
## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!--pyhanlpËá™Âä®ÂÆâË£ÖÂ§±Ë¥•ÔºåÊâãÂä®‰∏ãËΩΩreleaseÂíådataÊñá‰ª∂ÂêéÔºåÊâãÂä®ÈÖçÁΩÆ‰ªçÁÑ∂Áº∫Â∞ëjarÊñá‰ª∂ÔºåÁªôÁöÑ‰∏ãËΩΩÈìæÊé•Â§±ÊïàÔºåÊ±ÇÂä©ÔºåÊÑüË∞¢ -->

Downloading https://github.com/hankcs/HanLP/releases/download/v1.7.2/hanlp-1.7.2
-release.zip to E:\Anaconda3\lib\site-packages\pyhanlp\static\hanlp-1.7.2-releas
e.zip
Failed to download https://github.com/hankcs/HanLP/releases/download/v1.7.2/hanl
p-1.7.2-release.zip
Please refer to https://github.com/hankcs/pyhanlp for manually installation.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 122, in <modul
e>
    _start_jvm_for_hanlp()
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\__init__.py"", line 39, in _start_
jvm_for_hanlp
    from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_
DATA_PATH
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\static\__init__.py"", line 309, in
 <module>
    install_hanlp_jar()
  File ""E:\Anaconda3\lib\site-packages\pyhanlp\static\__init__.py"", line 200, in
 install_hanlp_jar
    with zipfile.ZipFile(jar_zip, ""r"") as archive:
  File ""E:\Anaconda3\lib\zipfile.py"", line 1090, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'E:\\Anaconda3\\lib\\sit
e-packages\\pyhanlp\\static\\hanlp-1.7.2-release.zip'"
"java mavenÈ°πÁõÆ,Â∑≤Â∞Ühanlp.propertiesÊîæÂÖ•resources‰∏ã,dataÂ∫îÊîæÂì™?hanlp.properties‰∏≠ÁöÑrootÂ∫îËØ•ÊÄé‰πàÂÜô,Â•ΩÂÉèËØï‰∫ÜÂêÑÁßçÈÉΩ‰∏çË°å ","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.72

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

JavaWeb mavenÈ°πÁõÆ, ÊÉ≥ÊäädataÂåÖÊîæÂú®È°πÁõÆÈáå‰ΩøÁî®.
ÊàëÁöÑÊìç‰Ωú: Êäähanlp.propertiesÂíådataÈÉΩÊîæÂú®resources‰∏ã,
hanlp.properties‰∏≠ÁöÑÈÖçÁΩÆroot=./    (Â∞±ÊòØÂΩìÂâçÁõÆÂΩï)
Êó†Ê≥ïËØªÂèñÂà∞Ê†∏ÂøÉËØçÂÖ∏.
ÊâÄ‰ª•ÊÉ≥ÈóÆhanlp.propertiesÂíådataÂú®maven‰∏≠Â∫îÊîæÂì™, hanlp.properties‰∏≠ÁöÑrootÂ¶ÇÊûúÈÖçÁΩÆ

"
CRFÂàÜËØçÂ§çÁî®ÊÑüÁü•Êú∫Áª¥ÁâπÊØîÁöÑÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊÇ®Â•ΩÔºåÈòÖËØª‰Ω†ÁöÑÊÑüÁü•Êú∫ËØçÊ≥ïÂàÜÊûêÁöÑÊó∂ÂÄôÔºåÁúãÂà∞ÊÇ®ËØ¥`Êñ∞ÁâàÊù°‰ª∂ÈöèÊú∫Âú∫Â§çÁî®‰∫ÜÊÑüÁü•Êú∫ÁöÑÁª¥ÁâπÊØîÁÆóÊ≥ïÔºåÊâÄ‰ª•ÈÄüÂ∫¶ÊåÅÂπ≥`ÔºåÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÊù°‰ª∂ÈöèÊú∫Âú∫Âú®È¢ÑÊµãÁöÑÊó∂ÂÄô‰πüÊòØÁî®ÁöÑÁª¥ÁâπÊØîÔºåËøô‰∏™ÂíåÊÑüÁü•Êú∫ÁöÑÁª¥ÁâπÊØîÊúâ‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑ÁöÑÂêóÔºüÂõ†‰∏∫ÂØπÊù°‰ª∂ÈöèÊú∫Âú∫‰∏çÊòØÁâπÂà´‰∫ÜËß£Ôºå‰ª£Á†ÅËøòÊ≤°ÈòÖËØªÂÆåÔºåÊÉ≥ËØ∑ÊÇ®ÁªôËØ¶ÁªÜËØ¥ËØ¥ÔºåÁÑ∂ÂêéÂØπÁÖß‰ª£Á†ÅÁúã

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
reloadÁÉ≠Êõ¥Êñ∞ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÈúÄË¶ÅÂÆûÁé∞ËΩΩÂÖ•ÂÜÖÂ≠òÂêéÂ≠óÂÖ∏ÁöÑÊõ¥Êñ∞
ÊÉ≥ËææÂà∞ÊïàÊûúÂ¶Ç‰øÆÊîπtxtËØçÂÖ∏Â¢ûÂä†ËØçËØ≠ÂêéÔºåjarËÉΩÈáçÊñ∞ËΩΩÂÖ•
ÂèëÁé∞Êèê‰æõ‰∫ÜCustomDictionary.reloadÔºåÊïÖ‰ΩøÁî®Ê≠§apiËøáÁ®ã‰∏≠‰∫ßÁîü‰∫Ü‰∏éÈ¢ÑÊúü‰∏ç‰∏ÄËá¥ÁöÑÂàÜËØçÊÉÖÂÜµ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂú®dictionary‰∏ãÊñ∞Â¢ûËá™ÂÆö‰πâËØçÂÖ∏menu.txt,ÂÖ∂‰∏≠ÊúâËØçÁõíÈ•≠ Êó•ÊñôÔºåËøêË°åÂ¶Ç‰∏ã‰ª£Á†Å
2. ÁÑ∂ÂêéÂú®Á∫øÁ®ãsleepÊó∂ÂæÄËØçÂÖ∏‰∏≠Êñ∞Â¢û ÁúüÂ•ΩÂêÉ

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
         String text = ""ÁõíÈ•≠Êó•ÊñôÁúüÂ•ΩÂêÉ""Ôºõ
         System.out.println(StandardTokenizer.segment(text ));
          Thread.sleep(60000); //Ê≠§Êó∂Âú®Êñ∞Â¢ûÁöÑËØçÂÖ∏menu.txt‰∏≠Âä†ÂÖ• <ÁúüÂ•ΩÂêÉ>
         CustomDictionary.reload();
        System.out.println(StandardTokenizer.segment(text));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

ÊúüÊúõÁ¨¨‰∏ÄÊ¨°ËæìÂá∫Ôºö<ÁõíÈ•≠> <Êó•Êñô> <Áúü> <Â•ΩÂêÉ>
ÊúüÊúõÁ¨¨‰∫åÊ¨°ËæìÂá∫Ôºö<ÁõíÈ•≠> <Êó•Êñô> <ÁúüÂ•ΩÂêÉ>
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

Á¨¨‰∏ÄÊ¨°ËæìÂá∫Ôºö<ÁõíÈ•≠> <Êó•Êñô> <Áúü> <Â•ΩÂêÉ>
Á¨¨‰∫åÊ¨°ËæìÂá∫Ôºö<ÁõíÈ•≠> <Êó•Êñô> <Áúü> <Â•ΩÂêÉ>

reloadÂπ∂Ê≤°ÊúâÊõ¥Êñ∞ÂêÑ‰∏™ÂàÜËØçÂô®‰∏≠datÂéüÊúâÊï∞ÊçÆÂÜÖÂÆπ Ôºå‰ªÖÊõ¥Êñ∞‰∫Ü CustomDictionary‰∏≠Ëá™ÊúâÁöÑdat
```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰æùÂ≠òÂÖ≥Á≥ªÊãÜÂàÜ‰∏éÂÆòÁΩë‰∏ç‰∏ÄËá¥,"Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
È¶ñÈ°µÊñáÊ°£
wiki
Â∏∏ËßÅÈóÆÈ¢ò
ÊàëÂ∑≤ÁªèÈÄöËøáGoogleÂíåissueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
 ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ
**ÁâàÊú¨Âè∑**
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

**ÊàëÁöÑÈóÆÈ¢ò**
‰æùÂ≠òÂÖ≥Á≥ªÊãÜÂàÜÊúÄÊñ∞Áâà‰∏éÂÆòÁΩëÈ¢ÑËßà‰∏ç‰∏ÄËá¥

Â§çÁé∞ÈóÆÈ¢ò
Ê≠•È™§
Ëß¶Âèë‰ª£Á†Å
   String  chineseSentence = ‚Äú‰Ω†Áà∏Âè´‰ªÄ‰πà?‚Äù;
   CoNLLSentence sentence = HanLP.parseDependency(chineseSentence);
        System.out.println(sentence);
        List<WordRelate> wordRelateList = new ArrayList<WordRelate>();
        // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
            WordRelate wordRelate = new WordRelate();
            wordRelate.setCont(word.LEMMA);
            wordRelate.setId(word.ID);
            wordRelate.setParent(word.HEAD.ID);
            wordRelate.setRelate(RelateEnum.getEnumByKey(word.DEPREL).getRelateTag());
            wordRelate.setPos(word.POSTAG);
            wordRelateList.add(wordRelate);
        }

ÊúüÊúõËæìÂá∫
1	‰Ω†	‰Ω†	r	r	_	2	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
2	Áà∏	Áà∏	n	n	_	0	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
3	Âè´	Âè´	v	v	_	2	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
4	‰ªÄ‰πà	‰ªÄ‰πà	r	r	_	3	Âä®ÂÆæÂÖ≥Á≥ª	_	_
5	?	?	wp	w	_	3	Ê†áÁÇπÁ¨¶Âè∑	_	_

make
ÂÆûÈôÖËæìÂá∫

1	‰Ω†	‰Ω†	r	r	_	2	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	Áà∏	Áà∏	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	Âè´	Âè´	v	v	_	2	Âπ∂ÂàóÂÖ≥Á≥ª	_	_
4	‰ªÄ‰πà	‰ªÄ‰πà	r	r	_	3	Âä®ÂÆæÂÖ≥Á≥ª	_	_
5	?	?	wp	w	_	3	Ê†áÁÇπÁ¨¶Âè∑	_	_"
Êñ∞ËØçÊèêÂèñËã±ÊñáÊãÜËØç‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®Êñ∞ËØçÊèêÂèñ‰∏≠ÔºåËã±ÊñáÂçïËØçË¢´ÊãÜÂàÜ‰∫Ü


## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§


### Ëß¶Âèë‰ª£Á†Å

```
     BufferedReader bufferedReader = new BufferedReader(new FileReader(fileName));

                List<WordInfo> keywordList = HanLP.extractWords(bufferedReader, 100,true);


                for (WordInfo info : keywordList) {

                    System.out.println(info.text+""_""+info.frequency+""_""+info.entropy+""_""+info.aggregation);

                    insertData(info.text,info.frequency);

                }

```
### ÊúüÊúõËæìÂá∫

```
make
```

### ÂÆûÈôÖËæìÂá∫

```
ma
mak
make
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![image](https://user-images.githubusercontent.com/31303643/54976835-7f5e6880-4fd6-11e9-9295-c772918cc7a0.png)

"
Â¶Ç‰ΩïÂú®ÂàÜËØçÁöÑÊó∂ÂÄôÂ¢ûÂä†ÁâπÊÆäÁ¨¶Âè∑,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Â¶Ç‰ΩïÂú®ÂàÜËØçÁöÑÊó∂ÂÄôÂ¢ûÂä†ÁâπÊÆäÁ¨¶Âè∑

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[<‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ>/n, ÁöÑ/u, ÂÆó/n, ÊàêÂ∫Ü/nr, ÊïôÊéà/n, Ê≠£Âú®/d, ÊïôÊéà/n, Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ/v, ËØæÁ®ã/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[</v, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ/n, >/v, ÁöÑ/u, ÂÆó/n, ÊàêÂ∫Ü/nr, ÊïôÊéà/n, Ê≠£Âú®/d, ÊïôÊéà/n, Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ/v, ËØæÁ®ã/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïÂú®ÂàÜËØçÁöÑÊó∂ÂÄôÂ¢ûÂä†ÁâπÊÆäÁ¨¶Âè∑,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò 
Â¶Ç‰ΩïÂú®ÂàÜËØçÁöÑÊó∂ÂÄôÂ¢ûÂä†ÁâπÊÆäÁ¨¶Âè∑

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
  NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
print(NLPTokenizer.segment('<‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ>ÁöÑÂÆóÊàêÂ∫ÜÊïôÊéàÊ≠£Âú®ÊïôÊéàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã'))
```
### ÊúüÊúõËæìÂá∫

[<‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ>/n, ÁöÑ/u, ÂÆó/n, ÊàêÂ∫Ü/nr, ÊïôÊéà/n, Ê≠£Âú®/d, ÊïôÊéà/n, Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ/v, ËØæÁ®ã/n]

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫
[</v, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ/n, >/v, ÁöÑ/u, ÂÆó/n, ÊàêÂ∫Ü/nr, ÊïôÊéà/n, Ê≠£Âú®/d, ÊïôÊéà/n, Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ/v, ËØæÁ®ã/n]

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËÆ≠ÁªÉÊÑüÁü•Êú∫Ê®°ÂûãÊó∂ÊÄé‰πàÂä†ÂÖ•ËØçÂÖ∏Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºåÊàëÈòÖËØªWiki‰∏≠ÊÑüÁü•Êú∫ËØçÊ≥ïÂàÜÊûê‰∏≠ÔºåÁúãÂà∞ÊÇ®ÊèèËø∞ÁöÑÔºöËÆ≠ÁªÉÊó∂`Âè™‰ΩøÁî®‰∫Ü7ÁßçÁä∂ÊÄÅÁâπÂæÅÔºåÊú™‰ΩøÁî®ËØçÂÖ∏`ÔºåÊàëÊÉ≥Áü•ÈÅìÂú®ËÆ≠ÁªÉÊó∂ÂèØ‰ª•Âä†ÂÖ•ÂàÜËØçËØçÂÖ∏ÂêóÔºüËØ•ÊÄé‰πàÂä†ÂÖ•ÔºüÂú®ÂÅöÈ¢ÑÊµãÊó∂‰πüÂèØ‰ª•ËÄÉËôëËØçÂÖ∏ÁöÑ‰ΩúÁî®ÂêóÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
/**
     * ËÆ≠ÁªÉ
     *
     * @param trainingFile  ËÆ≠ÁªÉÈõÜ
     * @param developFile   ÂºÄÂèëÈõÜ
     * @param modelFile     Ê®°Âûã‰øùÂ≠òË∑ØÂæÑ
     * @param compressRatio ÂéãÁº©ÊØî
     * @param maxIteration  ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞
     * @param threadNum     Á∫øÁ®ãÊï∞
     * @return ‰∏Ä‰∏™ÂåÖÂê´Ê®°ÂûãÂíåÁ≤æÂ∫¶ÁöÑÁªìÊûÑ
     * @throws IOException
     */
    public Result train(String trainingFile, String developFile,
                        String modelFile, final double compressRatio,
                        final int maxIteration, final int threadNum) throws IOException
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
WordVectorModel APIÂä†ËΩΩÊ®°ÂûãÂá∫Áé∞Á©∫ÊåáÈíàÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®
```
java  -cp ~/.m2/repository/com/hankcs/hanlp/portable-1.7.2/hanlp-portable-1.7.2.jar: com.hankcs.hanlp.mining.word2vec.Train  -input ./data/hanlp-wiki-vec-zh/hanlp-wiki-vec-zh.txt -output data/hanlp-wiki-vec-zh.model
```
ËÆ≠ÁªÉÂæóÂà∞Ê®°ÂûãÊñá‰ª∂‰πãÂêéÔºå‰ΩøÁî®

```
DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(MODEL_FILE_NAME));
```
Âä†ËΩΩÊä•ÈîôÂ¶Ç‰∏ãÔºö
```
java.lang.NullPointerException
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:40)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
```

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(MODEL_FILE_NAME));
float sim = docVectorModel.similarity(doc1,doc2);
...

```
### ÊúüÊúõËæìÂá∫

Ê≠£Â∏∏ËÆ°ÁÆóÂæóÂà∞sim

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

Âá∫Áé∞Á©∫ÊåáÈíàÈîôËØØ„ÄÇ

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.NegativeArraySizeException,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®windows 10‰∏ãËøêË°åÁõ∏ÂêåÁöÑ‰ª£Á†ÅÔºåÊ≤°ÊúâÈóÆÈ¢òÔºåÂ∞ÜÈ°πÁõÆÈÉ®ÁΩ≤Âú®ubuntu18.04‰∏äÊó∂ÔºåÂàô‰ºöÂá∫Áé∞BUG„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    Stream<Term> ab= HanLP.segment(‚ÄúTEST‚Äù).stream();
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
Êó†Êä•Èîô
### ÂÆûÈôÖËæìÂá∫
2019-03-21 22:46:39 WARN [HanLP]:368 - ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.NegativeArraySizeException
        at com.hankcs.hanlp.dictionary.CoreDictionary$Attribute.<init>(CoreDictionary.java:232)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:356)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:320)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:70)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.<init>(ViterbiSegment.java:47)
        at com.hankcs.hanlp.HanLP.newSegment(HanLP.java:648)
        at com.hankcs.hanlp.tokenizer.StandardTokenizer.<clinit>(StandardTokenizer.java:31)
        at com.hankcs.hanlp.HanLP.segment(HanLP.java:636)


2019-03-21 22:46:39 WARN [HanLP]:161 - ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.ArrayIndexOutOfBoundsException: 239
2019-03-21 22:46:40 WARN [HanLP]:204 - Â∞ùËØïËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂/var/lib/tomcat8/webapps/survey-assessment/data/dictionary/CoreNatureDictionary.ngram.mini.txt.table.binÂèëÁîüÂºÇÂ∏∏[java.io.StreamCorruptedException: invalid stream header: EFBFBDEF]Ôºå‰∏ãÈù¢Â∞ÜËΩΩÂÖ•Ê∫êÊñá‰ª∂Âπ∂Ëá™Âä®ÁºìÂ≠ò‚Ä¶‚Ä¶
2019-03-21 22:46:41 DEBUG [org.springframework.web.multipart.commons.CommonsMultipartResolver]:282 - Cleaning up multipart file [file] with original filename [src_1.jpg], stored at [/var/lib/tomcat8/work/Catalina/localhost/survey-assessment/upload_01ae546b_0ad4_4dea_b1b7_fde6177867f5_00000000.tmp]
2019-03-21 22:46:41 DEBUG [org.springframework.web.servlet.DispatcherServlet]:984 - Could not complete request
org.springframework.web.util.NestedServletException: Handler processing failed; nested exception is java.lang.ExceptionInInitializerError

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
python Ë∞ÉÁî®ÔºàLazyLoadingJClassÔºâ ÊñáÊ°£ÂêëÈáèDocVectorModelÊä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp  0.1.45
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp  0.1.45


<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®JClassÁöÑÁ∫øÁ®ãÂÆâÂÖ®ÁâàSafeJClassÊàñÊÉ∞ÊÄßÂä†ËΩΩLazyLoadingJClassÔºåÂä†ËΩΩÂÆåËØçÂêëÈáèÂêéÔºåÁªßÁª≠Âä†ËΩΩÊñáÊ°£ÂêëÈáèÊä•Èîô„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÔºö
WordVectorModel =LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
word2vec = WordVectorModel(save_model_path)
2. ÁÑ∂ÂêéÔºö
DocVectorModel = LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
doc2vec = DocVectorModel(word2vec)
3. Êé•ÁùÄÔºö
ËøêË°å‰πãÂêéÂ∞±Êä•ÈîôNo matching overloads found for [init in find. at native\common\jp_method.cpp:127

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
WordVectorModel =  LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel =  LazyLoadingJClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
word2vec = WordVectorModel(save_model_path)
doc2vec = DocVectorModel(word2vec)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Ê≠£Â∏∏ËøêË°å
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
RuntimeError: No matching overloads found for [init in find. at native\common\jp_method.cpp:127
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
datÂàÜËØçÂ§ßÈÉ®ÂàÜÂ∏¶‚Äú‰∏ç‚ÄùÂ≠óÁöÑÈÉΩÂàÜËØçÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
@hankcs ‰ΩøÁî®ÂèåÊï∞ÁªÑtrieÊ†ëÂàÜËØçÊó∂ÔºåÂèëÁé∞‰∏Ä‰∏™ÊØîËæÉÂΩ±Âìç‰∏öÂä°ÁªìÊûúÁöÑÈóÆÈ¢òÔºö
Â¶ÇÊûúÂæÖÂàÜËØçËØ≠Âè•‰∏≠Â∏¶Êúâ‚Äú‰∏ç‚Ä¶‚Ä¶‚ÄùÁöÑÊ†ºÂºèÔºå‚Äú‚Ä¶‚Ä¶‚ÄùË°®Á§∫ÂΩ¢ÂÆπËØçÊàñÂä®ËØçÔºåÂ§ßÈÉ®ÂàÜËØ≠Âè•‰ºöÂàÜËØçÈîôËØØÔºåÂ¶Ç‚Äú‰∏çÊàêÂäü‚Äù„ÄÅ‚Äú‰∏çÂ§±Ë¥•‚Äù„ÄÅ‚Äú‰∏çÂ•ΩÁúã‚ÄùÁ≠âÁ≠âÔºå‰ºöÂàÜËØç‰∏∫‚Äú‰∏çÊàê‚Äù+‚ÄúÂäü‚Äù„ÄÅ‚Äú‰∏çÂ§±‚Äù+‚ÄúË¥•‚ÄùÁ≠âÁ≠âÔºåÂú®CoreNatureDictionary.txtËØçÂÖ∏ÂíåÁî®Êà∑ËØçÂÖ∏CustomDictionary.txtÂèäÁé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txtÁ≠âÁ≠âËØçÂÖ∏‰∏≠Âà†Èô§Áõ∏ÂÖ≥ËØçÂ¶Ç‚Äú‰∏çÂ§±‚Äù„ÄÅ‚Äú‰∏çÊàê‚ÄùÊâçËÉΩÂàÜËØçÊàêÂäü„ÄÇ
ËøôÈáåÁöÑdatÂàÜËØçÂô®Ôºå‰ºº‰πéÂπ∂Ê≤°ÊúâÁî®Âà∞CoreNatureDictionary.ngram.txtÂèängram.miniÁ≠âÊ®°ÂûãÔºàÂíåÂÆÉ‰ª¨ÁºìÂ≠òbinÊñá‰ª∂ÔºâÔºåÂà©Áî®[#384](https://github.com/hankcs/HanLP/issues/384)ÁöÑÊñπÂºè‰øÆÊ≠£Êó†Êïà„ÄÇ
‰ΩøÁî®datÂàÜËØçÂô®ÁöÑËØùÔºåÊúâ‰ªÄ‰πàÊñπÂºèËÉΩ‰øÆÂ§çËøôÁ±ªÈóÆÈ¢òÂêóÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        Segment segment = HanLP.newSegment(""dat"");
        List<Term> termList1 = segment .seg(""ËÄÅÊòØÊòæÁ§∫‰∏çÊ≠£Á°Æ"");
        List<Term> termList2 = segment .seg(""‰∏úË•ø‰∏çÂ•ΩÁúãÔºåÂàÜËØç‰∏çÂ§±Ë¥•ÊòØÂ§ßÊ¶ÇÁéá‰∫ã‰ª∂"");
        //ÈÅçÂéÜlistËæìÂá∫word
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ËÄÅÊòØ    ÊòæÁ§∫    ‰∏ç    Ê≠£Á°Æ
‰∏úË•ø    ‰∏ç    Â•ΩÁúã   Ôºå    ÂàÜËØç    ‰∏ç    Â§±Ë¥•     ÊòØ    Â§ß   Ê¶ÇÁéá    ‰∫ã‰ª∂
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ËÄÅÊòØ    ÊòæÁ§∫    ‰∏çÊ≠£    Á°Æ
‰∏úË•ø    ‰∏çÂ•Ω    Áúã   ÂàÜËØç    ‰∏çÂ§±    Ë¥•     ÊòØ    Â§ßÊ¶Ç    Áéá    ‰∫ã‰ª∂
```
ÔºàËøôÈáåÁöÑ‚ÄúÂ§ßÊ¶ÇÁéá‚ÄùÔºåÂàÜËØçÊàê‰∫Ü‚ÄúÂ§ßÊ¶Ç    Áéá‚Äù‰πüÈîôËØØ‰∫ÜÔºâ
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
‰ΩøÁî®viterbiÂíåcrfÂàÜËØçÁªìÊûúÊòØÊ≠£Á°ÆÁöÑÔºå‰ΩÜÊòØ‰∏∫‰ªÄ‰πàÈÄâÁî®datÊûÅÈÄüÂàÜËØçÔºåÂõ†‰∏∫ÊàëÂèëÁé∞Âú®ÂÅöÁõ∏‰ººÂ∫¶ÂåπÈÖçÊó∂Ôºå‰ΩøÁî®datÂàÜËØçÁÑ∂Âêé‰ΩøÁî®ÊñáÊ°£ÂêëÈáèÁõ∏‰ººÂåπÈÖçÔºåÊïàÊûúÊòØÊúÄÂ•ΩÁöÑÔºåÁåúÊµãÂéüÂõ†ÂèØËÉΩÊòØÂ§ßÂ§öÊï∞word2vecËØçÂêëÈáèÊ®°ÂûãÁöÑÂàÜËØçÈááÁî®ÁöÑÊòØËøôÁßçÊûÅÈÄüÂàÜËØçÁöÑÊñπÂºè„ÄÇ
"
Áõ¥Êé•Âú®githun‰∏ä‰∏ãËΩΩhanlp-1.7.2-release.zip ÔºåÂà∞200kÂ∑¶Âè≥ÂêéÂ∞±Â§±Ë¥•ÔºåÈúÄË¶ÅÂ∏ÆÂä©Ë∞¢Ë∞¢„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Where I can find the CI service?,"Is this project using continuous integration services (e.g., Travis-CI or Jenkings)? Or has it used at any point of its lifetime?"
ÂÖ≥‰∫édata-for-1.7.2.zip‰∏ãËΩΩÊÖ¢ÁöÑÈóÆÈ¢òÔºåÊèê‰æõ‰∏Ä‰∏™ÁßÅ‰∫∫‰∫ëÁõòÈìæÊé•‰æõÂ§ßÂÆ∂‰∏ãËΩΩ,https://drive.google.com/open?id=1NEN5KoPkgw0o_hpENfUyo0sitZtiIORN
ËÄÅÁôΩÂπ≤ÈÖíÊ±âÂ≠óËΩ¨ÊãºÈü≥ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÈíàÂØπ‰∏≠Êñá""ËÄÅÁôΩÂπ≤ÈÖí""ÁöÑÊãºÈü≥ËΩ¨Êç¢ÈóÆÈ¢òÔºåÊ≠£Á°ÆËØªÈü≥Â∫îËØ•‰∏∫‚Äúlaobaiganjiu‚Äù""lbgj""Ôºå‰ΩøÁî®HanLP.convertToPinyinList()ÊñπÊ≥ïÂæóÂà∞ÁöÑÊòØ‚Äúlaobaiqianjiu‚Äù„ÄÇ
ËôΩÁÑ∂ÊàëÈÄöËøáÂú®ÊãºÈü≥Â≠óÂÖ∏‰∏≠Ê∑ªÂä†ËØçÈ°πÁöÑÊñπÂºèÂèØ‰ª•Ëß£ÂÜ≥ÔºåÊÉ≥Áü•ÈÅìËøô‰∏™ÈóÆÈ¢ò‰∫ßÁîüÁöÑÂéüÂõ†Ôºü

Ê≠§Â§ñËøòÁ¢∞Âà∞‰∫Ü‚ÄúÈòø=a1,e1‚ÄùÔºåÈúÄË¶ÅËé∑ÂèñÊüê‰∏™Â§öÈü≥Â≠óÁöÑÊâÄÊúâËØªÈü≥ËØ•Ë∞ÉÁî®‰ªÄ‰πàÊñπÊ≥ïÂÆûÁé∞Ôºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        String text = ""ËÄÅÁôΩÂπ≤ÈÖí"";
        List<Pinyin> pinyinList = HanLP.convertToPinyinList(text);
        System.out.println(pinyinList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫ laobaiganjiu
```

### ÂÆûÈôÖËæìÂá∫ 

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ laobaiqianjiu  Âπ≤Â≠óÂçïÁã¨Ê≠£Á°ÆÔºå‰ΩÜÂú®ËÄÅÁôΩÂπ≤ÈÖí‰∏≠Ë¢´ÂΩìÂÅö‰∫ÜÂçÉ
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êú™Êù•ÊúâÂä†ÂÖ•ERNIEÊ®°ÂûãÁöÑÁöÑËÆ°ÂàíÂêó,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
 

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
 ÊúÄËøëÁúãÂà∞BERT„ÄÅGPT2.0Á≠âÊ®°ÂûãÔºåËÄå‰∏îÁôæÂ∫¶ËøôÂá†Â§©ÂèëÂ∏É‰∫ÜERNIEÔºåÊçÆËØ¥ÊïàÊûúÊØîBERTËøòÂ•ΩÔºå‰∏çÁü•ÈÅìHanLPÊòØÂê¶ÊúâÂä†ÂÖ•ERNIEÁöÑËÆ°ÂàíÂë¢Ôºü
https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE
 https://www.jiqizhixin.com/articles/2019-03-16-3?from=synced&keyword=ERNIE
   
"
spark‰ΩøÁî®hanlpÔºåÊèê‰æõ‰∏ÄÁßçÁÆÄÂçïÁöÑÊñπÊ°à,"spark‰ΩøÁî®hanlpÔºåÊèê‰æõ‰∏ÄÁßçÁÆÄÂçïÁöÑÊñπÊ°à„ÄÇÊàëÂè™Áî®ÂàÜËØçÂíåËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÂäüËÉΩÔºåÊêúÁ¥¢‰∫ÜÂÖ∂‰ªñÁ±ª‰ººÁöÑÈóÆÈ¢òÂíåÁ≠îÊ°àÊÑüËßâÂØπÊàëÊù•ËØ¥Â§™È∫ªÁÉ¶‰∫ÜÔºåÊèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊñπÊ°à„ÄÇ
‰∏ÄÂÆöÊòØÂú®ÂêÑÁßçPartitionsÂáΩÊï∞ÈáåÁî®ÔºåÊàëÊòØmapPartitionsÔºåËá™ÂÆö‰πâËØçÂÖ∏ÔºåÊàëÈááÁî®‰∏ãÈù¢ÁöÑ‰ª£Á†Å
     def addStopWord(){
      CustomDictionary.dat.synchronized{
         brdicStopDic.value.foreach(x=> {
          CoreStopWordDictionary.add(x)})
      }
    }
ÁÆÄÂçïÂ∞±ÊòØÊ∑ªÂä†ÂÅúÁî®ËØçÁöÑÂáΩÊï∞ÈáåÂä†ÈîÅÔºåÂõ†‰∏∫Â§ßÊï∞ÊçÆÁéØÂ¢É‰∏ãËøêË°åÔºåËØçÂÖ∏Ê∑ªÂä†Â§öÊ¨°Â∞±Â§öÊ¨°Ê≤°ÊúâÂÖ≥Á≥ªÔºå‰∏çÂú®‰πéËøôÁÇπÊÄßËÉΩÔºåÂ¶ÇÊûú‰∏çÂä†ÈîÅ‰ºöÊä•ArrayIndexOutOfBoundsExceptionÂºÇÂ∏∏ÔºåÂéüÂõ†ÂæàÁÆÄÂçïÔºåÂπ∂ÂèëË∞ÉÁî®„ÄÇ‰∏äÈù¢ÁöÑÂáΩÊï∞ÊòØ‰∏™‰æãÂ≠êÔºåÊâæ‰∫Ü‰∏™ÈùôÊÄÅÂØπË±° CustomDictionary.datÂä†ÈîÅÔºåÂè™Ë¶ÅËÉΩ‰øùËØÅÁ∫øÁ®ãÂÆâÂÖ®ÔºåÁî®ÂÖ∂‰ªñÂØπË±°‰πü‰∏ÄÊ†∑„ÄÇ"
ÂÖ≥ÈîÆËØçÊèêÂèñÊó∂Âèñ‰∏çÂêå‰∏™Êï∞ÂÖ≥ÈîÆÂ≠óÊâÄÂæóÂà∞ÁöÑÁªìÊûú‰∏çÂêåÔºü,"```
String content = ""„ÄêÂèå11ÁãÇÊ¨¢‰ª∑„ÄëDHS/Á∫¢ÂèåÂñú‰πí‰πìÁêÉÊãç‰∫îÊòüÁ∫ßÂÖ®ËÉΩÂûã5Êòü‰πí‰πìÁêÉÊàêÂìÅÊãçÂçïÊãçÊóóËà∞Â∫óÂÆòÁΩë"";
for (int i = 1; i <= 10; i++) {
    System.out.println(HanLP.extractKeyword(content, i));
}
```
### ÁªìÊûú
```
[ÂÖ®ËÉΩÂûã]
[‰πí‰πìÁêÉ, ÂÖ®ËÉΩÂûã]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ‰πí‰πìÁêÉÊãç]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ÊàêÂìÅ, ‰πí‰πìÁêÉÊãç]
[‰πí‰πìÁêÉ, ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉÊãç, ÊàêÂìÅ, ÂçïÊãç]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ÊàêÂìÅ, ‰πí‰πìÁêÉÊãç, Á∫¢ÂèåÂñú, ÂçïÊãç]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ‰πí‰πìÁêÉÊãç, ÊàêÂìÅ, Á∫¢ÂèåÂñú, ÂçïÊãç, ÊóóËà∞Â∫ó]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ÊàêÂìÅ, ‰πí‰πìÁêÉÊãç, Á∫¢ÂèåÂñú, ÂçïÊãç, DHS, ÊóóËà∞Â∫ó]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ÊàêÂìÅ, ‰πí‰πìÁêÉÊãç, Á∫¢ÂèåÂñú, ÂçïÊãç, ÊóóËà∞Â∫ó, DHS, ÂÆòÁΩë]
[ÂÖ®ËÉΩÂûã, ‰πí‰πìÁêÉ, ÊàêÂìÅ, ‰πí‰πìÁêÉÊãç, Á∫¢ÂèåÂñú, ÂçïÊãç, ÊóóËà∞Â∫ó, DHS, ÁãÇÊ¨¢, ÂÆòÁΩë]
```
-----

Ëøô‰ºöÂØºËá¥ÊàëÊÉ≥ÂèñÂá∫ÂîØ‰∏ÄÁöÑÂÖ≥ÈîÆÂ≠óÊó∂Âá∫ÈîôÔºåÂá∫Áé∞ËøôÁßçÊÉÖÂÜµÊòØÂõ†‰∏∫ËæìÂÖ•Âè•Â≠êÁöÑÈóÆÈ¢ò‰πàÔºüÔºü"
‰ΩøÁî®NLPTokenizerÊä•Ôºödata\dictionary\other\CharType.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ),"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2.

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## data\dictionary\other\CharType.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

##Âè™ÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑdemoÔºåÁõ¥Êé•Ë∞ÉÁî®NLPTonizer.segment(""ÊàëÁöÑÊñáÊ°£ÂÜÖÂÆπÔºåÁØáÂπÖÂ§ßÊ¶ÇÊúâ300Â≠ó""Ôºâ
ÊâßË°åÊä•Èîô
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
System.out.println(NLPTokenizer.segment(""ÈÉ®ÂàÜÊñáÊ°£ÔºöÊà™Ê≠¢ÁõÆÂâçÔºåÁî≥ÈÄö„ÄÅ‰∏≠ÈÄö„ÄÅÂúÜÈÄö„ÄÅÊ±áÈÄöÔºàÁé∞Âú®Âè´Áôæ‰∏ñÔºâÈòøÈáåÈÉΩÂèÇ‰∫ÜËÇ°ÔºåËøòÂú®ËøòÂâ©ÈüµËææÔºå‰∏çËøáÊàë‰º∞ËÆ°Âπ¥ÂÜÖ‰πü‰ºöÂÖ•ËÇ°„ÄÇ""
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ËØªÂèñdata/dictionary/other/CharType.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: data\dictionary\other\CharType.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊÑüÁü•Êú∫ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÁªìÊûú‰∏çÊ≠£Á°Æ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.7.2

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÁî®‰∏ãÈù¢ÁöÑÂè•Â≠ê
```
Ëëõ‰ºò,Êù®Êµ¶Âå∫ÂõΩÂíåË∑Ø76544Âè∑74202ÂÆ§,7b01444445@126.com
```
ÁöÑËØÜÂà´ÁªìÊûú‰∏≠,Êù®Êµ¶Âå∫ÂõΩË¢´ËØÜÂà´‰∏∫‰∫∫Âêç,ÊÉ≥ËØ∑ÈóÆËØ•Â¶Ç‰Ωï‰øÆÊ≠£ÁªìÊûú


## Â§çÁé∞ÈóÆÈ¢ò


### Ê≠•È™§

Âéªhanlp.comÁöÑÂÆòÁΩëÊµãËØï

### Ëß¶Âèë‰ª£Á†Å


### ÊúüÊúõËæìÂá∫


```
Ëëõ‰ºò/nr ,/w Êù®Êµ¶Âå∫/ns ÂõΩÂíåË∑Ø/ns .......
```

### ÂÆûÈôÖËæìÂá∫


```
Ëëõ‰ºò/nr ,/w Êù®Êµ¶Âå∫ÂõΩ/nr Âíå/c  Ë∑Ø/n.......
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

![image](https://user-images.githubusercontent.com/10104473/54171691-3bd40c80-44b6-11e9-9b5f-dc8e85aa8135.png)

"
KBeamArcEagerDependencyParserËæìÂá∫ÁöÑËØçÁöÑDEPRELÂÄºÔºåÂ¶Ç rcmodÔºånsubjËøô‰∫õÂ¶Ç‰ΩïÁêÜËß£,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.2

ËØ∑ÈóÆÔºå ÊúÄÊñ∞Êèê‰æõÁöÑKBeamArcEagerDependencyParserËæìÂá∫ÁöÑËØçÁöÑDEPRELÂÄºÔºåÂ¶Ç rcmodÔºånsubjËøô‰∫õÂ¶Ç‰ΩïÁêÜËß£ÔºåÂì™ÈáåÊúâÊñáÊ°£‰πà

"
ÈóÆ‰∏ãÔºåÊää‰∏ÄÊÆµËØùÂàÜÊàêÂá†‰∏™Âè•Â≠êÂ∫îËØ•Áî®Âì™‰∏™Á±ªÔºüÔºüÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöÊú™‰ΩøÁî®Ëøá

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÉ≥Êää‰∏ÄÂ§ßÊÆµËØùÊñ≠Âè•ÔºåÂàÜÈöîÊàêÂá†‰∏™Â∞èÂè•Â≠êÔºåÂ∏åÊúõËÉΩÁªìÂêà‰∏Ä‰∫õÁÆóÊ≥ï‰πãÁ±ªÁöÑÔºå‰∏çË¶ÅÁ∫ØÂè•Âè∑Â§ÑÂàÜÈöî‰πãÁ±ªÁöÑ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```java
    // Êó†
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```text
‰∏Ä‰∏™Êï∞ÁªÑÂåÖÂê´ÂêÑ‰∏™Â∞èÂàÜÂè•
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïByteArray.createByteArrayËΩΩÂÖ•jarÂåÖÂÜÖÈÉ®ÁöÑbinÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

Êàë‰ΩøÁî®ByteArray.createByteArrayËΩΩÂÖ•jarÂåÖÂÜÖÈÉ®ÁöÑbinÊñá‰ª∂Ôºå‰ΩøÁî®‰∫ÜAhoCorasickDoubleArrayTrieËøô‰∏™ÁªìÊûÑ
ÊÉ≥ÊääËøô‰∏™ËØçÂÖ∏binÊîæÂà∞jarÂåÖÈáåÈù¢Ôºå‰ΩÜÊòØÁõ¥Êé•‰º†ÂÖ•Ë∑ØÂæÑÊä•ÈîôÔºåÊàëÁöÑËØçÂÖ∏Ë∑ØÂæÑÊòØÔºö
com/data/core/element/searchname_element.csv
```
    /**
     * ËΩΩÂÖ•ËØçÂÖ∏
     */
    public static boolean loadDat(String path, AhoCorasickDoubleArrayTrie<String> trie) {
        ByteArray byteArray = ByteArray.createByteArray(path + Predefine.BIN_EXT);
        if (byteArray == null) return false;
        int size = byteArray.nextInt();
        String[] valueArray = new String[size];
        for (int i = 0; i < valueArray.length; ++i)
        {
            valueArray[i] = byteArray.nextString();
        }
        trie.load(byteArray, valueArray);
        return true;
    }
```
Ë∞ÉÁî®ÊñπÊ≥ïÊòØÔºö
```
        String actPath=dictPath.substring(0, dictPath.lastIndexOf('.'));//ÂéªÊéâÊñá‰ª∂Êâ©Â±ïÂêç
        AhoCorasickDoubleArrayTrie<String> act =new AhoCorasickDoubleArrayTrie<String>();
        try{
            loadDat(actPath, act);//Â∞ùËØïËΩΩÂÖ•
        }catch (Exception ex){
            ex.printStackTrace();
        }
```

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

Â§çÁé∞ÈóÆÈ¢òÔºåÊääËØ•‰ª£Á†ÅÊîæÂà∞mainÂáΩÊï∞‰∏ãÂç≥ÂèØÊâßË°å

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
```
ÊàëÊÉ≥ËØ•‰ª£Á†ÅÂèØ‰ª•Ê≠£Â∏∏ËΩΩÂÖ•Ëá™Â∑±‰∫ßÁîüÁöÑÁºìÂ≠òbinËØçÂÖ∏ÔºåÈÄöËøáloadDatËΩΩÂÖ•
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÂÆûÈôÖÊä•Âá∫ÂºÇÂ∏∏ÔºåÊòØÂõ†‰∏∫Âè™ËÉΩËΩΩÂÖ•Â§ñÈÉ®Ë∑ØÂæÑÂêó?‰∏çÊîØÊåÅjarÂåÖÈáåÈù¢ÁöÑË∑ØÂæÑÔºü
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçÂÖ∏reloadÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ë∞ÉÁî® CustomDictionary.reload() Êé•Âè£Ôºå‰ºöÊä• java.lang.ArrayIndexOutOfBoundsException ÂºÇÂ∏∏

## Â§çÁé∞ÈóÆÈ¢ò
Ëøô‰∏™ÈóÆÈ¢òÂ∫îËØ•ÊòØÊàëÊ∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏Ôºå‰øÆÊîπCustomDictionaryPathÂêé‰ºöÂá∫Áé∞
![image](https://user-images.githubusercontent.com/32827610/53787524-20b74900-3f5a-11e9-913d-9e339ac922a1.png)
ÂºÇÂ∏∏ÊòØ reload() Êó∂ÔºåËµ∞Âà∞Ëøô‰∏ÄÊ≠•Êä•ÁöÑ allocSize Â§ß‰∫é base ÁöÑÂ§ßÂ∞è
![image](https://user-images.githubusercontent.com/32827610/53787581-53614180-3f5a-11e9-8515-2f6f5cce80ac.png)

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
ÂàõÂª∫‰∏Ä‰∏™ËØçÂÖ∏ÁÑ∂Âêé‰øÆÊîπCustomDictionaryPathÔºåÊñπÂºèÔºö
![image](https://user-images.githubusercontent.com/32827610/53787524-20b74900-3f5a-11e9-913d-9e339ac922a1.png)
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ËΩΩÊàêÂäüÔºåË∞ÉÁî® CustomDictionary.reload() ÁÉ≠Êõ¥Êñ∞
3. Êé•ÁùÄ‚Ä¶‚Ä¶
ÂèëÁîüÂºÇÂ∏∏
![image](https://user-images.githubusercontent.com/32827610/53787820-fdd96480-3f5a-11e9-8aa5-6f027ebbcb10.png)

### Ëß¶Âèë‰ª£Á†Å

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
ÊòØÂê¶ÊòØÊàëËá™ÂÆö‰πâËØçÂÖ∏Ë∑ØÂæÑË¶ÜÁõñÈîôËØØÔºü 
Â¶ÇÊûúÊàëÂ¶Ç‰∏ãÂ§ÑÁêÜÔºåÊ≤°ÊúâÈîôËØØ‰ΩÜÊòØËá™ÂÆö‰πâÁöÑËØçÂÖ∏Âπ∂‰∏çÁîüÊïàÔºõÊàëÁöÑÂàÜËØçÂô®ËÆæÁΩÆ‰∫ÜenableCustomDictionaryForcing(true)

HanLP.Config.CustomDictionaryPath = new String[]{pathÔºå""zeus-rest-crawler/zeus-crawler-core/src/main/data/dictionary/customize/customDictionary.txt""};
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçÂ∫ìÊó†ÊïàÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/24597473/53779150-12593500-3f3a-11e9-8e69-f8983733e830.png)

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) {
    {
       // Âä®ÊÄÅÂ¢ûÂä†
        CustomDictionary.add(""ÊîªÂüéÁãÆ"");
        CustomDictionary.add(""Â®∂ÁôΩÂØåÁæé"");
        // Âº∫Ë°åÊèíÂÖ•
        CustomDictionary.insert(""Â®∂ÁôΩÂØåÁæé"", ""nz 1024"");
        // Âà†Èô§ËØçËØ≠ÔºàÊ≥®ÈáäÊéâËØïËØïÔºâ
        CustomDictionary.remove(""ËøéÂ®∂"");
        System.out.println(CustomDictionary.add(""ÂçïË∫´Áãó"", ""nz 1024 n 1""));
        System.out.println(CustomDictionary.get(""ÂçïË∫´Áãó""));

        String text = ""ÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´ÁãóÔºåËøéÂ®∂ÁôΩÂØåÁæéÔºåËµ∞‰∏ä‰∫∫ÁîüÂ∑ÖÂ≥∞"";  // ÊÄé‰πàÂèØËÉΩÂôóÂìàÂìàÔºÅ

        // AhoCorasickDoubleArrayTrieËá™Âä®Êú∫Êâ´ÊèèÊñáÊú¨‰∏≠Âá∫Áé∞ÁöÑËá™ÂÆö‰πâËØçËØ≠
        final char[] charArray = text.toCharArray();
        CustomDictionary.parseText(charArray, new 
 AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>()
        {
            @Override
            public void hit(int begin, int end, CoreDictionary.Attribute value)
            {
                System.out.printf(""[%d:%d]=%s %s\n"", begin, end, new String(charArray, begin, end - begin), value);
            }
        });

        // Ëá™ÂÆö‰πâËØçÂÖ∏Âú®ÊâÄÊúâÂàÜËØçÂô®‰∏≠ÈÉΩÊúâÊïà
        System.out.println(HanLP.segment(text));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫  
true
nz 1024 n 1 
[0:3]=ÊîªÂüéÁãÆ nz 1 
[5:8]=ÂçïË∫´Áãó nz 1024 n 1 
[10:14]=Â®∂ÁôΩÂØåÁæé nz 1024 
[11:14]=ÁôΩÂØåÁæé nr 33 
[ÊîªÂüéÁãÆ/nz, ÈÄÜË¢≠/nz, ÂçïË∫´Áãó/nz, Ôºå/w, Ëøé/v, Â®∂ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫  
true
nz 1024 n 1 
[0:3]=ÊîªÂüéÁãÆ nz 1 
[5:8]=ÂçïË∫´Áãó nz 1024 n 1 
[10:14]=Â®∂ÁôΩÂØåÁæé nz 1024 
[11:14]=ÁôΩÂØåÁæé nr 33 
[ÊîªÂüéÁãÆ/nz, ÈÄÜË¢≠/nz, ÂçïË∫´Áãó/nz, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊñáÊú¨ÂàÜÁ±ª‰∏çÊîØÊåÅÂ¢ûÈáèÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.72
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.72

‰Ω†Â•Ω‰ΩúËÄÖÔºåÂêÑ‰ΩçÊúãÂèã
ÊØèÊ¨°Êñ∞Â¢ûÊñáÊú¨ÂàÜÁ±ªÊó∂ÈÉΩË¶ÅÈáçÊñ∞ËÆ≠ÁªÉÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ
ÊàëËØïËøáÂÖàÂä†ËΩΩÊ®°ÂûãÂú®ËÆ≠ÁªÉÔºå‰ΩÜËøô‰∏çË°åÔºåËøòÊòØË¶ÜÁõñ‰∫ÜÂéüÊúâÁöÑÊ®°Âûã„ÄÇ"
Âú®CRFNERecgonizer‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâNERTagsÂäüËÉΩ,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Âú®com.hankcs.hanlp.model.crf.CRFNERecognizer‰∏≠, construct‰∏Ä‰∏™Á©∫Ê®°ÂûãÁöÑÊó∂ÂÄô‰ºöËá™Âä®Ê∑ªÂä†‰∏â‰∏™ÈªòËÆ§label, nerTagSet‰Ωú‰∏∫‰∏Ä‰∏™private attribute, Âπ∂Ê≤°ÊúâÂæàÂ•ΩÁöÑÊîØÊåÅËá™ËÆ≠ÁªÉnerÊó∂Ê∑ªÂä†Êñ∞ÁöÑnerTagÁöÑÈúÄÊ±Ç. ‰øÆÊîπÂêé, Âú®‰øùÁïôÂéüÊúâÂäüËÉΩÁöÑÊÉÖÂÜµ‰∏ã, ÂèØ‰ª•Áõ¥Êé•ÈÄöËøánew CRFNERecognizer(null, customTagSet)


## Áõ∏ÂÖ≥issue
ÊöÇÊó†"
ÊÑüÁü•Êú∫Ê†áÁ≠æÂàÜÊï∞ËÆ°ÁÆóÈÉ®ÂàÜÊèêÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊúÄËøëÂú®ÈòÖËØª‰∏ª‰ΩìËØÜÂà´ÊÑüÁü•Êú∫ÈÉ®ÂàÜÁöÑÊ∫êÁ†Å„ÄÇÂú®com.hankcs.hanlp.model.perceptron.model.LinearModel.class Á±ªÂÜÖÁöÑÊ†áÁ≠æÂàÜÊï∞ËÆ°ÁÆóÈÉ®ÂàÜÊúâÂ¶Ç‰∏ãÂÖ¨Âºè„ÄÇ
```
index = index * this.featureMap.tagSet.size() + currentTag;
score += (double)this.parameter[index];
```
ËÄå‰∏çÊòØÁõ¥Êé•Â∞ÜÁâπÂæÅ‰πò‰ª•ÊùÉÈáçËé∑ÂæóÁõ∏Â∫îÂæóÂàÜÔºåËØ∑ÈóÆËøôÊòØÂá∫‰∫é‰ªÄ‰πàËÄÉËôëÁöÑÂë¢Ôºü

"
ÈÄöËøátrainÂáΩÊï∞ÂØπ‰∏≠ÊñáËØ≠ÊñôËÆ≠ÁªÉCRFÊ®°ÂûãÊó∂Âá∫Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®ÈÄöËøátrainÂáΩÊï∞ÂØπ‰∏≠ÊñáËØ≠ÊñôËÆ≠ÁªÉCRFÊ®°ÂûãÔºåÂæóÂà∞Ê®°ÂûãÊñá‰ª∂CWS_MODEL_FILE Êó∂Ôºå‰∏ÄÁõ¥Âá∫ÈîôÔºåÂπ∂‰∏îÂ¶ÇÊûúCRFSegmenterÂÅúÁî®ÔºåCRFLexicalAnalyzerÊé•Âè£Ê≤°ÊúâÁõ∏ÂÖ≥ÁöÑËÆ≠ÁªÉÂáΩÊï∞ÔºåËØ•ÊÄé‰πàÂØπËØ≠ÊñôËøõË°åËÆ≠ÁªÉCRFÊ®°ÂûãÔºü

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

	public static void main(String[] args) throws Exception {
		//CRFTrain();
		testCWS();
	}
	//ÈÄöËøátrainÂáΩÊï∞ÂØπ‰∏≠ÊñáËØ≠ÊñôËÆ≠ÁªÉCRFÊ®°ÂûãÔºåÂæóÂà∞Ê®°ÂûãÊñá‰ª∂CWS_MODEL_FILE
	public static void CRFTrain() throws Exception
    {        
        CRFSegmenter segmenter = new CRFSegmenter(null);
        segmenter.train(CORPUSPATH,CWS_MODEL_FILE);
    }
	//ÈÄöËøáCRFSegmenterÂáΩÊï∞ÂíåCRFÊ®°ÂûãÊñá‰ª∂CWS_MODEL_FILEÔºåÂØπÊµãËØïËØ≠Âè•ËøõË°åÂàÜËØç
	public static void testCWS() throws Exception
	{
		//CRFSegmenter segmenter = new CRFSegmenter(CWS_MODEL_FILE);
        CRFSegmenter segmenter = new CRFSegmenter();
        List<String> wordList = segmenter.segment(SENTENCE);
        System.out.println(wordList);
    }
}
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊµãËØï‰∏äËø∞‰ª£Á†ÅÔºå Âú®ÂæóÂà∞CRFÊ®°ÂûãËØ≠Âè•segmenter.train(CORPUSPATH,CWS_MODEL_FILE);Êó∂Âá∫Èîô

```


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
reloadÂä†ËΩΩÈîôËØØArrayIndexOutOfBoundsException,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®ËØçÂ∫ìËøõË°åreloadÊìç‰ΩúÊó∂Âá∫Áé∞Êï∞ÁªÑÊ∫¢Âá∫ÔºåÂú®1.7.0ÁâàÊú¨‰∏≠Ê≤°ÊúâËØ•ÈóÆÈ¢ò„ÄÇ
ÊòØÂê¶Âíå1.7.1‰øÆÊîπreloadÁîüÊàêËá™ÂÆö‰πâËØçÂÖ∏binÊñá‰ª∂Âú®ÈáçÊñ∞ËΩΩÂÖ•Êó∂ÊäõÂá∫ÂºÇÂ∏∏ArrayIndexOutOfBoundsException #1028ËØ•ÈóÆÈ¢òÊúâÂÖ≥

## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§
`Âú®CustomDictionaryÁ±ªÁöÑloadMainDictionaryÊñπÊ≥ï‰∏≠dat.build(map);Êó∂ÂèëÁîüÂºÇÂ∏∏„ÄÇ
ÁªèËøáË∑üË∏™ÂèëÁé∞Âú®DoubleArrayTrieÁöÑ

```
   private int resize(int newSize)
    {
        int[] base2 = new int[newSize];
        int[] check2 = new int[newSize];
        if (allocSize > 0)
        {
            System.arraycopy(base, 0, base2, 0, allocSize);
            System.arraycopy(check, 0, check2, 0, allocSize);
        }

        base = base2;
        check = check2;

        return allocSize = newSize;
    }
```
Âú®System.arraycopy(base, 0, base2, 0, allocSize);‰∏≠baseÂÅö‰∏∫Êã∑Ë¥ùÊ∫êÊï∞ÊçÆÈïøÂ∫¶‰∏∫1426181
base2ÁöÑÈïøÂ∫¶‰∏∫2097152ÔºåËÄåallocSizeÂ§çÂà∂ÈïøÂ∫¶‰πü‰∏∫2097152ÔºåÂØºËá¥Êï∞ÊçÆÊ∫¢Âá∫
### Ëß¶Âèë‰ª£Á†Å

```
   HanLP.segment(""Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ"");
   CustomDictionary.reload();
```
### ÊúüÊúõËæìÂá∫

Ê≠£Â∏∏ÈáçËΩΩ


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Ë≠¶Âëä: Ëá™ÂÆö‰πâËØçÂÖ∏./work/data/dictionary/custom/CustomDictionary.txtÁºìÂ≠òÂ§±Ë¥•ÔºÅ
java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 2097152 out of bounds for int[1426181]
	at java.base/java.lang.System.arraycopy(Native Method)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.resize(DoubleArrayTrie.java:94)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:403)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:338)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:365)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:378)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:107)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
	at com.hankcs.hanlp.dictionary.CustomDictionary.reload(CustomDictionary.java:657)
	at com.gildata.removal.Test_Hanlp.main(Test_Hanlp.java:21)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Â∏åÊúõËÉΩÂ§üÊåáÊïô‰∏Ä‰∏ã
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑ÊïôËØçÈ¢ëËé∑ÂèñÈóÆÈ¢ò„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®ÂØπÂπø‰∏úÁúÅËøõË°å‰∏ª‰ΩìËØÜÂà´Êó∂ÔºåËé∑ÂèñformÔºöÂßã##Âßã toÔºöÂπøÔºåÁöÑËØçÈ¢ëÊòØ78„ÄÇ
‰ΩÜÂú®CoreNatureDictionary.ngram.txtÂÜÖÂπ∂Êú™ÊâæÂà∞ÔºåÂßã##Âßã@Âπø  ÁöÑËÆ∞ÂΩï„ÄÇ
ËØ∑ÈóÆËøô‰∏™ËØçÈ¢ëÊòØ‰ªé‰ΩïËé∑ÂèñÁöÑ?Â∞èÁôΩÊèêÈóÆÊèêÂâçË∞¢Ëøá„ÄÇ
![image](http://thyrsi.com/t6/672/1550717998x2890202402.jpg)

"
ÂèØ‰ª•Êèê‰æõÈöêÈ©¨Áª¥ÁâπÊØîËÆ≠ÁªÉÂëΩÂêçÂÆû‰ΩìÁöÑËØ≠ÊñôÊ†ºÂºèÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
hanks‰Ω†Â•ΩÔºåÂú®‰Ω†ÁöÑÂ∏ÆÂä©‰∏ãÊàëËøôËæπÂ∑≤ÁªèÁÜüÊÇâÊÑüÁü•Êú∫ÁöÑÊìç‰ΩúÔºåÊâÄ‰ª•Áé∞Âú®ÊÉ≥ËØïËØïÁî®ÈöêÈ©¨ÂíåÁª¥ÁâπÊØîÊù•ËÆ≠ÁªÉÊñ∞ÂÆû‰Ωì„ÄÇ
ÊàëÂèÇÁÖßwiki‰∏≠ÁöÑhttps://github.com/hankcs/HanLP/wiki/%E8%A7%92%E8%89%B2%E6%A0%87%E6%B3%A8%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93  ËøõË°å‰∫Ü‰ªøÂÜôÔºå
‰ΩÜÊòØÂú®TestNRDctionaryMaker.java‰∏≠ÁöÑËØ≠ÊñôÊñá‰ª∂„Äê""data/dictionary/2014_dictionary.txt""„ÄëÂíå„ÄêD:\JavaProjects\CorpusToolBox\data\2014\„ÄëÂπ∂Ê≤°ÊúâÂú®‰ªª‰ΩïÂú∞ÊñπÊâæÂà∞ÔºåÂ∏åÊúõÂèØ‰ª•Êèê‰æõËØ•Â§ÑËØ≠ÊñôÊâÄÈúÄÁöÑÊ†ºÂºèÔºå‰ª•ÂáÜÂ§áÊñ∞ÂÆû‰ΩìÁöÑËØ≠Êñô„ÄÇ
ÊàëÂ∑≤Âú®relese‰∏≠‰∏ãËΩΩ‰∫ÜÊúÄÊñ∞ÁöÑdataÁöÑÂéãÁº©ÂåÖÂπ∂Ê≤°ÊúâÊâæÂà∞ÂØπÂ∫îÊñá‰ª∂Ôºåhttps://github.com/hankcs/HanLP/issues/311  Ëøô‰∏™issue‰∏≠ÊâÄÊèê‰æõÁöÑÈ°µÈù¢‰πüÂ∑≤ÁªèÊâæ‰∏çÂà∞‰∫Ü„ÄÇ
ÊÑüË∞¢ÊÇ®ÁöÑÊó∂Èó¥ÔºÅ
"
pyhanlpÊúârasaÁöÑÁªÑ‰ª∂ÂêóÔºåÁ±ª‰ººtokenizer_jiebaËøôÊ†∑ÁöÑ,"ÊúÄËøëÂú®ÁúãRASA, ËßâÂæóÂàÜËØçÁªÑ‰ª∂ËøòÊòØHanLPÂ•ΩÔºåËØ∑ÈóÆpyhanlpÊúârasaÁöÑÁªÑ‰ª∂ÂêóÔºåÁ±ª‰ººtokenizer_jiebaËøôÊ†∑ÁöÑÔºü
tokenizer_jiebaÂú®‰∏ãÈù¢ÁöÑÈáåÈù¢Êúâ‰ªãÁªç
https://blog.csdn.net/u010505246/article/details/82997100
https://www.jianshu.com/u/4b912e917c2e
https://blog.csdn.net/u010505246/article/details/83276354
"
KBeamArcEagerDependencyParser Áº∫Â∞ëdataÊñá‰ª∂,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ªégithub‰∏äÊãøÁöÑÊúÄÊñ∞‰ª£Á†ÅÂíådata. ‰ΩøÁî®KBeamArcEagerDependencyParserÂÅöÂè•Ê≥ïÂàÜÊûê
ËøêË°åÁº∫Â∞ë data/model/dependency/perceptron.bin
Âíå data/model/perceptron/ctb/pos.bin

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
"
ÂÖ≥‰∫éÂ§çÂêàËØçÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP 1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP 1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÈùûÂ∏∏ÊÑüË∞¢‰ΩúËÄÖÊèê‰æõÂäüËÉΩÂº∫Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ
1.Êàë‰ΩøÁî®data-for-1.7.1ÁöÑÊ®°ÂûãËøõË°åÂàÜËØçÂíåËØçÊÄßÊ†áËÆ∞ÔºåÂÅáÂ¶ÇÊàëË¶ÅÂ§ÑÁêÜÁöÑÂè•Â≠ê‰∏∫Ôºö‚ÄúÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫ÁöÑÂº†ÊúùÈò≥ËØ¥ÂïÜÂìÅÂíåÊúçÂä°ÊòØ‰∏âÂéüÂéøÈ≤ÅÊ°•È£üÂìÅÂéÇÁöÑ‰∏ªËê•‰∏öÂä°„ÄÇ‚ÄùÔºåÊåâÁÖßÁªìÊûÑÂåñÊÑüÁü•Êú∫Ê†áÊ≥®Ê°ÜÊû∂‰∏≠ÁöÑËØ¥ÊòéÔºå‚ÄúÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫‚ÄùÂíå‚Äú‰∏âÂéüÂéøÈ≤ÅÊ°•È£üÂìÅÂéÇ‚ÄùÂ∫îËØ•‰Ωú‰∏∫Â§çÂêàËØçÂá∫Áé∞Âú®ÁªìÊûú‰∏≠Ôºå‰ΩÜÊòØÊàëÂà©Áî®ÊÑüÁü•Êú∫Ê®°ÂûãËøõË°åÂàÜËØçÊó∂Âπ∂Ê≤°ÊúâÂæóÂà∞ËØ¥Êòé‰∏≠ÁöÑÁªìÊûú„ÄÇ
2.ÂàÜËØçÁªìÊûúÔºö""[È¶ôÊ∏Ø/ns, ÁâπÂà´/a, Ë°åÊîøÂå∫/n]/ns""Ë°®ÊòéÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫ÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ËØçËøõË°åÂàÜÂâ≤ÁöÑÔºåÂíåÁªìÊûúÔºö‚ÄúÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫/ns‚ÄùÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºüÊàëÁöÑÁêÜËß£ÊòØÔºåÂØπ‰∫éÂâçËÄÖÔºåÂΩì‚ÄúÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫‚ÄùËøôÂá†‰∏™Â≠óÂá∫Áé∞Âú®‰∏ÄËµ∑ÁöÑÊó∂ÂÄôÔºå‰Ωú‰∏∫‰∏Ä‰∏™ËØçÂàÜÂâ≤ÔºåÂè™Âá∫Áé∞‚ÄúÈ¶ôÊ∏Ø‚ÄùÁöÑÊó∂ÂÄôÔºå‰πüÂèØ‰ª•ÊääÈ¶ôÊ∏ØÂàÜÂâ≤Âá∫Êù•ÔºåÊØîÂ¶Ç‚ÄúÈ¶ôÊ∏ØÊòØË¥≠Áâ©Â§©Â†Ç‚Äù‰ºöË¢´ÂàÜÊàê‚ÄúÈ¶ôÊ∏Ø/ns ÊòØ/v Ë¥≠Áâ©/v Â§©Â†Ç/n‚ÄùÔºåËÄåÂØπ‰∫éÂêéËÄÖÔºåÂ¶ÇÊûúËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Âè™ÊúâÊ†áÊ≥®ÁöÑ‚ÄúÈ¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫/ns‚ÄùÔºåËÄåÊ≤°Êúâ‚ÄúÈ¶ôÊ∏Ø/ns‚ÄùÁöÑÊ†áÊ≥®ÔºåÂÖ≥‰∫éÈ¶ôÊ∏ØÁöÑÂàÜÂâ≤ÂèäÊ†áËÆ∞ÂèØËÉΩ‰ºöÂá∫Èîô„ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    String sentence = ""È¶ôÊ∏ØÁâπÂà´Ë°åÊîøÂå∫ÁöÑÂº†ÊúùÈò≥ËØ¥ÂïÜÂìÅÂíåÊúçÂä°ÊòØ‰∏âÂéüÂéøÈ≤ÅÊ°•È£üÂìÅÂéÇÁöÑ‰∏ªËê•‰∏öÂä°„ÄÇ"";
PerceptronLexicalAnalyzer segmenter = new PerceptronLexicalAnalyzer(
                ""data-for-1.7.1\\data\\model\\perceptron\\pku199801\\cws.bin"",
                ""data-for-1.7.1\\data\\model\\perceptron\\pku199801\\pos.bin"");
        Sentence sentence = segmenter.analyze(sentences);
        System.out.println(sentence);
```
### ÊúüÊúõËæìÂá∫
[È¶ôÊ∏Ø/ns ÁâπÂà´/a Ë°åÊîøÂå∫/n]/ns ÁöÑ/n Âº†ÊúùÈò≥/nr ËØ¥/v ÂïÜÂìÅ/n Âíå/c ÊúçÂä°/vn ÊòØ/v [‰∏âÂéüÂéø/ns È≤ÅÊ°•/nz È£üÂìÅÂéÇ/n]/nt ÁöÑ/z ‰∏ªËê•/vn ‰∏öÂä°/n


### ÂÆûÈôÖËæìÂá∫

È¶ôÊ∏Ø/ns ÁâπÂà´Ë°åÊîøÂå∫/nz ÁöÑ/u Âº†ÊúùÈò≥/nr ËØ¥/v ÂïÜÂìÅ/n Âíå/c ÊúçÂä°/vn ÊòØ/v ‰∏âÂéüÂéø/ns È≤ÅÊ°•/nz È£üÂìÅÂéÇ/n ÁöÑ/u ‰∏ªËê•/vn ‰∏öÂä°/n „ÄÇ/w
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‚ÄúÁùÄ‚ÄùÁöÑÊãºÈü≥‰∏çÊ≠£Á°Æ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

‰ΩøÁî®ËΩ¨ÊãºÈü≥ÂäüËÉΩÊó∂ÔºåÂèëÁé∞Ê±âÂ≠ó‚ÄúÁùÄ‚ÄùÂú®‰ªª‰ΩïÊó∂ÂÄôÈÉΩËæìÂá∫‰∫Ü‚Äúzhu√≥‚ÄùÔºå‰ΩÜÊòØÂÖ∂ÂÆûÂ§ßÂ§öÊï∞Â∫îËØ•ËæìÂá∫‚Äúzhe‚ÄùÁöÑ„ÄÇ

ÊØîÂ¶ÇÔºöÁõºÊúõÁùÄÁõºÊúõÁùÄÔºåÊò•Â§©Êù•‰∫ÜÔºõÂäüÂêçÊµëÊòØÈîôÔºåÊõ¥Ëé´ÊÄùÈáèÁùÄÔºõÊÇÑÊ≤°Â£∞ÁöÑÊ≤≥Ê≤ø‰∏äÔºåÊª°Èì∫ÁùÄÂØÇÂØûÂíåÈªëÊöóÔºõÊàë‰ªéÂ±±‰∏≠Êù•ÔºåÂ∏¶ÁùÄÂÖ∞Ëä±Ëçâ„ÄÇ„ÄÇÁ≠âÁ≠âËøô‰∫õÂè•Â≠êÈáåÁöÑ‚ÄúÁùÄ‚ÄùÈÉΩËæìÂá∫‰∫ÜÊãºÈü≥‚Äúzhu√≥‚ÄùÔºå‰ΩÜÊòØÊòéÊòæÂ∫îËØ•ÊòØ‚Äúzhe‚Äù

ÔºàÂ§ßÈÉ®ÂàÜÂÖ∂‰ªñÂ≠óÁöÑÊãºÈü≥ÁªìÊûúÈÉΩÊòØÊ≠£Á°ÆÁöÑÔºâ

### Ëß¶Âèë‰ª£Á†Å

‰ΩøÁî®gradleËá™Âä®ÈÖçÁΩÆÊñπÂºèÔºåÁÑ∂Âêé‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅÔºö
List<Pinyin> pinyinList = HanLP.convertToPinyinList(text);
for (Pinyin pinyin : pinyinList)
        {
            System.out.printf(""%s,"", pinyin.getPinyinWithToneMark());
        }
"
DoubleArrayTrieÁöÑ2‰∏™ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
*[x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlpVersion='portable-1.7.1'

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1. ÊØèÊ¨°Âà∑Êñ∞Ëá™ÂÆö‰πâËØçÂÖ∏Êó∂Âá∫Áé∞‰∏ãÈù¢ÁöÑÈîôËØØÔºåÁªèËøáÂàÜÊûêÔºåÊòØÂõ†‰∏∫Êâ©Â±ïÊï∞ÁªÑÊó∂Âá∫ÈîôÔºå
     DoubleArrayTrie.javaÁöÑprivate int resize(int newSize)ÊñπÊ≥ïÁöÑÂ¶Ç‰∏ã2Ë°å‰ª£Á†ÅÔºö Êï∞ÊçÆ‰∏ãÊ†áË∂äÁïåÂºÇÂ∏∏
  ÊØèÊ¨°ÈÉΩÊòØallocSizeÊØîbase2ÁöÑÂÆûÈôÖsizeÂ§ß‰∏Ä‰∏™„ÄÇ
      System.arraycopy(base, 0, base2, 0, allocSize);
      System.arraycopy(check, 0, check2, 0, allocSize);

---ÈîôËØØÂ†ÜÊ†à‰ø°ÊÅØÂ¶Ç‰∏ã
[2019-02-12 17:43:31,193] WARN  [http-nio-0.0.0.0-8083-exec-3] com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:150) Ëá™ÂÆö‰πâËØçÂÖ∏D:/myDev/hanlpData/data/dictionary/custom/CustomDictionary.txtÁºìÂ≠òÂ§±Ë¥•ÔºÅ
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.resize(DoubleArrayTrie.java:94)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:403)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:338)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:365)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:378)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:107)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
	at com.hankcs.hanlp.dictionary.CustomDictionary.reload(CustomDictionary.java:658)
   
2.     DoubleArrayTrie.javaÁöÑ private BitSet used; Ëøô‰∏™Â≠óÊÆµÊõæÁªèÂ§öÊ¨°Âá∫Áé∞ËøáÁ©∫ÊåáÈíàÂºÇÂ∏∏„ÄÇ
        ÂΩìÊó∂debugÔºåÂèëÁé∞ËôΩÁÑ∂ÊûÑÈÄ†ÂáΩÊï∞ÈáåÈù¢Êúânew, ‰ΩÜÊòØclearÂíåbuildÁ≠âËØ∏Â§öÊñπÊ≥ïÈÉΩÊúâËÆæÁΩÆ‰∏∫null,
        ËÆæÁΩÆ‰∏∫null‰πãÂêéÔºå‰∏ã‰∏ÄÊ¨°ËøêË°åÔºåËÇØÂÆöÁ©∫ÊåáÈíà„ÄÇ 
         debug‰∏≠ÈÄîÊÑèÂ§ñÈáçÂêØÔºå‰πãÂêéÂèàok‰∫Ü„ÄÇÂ∞±Ê≤°ÊúâÊ∑±ÂÖ•ÂàÜÊûê‰∫ÜÔºåÁõÆÂâçÊ≤°ÊúâÂÜçÊ¨°ÈáçÁé∞„ÄÇ
"
Âè•Â≠êÁöÑÂè•Ê≥ïÂàÜÊûêÁªìÊûú‰∏çÊòØ‰ª•ÂÖ≥Á≥ªÂõæÁöÑÁªìÊûúÊòæÁ§∫‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0


## ÊàëÁöÑÈóÆÈ¢ò

Âè•Â≠êÁöÑÂè•Ê≥ïÂàÜÊûêÁªìÊûú‰∏çÊòØ‰ª•ÂÖ≥Á≥ªÂõæÁöÑÁªìÊûúÊòæÁ§∫‰∫Ü


"
pyhanlpÂ¶Ç‰ΩïË∞ÉÁî®javaÁâàÊú¨ÁöÑËÅöÁ±ªÊñπÊ≥ï,"Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö
ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö 
È¶ñÈ°µÊñáÊ°£
wiki
Â∏∏ËßÅÈóÆÈ¢ò
ÊàëÂ∑≤ÁªèÈÄöËøáGoogleÂíåissueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
 ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ
ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.0
ÊàëÁöÑÈóÆÈ¢òÔºö
Âú® com.hankcs.hanlp.mining.cluster.ClusterAnalyzer ‰∏≠ÔºåClusterAnalyzerÁöÑÂàùÂßãÂåñÊòØÁî®Ê≥õÂûãÂèÇÊï∞Ôºö
public class ClusterAnalyzer<K>
{...}
‰ΩÜÊòØjpype‰∏≠Ê≤°ÊâæÂà∞‰º†Ê≥õÂûãÂèÇÊï∞ÁöÑÊñπÊ≥ïÔºåËØ∑ÈóÆÂ¶Ç‰ΩïÁî®pythonË∞ÉÁî®Ôºü"
Ë°çÁîüÈ°πÁõÆ‰πã elasticsearch-hanlp ÂàÜËØçÊèí‰ª∂ÂåÖ,"È°πÁõÆÂú∞ÂùÄÔºöhttps://github.com/AnyListen/elasticsearch-analysis-hanlp
ÂÖºÂÆπËåÉÂõ¥ÔºöES5.X„ÄÅES6.X

Âü∫‰∫é HanLP ÁöÑ Elasticsearch ‰∏≠ÊñáÂàÜËØçÊèí‰ª∂ÔºåÊ†∏ÂøÉÂäüËÉΩÔºö
- ÂÜÖÁΩÆËØçÂÖ∏ÔºåÊó†ÈúÄÈ¢ùÂ§ñÈÖçÁΩÆÂç≥ÂèØ‰ΩøÁî®Ôºõ
- ÊîØÊåÅÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Ôºõ
- ÊîØÊåÅËøúÁ®ãËØçÂÖ∏ÁÉ≠Êõ¥Êñ∞ÔºàÂæÖÂºÄÂèëÔºâÔºõ
- ÂÜÖÁΩÆÂ§öÁßçÂàÜËØçÊ®°ÂºèÔºåÈÄÇÂêà‰∏çÂêåÂú∫ÊôØÔºõ
- ÊãºÈü≥ËøáÊª§Âô®ÔºàÂæÖÂºÄÂèëÔºâÔºõ
- ÁÆÄÁπÅ‰ΩìËΩ¨Êç¢ËøáÊª§Âô®ÔºàÂæÖÂºÄÂèëÔºâ„ÄÇ"
Ëøô‰∏™ÂàÜËØçÂàÜÊàêËøôÊ†∑Ôºå‰∏∫Âï•Âë¢Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂàÜËØçÈîôËØØ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
""Âä™ÂäõÈÄ†Â∞±‰∏ÄÊîØÂø†ËØöÂπ≤ÂáÄÊãÖÂΩìÁöÑÈ´òÁ¥†Ë¥®Âπ≤ÈÉ®Èòü‰ºç""ÂàÜËØçÈîôËØØ
## Â§çÁé∞ÈóÆÈ¢ò
### Ëß¶Âèë‰ª£Á†Å

```
List<Term> testA = HanLP.segment(""Âä™ÂäõÈÄ†Â∞±‰∏ÄÊîØÂø†ËØöÂπ≤ÂáÄÊãÖÂΩìÁöÑÈ´òÁ¥†Ë¥®Âπ≤ÈÉ®Èòü‰ºç"");
```
### ÊúüÊúõËæìÂá∫

```
ÊúüÊúõËæìÂá∫:‰∏ÄÊîØ/  Âø†ËØö/ Âπ≤ÂáÄ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
‰∏Ä/ÊîØÂø†ËØö/ Âπ≤ÂáÄ
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
none
"
ÂÖ≥‰∫éÊÑüÁü•Êú∫Âú®Á∫øÂ≠¶‰π†Ôºå‰∏≠ÊñáÁ¨¶Âè∑Êó†Ê≥ïÊ≠£Á°ÆËØÜÂà´ËØçÊÄßÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Êàë‰ΩøÁî®ÁöÑÊòØpythonÁâàÊú¨ÁöÑÔºå‰ΩÜÊàëÊÉ≥Â∫îËØ•‰∏ç‰ºöÂΩ±Âìç‰∏ãÈù¢ÁöÑÈóÆÈ¢òÔºõÂú®ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÂßãÁªàÊó†Ê≥ïËÆ≠ÁªÉÂà∞ÊÉ≥Ë¶ÅÁöÑÁªìÊûú„ÄÇÈÇ£‰∏™‰∏≠ÊñáÁöÑ 'Ôºö'ÂßãÁªàÊòØnÔºå‰∏çÁü•ÈÅì‰ªÄ‰πàÂéüÂõ†ÔºåËØ∑Êïô‰∏Ä‰∏ã

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```

from pyhanlp import *
from jpype import *
jvmPath = getDefaultJVMPath()
if not isJVMStarted():
    startJVM(jvmPath)
PerceptronLexicalAnalyzer = JPackage(""com"").hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer
Config = JPackage(""com"").hankcs.hanlp.model.perceptron.Config
LinearModel = JPackage(""com"").hankcs.hanlp.model.perceptron.model.LinearModel
CoreStopWordDictionary = JPackage(""com"").hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary
NotionalTokenizer = JPackage(""com"").hankcs.hanlp.tokenizer.NotionalTokenizer

cws_model = LinearModel(HanLP.Config.PerceptronCWSModelPath)
ner_model = LinearModel(HanLP.Config.PerceptronNERModelPath)
pos_model = LinearModel(HanLP.Config.PerceptronPOSModelPath)

analyzer = PerceptronLexicalAnalyzer(cws_model, pos_model, ner_model)

analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
print(analyzer.analyze(""ÁâàÂºèËÆæËÆ°ÔºöÊ≤à‰∫¶‰º∂„Ää‰∫∫Ê∞ëÊó•Êä•„ÄãÔºà2018Âπ¥11Êúà09Êó•06ÁâàÔºâ""))

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/n Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•06/m Áâà/n Ôºâ/w

// Â§öËÆ≠ÁªÉÂá†ÈÅç
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/n Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éÊÑüÁü•Êú∫Âú®Á∫øÂ≠¶‰π†Ôºå‰∏≠ÊñáÁ¨¶Âè∑Êó†Ê≥ïÊ≠£Á°ÆËØÜÂà´ËØçÊÄßÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

Êàë‰ΩøÁî®ÁöÑÊòØpythonÁâàÊú¨ÁöÑÔºå‰ΩÜÊàëÊÉ≥Â∫îËØ•‰∏ç‰ºöÂΩ±Âìç‰∏ãÈù¢ÁöÑÈóÆÈ¢òÔºõÂú®ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÂßãÁªàÊó†Ê≥ïËÆ≠ÁªÉÂà∞ÊÉ≥Ë¶ÅÁöÑÁªìÊûú„ÄÇÈÇ£‰∏™‰∏≠ÊñáÁöÑ 'Ôºö'ÂßãÁªàÊòØnÔºå‰∏çÁü•ÈÅì‰ªÄ‰πàÂéüÂõ†ÔºåËØ∑Êïô‰∏Ä‰∏ã

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶


### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
from jpype import *
jvmPath = getDefaultJVMPath()
if not isJVMStarted():
    startJVM(jvmPath)
PerceptronLexicalAnalyzer = JPackage(""com"").hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer
Config = JPackage(""com"").hankcs.hanlp.model.perceptron.Config
LinearModel = JPackage(""com"").hankcs.hanlp.model.perceptron.model.LinearModel
CoreStopWordDictionary = JPackage(""com"").hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary
NotionalTokenizer = JPackage(""com"").hankcs.hanlp.tokenizer.NotionalTokenizer

cws_model = LinearModel(HanLP.Config.PerceptronCWSModelPath)
ner_model = LinearModel(HanLP.Config.PerceptronNERModelPath)
pos_model = LinearModel(HanLP.Config.PerceptronPOSModelPath)

analyzer = PerceptronLexicalAnalyzer(cws_model, pos_model, ner_model)

analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
#analyzer.learn(""ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w"")
print(analyzer.analyze(""ÁâàÂºèËÆæËÆ°ÔºöÊ≤à‰∫¶‰º∂„Ää‰∫∫Ê∞ëÊó•Êä•„ÄãÔºà2018Âπ¥11Êúà09Êó•06ÁâàÔºâ""))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/w Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/n Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•06/m Áâà/n Ôºâ/w

// Â§öËÆ≠ÁªÉÂá†ÈÅç
ÁâàÂºè/n ËÆæËÆ°/vn Ôºö/n Ê≤à‰∫¶‰º∂/nr „Ää/w ‰∫∫Ê∞ëÊó•Êä•/nz „Äã/w Ôºà/w 2018Âπ¥11Êúà09Êó•/t 06Áâà/m Ôºâ/w
```
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
ËØ∑ÊïôÂá†‰∏™ÂÖ≥‰∫éÁâπÊÆä‰∫∫ÂêçÂÆû‰ΩìËØÜÂà´ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1„ÄÅÊàëÊêúÁ¥¢TranslatedPersonRecognitionÂíåJapenesePersonRecognitionÂèëÁé∞Ëøô‰∏§‰∏™javaÁ±ªÂè™Ë¢´Áª¥ÁâπÊØî„ÄÅÊúÄÁü≠Ë∑ØÂæÑ„ÄÅnÊúÄÁü≠Ë∑ØÂæÑÂàÜËØçÊâÄÁî®Âà∞ÔºåCRFÂíåÊÑüÁü•Êú∫ÂàÜËØçÊ≤°ÊúâÁî®Âà∞ÔºåÈÇ£‰πàÂ¶ÇÊûúÊàëÊÉ≥ËÆ©CRFÂíåÊÑüÁü•Êú∫ÂàÜËØçËÉΩËØÜÂà´Êó•Êú¨‰∫∫ÂêçÂíåÁøªËØë‰∫∫ÂêçÊòØÂê¶Âè™ËÉΩ‰æùÈù†ËÆ≠ÁªÉÊñ∞ÁöÑÁõ∏ÂÖ≥ËØ≠ÊñôÔºü
2„ÄÅÂ¶ÇÊûúÊàëÊÉ≥ËØÜÂà´Êñ∞ÁöÑ‰∫∫ÂêçÁ±ªÂûãÔºàÂ¶ÇÂ∞ëÊï∞Ê∞ëÊóè‰∫∫ÂêçÔºâÔºåÂ≠óÂÖ∏Êñá‰ª∂ÊòØÂê¶Âè™ÈúÄË¶ÅÂáÜÂ§á‰∏Ä‰ªΩÂ∏¶Êúâ‰∫∫ÂêçÂÖ≥ÈîÆÂ≠óÁöÑnrx.txtÂë¢Ôºütrie.datÂíåvalue.datÊòØËá™Âä®ÁîüÊàêÁöÑÂêóÔºü
ÂÜçÊ¨°ÊÑüË∞¢ÊÇ®ÁöÑÊó∂Èó¥ÂíåËÄêÂøÉÔºÅ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

### Ëß¶Âèë‰ª£Á†Å

```
   public class DemoJapaneseNameRecognition
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""ÂåóÂ∑ùÊôØÂ≠êÂèÇÊºî‰∫ÜÊûóËØ£ÂΩ¨ÂØºÊºîÁöÑ„ÄäÈÄüÂ∫¶‰∏éÊøÄÊÉÖ3„Äã"",
                ""ÊûóÂøóÁé≤‰∫ÆÁõ∏ÁΩëÂèã:Á°ÆÂÆö‰∏çÊòØÊ≥¢Â§öÈáéÁªìË°£Ôºü"",
                ""ÈæüÂ±±ÂçÉÂπøÂíåËøëËó§ÂÖ¨Âõ≠Âú®ÈæüÂ±±ÂÖ¨Âõ≠ÈáåÂñùÈÖíËµèËä±"",
        };
        Segment segment = HanLP.newSegment(""crf"").enableJapaneseNameRecognize(true);
        for (String sentence : testCase)
        {
            List<Term> termList = segment.seg(sentence);
            System.out.println(termList);
        }

    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÂåóÂ∑ùÊôØÂ≠ê/nrj, ÂèÇÊºî/v, ‰∫Ü/ule, ÊûóËØ£ÂΩ¨/nr, ÂØºÊºî/nnt, ÁöÑ/ude1, „Ää/w, ÈÄüÂ∫¶/n, ‰∏é/cc, ÊøÄÊÉÖ/n, 3/m, „Äã/w]
[ÊûóÂøóÁé≤/nr, ‰∫ÆÁõ∏/vi, ÁΩëÂèã/n, :/w, Á°ÆÂÆö/v, ‰∏çÊòØ/c, Ê≥¢Â§öÈáéÁªìË°£/nrj, Ôºü/w]
[ÈæüÂ±±ÂçÉÂπø/nrj, Âíå/cc, ËøëËó§ÂÖ¨Âõ≠/nrj, Âú®/p, ÈæüÂ±±/nz, ÂÖ¨Âõ≠/n, Èáå/f, ÂñùÈÖí/vi, ËµèËä±/nz]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂåóÂ∑ù/ns, ÊôØÂ≠ê/n, ÂèÇÊºî/v, ‰∫Ü/u, ÊûóËØ£ÂΩ¨/nr, ÂØºÊºî/n, ÁöÑ/u, „Ää/w, ÈÄüÂ∫¶/n, ‰∏é/c, ÊøÄÊÉÖ/n, 3/m, „Äã/w]
[ÊûóÂøóÁé≤/nr, ‰∫ÆÁõ∏/v, ÁΩëÂèã/n, :/w, Á°ÆÂÆö/v, ‰∏ç/d, ÊòØ/v, Ê≥¢Â§öÈáé/n, ÁªìË°£/n, Ôºü/w]
[Èæü/v, Â±±/n, ÂçÉ/m, Âπø/q, Âíå/c, ËøëËó§/a, ÂÖ¨Âõ≠/n, Âú®/p, ÈæüÂ±±ÂÖ¨Âõ≠/ns, Èáå/f, Âñù/v, ÈÖí/n, Ëµè/v, Ëä±/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊúâÊñπÊ≥ïËÉΩÂà§Êñ≠‰∏Ä‰∏™Â≠óÊòØ‰∏çÊòØÂ§öÈü≥Â≠óÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âä†ËΩΩÊñ∞ËÆ≠ÁªÉÂ•ΩÁöÑCRFÊ®°ÂûãÊó∂Âá∫Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp 1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËÆ≠ÁªÉÂ•ΩCRFÊ®°ÂûãÂêéÔºåÂú®‰ΩøÁî®CRFLexicalAnalyzerÊàñËÄÖCRFSegmenterÂàÜËØçÊó∂ÔºåÂä†ËΩΩÊó∂Âá∫ÈîôÔºå
ÂÖ∑‰ΩìÈîôËØØÊòØ
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""1@?@@?@""
	at java.lang.NumberFormatException.forInputString(Unknown Source)
	at java.lang.Integer.parseInt(Unknown Source)
	at java.lang.Integer.parseInt(Unknown Source)
	at com.hankcs.hanlp.model.crf.LogLinearModel.convert(LogLinearModel.java:124)
	at com.hankcs.hanlp.model.crf.LogLinearModel.<init>(LogLinearModel.java:101)
	at com.hankcs.hanlp.model.crf.CRFTagger.<init>(CRFTagger.java:41)
	at com.hankcs.hanlp.model.crf.CRFSegmenter.<init>(CRFSegmenter.java:47)
	at com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer.<init>(CRFLexicalAnalyzer.java:71)
	at fenci.CRFCWSTrain.main(CRFCWSTrain.java:59)

ÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÂá∫Áé∞‰∏äËø∞ÈîôËØØÂèØËÉΩÊòØÁî±Âì™‰∫õÂéüÂõ†ÂØºËá¥ÁöÑÔºåÂú®Ë∞ÉÁî®HanLPÂíåCRF++Ê®°ÂûãÁöÑÊó∂ÂÄôÈÉΩ‰ºöÂá∫Áé∞‰∏äËø∞ÈóÆÈ¢ò„ÄÇÊÑüË∞¢„ÄÇ"
Â¶Ç‰ΩïÂú®ËÆ≠ÁªÉËá™ÂÆö‰πâËØçÂÖ∏„ÄÅCRFÊ®°Âûã„ÄÅÊÑüÁü•Êú∫Ê®°ÂûãÁöÑËøáÁ®ã‰∏≠Ê∑ªÂä†Êñ∞ÁöÑËØçÊÄß,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËØ∑ÈóÆ‰∏Ä‰∏ãÔºåÂ¶Ç‰ΩïÂú®ËÆ≠ÁªÉËá™ÂÆö‰πâËØçÂÖ∏„ÄÅCRFÊ®°Âûã„ÄÅÊÑüÁü•Êú∫Ê®°ÂûãÁöÑËøáÁ®ã‰∏≠Ê∑ªÂä†Êñ∞ÁöÑËØçÊÄßÔºü

"
"chisquareScore = stats.n * Math.pow(N11 * N00 - N10 * N01, 2) / ((N11 + N01) * (N11 + N10) * (N10 + N00) * (N01 + N00))ËÆ°ÁÆóÂÄºÊ∫¢Âá∫","ChiSquareFeatureExtractor‰∏≠ÁöÑËøôË°å‰ª£Á†ÅN11Á≠âÂèòÈáèÂùá‰∏∫intÂûãÔºå ËÆ°ÁÆóÊó∂‰ºöÂºïÂèëÊï∞ÂÄºÊ∫¢Âá∫Ôºå ËôΩÁÑ∂‰∏ç‰ºöÊä•ÈîôÔºå ‰ΩÜÊòØËÆ°ÁÆóÁªìÊûúÊòØÈîôËØØÁöÑ
2000L * 82 * 2001 * 3919 = 1286074716000
‰ΩÜÊòØÂÆûÈôÖÁ®ãÂ∫è‰∏≠Ôºö
2000 * 82 * 2001 * 3919 = 1879494496ÔºåËøô‰∏™ÂÄºÊòØÈîôËØØÁöÑÔºå ÂØºËá¥Âç°ÊñπÊ£ÄÊµãËøêÁÆóÁªìÊûúÂÖ∂ÂÆûÊòØÈîôËØØÁöÑ"
ÊÇ®Â•ΩÔºå‰∏∫‰ªÄ‰πàhanlp-1.7.0-release.zipÊñá‰ª∂‰∏ãËΩΩ‰∏ç‰∫ÜÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
NLPTokenizerÂàÜËØçÂèØËÉΩÂ≠òÂú®bugÔºåÂàÜËØçÊó∂‰∏çËÉΩÊåâÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂàáÂàÜ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

NLPTokenizerÂàÜËØçÂèØËÉΩÂ≠òÂú®bugÔºåÂàÜËØçÊó∂‰∏çËÉΩÊåâÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂàáÂàÜ


### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) throws IOException
    {
      		CustomDictionary.insert(""ÊôöÈúû"", ""wx 1"");
		System.out.println(NLPTokenizer.segment(""ÊàëÁöÑÂ∏åÊúõÊòØÂ∏åÊúõÂº†ÊôöÈúûÁöÑËÉåÂΩ±Ë¢´ÊôöÈúûÊò†Á∫¢""));
        // Ê≥®ÊÑèËßÇÂØü‰∏ãÈù¢‰∏§‰∏™‚ÄúÂ∏åÊúõ‚ÄùÁöÑËØçÊÄß„ÄÅ‰∏§‰∏™‚ÄúÊôöÈúû‚ÄùÁöÑËØçÊÄß
		System.out.println(NLPTokenizer.analyze(""ÊàëÁöÑÂ∏åÊúõÊòØÂ∏åÊúõÂº†ÊôöÈúûÁöÑËÉåÂΩ±Ë¢´ÊôöÈúûÊò†Á∫¢"").translateLabels());
    }
```
### ÊúüÊúõËæìÂá∫



```
[Êàë/r, ÁöÑ/u, Â∏åÊúõ/n, ÊòØ/v, Â∏åÊúõ/v, Âº†ÊôöÈúû/nr, ÁöÑ/u, ËÉåÂΩ±/n, Ë¢´/p, ÊôöÈúû/wx, Êò†Á∫¢/nr]
Êàë/‰ª£ËØç ÁöÑ/Âä©ËØç Â∏åÊúõ/ÂêçËØç ÊòØ/Âä®ËØç Â∏åÊúõ/Âä®ËØç Âº†ÊôöÈúû/‰∫∫Âêç ÁöÑ/Âä©ËØç ËÉåÂΩ±/ÂêçËØç Ë¢´/‰ªãËØç ÊôöÈúû/wx Êò†Á∫¢/‰∫∫Âêç
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Êàë/r, ÁöÑ/u, Â∏åÊúõ/n, ÊòØ/v, Â∏åÊúõ/v, Âº†ÊôöÈúû/nr, ÁöÑ/u, ËÉåÂΩ±/n, Ë¢´/p, ÊôöÈúû/n, Êò†Á∫¢/nr]
Êàë/‰ª£ËØç ÁöÑ/Âä©ËØç Â∏åÊúõ/ÂêçËØç ÊòØ/Âä®ËØç Â∏åÊúõ/Âä®ËØç Âº†ÊôöÈúû/‰∫∫Âêç ÁöÑ/Âä©ËØç ËÉåÂΩ±/ÂêçËØç Ë¢´/‰ªãËØç ÊôöÈúû/wx Êò†Á∫¢/‰∫∫Âêç
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
add DeprelTranslator support for MaxEntDependencyParser,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
MaxEntDependencyParser ÊîØÊåÅËã±ÊñáÊ†áÁ≠æËæìÂá∫


"
Ê±ÇÈóÆdemoÔºàhanlp.comÔºâÈáåÈù¢Áî®ÁöÑÂàÜËØçÂô®Âíå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ËØ∑ÈóÆdemoÔºàhttp://hanlp.comÔºâ‰ΩøÁî®‰∫Ü‰ªÄ‰πàÂàÜËØçÂô®Âíå‰æùÂ≠òÂàÜÊûêÂô®Ôºü

ÊàëÂú®demo‰∏äÊµã‰∫Ü‰∏Ä‰∫õÂè•Â≠êÔºåÂèëÁé∞ÊïàÊûúÊØîËæÉÂ•ΩÔºåÁÑ∂ËÄåÂú®Êú¨Âú∞‰ΩøÁî®1.7.1‰ª£Á†ÅÊµãËØï„ÄÇÂèëÁé∞ÂíådemoÊúâÂæàÂ§ßÂ∑ÆË∑ù

1. È¶ñÂÖàÊòØÂàÜËØçÂô®„ÄÇ
NeuralNetworkDependencyParser.compute(String sentence)‰ΩøÁî®ÁöÑÊòØCharacterBasedSegmentÔºåÊó†Ê≥ï‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÊâÄ‰ª•ÊïàÊûú‰∏çÂ§™Â•Ω„ÄÇÊàëÁªôÊõøÊç¢Êàê‰∫ÜViterbiSegmentÂàÜËØçÂô®ÔºåÊïàÊûúËøòÂèØ‰ª•Ôºå‰ΩÜÊòØÂíådemoËøòÊòØÊúâ‰∫õÂá∫ÂÖ•ÔºåÊÉ≥ÈóÆ‰∏ãdemo‰∏≠‰ΩøÁî®ÁöÑÂàÜËØçÂô®ÊòØÂì™‰∏™Ôºü

2. ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®
ÊµãËØïÂè•Â≠êÔºöÊàëÈÄÅÂ•π‰∏ÄÊùüËä±

demoÁöÑÁªìÊûúÔºö
![21_05_33__01_02_2019](https://user-images.githubusercontent.com/11264714/50593202-56946e80-0ed2-11e9-9947-df12a1505ac3.jpg)

Êú¨Âú∞ÊµãËØï‰ª£Á†ÅÔºö
NeuralNetworkDependencyParser dependencyParser = new NeuralNetworkDependencyParser();
dependencyParser.setSegment(new ViterbiSegment());
System.out.println(dependencyParser.parse(""ÊàëÈÄÅÂ•π‰∏ÄÊùüËä±„ÄÇ""));
Êú¨Âú∞ÊµãËØïÁªìÊûúÔºö
![21_07_51__01_02_2019](https://user-images.githubusercontent.com/11264714/50593297-c0ad1380-0ed2-11e9-833f-b64f84158655.jpg)


ËØï‰∫Ü‰∏Ä‰∫õÂè•Â≠êÔºåÂπ∂Ê≤°ÊúâÂàÜÊûêÂá∫Èó¥ÂÆæÂÖ≥Á≥ªÔºåÁÑ∂ËÄådemo‰∏≠ÂèØ‰ª•Ê≠£Â∏∏ÂàÜÊûêÂá∫„ÄÇÊâÄ‰ª•ÊÉ≥ËØ∑ÈóÆ‰∏ãdemo‰∏≠‰ΩøÁî®ÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®Êúâ‰ªÄ‰πà‰∏çÂêåÔºåË∞¢Ë∞¢ÔºÅ


"
DemoWord2VecÂÆûÈôÖËøêË°åÁªìÊûúÂíåwikiÊñáÊ°£‰∏≠ÊèèËø∞ÁöÑÁªìÊûúÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP 1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö HanLP 1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂ∞Üword2vecDemoÂéüÂ∞Å‰∏çÂä®ÁöÑcopy‰∏ãÊù•ÔºåÊï∞ÊçÆÈõÜ‰πüÊòØÊñáÊ°£‰∏≠ÊèêÂà∞ÁöÑÊêúÁãóÊï∞ÊçÆÔºåÊåâÁÖßwikiÊñáÊ°£‰∏≠ÁöÑÊØè‰∏ÄÁßçÂ∫îÁî®Âú∫ÊôØ(ÊØîÂ¶ÇËÆ°ÁÆóÂ±±‰∏ú-Ê±üËãèÁöÑËØ≠‰πâË∑ùÁ¶ª)ËøêË°å‰∫Ü‰∏ÄÈÅç„ÄÇ‰ΩÜÂèëÁé∞ÂÆûÈôÖÁöÑËøêË°åÁªìÊûú‰∏éÊñáÊ°£‰∏≠ÁöÑÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇËØ∑Ëß£Èáä‰∏Ä‰∏ãËøôÊòØ‰∏∫‰ªÄ‰πàÔºüÂè¶Â§ñÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÁöÑÁªìÊûú‰∏∫‰ΩïÂá∫Áé∞‰∫ÜË¥üÂÄºÔºüÈöæÈÅì‰∏çÂ∫îËØ•ÊòØ0-1‰πãÈó¥ÁöÑÂÄºÂêóÔºü

ÊñáÊ°£‰º†ÈÄÅÈó®Ôºöhttps://github.com/hankcs/HanLP/wiki/word2vec

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàCopyÊñáÊ°£‰∏≠ÁöÑDemoÁ®ãÂ∫èÔºå‰∏ãËΩΩÊêúÁãóËØ≠ÊñôÔºåËÆ≠ÁªÉÊ®°Âûã
2. ÁÑ∂ÂêéÊ∑ªÂä†‰∫Ü‰∏§Ë°åÊñáÊ°£‰∏≠ÁöÑ‰ª£Á†Å
3. Êé•ÁùÄËøêË°åÁªìÊûú

### Ëß¶Âèë‰ª£Á†Å

```
  System.out.println(wordVectorModel.similarity(""Â±±‰∏ú"", ""Ê±üËãè""));
  System.out.println(wordVectorModel.similarity(""Â±±‰∏ú"", ""‰∏äÁè≠""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
0.81871825
0.25067142
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
0.39530003
-0.004012134
```


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éÂàÜËØçÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊúÄËøë‰∏ãËΩΩÊúÄÊñ∞ÁöÑÁâàÊú¨1.7.1ÔºåËøêË°å DemoCRFLexicalAnalyzer Âíå DemoPerceptronLexicalAnalyzer ÊµãËØïÂàÜËØçÔºåÂèëÁé∞ ÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥Áî±ÊØîÁàæ¬∑ËìãËå≤Âíå‰øùÁæÖ¬∑ËâæÂÄ´ÂâµÁ´ãÔºå18Âπ¥ÂïüÂãï‰ª•Êô∫ÊÖßÈõ≤Á´Ø„ÄÅÂâçÁ´ØÁÇ∫Â∞éÂêëÁöÑÂ§ßÊîπÁµÑ„ÄÇÂØπ‰∫é‰∫∫ÂêçÁöÑËØÜÂà´‰∏çÂØπÔºå‰ª•ÂâçÁâàÊú¨ÊàëËÆ∞ÂæóÊòØÊ≤°ÈóÆÈ¢òÁöÑ„ÄÇÁé∞Âú®ÂàÜËØçÁöÑÁªìÊûúÊòØÔºö
ÂæÆËΩØÂÖ¨Âè∏/ntc Êñº/p 1975Âπ¥/t Áî±/p ÊØîÁàæ¬∑ËìãËå≤/n Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/nr ÂâµÁ´ã/v Ôºå/w 18Âπ¥/t ÂïüÂãï/v ‰ª•/p Êô∫ÊÖß/n Èõ≤Á´Ø/n „ÄÅ/w ÂâçÁ´Ø/f ÁÇ∫/v Â∞éÂêë/n ÁöÑ/u Â§ß/a ÊîπÁµÑ/vn „ÄÇ/w

## Â§çÁé∞ÈóÆÈ¢ò
ËøêË°å DemoCRFLexicalAnalyzer Á±ª‰∏≠ÁöÑÊñπÊ≥ï

### Ê≠•È™§
ËøêË°å DemoCRFLexicalAnalyzer Á±ª‰∏≠ÁöÑÊñπÊ≥ï

### Ëß¶Âèë‰ª£Á†Å
public static void main(String[] args) throws IOException
    {
        CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
        analyzer.enableAllNamedEntityRecognize(true)
        .enableCustomDictionary(true)
        .enableOrganizationRecognize(true)
        .enablePartOfSpeechTagging(true)
        .enableTranslatedNameRecognize(true)
        .enableNameRecognize(true);
        
        String[] tests = new String[]{
            ""ÂïÜÂìÅÂíåÊúçÂä°"",
            ""‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏Ëë£‰∫ãÈïøË∞≠Êó≠ÂÖâÂíåÁßò‰π¶ËÉ°Ëä±ËïäÊù•Âà∞ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶ÜÂèÇËßÇ"",
            ""ÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥Áî±ÊØîÁàæ¬∑ËìãËå≤Âíå‰øùÁæÖ¬∑ËâæÂÄ´ÂâµÁ´ãÔºå18Âπ¥ÂïüÂãï‰ª•Êô∫ÊÖßÈõ≤Á´Ø„ÄÅÂâçÁ´ØÁÇ∫Â∞éÂêëÁöÑÂ§ßÊîπÁµÑ„ÄÇ"", 
            ""ÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥Áî±ÊØîÂ∞î¬∑ÁõñËå®Âíå‰øùÁΩó¬∑Ëâæ‰º¶ÂàõÁ´ãÔºå18Âπ¥ÂêØÂä®‰ª•Êô∫ÊÖß‰∫ëÁ´Ø„ÄÅÂâçÁ´Ø‰∏∫ÂØºÂêëÁöÑÂ§ßÊîπÁªÑ„ÄÇ"" 
        };
        for (String sentence : tests)
        {
            System.out.println(analyzer.analyze(sentence));
//            System.out.println(analyzer.seg(sentence));
        }
    }

ÊúüÊúõËæìÂá∫
ÂæÆËΩØÂÖ¨Âè∏/ntc Êñº/p 1975Âπ¥/t Áî±/p ÊØîÁàæ¬∑ËìãËå≤/nrf Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/nrf ÂâµÁ´ã/v Ôºå/w 18Âπ¥/t ÂïüÂãï/v ‰ª•/p Êô∫ÊÖß/n Èõ≤Á´Ø/n „ÄÅ/w ÂâçÁ´Ø/n ÁÇ∫/v Â∞éÂêë/n ÁöÑ/u Â§ß/a ÊîπÁµÑ/vn „ÄÇ/w

ÂÆûÈôÖËæìÂá∫
ÂæÆËΩØÂÖ¨Âè∏/ntc Êñº/p 1975Âπ¥/t Áî±/p ÊØîÁàæ¬∑ËìãËå≤/n Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/v ÂâµÁ´ã/v Ôºå/w 18Âπ¥/t ÂïüÂãï/v ‰ª•/p Êô∫ÊÖß/n Èõ≤Á´Ø/n „ÄÅ/w ÂâçÁ´Ø/n ÁÇ∫/v Â∞éÂêë/n ÁöÑ/u Â§ß/a ÊîπÁµÑ/vn „ÄÇ/w
Â≠òÂú®ÈóÆÈ¢òÁöÑÂú∞ÊñπÔºöÊØîÁàæ¬∑ËìãËå≤/n Âíå/c ‰øùÁæÖ¬∑ËâæÂÄ´/v
Ëøô‰∏§‰∏™‰∫∫ÂêçËØÜÂà´‰∏çÂØπ„ÄÇ
"
Âú®CRFÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äËÆ≠ÁªÉËá™Â∑±ÁöÑÊ®°Âûã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP v1.2.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÑüÁü•Êú∫ÂàÜËØç/ËØçÊÄßËØÜÂà´„ÄÅCRFÂàÜËØçÔºåËÉΩÂê¶ÈÄöËøáÊàë‰ª¨ÁöÑËá™Â∑±ÁöÑËÆ≠ÁªÉÊ®°Âûã‰Ωú‰∏∫ÂØπÁ®ãÂ∫èÊú¨Ë∫´Êèê‰æõÁöÑÊ®°ÂûãÁöÑË°•ÂÖÖÔºå‰∏ç‰ªÖÈôê‰∫éÂú®Á∫øÂ≠¶‰π†ÔºåËØ•ÊÄé‰πàÂÆûÁé∞ÂØπÊ®°ÂûãÁöÑ‰øÆÊîπÔºåÊàñËÄÖÊòØ‰∏çÊòØÂèØ‰ª•ÈÄöËøáËÆ≠ÁªÉÂá∫Ê®°ÂûãÊñá‰ª∂ÊòØÂê¶ÊîæÂà∞Áõ∏ÂÖ≥ÁõÆÂΩï‰∏ãÂç≥ÂèØÂêØÁî®Ôºü"
‰ΩøÁî®hanlpÂàÜËØçÈÄ†ÊàêÂÜÖÂ≠òÊ≥ÑÊºè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ‰ΩøÁî®hanlpÂàÜËØçÂÅö‰∏∫elasticsearchÁöÑÂàÜËØçÂô®

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ΩøÁî®hanlp 1.7.1ÂÅö‰∏∫elasticsearchÁöÑÂàÜËØçÂô®,ÂêØÂä®‰πãÂêéÂÜÖÂ≠òÊ∂àËÄó‰∏äÂçá,‰πãÂêéÂá∫Áé∞java.lang.OutOfMemoryError: Java heap space JavaÁöÑÂÜÖÂ≠òÂàÜÈÖç‰∏∫2g
jvm ÂÜÖÂ≠òÂàÜÈÖçÂèÇÊï∞ 
-Xms2g
-Xmx2g

## Â§çÁé∞ÈóÆÈ¢ò
ÊöÇÊó†

### Ê≠•È™§
ÊöÇÊó†
### Ëß¶Âèë‰ª£Á†Å
    @Override
    public boolean load(ByteArray byteArray)
    {
        if (byteArray == null)
            return false;
        featureMap = new ImmutableFeatureMDatMap();
        featureMap.load(byteArray);
        int size = featureMap.size();
        TagSet tagSet = featureMap.tagSet;
        if (tagSet.type == TaskType.CLASSIFICATION)
        {
            parameter = new float[size];
            for (int i = 0; i < size; i++)
            {
                parameter[i] = byteArray.nextFloat();
            }
        }
        else
        {
            parameter = new float[size * tagSet.size()];
            for (int i = 0; i < size; i++)
            {
                for (int j = 0; j < tagSet.size(); ++j)
                {
                    parameter[i * tagSet.size() + j] = byteArray.nextFloat();
                }
            }
        }
//        assert !byteArray.hasMore();
//        byteArray.close();
        if (!byteArray.hasMore())
            byteArray.close();
        return true;
    }
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÈîôËØØÊó•Âøó‰ø°ÊÅØ
[2018-12-25T20:20:51,407][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [chechi] fatal error in thread [elasticsearch[chechi][masterService#updateTask][T#1]], exiting
java.lang.OutOfMemoryError: Java heap space
	at com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(IntArrayList.java:180) ~[?:?]
	at com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger.load(MutableDoubleArrayTrieInteger.java:1185) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:93) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70) ~[?:?]
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:95) ~[?:?]
	at com.hankcs.hanlp.HanLP.newSegment(HanLP.java:685) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper.lambda$getSegment$0(ConfigHelper.java:149) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper$$Lambda$2916/0x0000000800aed840.run(Unknown Source) ~[?:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at org.elasticsearch.plugin.hanlp.conf.ConfigHelper.getSegment(ConfigHelper.java:132) ~[?:?]
	at org.elasticsearch.plugin.hanlp.analysis.HanLPAnalyzerProvider.<init>(HanLPAnalyzerProvider.java:35) ~[?:?]
	at org.elasticsearch.plugin.hanlp.analysis.HanLPAnalyzerProvider.getPerceptronAnalyzerProvider(HanLPAnalyzerProvider.java:51) ~[?:?]
	at org.elasticsearch.plugin.hanlp.AnalysisHanLPPlugin$$Lambda$778/0x0000000800512840.get(Unknown Source) ~[?:?]
	at org.elasticsearch.index.analysis.AnalysisRegistry.buildMapping(AnalysisRegistry.java:367) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.analysis.AnalysisRegistry.buildAnalyzerFactories(AnalysisRegistry.java:191) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.analysis.AnalysisRegistry.build(AnalysisRegistry.java:160) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.IndexService.<init>(IndexService.java:165) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.index.IndexModule.newIndexService(IndexModule.java:397) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.indices.IndicesService.createIndexService(IndicesService.java:503) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:457) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService.validateAndAddTemplate(MetaDataIndexTemplateService.java:233) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService.access$300(MetaDataIndexTemplateService.java:64) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService$2.execute(MetaDataIndexTemplateService.java:174) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:639) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:268) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:198) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:133) ~[elasticsearch-6.5.4.jar:6.5.4]
	at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) ~[elasticsearch-6.5.4.jar:6.5.4]"
‰∏≠ÊñáAPI‰π±Á†ÅÔºåeclipse maven ÂØºÂÖ•ÂêéÊñπÊ≥ïÊñáÊ°£ÊòØ‰π±Á†Å,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

apiÊñáÊ°£ÊòØ‰π±Á†Å
Á±ª‰ºº‰∫éËøôÊ†∑Ôºö
ÊùûÓÑÄÂØ≤Ê∂ìÁÉòÂ´æÈóäÁ≠πÁ¥ôÊ££Ê†ßÁìßÂß£Â∂èÁ¥öParameters:text ÈèÇÂõ®Êπ∞separator ÈçíÂóõÊÆßÁªóÔøΩremainNone ÈèàÂ§âÁ∞∫ÁÄõÊ•ÅÁóÖÈèàÂ§ãÂ´æÈóäÁ≠πÁ¥ôÊø°ÂÇõÁà£ÈêêÁô∏Á¥öÈîõÂ±æÊß∏ÈçöÔ∏øÁπöÈê£Ê¨èÁï†Êµ†ÓÑäÁ¥ôÈê¢‚ï™oneÁêõ„Ñß„ÅöÈîõÔøΩReturns:Ê∂ìÔøΩÊ∂ìÓÅÑÁìßÁªóÔ∏øË¶ÜÈîõÂ≤ÄÊï±[Ê££Ê†ßÁìßÂß£Â≥ï[ÈçíÂóõÊÆßÁªóÓõÉ[Ê££Ê†ßÁìßÂß£Â≥ïÈèãÂã¨Âûö
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlpÊîØÊåÅword2vecÊ®°ÂûãËÆ≠ÁªÉÂêóÔºü, pyhanlpÊîØÊåÅword2vecÊ®°ÂûãËÆ≠ÁªÉÂêóÔºüË∞¢Ë∞¢ÔºÅ
‰ΩøÁî®ÁπÅÈ´îÂàÜË©ûÂæåÔºåÈ´ÆËÆäÁôº,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî® TraditionalChineseTokenizer.segment ‰æÜÂàÜË©û ""È£õÂà©Êµ¶Êï¥È´ÆÈÄ†ÂûãÂêπÈ¢®Ê¢≥""

### ÊúüÊúõËæìÂá∫

```
[È£õÂà©Êµ¶/ntc, Êï¥È´Æ/v, ÈÄ†Âûã/n, ÂêπÈ¢®/vn, Ê¢≥/v]
```

### ÂÆûÈôÖËæìÂá∫

```
[È£õÂà©Êµ¶/ntc, Êï¥Áôº/v, ÈÄ†Âûã/n, ÂêπÈ¢®/vn, Ê¢≥/v]
```

### ÂÖ∂‰ªñ‰ø°ÊÅØ
Áî®NLPTokenizer.analyze‰æÜÂàÜË©ûÂâáÊòØÊúüÊúõÁöÑËº∏Âá∫

"
ÊóßÁâàÁöÑdataÊñá‰ª∂404‰∫Ü,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

## ÊàëÁöÑÈóÆÈ¢ò

‰ªéreleasesÈ°µÈù¢‰∏≠ÔºåÊâæÂà∞v1.6.4Â∏∏ËßÑÁª¥Êä§Áâà, ÈáåÈù¢ÁöÑ data-for-1.6.4.zip ÈìæÊé•Â§±Êïà‰∫Ü, 404
"
ÁπÅÁÆÄËΩ¨Êç¢Âá∫Èîô,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

1.7masterÁâàÊú¨Ôºå‰ΩøÁî®ÁπÅÁÆÄËΩ¨Êç¢Ôºå‚Äú21ÂÖãÊãâ‚ÄùÔºåËΩ¨Êç¢ÁªìÊûú‰∏∫‚Äú21ÂÖãÊãâÂ∏É‚Äù„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1.7masterÁâàÊú¨Ôºå‰ΩøÁî®ÁπÅÁÆÄËΩ¨Êç¢Ôºå‚Äú21ÂÖãÊãâ‚ÄùÔºåËΩ¨Êç¢ÁªìÊûú‰∏∫‚Äú21ÂÖãÊãâÂ∏É‚Äù„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
    System.out.println(HanLP.convertToSimplifiedChinese(""21ÂÖãÊãâ""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
21ÂÖãÊãâ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
21ÂÖãÊãâÂ∏É
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊúÄÂ§ßÁÜµÊ®°ÂûãÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊ≤°ÊúâËÆ≠ÁªÉÊé•Âè£ÂêóÔºü‰ª£Á†Å‰∏≠Âè™ÁúãÂà∞Áõ¥Êé•Âä†ËΩΩËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïËÆæÁΩÆËá™ÂÆö‰πâËØçÂÖ∏ÊúÄÈ´ò‰ºòÂÖàÁ∫ßÁ∫ß ‰∏çÁÆ°ÂÖ∂‰ªñËØçÂàÜËØçÊïàÊûú,"ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP v1.2.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.7.0

## ÊàëÁöÑÈóÆÈ¢ò

Ëá™ÂÆö‰πâ‰∫Ü‰∏ÄÊâπËØçÔºåÂπ∂‰ΩøËØçÂÖ∑ÊúâÁªü‰∏ÄÁöÑËØçÊÄßÔºå‰ΩÜÊòØÈÉ®ÂàÜËØçÊú™ÊåâËá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØçÔºõÊàë‰∏çÈúÄË¶ÅËÄÉËôëÂÖ∂ÂÆÉÂàÜËØçÁöÑÂáÜÁ°ÆÊÄßÔºåÂè™Ë¶ÅÊ±ÇËá™ÂÆö‰πâÁöÑËØçË¢´ÂÖ®ÈÉ®ËØÜÂà´„ÄÇË∞¢Ë∞¢

    public void testIssue1234() throws Exception
    {
        CustomDictionary.insert(""‰∏≠ÂÖ≥ÊùëÁßëÊäÄ"",""company  10"");
        CustomDictionary.insert(""‰∫∫Ê∞ëÁΩë"",""company  10"");

       HanLP.newSegment().enableCustomDictionaryForcing(true);
        System.out.println(HanLP.segment(""‰∫∫Ê∞ëÁΩëÔºå‰∏≠ÂÖ≥ÊùëÁßëÊäÄÂõ≠Âå∫ÁÆ°ÁêÜÂßîÂëò‰ºöÂâØÂ∑°ËßÜÂëòÂàòËà™‰ªãÁªç‰∫Ü""));
    }

 ÊúüÊúõËæìÂá∫
[‰∫∫Ê∞ëÁΩë/company, Ôºå/w, ‰∏≠ÂÖ≥ÊùëÁßëÊäÄ/companyÔºåÂõ≠Âå∫/ns, ÁÆ°ÁêÜ/vn, ÂßîÂëò‰ºö/n, ÂâØ/b, Â∑°ËßÜÂëò/n, ÂàòËà™/nr, ‰ªãÁªç/v, ‰∫Ü/ul]

ÂÆûÈôÖËæìÂá∫
[‰∫∫Ê∞ëÁΩë/company, Ôºå/w, ‰∏≠ÂÖ≥Êùë/ns, ÁßëÊäÄÂõ≠Âå∫/ns, ÁÆ°ÁêÜ/vn, ÂßîÂëò‰ºö/n, ÂâØ/b, Â∑°ËßÜÂëò/n, ÂàòËà™/nr, ‰ªãÁªç/v, ‰∫Ü/ul]"
ËØ∑ÈóÆhanlpÊúâÊ≤°ÊúâÁ±ª‰ººjiebaÂàÜËØçÁöÑÂÖ®Ê®°Âºè,"ÊØîÂ¶ÇÔºö
‚ÄúÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶‚Äù
Áî®jiebaÂàÜËØçÔºö
„ÄêÂÖ®Ê®°Âºè„Äë: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçé/ Ê∏ÖÂçéÂ§ßÂ≠¶/ ÂçéÂ§ß/ Â§ßÂ≠¶
„ÄêÁ≤æÁ°ÆÊ®°Âºè„Äë: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçéÂ§ßÂ≠¶
ÂÖ®Ê®°ÂºèÊääÁâπÊÆäÂêçËØç‰πãÁ±ªÁöÑÂàáÂàÜÂá∫Êù•‰πãÂêéÔºåËøòÂèØ‰ª•ÁªßÁª≠ÂàáÂàÜ‰∏ãÂéªÔºåhanlpÊúâÁ±ª‰ººÁöÑÂäüËÉΩÂêó

"
HMMÁöÑÊú∫ÊûÑÂêçËØÜÂà´ÁöÑËá™Âä®ËΩ¨Êç¢Á®ãÂ∫èÁîüÊàêÁöÑËßÑÂàô‰∏≤Áº∫Â§±,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0ÔºàmasterÔºâ

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÁõÆÂâçÈúÄË¶ÅËØÜÂà´ÊòØ1ÂçÉ‰∏áÁöÑÂÖ¨Âè∏ÂêçÔºå‰ΩøÁî®‰∏Ä‰∏ã‰ª£Á†ÅÊèêÂèñ‰πãÂêéÔºö
`       
 
        EasyDictionary dictionary = new EasyDictionary();
        final NTDictionaryMaker ntDictionaryMaker = new NTDictionaryMaker(dictionary);
        CorpusLoader.walk(""/home/gildata/cheny/company/"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(Document document)
            {
                ntDictionaryMaker.compute(document.getComplexSentenceList());
            }
        });
        ntDictionaryMaker.saveTxtTo(""/home/gildata/cheny/nt"");

`
ÂèëÁé∞ËÆ≠ÁªÉÁîüÊàêÁöÑnt.pattern.txtÂÖ±19wÂ∑¶Âè≥ÁöÑÔºå‰ΩÜÊòØÁªèËøáÊµãËØïËøòÁº∫Â∞ëÊ®°Âºè‰∏≤ÔºåÂØºËá¥1000wÊï∞ÊçÆ‰∏≠Êúâ123wÁöÑÂÖ¨Âè∏ÂêçÊ≤°ÊúâËØÜÂà´Âá∫Êù•„ÄÇ

### ÊúüÊúõËæìÂá∫

ËÆ≠ÁªÉÁîüÊàêÁöÑnt.pattern.txtËÉΩÂ§üÂåÖÂê´ÊâÄÊúâÁöÑËßÑÂàôÊ®°Âºè‰∏≤

Ë∞¢Ë∞¢ÂêÑ‰ΩçÂ§ßÂ§ß

"
ËÉΩÂê¶Êü•ÊâæÊüê‰∏Ä‰∏™ËØçËØ≠ÁöÑ‰πâÈ°πÊï∞Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP 1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
  ÊàëÊÉ≥ËÆ°ÁÆóÊñáÁ´†ÁöÑ‰∏ÄËØçÂ§ö‰πâÊØîÁéáÔºåÈúÄË¶ÅËé∑Âæó‰∏§‰∏™ÂÄº„ÄÇ‰∏Ä‰∏™ÊòØËØçËØ≠ÁöÑÊâÄÊúâ‰πâÈ°πÊï∞‰πãÂíåÔºåÂè¶‰∏Ä‰∏™ÊòØËØçËØ≠Êï∞„ÄÇÂêéËÄÖÂèØ‰ª•ÈÄöËøáÂàÜËØçËΩªÊùæËé∑ÂæóÔºå‰ΩÜÂâçËÄÖ‰∏çÁü•HanLPËÉΩÂê¶ÂÆûÁé∞Ôºü‰πüÂ∞±ÊòØËØ¥ÔºåÊàëÊÉ≥ËæìÂÖ•‰∏Ä‰∏™ËØçËØ≠ÔºåËé∑ÂæóËøô‰∏™ËØçËØ≠ÁöÑÊâÄÊúâÊÑèÊÄù‰∏™Êï∞„ÄÇËØ∑ÈóÆHanLPÊúâËøô‰∏™ÂäüËÉΩÂêóÔºü

Â¶ÇÊûúÊ≤°ÊúâÈõÜÊàêËøô‰∏™ÂäüËÉΩÔºåËØ∑ÈóÆÂ¶Ç‰ΩïÂÆûÁé∞Ëøô‰∏™ÂäüËÉΩÂë¢ÔºüÊâæ‰∫ÜÂæà‰πÖÔºå‰πüÊ≤°ÊúâÊâæÂà∞Â≠òÊúâ‰πâÈ°πÊï∞ÁöÑËØçÂÖ∏„ÄÇ
Ë∞¢Ë∞¢ÔºÅ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
Êó†

### Ê≠•È™§

Êó†

### Ëß¶Âèë‰ª£Á†Å

```
   Êó†
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```ËæìÂÖ•‰∏Ä‰∏™ËØçËØ≠ÔºåËé∑ÂæóËøô‰∏™ËØçËØ≠ÁöÑÊâÄÊúâÊÑèÊÄù‰∏™Êï∞

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫Êó†
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
getAccuracyÁöÑËøîÂõûÂÄº‰∏™Êï∞Â¶Ç‰ΩïÊéßÂà∂Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ËØ∑Êïô‰∏Ä‰∏ãÊ®°ÂûãËÆ≠ÁªÉÔºå‰πüÂ∞±ÊòØPerceptronTrainerÈáåÈù¢ÁöÑgetAccuracyÂáΩÊï∞‰ΩïÊó∂‰ºöËøîÂõû‰∏Ä‰∏™ÂáÜÁ°ÆÂ∫¶pÔºå‰ΩïÊó∂Âèà‰ºöËøîÂõû‰∏â‰∏™ÂÄºp„ÄÅr„ÄÅf„ÄÇËøô‰∏™ËøîÂõûÊòØÂíåËØ≠ÊñôÊúâÂÖ≥ÂêóÔºüËØ∑ÈóÆÊúâÊòéÁ°ÆÁöÑÊ†áÂáÜ‰ΩøÂæóËÆ≠ÁªÉÁªìÊûúËÉΩÂæóÂà∞‰∏Ä‰∏™pÂÄºÊàñÂæóÂà∞3‰∏™ÂÄºÂêóÔºüË∞¢Ë∞¢ÔºÅ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

"
Âè•Â≠ê‰∏≠Ê†áÁÇπÂâçÂ¶ÇÊúâÁ©∫Ê†º seg2sentence ‰ºöÂàÜÂâ≤‰∏∫‰∏ÄÂè•ËØù,"ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP 1.7.0 pyhanlp 0.1.44
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP 1.7.0 pyhanlp 0.1.44

## ÊàëÁöÑÈóÆÈ¢ò

Â¶ÇÈ¢ò Â¶ÇÊûúÂè•Â≠ê‰∏≠ÁöÑÊ†áÁÇπÔºàÊó†ËÆ∫‰∏≠Ëã±ÊñáÔºâÂâçÊúâÁ©∫Ê†ºÊó∂ seg2sentence ‰ºöÂ∞ÜÂÖ∂ÂàÜÂâ≤‰∏∫‰∏ÄÂè•ËØù Êï∞Â≠ó‰πüÊòØ
Âè¶Â§ñÂ¶ÇÊûúÂè•Â≠ê‰∏≠ÊúâËã±ÊñáÁ≠âÈùû‰∏≠Êñá ‰∏îÂçïËØçÂíåÊ±âÂ≠óÈó¥ÊúâÁ©∫Ê†ºÁöÑËØù ‰πü‰ºöË¢´ÂàÜÂâ≤‰∏∫‰∏ÄÂè•ËØù
ËôΩÁÑ∂‰∏≠ÊñáÊ†áÁÇπÂíåÊ±âÂ≠ó‰πãÈó¥ÊåâÊ†áÂáÜÁöÑÊéíÁâàÊù•ËØ¥ËÇØÂÆöÊòØ‰∏çËÉΩÂä†Á©∫Ê†ºÁöÑ ‰ΩÜÊØïÁ´ü‰∏çÊòØÊâÄÊúâÊñáÊú¨ÈÉΩ‰ºöÈÅµÂÆà
ËÄå‰∏îÂ¶ÇÊûú‰∏≠ÊñáÂè•Â≠êÈáåÂê´ÊúâËã±Êñá Êúâ‰∫∫ÊéíÁâà‰∏äÊòØ‰π†ÊÉØÂú®Ëã±ÊñáÂçïËØçÂíåÊ±âÂ≠óÁöÑË°îÊé•Â§ÑÂä†‰∏Ä‰∏™Ê±âÂ≠óÁöÑ Âè¶Â§ñÂ¶ÇÊûú‰∏≠ÊñáÂè•Â≠êÈáåÂê´ÊúâËøûÁª≠ÁöÑ‰∏§‰∏™Êàñ‰∏§‰∏™‰ª•‰∏äÁöÑËã±ÊñáÔºàÊàñÂÖ∂‰ªñÂç∞Ê¨ßËØ≠Á≥ªÁöÑËØ≠ÁßçÔºâÂçïËØç ÈÇ£‰πàÂÖ∂‰∏≠ÂøÖÂÆöÂåÖÂê´Á©∫Ê†º„ÄÇ

Âè¶Â§ñÊàëÁúã[Ëøô‰∏™ Commit](https://github.com/hankcs/HanLP/commit/1094563c0f13fb1438cc1f9f155a88351e38ac8c) ËØ¥‰øÆÂ§ç‰∫ÜÊàëÊèêÁöÑ #1019 ‰ΩÜÊàëËØï‰∫Ü‰∏ã
`for s in standard_tokenizer.seg2sentence('ËøôÊòØ‰∏Ä‰∏™Âè•Â≠êÔºå‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ', shortest = False): print(s)`

ÁªìÊûúËøòÊòØ‰∏çÂØπ ÂÆûÂú®ÊòØ‰∏ç‰ºö Java ËøôÂá†‰∏™ÈáçËΩΩÁöÑÂáΩÊï∞Â§¥ÈÉΩÊôï‰∫Ü

ËøòÊúâÁî®ÂàÜËØçÂô®ÁöÑ seg2sentence ÂáΩÊï∞‰ºöËøõ‰∏ÄÊ≠•ÊääÂè•Â≠êÂÜçÂàáÂàÜ‰∏∫ token ÁöÑÂàóË°® ËøôÊ†∑ÊàëÊÉ≥Ë¶ÅÂè•Â≠êÁöÑÂ≠óÁ¨¶‰∏≤ËøòË¶ÅÊâãÂä®ÊãºÂõûÂéª
‰ΩÜÊòØ‰∏äÈù¢ÊàëËØ¥ÁöÑÈóÆÈ¢ò ‰∏≠ÊñáÈáåÂê´ÊúâËã±ÊñáÊó∂ Â∞±ÁÆóÂàáÂâ≤Ê≠£Á°Æ Ëã±ÊñáÂíåÊï∞Â≠óÈÉΩ‰ºöË¢´ÂàáÊàêÁã¨Á´ãÁöÑ token ËøôÊ†∑ÊàëÊãºÂõûÂéªËøòÂæóÂÜçÂà§Êñ≠‰∏Ä‰∏ã ÂæàÈ∫ªÁÉ¶
com.hankcs.hanlp.utility.SentencesUtil.toSentenceList Áõ¥Êé•Â∞±ÊòØËøîÂõûÊØèÂè•Âè•Â≠êÁöÑÂ≠óÁ¨¶‰∏≤ Ëøô‰∏™Êé•Âè£ËÉΩÁõ¥Êé•Âè•Â≠êÁ∫ßÂà´ÁöÑÂàÜÂâ≤ÂêóÔºüÔºà‰∏çÂú®ÈÄóÂè∑Êñ≠ÂºÄÔºâ

ÊúÄÂêéÂº∫Ëø´ÁóáË°®Á§∫ pyhanlp È°πÁõÆÁöÑÂëΩÂêçÊòØÂê¶Â∫îËØ•ËßÑËåÉ‰∏∫ PyHanLP ÊØîËæÉÂ•Ω Âíå HanLP ÂØπÂ∫î„ÄÇ„ÄÇ„ÄÇÔºàÊàëÊåá ReadMe Âíå PyPI ÈáåÁöÑÊ†áÈ¢ò Package ÂêçÂÄíÊòØÊó†ÊâÄË∞ìÔºâ

### Ëß¶Âèë‰ª£Á†Å

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')

for s in standard_tokenizer.SEGMENT.seg2sentence('ÊàëÁî®‰∏çÊù• JavaÔºåÂè™‰ºö Python„ÄÇ', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('‰ªäÂ§©Â§©Ê∞îÁúüÂ•Ω ÔºàÂÆúÂá∫Ë°å * Â∑¶Êã¨Âè∑ÂâçÊúâÁ©∫Ê†ºÔºâ', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('pyhanlp ÈúÄË¶Å Java 1.8+ ÊâçËÉΩËøêË°å„ÄÇ', False):
	print(s)
for s in standard_tokenizer.SEGMENT.seg2sentence('‰Ω†Áü•ÈÅìgangnam styleÂêó„ÄÇ', False):
	print(s)
```
### ÊúüÊúõËæìÂá∫

```
[Êàë/rr, Áî®/p, ‰∏çÊù•/v, Java/nx, Ôºå/w, Âè™/d, ‰ºö/v, Python/nx, „ÄÇ/w]
[‰ªäÂ§©Â§©Ê∞î/nz, ÁúüÂ•Ω/d, Ôºà/w, ÂÆú/ag, Âá∫Ë°å/vi, */w, Â∑¶/f, Êã¨Âè∑/n, Ââç/f, Êúâ/vyou, Á©∫Ê†º/n, Ôºâ/w]
[pyhanlp/nx, ÈúÄË¶Å/v, Java/nx, 1.8/m, +/w, ÊâçËÉΩ/n, ËøêË°å/vn, „ÄÇ/w]
[‰Ω†/rr, Áü•ÈÅì/v, gangnam/nx, style/nx, Âêó/y, „ÄÇ/w]
```

### ÂÆûÈôÖËæìÂá∫

```
[Êàë/rr, Áî®/p, ‰∏çÊù•/v]
[Java/nx, Ôºå/w, Âè™/d, ‰ºö/v]
[Python/nx, „ÄÇ/w]

[‰ªäÂ§©Â§©Ê∞î/nz, ÁúüÂ•Ω/d]
[Ôºà/w, ÂÆú/ag, Âá∫Ë°å/vi]
[*/w]
[Â∑¶/f, Êã¨Âè∑/n, Ââç/f, Êúâ/vyou, Á©∫Ê†º/n, Ôºâ/w]

[pyhanlp/nx]
[ÈúÄË¶Å/v]
[Java/nx]
[1.8/m, +/w]
[ÊâçËÉΩ/n, ËøêË°å/vn, „ÄÇ/w]

[‰Ω†/rr, Áü•ÈÅì/v, gangnam/nx]
[style/nx, Âêó/y, „ÄÇ/w]
```
"
Â¶ÇÊûúËØÜÂà´ÊâÄÊúâÁöÑÂïÜÂìÅÂìÅÁâåÔºåÂèØ‰ª•Áî®‰ªÄ‰πàÊñπÂºèÂÆûÁé∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0ÔºàmasterÔºâ

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÁõÆÂâçÈúÄË¶ÅËØÜÂà´ÊâÄÊúâÁöÑÂìÅÁâåÂêçÁß∞ÔºåÊúâËøë‰∫øÊù°ÔºåËøô‰∫õÂìÅÁâåÂêçÁß∞Êï∞ÊçÆÈÉΩÊúâÔºåÂõ†‰∏∫Êï∞ÊçÆÈáèÂ§™Â§öÔºåÊâÄÊúâ‰∏çÂèØËÉΩÂä†ËΩΩÂà∞Ëá™ÂÆö‰πâËØçÂ∫ì‰∏≠ÔºåËÄå‰ªñÂèà‰∏çÂÉè‰∫∫ÂêçÂíåÂÖ¨Âè∏ÂêçÈÇ£Ê†∑ÊòØÊúâ‰∏ÄÂÆöÁöÑËßÑÂàôÁöÑÔºåÊâÄ‰ª•ÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÔºåÊúâ‰ªÄ‰πàÂÖ∂‰ªñÂèØ‰ª•ÂÆûÁé∞ÁöÑË∑ØÂæÑÂêó


"
ÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®HanLPÁî±ÁπÅ‰ΩìËΩ¨Êç¢‰∏∫ÁÆÄ‰ΩìÊó∂Ôºå‚Äú‰∏ÄÂÖãÊãâ‚Äù‰∏∫ËΩ¨Êç¢‰∏∫‚Äú‰∏ÄÂÖãÊãâÂ∏É‚Äù
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§


### Ëß¶Âèë‰ª£Á†Å

```
    import com.hankcs.hanlp.HanLP;

/**
 * Â∞ÜÁÆÄÁπÅËΩ¨Êç¢ÂÅöÂà∞ÊûÅËá¥
 *
 * @author hankcs
 */
public class DemoTraditionalChinese2SimplifiedChinese
{
    public static void main(String[] args)
    {
        System.out.println(HanLP.convertToSimplifiedChinese(""‰∏ÄÂÖãÊãâ""));
    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
‚Äú‰∏ÄÂÖãÊãâ‚Äù
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
‚Äú‰∏ÄÂÖãÊãâÂ∏É‚Äù
## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÊâÄÊúâÁöÑÁâàÊú¨ÈÉΩÂ≠òÂú®Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ
Âè¶Â§ñ‚Äú‰πæÈöÜ‚ÄùË¢´ËΩ¨Êç¢‰∏∫‚ÄúÂπ≤ÈöÜ‚ÄùÔºåÊòéÊòü‚ÄúÂº†ÈíßÁîØ‚Äù‰∫¶Ë¢´ËΩ¨Êç¢‰∏∫‚ÄúÂº†ÈíßÂÆÅ‚Äù
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ΩøÁî®pyhanlpËøõË°åÂàÜËØçÊó∂ÊÑèÂ§ñÂÅúÊ≠¢Êä•ÈîôProcess finished with exit code -1073740940 (0xC0000374),"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËØ≠ÊñôÂ∫ìÊòØwikiÁπÅ‰ΩìÊúÄÊñ∞ÁâàÊú¨
Âú®‰∏§Ê¨°Êìç‰ΩúÁöÑÊó∂ÂÄôÈÉΩ‰ºöÊä•Èîô
1.ÁπÅ‰ΩìËΩ¨ÁÆÄ‰Ωì
2.‰ΩøÁî®ÊÑüÁü•Êú∫ÂàÜËØç
ÈÉΩÊòØËøêË°åÂà∞Á¨¨57Ë°åÊä•Process finished with exit code -1073740940 (0xC0000374)

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

# coding=utf8
from opencc import OpenCC
import codecs,sys
from pyhanlp import *
'''
‰ΩøÁî®ÊÑüÁü•Êú∫ËøõË°åÂàÜËØçÔºåÈúÄË¶ÅË∞ÉÁî®
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
seg=analyzer.seg(sentence)
'''
def cut_words(sentence):
    #print sentence
    # seg=HanLP.segment(sentence)
    PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
    analyzer = PerceptronLexicalAnalyzer()
    seg=analyzer.seg(sentence)
    print(seg)
    print(type(seg))
    list=[]
    for s in seg:
        list.append(s.word)
    condition = lambda t: t != "" ""
    new_list = filter(condition, list)
    after_cut = "" "".join(new_list)
    return after_cut

filepath=r'C:\Users\01\Desktop\„Äê„ÄëNLP\[01]learning-file\wiki.zh.text'
f=codecs.open('wiki.zh.jian2.txt','r',encoding=""utf8"")
target = codecs.open(""wiki.zh.seg.txt"", 'w',encoding=""utf8"")
line_num=1
line = f.readline()

while line:
    print('---- processing ', line_num, ' article----------------')
    line_seg=cut_words(line)
    target.writelines(line_seg)
    print(line_seg)
    line_num = line_num + 1
    line = f.readline()
f.close()
target.close()
exit()




"
Merge pull request #1 from hankcs/master,"Á¨¨‰∏ÄÊ¨°‰ªéÊ∫êÂçáÁ∫ß

"
classpathÊñá‰ª∂Â§πÂ∏¶‰∏≠ÊñáÊÄé‰πàÂ§ÑÁêÜÔºü,"ÈáçÂÜô‰∫Ü ‰ªéclasspath‰∏≠Ëé∑ÂèñË∑ØÂæÑ 
public class SimpleClassPathIIOAdapter implements IIOAdapter {
	/**
	 * ÊâìÂºÄ‰∏Ä‰∏™Êñá‰ª∂‰ª•‰æõËØªÂèñ
	 *
	 * @param path
	 *            Êñá‰ª∂Ë∑ØÂæÑ
	 * @return ‰∏Ä‰∏™ËæìÂÖ•ÊµÅ
	 * @throws IOException
	 *             ‰ªª‰ΩïÂèØËÉΩÁöÑIOÂºÇÂ∏∏
	 */
	@Override
	public InputStream open(String path) throws IOException {
		return new FileInputStream(this.getClass().getClassLoader().getResource(path).getFile());
	}

	/**
	 * ÂàõÂª∫‰∏Ä‰∏™Êñ∞Êñá‰ª∂‰ª•‰æõËæìÂá∫
	 *
	 * @param path
	 *            Êñá‰ª∂Ë∑ØÂæÑ
	 * @return ‰∏Ä‰∏™ËæìÂá∫ÊµÅ
	 * @throws IOException
	 *             ‰ªª‰ΩïÂèØËÉΩÁöÑIOÂºÇÂ∏∏
	 */
	@Override
	public OutputStream create(String path) throws IOException {
		return new FileOutputStream(this.getClass().getClassLoader().getResource(path).getFile());
	}
}
WARN  HanLP - ËØªÂèñhanlp/data/dictionary/other/CharType.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: E:\%e5%85%ac%e5%8f%b8\%e9%87%8d%e6%9e%84\%e5%90%8e%e5%8f%b0%e6%ba%90%e7%a0%81\common\target\classes\hanlp\data\dictionary\other\CharType.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ)

Áî®ÁöÑspringboot Êñá‰ª∂Â§πÂ∏¶‰∏≠ÊñáÊÄé‰πàÂ§ÑÁêÜÔºü
"
Êñ∞Â¢ûÂèØËá™ÂÆö‰πâÁî®Êà∑ËØçÂÖ∏ÁöÑÁª¥ÁâπÊØîÂàÜËØçÂô®,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÈÄöËøáÊâ©Â±ïÁöÑÁª¥ÁâπÊØîÂàÜËØçÂô®ÔºåÂèØ‰ª•ÂÆåÂÖ®Ëá™‰∏ªËÆæÁΩÆÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Ôºõ
Â¶ÇÊûúÁî®Êà∑‰∏çËÆæÁΩÆÔºåÂàôÈªòËÆ§‰ΩøÁî®ÂÖ®Â±ÄÁî®Êà∑ËØçÂÖ∏Ôºõ
Â¶ÇÊûúÁî®Êà∑ÊåáÂÆö‰∫ÜËØçÂÖ∏Ë∑ØÂæÑÔºåÂàôÊåâÁÖßÁî®Êà∑ËÆæÁΩÆÁöÑË∑ØÂæÑÂä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏Ôºå‰∏çÂêåÂÆû‰æãÂèØ‰ª•‰ΩøÁî®‰∏çÂêåËØçÂÖ∏Ôºõ
ËØçÂÖ∏ÈªòËÆ§ËøõË°åÁºìÂ≠òÔºå‰∏îËØçÂÖ∏‰øÆÊîπÂêé‰ºöËá™Âä®ÈáçÊñ∞ÊûÑÂª∫„ÄÇ"
import pyhanlpÂêéjupyter notebookÊó†Ê≥ïprint,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöÊúÄÊñ∞Áâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
jupyter notebook‰∏≠import pyhanlpÂêéÊó†Ê≥ïprintÂá∫‰ªª‰ΩïÂÜÖÂÆπ


"
ÊÑüÁü•Êú∫‰∏≠ÂèØÂèòDATÁöÑentrySetÊñπÊ≥ïÊúâbug,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöÊúÄÊñ∞Áâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÑüÁü•Êú∫‰∏≠‰ΩøÁî®ÂèØÂèòDATÂä†ËΩΩÁâπÂæÅÈõÜÂêàÔºåÂä†ËΩΩlarge/cws.binÔºåÂú®Ë∞ÉËØïËøáÁ®ã‰∏≠ÂèëÁé∞
```
 System.out.println(featureMap.size());
 ËæìÂá∫ÂÄº‰∏∫8021239
```
‰ΩÜÊòØÈÄöËøá
```
 Set<String> set = new HashSet<String>();

Set<Map.Entry<String, Integer>> entries = featureMap.entrySet();

for (Map.Entry<String, Integer> m : entries) {
                    set.add(m.getKey());
 }
```
ÊéíÈô§ÈáçÂ§çÂêéset.size() == 8021206 Â∞ëÊéâ‰∫Ü33‰∏™key„ÄÇ
ÊØîÂ¶Ç ""\u0001/\u00014"" Ëøô‰∏™keyÔºå
ÈÄöËøáfeatureMap.idOf(""\u0001/\u00014"") ÂèØ‰ª•ËøîÂõûidÊòØ8
‰ΩÜÊòØfeatureMap.entrySet()‰∏≠Âπ∂ÂåÖÊã¨Ëøô‰∏™id=8ÁöÑÈ°π„ÄÇ
‰πüÂ∞±ÊòØid=8Âπ∂Ê≤°ÊúâË¢´ÊÅ¢Â§çÂá∫Êù•„ÄÇ


"
ËÉΩÂê¶Ëá™ÂÆö‰πâÊ®°ÊùøÂàÜËØçÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÊÉ≥Ë¶ÅËá™ÂÆö‰πâÊ®°ÊùøÊù•ÂàÜËØç„ÄÇ‰æãÂ¶Ç
Âè•Â≠ê‚Äú2018Âπ¥9Êúà1Êó•Ââç‚Äù
Â¶ÇÊûúÁî®hanlpÊ†áÂáÜÂàÜËØçÔºåÂàô 2018Âπ¥/9Êúà/1/Êó•Ââç
Â¶ÇÊûúÁî®crfÂàÜËØç,Âàô 2018Âπ¥/9Êúà/1Êó•/Ââç
ËôΩÁÑ∂crfÂàÜËØçÊ≠£Á°Æ‰∫ÜÔºå‰ΩÜÊòØÂú®ÊàëÁöÑ‰ªªÂä°‰∏≠ÔºåÊ†áÂáÜÂàÜËØçÁöÑÊïàÊûúÊõ¥ÊúâÂ∏ÆÂä©„ÄÇ

**ÊâÄ‰ª•ÊàëÊÉ≥ÈóÆËÉΩ‰∏çËÉΩ Âú®Ê†áÂáÜÂàÜËØçÊó∂ÔºåËá™ÂÆö‰πâÊ≠£ÂàôÊ®°ÊùøÔºåÈíàÂØπ ‚ÄòÊó•Ââç‚Äò Êù•ÂÅöÁâπÊÆäÂàÜËØçÔºü**
ÂΩìÁÑ∂ÊàëËØ¥ÁöÑÊÉÖÂÜµÔºå‰∏çÊòØ‰ªÖ‰ªÖÈíàÂØπ ‚ÄòÊó•Ââç‚ÄôËøô‰πà‰∏Ä‰∏™ËØçÊ±áÔºåÊòØÊåáËøôÁ±ªÂäüËÉΩ„ÄÇ
Ë∞¢Ë∞¢

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlpÂ¶Ç‰Ωï‰∏∫ÂàÜËØçÊ®°ÂûãÊ∑ªÂä†ËØ≠ÊñôËÆ≠ÁªÉ ‰ª•Âèä ÊòØÂê¶ÂèØ‰ª•Âü∫‰∫éÈüµÂæãËøõË°åÂàÜËØçÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP v1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP v1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÁõÆÂâçÊÉ≥ÂØπÂè•Â≠êÂú®ÈüµÂæãÂ±ÇÁ∫ßÁöÑÂàÜËØçÔºåÊØîÂ¶Ç‰∏ÄÂ±ÇÈüµÂæãËØçÔºå‰∫åÂ±ÇÈüµÂæãÁü≠ËØ≠Ôºå‰∏âÂ±ÇËØ≠Ë∞ÉÁü≠ËØ≠ÔºåÂõõÂ±ÇÂè•Êú´„ÄÇ
ÈóÆÈ¢òÊúâ‰∏âÔºö
ÂÖ∂‰∏ÄÔºåËØ∑ÈóÆÂü∫‰∫épythonÁöÑpyhanlpÂ¶Ç‰Ωï‰∏∫ÂàÜËØçÊ®°ÂûãÊ∑ªÂä†È¢ùÂ§ñÁöÑËØçÂÖ∏Ôºü
ÂÖ∂‰∫åÔºåËØ∑ÈóÆÂ¶Ç‰ΩïÂú®pyhanlp‰∏ãÊ∑ªÂä†È¢ùÂ§ñËØ≠ÊñôËøõË°åËÆ≠ÁªÉÔºü
ÂÖ∂‰∏âÔºåËØ∑ÈóÆÂØπ‰∫éÈüµÂæãÂ±ÇÁ∫ßÁöÑÂàÜËØçÔºåÊÇ®Êúâ‰ΩïÂª∫ËÆÆÊàñËÄÖÊåáÁÇπÂêóÔºü

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->



"
Â¶Ç‰ΩïÈÄöËøáËÆ≠ÁªÉËá™Â∑±ÁöÑÈ¢ÑÊñôÊèêÈ´òÊñáÊú¨ËÅöÁ±ªÂáÜÁ°ÆÂ∫¶,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->Â¶Ç‰ΩïÈÄöËøáËÆ≠ÁªÉËá™Â∑±ÁöÑÈ¢ÑÊñôÔºåÊàñËÄÖË∞ÉËäÇ‰ªÄ‰πàÂèÇÊï∞ÔºåÂèØ‰ª•ÊèêÈ´òÊñáÊú¨ËÅöÁ±ªÁöÑÂáÜÁ°ÆÂ∫¶Ôºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÊñáÊú¨ËÅöÁ±ª
2. ÁªüËÆ°ÂáÜÁ°ÆÂ∫¶

### Ëß¶Âèë‰ª£Á†Å

```
   
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ê¨¢ËøéÂèÇÂä†HanLPÁ∫ø‰∏ãÊäÄÊúØ‰∫§ÊµÅ‰ºö,"ÊÑüË∞¢Â§ßÂÆ∂ÂØπHanLPÁöÑÊîØÊåÅÔºåÊ¨¢ËøéÂèÇ‰∏é12Êúà7Âè∑‰∫éÈùíÂ≤õ‰∏æÂäûÁöÑHanLP‰∏ªÈ¢òËÆ∫Âùõ„ÄÇËøôÊ¨°ËÆ∫ÂùõÁî±Â§ßÂø´ÊêúÁ¥¢‰∏æÂäûÔºåÂú®Áæé‰∏ΩÁöÑÊµ∑Êª®ÂüéÂ∏ÇÂ¥ÇÂ±±‰∏æË°å„ÄÇ‰∏ªÈ¢òÂåÖÊã¨HadoopËß£ÂÜ≥ÊñπÊ°àDKH„ÄÅFreeRCH2ÂíåHanLP„ÄÇ‰ºöÂêéÊúâÂ¥ÇÂ±±Áà¨Â±±„ÄÅÊ∏∏Âõ≠ÂíåÂÜúÂÆ∂ÂÆ¥Á≠âÊ¥ªÂä®„ÄÇÂÆæËá≥Â¶ÇÂΩíÔºåÂÖ¨Âè∏Ë¥üË¥£‰ºöËÆÆÊúüÈó¥ÁöÑ‰ΩèÂÆøÂíåÈ§êÈ£ü„ÄÇ

HanLP‰∏ªÈ¢òËÆ∫ÂùõÂ∞Ü‰ªãÁªçHanLP1.7ÁöÑÊäÄÊúØÂπ≤Ë¥ßÔºåÂπ∂‰∏î‰ºöÂèëÂ∏ÉHanLP2.0ÁöÑÊ∂àÊÅØ„ÄÇÁé∞Âú®**ÂæÅÈõÜ‰∏§‰ΩçÂºÄÂèëËÄÖ**Ôºà‰∏™‰∫∫ÊàñÂÖ¨Âè∏ÔºâÂàÜ‰∫´Ëá™Â∑±ÁöÑÂºÄÂèëÊ°à‰æãÔºåÊ¨¢ËøéÂ§ßÂÆ∂Ë∏äË∑ÉÊä•Âêç„ÄÇËØ¶ÁªÜÁöÑ‰ªãÁªçËØ∑ÂèÇËÄÉ[
‰ºöËÆÆ‰ªãÁªç](https://github.com/hankcs/HanLP/files/2617987/2018.docx)Âíå[ÈÇÄËØ∑ÂáΩ](https://github.com/hankcs/HanLP/files/2617988/2018.docx)„ÄÇ

ÊÑüÂÖ¥Ë∂£ÁöÑÊúãÂèãÂèØ‰ª•Áõ¥Êé•Áïô‰∏ãËÅîÁ≥ªÊñπÂºèÔºå‰πüÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ãÊñπÂºèÂí®ËØ¢‰∏ªÂäûÊñπÔºö

ËÅî Á≥ª ‰∫∫ÔºöËµµÈáëÈπè
ËÅîÁ≥ªÁîµËØùÔºö15554211163
Â∞äÂ∏≠ËØöÂæÖÔºåÁâπÊ≠§ÁõõÈÇÄÔºÅ"
DemoPerceptronLexicalAnalyzer Âä†ËΩΩ‰∏çÂà∞cws.bin Êñá‰ª∂ Êñá‰ª∂Ë∑ØÂæÑÊ≠£Á°Æ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-protable-1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-protable-1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

mavenÊûÑÂª∫hanlpÈ°πÁõÆ

### Ëß¶Âèë‰ª£Á†Å

```
      PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(""data/model/perceptron/pku199801/cws.bin"",
                                                                           HanLP.Config.PerceptronPOSModelPath,
                                                                           HanLP.Config.PerceptronNERModelPath);
        System.out.println(analyzer.analyze(""‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏Ëë£‰∫ãÈïøË∞≠Êó≠ÂÖâÂíåÁßò‰π¶ËÉ°Ëä±ËïäÊù•Âà∞ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶ÜÂèÇËßÇ""));
        System.out.println(analyzer.analyze(""ÂæÆËΩØÂÖ¨Âè∏Êñº1975Âπ¥Áî±ÊØîÁàæ¬∑ËìãËå≤Âíå‰øùÁæÖ¬∑ËâæÂÄ´ÂâµÁ´ãÔºå18Âπ¥ÂïüÂãï‰ª•Êô∫ÊÖßÈõ≤Á´Ø„ÄÅÂâçÁ´ØÁÇ∫Â∞éÂêëÁöÑÂ§ßÊîπÁµÑ„ÄÇ""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÔºàÈõÜÂõ¢ÔºâÂÖ¨Âè∏Ëë£‰∫ãÈïøË∞≠Êó≠ÂÖâÂíåÁßò‰π¶ËÉ°Ëä±ËïäÊù•Âà∞ÁæéÂõΩÁ∫ΩÁ∫¶Áé∞‰ª£Ëâ∫ÊúØÂçöÁâ©È¶ÜÂèÇËßÇ ÁöÑÂàÜËØçÁªìÊûú
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

ÂçÅ‰∏ÄÊúà 27, 2018 9:54:53 ‰∏äÂçà com.hankcs.hanlp.corpus.io.ByteArrayStream createByteArrayStream
Ë≠¶Âëä: ÊâìÂºÄÂ§±Ë¥•Ôºödata/model/perceptron/pku199801/cws.bin
Exception in thread ""main"" java.io.IOException: data/model/perceptron/pku199801/cws.bin Âä†ËΩΩÂ§±Ë¥•
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:390)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70)
	at com.hankcs.demo.DemoPerceptronLexicalAnalyzer.main(DemoPerceptronLexicalAnalyzer.java:34)


## ÂÖ∂‰ªñ‰ø°ÊÅØ

"
Â∑≤ÁªèÊàêËØçÁöÑËØç‰∏çËÉΩÈÄöËøálearnÊãÜÂàÜ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âåó‰∫¨ÁôæÂ∫¶ÁΩëËÆØÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ÔºåÁî®ÊÑüÁü•Êú∫ÂàÜËØçÊ∞∏ËøúÈÉΩÊòØ‰∏Ä‰∏™ËØçÔºåÁî®learnÊâãÂä®ÂàÜÂºÄÊó¢ÁÑ∂Ê≤°Áî®
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
   analyzer.learn(""Âåó‰∫¨ ÁôæÂ∫¶ ÁΩëËÆØ ÁßëÊäÄ ÊúâÈôêÂÖ¨Âè∏"")
   analyzer.analyze(""Âåó‰∫¨ÁôæÂ∫¶ÁΩëËÆØÁßëÊäÄÊúâÈôêÂÖ¨Âè∏"");
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Âåó‰∫¨/ns ÁôæÂ∫¶/nz ÁΩëËÆØ/n ÁßëÊäÄ/n ÊúâÈôêÂÖ¨Âè∏/n]/nt
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Âåó‰∫¨ÁôæÂ∫¶ÁΩëËÆØÁßëÊäÄÊúâÈôêÂÖ¨Âè∏/nt
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊÑüÁü•Êú∫Êä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1.ËØ≠ÊñôÈááÁî®2014Âπ¥ÁöÑÊ†áÊ≥®Ê†áÂáÜÔºåÁî®ÊÑüÁü•Êú∫ËÆ≠ÁªÉËØçÊÄßÊ†áÊ≥®Ê®°ÂûãÔºå‰πãÂêéÊµãËØï‚ÄúÊï¥ËΩ¶ÂèØÂä®ÊÄßÂ∑Æ""Êä•Èîô
Êó•Âøó‰ø°ÊÅØÂ¶Ç‰∏ãÔºö
Exception in thread ""main"" java.lang.IllegalArgumentException: Âú®ÊâìÂàÜÊó∂‰º†ÂÖ•‰∫ÜÈùûÊ≥ïÁöÑ‰∏ãÊ†á
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.score(LinearModel.java:366)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.viterbiDecode(LinearModel.java:296)
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.tag(PerceptronPOSTagger.java:72)
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.tag(PerceptronPOSTagger.java:65)
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.tag(AbstractLexicalAnalyzer.java:154)
	at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.analyze(AbstractLexicalAnalyzer.java:193)


2.ÈááÁî®Êõ¥Â§öËØ≠ÊñôËÆ≠ÁªÉËØçÊÄßÊ†áÊ≥®ÔºåÂä†ËΩΩÊ®°ÂûãÊä•Èîô
Êó•Âøó‰ø°ÊÅØÂ¶Ç‰∏ãÔºö
Exception in thread ""main"" java.lang.IllegalArgumentException
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:334)
	at com.hankcs.hanlp.corpus.io.ByteArrayFileStream.ensureAvailableBytes(ByteArrayFileStream.java:83)
	at com.hankcs.hanlp.corpus.io.ByteArrayStream.nextInt(ByteArrayStream.java:56)
	at com.hankcs.hanlp.collection.trie.datrie.IntArrayList.load(IntArrayList.java:182)
	at com.hankcs.hanlp.collection.trie.datrie.MutableDoubleArrayTrieInteger.load(MutableDoubleArrayTrieInteger.java:1185)
	at com.hankcs.hanlp.model.perceptron.feature.ImmutableFeatureMDatMap.load(ImmutableFeatureMDatMap.java:93)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:421)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.load(LinearModel.java:388)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.<init>(LinearModel.java:65)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:75)
"
ÊÑüÁü•Êú∫ÂàÜËØçËØÜÂà´ÁöÑ‰∏Ä‰∫õÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0„ÄÇ

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÈùûÂ∏∏ÊÑüË∞¢‰ΩúËÄÖÂºÄÊîæËá™Â∑±ÁöÑ‰∫øÁ∫ßËÆ≠ÁªÉÁªìÊûúÔºåÂú®‰ΩøÁî®‰∏≠Êúâ‰∫õÁñëÈóÆÂ∏åÊúõËÉΩÁªô‰∫àÂ∏ÆÂä©Ôºö
1.‰ΩøÁî®ÊÑüÁü•Êú∫ÂàÜËØçÊó∂ÂØπ‰∏Ä‰∫õÊØîËæÉÊòéÊòæ‰∏çÊòØ‰∫∫ÂêçÁöÑËØçÊàñÁ¨¶Âè∑ËØÜÂà´‰∏∫‰∫∫ÂêçÔºåÂ¶Ç‚ÄúÂê¶‚ÄùÔºå‚ÄúÂΩíÊ°£‚ÄùËøô‰∫õÂ≠óÊàñËØçÂçïÁã¨Âá∫Áé∞Êó∂ ËØÜÂà´‰∏∫nr   ""ÊôÆÂç°ÂÆ¢Êà∑""‰∏≠ÁöÑ‚ÄúÊôÆÂç°‚Äù‰ºöË¢´ËØÜÂà´‰∏∫Âú∞ÂêçÁ≠â
2.Âú®Á∫øÊºîÁ§∫ÁöÑÊ®°ÂûãÂíå1.6.8ÂèëÂ∏ÉÁöÑÊ®°ÂûãÊòØ‰∏Ä‰∏™‰πàÔºåÊàëÁúãÂà∞‰πãÂâçÊúâ‰∫∫ÈóÆÂà∞Ëøô‰∏™ÔºåÂ∑ÆÂºÇ‰∏ªË¶ÅÂú®Áé∞‰ª£Ê±âËØ≠ËØçÂÖ∏Ôºå‰∏çËøá‰ΩøÁî®ÁöÑÊó∂ÂÄô‰∏ÄËà¨ÊàëÂú®Êú¨Âú∞ËØÜÂà´‰∏çÂ¶ÇÈ¢ÑÊúüÁöÑÊó∂ÂÄôÔºåÊãøÂà∞Âú®Á∫øÊºîÁ§∫‰∏äÈù¢ÊµãËØïÊó∂ÊïàÊûúÊõ¥Â•Ω‰∏Ä‰∫õÔºåÊúâÁÇπ‰∏çÊ∏ÖÊ•öËøô‰πãÈó¥ÂÖ∑‰ΩìÊúâÂì™‰∫õÂ∑ÆÂºÇÂØºËá¥„ÄÇ
3.Áé∞Âú®ÁöÑ‰ΩøÁî®‰∏äÂåÖÂê´ÁöÑ‰∫∫ÂêçÂú∞ÂêçËæÉÂ§öÔºå  Â∏∏Â∏∏‰ºöÂ∞ÜÂú∞ÂêçËØÜÂà´Êàê‰∫∫ÂêçÔºåÊàëÁé∞Âú®ÁöÑËß£ÂÜ≥ÂäûÊ≥ïÂè™ÊúâÂä†ËØçÂÖ∏ÔºåËØ∑ÈóÆ‰∏ãËøòÊúâÊõ¥Â•ΩÁöÑÊñπÂºèËß£ÂÜ≥‰πà„ÄÇ 
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§
ËØçÂÖ∏ÂíåÊ®°Âûã‰ΩøÁî®ÁöÑ1.7.0ÁâàÊú¨ÔºåÂä†‰∫Ü‰∫õÂú∞ÂêçÁöÑËØçÂÖ∏ ‰ΩÜÂ∑≤Á°ÆËÆ§Âá∫Áé∞ÈóÆÈ¢òÁöÑËØçÁöÑ‰∏çÂú®Êñ∞Âä†ÁöÑËØçÂÖ∏‰∏≠Âá∫Áé∞

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.analyze(""ÂΩíÊ°£""));
        System.out.println(analyzer.analyze(""ÊôÆÂç°ÂÆ¢Êà∑""));
        System.out.println(analyzer.analyze(""Ë∞¢Â∏àÂÇÖÂ∏ÆÂøôÂ§ÑÁêÜ""))Ôºõ
    }
```
### ÊúüÊúõËæìÂá∫
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
ÂΩíÊ°£/vn
ÊôÆÂç°ÂÆ¢Êà∑/n
Ë∞¢/v Â∏àÂÇÖ/n Â∏ÆÂøô/v Â§ÑÁêÜ/v
### ÂÆûÈôÖËæìÂá∫
ÂΩíÊ°£/nr
ÊôÆÂç°/ns ÂÆ¢Êà∑/n
Ë∞¢/nr Â∏àÂÇÖ/n Â∏ÆÂøô/v Â§ÑÁêÜ/v
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![qq 20181122140354](https://user-images.githubusercontent.com/18030444/48884619-2cc24380-ee60-11e8-8d9d-da0f8374c31f.png)
![qq 20181122140502](https://user-images.githubusercontent.com/18030444/48884620-2cc24380-ee60-11e8-96a7-d37ae4f49b3d.png)
![qq 20181122140536](https://user-images.githubusercontent.com/18030444/48884622-2d5ada00-ee60-11e8-89fb-acc6be19706c.png)
"
ÂàÜËØçÈóÆÈ¢ò,"ÂØπ‰∏ÄÁØáÂê´Êúâ‚ÄúËøàÂáØ‰º¶Ê≥ïÊãâÂà©ÊÅ©‰ΩêÂÖ∞ÂçöÂü∫Â∞º‚Äù„ÄÅ‚ÄúËøàÂáØ‰º¶Â°ûÁ∫≥ËøàÂáØ‰º¶Âç°ÂÆæ‚ÄùÁöÑÊñáÊú¨ÈááÁî®Ê†áÂáÜÂàÜËØçÊó∂ÔºåÂàÜËØçÁöÑÁªìÊûú‰∏≠ÂåÖÂê´Ëøô‰∫õÈïøËØç„ÄÇÂ¶ÇÊûúÂ∞ÜËøô‰∏äËø∞‰∏§‰∏™ËØçËøûÂú®‰∏ÄËµ∑ÔºåÂàÜËØçÁªìÊûúÊòØ‰∏§‰∏™ËØçÁöÑÁªìÂêà„ÄÇÂØπ‰∫éÈü≥ËØëÂêçÁöÑÂàÜËØçÔºåËøûÂú®‰∏ÄËµ∑ÁöÑÈü≥ËØëËØç‰ºöË¢´ÂΩì‰Ωú‰∏Ä‰∏™ËØç„ÄÇÊòØË¶ÅÁî®Á¥¢ÂºïÂàÜËØçÂêóÔºü
‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
List termList1 = HanLp.segment(""ËøàÂáØ‰º¶Ê≥ïÊãâÂà©ÊÅ©‰ΩêÂÖ∞ÂçöÂü∫Â∞º, ËøàÂáØ‰º¶Â°ûÁ∫≥ËøàÂáØ‰º¶Âç°ÂÆæ"");
List termList2 = HanLp.segment(""ËøàÂáØ‰º¶Ê≥ïÊãâÂà©ÊÅ©‰ΩêÂÖ∞ÂçöÂü∫Â∞ºËøàÂáØ‰º¶Â°ûÁ∫≥ËøàÂáØ‰º¶Âç°ÂÆæ"");
System.out.println(termList1);
System.out.println(termList2);
ÁªìÊûúÂ¶Ç‰∏ãÔºö
[ËøàÂáØ‰º¶Ê≥ïÊãâÂà©ÊÅ©‰ΩêÂÖ∞ÂçöÂü∫Â∞º/nrf, Ôºå/w, ËøàÂáØ‰º¶Â°ûÁ∫≥ËøàÂáØ‰º¶Âç°ÂÆæ/nrf]
[ËøàÂáØ‰º¶Ê≥ïÊãâÂà©ÊÅ©‰ΩêÂÖ∞ÂçöÂü∫Â∞ºËøàÂáØ‰º¶Â°ûÁ∫≥ËøàÂáØ‰º¶Âç°ÂÆæ/nrf]"
reloadÁîüÊàêËá™ÂÆö‰πâËØçÂÖ∏binÊñá‰ª∂Âú®ÈáçÊñ∞ËΩΩÂÖ•Êó∂ÊäõÂá∫ÂºÇÂ∏∏ArrayIndexOutOfBoundsException,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂΩìÊñ∞Â¢ûËá™ÂÆö‰πâËØçÂÖ∏Êñá‰ª∂Êó∂, ÈÄöËøáË∞ÉÁî®CustomDictionary‰∏≠ÁöÑreloadÊñπÊ≥ïÁîüÊàêÊñ∞ÁöÑCustomDictionary.txt.binÊñá‰ª∂ÁîüÊàêÂπ∂Âä†ËΩΩ, ‰ªéËÄåÂÆûÁé∞ÂØπÊñ∞Âä†Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑËØçÁöÑÂàáÂàÜÂäüËÉΩ. 



## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÁõÆÂâçÂèëÁé∞ÁöÑÈóÆÈ¢òÊòØ : ÈÄöËøáreloadÊñπÊ≥ïÁîüÊàêÁöÑÊñ∞binÊñá‰ª∂Âú®Á¨¨‰∫åÊ¨°Á®ãÂ∫èÂêØÂä®ËøáÁ®ã‰∏≠‰∏çËÉΩÂ§üË¢´Ê≠£Á°ÆËØÜÂà´, ‰ºöÊäõÂá∫
```python
ÂçÅ‰∏ÄÊúà 21, 2018 2:41:38 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadDat
Ë≠¶Âëä: ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.ArrayIndexOutOfBoundsException: 149
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:327)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:64)
	at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
```
‰∏îÈÄöËøáreloadÁîüÊàêÁöÑbinÊñá‰ª∂Â§ßÂ∞èÂíåÂàùÂßãÂåñHanLPÁîüÊàêÁöÑbinÊñá‰ª∂Â§ßÂ∞è‰∏ç‰∏ÄËá¥.
ÊàëÁöÑ‰æãÂ≠ê‰∏≠, ÈÄöËøáreloadÁîüÊàêÁöÑbinÊñá‰ª∂Â§ßÂ∞è‰∏∫28788656, ÈÄöËøáHanLPÂàùÂßãÂåñÁîüÊàêÁöÑbinÊñá‰ª∂Â§ßÂ∞è‰∏∫28788888. 
Â∑≤ÁªèÁ°ÆÂÆö‰∏çÊòØÂõ†‰∏∫ËøõÁ®ãÊèêÂâçÂÖ≥Èó≠ÂØºËá¥Êñá‰ª∂Ê≤°ÊúâÂÜôÂÆåËøôÁßçÊÉÖÂÜµ.

### Ê≠•È™§
 

### Ëß¶Âèë‰ª£Á†Å

```
from jpype import JClass
from pyhanlp import HanLP, SafeJClass
from datetime import datetime
custom_dictionary = SafeJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
custom_dictionary.reload()
HanLP.segment('‰ªäÂ§©Â§©Ê∞î‰∏çÈîô')
```
Âú®ËøêË°åËøá‰∏äËø∞‰ª£Á†ÅÂêé, ‰ºöÁîüÊàê‰∏Ä‰∏™binÊñá‰ª∂, ‰∏∫‰∫Ü‰æø‰∫éÂå∫ÂàÜÁÆÄÁß∞‰∏∫reload.bin
```
from jpype import JClass
from pyhanlp import HanLP, SafeJClass
from datetime import datetime
#custom_dictionary = SafeJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
#custom_dictionary.reload()
HanLP.segment('‰ªäÂ§©Â§©Ê∞î‰∏çÈîô')
```
Âú®ËøêË°å‰∏äËø∞‰ª£Á†ÅËøáÁ®ã‰∏≠, ‰ºöÊäõÂá∫ÈóÆÈ¢ò‰∏≠ÈÅáÂà∞ÁöÑÂºÇÂ∏∏, ËÄå‰∏î‰ºöÁîüÊàêÊñ∞ÁöÑbinÊñá‰ª∂, Áß∞‰∏∫init.bin

reload.binÊñá‰ª∂Â§ßÂ∞è‰∏∫28788656
init.binÊñá‰ª∂Â§ßÂ∞è‰∏∫28788888

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
2018-11-21 17:00:55
Êé®ÊµãÂèØËÉΩÊòØloadDatÈáåÈù¢Êúâ‰∫õÈÄªËæëÊòØÁ®ãÂ∫èÂàùÂßãÂåñ‰∏≠‰ºöËµ∞, ‰ΩÜÊòØreload‰∏ç‰ºöËµ∞ÁöÑ. ÊâÄ‰ª•ÂØºËá¥ÂÜôÂÖ•Êñá‰ª∂ÁöÑÂÜÖÂÆπ‰∏ç‰∏ÄËá¥.

2018-11-22 10:33:15
ÈáçÊñ∞Â§çÁé∞‰∫Ü‰∏ÄÈÅçËøáÁ®ã, Êü•Áúã‰∫ÜÂÖ∂‰ªñÁöÑissue. ËøáÂæÄÁöÑissueÈÉΩÊòØÂõ†‰∏∫ËÄÅÁâàÊú¨ÁöÑÂèçÂ∞ÑÊú∫Âà∂ÂØºËá¥Ëá™ÂÆö‰πâËØçÊÄßÊä•Èîô. ‰ΩÜÊòØÁõÆÂâçÁî®ÁöÑÊòØ1.6.8ÁâàÊú¨, Â∫îËØ•‰∏çÂ≠òÂú®Ëøô‰∏™ÈóÆÈ¢ò. ÂêåÊó∂ÈááÂèñ‰∫Ü‰∏çÂêåÁöÑÁ≠ñÁï•ËøõË°åÊµãËØï

1) Á≠ñÁï•:  Êú¨Âú∞Êúâinit.bin  ÁªìÊûú : Ê≠£Â∏∏ËΩΩÂÖ• ‰∏çÊä•Èîô Ê≠£Â∏∏ÂàÜËØç
2) Á≠ñÁï•: Êú¨Âú∞Êó†init.bin ÁªìÊûú : Ê≠£Â∏∏ÁîüÊàêinit.bin Ê≠£Â∏∏ËΩΩÂÖ• ‰∏çÊä•Èîô Ê≠£Â∏∏ÂàÜËØç.
3) Á≠ñÁï•: Êú¨Âú∞Êúâinit.bin Á¨¨‰∏ÄÊ¨°ÂíåÁ¨¨‰∫åÊ¨°ÂàÜËØç‰πãÈó¥ËøõË°åreload ÁªìÊûú : Ê≠£Â∏∏ÁîüÊàêreload.bin ‰∏§Ê¨°ÂàÜËØçÂùáÊ≠£Â∏∏ ‰ªªÂä°ÁªìÊùüÂêéÂú®ÂÖ∂‰ªñ‰ªªÂä°‰∏≠ËøõË°åÂàÜËØç, Êó†Ê≥ïÊ≠£Â∏∏ËΩΩÂÖ•reload.bin, ÈáçÊñ∞ÁîüÊàêinit.bin.

reload‰∏≠‰πüÊòØÈÄöËøáÂà†Èô§Êú¨Âú∞binÊñá‰ª∂ÂêéÈáçÊñ∞Ë∞ÉÁî®loadMainDictionaryÊñπÊ≥ïËøõË°åÊñ∞binÊñá‰ª∂ÁîüÊàê, ÂíåinitËøáÁ®ã‰∏≠ÁöÑÈÄªËæëÊòØ‰∏ÄÊ†∑ÁöÑ, ‰ΩÜÊòØÁîüÊàêÁöÑÊñá‰ª∂Â§ßÂ∞è‰∏ç‰∏ÄËá¥, ‰∏î‰∏çËÉΩÊ≠£Â∏∏ËØªÂèñ. ÊÄÄÁñëÊòØÂú®reloadËøáÁ®ã‰∏≠ÊòØÂê¶Ê≤°ÊúâÂàùÂßãÂåñ‰∏Ä‰∫õÂÖ®Â±ÄÂèòÈáèÁöÑÂÄºÂØºËá¥realoadÁîüÊàêÁöÑbinÊñá‰ª∂Áº∫Â∞ë‰∏Ä‰∫õÂÜÖÂÆπ.

"
ÂàÜËØçÁÆóÊ≥ïÈóÆÈ¢ò,ÊàëÊÉ≥Êää ‚ÄúÊú∫Âô®‰∫∫‚Äù ÂàÜËØçÊàê ‚ÄúÊú∫Âô®‚Äù ‚ÄúÊú∫Âô®‰∫∫‚Äù ‚ÄúÊú∫‚Äù ‚ÄúÂô®‚Äù ‚Äú‰∫∫‚Äù Ëøô‰∫î‰∏™ËØçÂ∫îËØ•ÈÄâÊã©Âì™‰∏™ÂàÜËØçÁÆóÊ≥ï
androidÂºÄÂèëÊñá‰ª∂Êó†Ê≥ïÂÜôÂÖ•Âà∞data/dictionary/person/nrf.txt.trie.dat,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

## ÊàëÁöÑÈóÆÈ¢ò
Âú®androidÊú∫‰∏äÈù¢ÂÅöÂä®ÊÄÅÂàÜËØçÔºåÁªèÂ∏∏‰ºöÊäõÂá∫
11-20 19:26:18.068 1366-2392/com.mtrobot.studio.mtrobotstudio_product E/CrashHandler: java.lang.IllegalAccessError: ‰∏çÊîØÊåÅÂÜôÂÖ•data/dictionary/person/nrf.txt.trie.datÔºÅËØ∑Âú®ÁºñËØëÂâçÂ∞ÜÈúÄË¶ÅÁöÑÊï∞ÊçÆÊîæÂÖ•app/src/main/assets/data

ËØ•ÂºÇÂ∏∏Áõ¥Êé•ÂØºËá¥Â∫îÁî®Á®ãÂ∫èÂ¥©Ê∫ÉÔºåÂπ∂‰∏îÊàë‰ª¨ÁöÑÁ≥ªÁªüÊòØrootÊùÉÈôêÁöÑÔºå‰∏ç‰ºöÂèëÁîüËØªÂÜôÂ§±Ë¥•ÁöÑÈóÆÈ¢òÔºåÊü•ÁúãÁ≥ªÁªüÊó•ÂøóÂèëÁé∞nrf.txt.trie.dat‰ªéÊú™ÂÜôÂÖ•ÊàêÂäüËøáÔºåËØ∑Â§ßÁâõÂ∏ÆÂøôËß£ÂÜ≥,Ë∞¢Ë∞¢


"
"Êú™ËøõË°å‰øÆÊîπ, Ëá™ÂÆö‰πâËØçÂÖ∏ÁºìÂ≠òÂ§±Ë¥•","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
‰ΩøÁî®python3.7.1 ÁâàÊú¨, Áõ¥Êé•pip ÂÆâË£ÖÂæóÂà∞
Windows 10 ÂÆ∂Â∫≠‰∏≠ÊñáÁâà1803 17134.376
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®Êàë‰ΩøÁî®pip ÂÆâË£ÖÂêé, Âú®ÂëΩ‰ª§Ë°åËøêË°å`hanlp segment` ‰∏ãËΩΩËøájarÂíådataÂêé, ËæìÂÖ•ÊµãËØï‰ªªÊÑèÁî®Âè•, ‰ºöÂá∫Áé∞Êä•Èîô‰ø°ÊÅØ.
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Âú®ÊàëÊ∏ÖÁ©∫‰ª•ÂâçÁöÑhanlpÊñá‰ª∂Âêé, ÈáçÊñ∞pip ÂÆâË£Ö, Ê≤°ÊúâÂØπdata ‰∏≠ÁöÑ‰ªª‰ΩïÊï∞ÊçÆ‰øÆÊîπ, Áõ¥Êé•‰ΩøÁî®hanlp segmentÂç≥Âá∫Áé∞ÈóÆÈ¢ò.

### Ëß¶Âèë‰ª£Á†Å

```
$ hanlp segment
ÊµãËØïÁî®Âè•

```

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadDat
Ë≠¶Âëä: ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.ArrayIndexOutOfBoundsException: 7
        at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToInt(ByteUtil.java:239)
        at com.hankcs.hanlp.corpus.io.ByteArray.nextInt(ByteArray.java:68)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:325)
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:64)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)

ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏c:/programËØªÂèñÈîôËØØÔºÅjava.io.FileNotFoundException: c:\program (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•Ôºöc:/program
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Ê≤°ÊúâÂä†ËΩΩÂà∞‰ªª‰ΩïËØçÊù°
ÂçÅ‰∏ÄÊúà 17, 2018 3:09:57 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Ëá™ÂÆö‰πâËØçÂÖ∏c:/program files/python37/lib/site-packages/pyhanlp/static/data/dictionary/custom/CustomDictionary.txtÁºìÂ≠òÂ§±Ë¥•ÔºÅ
java.lang.NullPointerException
        at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:116)
        at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:51)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:203)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)

ÊµãËØï/vn Áî®/p Âè•/q
```

Ë∞¢Ë∞¢ÊÇ®ÁöÑËß£Á≠î!

"
ÊûÑÈÄ†PerceptronLexicalAnalyzerÊä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊûÑÈÄ†ËØçÊ≥ïÂàÜÊûêÂô®ÔºåÊûÑÈÄ†ÂáΩÊï∞Âá∫Èîô
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§



### Ëß¶Âèë‰ª£Á†Å

```
		PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(""D:/myeclipse-workplace/NLP/data-for-1.6.8/data/model/perceptron/pku199801/cws.bin"",
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: ÈîôËØØÁöÑÊ®°ÂûãÁ±ªÂûã: ‰º†ÂÖ•ÁöÑ‰∏çÊòØËØçÊÄßÊ†áÊ≥®Ê®°ÂûãÔºåËÄåÊòØ CWS Ê®°Âûã
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.<init>(PerceptronPOSTagger.java:37)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:50)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70)
	at com.NLP.DemoPerceptronLexicalAnalyzer.main(DemoPerceptronLexicalAnalyzer.java:24)

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlp JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')ËÉΩÂê¶Âä†ËΩΩ.binÊ®°Âûã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

pythonË∞ÉÁî®hanlpÊó∂ÔºåWordVectorModelÂè™ËÉΩÂä†ËΩΩÂêéÁºÄ‰∏∫txtÁöÑËØçÂêëÈáèÊ®°ÂûãÔºåËÉΩÂê¶Âä†ËΩΩÂêéÁºÄ‰∏∫bin‰∏∫Ê®°ÂûãÔºåÂ¶Ç‰ΩïÂä†ËΩΩ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = 'E:/data/baike_26g_news_13g_novel_229g.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
NormalizationËØçÂÖ∏CharTable.txt‰∏≠‚ë†Êò†Â∞Ñ‰∏∫Êï∞Â≠ó1ÊòØÂê¶Êõ¥ÂêàÈÄÇÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->
ÂàÜËØçÂâçËøõË°åNormalization Êìç‰ΩúÔºå‰ºöÊ†πÊçÆ `data/dictionary/other/CharTable.txt`ÂØπÊØè‰∏Ä‰∏™ Â≠óÁ¨¶ ËøõË°å‰∏Ä‰∏ÄÊò†Â∞Ñ„ÄÇÊàëÂèëÁé∞Â∏¶ÂúÜÂúàÁöÑÊï∞Â≠óË¢´Êò†Â∞Ñ‰∏∫ÂØπÂ∫îÁöÑÊ±âÂ≠óÔºåÊòØÂê¶‰øÆÊîπ‰∏∫ÈòøÊãâ‰ºØÊï∞Â≠óÊõ¥ÂêàÈÄÇÔºü

ÊàëÈÅáÂà∞ÁöÑÂú∫ÊôØÊòØ Êï∞Â≠óÂüüÂêç„ÄÅÂè∑Á†ÅÁ≠âÔºåÈòøÊãâ‰ºØÊï∞Â≠óÊòØÊõ¥ÂêàÈÄÇÁöÑÔºå‰∏çËøáÊòØÂê¶Êúâ‰∫õÂú∫ÊôØÊõ¥ÈÄÇÂêàÁî®Ê±âÂ≠óÔºü

https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/CharTable.txt#L40"
ÊÇ®Â•ΩÔºåËØ∑ÈóÆÂú®Á∫øÊºîÁ§∫‰∏≠‰ΩøÁî®ÁöÑÊòØ‰ªÄ‰πàÂàÜËØçÂô®Âë¢ÔºåÊàëËØï‰∫ÜÂæàÂ§öÊ≤°ÊúâÂèëÁé∞Áõ∏ÂêåÁöÑÂàÜËØçÂô®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºåËØ∑ÈóÆÂú®Á∫øÊºîÁ§∫‰∏≠‰ΩøÁî®ÁöÑÊòØ‰ªÄ‰πàÂàÜËØçÂô®Âë¢ÔºåÊàëËØï‰∫ÜÂæàÂ§öÊ≤°ÊúâÂèëÁé∞Áõ∏ÂêåÁöÑÂàÜËØçÂô®
Êàë‰ΩøÁî®ÁöÑÊòØpyhanlp

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÊàëÂØπ‰∏§Âè•ËØùËøõË°åÂàÜËØç
s1 = 'ËÄåËøô‰∫õ‰ºü‰∫∫Áî®Ë°åÂä®ÈòêËø∞‰∫ÜÂÆûÁé∞ÁêÜÊÉ≥ÁöÑÁúüË∞õ„ÄÇ'
s2 = '‰∏∫‰∫ÜÊîªÂÖã‚ÄúÂì•Âæ∑Â∑¥Ëµ´ÁåúÊÉ≥‚ÄùÔºå‰ªñÊï¥Â§©ËøõË°åÊºîÁÆóÔºåÈÅáÂà∞‰∫ÜËÆ∏Â§öÊå´ÊäòÔºå‰ΩÜ‰ªñÈÉΩÂùöÊåÅ‰∏ãÊù•‰∫Ü'
2. Âú®Á∫øÊºîÁ§∫ÁöÑÂàÜËØçÁªìÊûú
![image](https://user-images.githubusercontent.com/32705832/48398994-593dd780-e75d-11e8-8c22-6db8ce44b405.png)
![image](https://user-images.githubusercontent.com/32705832/48399018-6955b700-e75d-11e8-94e4-3d2b960a30a1.png)





3. ÊàëËØïÁöÑÂàÜËØçÁªìÊûú

‰∏∫‰∫Ü/p, ÊîªÂÖã/v, ‚Äú/w, Âì•/n, Âæ∑/b, Â∑¥Ëµ´/nrf, ÁåúÊÉ≥/v, ‚Äù/w, Ôºå/w, ‰ªñ/rr, Êï¥Â§©/d, ËøõË°å/vn, ÊºîÁÆó/vn, Ôºå/w, ÈÅáÂà∞/v, ‰∫Ü/ule, ËÆ∏Â§ö/m, Êå´Êäò/n, Ôºå/w, ‰ΩÜ/c, ‰ªñ/rr, ÈÉΩ/d, ÂùöÊåÅ/v, ‰∏ãÊù•/vf, ‰∫Ü/ule

ËÄå/c Ëøô‰∫õ/r ‰ºü‰∫∫/n Áî®/p Ë°åÂä®/vn ÈòêËø∞/v ‰∫Ü/u ÂÆûÁé∞ÁêÜÊÉ≥/n ÁöÑ/u ÁúüË∞õ/n „ÄÇ/w

### Ëß¶Âèë‰ª£Á†Å

```
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(s))
print(analyzer.analyze(s1))
```
### ÊúüÊúõËæìÂá∫
ÂÉèÂú®Á∫øÊºîÁ§∫ÁöÑÂàÜËØçÁªìÊûú‰∏ÄÊ†∑
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫
Â¶Ç‰∏äÊàëÁöÑÂàÜËØçÁªìÊûú
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂëΩÂêçÂÆû‰ΩìÊèêÂèñÔºåÊó∂Èó¥ËØçËØÜÂà´ÊúâËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""‰∏ãÂë®ÂõõÂ§©Ê∞îÊÄé‰πàÊ†∑"");
```

ËæìÂá∫ÁªìÊûúÔºö
![image](https://user-images.githubusercontent.com/3538629/48359900-f276da80-e6d8-11e8-9bd4-fff6e366d647.png)

ËØ•ÁªìÊûúÁ¨¶ÂêàÈ¢ÑÊúü„ÄÇ

```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""‰∏ãÂë®‰∏âÂ§©Ê∞îÊÄé‰πàÊ†∑"");
            System.out.println(wordList);
```

ËæìÂá∫ÁªìÊûú

![image](https://user-images.githubusercontent.com/3538629/48359938-03275080-e6d9-11e8-8676-3cb230f9570b.png)

ËØ•ÁªìÊûú‰∏çÁ¨¶ÂêàÈ¢ÑÊúüÔºåÂ∫îËØ•Â∞Ü*‰∏ãÂë®‰∏â*ËØÜÂà´‰∏∫Êó∂Èó¥ËØç„ÄÇ

ËØ∑ÈóÆÊÄé‰πàËÉΩÊèêÈ´òÊó∂Èó¥ËØçÁöÑËØÜÂà´ÊïàÊûúÔºü

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

```
    public void testDateNer() {
        try {
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""‰∏ãÂë®‰∏âÂ§©Ê∞îÊÄé‰πàÊ†∑"");
            System.out.println(wordList);
        } catch (IOException e) {
            e.printStackTrace();
        }

    }
```


"
1.7.0 ÊåáÂÆö shortest ‰∏∫ False seg2sentence() ËøòÊòØÊåâÈÄóÂè∑ÂàáÂàÜ,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.7.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.7.0

## ÊàëÁöÑÈóÆÈ¢ò

Â¶ÇÈ¢ò Êó†ËÆ∫ shortest ‰∏∫ True ËøòÊòØ False seg2sentence ÈÉΩÊòØÊåâÈÄóÂè∑ÂíåÂàÜÂè∑ÂàáÂàÜÁöÑ
Áõ¥Êé•Ë∞ÉÁî® com.hankcs.hanlp.utility.SentencesUtil.toSentenceList() ÊÉÖÂÜµ‰πü‰∏ÄÊ†∑

### Ëß¶Âèë‰ª£Á†Å

‰∏ç‰ºö Java Áî®ÁöÑ pyhanlp (Êõ¥Êñ∞Ëøá jar ÂåÖ‰∫Ü)

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')
sentence_util = jpype.JClass('com.hankcs.hanlp.utility.SentencesUtil')

for s in standard_tokenizer.seg2sentence('ËøôÊòØ‰∏Ä‰∏™Âè•Â≠êÔºå‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ', shortest = False): print(s)
for s in sentence_util.toSentenceList('ËøôÊòØ‰∏Ä‰∏™Âè•Â≠êÔºå‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ', shortest = False): print(s)

```
### ÊúüÊúõËæìÂá∫

```
[Ëøô/rzv, ÊòØ/vshi, ‰∏Ä‰∏™/mq, Âè•Â≠ê/n, Ôºå/w, ‰∏≠Èó¥/f, Êúâ/vyou, ‰∏™/q, ÈÄóÂè∑/n, „ÄÇ/w]
ËøôÊòØ‰∏Ä‰∏™Âè•Â≠êÔºå‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ
```

### ÂÆûÈôÖËæìÂá∫

```
[Ëøô/rzv, ÊòØ/vshi, ‰∏Ä‰∏™/mq, Âè•Â≠ê/n, Ôºå/w]
[‰∏≠Èó¥/f, Êúâ/vyou, ‰∏™/q, ÈÄóÂè∑/n, „ÄÇ/w]
ËøôÊòØ‰∏Ä‰∏™Âè•Â≠êÔºå
‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ
```
"
seg2sentence() ÊääÈÄóÂè∑ÂíåÂàÜÂè∑‰πüÂàÜÂâ≤‰∏∫‰∏ÄÂè•ËØù,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

## ÊàëÁöÑÈóÆÈ¢ò
Ëøô‰∏™ÈóÆÈ¢ò [Issue 876](https://github.com/hankcs/HanLP/issues/876) ÈáåÊèêÂà∞Ëøá ‰ΩÜÊòØË¢´ÂÖ≥Èó≠‰∫Ü
ÁõÆÂâç StandardTokenizer, BasicTokenizer, NLPTokenizer, SpeedTokenizer Êèê‰æõÁöÑ seg2sentence ÂàÜÂè•Êé•Âè£ÔºàÂÖ∂‰ªñÂàÜËØçÂô®ÊØîÂ¶Ç TraditionalChineseTokenizer, URLTokenizer, CRFLexicalAnalyzer, PerceptronLexicalAnalyzer, DijkstraSegment, NShortSegment, ViterbiSegment ÊàëÂ§ßËá¥Áúã‰∫Ü‰∏ãÊ≤°ÊâæÂà∞ÂàÜÂè•ÁöÑÊé•Âè£ ‰∏çÁü•ÈÅìÊòØ‰∏çÊòØ‰∏ÄÊ†∑ÁöÑË°®Áé∞Ôºâ‰ºöÊääÈÄóÂè∑‰πüÂàáÂàÜ‰∏∫‰∏ÄÂè•ËØù ‰∏çÁü•Ëøô‰∏™ÂäüËÉΩÊòØÂê¶ÊòØ‰∏∫‰∫ÜÂêéÊúüÂÖ∂‰ªñÊñáÊú¨Â§ÑÁêÜÁöÑËÄÉËôëÊâçËøôÊ†∑ÂàáÂàÜÁöÑÔºü

Â¶ÇÊûúÊòØÁî®‰∫éÁîü‰∫ßÁéØÂ¢ÉÁöÑÂàÜÂè•ÁöÑËØù ÊÑüËßâÈÄóÂè∑ËÇØÂÆö‰∏çËÉΩÂàáÊàê‰∏ÄÂè• ÊåâÁÖßÊúÄÊñ∞ÁöÑÂõΩÂÆ∂Ê†áÂáÜÔºö[ ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂõΩÂÆ∂Ê†áÂáÜGB/T15834-2011Ê†áÁÇπÁ¨¶Âè∑Áî®Ê≥ï](http://yywz.sumhs.edu.cn/f4/ad/c4705a193709/page.htm) 3.1.1 ‰∏ÄËäÇ‰∏≠ÊâÄËø∞ Âè™ÊúâÂè•Âè∑ ÈóÆÂè∑ ÂèπÂè∑ Ëøô‰∏â‰∏™Ê†áÁÇπÁ¨¶Âè∑ÊâçÁÆó‰∏ÄÂè•ËØù ÊâÄ‰ª•‰∏•Ê†ºÊù•ËØ¥ ÂàÜÂè∑‰πü‰∏çËÉΩÁÆó

### Ëß¶Âèë‰ª£Á†Å
Âõ†‰∏∫Êàë‰∏ç‰ºö Java Âè™‰ºö Python ÊâÄ‰ª•Áî®ÁöÑ pyhanlp Â∞±Áî® StandardTokenizer ‰∏æ‰∏™‰æãÂ≠ê ÂÖ∂‰ªñÂàÜËØçÂô®ÁöÑÁªìÊûúÁ±ª‰ºº

```
import jpype
import pyhanlp

standard_tokenizer = jpype.JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')

for sentence in standard_tokenizer.seg2sentence('ËøôÊòØ‰∏ÄÂè•Áî®Êù•ÊµãËØïÁöÑÂè•Â≠êÔºå‰∏≠Èó¥Êúâ‰∏™ÈÄóÂè∑„ÄÇ'): 
    print(sentence)
```
### ÊúüÊúõËæìÂá∫

```
[Ëøô/rzv, ÊòØ/vshi, ‰∏Ä/m, Âè•/q, Áî®Êù•/v, ÊµãËØï/vn, ÁöÑ/ude1, Âè•Â≠ê/n, Ôºå/w, ‰∏≠Èó¥/f, Êúâ/vyou, ‰∏™/q, ÈÄóÂè∑/n, „ÄÇ/w]
```

### ÂÆûÈôÖËæìÂá∫

```
[Ëøô/rzv, ÊòØ/vshi, ‰∏Ä/m, Âè•/q, Áî®Êù•/v, ÊµãËØï/vn, ÁöÑ/ude1, Âè•Â≠ê/n, Ôºå/w]
[‰∏≠Èó¥/f, Êúâ/vyou, ‰∏™/q, ÈÄóÂè∑/n, „ÄÇ/w]
```
"
ËØ∑ÈóÆ‰∏ãËøô‰∏™ÈáåÈù¢ÊòØÂê¶ÂèØ‰ª•Áî®Ê∑±Â∫¶Â≠¶‰π†ÁöÑÁÆóÊ≥ïÊù•ËÆ≠ÁªÉ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â§ßÂπÖÂ∫¶,"<!-Â§ßÂπÖÂ∫¶-
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
Â§ßÂπÖÂ∫¶
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰∫∫ÂêçÂàÜËØçÈóÆÈ¢ò,"‰æãÂè•Ôºö‚ÄúÊã®Âº†‰∏âÁöÑÁîµËØù‚Äú  ÂàÜËØçÂêé  ‚Äú Êã®/v, Âº†/q, ‰∏â/m, ÁöÑ/uj, ÁîµËØù/n‚Äú
‰æãÂè•Ôºö""Êã®ÊâìÂº†‰∏âÁöÑÁîµËØù"" ÂàÜËØçÂêé  ‚ÄúÊã®Êâì/v, Âº†‰∏â/nr, ÁöÑ/uj, ÁîµËØù/n‚Äù
‰∏∫‰ªÄ‰πà‰∫∫ÂêçÂàÜËØç‰∏ç‰∏Ä"
ngramËØçÂÖ∏‰øÆÊîπÊó†Êïà,"## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊàëÂú®ngramËØçÂÖ∏ÈáåÊää‚ÄúÂæà@ÊºÇ‰∫Æ‚ÄùÂà†Èô§‰∫ÜÔºåË∞ÉÁî®com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary#reloadÈáçÊñ∞Âä†ËΩΩËØçÂÖ∏ÔºåbinÊñá‰ª∂‰πüÂà†Èô§ÈáçÂª∫‰∫ÜÔºå‰∏∫‰ªÄ‰πàÂàÜËØçËøòÊòØÊää‚ÄúÂæàÊºÇ‰∫Æ‚ÄùËøô‰∏™ËØçÂêàÂπ∂Âú®‰∏ÄËµ∑‰∫ÜÔºüÊàëËøôËæπÈúÄË¶ÅÊäängramÈáåÁöÑÂæàÂ§öÂâØËØçÂíåÂΩ¢ÂÆπËØçÂàÜÂºÄÔºåÊúâÊ≤°Êúâ‰ªÄ‰πàÊñπÊ≥ïÊäängramËØçÂÖ∏Á¶ÅÁî®ÊéâÔºü

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""ËΩ¶Â≠êÂæàÊºÇ‰∫Æ„ÄÇ""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
[ËΩ¶Â≠ê/n, Âæà/d, ÊºÇ‰∫Æ/a, „ÄÇ/w]


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
[ËΩ¶Â≠ê/n, ÂæàÊºÇ‰∫Æ/n, „ÄÇ/w]


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
test_word2vec .py‰∏≠Âä†ËΩΩËØçÂêëÈáèÊ®°ÂûãWordVectorModel(model_file)Êä•ÂÜÖÂ≠ò‰∏çË∂≥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
```
from pyhanlp import *
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
model_file = 'user/data/baidubaike.txt'
word2vec = WordVectorModel(model_file)
```
ÂÖ∂‰∏≠baidubaike.txtÊñá‰ª∂Â§ßÂ∞è4GÔºåËøêË°åÂá∫Áé∞Â¶Ç‰∏ãÊä•ÈîôÔºö
```
File ""D:\Anaconda3\lib\site-packages\jpype\_jclass.py"", line 111, in _javaInit*args)
java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded
```
‰øÆÊîπjvmÂÜÖÂ≠ò‰πüÊ≤°ÊúâËß£ÂÜ≥ÈóÆÈ¢òÔºåËØ∑ÈóÆÂΩìÊñá‰ª∂ËæÉÂ§ßÊó∂ÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->


"
Translating docs and codebase to English,Are there any forks of this repository that translate it into English ?
ÁπÅÁÆÄËΩ¨Êç¢ Ôºö‰ΩòÂ≠óËΩ¨Èîô ÔºöÁï≤,"
System.out.println(HanLP.convertToSimplifiedChinese(""‰ΩòËØóÊõº""));
‰ºöËØØÂ∞Ü ‰ΩòÂ≠óËΩ¨Èîô ÔºöÁï≤"
doc2vecÁöÑÂÆûÁé∞ÂéüÁêÜËÉΩÂê¶ËØ¥‰∏Ä‰∏ãÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÉ≥‰ΩøÁî®doc2vecÁöÑÊé•Âè£ËÆ°ÁÆóÂè•Â≠êÁöÑÁõ∏‰ººÂ∫¶ÔºåÊÉ≥‰ªîÁªÜ‰∫ÜËß£‰∏Ä‰∏ãËøô‰∏™Ê®°ÂùóÁöÑÂéüÁêÜÔºü


<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
"
pyhanlp Âú®python3ÁéØÂ¢É‰∏ãdata-for-1.6.8.zip Ëß£ÂéãÁº©‰π±Á†Å,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
pyhanlp Âú®Á¨¨‰∏ÄÊ¨° import ÁöÑÊó∂ÂÄô‰ºö‰∏ãËΩΩÂπ∂Ëß£ÂéãÊï∞ÊçÆÊñá‰ª∂ÔºåÂÆûÊµãpython3 ÁéØÂ¢É‰∏ãÔºådata-for-1.6.8‰ΩøÁî®`zipfile`Áõ¥Êé•Ëß£Âéã‰ºöÂá∫Áé∞‰∏≠Êñá‰π±Á†ÅÔºàcustomÁõÆÂΩïÁöÑÂá†‰∏™Êñá‰ª∂Ôºâ„ÄÇpython2 Ê≤°ÊúâËøô‰∏™ÈóÆÈ¢òÔºåÂèÇËÄÉÂêéÈù¢‰∏§‰∏™ÈìæÊé•„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
ÊâãÂ∑•ÊâßË°å`static.__init__.py` ÈáåÈù¢ÁöÑ‰ª£Á†Å
https://github.com/hankcs/pyhanlp/blob/master/pyhanlp/static/__init__.py#L241

### Ëß¶Âèë‰ª£Á†Å

```python
with zipfile.ZipFile(""data-for-1.6.8.zip"", ""r"") as f:
  for fn in f.namelist():
    print(fn)
```
### ÊúüÊúõËæìÂá∫

Ê≠£Á°ÆÁöÑ‰∏≠ÊñáÊñá‰ª∂Âêç

### ÂÆûÈôÖËæìÂá∫

‰π±Á†Å

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÂèÇËßÅËøô‰∏§‰∏™ StackOverflow ÈóÆÈ¢òÔºö
https://stackoverflow.com/questions/41019624/python-zipfile-module-cant-extract-filenames-with-chinese-characters
https://stackoverflow.com/questions/37723505/namelist-from-zipfile-returns-strings-with-an-invalid-encoding

ËØùËØ¥ÊàëËß£Âéã‰∫ÜÔºåÂÜçmacÁî®Á≥ªÁªüËá™Â∏¶ÁöÑzipÂÜçÊâìÂåÖÔºåÁªìÊûúËøòÊòØ‰π±Á†ÅÔºö`zip -r data data`Ôºå‰ΩøÁî®7zÂéãÁº©ÂêéÂ∞±Ê≤°ÈóÆÈ¢ò‰∫Ü„ÄÇ`7z a -tzip data.zip data`„ÄÇÂè¶Â§ñ7zÂéãÁº©ÁöÑÊñá‰ª∂Êõ¥Â∞èÔºåÊé®Ëçê„ÄÇ

"
ËÉΩÂê¶Âú®‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁöÑËøáÁ®ã‰∏≠‰ΩøÁî®ÊåáÂÆöÂàÜËØçÂô®ÂÜçÂÅöÂêéÁª≠Â§ÑÁêÜÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®‰ΩìÈ™åËøáÊÑüÁü•Êú∫ÂàÜËØçÂêéÔºåÁ°ÆÂÆûÊÑüÂèóÂà∞ËØ≠ÊñôÂ∫ìÁöÑÈáçË¶ÅÊÄßÔºåÂêåÊó∂ÂèØ‰ª•ÊîØÊåÅÂú®Á∫øÂ¢ûÈáèËÆ≠ÁªÉ‰ª•ÂêéÊñπ‰æø‰∫ÜËÆ∏Â§ö„ÄÇ
ÁõÆÂâçÊàëËøôËæπÈ°πÁõÆ‰∏≠ÈúÄË¶Å‰ΩøÁî®‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂêéÂÜçÂÅöËßÑÂàôÂ§ÑÁêÜÔºåÁõÆÂâçËÄÉËôëÊòØ‰ΩøÁî®nnparserÔºåÁúã‰∫Ü‰∏ã‰πãÂâçÁöÑ‰ª£Á†Å„ÄÇ‰∏™‰∫∫ÁêÜËß£ÊòØÂÖàÂàÜËØçÂêéÂÜçËøõË°åÂè•Ê≥ïÂàÜÊûêÂ§ÑÁêÜÔºåËØ∑ÈóÆÊòØÂê¶ËÉΩÂ§üÊåáÂÆönnparserÊâÄ‰ΩøÁî®ÁöÑÂè•Ê≥ïÂàÜÊûêÂô®Ôºå‰æãÂ¶ÇÊõøÊç¢ÊàêÊÑüÁü•Êú∫ÂàÜËØçÔºüÔºàÂõ†‰∏∫Âú®ÊÑüÁü•Êú∫ÂàÜËØçÁöÑÊ®°ÂûãÂ¢ûÈáèËÆ≠ÁªÉ‰∫ÜËá™Â∑±ÁöÑ‰∏ìÊúâËØçËØ≠ÔºåÂ∏åÊúõÁªü‰∏ÄÁª¥Êä§„ÄÇÔºâ

ÊàëÁúãÂà∞nnparserÂú®26Â§©ÂâçÊèê‰∫§‰∫ÜÊûÑÈÄ†ÂáΩÊï∞ÊîØÊåÅ‰º†ÂÖ•segment„ÄÇÊòØÂê¶Â∞±ÊòØÊÑèÂõæÊîØÊåÅnnparserÊåáÂÆö‰∏çÂêåÁöÑÂàÜËØçÂô®Ôºü
Â¶ÇÊûúËøô‰∏™ÊîπÂä®ÊòØËøô‰∏™ÊÑèÂõæÁöÑËØùÔºåÊÉ≥ËØ∑ÈóÆ‰∏Ä‰∏ãÂ§ßÊ¶Ç‰ΩïÊó∂‰ºöÊääËøô‰∏™featureÁªôrelease‰∏Ä‰∏™ÁâàÊú¨Âá∫Êù•Ôºü

Â¶ÇÊûúÁü≠Êúü‰∏ç‰ºöÊîæÂá∫Ëøô‰∏™releaseÁöÑËØùÔºåÊòØÂê¶ÊàëÂè™ÈúÄË¶ÅÊ†πÊçÆËøôÊ¨°Êèê‰∫§ÊâÄ‰øÆÊîπÁöÑnnparserÁöÑ‰ª£Á†ÅÂíåabstractLexicalAnalyzerËøõË°å‰øÆÊîπÂ∞±ËÉΩÊîØÊåÅÔºüÔºàÊöÇÊó∂ÁúãÂà∞ËøôÊ¨°Êèê‰∫§Âè™ÊúâËøô2‰∏™ÊîπÂä®Ôºå‰∏çÁü•ÊòØÂê¶ËøòÊúâÂÖ∂‰ªñÈúÄË¶Å‰øÆÊîπÁöÑÂú∞ÊñπÔºâ
"
ÊÑüÁü•Êú∫ËÆ≠ÁªÉÂØπÁ°¨‰ª∂ÁöÑË¶ÅÊ±Ç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv 1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv 1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÈóÆÈ¢ò
‰ΩøÁî®ÁªìÊûÑÂåñÊÑüÁü•Êú∫Ê†áÊ≥®Ê°ÜÊû∂ËÆ≠ÁªÉËá™Â∑±ÁöÑËØçÊÄßÊ†áÊ≥®Ê®°ÂûãÊó∂ÂÜÖÂ≠òÊ∫¢Âá∫„ÄÇ
1.Êñ∞ÈóªËØ≠ÊñôÊñá‰ª∂Â§πÂ§ßÂ∞è1.7gÂ∑¶Âè≥
2.‰ΩøÁî®ÂëΩ‰ª§Ôºöjava -Xmx30720m -cp hanlp-portable-1.6.8.jar:src/main/resources com.hankcs.hanlp.model.perceptron.Main -task POS -train true -model pos.bin -reference test
3.Á≥ªÁªüÊÄßËÉΩÂ¶Ç‰∏ãÔºö
![image](https://user-images.githubusercontent.com/18524483/47700860-e3147d80-dc52-11e8-957a-65fc6c061bb1.png)
4.ËøêË°åÁªìÊûúÂ¶Ç‰∏ãÔºö
![a5f85674429ccc04acaf8366e186c03](https://user-images.githubusercontent.com/18524483/47700984-50281300-dc53-11e8-9e68-a6ab27f0e81c.png)
ËØ∑ÈóÆÔºö
1.ËØçÊÄßÊ†áÊ≥®Ê®°ÂûãËÆ≠ÁªÉÂØπÂÜÖÂ≠òÁöÑË¶ÅÊ±ÇÔºåÂè¶Â§ñÁâπÂæÅÊÄªÊï∞ËææÂà∞ÂÖ≠‰∫øÂ§öËøôÊ≠£Â∏∏ÂêóÔºü
2.ÊÇ®ÂΩìÊó∂ËÆ≠ÁªÉÂàÜËØçÊ®°Âûãlarge.binÁöÑÊó∂ÂÄôÁöÑÁ°¨‰ª∂ÁéØÂ¢É
ÊúüÊúõÂ∞ΩÂø´ÂæóÂà∞ÊÇ®ÁöÑËµêÊïôÔºåË∞¢Ë∞¢ÔºÅ





"
ÂÖ≥‰∫éelasticsearchË°çÁîüÈ°πÁõÆÁöÑÈóÆÈ¢ò,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.8

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

## ÊàëÁöÑÈóÆÈ¢ò

[Ë°çÁîüÈ°πÁõÆÁöÑwiki](https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE) ÈáåËØ¥elasticsearch ÁöÑ‰∏§‰∏™Êèí‰ª∂ÈÉΩË∑ë‰∏çËµ∑Êù•

1.  hanlp-ext @hualongdata
2. https://github.com/shikeio/elasticsearch-analysis-hanlp

ÂèçËÄåÊòØËøô‰∏™Ê≤°ÂàóÂá∫ https://github.com/KennFalcon/elasticsearch-analysis-hanlp ËÉΩË∑ëËµ∑Êù•„ÄÇ
Á¨¨‰∫å‰∏™ÈóÆÈ¢òÂíåËøô‰∏™Ë°çÁîüÈ°πÁõÆÊúâÂÖ≥‰πüÁªôÂÆÉÂºÄ‰∫Üissue ÔºåÂÆÉÁõÆÂâçÊîØÊåÅ1.6.6Ôºå ÊàëÂ¶ÇÊûúÁõ¥Êé•ÊõøÊç¢Êàê1.6.8ÁöÑdata‰∏ç‰ºöÊúâÈóÆÈ¢òÂêßÔºü"
ËÉΩÂê¶Âü∫‰∫éWord2VecTrainerÂÅöword2vecÁöÑÂ¢ûÈáèËÆ≠ÁªÉÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1„ÄÅÊàëÂú®Â∞ùËØïÁî®HanLPÁöÑWord2VecÂÅöÊñáÊ°£Áõ∏‰ººÊÄßÂàÜÊûêÔºåÊÉ≥ÈóÆ‰∏ãÈÄöËøáWord2VecTrainerÂèØÂê¶ÂÅöword2vecÂêëÈáèËØçÁöÑÂ¢ûÈáèËÆ≠ÁªÉÔºåÊàëÂÅö‰∫ÜÂ∞ùËØïÔºåÂèëÁé∞‰ºöË¶ÜÁõñÂ∑≤ÊúâÊñá‰ª∂ÔºåËØ∑ÈóÆÊòØÂê¶ÊúâÂÖ∂‰ªñÊñπÊ≥ïÔºåË∞¢Ë∞¢ÔºÅ

"
Âú®Á∫øÂ≠¶‰π†Âπ∂Â∫èÂàóÂåñÂêéÁöÑÊñ∞Ê®°ÂûãÊó†Êïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!---->
Âú®Á∫øÂ≠¶‰π†ÂêéÁöÑÊñ∞Ê®°ÂûãÊó†Êïà
## Â§çÁé∞ÈóÆÈ¢ò
<!--  -->
 ÊÇ®Â•ΩÔºåÊÑüË∞¢ÊÇ®ÁöÑÂºÄÊ∫êÔºÅÊàëÊ≠£Âú®Â∞ùËØïÊÑüÁü•Êú∫ÁöÑÂú®Á∫øÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÁî®learnÊñπÊ≥ïÂú®Á∫øÂ≠¶‰π†ÂêéÔºåÁî®getPerceptronSegmenter().getModel().save()ËøõË°å‰∫ÜÂ∫èÂàóÂåñÂπ∂‰øùÂ≠òÂà∞Ê®°ÂûãÔºå‰ΩÜÊòØÊ≥®ÈáäÊéâÂ∫èÂàóÂåñÂíålearn‰πãÂêéÂÜçÊ¨°Ë∞ÉÁî®DemoPerceptronLexicalAnalyzer.javaÂπ∂Ê≤°ÊúâËææÂà∞Âú®Á∫øÂ≠¶‰π†ÁöÑÊïàÊûú 

### Ê≠•È™§

1. È¶ñÂÖàÊääDemoPerceptronLexicalAnalyzer.javaÊ∫ê‰ª£Á†Å‰∏≠Â∫èÂàóÂåñÁöÑÊ≥®ÈáäÂéªÊéâÔºåÁÑ∂ÂêéËøêË°åÔºåÂæóÂà∞Ê≠£Á°ÆÁöÑÂú®Á∫øÂ≠¶‰π†ÁªìÊûúÔºåÂπ∂Â∫èÂàóÂåñË¶ÜÁõñÂéüÊù•ÁöÑCWS.binÊ®°Âûã
2. ÁÑ∂ÂêéÊ≥®ÈáäÊéâÂ∫èÂàóÂåñËØ≠Âè•ÂíåÂú®Á∫øÂ≠¶‰π†learnÊñπÊ≥ï
3. Êé•ÁùÄÂÜçÊ¨°ËøêË°åDemoPerceptronLexicalAnalyzer.java

### Ëß¶Âèë‰ª£Á†Å

```
 public class DemoPerceptronLexicalAnalyzer extends TestUtility
{
    public static void main(String[] args) throws IOException
    {
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                                                                           HanLP.Config.PerceptronPOSModelPath,
                                                                           HanLP.Config.PerceptronNERModelPath);

        // ‰ªª‰ΩïÊ®°ÂûãÊÄª‰ºöÊúâÂ§±ËØØÔºåÁâπÂà´ÊòØ98Âπ¥ËøôÁßçÈôàÊóßÁöÑËØ≠ÊñôÂ∫ì
        System.out.println(analyzer.analyze(""ÊÄªÁªüÊôÆ‰∫¨‰∏éÁâπÊúóÊôÆÈÄöÁîµËØùËÆ®ËÆ∫Â§™Á©∫Êé¢Á¥¢ÊäÄÊúØÂÖ¨Âè∏""));
        // ÊîØÊåÅÂú®Á∫øÂ≠¶‰π†
        //analyzer.learn(""‰∏é/c ÁâπÊúóÊôÆ/nr ÈÄö/v ÁîµËØù/n ËÆ®ËÆ∫/v [Â§™Á©∫/s Êé¢Á¥¢/vn ÊäÄÊúØ/n ÂÖ¨Âè∏/n]/nt"");

        // Â≠¶‰π†Âà∞Êñ∞Áü•ËØÜ
        System.out.println(analyzer.analyze(""ÊÄªÁªüÊôÆ‰∫¨‰∏éÁâπÊúóÊôÆÈÄöÁîµËØùËÆ®ËÆ∫Â§™Á©∫Êé¢Á¥¢ÊäÄÊúØÂÖ¨Âè∏""));
        // ËøòÂèØ‰ª•‰∏æ‰∏ÄÂèç‰∏â
        System.out.println(analyzer.analyze(""‰∏ªÂ∏≠ÂíåÁâπÊúóÊôÆÈÄöÁîµËØù""));

        // Áü•ËØÜÁöÑÊ≥õÂåñ‰∏çÊòØÊ≠ªÊùøÁöÑËßÑÂàôÔºåËÄåÊòØÊØîËæÉÁÅµÊ¥ªÁöÑÁªüËÆ°‰ø°ÊÅØ
        System.out.println(analyzer.analyze(""ÊàëÂú®ÊµôÊ±üÈáëÂçéÂá∫Áîü""));
        //analyzer.learn(""Âú®/p ÊµôÊ±ü/ns ÈáëÂçé/ns Âá∫Áîü/v"");

        System.out.println(analyzer.analyze(""ÊàëÂú®ÂõõÂ∑ùÈáëÂçéÂá∫ÁîüÔºåÊàëÁöÑÂêçÂ≠óÂè´ÈáëÂçé""));

        // Âú®Á∫øÂ≠¶‰π†ÂêéÁöÑÊ®°ÂûãÊîØÊåÅÂ∫èÂàóÂåñÔºå‰ª•ÂàÜËØçÊ®°Âûã‰∏∫‰æãÔºö
        //analyzer.getPerceptronSegmenter().getModel().save(HanLP.Config.PerceptronCWSModelPath);

        // ËØ∑Áî®Êà∑ÊåâÈúÄÊâßË°åÂØπÁ©∫Ê†ºÂà∂Ë°®Á¨¶Á≠âÁöÑÈ¢ÑÂ§ÑÁêÜÔºåÂè™Êúâ‰Ω†ÊúÄÊ∏ÖÊ•öËá™Â∑±ÁöÑÊñáÊú¨‰∏≠ÈÉΩÊúâ‰∫õ‰ªÄ‰πàÂ•áÊÄ™ÁöÑ‰∏úË•ø
        System.out.println(analyzer.analyze(""Á©∫Ê†º \t\n\r\f&nbsp;ÁªüÁªüÈÉΩ‰∏çË¶Å""
                                                .replaceAll(""\\s+"", """")    // ÂéªÈô§ÊâÄÊúâÁ©∫ÁôΩÁ¨¶
                                                .replaceAll(""&nbsp;"", """")  // Â¶ÇÊûú‰∏Ä‰∫õÊñáÊú¨‰∏≠Âê´ÊúâhtmlÊéßÂà∂Á¨¶
        ));
    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊÄªÁªü/n ÊôÆ‰∫¨/nr ‰∏é/p ÁâπÊúóÊôÆ/nr ÈÄö/v ÁîµËØù/n ËÆ®ËÆ∫/v [Â§™Á©∫/s Êé¢Á¥¢/vn ÊäÄÊúØ/n ÂÖ¨Âè∏/n]/nt
ÊÄªÁªü/n ÊôÆ‰∫¨/nr ‰∏é/p ÁâπÊúóÊôÆ/nr ÈÄö/v ÁîµËØù/n ËÆ®ËÆ∫/v [Â§™Á©∫/s Êé¢Á¥¢/vn ÊäÄÊúØ/n ÂÖ¨Âè∏/n]/nt
‰∏ªÂ∏≠/n Âíå/c ÁâπÊúóÊôÆ/nr ÈÄö/v ÁîµËØù/n
Êàë/r Âú®/p ÊµôÊ±ü/ns ÈáëÂçé/ns Âá∫Áîü/v
Êàë/r Âú®/p ÂõõÂ∑ù/ns ÈáëÂçé/ns Âá∫Áîü/v Ôºå/w Êàë/r ÁöÑ/u ÂêçÂ≠ó/n Âè´/v ÈáëÂçé/nr
Á©∫Ê†º/n ÁªüÁªü/d ÈÉΩ/d ‰∏çË¶Å/d
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÊÄªÁªü/n ÊôÆ‰∫¨/nr ‰∏é/p ÁâπÊúóÊôÆ/b ÈÄö/vn ÁîµËØù/n ËÆ®ËÆ∫/v Â§™Á©∫/s Êé¢Á¥¢/vn ÊäÄÊúØ/n ÂÖ¨Âè∏/n
ÊÄªÁªü/n ÊôÆ‰∫¨/nr ‰∏é/p ÁâπÊúóÊôÆ/b ÈÄö/vn ÁîµËØù/n ËÆ®ËÆ∫/v Â§™Á©∫/s Êé¢Á¥¢/vn ÊäÄÊúØ/n ÂÖ¨Âè∏/n
‰∏ªÂ∏≠/n Âíå/c ÁâπÊúóÊôÆ/b ÈÄö/vn ÁîµËØù/n
Êàë/r Âú®/p [ÊµôÊ±ü/ns ÈáëÂçé/nz]/nt Âá∫Áîü/v
Êàë/r Âú®/p ÂõõÂ∑ù/ns ÈáëÂçé/nr Âá∫Áîü/v Ôºå/w Êàë/r ÁöÑ/u ÂêçÂ≠ó/n Âè´/v ÈáëÂçé/nr
Á©∫Ê†º/n ÁªüÁªü/d ÈÉΩ/d ‰∏çË¶Å/d
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!---->
CWS.binÊòØÁî®pku98ËÆ≠ÁªÉÁöÑÔºõ
ÊàëËÉΩÁ°ÆÂÆöÂ∫èÂàóÂåñÂ∑≤ÁªèÁîüÊïàÔºåÊúâ‰∏ÄÊ¨°saveÂà∞config.POSÈáåÈù¢Âéª‰∫ÜÔºåÂÜçËøêË°åÁöÑÊó∂ÂÄô‰ºöÊä•ÈîôÊ®°ÂûãËæìÂÖ•ÊúâËØØÔºåÈáçÊñ∞ËÆ≠ÁªÉ‰∫ÜPOSÊâçËÉΩÁªßÁª≠ËøêË°åÔºõ
ÊÑüË∞¢ÊÇ®ÁöÑÊó∂Èó¥ÔºÅ

ÂÅö‰∫Ü‰∫Ü‰∏§Â§©ÂÆûÈ™åÂèàÂèëÁé∞‰∫ÜÊñ∞ÈóÆÈ¢òÔºåÂ¶ÇÊûúÂçïÁã¨‰∏∫Êüê‰∏ÄÁ±ªÂÆû‰ΩìÁâπÂú∞ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™NERÊ®°ÂûãÔºåËøô‰∏™Ê®°ÂûãÊòØË¶ÜÁõñÂú®Âü∫Á°ÄËØ≠ÊñôÊ®°Âûã‰∏äÔºåËøòÊòØÂè¶Â§ñÂÇ®Â≠òÔºüÂ¶ÇÊûúÊòØË¶ÜÁõñÁöÑËØùÔºåÊúâÊ®°ÂûãÂ∫èÂàóÂåñÂà∞Ê®°ÂûãÁöÑÊñπÊ≥ïÂêóÔºüÂ¶ÇÊûúÊòØÂè¶Â§ñÂÇ®Â≠òÔºåÈÇ£‰∏ãÊ¨°Ë∞ÉÁî®analyzeÊñπÊ≥ïÁöÑÊó∂ÂÄôÈúÄË¶Å‰∏ÄÊ¨°Êää‰∏§‰∏™Ê®°ÂûãÈÉΩËØªÂèñÊâçË°åÔºü
Ë∞¢Ë∞¢ÔºÅ

"
Â∏¶Ë°®ÊÉÖÁ¨¶ÁöÑCRFÂàÜËØçÂá∫Áé∞ÁºñÁ†ÅÈîôËØØ UnicodeEncodeError,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
hanlp-1.6.3.jar
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
pyhanlp
ÁéØÂ¢ÉÊòØpython 3.6
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
text = '‰∏ªÂä®ÊèêÂá∫ÊãøÂÆùÂÆùÊ§ÖÔºåüëç„ÄÇÂçÅÂùóÈí±üòÑ'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)

Âá∫Áé∞‰ª•‰∏ãÈîôËØØÔºöUnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed

ËÄåÁî®HanLP.segment(text)ÂàôÊòØÊ≠£Â∏∏ÁöÑ„ÄÇÂéªÊéâüëçÂíåüòÑÔºåCRF‰πüÊòØÊ≠£Â∏∏ÁöÑ„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
 text = '‰∏ªÂä®ÊèêÂá∫ÊãøÂÆùÂÆùÊ§ÖÔºåüëç„ÄÇÂçÅÂùóÈí±üòÑ'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ΩøÁî®HanLP.segment()ÂàÜËØçÂØºËá¥pythonÂÅúÊ≠¢Â∑•‰Ωú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ΩøÁî®HanLP.segment()ÂØπÂ≠óÁ¨¶‰∏≤ËøõË°åÂàÜËØçÊó∂ÂØºËá¥pythonÂÅúÊ≠¢Â∑•‰Ωú„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

```
except_list = []
for i,item in enumerate(train_content):
    try:
        #item = text_seg(item,seg_tool='hanlp')
        item=HanLP.segment(item) # Ëß¶ÂèëÈóÆÈ¢òËØ≠Âè•
        item=[term.word for term in item]
        train_content[i] = item
        print('sent: ', i, ' ,', item)
        if i % 1000 == 0:
            print(str(i)+' lines had been segmented')
    except:
        except_list.append(i)
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->



![image](https://user-images.githubusercontent.com/44291990/47215523-46d9b380-d3d4-11e8-83ae-225f264c669c.png)
![image](https://user-images.githubusercontent.com/44291990/47215609-a041e280-d3d4-11e8-9a33-d0b4898eb8c6.png)




## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êñ∞ËØçÂèëÁé∞Âú®300mbÁöÑËØ≠Êñô‰∏äÊä•ÂÜÖÂ≠ò‰∏çÂ§ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp-0.1.44
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp-0.1.44

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Áî®pyhanlpÁöÑÊé•Âè£Âú®300mbÁöÑËØ≠Êñô‰∏äÂÅöÊñ∞ËØçÂèëÁé∞ÔºåÊúçÂä°Âô®ÂÜÖÂ≠ò‰∏∫128gÔºåÊä•‰∫Üout of memoryÔºåÊúâ‰ªÄ‰πà‰ºòÂåñÊàñËÄÖËß£ÂÜ≥ÁöÑÂäûÊ≥ïÂêó„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Êñ∞ËØç
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
jpype._jexception.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
hanlpÁöÑÂàÜËØçÂú®sighan2005ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËØ∑ÈóÆÊúâÊ≤°ÊúâÂØπÊØîhanlpÁöÑÂàÜËØçÊïàÊûúÂú®sighan2005ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞ÔºüË∞¢Ë∞¢~


"
"""Â§ßÂÆ∂Áîµ""ÂàÜËØçÈóÆÈ¢ò","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
""Â§ßÂÆ∂Áîµ""ÂàÜËØçÈîôËØØ,ÂàáÂàÜ‰∏∫""Â§ßÂÆ∂""Âíå""Áîµ"", Â∫îËØ•ÂàáÂàÜ‰∏∫""Â§ß""Âíå""ÂÆ∂Áîµ"",ËØ∑ÈóÆÂ¶Ç‰Ωï‰øÆÊîπÊ†∏ÂøÉËØçÂÖ∏Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò.
ÊàëÈÄöËøáÂà†Èô§""Â§ßÂÆ∂""Ëøô‰∏™ËØç,ÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò,‰ΩÜÊòØËøôÊ†∑‰πãÂêé,""Â§ßÂÆ∂""Ëøô‰∏™ËØçÂ∞±ÂàáÂàÜ‰∏çÂá∫Êù•‰∫Ü.

## Â§çÁé∞ÈóÆÈ¢ò
‰ΩøÁî®viterbiÂàÜËØç""Â§ßÂÆ∂Áîµ""Âç≥ÂèØÂ§çÁé∞Ê≠§ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""Â§ßÂÆ∂Áîµ""));
    }
```
### ÊúüÊúõËæìÂá∫
```
""Â§ß""Âíå""ÂÆ∂Áîµ""
```
### ÂÆûÈôÖËæìÂá∫
```
""Â§ßÂÆ∂""Âíå""Áîµ""
```


"
ÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÂØπÂ∏ÅÁßçËΩ¨Êç¢ÈîôËØØÁöÑÈóÆÈ¢ò,"Hi,

Ë∞ÉÁî®HanLP.convertToSimplifiedChinese()ÂÅöÁπÅ‰ΩìÂà∞ÁÆÄ‰ΩìÁöÑËΩ¨Êç¢Ôºå‰ΩÜÊòØÂØπ‰∫é‰∏ãÈù¢ÁöÑËøô‰∫õÂ∏ÅÁßçÔºåÂ≠òÂú®‰∏ãÈù¢ÁöÑÈóÆÈ¢òÔºåËøòÁÉ¶Âä≥Áúã‰∏ãÊÄé‰πà‰øÆÊîπËøô‰∏™ÈóÆÈ¢òÔºö
Ëç∑Â±ûÂÆâÁöÑÂàóÊñØÁõæ ‚Äî‚Äî> Ëç∑Â±ûÂÆâÁöÑÂàóÊñØÊñØÁõæ
Â°îÂêâÂÖãÂç¢Â∏É ‚Äî‚Äî> Â°îÂêâÂÖãÊñØÂù¶Âç¢Â∏É
Â∑¥ÂìàÈ©¨Â∏Å ‚Äî‚Äî> Â∑¥Ëµ´È©¨Â∏Å
ÂéÑÁìúÂ§öÂ∞îUCV ‚Äî‚Äî> ÂéÑÁìúÂ§öÂ∞îÂ∞îUCV

Â∑¶ËæπÊòØËæìÂÖ•ÁöÑQueryÔºåÂè≥ËæπÊòØËΩ¨Êç¢ÂêéÁöÑÁªìÊûúÔºåÂè≥ËæπËΩ¨Êç¢ÈÉΩÊòØÊúâÈóÆÈ¢òÁöÑ„ÄÇ"
Âä†ËΩΩCustomDictionary.txt.binÊäõÂá∫java.lang.IllegalArgumentExceptionÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
CustomDictionaryÊ∑ªÂä†‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏(1.35GB), Âπ∂Âà†Èô§ÁºìÂ≠òÈáçÊñ∞ÁîüÊàê‰∫Ü‰∏Ä‰∏™3GB+ÁöÑCustomDictionary.txt.bin

ÂΩìÁ®ãÂ∫èÂÜçÊ¨°ËøêË°åÊó∂... Âä†ËΩΩCustomDictionary.txt.binÂ§±Ë¥•
readBytes‰∏≠ÊäõÂá∫java.lang.IllegalArgumentExceptionÂºÇÂ∏∏

INFO: Ëá™ÂÆö‰πâËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:../../myproject/data/dictionary/custom/CustomDictionary.txt
ÂçÅÊúà 16, 2018 3:12:39 ‰∏ãÂçà **com.hankcs.hanlp.corpus.io.IOUtil readBytes**
**WARNING: ËØªÂèñ../../myproject/data/dictionary/custom/CustomDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.lang.IllegalArgumentException**
ÂçÅÊúà 16, 2018 3:12:39 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
INFO: ‰ª•ÈªòËÆ§ËØçÊÄß[n]Âä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏../../myproject/data/dictionary/custom/CustomDictionary.txt‰∏≠‚Ä¶‚Ä¶

Â∞ùËØïÈáçÊñ∞ÁîüÊàê‰∫Ü‰∏Ä‰∏™Â∞èËØçÂÖ∏ ÊòØËÉΩÊ≠£Â∏∏ËøêË°åÁöÑ ÊòØËØçÂÖ∏ÁºìÂ≠òËøáÂ§ßÂØºËá¥?
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

"
pyhanlpÁöÑÊñ∞ËØçÂèëÁé∞Ê≤°ÊúâÊ∂âÂèäËã±ÊñáÁöÑÊñ∞ËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp-0.1.44
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp-0.1.44

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Áî®pyhanlpÁöÑÊñ∞ËØçÂèëÁé∞Êé•Âè£‰∏ç‰ºöÂá∫Áé∞Ê∂âÂèäËã±ÊñáÂçïËØçÁöÑÊñ∞ËØçÔºåÂêåÊ†∑‰ΩøÁî®Â∑¶Âè≥ÁÜµÂíå‰∫í‰ø°ÊÅØÁöÑÂè¶Â§ñ‰∏Ä‰∏™ÁâàÊú¨ÂÆûÁé∞‰ºöÂèëÁé∞Ëøô‰∏™ËØçÔºåÊòØhanlpÂØπÊâÄÊúâÁöÑËã±ÊñáÈÉΩËøáÊª§Êéâ‰∫ÜÂêóÔºüÂèØ‰ª•Êúâ‰ªÄ‰πàÂäûÊ≥ï‰øùÁïôËã±ÊñáÂêóÔºü

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
KÁ∫øÂõæ
AËÇ°Ë¥¶Êà∑
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Êó†Ê∂âÂèäËã±ÊñáÁöÑÊñ∞ËØç
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÆâË£ÖÂÆåpyhanlpÂíåjpype‰πãÂêéÔºåModuleNotFoundError: No module named '_jpype',"import from pyhanlp import *
Êä•ÈîôÔºö+
Traceback (most recent call last):
  File ""/home/vip/wusaifei/project/preprocess.py"", line 2, in <module>
    from pyhanlp import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 11, in <module>
    from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/__init__.py"", line 17, in <module>
    from ._jpackage import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/_jpackage.py"", line 18, in <module>
    import _jpype
ModuleNotFoundError: No module named '_jpype'"
pyhanlpÂÆâË£ÖJDK‰πãÂêéÊä•Èîô,"Êä•Èîô‰ø°ÊÅØ
Traceback (most recent call last):
  File ""C:/zengxianfeng/lac/utils.py"", line 4, in <module>
    from pyhanlp import *
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\pyhanlp\__init__.py"", line 116, in <module>
    _start_jvm_for_hanlp()
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\pyhanlp\__init__.py"", line 108, in _start_jvm_for_hanlp
    getDefaultJVMPath(),
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\jpype\_core.py"", line 121, in get_default_jvm_path
    return finder.get_jvm_path()
  File ""C:\zengxianfeng\Anaconda\lib\site-packages\jpype\_jvmfinder.py"", line 153, in get_jvm_path
    .format(self._libfile))
jpype._jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properl
ÁéØÂ¢ÉÂèòÈáèÈáåÈù¢Â∑≤ÁªèÊ∑ªÂä†‰∫Ü JAVA_HOME C:\zengxianfeng\Anaconda\Lib\site-packages\jdk"
com.hankcs.hanlp.utility.TestUtilityÊâæ‰∏çÂà∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

ÂΩìÊàëÂú®ËøêË°åDemoSentimentAnalysisÊó∂Ôºåimport com.hankcs.hanlp.utility.TestUtilityÊä•ÈîôÔºåÂèëÁé∞Âú®utility‰∏≠‰∏çÂ≠òÂú®TestUtility

"
ÂèØ‰ª•Áî®Êù•ÂÅöËæìÂÖ•Ê≥ïÁöÑËÅîÊÉ≥ËØçÂòõ,
ÊÑüÁü•Êú∫Êó†Ê≥ïÂØπËã±ÊñáËøõË°åÂàÜËØçÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

ÊòØÂê¶ÊòØËã±ÊñáËÆ≠ÁªÉÁöÑÊ®°ÂûãÊ≤°ÊúâÂä†ÂÖ•Ëã±ÊñáÔºåÊâÄ‰ª•Êó†Ê≥ïÂØπËã±ÊñáËøõË°åÂàÜËØçÔºü

### Ëß¶Âèë‰ª£Á†Å

```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
   String str = ""Apache Commons is an Apache project focused on all aspects of reusable Java components. "";
   System.out.println(analyzer.analyze(str));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Apache/nx,  /w, Commons/nx,  /w, is/nx,  /w, an/nx,  /w, Apache/nx,  /w, project/nx,  /w, focused/nx,  /w, on/nx,  /w, all/nx,  /w, aspects/nx,  /w, of/nx,  /w, reusable/nx,  /w, Java/nx,  /w, components/nx, ./w,  /w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Apache Commons is an Apache project focused on all aspects of reusable Java components. /nr
```


"
Â¶Ç‰ΩïÊîπËøõËé∑ÂèñÊëòË¶ÅÁöÑÁªìÊûú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Â¶Ç‰ΩïÂèØ‰ª•‰ΩøÂæóËé∑ÂèñÊëòË¶ÅÁöÑÁªìÊûúÊõ¥‰∏∫Á¨¶ÂêàÈ¢òÊÑèÔºü
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàËæìÂÖ•Ë¶ÅËé∑ÂèñÊëòË¶ÅÁöÑÂÜÖÂÆπÔºå
       String answerString = ""‰∏ÄÔºåËæõ‰∫•Èù©ÂëΩÁªôÂ∞ÅÂª∫‰∏ìÂà∂Âà∂Â∫¶‰ª•Ëá¥ÂëΩÁöÑ‰∏ÄÂáª„ÄÇ‰∫åÔºåËæõ‰∫•Èù©ÂëΩÊé®Áøª‰∫Ü‚ÄúÊ¥ã‰∫∫ÁöÑÊúùÂª∑‚Äù, Ê≤âÈáçÊâìÂáª‰∫ÜÂ∏ùÂõΩ‰∏ª‰πâÁöÑ‰æµÁï•ÂäøÂäõ„ÄÇ‰∏âÔºåËæõ‰∫•Èù©ÂëΩ‰∏∫Ê∞ëÊóèËµÑÊú¨‰∏ª‰πâÁöÑÂèëÂ±ïÂàõÈÄ†‰∫ÜÊúâÂà©ÁöÑÊù°‰ª∂„ÄÇÂõõÔºåËæõ‰∫•Èù©ÂëΩÂØπËøë‰ª£‰∫öÊ¥≤ÂêÑÂõΩË¢´ÂéãËø´Ê∞ëÊóèÁöÑËß£ÊîæËøêÂä®‰∫ßÁîü‰∫ÜÊØîËæÉÂπøÊ≥õÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÊòØÂØπË∂äÂçó„ÄÅÂç∞Â∫¶Â∞ºË•ø‰∫öÁ≠âÂõΩÂÆ∂ÁöÑÂèçÂØπÊÆñÊ∞ë‰∏ª‰πâÁöÑÊñó‰∫âËµ∑‰∫ÜÊé®Âä®‰ΩúÁî®„ÄÇ"";
2. ÁÑ∂ÂêéË∞ÉÁî®HanLP.extractSummaryËé∑ÂèñÊëòË¶Å
3. Êé•ÁùÄÂØπÊëòË¶ÅÁªìÊûúËøõË°åÂàÜÊûê„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
   List<String> summaryList = HanLP.extractSummary(answerString, 4, ""[Ôºå,„ÄÇ:Ôºö‚Äú‚ÄùÔºü?ÔºÅ!Ôºõ;„ÄÅ]"");
		 for(String summary : summaryList){
			 System.out.println(summary);
		 }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Ëæõ‰∫•Èù©ÂëΩÁªôÂ∞ÅÂª∫‰∏ìÂà∂Âà∂Â∫¶‰ª•Ëá¥ÂëΩÁöÑ‰∏ÄÂáª„ÄÇ
Ëæõ‰∫•Èù©ÂëΩÊé®Áøª‰∫Ü‚ÄúÊ¥ã‰∫∫ÁöÑÊúùÂª∑‚Äù, Ê≤âÈáçÊâìÂáª‰∫ÜÂ∏ùÂõΩ‰∏ª‰πâÁöÑ‰æµÁï•ÂäøÂäõ„ÄÇ
Ëæõ‰∫•Èù©ÂëΩ‰∏∫Ê∞ëÊóèËµÑÊú¨‰∏ª‰πâÁöÑÂèëÂ±ïÂàõÈÄ†‰∫ÜÊúâÂà©ÁöÑÊù°‰ª∂„ÄÇ
Ëæõ‰∫•Èù©ÂëΩÂØπËøë‰ª£‰∫öÊ¥≤ÂêÑÂõΩË¢´ÂéãËø´Ê∞ëÊóèÁöÑËß£ÊîæËøêÂä®‰∫ßÁîü‰∫ÜÊØîËæÉÂπøÊ≥õÁöÑÂΩ±Âìç„ÄÇ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Ëæõ‰∫•Èù©ÂëΩÊé®Áøª‰∫Ü
Ëæõ‰∫•Èù©ÂëΩÁªôÂ∞ÅÂª∫‰∏ìÂà∂Âà∂Â∫¶‰ª•Ëá¥ÂëΩÁöÑ‰∏ÄÂáª
Ëæõ‰∫•Èù©ÂëΩÂØπËøë‰ª£‰∫öÊ¥≤ÂêÑÂõΩË¢´ÂéãËø´Ê∞ëÊóèÁöÑËß£ÊîæËøêÂä®‰∫ßÁîü‰∫ÜÊØîËæÉÂπøÊ≥õÁöÑÂΩ±Âìç
Âç∞Â∫¶Â∞ºË•ø‰∫öÁ≠âÂõΩÂÆ∂ÁöÑÂèçÂØπÊÆñÊ∞ë‰∏ª‰πâÁöÑÊñó‰∫âËµ∑‰∫ÜÊé®Âä®‰ΩúÁî®
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ≠ÊñôÂàÜÁ±ªËÆ≠ÁªÉÂáÜÁ°ÆÁéá‰ΩéÔºåÈúÄË¶ÅÂ¶Ç‰ΩïÊèêÈ´òÂë¢,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Êàë‰ΩøÁî®ËØ≠ÊñôÂ∫ì ËøõË°åÂàÜÂâ≤ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÁöÑÊµãËØïÔºå‰∏∫‰ªÄ‰πàÂáÜÁ°ÆÁéáËøô‰πà‰ΩéÂë¢ÔºåÊòØËØ≠ÊñôÂ∫ìÁöÑÂéüÂõ†Ôºå
ËøòÊòØÈúÄË¶ÅÂ¶Ç‰ΩïËÆ≠ÁªÉÁöÑÈóÆÈ¢òÂë¢„ÄÇÊàëËØ•Â¶Ç‰ΩïÊèêÈ´òÂáÜÁ°ÆÁéá
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§
ËÄóÊó∂ 7557 ms Âä†ËΩΩ‰∫Ü 9 ‰∏™Á±ªÁõÆ,ÂÖ± 1791 ÁØáÊñáÊ°£
     P       R      F1       A        
 34.82   66.83   45.78   82.41  ‰∫íËÅîÁΩë
 77.19   44.22   56.23   92.35   ‰ΩìËÇ≤
 51.61   24.12   32.88   89.06   ÂÅ•Â∫∑
 61.84   47.24   53.56   90.90   ÂÜõ‰∫ã
 64.02   76.88   69.86   92.63   ÊãõËÅò
 59.52   12.56   20.75   89.34   ÊïôËÇ≤
 26.04   69.35   37.86   74.71   ÊñáÂåñ
 47.06   12.06   19.20   88.72   ÊóÖÊ∏∏
 53.72   50.75   52.20   89.67   Ë¥¢Áªè
 52.87   44.89   48.55   44.89  avg.
data size = 1791, speed = 36551.02 doc/s


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        //CORPUS_FOLDER ÊòØÁΩë‰∏äÊâæÁöÑsogouÂàÜÁ±ªÂ∫ìÔºå‰∏çÊòØminiÁâàÁöÑ
        DataSet trainingCorpus = new FileDataSet().                          // FileDataSetÁúÅÂÜÖÂ≠òÔºåÂèØÂä†ËΩΩÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ
            setTokenizer(new HanLPTokenizer()).                               // ÊîØÊåÅ‰∏çÂêåÁöÑITokenizerÔºåËØ¶ËßÅÊ∫êÁ†Å‰∏≠ÁöÑÊñáÊ°£
            load(CORPUS_FOLDER, ""UTF-8"", 0.9);               // Ââç90%‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜ
        IClassifier classifier = new NaiveBayesClassifier();
        classifier.train(trainingCorpus);
        IDataSet testingCorpus = new MemoryDataSet(classifier.getModel()).
            load(CORPUS_FOLDER, ""UTF-8"", -0.1);        // Âêé10%‰Ωú‰∏∫ÊµãËØïÈõÜ
        // ËÆ°ÁÆóÂáÜÁ°ÆÁéá
        FMeasure result = Evaluator.evaluate(classifier, testingCorpus);
        System.out.println(result);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âä®ÊÄÅÊõ¥Êñ∞ÂÆåËØçÂÖ∏ÊÄéÊ†∑ÂÜôÂÖ•Êñá‰ª∂CustomDictionary.txt.bin,"ÊàëÊòØÊÉ≥Âä®ÊÄÅÊõ¥Êñ∞ËØçÂÖ∏Áî®‰∫éÁé∞ÊúâÁöÑÊêúÁ¥¢ÔºåÂêåÊó∂‰πüÂÜôÂÖ•Êñá‰ª∂ÔºåÂπ∂ÈáçÂª∫CustomDictionary.txt.bin
ËøôÊ†∑Â∞±‰∏çÁî®ÈáçÂêØsolrÊúçÂä°Âô®ÔºåÂπ∂ÈáçÂêØsolrÊúçÂä°Âô®‰ª•ÂêéÔºå‰πü‰∏ç‰ºöÂá∫ÈóÆÈ¢ò„ÄÇ
Êâæ‰∫ÜÈáåÈù¢ÁöÑ‰ª£Á†ÅÔºå‰ΩÜÊòØÊ≤°ÊúâÊâæÂà∞Ôºå ÊòØ‰∏çÊòØÊúâÂÖ∂‰ªñÂäûÊ≥ïÂèØ‰ª•ÂÆûÁé∞Ê≠§ÂäüËÉΩÔºü"
ËØçÂÖ∏‰∏çËÉΩÁî®Á©∫Ê†º,"Âà†Èô§CustomDictionary.txt.bin‰ª•ÂêéÔºå‰ºöÈáçÊñ∞ÁîüÊàêCustomDictionary.txt.bin
‰ΩÜÂú®ÁîüÊàêCustomDictionary.txt.binÁöÑÊó∂‰æØÔºåÂá∫Áé∞Â¶Ç‰∏ãÈîôËØØÔºö
`data/dictionary/custom/words-my.dicËØªÂèñÈîôËØØÔºÅjava.lang.NumberFormatException: For input string: ""Jill""`
Âõ†‰∏∫Êúâ‰∏ÄË°åËÆ∞ÂΩï‰∏∫Ôºö
`Jack N' Jill`
‰∏çÁü•ÈÅìËÉΩ‰∏çËÉΩÁî®Á©∫Ê†ºÂàÜÈöîËá™ÂÆö‰πâËØçÂÖ∏ÔºüÊàñËÄÖÊúâÂÖ∂‰ªñÂäûÊ≥ïÂêóÔºü"
Â∏åÊúõËÉΩÊé®Âá∫goÁâàÊú¨,"Â∏åÊúõËÉΩÊé®Âá∫goÁâàÊú¨
"
pyhanlp‰ΩøÁî®SafeJClassË∞ÉÁî®crfÂàÜËØçÂô®ÊÑèÂõæÂ§öÁ∫øÁ®ãÂàÜËØçÔºåÊä•jvmÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp GitHub master
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp GitHub master


<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
      pyhanlp‰ΩøÁî®SafeJClassË∞ÉÁî®crfÂàÜËØçÂô®ÊÑèÂõæÂ§öÁ∫øÁ®ãÂàÜËØçÔºåÊä•jvmÈîôËØØ


<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

class Divider(threading.Thread):
    def __init__(self, threadID, data):
        threading.Thread.__init__(self)
        self.threadID=threadID
        self.data=data
        CRFLexicalAnalyzer = SafeJClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
        #CustomDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
        #HanLP = SafeJClass('com.hankcs.hanlp.HanLP')
        extend_dic = format_dic(external_dic_path)
        for word_e in extend_dic:
            CustomDictionary.add(word_e)  # Ê∑ªÂä†Â§ñÈÉ®ÂÜõ‰∫ãËØçÂÖ∏
        # ÊâìÂºÄËØçÊÄßÊ†áÊ≥®
        s = CRFLexicalAnalyzer().enableCustomDictionary(True).enablePartOfSpeechTagging(True)
        self.segment=s

    def run(self):
        divide_word(self.data,self.threadID,self.segment)


def divide_word(data,pid,segment,Debug=False):
    all_words=[]

    for index,article in enumerate(data):
        article_contont = article[""article_content""]
        article_contont = norm_article(article_contont)
        terms=segment.seg(article_contont)
        words=[term.word for term in terms]
        if Debug:
            print(words)
        all_words.append(words)
        if index%500==0:
            print(""Thread:"",pid,""divide...line:"",index)
    return all_words


    data=load_json(data_path)
    mid=int(len(data)*0.5)
    data1=data[:mid]
    data2=data[mid:]
    thread1 = Divider( ""Thread-1"", data1)
    thread2 = Divider( ""Thread-2"", data2)

    # ÂºÄÂêØÁ∫øÁ®ã
    thread1.start()
    thread2.start()
    thread1.join()
    thread2.join()

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7c4d07fbb0, pid=17521, tid=0x00007f7bc90ce700
#
# JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13)
# Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-36m-x86_64-linux-gnu.so+0x3cbb0]  JPJavaEnv::FindClass(char const*)+0x20
#
# Core dump written. Default location: /home/ygwang/workspace/hannlp/handler/core or core.17521
#
# An error report file with more information is saved as:
# /home/ygwang/workspace/hannlp/handler/hs_err_pid17521.log
[thread 140169611896576 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
Aborted (core dumped)



```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éDoubleArrayTrieÁöÑÈªòËÆ§ÂàùÂßãÂåñÂ§ßÂ∞è,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/26922926/46203542-b2d47900-c34c-11e8-9b0d-efccb5a6ce0e.png)
ËØ∑ÈóÆÂàùÂßãÂåñÊó∂ÔºåÂàùÊúüÂ§ßÂ∞èÂøÖÈ°ªÊòØ 65536*32Ëøô‰πàÂ§ßÂêóÔºåÂ¶ÇÊûúË¶ÅÂåπÈÖçÁöÑËØç‰∏çÊòØÂæàÂ§öÔºàÂá†‰∏™Âà∞Âá†Áôæ‰∏™‰∏çÁ≠âÔºâÔºå‰πüÂøÖÈ°ªËÆæÂÆöËøô‰πàÂ§ßÂêó„ÄÇÔºàÁ®ãÂ∫è‰∏≠ÈúÄË¶ÅÁî®Âà∞ÂæàÂ§öËøôÁßçÂ∞èÂ≠óÂÖ∏ÔºåËÄå‰∏îÂ≠óÂÖ∏‰ºöÈ¢ëÁπÅÁöÑÂèòÂåñÔºâ
ÊàëÊîπÊàê65536ÂêéÔºåÊúâÊó∂ÂÄôÁî®SearcherÂåπÈÖçÁöÑÊó∂ÂÄôÊä•ArrayIndexOutOfBoundsExceptionÁöÑÂºÇÂ∏∏„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
1.Áî®„ÄêÊΩòÂÆ∂Âõ≠Ë°óÈÅì„ÄëÊûÑÂª∫‰∏Ä‰∏™ËØçÂÖ∏
2.Áî®ÊûÑÂª∫ÁöÑËØçÂÖ∏ÂåπÈÖçÈôÑ‰ª∂ÁöÑËøôÁØáÊñáÁ´†
3.Áî®dart.getSearcher(text, 0)ÂåπÈÖçÔºå‰ºöÂú®Ëøô‰∏™Âú∞ÊñπÂºÇÂ∏∏
![image](https://user-images.githubusercontent.com/26922926/46203923-e8c62d00-c34d-11e8-8a2b-c4c3db18942d.png)

[doc.txt](https://github.com/hankcs/HanLP/files/2427850/doc.txt)

"
hanlpÊâìÂåÖÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

ÊâìÂåÖÂêéÊâæ‰∏çÂà∞hanlp.properties

### Ê≠•È™§

1. È¶ñÂÖà:  ‰∏ãËΩΩhanlpÊ∫êÁ†Å(masterÂàÜÊîØ),Âú®com.hankcs.hanlp.HanLPÁ±ª‰∏≠Âä†‰∫Ü‰∏Ä‰∏™mainÊñπÊ≥ïÁî®‰∫éÊµãËØïÔºö
```
    public static void main(String[] args) {
        System.out.println(HanLP.segment(""ÂïÜÂìÅÂíåÊúçÂä°""));
    }
```
2. ÁÑ∂Âêé:  ÊâìÂåÖÔºåÊâßË°åÊâìÂåÖÂëΩ‰ª§ `mvn clean package -Dmaven.test.skip=true`

3. Êé•ÁùÄ:  ‰øÆÊîπhanlp.properties‰∏≠ÁöÑrootË∑ØÂæÑ‰∏∫dataÁöÑÁà∂Êñá‰ª∂Â§πÂπ∂ÊîæÂà∞targetÁõÆÂΩï‰∏ãÔºåÂíåhanlp-1.6.8-sources.jar„ÄÅhanlp-1.6.8.jarÂú®Âêå‰∏ÄÁõÆÂΩï‰∏ãÔºåÂëΩ‰ª§ËøêË°åcom.hankcs.hanlp.HanLPÁ±ªÁöÑmainÊñπÊ≥ï:
```
java -cp hanlp-1.6.8.jar com.hankcs.hanlp.HanLP start
```

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÂïÜÂìÅ/n, Âíå/cc, ÊúçÂä°/vn]
```

ÊúüÊúõÂíåhanlp-1.6.8-release.zipÈáåÈù¢ÊâìÂåÖÂá∫Êù•ÁöÑÊïàÊûú‰∏ÄÊ†∑Ôºå‰∏§‰∏™jarÂåÖ‰∏Ä‰∏™ÈÖçÁΩÆÊñá‰ª∂„ÄÇ

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
sep 28, 2018 1:47:03 PM com.hankcs.hanlp.HanLP$Config <clinit>
SEVERE: Ê≤°ÊúâÊâæÂà∞hanlp.propertiesÔºåÂèØËÉΩ‰ºöÂØºËá¥Êâæ‰∏çÂà∞data
========Tips========
ËØ∑Â∞Ühanlp.propertiesÊîæÂú®‰∏ãÂàóÁõÆÂΩïÔºö
WebÈ°πÁõÆÂàôËØ∑ÊîæÂà∞‰∏ãÂàóÁõÆÂΩïÔºö
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
Âπ∂‰∏îÁºñËæëroot=PARENT/path/to/your/data
Áé∞Âú®HanLPÂ∞ÜÂ∞ùËØï‰ªé/Users/pan/github/HanLP-master/targetËØªÂèñdata‚Ä¶‚Ä¶
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: ËØªÂèñdata/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: Ê†∏ÂøÉËØçÂÖ∏data/dictionary/CoreNatureDictionary.txt‰∏çÂ≠òÂú®ÔºÅjava.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt (No such file or directory)
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at com.hankcs.hanlp.HanLP.main(HanLP.java:860)
Caused by: java.lang.IllegalArgumentException: Ê†∏ÂøÉËØçÂÖ∏data/dictionary/CoreNatureDictionary.txtÂä†ËΩΩÂ§±Ë¥•
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 7 more
```


## ÂÖ∂‰ªñ‰ø°ÊÅØ


"
„ÄêÂàÜËØçÈóÆÈ¢ò„ÄëÈí±Â∑±ÁªèÊâìÂà∞Âç°‰∏ä‰∫Ü=>Èí±Â∑±Áªè Êâì Âà∞ Âç° ‰∏ä ‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->"
ÂÖ≥‰∫éÊ†áÂáÜÂàÜËØçÁöÑËØ¶ÁªÜÂéüÁêÜ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÁé∞Âú®ÊÉ≥‰∫ÜËß£Ê†áÂáÜËØçÊÄßÊ†áÊ≥®ÁöÑÂéüÁêÜÔºåÊàëÁü•ÈÅìÔºåÊòØÁî®ÈöêÈ©¨Â∞îÁßëÂ§´ÂíåÊØîÁâπÁÆóÊ≥ïÂÆûÁé∞Ôºå‰ΩÜÊàëÊÉ≥Áü•ÈÅìÁöÑÊõ¥ÂÖ∑‰Ωì‰∫õ„ÄÇ
ËøòÊúâÔºåËΩ¨ÁßªÁü©ÈòµÊòØÊÄé‰πàÂæóÂà∞ÁöÑÔºüÊòØÂÖ±Áé∞ÂêóÔºü
ÊúüÊúõÂ∞ΩÂø´ËÉΩÁªôÁ≠îÂ§çÔºåÈùûÂ∏∏ÊÑüË∞¢‰∫Ü
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊòØÂê¶ËÉΩÊèê‰æõ1.6.8ÁöÑÂéüÂßãËØ≠ÊñôÂ∫ì,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

ÈùûÂ∏∏ÊÑüË∞¢‰ΩúËÄÖÊó†ÁßÅÁöÑË¥°ÁåÆ‰∫Ü1.6.8ÁâàÊú¨ÔºåÊèê‰æõ‰∫Ü1‰∫øËØ≠ÊñôÂ∫ìËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇÁõÆÂâçÈ°πÁõÆÁªÑÊ≠£Âú®‰ΩøÁî®1.6.8ÁâàÊú¨Êèê‰æõÁöÑÊ®°ÂûãÔºåÂØπ‰∫éÂ∏∏ËßÑÁöÑÂàÜËØçÂíåËØçÊÄßËØÜÂà´ÊïàÊûúÂü∫Êú¨ËææÂà∞È¢ÑÊúüÔºå‰ΩÜÊòØÂØπ‰∫éË°å‰∏öÂÜÖÁöÑÂàÜËØçÊïàÊûúÂπ∂‰∏çÁêÜÊÉ≥ÔºåËØ∑ÈóÆÊòØÂê¶ËÉΩÊèê‰æõ1.6.8ÁöÑËØ≠ÊñôÂ∫ìÔºåÊÉ≥Âü∫‰∫éËøô‰∏™ËØ≠ÊñôÂ∫ì‰∏äÂ¢ûÂä†Ë°å‰∏öÂÜÖÁöÑËØ≠ÊñôÔºå‰ΩøÂÖ∂ËÉΩÊª°Ë∂≥Ë°å‰∏öÂÜÖÁöÑÂàÜËØçÈúÄÊ±Ç„ÄÇ

"
ËØ∑ÈóÆ1.6.8‰∏≠Â§ßËßÑÊ®°ËØ≠ÊñôËÆ≠ÁªÉÁöÑÊÑüÁü•Êú∫Ê®°ÂûãÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´ner.bin‰ºöÊèê‰æõÂòõ?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö**1.6.8**
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö**1.6.8**


## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂèëÁé∞**data-for-1.6.8**Êï∞ÊçÆÂåÖ‰∏≠data/model/perceptron/largeÁõÆÂΩï‰∏ãÂè™ÊúâÂàÜËØçÊ®°Âûãcws.bin,Âπ∂Êú™Êèê‰æõner.bin,ËØ∑ÈóÆÊñπ‰æøÊèê‰æõÂòõ?


"
Â§öÁ∫øÁ®ã‰∏ãÂÜÖÂ≠òÊ∫¢Âá∫,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
pyhanlp 0.1.44
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
pyhanlp   0.1.41

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Â§öÁ∫øÁ®ã‰∏ãË∞ÉÁî®Âè•Ê≥ï‰æùÂ≠ò‰ºöÂá∫Áé∞ÂÜÖÂ≠òÊ∫¢Âá∫
get_relations_pos_head(sentence)‰∏≠ÁöÑ
word_array = HanLP.parseDependency(sentence).getWordArray()
Êä•ÈîôÔºöjpype._jexception.java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded

Âπ∂‰∏îÁ®ãÂ∫è‰∏ÄÂºÄÂßãËøêË°åÊó∂ÔºåÁâ©ÁêÜÂÜÖÂ≠òÂç†1.9G ÔºåËôöÊãüÂÜÖÂ≠òÂç†5G, ËøêË°å‰∏ÄÊÆµÊó∂Èó¥ÂêéËôöÊãüÂÜÖÂ≠òÂ∞±Âà∞9G




"
stopwordsËÉΩ‰∏çËÉΩÂÉèËá™ÂÆö‰πâÂ≠óÂÖ∏‰∏ÄÊ†∑Âú®ÈÖçÁΩÆÈáåËá™ÂÆö‰πâÊñ∞Êñá‰ª∂,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö
* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [üëç ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
pyhanlp
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
stopwords ËÉΩ‰∏çËÉΩËá™ÂÆö‰πâÊñá‰ª∂
print(HanLP.Config.CoreStopWordDictionaryPath)
/home/q/pyhanlp/data/dictionary/stopwords.txt; stop_dict.txt;
ËÉΩ‰∏çËÉΩËøôÊ†∑ÈÖçÁΩÆ
ËøôÊ†∑ÈÖçÁΩÆÔºå‰∏ãÈù¢ËøôÊÆµ‰ª£Á†Å‰ºöÊä•ÈîôÔºå‰πüÂ∞±ÊòØ‰∏çËÉΩËøôÊ†∑ÈÖçÁΩÆÂêóÔºü

--------------------------------

from pyhanlp import *
NotionalTokenizer = JClass(""com.hankcs.hanlp.tokenizer.NotionalTokenizer"")
text = ""Â∞èÂå∫Â±ÖÊ∞ëÊúâÁöÑÂèçÂØπÂñÇÂÖªÊµÅÊµ™Áå´ÔºåËÄåÊúâÁöÑÂ±ÖÊ∞ëÂç¥ËµûÊàêÂñÇÂÖªËøô‰∫õÂ∞èÂÆùË¥ù""
print(NotionalTokenizer.segment(text))

-----------------------------------

java.lang.NullPointerExceptionPyRaisable  Traceback (most recent call last)
<ipython-input-1-bfb7f4ece71a> in <module>()
      4 
      5 text = ""Â∞èÂå∫Â±ÖÊ∞ëÊúâÁöÑÂèçÂØπÂñÇÂÖªÊµÅÊµ™Áå´ÔºåËÄåÊúâÁöÑÂ±ÖÊ∞ëÂç¥ËµûÊàêÂñÇÂÖªËøô‰∫õÂ∞èÂÆùË¥ù""
      6 print(NotionalTokenizer.segment(text))

java.lang.NullPointerExceptionPyRaisable: java.lang.NullPointerException

----------------------------------------

## ÊàëÁöÑÈóÆÈ¢ò2
win10‰πüÂêåÊ†∑ÈÖçÁΩÆ‰∫Ühanlp.properties
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; Custom.txt; Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt ns; my.txt; feature_dict.txt; data/dictionary/person/nrf.txt nrf;
ÂêåÊ†∑Âà†Èô§‰∫ÜÁºìÂ≠òÊñá‰ª∂Ôºå‰∏çÁü•‰∏∫‰ªÄ‰πàÔºåÁîüÊàêÂá∫Êù•ÁöÑÂú∞ÂùÄÂßãÁªàÊ≤°Êúâfeature_dict.txtÔºå Custom.txt; 

-----------------------------------

print(HanLP.Config.CustomDictionaryPath)
('d:/applications/pyhanlp/data/dictionary/custom/CustomDictionary.txt', 'd:/applications/pyhanlp/data/dictionary/custom/Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt', 'd:/applications/pyhanlp/data/dictionary/custom/ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns', 'd:/applications/pyhanlp/data/dictionary/custom/‰∫∫ÂêçËØçÂÖ∏.txt', 'd:/applications/pyhanlp/data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt', 'd:/applications/pyhanlp/data/dictionary/custom/‰∏äÊµ∑Âú∞Âêç.txt ns', 'd:/applications/pyhanlp/data/dictionary/person/nrf.txt nrf')

-----------------------------------

‰∏äÈù¢Â∞±ÂéüÂßãÁöÑ‰∏É‰∏™ÔºåÊúâ‰ªÄ‰πàÊ£ÄÊü•ÊñπÊ≥ïÂêóÔºåjava‰πü‰∏çÊòØÁâπÂà´ÁÜü„ÄÇ"
n-gramÂÖ≥Á≥ªÁî±Êù•,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
GitHub‰ªìÂ∫ìÁâà masterÂàÜÊîØ

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö GitHub‰ªìÂ∫ìÁâà masterÂàÜÊîØ 
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö GitHub‰ªìÂ∫ìÁâà masterÂàÜÊîØ 

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÊÉ≥Âí®ËØ¢‰∏ãhanlpÁöÑ‰∫åÂÖÉÂÖ≥Á≥ªÊòØÊÄé‰πàÂæóÂà∞ÁöÑÔºü‰æãÂ¶Ç‚Äò‰∏Ä@ÂÖ¨Áõä 1‚ÄôÔºå‚Äò‰∏Ä@‰∏ÄÂØπ‰∏Ä 5‚Äô„ÄÇÊàëÁêÜËß£ÁöÑ‰∫åÂÖÉÂÖ≥Á≥ªÊòØÂ∑¶ÂÖÉ+Âè≥ÂÖÉËÉΩÂ§üÊòØ‰∏Ä‰∏™Êê≠ÈÖç‰πãÁ±ªÁöÑÔºå‰ΩÜÊòØËøôÊ†∑ÁöÑ‰∫åÂÖÉÂÖ≥Á≥ªÁúãËµ∑Êù•ÊúâÁÇπÊÄ™ÔºåÊòØ‰∏çÊòØÊàëÁêÜËß£Èîô‰∫ÜhanlpÁöÑ‰∫åÂÖÉÂÖ≥Á≥ªÔºüÂè¶Â§ñÂØπ‰∫én-gramÁöÑÊê≠ÈÖçÁöÑ‚ÄòÂá∫Âú∫È°∫Â∫è‚Äô‰πüÂ≠òÂú®ÁñëÈóÆÔºå‰∏∫‰ªÄ‰πàÊï¥‰Ωì‰∏ä‰∫åÂÖÉÂÖ≥Á≥ªÂá∫Áé∞ÁöÑÈ¢ëÁéáÈÉΩÊØîËæÉÂ∞èÔºåÊòØË∑üËØ≠ÊñôÊúâÂÖ≥Á≥ªÂêóÔºüÁÉ¶ËØ∑ËÉΩÂ§üÊåáÁÇπ„ÄÇÈùûÂ∏∏ÊÑüË∞¢ÔºÅ
"
Ëã±ÊñáÊã¨Âè∑Êó†Ê≥ïÊãÜÂàÜ,"ÁõÆÂâçÂ≠òÂú®ÈóÆÈ¢òÔºå‰º™‰ª£Á†ÅÂ¶Ç‰∏ãÔºö

   ```
       String url = ""(((?)))"";
        CharType.type['('] = CharType.CT_DELIMITER;
       CustomDictionary.add(""("");
       Ë∞ÉÁî® HanLP.segment(url);
   ```
ÁªìÊûúÂ¶Ç‰∏ãÔºö
(((?)))
ÁõÆÂâçÁâàÊú¨ÊòØ portable-1.6.8
ËØ∑ÈóÆÂ¶Ç‰ΩïÊääÂ≠óÁ¨¶ÊãÜÂºÄÔºü


   "
urllib.error.URLError: <urlopen error unknown url type: https>,"python 3.6
pyhanlp  0.1.44
Âú®ËøêË°åÂ¶Ç‰∏ã‰ª£Á†ÅÊó∂ÔºåÊä•Èîô

from pyhanlp import *
text = """"
print(HanLP.segment(text))

errorÔºöFile ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 116, in <module>
    _start_jvm_for_hanlp()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 38, in _start_jvm_for_hanlp
    from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 305, in <module>
    install_hanlp_jar()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 191, in install_hanlp_jar
    version = hanlp_latest_version()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 68, in hanlp_latest_version
    return hanlp_releases()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 76, in hanlp_releases
    content = urllib.urlopen(""https://api.github.com/repos/hankcs/HanLP/releases"").read()
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 549, in _open
    'unknown_open', req)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 1388, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: https>

"
Âü∫‰∫é‰ø°ÊÅØÂáùÂõ∫Â∫¶‰∏éÂ§ñÈÉ®‰ø°ÊÅØÁÜµÁöÑÊñ∞ËØçËØÜÂà´ÁöÑJavaÂÆûÁé∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

HiÔºåHankcs.ÂæàÊó©‰ª•ÂâçÂ∞±‰∏ÄÁõ¥Âú®ÂÖ≥Ê≥®ÂíåÂ≠¶‰π†‰Ω†ÁöÑBlog„ÄÇÊàëÁöÑÈóÆÈ¢òÂ¶Ç‰∏ãÔºö
ÊúÄËøëÂú®ÂÅö‰∏Ä‰∏™Êó†ÁõëÁù£ÊèêÂèñÈ¢ÜÂüüËØçÂÆûÈ™å„ÄÇÁúãÂà∞ÊÇ®ÁöÑ„ÄäÂü∫‰∫é‰ø°ÊÅØÁÜµÂíå‰∫í‰ø°ÊÅØÁöÑÊñ∞ËØçËØÜÂà´„Äã‰∏ÄÊñáÁªìÂêà‰∫ÜÂÜÖÈÉ®ÂáùÂõ∫Â∫¶‰∏éÂ§ñÈÉ®Â∑¶Âè≥ÁÜµÁöÑJavaÂÆûÁé∞ÔºåËØ∑ÈóÆËøôÈÉ®ÂàÜ‰ª£Á†ÅÊòØÂê¶Âú®Hanlp‰∏≠ÈõÜÊàêÔºü Âú®ËØÑËÆ∫‰∏ãÁúãÂà∞‰Ω†ÁöÑÂõûÂ§çÔºö‚ÄúÂì¶ÔºåHanLPÈáåÈù¢ÊòØÊèêÂèñÁü≠ËØ≠Áî®ÁöÑÔºåËøô‰∏™ÊòØÊèêÂèñËØçËØ≠Áî®ÁöÑÔºå‰∏ç‰∏ÄÊ†∑ÁöÑ‚ÄùÔºåËØ∑ÈóÆ‰∏Ä‰∏ãHanLP‰∏≠ÊâÄÂºÄÊ∫êÁöÑ‰ø°ÊÅØÁÜµÁü≠ËØ≠ËØÜÂà´‰∏é„ÄäÂü∫‰∫é‰ø°ÊÅØÁÜµÂíå‰∫í‰ø°ÊÅØÁöÑÊñ∞ËØçËØÜÂà´„ÄãÊñá‰∏≠ÊèêÂà∞ÁöÑÊúâ‰ΩïÂºÇÂêåÔºüÂ¶ÇÊúâ‰∏çÂêåÔºåÂèØÂê¶ÂèÇËÄÉ‰∏Ä‰∏ãÊÇ®ÁöÑËØ•ÊñáÁöÑJavaÂÆûÁé∞ÔºüÈùûÂ∏∏ÊÑüË∞¢ÔºÅ

## Â§çÁé∞ÈóÆÈ¢ò
ÊöÇÊó†

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   NULL
```
### ÊúüÊúõËæìÂá∫

NULL

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

[„ÄäÂü∫‰∫é‰ø°ÊÅØÁÜµÂíå‰∫í‰ø°ÊÅØÁöÑÊñ∞ËØçËØÜÂà´„Äã](http://www.hankcs.com/nlp/new-word-discovery.html)

"
hanlp ÂèØ‰ª•Ê∑ªÂä†Ëã±ÊñáÂçïËØçÁªÑÂêàÁöÑ‰Ωú‰∏∫Êñ∞ËØçÊîæÂà∞ËØçÂÖ∏ÈáåÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

Âú®CustomDictionary.txt‰∏≠Ê∑ªÂä†Â¶Ç‰∏ãËã±ÊñáÂçïËØçÁªÑÂêàÁöÑÊñ∞ËØçÔºåËã±ÊñáÈÉΩÊòØÁ©∫Ê†ºÈöîÂºÄÁöÑ‰∫ÜÊàëÊääÂàÜÂâ≤Á¨¶Êç¢Êàê\t‰πü‰∏çË°å
air motor n 1

Êä•ÈîôÔºö‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏D:/An/HanLP/data/dictionary/custom/CustomDictionary.txtËØªÂèñÈîôËØØÔºÅjava.lang.NumberFormatException: For input string: ""n""


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""air motor is very good""));
    }
```"
HanlpÂàÜËØçÈÅáÂà∞UnicodeË°®ÊÉÖÁ¨¶Âè∑Á®ãÂ∫èÂ∞±Â¥©Ê∫É,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âè™Ë¶ÅÂàÜÂè•‰∏≠Â∏¶ÊúâunicodeË°®ÊÉÖÁ¨¶Âè∑ÔºåÊñ≠ÁÇπÈÉΩÊ≤°Êúâ‰ΩúÁî®ÔºåPythonÂ∞±ÂÅúÊ≠¢Â∑•‰ΩúÔºåJava‰πüÊòØÂ¶ÇÊ≠§

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

```python
from pyhanlp import *

if __name__ == '__main__':
    try:
        result=HanLP.segment(""üôèüôèüôè"")
    except Exception:
        result=""""
    print(result)

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
Â∫îËØ•Ê†πÊçÆÊ≠£Â∏∏ÁöÑUnicodeË°®ÊÉÖÁ¨¶Âè∑ÂàÜËØç
"
elasticsearch-analysis-hanlp Êèí‰ª∂Âú®ÂàÜËØç(analyze)Êó∂‰∏çÊîØÊåÅÂΩí‰∏ÄÂåñ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP 1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöelasticsearch-analysis-hanlp 6.3.2(ElasticSearchÊèí‰ª∂ÁâàÊú¨)

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÈÄöËøá`./bin/elasticsearch-plugin install https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v6.3.2/elasticsearch-analysis-hanlp-6.3.2.zip`ÂÆâË£Ö‰∫Ü elasticsearch-analysis-hanlp 6.3.2 Êèí‰ª∂„ÄÇÂú®ElasticSearch-Kibana devTools‰∏äÂàÜÊûêÔºö
```html
GET index_ik_test/_analyze
{
  ""analyzer"": ""hanlp"",
  ""text"": [""‚ë£‚ëß‚ë¶‚ë£22‚ë°8QÁ¨≥Áúã""]
}
```
ÂæóÂà∞ÁöÑÂàÜËØçÁªìÊûúÂ¶Ç‰∏ãÔºö(Áî® ÊñúÊù† Êù•ÂàÜÈöîÂàÜËØçÁªìÊûú)„ÄÇ
>‚ë£‚ëß‚ë¶‚ë£/22/‚ë°/8/Q/Á¨≥/Áúã

Áî±‰∫éÊúâÁâπÊÆäÁöÑÊï∞Â≠óÂ≠óÁ¨¶ÔºåËøô‰∫õÊï∞Â≠óÂπ∂Ê≤°ÊúâÂàÜÂú®‰∏ÄËµ∑„ÄÇËÄåÊàëÁöÑÈúÄÊ±ÇÊòØÔºöÂú®ÂàÜËØç‰πãÂâçÔºåÂÖàÂ∞Ü‰∏Ä‰∫õÁâπÊÆäÊï∞Â≠óÂ≠óÁ¨¶ËøõË°åÂΩí‰∏ÄÂåñÔºåËøôÊ†∑ÂàÜËØç‰πãÂêéÊï∞Â≠óÂ∞±ËÉΩÂàÜÂú®‰∏ÄËµ∑‰∫ÜÔºåÂú®HanLP1.6.3‰∏≠ÊµãËØïÂΩí‰∏ÄÂåñÂ¶Ç‰∏ãÔºö
```java
    @Test
    public void testNorm() {
        HanLP.Config.Normalization = true;
        System.out.println(HanLP.segment(""‚ë£‚ëß‚ë¶‚ë£22‚ë°8QÁ¨≥Áúã""));
    }
```
ËæìÂá∫Ôºö
>[48742228/m, q/nx, Á¨≥/g, Áúã/v]

Á¨¶ÂêàÊàëÁöÑË¶ÅÊ±Ç„ÄÇ‰ΩÜÊòØElasticSearch‰∏≠Áî® elasticsearch-analysis-hanlpÊèí‰ª∂(hanlp„ÄÅhanlp_standard„ÄÅhanlp_indexÁ≠âÂàÜËØçÂô®)Êù•ÂàÜËØçÊó∂ÔºåÊ≤°ÊúâÂΩí‰∏ÄÂåñÊïàÊûú„ÄÇ

ÊàëÁöÑÈóÆÈ¢òÊòØÔºö elasticsearch-analysis-hanlpÊèí‰ª∂ ÊòØÂê¶ËÄÉËôëÂú®ÂàÜËØçÊó∂ÈÄöËøáÈÖçÁΩÆÊñπÂºèÊù•ÊîØÊåÅÂΩí‰∏ÄÂåñÔºü

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÊàëÁúã‰∫Ü‰∏ãÊèí‰ª∂Ê∫êÁ†Å‰∏≠Ôºö [com.hankcs.cfg.Configuration.java](https://github.com/KennFalcon/elasticsearch-analysis-hanlp/blob/master/src/main/java/com/hankcs/cfg/Configuration.java)ÈáåÈù¢ÂÆö‰πâ‰∫Ü‰∏Ä‰∫õ ÂÖ≥‰∫éÂàÜËØçÁöÑÈÖçÁΩÆ‰ø°ÊÅØÔºå‰ΩÜÊòØÊ≤°ÊúâÂÖ≥‰∫éÊòØÂê¶ÂºÄÂêØÂΩí‰∏ÄÂåñÁöÑÈÖçÁΩÆ `HanLP.Config.Normalization = true`„ÄÇ
"
Ëé∑ÂèñÂÖ≥ÈîÆËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
hancks,ÊÇ®Â•ΩÔºÅÊàëÊÉ≥Âí®ËØ¢‰∏Ä‰∏ãÔºåÂ∞±ÊòØËé∑ÂæóÂÖ≥ÈîÆËØçÁöÑÊó∂ÂÄôÔºåÁ¨¨‰∏ÄÊ≠•ÊòØË¶ÅËøõË°åÂàÜËØçÔºåÊ∫êÁ†Å‰∏≠ÁúãÂà∞ÂàÜËØçÊòØÁî®ÁöÑDefaultSegmentÔºåËøô‰∏™ÂàÜËØçÊòØËøêÁî®‰ªÄ‰πàËøõË°åÂàÜËØçÁöÑÔºüÂ¶ÇÊûúÊÉ≥Áî®ÂÖ∂‰ªñÂàÜËØçÂô®ÔºåÂèØ‰ª•ÂÆûÁé∞‰πàÔºüÊÑüË∞¢Â§ßÁ•ûÁåÆÂá∫Ëøô‰πàÂ•ΩÁöÑ‰∏Ä‰∏™ËµÑÊ∫êÔºåÂ∏åÊúõÂæóÂà∞Â§ßÁ•ûÁöÑÊåáÁÇπÔºåÂìàÂìàÔºÅ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
pyhanlpÂèØ‰ª•ÈÖçÁΩÆÁõ¥Êé•‰ΩøÁî®hanlp-portable-1.6.8.jarÔºåËÄå‰∏çÈúÄË¶Åhanlp.propertiesÊñá‰ª∂ÂêóÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

    ÊàëÂ∞ÜpyhanlpÂÆâË£ÖÂêéÔºåÊäähanlp-1.6.8.jarÁî®hanlp-portable-1.6.8.jarÊõøÊç¢
  
    ‰ΩÜÊòØ‰æùÁÑ∂ÈúÄË¶Åhanlp.propertiesÊñá‰ª∂ÔºåÂèØ‰ª•Áõ¥Êé•Âíåjava‰∏ÄÊ†∑‰ΩøÁî®Âø´ÈÄüÁâàÊú¨ÂêóÔºü
    
    ÁÑ∂ÂêéÁî®Êà∑ÈÄöËøáÂä†ÂÖ•ËØçÂÖ∏Áõ¥Êé•Êõ¥Êñ∞hanlp-portable-1.6.8.jarÈáåÈù¢ÁöÑtxt.binÊñá‰ª∂Âç≥ÂèØÔºü

    ÊÅïÊàëÂÜíÊòßÂìàÔºåÊèêÂá∫ËøôÊ†∑ÁöÑ‰∏Ä‰∏™ËØ∑Ê±ÇÔºåÈáåÈù¢pyhanlp‰ª£Á†ÅÂ•ΩÂ§çÊùÇÔºåÊ≤°ÊÄé‰πàÁúãÊáÇ

"
hanlp-portable-1.6.8.jarÂåÖ‰∏≠ÁöÑCustomDictionary.txt.binÂè™Êúâ3802KBÔºå‰ΩÜÊòØËá™Â∑±Ê†πÊçÆÈªòËÆ§ÈÖçÁΩÆ‰∫ßÁîüÊúâ16583KB,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò


ÊàëÊÉ≥Ê∑ªÂä†‰∏ÄÁÇπËá™Â∑±ÁöÑËØçÂà∞\data\dictionary\customÁõÆÂΩï‰∏ãÔºåÁÑ∂Âêé‰∫ßÁîübinÊñá‰ª∂

ÁÑ∂ÂêéÊîæÂà∞hanlp-portable-1.6.8.jar‰∏≠Ë¶ÜÁõñCustomDictionary.txt.bin

‰ΩÜÊòØ‰∏∫‰∫Ü‰øùËØÅjarÂåÖÂ§ßÂ∞è‰∏çË¶ÅÂ¢ûÂä†Â§™Â§ö

ÊâÄ‰ª•ÊÉ≥Âü∫‰∫éhanlp-portable-1.6.8.jarÈªòËÆ§ÈÄâÁöÑÂá†‰∏™ËØçÂÖ∏Êñá‰ª∂,‰ΩÜÊòØÊÄé‰πàÂ∞ùËØïÈÉΩÂæó‰∏çÂà∞3802KB

ÊâÄ‰ª•ËØ∑Êïôhanlp-portable-1.6.8.jar‰∫ßÁîüÁöÑCustomDictionary.txt.binÊòØÂü∫‰∫éÂì™Âá†‰∏™txtÊñá‰ª∂ÂêóÔºü

"
DoubleArrayTrieÁöÑLongestSearcherÊúâÊó∂ÂÄôÂåπÈÖçÈÅóÊºè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.6

## ÊàëÁöÑÈóÆÈ¢ò
Âú®‰ΩøÁî®DoubleArrayTrieÁ±ªÁöÑLongestSearcherÊñπÊ≥ïÊó∂ÔºåÊúâ‰∫õËØçÂåπÈÖç‰∏çÂà∞„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
### Ê≠•È™§
1. ÊûÑÂª∫‰∏Ä‰∏™DoubleArrayTrieÔºåÂåÖÂê´‰ª•‰∏ãÂÖ≥ÈîÆËØçÔºö
001‰π°ÈÅì, Âåó‰∫¨, Âåó‰∫¨Â∏ÇÈÄö‰ø°ÂÖ¨Âè∏, Êù•ÂπøËê•‰π°, ÈÄöÂ∑ûÂå∫
2. Áî®‰∏äÈù¢ÊûÑÂª∫ÁöÑDARTÔºå‰ΩøÁî®getLongestSearcher()ÔºåÂåπÈÖç‰∏ãÈù¢ÁöÑÂè•Â≠êÔºö
Âåó‰∫¨Â∏ÇÈÄöÂ∑ûÂå∫001‰π°ÈÅìÂèëÁîü‰∫Ü‰∏Ä‰ª∂ÊúâÊÑèÊÄùÁöÑ‰∫ãÊÉÖÔºåÊù•ÂπøËê•‰π°Ê≠åËàûÈòüÊ≠£Âú®Ë∑≥Ëàû

### Ëß¶Âèë‰ª£Á†Å
		DoubleArrayTrie<String>.LongestSearcher searcher = dart.getLongestSearcher(str, 0);
		while (searcher.next())
		{
                     ...
		}
### ÊúüÊúõËæìÂá∫
ÊúüÊúõËÉΩÊåâÈ°∫Â∫èÂåπÈÖçÂà∞‰∏ãÈù¢ÁöÑËØçÔºö
Âåó‰∫¨ ÈÄöÂ∑ûÂå∫ 001‰π°ÈÅì Êù•ÂπøËê•‰π°

### ÂÆûÈôÖËæìÂá∫
Âè™ÂåπÈÖçÂà∞‰∫Ü‰∏ãÈù¢ÁöÑËØçÔºàÈÄöÂ∑ûÂå∫Ê≤°ÊúâË¢´ÂåπÈÖçÂà∞Ôºâ
Âåó‰∫¨ 001‰π°ÈÅì Êù•ÂπøËê•‰π°

## ÂÖ∂‰ªñ‰ø°ÊÅØ
‰ΩøÁî®getSearcher()Ê≤°ÊúâÈóÆÈ¢ò
"
javaÊ∫êÁ†ÅÂá∫Áé∞ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.8

## ÊàëÁöÑÈóÆÈ¢ò

Âú®hanlp-1.6.8‰∏≠hmm2 ÁöÑÂàÜËØçÊ®°ÂûãÂ∑≤ÁªèË¢´Â∫üÈô§ÔºåÂú®data‰∏≠ÂÆûÈôÖ‰∏çÂ≠òÂú®Ôºå‰ΩÜÊòØÂú®hanlp.javaÁöÑ
ÁöÑÊ∫êÁ†Å‰∏≠Ôºå‰ªçÁÑ∂ÂèØ‰ª•ÈÄöËøá‰º†ÂÖ•hmm2Â≠óÁ¨¶‰∏≤Êù•Ëé∑Âæóhmm2ÁöÑÂàÜËØçÂô®„ÄÇ

Ê∫êÁ†Å->hanlp.java
```
 /**
         * HMMÂàÜËØçÊ®°Âûã
         *
         * @deprecated Â∑≤Â∫üÂºÉÔºåËØ∑‰ΩøÁî®{@link PerceptronLexicalAnalyzer}
         */
        public static String HMMSegmentModelPath = ""data/model/segment/HMMSegmentModel.bin"";


......
        else if (""hmm2"".equals(algorithm) || ""‰∫åÈò∂ÈöêÈ©¨"".equals(algorithm))
            return new HMMSegment();

```

Â¶ÇÊûúÁúüÁöÑ‰ΩøÁî®hmm2Â∞±‰ºöÂºïËµ∑‰ª•‰∏ãÈîôËØØ
ÈîôËØØ
```
java.lang.IllegalArgumentExceptionPyRaisable: java.lang.IllegalArgumentException: ÂèëÁîü‰∫ÜÂºÇÂ∏∏Ôºöjava.lang.IllegalArgumentException: HMMÂàÜËØçÊ®°Âûã[ /home/font/anaconda3/lib/python3.6/site-packages/pyhanlp/static/data/model/segment/HMMSegmentModel.bin ]‰∏çÂ≠òÂú®
```"
pyhanlpÂú®DebugÊñ≠ÊéâË∞ÉËØïÊ®°ÂûãÂ∞±‰ºöPythonÂÅúÊ≠¢Â∑•‰Ωú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®‰ΩøÁî®pyhanlpËøáÁ®ã‰∏≠ÔºåÂ¶ÇÊûúÊòØDebugÊ®°ÂºèÔºå‰ΩøÁî®IDEAÊñ≠ÁÇπË∞ÉËØïÔºåÂ∞±‰ºöÂá∫Áé∞PythonÂÅúÊ≠¢Â∑•‰Ωú
![1](https://user-images.githubusercontent.com/6327360/45194246-ced08780-b284-11e8-97c2-a519004e9fdd.png)


"
data‰∏≠Áº∫Â∞ësegmentÊñá‰ª∂Â§π?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
pyhanlp-1.6.8

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöpyhanlp-1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöpyhanlp-1.6.8



## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®Ëá™Â∏¶demo‰∏≠ÁöÑCRFÂàÜËØçËøõË°åÂàÜËØç,Âá∫Áé∞Ê®°ÂûãÂä†ËΩΩÈîôËØØ,ÂèëÁé∞data-model‰∏≠Áº∫Â∞ëÁõ∏Â∫îsegmentÊñá‰ª∂Â§π.‰∏ãËΩΩdataÂêéËß£ÂéãÂèëÁé∞,ËøòÊòØÊ≤°ÊúâËØ•Êñá‰ª∂Â§π.

## ‰ΩøÁî®‰ª£Á†Å
Âú®‰ΩøÁî®demo‰∏≠ÁöÑCRFÂàÜËØçÊó∂Âá∫Áé∞ÈóÆÈ¢ò,‰ª£Á†ÅÂ¶Ç‰∏ã
```
Config = JClass(""com.hankcs.hanlp.HanLP$Config"")
Config.ShowTermNature = False
CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
segment = CRFSegment().enableCustomDictionary(False)

for sentence in sentence_array:
    term_list = segment.seg(sentence)
    print(term_list)

```

## Âá∫Áé∞ÈîôËØØ
```
---------------------------------------------------------------------------
java.lang.IllegalArgumentExceptionPyRaisableTraceback (most recent call last)
<ipython-input-6-d745364c843d> in <module>()
     25 Config.ShowTermNature = False
     26 CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
---> 27 segment = CRFSegment().enableCustomDictionary(False)
     28 
     29 for sentence in sentence_array:

~/anaconda3/lib/python3.6/site-packages/jpype/_jclass.py in _javaInit(self, *args)
    109     else:
    110         self.__javaobject__ = self.__class__.__javaclass__.newClassInstance(
--> 111             *args)
    112 
    113 

java.lang.IllegalArgumentExceptionPyRaisable: java.lang.IllegalArgumentException: CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ /home/font/anaconda3/lib/python3.6/site-packages/pyhanlp/static/data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåËÄóÊó∂ 6 ms
```
"
I did not see the download address of 100 million corps,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

1‰∫øÂ≠óÁöÑ‰∏≠ÊñáËØ≠ÊñôÔºåÊ≤°ÊúâÁúãÂà∞‰∏ãËΩΩÂú∞ÂùÄÔºåÊÉ≥Ëá™Â∑±ËÆ≠ÁªÉÊ®°Âûã
"
[ÊèêÈóÆ]Ëá™Âª∫Êüê‰∏™Ë°å‰∏öÁöÑËØ≠ÊñôÂèäÈÅáÂà∞ÁöÑÂàÜËØç‰∏çÂáÜÈóÆÈ¢ò ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò


ÊàëÊÉ≥Âí®ËØ¢‰∏Ä‰∏ãÊ•º‰∏ªÔºåÂ¶ÇÊûúÊÉ≥Ëá™Âª∫Êüê‰∏™Ë°å‰∏öÁöÑËØ≠ÊñôÔºåÊàëÈúÄË¶ÅÂì™‰∫õÊìç‰ΩúÊ≠•È™§Ôºü
ÊàëÁé∞Âú®ÁöÑÊìç‰ΩúÊòØÔºö

1. ÂÖàÂØπË°å‰∏öÊï∞ÊçÆËøõË°åÊ†áÂáÜÂàÜËØçÔºàÊ†áÂáÜÂàÜËØçÂêéÔºå‰ºöËá™Âä®ÁîüÊàêËØçÊÄßÔºâ
2. ÂØπÂÖ≥ÈîÆËØçËøõË°åËá™ÂÆö‰πâËØçÊÄßÊ†áÊ≥®ÁöÑÊ£ÄÊü•Âíå‰øÆÊ≠£
3. ‰ΩøÁî®PerceptronLexicalAnalyzerÔºàÊÑüÁü•Êú∫ËØçÊ≥ïÂàÜÊûêÂô®ÔºâËøõË°åÂàÜËØçÂíåÂÆû‰ΩìËØÜÂà´
4. ËßÑÂàôÂÆ°Ê†∏

‰ΩÜÊòØÂú®ËÆ≠ÁªÉÂàÜÂ•ΩËØçÂíåÊ†áÊ≥®Â•ΩËØçÊÄßÁöÑËØ≠ÊñôÊï∞ÊçÆÊó∂ÔºàÊåâÁÖß199801ÁöÑËØ≠ÊñôÂàÜËØçÊ†áÂáÜÂáÜÂ§áÁöÑÊï∞ÊçÆÔºâÔºåÊúâ‰∫õÂàÜËØçÂíåËØçÊÄßËØÜÂà´Âπ∂‰∏çÊòØÂæàÂáÜËøòÊúâ‰∫õËØçÊÄß‰ºöË¢´Ë¶ÜÁõñ„ÄÇ
1. ÈóÆÈ¢ò1ÔºöËøô‰∏™Êó∂ÂÄôÊòØ‰∏çÊòØÈúÄË¶Å‰ΩøÁî®PerceptronLexicalAnalyzer.learnÊñπÊ≥ïÊù•ËøõË°åÂ≠¶‰π†‰øÆÊ≠£ÔºåËøòÊòØÊàë‰∏äËø∞ÁöÑÊìç‰ΩúÊú¨Ë∫´ÊòØÊúâÈóÆÈ¢òÁöÑÔºü
2. ÈóÆÈ¢ò2ÔºöÂú®ËøõË°åËØ≠ÊñôÊ†áÊ≥®Êó∂ÔºåÊ†áÂáÜËØçÊÄßÂíåËá™ÂÆö‰πâÂ§çÂêàËØçÊÄßÂØπ‰∫é‰∏ä‰∏ãÊñáÊòØÂê¶ÊúâË¶ÅÊ±ÇÔºüÂ¶ÇÊûúÊúâË¶ÅÊ±ÇÔºåÂ¶Ç‰ΩïÁïåÂÆö‰∏ä‰∏ãÊñáÁöÑËæπÁïåÔºü
3. ÈóÆÈ¢ò3ÔºöËØ≠ÊñôËÆ≠ÁªÉÊó∂ÂÄôÔºåÊòØÈúÄË¶ÅÂ∞ÜÊï¥ÁâáÊñáÁ´†ËøõË°åËÆ≠ÁªÉÔºåËøòÊòØÂè™ÂØπÈúÄË¶ÅÊèêÂèñÂÜÖÂÆπÁöÑÈÉ®ÂàÜÊÆµËêΩËøõË°åËÆ≠ÁªÉÔºü
4. ÈóÆÈ¢ò4ÔºöÂÆû‰ΩìËØÜÂà´Êó∂ÔºåÂ¶ÇÊûú‰∏Ä‰∏™ÂÆû‰ΩìÂ≠òÂú®‰∏çÂêåËØçÊÄßÊó∂ÔºåÊòØÂê¶ÈúÄË¶ÅÊ†πÊçÆ‰∏ä‰∏ãÊñáÂàÜËØçÊù•Âà§Êñ≠ÂΩìÂâçÂÆû‰ΩìÁöÑÂÖ∑‰ΩìËØçÊÄßÔºüÂ¶Ç‰ΩïËÆæÁΩÆ‰∏ä‰∏ãÊñáÁöÑÁ™óÂè£Â§ßÂ∞èÔºü


## Â§çÁé∞ÈóÆÈ¢ò


### Ê≠•È™§

1. ÊàëÂÖàËÆ≠ÁªÉËá™Â∑±ÁöÑËØ≠Êñô(‰∏Ä‰ªΩËøõË°åÊ†áÂáÜÂàÜËØçÁöÑÂêàÂêåÊ≠£Êñá)ÔºåÂú®ËøõË°åÊ†áÂáÜÂàÜËØçÂêéÔºåÊàëÂØπËØ≠ÊñôËøõË°å‰∫ÜËØçÊÄß‰øÆÊîπÔºàÊ†áÊ≥®‰∏∫Ëá™ÂÆö‰πâËØçÊÄßÔºâÔºåÁÑ∂ÂêéÂ∞ÜÊï¥ÁØáÊñáÁ´†ËøõË°åËÆ≠ÁªÉ
```
ÂÆ¢Êà∑‰ø°ÊÅØ/khxx Ôºö/w 
ÂÆ¢Êà∑Êñπ/khfbq Ôºö/w Ëá¥ÂØåÈì∂Ë°åËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/khfname
[Âèë/v Á•®/n ÂÜÖÂÆπ/n]/fpxx
ÂêçÁß∞/fpxxmcbq Ôºö/w Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ122Âè∑/fpxxmc
[ÂèëÁ•®/n Êä¨Â§¥/vi]/fptt Ôºö/w Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/fpttname
.......
```

2. ÁÑ∂ÂêéË∞ÉÁî®HanLPÁöÑAPIËÆ≠ÁªÉ‰∏äÈù¢ÁöÑËØ≠Êñô
```
               //1„ÄÅÂàÜËØç
		CWSTrainer();
		//2„ÄÅËØçÊÄßÊ†áÊ≥®
		POSTrainer();
		//3„ÄÅÂÆû‰ΩìËØÜÂà´
		NERTrainer();
```
3. ËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÁîüÊàêËá™ÂÆö‰πâÁöÑcws„ÄÅpos„ÄÅner.binÂíåbin.txtÊñá‰ª∂ÔºåÁÑ∂Âêé‰ΩøÁî®ÊÑüÁü•Êú∫ÂàÜËØçËøõË°åÁîü‰∫ßÊï∞ÊçÆÁöÑÂàÜËØç
```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(Ëá™ÂÆö‰πâ_CWS_MODEL_FILE, 
   Ëá™ÂÆö‰πâ_POS_MODEL_FILE,Ëá™ÂÆö‰πâ_NER_MODEL_FILE);
   String testInfoTxt = FileUtils.readTxt(""D:\\testInfo.txt"");
   List<List<Term>> list = analyzer.seg2sentence(testInfoTxt );
   
```

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
--->[ÂÆ¢Êà∑‰ø°ÊÅØ/khxx, Ôºö/w]
--->[ÂÆ¢Êà∑Êñπ/khfbq, Ôºö/w, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/khfname]
--->[ÂèëÁ•®ÂÜÖÂÆπ/fpxx]
--->[ÂêçÁß∞/fpxxmcbq, Ôºö/w, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ122Âè∑/fpxxmc]
--->[ÂèëÁ•®Êä¨Â§¥Ôºö/fptt, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/fpttname]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
--->[ÂÆ¢Êà∑‰ø°ÊÅØ/khxx, Ôºö/w]
--->[ÂÆ¢Êà∑/n]
--->[Êñπ/q, Ôºö/w, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/khfname]
--->[ÂèëÁ•®ÂÜÖÂÆπ/fpxx]
--->[ÂêçÁß∞/n, Ôºö/w, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ122Âè∑/khfname]
--->[ÂèëÁ•®Êä¨Â§¥Ôºö/fptt, Ëá¥ÂØåÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ËÆ∞Ë¥¶‰∏≠ÂøÉ/khfname]
```


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊÑüÁü•Êú∫ÂàÜËØçÂô®‰∏çËÉΩÊ≠£Á°ÆÂ§ÑÁêÜÊñáÊú¨‰∏≠ÁöÑÁ©∫Ê†º,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÑüÁü•Êú∫ÂàÜËØçÂô®‰∏çËÉΩÊ≠£Á°ÆÂ§ÑÁêÜÊñáÊú¨‰∏≠ÁöÑÁ©∫Ê†º

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÂæÖÂàÜËØçÁöÑÊñáÊú¨Ôºö
```Âåó‰∫¨‰∫åÊâãÊàøÊàê‰∫§Ê∂®‰∫îÊàê ‰∏öÂÜÖÔºöË¥∑Ê¨æÂà©Áéá‰ªçÂ∞Ü‰∏äÂçá```

ÊÑüÁü•Êú∫ÂàÜËØçÂô®ÔºàPerceptronLexicalAnalyzerÔºâÁöÑÁªìÊûúÔºö
```Âåó‰∫¨/ns ‰∫åÊâãÊàø/n Êàê‰∫§/v Ê∂®‰∫îÊàê ‰∏öÂÜÖ/s Ôºö/vn Ë¥∑Ê¨æ/vn Âà©Áéá/n ‰ªç/d Â∞Ü/d ‰∏äÂçá/v```
ËøôÈáå‚ÄúÊ∂®‰∫îÊàê ‰∏öÂÜÖ‚ÄùË¢´ÂàÜÊàê‰∫Ü‰∏Ä‰∏™ËØç„ÄÇ

ÂØπÊØîÈªòËÆ§ÂàÜËØçÂô®ÔºàViterbiSegmentÔºâÁöÑÁªìÊûúÔºö
```Âåó‰∫¨/ns ‰∫åÊâãÊàø/n Êàê‰∫§/v Ê∂®/vi ‰∫î/m Êàê/v  /w ‰∏öÂÜÖ/n Ôºö/w Ë¥∑Ê¨æ/n Âà©Áéá/n ‰ªç/d Â∞Ü/d ‰∏äÂçá/vi```
ËøôÈáå‚ÄúÊ∂®‰∫îÊàê‚ÄùÂêéÁöÑÁ©∫Ê†ºË¢´Ê≠£Á°ÆÂàáÂàÜÂá∫Êù•„ÄÇ

## ÂÖ∂‰ªñ‰ø°ÊÅØ
hanlp.propertiesÁ≠âÈÉΩÊòØÈªòËÆ§ÈÖçÁΩÆÔºå1.6.8Âä†ËΩΩÁöÑÊÑüÁü•Êú∫ÂàÜËØçÊ®°ÂûãÂ∫îËØ•ÊòØlarge/cws.bin„ÄÇ

"
Ëá™ÂÆö‰πâËØçÂÖ∏Êõ¥Êñ∞Êó∂Ëá™Âä®Âà†Èô§ÁºìÂ≠òÊñá‰ª∂,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
ÁªèÂ∏∏Âú® Issues ‰∏≠ÁúãÂà∞Âõ†‰∏∫Ê≤°ÊúâÂà†Èô§ÁºìÂ≠òÈÄ†ÊàêÁöÑËá™ÂÆö‰πâËØçÂÖ∏‰∏çÁîüÊïàÁöÑÈóÆÈ¢ò„ÄÇ

Êú¨Êõ¥Êñ∞‰ΩøÂæóÁî®Êà∑Âú®‰ΩøÁî®Êú¨Âú∞Êñá‰ª∂Á≥ªÁªüÁöÑÊÉÖÂÜµ‰∏ãÔºà`com.hankcs.hanlp.corpus.io.FileIOAdapter`ÔºâÔºå‰øÆÊîπËá™ÂÆö‰πâËØçÂÖ∏ÂêéÔºåÁºìÂ≠òÊñá‰ª∂‰ºöËá™Âä®Âà†Èô§Ôºå‰ªéËÄåËææÂà∞Ëá™ÂÆö‰πâËØçÂÖ∏ÂèäÊó∂ÁîüÊïàÁöÑÁõÆÁöÑ„ÄÇ

"
Ëá™ÂÆö‰πâËØçÂÖ∏Ôºå‰ΩøÁî®Â§ö‰∏™ÔºåÂè™ÊúâÁ¨¨‰∏Ä‰∏™Ëµ∑‰ΩúÁî®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊåâÁÖßÂ∏ñÂ≠êCustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; ÊàëÁöÑËØçÂÖ∏.txt;
ÊñπÊ≥ïÊ∑ªÂä†Ëá™ÂÆö‰πâÂ≠óÂÖ∏Ôºå‰ΩÜÊòØÂè™Êúâ‰∏Ä‰∏™Ëµ∑‰ΩúÁî®ÔºåÊàëÁöÑËØçÂÖ∏.txtÂíåCustomDictionary.txtÂêåÁõÆÂΩïÔºåÂç≥‰ΩøÊàëÁöÑËØçÂÖ∏Áî®ÂÖ®Ë∑ØÂæÑdata/dictionary/custom/ÊàëÁöÑËØçÂÖ∏.txt;‰πü‰∏çÂèØÁî®„ÄÇ
ÊµãËØïÂàÜËØçÁöÑÊó∂ÂÄôÁ¨¨‰∫å‰∏™Â≠óÂÖ∏ÈáåÁöÑËØçÊÄªÊòØÂàÜ‰∏ç Âá∫Êù•Ôºå‰ΩÜÊòØÂàáÊç¢Á¨¨1,2‰∏™Â≠óÂÖ∏‰ΩçÁΩÆÔºåÂ∞±ËÉΩÂàÜËØçÂá∫Êù•Ôºå‰ΩÜÊòØÂéüÊù•ÁöÑÂ≠óÂÖ∏ÈáåÁöÑËØçÂ∞±ÂàÜ‰∏çÂá∫Êù•‰∫Ü„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊÑüÁü•Êú∫Â≠¶‰π†ËØ≠ÊñôÊä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰Ω†Â•ΩÔºåÊàëÂú®‰ΩøÁî®ÊúÄÊñ∞ÁâàÁöÑÊÑüÁü•Êú∫ÂÅöÂú®Á∫øÂ≠¶‰π†ÂàÜËØçÁöÑÊó∂ÂÄôÔºåÂèëÁé∞ËØ≠Âè•`ÊùéÁê≥/nr ÈÉ®Èïø/nnt ÊπòÈõÖÈôÑ‰∫åÈô¢/nth`Âú®Á∫øÂ≠¶‰π†ÁöÑÊó∂ÂÄô‰ºöÊä•Èîô„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÊúÄÂºÄÂßãÊàëÊòØÂú®‰øÆÊîπËøáÁöÑ`large/cws.bin`ÂèëÁé∞ÁöÑÈóÆÈ¢òÔºåËµ∑ÂàùÊÄÄÁñëÊòØÊàëÁöÑÊñ∞Ê®°ÂûãÊúâÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÈ™åËØÅÔºåÊàëÈáçÊñ∞‰∏ã‰∫Ü`large/cws.bin`ÔºåÂÜç‰∏ÄÊ¨°Ë∑ë‰∫Ü`Ëß¶Âèë‰ª£Á†Å`ÔºåÂèëÁé∞ËøòÊòØÊúâÂêåÊ†∑ÁöÑÈóÆÈ¢ò„ÄÇÂêåÊó∂ÔºåÊàëÂÅö‰∫ÜÂØπÊØîÔºå`1.6.6`ÁâàÊú¨‰πüÊúâËøô‰∏™ÈóÆÈ¢ò„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
 HanLP.Config.enableDebug();
 PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
 System.out.println(analyzer.learn(""Ê≠¶Ê±â/ns Ê∞¥ÁîµÂ∑•/n Âê¥Â∏àÂÇÖ/nr""));
 System.out.println(analyzer.learn(""ÊùéÁê≥/nr ÈÉ®Èïø/nnt ÊπòÈõÖÈôÑ‰∫åÈô¢/nth""));
 System.out.println(analyzer.analyze(""ÊùéÁê≥ÈÉ®ÈïøÊπòÈõÖÈôÑ‰∫åÈô¢""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊùéÁê≥/nr ÈÉ®Èïø/nnt ÊπòÈõÖÈôÑ‰∫åÈô¢/nth
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Âä†ËΩΩ /Users/fiveddd/hanlp_data/data/model/perceptron/large/cws.bin ... ËÄóÊó∂ 1228 ms Âä†ËΩΩÂÆåÊØï
Âä†ËΩΩ /Users/fiveddd/hanlp_data/data/model/perceptron/pku199801/pos.bin ... ËÄóÊó∂ 403 ms Âä†ËΩΩÂÆåÊØï
Âä†ËΩΩ /Users/fiveddd/hanlp_data/data/model/perceptron/pku199801/ner.bin ... ËÄóÊó∂ 7 ms Âä†ËΩΩÂÆåÊØï
ÂÖ´Êúà 31, 2018 12:03:22 ‰∏ãÂçà com.hankcs.hanlp.dictionary.other.CharTable <clinit>
‰ø°ÊÅØ: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®ÂºÄÂßãÂä†ËΩΩ/Users/fiveddd/hanlp_data/data/dictionary/other/CharTable.txt
ÂÖ´Êúà 31, 2018 12:03:22 ‰∏ãÂçà com.hankcs.hanlp.dictionary.other.CharTable <clinit>
‰ø°ÊÅØ: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®Âä†ËΩΩÊàêÂäüÔºö115 ms
true
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 13983494
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.score(LinearModel.java:371)
	at com.hankcs.hanlp.model.perceptron.model.LinearModel.viterbiDecode(LinearModel.java:296)
	at com.hankcs.hanlp.model.perceptron.model.StructuredPerceptron.update(StructuredPerceptron.java:68)
	at com.hankcs.hanlp.model.perceptron.PerceptronTagger.learn(PerceptronTagger.java:58)
	at com.hankcs.hanlp.model.perceptron.PerceptronTagger.learn(PerceptronTagger.java:70)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.learn(PerceptronLexicalAnalyzer.java:163)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.learn(PerceptronLexicalAnalyzer.java:150)
	at trainHanlpModel.main(trainHanlpModel.java:38)

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
List<Term> ÊÄé‰πàËΩ¨Êç¢ÊàêList<String>,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8  master text ÈáåÈù¢ÁöÑcom

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÊÉ≥ÂÅö‰∏§‰∏™Áü≠ÊñáÊú¨ÁöÑÁõ∏‰ººÊÄßÂà§Êñ≠Ôºå‰ªÖ‰ªÖÊ†πÊçÆÂÖ≥ÈîÆËØçÊàñËÄÖÂàÜËØçÁöÑÁõ∏‰ººÊÄßÔºåÁõ∏‰ººÁöÑÂçïËØçË∂äÂ§öÂàô‰∏§ÊñáÊú¨Áõ∏‰ººÊÄßÊõ¥Â•Ω„ÄÇ
Âõ†‰∏∫HanLPÂæàÂ§öÂàÜËØçÂáΩÊï∞ËøîÂõûÊòØList<Term>ÔºåËÄåÈÄöËøáDemoWordDistanceÊñá‰ª∂Âà§Êñ≠‰∏§‰∏™ÂçïËØçÁõ∏‰ººÊÄßÊòØStringÁ±ªÂûãÔºåÊâÄ‰ª•ÊàëÈúÄË¶ÅÊääTermËΩ¨ÂèòÊàêString.

Âõ†‰∏∫‰ª£Á†ÅÊä•ÈîôÔºåÊó†Ê≥ïËæìÂá∫ÁªìÊûú„ÄÇÂêéÈù¢ËØ¶ÁªÜÈóÆÈ¢òÂπ∂Êú™Â°´ÂÜô„ÄÇ

Update Comment:ÊÑüË∞¢yaleimeng,ÈóÆÈ¢òÂæóÂà∞Ëß£ÂÜ≥ÔºåÂõ†‰∏∫ÊàëÊµãËØïÁî®ÁöÑÊòØgithubÁöÑtestÈáåÈù¢‰∏ãËΩΩÁöÑcomÂ§ßÊñá‰ª∂Â§πÔºåËÄåtestÈáåÈù¢ÁöÑcom/hankcs/hanlp/seg/common/‰∏ãÈù¢Ê≤°ÊúâTerm.javaÔºå‰ΩÜÊòØimport Ëøô‰∏™‰ΩçÁΩÆÁöÑTermÊ≤°ÈóÆÈ¢òÔºåÂ•ΩÂ•áÊÄ™Âïä
/
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
  
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂØπ‚ÄúÈæôÂ≠êÊπñÈ´òÊ†°Âõ≠Âå∫15Âè∑Ê≤≥ÂçóÂÜú‰∏öÂ§ßÂ≠¶‚ÄùÂàÜËØçÊó∂Â¥©Ê∫É

## Â§çÁé∞ÈóÆÈ¢ò
ÂØπ‚ÄúÈæôÂ≠êÊπñÈ´òÊ†°Âõ≠Âå∫15Âè∑Ê≤≥ÂçóÂÜú‰∏öÂ§ßÂ≠¶‚ÄùÂàÜËØçÊó∂Â¥©Ê∫É

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args){
        Segment seg = new DijkstraSegment();
        seg.seg(""ÈæôÂ≠êÊπñÈ´òÊ†°Âõ≠Âå∫Ê≤≥ÂçóÂÜú‰∏öÂ§ßÂ≠¶"");
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰∏çÂ¥©Ê∫É
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Â¥©Ê∫É‰∫Ü
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Merge pull request #1 from hankcs/master,"merge

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
solr‰∏≠‰ΩøÁî®hanlpËá™ÂÆö‰πâËØçÂÖ∏Áõ∏ÂÖ≥ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

solr‰∏≠‰ΩøÁî®Ëá™ÂÆö‰πâÂêå‰πâËØçËØçÂ∫ìÔºåÂ§ö‰∏™Âêå‰πâËØçÊó∂ËøîÂõû‰∏çÂÆåÊï¥

## Â§çÁé∞ÈóÆÈ¢ò

ÊàëÁöÑÂêå‰πâËØçËØçÂÖ∏synonyms_department.txt, ÊØè‰∏ÄË°åÁöÑÂÜÖÂÆπÂ¶Ç‰∏ãÔºö

`ËÇæÂÜÖÁßë,ËÇæËÑèÁßë,ËÇæÁóÖÂÜÖÁßë,ËÇæÁóÖÁßë,ËÇæËÑèÂÜÖÁßë,ËÇæÂÜÖ`

ÂÆö‰πâ‰∏Ä‰∏™hanlpÂàÜËØçÁöÑtype:
```
     <fieldType name=""hanlp_syn_department"" class=""solr.TextField"" positionIncrementGap=""100"">
          <analyzer type=""index"">
               <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""true"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
               <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms_department.txt"" ignoreCase=""true"" expand=""false"" />
          </analyzer>
          <analyzer type=""query"">
              <!-- ÂàáËÆ∞‰∏çË¶ÅÂú®query‰∏≠ÂºÄÂêØindexÊ®°Âºè -->     
              <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""false"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
          </analyzer>              
     </fieldType>
```
‰ΩøÁî®hanlp_syn_departmentÁ±ªÂûãÁöÑÂ≠óÊÆµÂÅöanalysisÔºö
![screen shot 2018-08-29 at 1 05 08 pm](https://user-images.githubusercontent.com/38783332/44766503-479c5900-ab8c-11e8-8d6c-bfc22b62e21a.png)


Áõ∏ÂêåÁöÑÂêå‰πâËØçËØçÂÖ∏ÔºåÂú®ik‰∏≠ÁöÑÊïàÊûúÔºö
![screen shot 2018-08-29 at 12 25 31 pm](https://user-images.githubusercontent.com/38783332/44765325-c4c4cf80-ab86-11e8-853a-718b20f8bab1.png)


ÂÖ∂‰ªñissuesÂÖ≥‰∫éÂêå‰πâËØçÁöÑÈÖçÁΩÆ‰πüÁøª‰∫ÜËøòÊòØÊ≤°ÊúâÊâæÂà∞ÂéüÂõ†ÔºåËØ∑ÊåáÊïôÈóÆÈ¢òÂá∫Âú®Âì™Èáå. "
ËØ∑Êïô‰∏ãNLPTokenizerÂàáÂàÜÁü≠ÁΩëÂùÄÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.8

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÈõÜÂç°‰Ω†ÊúÄÊ£íÔºÅÂÆ∂‰∫∫ÂíåÊúãÂèãË∂äÂ§öÔºåÈõÜÈΩêÁìúÂàÜÂ•ñÈáëË∂äÂ§öÔºåÊú¨Âë®ÈõÜÂç°ÁÅ´ÁÉ≠ËøõË°å‰∏≠Ôºå‰∏ÄËµ∑Êù• t.cn/RdHCuCh ÈÄÄËÆ¢ÂõûT
Áü≠ÁΩëÂùÄÂâçÂêéÊúâÁ©∫Ê†ºÔºåÂàáÂàÜÁöÑËØçÊòØÔºö‰∏ÄËµ∑Êù• t.cn/rdhcuch ÈÄÄËÆ¢/d,„ÄÇ
Ëøô‰∏™ÂàáÂàÜÊòØ‰∏çÊòØÊúâÈóÆÈ¢òÔºüÊàëÊääÁ©∫Ê†ºÂéªÊéâÂêéÂàáÂàÜÊòØËøôÊ†∑ÁöÑ
‰∏ÄËµ∑/d, Êù•t/v, ./v, cn/rdhcuch/nx, ÈÄÄËÆ¢/v
Êù•tÂíå.Âè∑ÂàáÂàÜÊàêÂä®ËØçÔºåÊÄé‰πà‰ºöËøôÊ†∑Ê†∑ÂàáÂàÜÂë¢Ôºü

ÊúâÊ≤°ÊúâË°•ÊïëÁöÑÂäûÊ≥ï

## Â§çÁé∞ÈóÆÈ¢ò
System.out.println(NLPTokenizer
            .segment(""ÈõÜÂç°‰Ω†ÊúÄÊ£íÔºÅÂÆ∂‰∫∫ÂíåÊúãÂèãË∂äÂ§öÔºåÈõÜÈΩêÁìúÂàÜÂ•ñÈáëË∂äÂ§öÔºåÊú¨Âë®ÈõÜÂç°ÁÅ´ÁÉ≠ËøõË°å‰∏≠Ôºå‰∏ÄËµ∑Êù• t.cn/RdHCuCh ÈÄÄËÆ¢ÂõûT""));

System.out.println(NLPTokenizer
            .segment(""ÈõÜÂç°‰Ω†ÊúÄÊ£íÔºÅÂÆ∂‰∫∫ÂíåÊúãÂèãË∂äÂ§öÔºåÈõÜÈΩêÁìúÂàÜÂ•ñÈáëË∂äÂ§öÔºåÊú¨Âë®ÈõÜÂç°ÁÅ´ÁÉ≠ËøõË°å‰∏≠Ôºå‰∏ÄËµ∑Êù• t.cn/RdHCuChÈÄÄËÆ¢ÂõûT""));

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑Êïô‰∏Ä‰∏ãÂÖ≥‰∫éÂàÜËØçÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

1.6.8

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÊÉ≥ÂèñÂá∫Â¶Ç:""Âåó‰∫¨Â∏Ç‰∫∫Ê∞ëÂåªÈô¢"" ËøôÊ†∑‰∏Ä‰∏™ËØç,‰ΩÜÊòØÊÄªÊòØ‰ºöÂàÜÊàê ""Âåó‰∫¨Â∏Ç"" + ""‰∫∫Ê∞ëÂåªÈô¢"" ‰∏§‰∏™ËØç,ËØ∑ÈóÆÊúâ‰ªÄ‰πàÂ•ΩÁöÑËß£ÂÜ≥ÊñπÂºèÂêó? (Âú∞Âêç‰πüÈúÄË¶ÅÂèñÂá∫Êù•)

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊéíÈô§‰∫∫Âêç<Âë®Êúâ>,"‰∏ãÂë®ÊúâÈõ® ‰∏ãÂë®ÊúâÈõ™ Á≠â‰∏çÊéíÈô§‰ºöËØØËØÜÂà´Âá∫‰∫∫Âêç Âë®ÊúâÈõ® Âë®ÊúâÈõ™

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
‰ΩøÁî®Á¥¢ÂºïÂàÜËØçÔºåÈÅáÂà∞‰∫ÜÂ•áÊÄ™ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂàÜËØçÂÜÖÂÆπ‚ÄúÂÖ¨ÂãüÂü∫Èáë20Âπ¥Áª©‰ºòÂü∫ÈáëÂíåÁª©‰ºòÂü∫ÈáëÁªèÁêÜÊï∞ÊçÆ‚ÄùÔºåÂíåÈ¢ÑÊúüÁöÑÊúâÂ∑ÆÂºÇ„ÄÇ
1. ‰∏∫‰ªÄ‰πà‚Äú20Âπ¥Áª©‰ºò‚ÄùË¢´ÂàÜËØç‰∏∫‚Äú20‚ÄùÂíå‚ÄúÂπ¥Áª©‰ºò‚Äù
2. Áî®Âú®ÊêúÁ¥¢ÂºïÊìé‰∏≠ÔºåËØçËØ≠ÂàÜËØçÁöÑÊó∂ÂÄô‰∏çÁî®ËøõË°åÁªÑÂêàÂêóÔºüÊØîÂ¶Ç‚ÄúÂü∫ÈáëÁªèÁêÜ‚ÄùÔºåÂàÜËØçÂá∫Êù•ÁöÑ‰ºöÊúâ‰ª•‰∏ãÁªìÊûú‚ÄúÂü∫Èáë‚ÄùÔºå‚ÄúÁªèÁêÜ‚ÄùÔºå‚ÄúÂü∫ÈáëÁªèÁêÜ‚ÄùÔºå‚ÄúÂü∫ÈáëÁªèÁêÜ‚Äù‰ºöÂçïÁã¨‰Ωú‰∏∫‰∏Ä‰∏™ÂàÜËØçÁªìÊûú„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
ÂÖàÂêé‰ΩøÁî®Á®ãÂ∫èËá™Â∏¶ÁöÑËØçÂÖ∏Êñá‰ª∂Âíå‰∏ãËΩΩÁöÑËØçÂÖ∏ÂåÖ data.zipÔºåËæìÂá∫ÁöÑÂÜÖÂÆπÁõ∏Âêå

### Ëß¶Âèë‰ª£Á†Å

```
    public class DemoIndexSegment
{
    public static void main(String[] args)
    {
        List<Term> termList = IndexTokenizer.segment(""ÂÖ¨ÂãüÂü∫Èáë20Âπ¥Áª©‰ºòÂü∫ÈáëÂíåÁª©‰ºòÂü∫ÈáëÁªèÁêÜÊï∞ÊçÆ"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

        System.out.println(""\nÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö"");
        IndexTokenizer.SEGMENT.enableIndexMode(1);
        termList = IndexTokenizer.segment(""ÂÖ¨ÂãüÂü∫Èáë20Âπ¥Áª©‰ºòÂü∫ÈáëÂíåÁª©‰ºòÂü∫ÈáëÁªèÁêÜÊï∞ÊçÆ"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
    }
}
```
### ÊúüÊúõËæìÂá∫

```
ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
Âπ¥Áª©‰ºò/nr [6:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]

ÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö
ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
20Âπ¥...
Âπ¥...
Áª©‰ºò/nr [6:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]
```

### ÂÆûÈôÖËæìÂá∫


```
ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
Âπ¥Áª©‰ºò/nr [6:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]

ÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö
ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
Âπ¥Áª©‰ºò/nr [6:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

DEBUG Ê®°ÂºèËæìÂá∫
```
ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
Âπ¥Áª©‰ºò/nr [6:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]

ÊúÄÁªÜÈ¢óÁ≤íÂ∫¶ÂàáÂàÜÔºö
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[ÂÖ¨, ÂÖ¨Âãü]
2:[Âãü]
3:[Âü∫, Âü∫Èáë]
4:[Èáë]
5:[20]
6:[]
7:[Âπ¥]
8:[Áª©]
9:[‰ºò]
10:[Âü∫, Âü∫Èáë]
11:[Èáë]
12:[Âíå]
13:[Áª©]
14:[‰ºò]
15:[Âü∫, Âü∫Èáë]
16:[Èáë]
17:[Áªè, ÁªèÁêÜ]
18:[ÁêÜ]
19:[Êï∞, Êï∞ÊçÆ]
20:[ÊçÆ]
21:[ ]

Á≤óÂàÜÁªìÊûú[ÂÖ¨Âãü/nz, Âü∫Èáë/n, 20/m, Âπ¥/qt, Áª©/ng, ‰ºò/ag, Âü∫Èáë/n, Âíå/cc, Áª©/ng, ‰ºò/ag, Âü∫Èáë/n, ÁªèÁêÜ/nnt, Êï∞ÊçÆ/n]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][ÂÖ¨Âãü A 20833310 ][Âü∫Èáë L 96 K 1 U 1 ][20 L 32 K 1 ][Âπ¥ D 219 B 61 C 42 L 20 K 12 E 8 ][Áª© D 8 E 6 C 1 ][‰ºò E 110 C 8 D 6 K 1 ][Âü∫Èáë L 96 K 1 U 1 ][Âíå M 15401 L 2868 K 2281 D 538 E 34 C 1 ][Áª© D 8 E 6 C 1 ][‰ºò E 110 C 8 D 6 K 1 ][Âü∫Èáë L 96 K 1 U 1 ][ÁªèÁêÜ K 205 Z 42 L 5 ][Êï∞ÊçÆ L 3 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,ÂÖ¨Âãü/A ,Âü∫Èáë/K ,20/L ,Âπ¥/B ,Áª©/E ,‰ºò/E ,Âü∫Èáë/L ,Âíå/M ,Áª©/E ,‰ºò/E ,Âü∫Èáë/L ,ÁªèÁêÜ/Z ,Êï∞ÊçÆ/L , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÂπ¥Áª© BE
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÁª©‰ºò EE
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÂπ¥Áª©‰ºò BEE
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÁª©‰ºò EE
ÁªÜÂàÜËØçÁΩëÔºö
0:[ ]
1:[ÂÖ¨Âãü]
2:[]
3:[Âü∫Èáë]
4:[]
5:[20]
6:[]
7:[Âπ¥, Âπ¥Áª©, Âπ¥Áª©‰ºò]
8:[Áª©, Áª©‰ºò]
9:[‰ºò]
10:[Âü∫Èáë]
11:[]
12:[Âíå]
13:[Áª©, Áª©‰ºò]
14:[‰ºò]
15:[Âü∫Èáë]
16:[]
17:[ÁªèÁêÜ]
18:[]
19:[Êï∞ÊçÆ]
20:[]
21:[ ]

ÂÖ¨Âãü/nz [0:2]
Âü∫Èáë/n [2:4]
20/m [4:6]
Âπ¥Áª©‰ºò/nr [6:9]
Âπ¥/qt [6:7]
Áª©/ng [7:8]
‰ºò/ag [8:9]
Âü∫Èáë/n [9:11]
Âíå/cc [11:12]
Áª©‰ºò/nr [12:14]
Âü∫Èáë/n [14:16]
ÁªèÁêÜ/nnt [16:18]
Êï∞ÊçÆ/n [18:20]
```
"
update,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
"ÂÖ®ÊãºËØçÂÖ∏Êñá‰ª∂ÈîôËØØÔºà‰ΩìÈù¢=ti3,cao3Ôºâ","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩìÈù¢ËΩ¨ÂÖ®ÊãºÔºåÁªìÊûúÊó∂ticao

## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§


### Ëß¶Âèë‰ª£Á†Å

String str = ‚Äú‰ΩìÈù¢‚Äù;
List<Pinyin> pinyinList = HanLP.convertToPinyinList(str);

### ÊúüÊúõËæìÂá∫
timian

### ÂÆûÈôÖËæìÂá∫
ticao

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÁªèÁ°ÆËÆ§Ôºåpinyin.txt‰∏≠ÈÖçÁΩÆÁöÑÊòØÔºö‰ΩìÈù¢=ti3,cao3
Â∫îËØ•‰øÆÊîπ‰∏∫Ôºö‰ΩìÈù¢=ti3,mian4
"
data-for-1.6.8.zipËµÑÊ∫êÊñá‰ª∂Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Êñá‰π±Á†Å,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.8

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®windows‰∏ãÔºåËß£ÂéãÁº©ËµÑÊ∫êÂåÖÔºåÊñá‰ª∂ÂêçÈÉΩÊòæÁ§∫Ê≠£Â∏∏ÔºõÂú®ubuntuÁ≥ªÁªüËß£ÂéãÁº©Ôºådata/dictionary/custom/ÁõÆÂΩï‰∏ãÔºå‰∏≠ÊñáÂêçÂ≠óÁöÑÂ≠óÂÖ∏Êñá‰ª∂Âá∫Áé∞‰π±Á†Å„ÄÇ



<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
fail to set relative path in hanlp.properties,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËÆæÁΩÆhanlp.properties‰∏≠root‰∏∫Áõ∏ÂØπË∑ØÂæÑÊó†Êïà
ÂèÇËÄÉ https://github.com/hankcs/HanLP/pull/254
Â∞ùËØï‰∏çÂêåÁöÑrelative pathËÆæÁΩÆÊñπÂºèÈÉΩÂ§±Ë¥•‰∫ÜÔºå‰ª•‰∏ã‰∏∫ÂÖ∂‰∏≠‰∏ÄÁßç‰∏æ‰æã

## Â§çÁé∞ÈóÆÈ¢ò

### hanlp.properties:
#Êú¨ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑË∑ØÂæÑÁöÑÊ†πÁõÆÂΩïÔºåÊ†πÁõÆÂΩï+ÂÖ∂‰ªñË∑ØÂæÑ=ÂÆåÊï¥Ë∑ØÂæÑÔºàÊîØÊåÅÁõ∏ÂØπË∑ØÂæÑÔºåËØ∑ÂèÇËÄÉÔºöhttps://github.com/hankcs/HanLP/pull/254Ôºâ
#WindowsÁî®Êà∑ËØ∑Ê≥®ÊÑèÔºåË∑ØÂæÑÂàÜÈöîÁ¨¶Áªü‰∏Ä‰ΩøÁî®/
root=../HanLP/

### path
resources
\- HanLP
    \- data
        \- ...
   hanlp.properties

### report:
È¶ñÊ¨°ÁºñËØëËøêË°åÊó∂ÔºåHanLP‰ºöËá™Âä®ÊûÑÂª∫ËØçÂÖ∏ÁºìÂ≠òÔºåËØ∑Á®çÂÄô‚Ä¶‚Ä¶
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: ËØªÂèñ../HanLP/data/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: Ê†∏ÂøÉËØçÂÖ∏../HanLP/data/dictionary/CoreNatureDictionary.txt‰∏çÂ≠òÂú®ÔºÅjava.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt (No such file or directory)

java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at HanLPMain.test(HanLPMain.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:124)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:580)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:716)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:988)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.privateRun(TestRunner.java:648)
	at org.testng.TestRunner.run(TestRunner.java:505)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:455)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:450)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:415)
	at org.testng.SuiteRunner.run(SuiteRunner.java:364)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:84)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1208)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1137)
	at org.testng.TestNG.runSuites(TestNG.java:1049)
	at org.testng.TestNG.run(TestNG.java:1017)
	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72)
	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123)
Caused by: java.lang.IllegalArgumentException: Ê†∏ÂøÉËØçÂÖ∏../HanLP/data/dictionary/CoreNatureDictionary.txtÂä†ËΩΩÂ§±Ë¥•
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 31 more"
ÂëΩ‰ª§Ë°åËÆ≠ÁªÉWord2VecÊ®°ÂûãÊó∂ÔºåÂ¶Ç‰ΩïÂºÄÂêØdebugËæìÂá∫ËÆ≠ÁªÉËøõÂ∫¶Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®ËÆ≠ÁªÉWord2VecÊ®°ÂûãÊó∂ÂèëÁé∞ÂëΩ‰ª§Ë°åËÆ≠ÁªÉÁöÑÊó∂ÂÄôÈô§‰∫ÜÂºÄÂßãÁöÑthreadÂíåiterÔºåÊ≤°ÊúâÂÖ∂‰ªñËæìÂá∫ÔºåÊü•ÁúãÊ∫êÁ†ÅÂèëÁé∞ËÆ≠ÁªÉ‰∏≠ÊòØ‰ΩøÁî®ÁöÑloggerËæìÂá∫ËøõÂ∫¶‰ø°ÊÅØ„ÄÇ
ÈÄöËøá‰ª£Á†ÅËÆ≠ÁªÉÂèØ‰ª•Ê≥®ÂÜåcallbackËé∑ÂèñËÆ≠ÁªÉËøõÂ∫¶ÔºåËØ∑ÈóÆÂëΩ‰ª§Ë°åÊñπÂºèÊâßË°åÂ¶Ç‰ΩïÊâìÂºÄdebugËæìÂá∫logger‰ª•Êü•ÁúãËøõÂ∫¶Ôºü
"
ÂëΩÂêçÂÆû‰ΩìËØÜÂà´Êä•ÈîôÔºå‰∏çÁü•ÂéüÂõ†,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ôºå‰∏™Âà´Êä•Èîôjava.util.NoSuchElementException

## Â§çÁé∞ÈóÆÈ¢ò
Ê≠£Â∏∏ÊµãËØï

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
           String s1 = ""ÊúâÈôêÂÖ¨Âè∏"";
        String s2 = ""Â∑•Á®ãÊúâÈôêÂÖ¨Âè∏"";
        String s3 = ""ÁîüÊÄÅÂ∑•Á®ãÊúâÈôêÂÖ¨Âè∏"";
//        String s4 = ""Ê±üÂ∑ùÁîüÊÄÅÂ∑•Á®ãÊúâÈôêÂÖ¨Âè∏"";
//        String s4 = ""ÂÖÉ‰∫®ÁîüÊÄÅÂ∑•Á®ãÊúâÈôêÂÖ¨Âè∏"";
        String s4 = ""Á¶èÂìàÁîüÊÄÅÂ∑•Á®ãÊúâÈôêÂÖ¨Âè∏"";

        Segment segment = HanLP.newSegment().enableOrganizationRecognize(true);
        List<Term> term1 =segment.seg(s1);
        for (Term t :term1) {
            System.out.print(t);
        }
        System.out.println("""");
        List<Term> term2 =segment.seg(s2);
        for (Term t :term2) {
            System.out.print(t);
        }
        System.out.println("""");

        List<Term> term3 =segment.seg(s3);
        for (Term t :term3) {
            System.out.print(t);
        }
        System.out.println("""");
        List<Term> term4 =segment.seg(s4);
        for (Term t :term4) {
            System.out.print(t);
        }
        System.out.println("""");
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúâÈôêÂÖ¨Âè∏/nis
Â∑•Á®ãÊúâÈôêÂÖ¨Âè∏/nt
ÁîüÊÄÅÂ∑•Á®ã/nzÊúâÈôêÂÖ¨Âè∏/nis
Á¶èÂìà/nrÁîüÊÄÅÂ∑•Á®ã/nzÊúâÈôêÂÖ¨Âè∏/nis
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÊúâÈôêÂÖ¨Âè∏/nis
Â∑•Á®ãÊúâÈôêÂÖ¨Âè∏/nt
ÁîüÊÄÅÂ∑•Á®ã/nzÊúâÈôêÂÖ¨Âè∏/nis
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList$ListItr.next(LinkedList.java:890)
	at com.hankcs.hanlp.algorithm.Viterbi.computeEnum(Viterbi.java:188)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.viterbiCompute(OrganizationRecognition.java:124)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.recognition(OrganizationRecognition.java:53)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:100)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at com.cubr.slf.test.test.main(test.java:38)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![image](https://user-images.githubusercontent.com/36945916/44455017-99466000-a62f-11e8-9b32-2e09756a080f.png)

"
ÊâπÈáèÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÊó∂Á¢∞Âà∞ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂΩìÊàëÂ∞ùËØïÂ∞Ü‰∏ÄÊâπËØçËØ≠ÔºàÁÆÄ‰ΩìËØçÂíåÁπÅ‰ΩìËØçÈÉΩÊúâÔºâËΩ¨Êç¢‰∏∫ÁÆÄ‰ΩìÊó∂ÂèëÁé∞„ÄêÊàòÂàóËà∞„Äë‰∏ÄËØçËΩ¨Êç¢ÂºÇÂ∏∏

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å
    public static void main(String[] args) throws Exception
    {
               String str=""ÊàòÂàóËà∞"";
		System.out.println(HanLP.t2s(str));
		System.out.println(HanLP.convertToSimplifiedChinese(str));
    }
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
ÊàòÂàóËà∞
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
‰∏ªÂäõËâ¶
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
portable ÁâàÊèêÁ§∫„ÄêÊ≤°ÊúâÊâæÂà∞hanlp.propertiesÔºåÂèØËÉΩ‰ºöÂØºËá¥Êâæ‰∏çÂà∞data„Äë,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò


        <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.6.7</version>
        </dependency>

Êàë‰ΩøÁî®maven ÁöÑportableÁâà  Âú®‰ΩøÁî®HanLp Êó∂ËøòËØ¥ÊèêÁ§∫ÈúÄË¶ÅÊåáÂÆöÊï∞ÊçÆÁõÆÂΩïÔºåportable Áâà‰∏çÊòØÂÜÖÁΩÆ‰∫ÜdataÂêóÔºü ËøòÊòØÁé∞Âú®ÈúÄË¶ÅÂçïÁã¨ÈÖçÁΩÆÊàñËÄÖËøô‰∏™ÊèêÁ§∫ÂèØ‰ª•ÂøΩÁï•


![qq 20180822090353](https://user-images.githubusercontent.com/16408873/44436934-4f875680-a5ea-11e8-9358-eb06a2c30072.png)


<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÁúãÂà∞Êúâ‰∏™Á±ª‰ººÈóÆÈ¢òÔºå‰ΩÜÊòØ‰ªñÊòØÊâìÂåÖÔºåÊàëËøô‰∏™Áõ¥Êé•‰ΩøÁî®maven ‰∏çÈúÄË¶ÅÊâìÂåÖÂêß

[https://github.com/hankcs/HanLP/issues/226](url)
"
pyhanlp CRFËØçÊ≥ïÂàÜÊûê‰∏≠ËØçÊÄßÊòæÁ§∫‰∏çÂÖ®Ôºü,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.7  
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.7   pyhanlp 0.1.44

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®CRFËØçÊ≥ïÂàÜÊûêÊó∂ÔºåÂæóÂà∞ÁöÑËØçÊÄßÊ†áÊ≥®‰ø°ÊÅØ‰∏çÂÆåÊï¥„ÄÇ‰ΩÜ‰ª•Ââç‰ΩøÁî®CRFSegmentÊó∂ËØçÊÄßÊ†áÊ≥®Ê≠£Â∏∏„ÄÇÊòØ‰∏çÊòØ‰∏§ËÄÖÁöÑËØçÊÄß‰ΩìÁ≥ª‰∏ç‰∏ÄÊ†∑Ôºü


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
from pyhanlp import *

sent = '''Âì™‰∫õÈ°πÁõÆÂèØ‰ª•‰∫´Âèó‰ºòÊÉ†ÊîøÁ≠ñÔºåÂ¶Ç‰ΩïÁî≥ËØ∑Ôºü'''

# CRFËØçÊ≥ïÂàÜÊûê
CRFAnalyzer =  JClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
seg = CRFAnalyzer().enableCustomDictionary(True)
out = seg.analyze(sent)
print(out)  

# CRFÂàÜËØç
Config = JClass(""com.hankcs.hanlp.HanLP$Config"")
Config.ShowTermNature = True
CRFSegment = JClass(""com.hankcs.hanlp.seg.CRF.CRFSegment"")
seger = CRFSegment().enableCustomDictionary(True)
words =[str(term) for term in  seger.seg(sent)]
print(' '.join(words))
```
### ÊúüÊúõËæìÂá∫

```
ÊØè‰∏™ËØçËØ≠Ê†áÊ≥®ÂÆåÊï¥ÁöÑËØçÊÄßÔºåÊØîÂ¶Ç Â¶Ç‰Ωï/ryv   
```

### ÂÆûÈôÖËæìÂá∫

```
ÈÉ®ÂàÜËØçËØ≠ÁöÑÊ†áÊ≥®Âè™ÊúâÂºÄÂ§¥‰∏Ä‰∏§‰∏™Â≠óÊØç„ÄÇ‰æãÂ¶ÇÔºö
CRFËØçÊ≥ïÂàÜÊûêÁªìÊûúÔºö
- Âì™‰∫õ/r È°πÁõÆ/n ÂèØ‰ª•/v ‰∫´Âèó/v ‰ºòÊÉ†ÊîøÁ≠ñ/nz Ôºå/w Â¶Ç‰Ωï/r Áî≥ËØ∑/v Ôºü/w
- ÊØèÂπ¥/r Âú®/p ÈùíÊµ∑Êπñ/ns ËøÅÂæô/v ÂÅúÁïô/v ÁöÑ/u ÂÄôÈ∏ü/n Êúâ/v 92/m Áßç/q „ÄÇ/w
ËÄÅÁâàÊú¨CRFSegmentÁªìÊûúÔºö
- Âì™‰∫õ/ry È°πÁõÆ/n ÂèØ‰ª•/v ‰∫´Âèó/v ‰ºòÊÉ†ÊîøÁ≠ñ/nz Ôºå/w Â¶Ç‰Ωï/ryv Áî≥ËØ∑/v Ôºü/w
- ÊØèÂπ¥/t Âú®/p ÈùíÊµ∑Êπñ/ns ËøÅÂæô/v ÂÅúÁïô/vi ÁöÑ/ude1 ÂÄôÈ∏ü/n Êúâ/vyou 92/mq Áßç/q „ÄÇ/w

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->"
CRFNERecognizerÁî®pku1998Âπ¥1-6Êúà‰ªΩÊï∞ÊçÆËÆ≠ÁªÉÔºåjava.lang.OutOfMemoryErrorÔºöGC overhead limit exceeded,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.6.7

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
CRFNERecognizerÁî®pku1998Âπ¥1-6Êúà‰ªΩÊï∞ÊçÆ(64.6M)ËÆ≠ÁªÉÂÜÖÂ≠òÊ∫¢Âá∫:java.lang.OutOfMemoryErrorÔºöGC overhead limit exceeded
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) throws IOException {
	String CORPUS = ""data/test/pku98/1998-2.txt"";
	String NER_MODEL_PATH = ""data/model/crf/pku1998-2/ner.txt"";
	CRFTagger tagger = new CRFNERecognizer(null);
        tagger.train(CORPUS, NER_MODEL_PATH);
	}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âêå‰πâËØçÁöÑËØçÂÖ∏ÊòØÊÄé‰πàËÆ≠ÁªÉÁöÑÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöV1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöV1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âêå‰πâËØçÁöÑËØçÂÖ∏ÊòØÊÄé‰πàËÆ≠ÁªÉÁöÑÔºü
dictionary\synonym\CoreSynonym.txtÊÄé‰πàÊ∑ªÂä†Êñ∞ÁöÑË°åÔºü

"
ÂéªÈô§ÂÅúÁî®ËØçÔºåÊèêÂèñÊúâÁî®ÁöÑÂÖ≥ÈîÆËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.7.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.7.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÁöÑÈóÆÈ¢òÂú®‰∫éÊèêÂèñÁöÑÂÖ≥ÈîÆËØç‰ºöÊúâ‰∏Ä‰∫õ‰∏çÈúÄË¶ÅÁöÑËØç
ÊØîÂ¶ÇÔºöÊàëÊ†πÊçÆ‰∏Ä‰∏™ÁîµÂô®ÂÖ¨Âè∏ÁöÑÁÆÄ‰ªãÂíåÁªèËê•‰∫ßÂìÅÔºåÈÇ£‰πàÊèêÂèñÁöÑÂÖ≥ÈîÆËØç‰∏™‰∫∫ÊÑüËßâÂ∫îËØ•ÈÉΩË∑üÁîµÂô®Áõ∏ÂÖ≥ÁöÑ
‰ΩÜÊòØ‰ºöÊúâÂá†‰∏™‰∏çÈáçË¶ÅÁöÑËØç‰ºöÊèêÂèñÂá∫Êù•ÔºåËØ∑ÈóÆÊÄé‰πàËß£ÂÜ≥ÔºåÊúâ‰ªÄ‰πàÊÑèËßÅÔºåËØ∑ÂëäËØâÊàëÔºåË∞¢Ë∞¢Â§ß‰Ω¨„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å
![image](https://user-images.githubusercontent.com/32606897/44244108-d7ddb400-a204-11e8-8ebd-f6893840cd47.png)
```
  private static List getKeyWordRank(String path) throws IOException {
        String t1 = new String();
        BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), ""utf-8""));
        String str;
        while ((str = reader.readLine()) != null) {
            t1 += str;
        }
        List<String> list = HanLP.extractKeyword(t1, 10);
        for (String item:list) {
            System.out.println(item);
        }
        return list;
    }


```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫  [ÁîµËßÜ, ÁîµËÑë, ÁîµÂô®, Á©∫Ë∞É, ÁîµÂÜ∞ÁÆ±, ÁîµÂ≠ê, ÂæÆÊ≥¢ÁÇâ, Âê∏Â∞òÂô®, Ê∂àÊØíÊüú, Âé®Êàø]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ [ÁîµËßÜ, ÁîµËÑë, ÁîµÂô®, Á©∫Ë∞É, Êµ∑Â∞î, ÁîµÂ≠ê, ÁΩëÁÇπ, Ëê•‰∏öÈ¢ù, ÁôΩÁîµ, Âé®Êàø]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁ¥¢ÂºïÂàÜËØçÂÖ®ÂàáÂàÜÊ®°ÂºèËææ‰∏çÂà∞È¢ÑÊúüÁöÑÊïàÊûú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.6.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®‰ΩøÁî®HanLPÂàÜËØçÊó∂ÔºåËá™ÂÆö‰πâËØçÂÖ∏‰∏≠Â∑≤ÁªèÊ∑ªÂä†‰∫Ü‚ÄúÂçó‰øùÊùë‚Äù„ÄÅ‚Äú‰øùÊùë‚Äù„ÄÅ‚ÄúÂçó‰øù‚ÄùËØçÊù°Ôºå‰ΩÜÊòØÂØπ‚ÄúÂçó‰øùÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùÁöÑÂàÜËØçÁªìÊûú‰∏≠Ê≤°Êúâ‚ÄúÂçó‰øùÊùë‚ÄùÂíå‚Äú‰øùÊùë‚ÄùÔºåÂàÜËØçÁªìÊûú‰∏çÂáÜÁ°ÆÔºõÂØπÊØî‰ΩøÁî®IKÂàÜËØçÊó∂Ôºå‰ΩøÁî®Áõ∏ÂêåÂÜÖÂÆπÁöÑËá™ÂÆö‰πâËØçÂÖ∏Ôºå‚ÄúÂçó‰øùÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùÁöÑÂàÜËØçÁªìÊûú‰∏≠Âê´Êúâ‚ÄúÂçó‰øùÊùë‚Äù„ÄÅ‚Äú‰øùÊùë‚Äù„ÄÅ‚ÄúÂçó‰øù‚Äù„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
1.HanLPÂàÜËØçÁªìÊûúÂ¶Ç‰∏ã
![hanlp](https://user-images.githubusercontent.com/42425179/44185683-8cb19b80-a147-11e8-8999-67728033cbdb.png)

2.IK‰ΩøÁî®ÂêåÊ†∑ÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÂÜÖÂÆπÔºåÂàÜËØçÁªìÊûú‰∏∫Ôºö ‚ÄúÂçó‰øùÊùë‚ÄùÔºå‚ÄúÂçó‰øù‚ÄùÔºå‚Äú‰øùÊùë‚ÄùÔºå‚ÄúÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùÔºå‚ÄúÊùëÊ∞ë‚ÄùÔºå‚ÄúÊ∞ëÂßî‚ÄùÔºå‚ÄúÂßîÂëò‰ºö‚Äù
![ik](https://user-images.githubusercontent.com/42425179/44185677-815e7000-a147-11e8-984a-03b885cee745.png)


### Ê≠•È™§

1. È¶ñÂÖàËÆæÁΩÆËá™ÂÆö‰πâËØçÂÖ∏
2. ÁÑ∂ÂêéËÆæÁΩÆschema.xml‰∏≠ÁöÑfieldTypeËäÇÁÇπ
![image](https://user-images.githubusercontent.com/42425179/44185966-3a717a00-a149-11e8-8b91-ad9413f1da8b.png)
![image](https://user-images.githubusercontent.com/42425179/44185984-4f4e0d80-a149-11e8-9c9c-ad8f39b98541.png)

3. Êé•ÁùÄÂØπ‚ÄúÂçó‰øùÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùËøõË°åÂàÜËØç

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""‰øùÊùë"");
        CustomDictionary.add(""Âçó‰øùÊùë"");
        CustomDictionary.add(""Âçó‰øù"");
        System.out.println(IndexTokenizer.segment(""Âçó‰øùÊùëÊ∞ëÂßîÂëò‰ºö""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫ ‚ÄúÂçó‰øùÊùë‚ÄùÔºå‚ÄúÂçó‰øù‚ÄùÔºå‚Äú‰øùÊùë‚ÄùÔºå‚ÄúÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùÔºå‚ÄúÊùëÊ∞ë‚ÄùÔºå‚ÄúÊ∞ëÂßî‚ÄùÔºå‚ÄúÂßîÂëò‚ÄùÔºå‚ÄúÂßîÂëò‰ºö‚Äù
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ ‚ÄúÂçó‰øù‚ÄùÔºå‚ÄúÊùëÊ∞ëÂßîÂëò‰ºö‚ÄùÔºå‚ÄúÊùëÊ∞ë‚ÄùÔºå‚ÄúÊ∞ëÂßî‚ÄùÔºå‚ÄúÂßîÂëò‚ÄùÔºå‚ÄúÂßîÂëò‰ºö‚Äù
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ë¥ùÂè∂ÊñØÊ®°ÂûãËÆ≠ÁªÉÊñáÊú¨ÂàÜÁ±ªÊ®°ÂûãÊó∂ÔºåÈÄâ‰∏≠ÁâπÂæÅÊï∞:0 / 380 = 0.00%,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.7.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.7.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Êàë‰ΩøÁî®Êú¥Á¥†Ë¥ùÂè∂ÊñØÊ®°ÂûãËÆ≠ÁªÉËøõË°åÊñáÊú¨ÂàÜÁ±ªÊó∂ÔºåÊàëËá™Â∑±ËÆ≠ÁªÉ‰∏§‰∏™ÁßçÁ±ªÔºöÂïÜÁ•®„ÄÅ‰ΩìËÇ≤ÔºåÊØè‰∏™ÁßçÁ±ªËÆ≠ÁªÉ5‰∏™Âè•Â≠êÔºåÁªìÊûúÊó•ÂøóÊòæÁ§∫Ôºö
Ê≠£Âú®ÊûÑÈÄ†ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ...[ÂïÜÁ•®]...50.00%...[‰ΩìËÇ≤]...100.00%...ËÄóÊó∂ 260 ms Âä†ËΩΩÂÆåÊØï
ÂéüÂßãÊï∞ÊçÆÈõÜÂ§ßÂ∞è:10
‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅ‰∏≠...ËÄóÊó∂ 1 ms,ÈÄâ‰∏≠ÁâπÂæÅÊï∞:0 / 380 = 0.00%
Áî®ËÆ≠ÁªÉÂæóÂà∞ÁöÑÊ®°ÂûãÊµãËØï‰∏çÂêåÁßçÁ±ªÁöÑÊñáÊú¨Êó∂ÔºåÊØèÊ¨°ÈÉΩÊòØ‰ΩìËÇ≤ÔºåËøôÊòØÊÄé‰πàÂõû‰∫ãÂë¢ÔºüÊòØËÆ≠ÁªÉÁöÑÁ¥†Êùê‰∏çÂ§üÂØºËá¥ÁöÑÂêóÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑ÈóÆÊÄé‰πàËÆ≠ÁªÉ‰æùÂ≠òÂè•Ê≥ïÊ®°Âûã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöV1.6.7
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöV1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

ËØ∑ÈóÆÊÄé‰πàËÆ≠ÁªÉ‰æùÂ≠òÂè•Ê≥ïÊ®°ÂûãÔºü
ÊúâËøôÊ†∑ÁöÑÁ§∫‰æãÂêóÔºü
"
portable-1.6.7ÁâàÊú¨‰∏≠‰ΩøÁî®mavenÊûÑÂª∫ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâßË°åNLPTokenizer.analyzeÂàÜÊûêÊó∂ÔºåÊä•Â¶Ç‰∏ãÈîôËØØ,"Ë≠¶Âëä: ÊâìÂºÄÂ§±Ë¥•Ôºödata/model/perceptron/msra/cws.bin
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at HanLPTest.main(HanLPTest.java:18)
Caused by: java.lang.RuntimeException: java.io.IOException: data/model/perceptron/msra/cws.bin Âä†ËΩΩÂ§±Ë¥•
MavenÊûÑÂª∫ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰∏çÊòØ‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊâãÂä®ÂØºÂÖ•dataÂêóÔºü"
 MaxEntDependencyParserÂú®1.6.6ÁâàÊú¨ÊäõÂá∫ÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ª£Á†Å‰∏∫Ôºö CoNLLSentence sent =  MaxEntDependencyParser.compute(text);
Âú®1.6.4ÁâàÊú¨ËøêË°åÊ≤°ÈóÆÈ¢ò
Âú®1.6.6ÁâàÊú¨ËøêË°åÊäõÂá∫ÂºÇÂ∏∏Ôºö
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.dependency.MinimumSpanningTreeParser.parse(MinimumSpanningTreeParser.java:41)
	at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:56)
	at com.hankcs.hanlp.dependency.MaxEntDependencyParser.compute(MaxEntDependencyParser.java:86)
	at Test.main(Test.java:54)
Caused by: java.lang.NullPointerException
	at com.hankcs.hanlp.dependency.common.Node.<init>(Node.java:113)
	at com.hankcs.hanlp.dependency.common.Node.<clinit>(Node.java:28)
	... 4 more
"
solrÂàÜËØç‰∏éÂú®Á∫øÂàÜËØçÊúâÂ∑ÆÂà´ÁöÑÂéüÂõ†Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®solrÈÖçÁΩÆ‰∫ÜÊúÄÊñ∞ÁöÑjarÂåÖËøõË°åÂàÜËØç‰∏éhttp://hanlp.hankcs.com‰∏äÁöÑÂú®Á∫øÊºîÁ§∫ÁªìÊûú‰∏çÂêåÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØ‰ªÄ‰πàÔºü
‰æãÂ¶ÇÔºöÂ§úÂú£‰πîÊ≤ª‰∏ÄÁ∫ßËë°ËêÑÂõ≠
Âú®Á∫øÂàÜËØç‚ÄúÂ§úÂú£‰πîÊ≤ª‚ÄùÔºå‚Äú‰∏ÄÁ∫ß‚ÄùÔºå‚ÄúËë°ËêÑÂõ≠‚Äù
solrÂàÜËØç‚ÄúÂ§ú‚ÄùÔºå‚ÄùÂú£‚ÄùÔºå‚Äù‰πî‚ÄùÔºå‚ÄùÊ≤ª‚ÄùÔºå‚Äú‰∏ÄÁ∫ß‚ÄùÔºå‚ÄúËë°ËêÑÂõ≠‚Äù

"
ÊåâÁÖßËØçÊÄß‰ºòÂÖàÁ∫ßÈÖçÁΩÆ,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster


## ÊàëÁöÑÈóÆÈ¢ò
ÁõÆÂâçÂàÜËØçÁöÑËØç‰ºòÂÖàÁ∫ßÂ•ΩÂÉèÊòØÊ†πÊçÆÈÖçÁΩÆÁöÑÈ°∫Â∫èÁöÑ
ËØ∑ÈóÆÊòØÂê¶ÊîØÊåÅÊåâÁÖßËØçÊÄß‰ºòÂÖàÁ∫ßËæìÂá∫ÂàÜËØçÁªìÊûúÔºüÊØîÂ¶ÇËØ¥ÊàëËøôËæπÂÆö‰πâ‰∫Ü‰∏§‰∏™ËØçÔºö
ÂíåÊúç n1
ÊúçË£Ö n2
ÁÑ∂ÂêéÂàÜËØç Êó•Êú¨ÂíåÊúçË£Ö
‰ΩÜÈúÄË¶ÅÊ†πÊçÆ‰∏çÂêåÂú∫ÊôØÊàëÂèØËÉΩÈúÄË¶ÅÂàÜÂà´ËæìÂá∫ÂíåÊúçÊàñËÄÖÊúçË£ÖÔºåËøôÊó∂ÂÄôÊàëÂ∞±ÈúÄË¶ÅÈÖçÁΩÆ‰∏Ä‰∏™‰ºòÂÖàÁ∫ßÊù•ÊéßÂà∂ËæìÂá∫Ôºàn1Âíån2ÁöÑËæìÂá∫‰ºòÂÖàÁ∫ßÔºâ"
pyhanlpÂ¶Ç‰ΩïÈáçÂÜôFilterËøáÊª§Âô®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
pyhanlpÂ¶Ç‰ΩïÈáçÂÜôFilterËøáÊª§Âô®

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Ê≠£Âú®‰ΩøÁî®pyhanlpÔºåÊúâ‰∏Ä‰∏™ÈúÄÊ±ÇÊòØÈúÄË¶ÅÂú®‰ΩøÁî®corestopwordÂêéÔºåÂàÜËØçÁªìÊûú‰øùÁïôÊï∞Â≠ó„ÄÇÊåâÁÖßÁé∞Âú®javaÁâàÊú¨ÁöÑÈÄªËæëÊòØÈúÄË¶ÅÈáçÂÜôFilterÔºåËá™Â∑±‰∏çÂ§™Áü•ÈÅìÂ¶Ç‰ΩïÂú®pyhanlpÈáåÈù¢ÈáçÂÜôËøáÊª§Âô®

"
ÊèêÂèñÊñ∞ËØçÁöÑÂá†ÁÇπÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

1.Âú®com.hankcs.hanlp.mining.word.WordInfo.computeProbabilityEntropy()ËÆ°ÁÆóÂ∑¶Âè≥ÁÜµÁöÑÊó∂ÂÄô,Â¶ÇÊûúËØ•ËØç‰Ωç‰∫éÈ¶ñÊàñËÄÖÂ∞æÈÉ®Êó∂ÂèñÊúÄÂ∞èÂÄº ËøôÊ†∑ÂèñÂá∫ÁöÑÁÜµÂÄº‰∏∫0,ÂΩìËÆæÁΩÆÈªòËÆ§ÁÜµÂÄº,ÂºÄÂ§¥ÁªìÂ∞æÁöÑËØçÂ∞ÜÊó†Ê≥ïÂèñÂá∫
2.‰∫í‰ø°ÊÅØ
‰ª£Á†Å‰∏≠ÁªôÂá∫ÁöÑÁÆóÊ≥ï
![1](https://user-images.githubusercontent.com/36916036/43451100-c14b4b42-94e6-11e8-923a-912d6e534bd0.png)

![12379570-7c7355505813d673](https://user-images.githubusercontent.com/36916036/43620279-765a49ce-9704-11e8-80fa-7515e1a37ab4.png)
‰∏çÊòØÂÆåÂÖ®‰∏ÄËá¥




"
Á¥¢ÂºïÂàÜËØçÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂØπÈóÆÈ¢ò`Ë¥≠Áâ©ËΩ¶ÈáåÁöÑ‰∫ßÂìÅÊó†Ê≥ïÂà†Èô§`ËøõË°åÁ¥¢ÂºïÂàÜËØçÔºåÊ≤°ÊúâÂ∞Ü`Ë¥≠Áâ©ËΩ¶`ÂàÜËØçÂá∫Êù•

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

``` java
    public void testIssue1234() throws Exception
    {
         Segment segment = HanLP.newSegment().enableNameRecognize(false);
         segment.enableIndexMode(true);

         List<Term> termList = segment.seg(""Ë¥≠Áâ©ËΩ¶ÈáåÁöÑ‰∫ßÂìÅÊó†Ê≥ïÂà†Èô§"");
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Ë¥≠Áâ©ËΩ¶ÔºåË¥≠Áâ©, ËΩ¶Èáå, ÁöÑ, ‰∫ßÂìÅ, Êó†Ê≥ï, Âà†Èô§]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Ë¥≠Áâ©, ËΩ¶Èáå, ÁöÑ, ‰∫ßÂìÅ, Êó†Ê≥ï, Âà†Èô§]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
 ËÆ≠ÁªÉÁöÑword2vec Ê®°ÂûãÔºåÊîæÂú®hdfs‰∏äÔºåÁî®sparkÂàÜÂ∏ÉÂºèÁöÑÂä†ËΩΩË∞ÉÁî®Â§±Ë¥•ÈóÆÈ¢òÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
 ËÆ≠ÁªÉÁöÑword2vec Ê®°ÂûãÔºåÊîæÂú®hdfs‰∏äÔºåÁî®sparkÂàÜÂ∏ÉÂºèÁöÑÂä†ËΩΩË∞ÉÁî®Â§±Ë¥•ÈóÆÈ¢òÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->



### Ëß¶Âèë‰ª£Á†Å

```
rdd.mapPartitions(iteratos => myFunctions(iteratos, words))
def myFunctions(iterator: Iterator[String], word: String): Iterator[mutable.HashMap[Integer, Float]] = {
    val wordVecModel = new WordVectorModel(""data/model.txt"")
    val docmentsModel = new DocVectorModel(wordVecModel)
    val sets = mutable.Set[mutable.HashMap[Integer, Float]]()
    for (iterm <- iterator) {
      val arrays=iterm.split(""\t"")
      val id=arrays(0).toInt
      val contents =iterm
      docmentsModel.addDocument(id, HanLP.convertToSimplifiedChinese(contents))
      val list = docmentsModel.nearest(word)
      import scala.collection.JavaConversions._
      for (id <- list) {
        val maps = mutable.HashMap[Integer, Float]()
        maps.put(id.getKey, id.getValue)
        sets.add(maps)
      }
    }
    sets.iterator
  }

```

### ÈîôËØØËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
 Á©∫ÊåáÈíàÂºÇÂ∏∏ÔºåÂú®ËØªÂèñÊ®°ÂûãÈÇ£Ë°å‰ª£Á†Å‰∏ä
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÂçïÊú∫ÂèØ‰ª•Âä†ËΩΩÔºåÊîæÂú®ÈõÜÁæ§‰∏äÂä†ËΩΩÂ∞±‰∏çË°å
#ÈªòËÆ§ÁöÑIOÈÄÇÈÖçÂô®Â¶Ç‰∏ãÔºåËØ•ÈÄÇÈÖçÂô®ÊòØÂü∫‰∫éÊôÆÈÄöÊñá‰ª∂Á≥ªÁªüÁöÑ„ÄÇ
IOAdapter=com.npl.spark.HadoopFileIoAdapter ‰πüÂ∑≤ÁªèÈáçÂÜô‰∫Ü
"
Áª¥ÁâπÊØîÁÆóÊ≥ïÂú®ÂàÜËØç‰πãÂêéÂÖ≥‰∫éËØçÊÄßÊ†áÊ≥®ÁöÑÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö portable-1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö portable-1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂú®‰ΩøÁî®`Áª¥ÁâπÊØî`ÂàÜËØçÁöÑÊó∂ÂÄôÔºåÁ¢∞Âà∞‰∏Ä‰∏™ÂæàÂ•áÊÄ™ÁöÑÁé∞Ë±°ÔºöÂÜçÁ¢∞Âà∞‰∏ÄÂè•ËØùÔºåÂÆÉËÉΩÂ§üÂú®Á≤óÂàÜËØçÁöÑÊó∂ÂÄôÊ≠£Á°ÆËØÜÂà´ËØçÊÄßÔºå‰ΩÜÊòØÂú®ÊúÄÂêéÁöÑËßíËâ≤Ê†áÊ≥®ÁöÑÊó∂ÂÄôÔºåÂ¶ÇÊûúÂàÜËØçÁöÑÁªìÊûúÂèØ‰ª•ÁªÑÂª∫‰∏∫Êú∫ÊûÑÂêçÔºà‰πüÂ∞±ÊòØÂèØ‰ª•ÁªÑÂêà‰∏∫`FD`Ôºå`FCCD`Ëøô‰∏ÄÁ±ªÁöÑÁªÑÂêàÔºâÔºåÈÇ£‰πàÊúÄÂêéÁöÑÁªìÊûú‰ºöË¢´ËØÜÂà´‰∏∫Êú∫ÊûÑ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂ£∞Êòé‰∏Ä‰∏™`Áª¥ÁâπÊØîÂàÜËØç`Ê®°ÂûãÔºåÂº∫Âà∂ÂºÄÂêØËá™ÂÆö‰πâËØçÂÖ∏Ôºå‰∫∫ÂêçËØÜÂà´ÔºåÊú∫ÊûÑËØÜÂà´


### Ëß¶Âèë‰ª£Á†Å

```
import com.hankcs.hanlp.HanLP
HanLP.Config.enableDebug()

val segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableNameRecognize(true).enableAllNamedEntityRecognize(true).
            enableJapaneseNameRecognize(true).enableOrganizationRecognize(true).
            enableTranslatedNameRecognize(true)

println(segment.seg(HanLP.convertToSimplifiedChinese(""Ë¥æÁªèÁêÜÂÖ®Âà©ÈõÜÂõ¢"")))

println(segment.seg(""Ë¥∫ÂçéÂπ≥ÂäûÂÖ¨ÂÆ§""))
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Ë¥æÁªèÁêÜ/nr, ÂÖ®Âà©ÈõÜÂõ¢/nt]
[Ë¥∫ÂçéÂπ≥/nr, ÂäûÂÖ¨ÂÆ§/nis]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ë¥æ]
2:[Áªè, ÁªèÁêÜ]
3:[ÁêÜ]
4:[ÂÖ®]
5:[Âà©]
6:[ÈõÜ, ÈõÜÂõ¢]
7:[Âõ¢]
8:[ ]

Á≤óÂàÜÁªìÊûú[Ë¥æ/nz, ÁªèÁêÜ/nnt, ÂÖ®/a, Âà©/n, ÈõÜÂõ¢/nis]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Ë¥æ B 1308 D 4 E 2 C 1 ][ÁªèÁêÜ K 205 Z 42 L 5 ][ÂÖ® B 876 D 406 C 208 E 16 L 16 ][Âà© D 602 C 560 E 173 B 30 L 5 K 2 ][ÈõÜÂõ¢ L 119 K 26 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Ë¥æ/B ,ÁªèÁêÜ/Z ,ÂÖ®/L ,Âà©/B ,ÈõÜÂõ¢/L , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöË¥æÁªèÁêÜ BZ
Âú∞ÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1163565 ][Ë¥æ C 8 ][ÁªèÁêÜ A 1 B 1 ][ÂÖ® A 152 C 101 D 16 B 15 ][Âà© D 91 C 49 A 4 B 4 ][ÈõÜÂõ¢ B 38 A 1 ][  B 1322 ]
Âú∞ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /S ,Ë¥æ/C ,ÁªèÁêÜ/A ,ÂÖ®/C ,Âà©/D ,ÈõÜÂõ¢/B , /S]
Êú∫ÊûÑÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1169907 ][Ë¥æÁªèÁêÜ F 6781 B 769 A 266 X 6 ][ÂÖ® B 26 C 17 A 7 ][Âà© C 45 A 9 B 3 D 3 ][ÈõÜÂõ¢ K 1000 D 1000 ][  B 8423 ]
Êú∫ÊûÑÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /S ,Ë¥æÁªèÁêÜ/F ,ÂÖ®/C ,Âà©/C ,ÈõÜÂõ¢/D , /S]
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöË¥æÁªèÁêÜÂÖ®Âà©ÈõÜÂõ¢ FCCD
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöÂà©ÈõÜÂõ¢ CD
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöÂÖ®Âà©ÈõÜÂõ¢ CCD
ÁªÜÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ë¥æÁªèÁêÜ, Ë¥æÁªèÁêÜÂÖ®Âà©ÈõÜÂõ¢]
2:[Áªè]
3:[ÁêÜ]
4:[ÂÖ®, ÂÖ®Âà©ÈõÜÂõ¢]
5:[Âà©, Âà©ÈõÜÂõ¢]
6:[ÈõÜÂõ¢]
7:[]
8:[ ]

[Ë¥æÁªèÁêÜÂÖ®Âà©ÈõÜÂõ¢/nt]


Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ë¥∫]
2:[Âçé]
3:[Âπ≥]
4:[Âäû, ÂäûÂÖ¨, ÂäûÂÖ¨ÂÆ§]
5:[ÂÖ¨, ÂÖ¨ÂÆ§]
6:[ÂÆ§]
7:[ ]

Á≤óÂàÜÁªìÊûú[Ë¥∫/vg, Âçé/b, Âπ≥/v, Âäû/v, ÂÖ¨ÂÆ§/nz]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Ë¥∫ B 1011 E 78 C 70 D 25 ][Âçé D 4763 B 1296 C 966 E 595 L 1 ][Âπ≥ D 6844 E 544 C 189 B 36 L 11 ][Âäû L 4 K 2 ][ÂÖ¨ÂÆ§ A 20833310 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Ë¥∫/B ,Âçé/C ,Âπ≥/D ,Âäû/L ,ÂÖ¨ÂÆ§/A , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöË¥∫Âçé BC
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöË¥∫ÂçéÂπ≥ BCD
Âú∞ÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1163565 ][Ë¥∫ C 41 D 4 ][Âçé C 385 D 196 B 22 A 8 E 1 ][Âπ≥ D 1039 C 241 A 16 B 13 ][Âäû B 23 A 3 ][ÂÖ¨ÂÆ§ Z 20211628 ][  B 1322 ]
Âú∞ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /S ,Ë¥∫/C ,Âçé/D ,Âπ≥/B ,Âäû/A ,ÂÖ¨ÂÆ§/Z , /S]
Êú∫ÊûÑÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1169907 ][Ë¥∫ÂçéÂπ≥ F 6781 B 769 A 266 X 6 ][Âäû D 213 C 11 A 2 B 2 ][ÂÖ¨ÂÆ§ Z 19892007 ][  B 8423 ]
Êú∫ÊûÑÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /S ,Ë¥∫ÂçéÂπ≥/F ,Âäû/D ,ÂÖ¨ÂÆ§/Z , /S]
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöË¥∫ÂçéÂπ≥Âäû FD
ÁªÜÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ë¥∫ÂçéÂπ≥, Ë¥∫ÂçéÂπ≥Âäû]
2:[]
3:[]
4:[Âäû]
5:[ÂÖ¨ÂÆ§]
6:[ÂÆ§]
7:[ ]

[Ë¥∫ÂçéÂπ≥Âäû/nt, ÂÖ¨ÂÆ§/nz]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
1. ÊàëÂ∞ùËØï‰øÆÊîπËøá`CoreNatureDictionary.txt`Âíå`CoreNatureDictionary.ngram.txt`ÔºàÊ†πÊçÆ[issue813](https://github.com/hankcs/HanLP/issues/813#issuecomment-385966927)ÔºâÔºåÊØîÂ¶ÇÈíàÂØπ`Ë¥∫ÂçéÂπ≥ÂäûÂÖ¨ÂÆ§`Ëøô‰∏ÄÂè•ËØùÔºåÊàëÂàÜÂà´Â∞Ü`Ë¥∫ÂçéÂπ≥ nr 10000`Âíå`Ë¥∫ÂçéÂπ≥@ÂäûÂÖ¨ÂÆ§ 10000`Âä†ÂÖ•Âà∞‰∏§‰∏™Êñá‰ª∂‰∏≠ÔºåÁªìÊûúÂπ∂Ê≤°ÊúâÊîπÂèòÔºåËØ∑ÈóÆËøôÁßçÊó∂ÂÄôÂ∫îËØ•ÊÄé‰πà‰øÆÊîπÔºü

2. ÊàëÂêåÊó∂‰ΩøÁî®‰∫ÜÊÑüÁü•Êú∫ÂéªÂàÜËØçÔºå‰∏çÂ≠òÂú®‰∏äÈù¢ËØ¥ÁöÑÈóÆÈ¢òÔºå‰ΩÜÊòØÁî±‰∫éÁõÆÂâçÂæàÊÉ≥ÊêûÊòéÁôΩ‰∏∫‰ªÄ‰πà‰ºöÂá∫Áé∞‰∏äÈù¢ÁöÑÁªìÊûú„ÄÇ

3. ÈùûÂ∏∏ÊúüÂæÖÂπ∂‰∏îÊÑüË∞¢‰Ω†ÁöÑÂõûÂ§ç„ÄÇ
"
Âú®‰∏Ä‰∏™Â∑•Á®ãÈáåÈù¢ÂêåÊó∂‰ΩøÁî®Â§ö‰ªΩËá™ÂÆö‰πâËØçÂÖ∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö4.0.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö4.0.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ËÉΩ‰∏çËÉΩÂú®‰∏Ä‰∏™Â∑•Á®ãÈáåÈù¢ÂêåÊó∂‰ΩøÁî®Â§ö‰ªΩËá™ÂÆö‰πâËØçÂÖ∏Ôºå‰æãÂ¶ÇÊàëÊúâ‰∏§‰∏™‰∏öÂä°Âú∫ÊôØÔºåÂú∫ÊôØ‰∏ÄÈúÄË¶ÅËá™ÂÆö‰πâËØçÂÖ∏1ÔºåÂú∫ÊôØ‰∫åÈúÄË¶ÅËá™ÂÆö‰πâËØçÂÖ∏2

"
‚ÄúËãπÊûú‚ÄùÊãºÈü≥ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö portable-1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö portable-1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

‰∏≠Êñá `ËãπÊûú` ËΩ¨ÊãºÈü≥ÔºåÊúüÊúõËæìÂá∫ `pingguo`Ôºå‰ΩÜÊòØÁªìÊûúÂç¥ÊòØ `pinguo`

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

```
    System.out.println(HanLP.convertToPinyinString(""ËãπÊûú"", "" "", true));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
pingguo

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
pinguo

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ËØïËøáÂæàÂ§öÁâàÊú¨ÈÉΩÊòØËøôÊ†∑

"
‰∫∫ÂêçËØÜÂà´Âá∫ÈîôÔºå‰ºöÂ∞±ËøëÂåπÈÖçÂπ∂ËØÜÂà´‰∫∫Âêç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ΩøÁî® `HanLP.newSegment()` Êñ∞Ëµ∑ÂàÜËØç‰πãÂêéÔºåÂàÜËØç‰ºö‰ºòÂÖàÂåπÈÖçÁ¨¨‰∏Ä‰∏™‰∫∫ÂêçÔºåÂÆûÈôÖ‰∏äÂÆÉÂπ∂‰∏çÊòØ‰∫∫Âêç

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
val segment = HanLP.newSegment().enableCustomDictionaryForcing(true).enableCustomDictionary(true).enableNameRecognize(true).enableAllNamedEntityRecognize(true).
                        enableJapaneseNameRecognize(true).enableOrganizationRecognize(true).
                        enableTranslatedNameRecognize(true)
val termList = segment.seg(""Â∏à‰∏ìÁè≠Ëî°Áé≤ÊÄù"")
println(termList)

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Â∏à‰∏ìÁè≠/nis Ëî°Áé≤ÊÄù/nr]
```

### ÂÆûÈôÖËæìÂá∫
[Â∏à‰∏ì/nis, Áè≠Ëî°Áé≤/nr, ÊÄù/v]
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Â∏à‰∏ì/nis, Áè≠Ëî°Áé≤/nr, ÊÄù/v]
```

ÊàëÁåúÊµãÁöÑÂéüÂõ†ÊòØÔºåÂàÜËØçÁöÑÊó∂ÂÄôÔºåÂàÜËØçÂô®Â∞Ü`Áè≠Ëî°Áé≤`ÊúâÈôêÂåπÈÖç‰∫ÜÔºåÂπ∂ËØÜÂà´‰∏∫‰∫∫Âêç„ÄÇ

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØçÂÖ±Áé∞ÁªüËÆ°ÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.6

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
public static void main(String[] args)
    {
        Occurrence occurrence = new Occurrence();
        String co = ""ÂÖ±ÊúâÁ∫¶300‰∏™ÂèÇÂ±ïÂïÜÁöÑËøë2000ËæÜÁªèÂÖ∏ËÄÅÁà∑ËΩ¶ÂèÇÂ±ï„ÄÇ ""+
""1Ôºâ‰∏äÊµ∑Âä†ÈÄüÈáëËûç‰∏öÂºÄÊîæÔºåÁ°ÆÂÆöÂÖ≠Â§ßÊñπÈù¢ÂÖàË°åÂÖàËØïÔºåÊØîÂ¶ÇÊîØÊåÅÂ§ñÂõΩÈì∂Ë°åÂú®Ê≤™ÂêåÊó∂ËÆæÁ´ãÂàÜË°åÂíåÂ≠êË°åÁ≠â„ÄÇ ""+
""2ÔºâÁ§æÁßëÈô¢ÔºöÈ¢ÑËÆ°‰ªäÂπ¥Êàø‰ª∑Âπ≥Á®≥ÂõûËêΩÔºå‰∏Ä‰∫åÁ∫øÂüéÂ∏ÇÊàñÊé¢Â∫ï„ÄÇ ""+
""3ÔºâÁ¨¨‰∏âÂ±ä‰∏ùÂçö‰ºöÊñáÂåñ‰∫ß‰∏ö‰∏éËµÑÊú¨ÂàõÊñ∞ËûçÂêàÂèëÂ±ïÂàÜËÆ∫ÂùõÂú®Ë•øÂÆâ‰∏æË°å„ÄÇ ""+
""4ÔºâÁÅµÊ¥ªÊô∫ËÉΩÊï∞ÊçÆÈ©±Âä® ‚ÄúÊñ∞Èõ∂ÂîÆ‚ÄùÂºïÁâ©ÊµÅË°å‰∏öÊ∑±Â∫¶ÂèòÈù©„ÄÇ ""+
""5ÔºâÊò®Êó•Êî∂ÁõòÔºö ""+
""Ê≤™Êåá3192.12/+0.57%¬† ""+
""Ê∑±ÊàêÊåá10747.99/+0.72%¬† ""+
""Âàõ‰∏öÊùø1858.01/+1.48%¬† ""+
""ÊÅíÊåá31152.03/-1.23% ""+
""1ÔºâÂ≠¶ÁîüÂáèË¥üËé∑ÂæóÊÑüÊú™ÊòéÊòæÊèêÂçáÔºå‰∫∫Ê∞ëÊó•Êä•Ôºö¬†‰∏çËÉΩÂ§¥ÁóõÂåªÂ§¥ËÑöÁóõÂåªËÑö„ÄÇ ""+
""2ÔºâÂ≠îÂ≠êÊïÖÈáåÊõ≤Èòú‰∏æË°å‰º†ÁªüÁ§º‰ª™Ê¥ªÂä®Ôºå‰º†ÊâøÂ≠ùÂæ∑ÊñáÂåñ„ÄÇ ""+
""3ÔºâÂÖ®ÂõΩÈ¶ñ‰∏™ÂÖªÁîüÈ©¨ÊãâÊùæÂú®‰∫≥Â∑û‰ΩìËÇ≤Âú∫ÂºÄË∑ëÔºå2‰∏áÂêçÈÄâÊâãÂèÇÂä†„ÄÇ ""+
""4ÔºâÂ•≥ÊéíÊñ∞ËµõÂåó‰ªëËµ∑Ëà™ÔºåÈÉéÂπ≥Á°ÆËÆ§Êú±Â©∑ÊöÇÊó∂‰∏ç‰ºöÂèÇËµõ„ÄÇ ""+
""1ÔºâÂåó‰∫¨PM2.5Êù•Ê∫êÊú¨Âú∞ÊéíÊîæÂç†2/3ÔºåÂå∫Âüü‰º†ËæìÂç†1/3ÔºåÊú¨Âú∞ÊéíÊîæ‰∏≠ÁßªÂä®Ê∫êÂç†ÊØîÈ´òËææ45%„ÄÇÂåó‰∫¨Â§ßÂÖ¥Â¢û44ÂÆ∂Ê≥®ÂÜåËµÑÊú¨Ëøá‰∫øÂÖÉ‰ºÅ‰∏ö„ÄÇÂåó‰∫¨Â∏ÇÁªßÁª≠ÂÆûË°åÊãõÁîüËµÑÊ†ºÂ≠¶Ê†°ÂêçÂçïÂÖ¨Á§∫Âà∂Â∫¶Ôºå371ÊâÄÈ´ò‰∏≠ÊúâÊãõÁîüËµÑÊ†º„ÄÇ ""+
""2Ôºâ‰∏äÊµ∑Â∞ÜÈáçÂ°ëÂçó‰∫¨Ë∑ØÊ∑ÆÊµ∑Ë∑ØÔºåÁªù‰ª£È£éÂçéÂçé‰∏ΩËΩ¨Ë∫´„ÄÇ‰∏äÊµ∑Âá∫Âè£ÈÄÄÁ®éÁî≥Êä•‚ÄúÂÖçÂ°´Êä•‚ÄùÔºå‰∏∫‰ºÅ‰∏öÂáèÂ∞ëÈÄæ2‰∫øÈ°πÈáçÂ§çÂΩïÂÖ•È°πÁõÆ„ÄÇ ""+
""3ÔºâÊ∑±Âú≥Ê†∏ÂèëËá™Âä®È©æÈ©∂ÈÅìË∑ØÊµãËØïÁâåÁÖßÔºåÈ°ªÊúâÈ©æÈ©∂ÂëòÂíåÂÆâÂÖ®Âëò„ÄÇÂπø‰∏ú‚ÄîÈªëÈæôÊ±ü‚ÄúÂØíÊù•ÊöëÂæÄ„ÄÅÂçóÊù•ÂåóÂæÄ‚ÄùÊóÖÊ∏∏Â≠£Êó•ÂâçÂú®ÂπøÂ∑ûÂºÄÂπï„ÄÇÊô∫ÊÖß‰∫§ÈÄöÂ®ÅÂäõÂàùÁé∞ÔºåÂπø‰∏úÂá∫Ë°åÊñπÂºèÂá∫Áé∞ÂÖ®Êñπ‰ΩçÂèòÂåñ„ÄÇ ""+
""4ÔºâÊπñÂçóÔºöÈîªÈÄ†Âπ≤ÈÉ®Á°¨‰ΩúÈ£éÔºåË∞ÉÁ†î‰∏çÊâìÊãõÂëº‰∏çË¶ÅÈô™ÂêåÔºåÊâ∂Ë¥´Ëµ∞ËÆøÁõ¥Â•îÂü∫Â±ÇÁõ¥ÊèíÁé∞Âú∫„ÄÇÂ§©Ê¥•Â∏ÇÂßî‰∏•ËÇÉÈóÆË¥£Â∏ÇÁ§æ‰ºöÁªÑÁªáÁÆ°ÁêÜÂ±Ä‰∏ç‰Ωú‰∏∫ÊÖ¢‰Ωú‰∏∫ÈóÆÈ¢òÔºåÂ∏ÇÁ§æÁÆ°Â±ÄÂéüÂ±ÄÈïøÂº†ÂÆùÁî´Ë¢´ÂÖçËÅå„ÄÇ ""+
""5ÔºâÈôïË•øÊê≠Âª∫ÊñáÂåñ‰∫§ÊµÅÂπ≥Âè∞ÔºåÊé®Âä®‚ÄúÊñáÂåñÈôïË•ø‚ÄùÂìÅÁâåËµ∞Âêë‰∏ñÁïåÔºåÊ≤≥ÂçóÂçóÈò≥ÔºöËøûÁéØËÆ°È™óËÄÅ‰∫∫‚Äî‚ÄîÂ±ÇÂ±ÇËÆæÂ•óÔºåÈ™óËÄÅ‰∫∫Ë¥≠‰π∞‚ÄúÁâπÊïàËçØ‚Äù„ÄÇ ""+
""6Ôºâ5Êúà15Êó•20Êó∂Ëá≥16Êó•20Êó∂ÔºåÈªëÈæôÊ±ü„ÄÅËæΩÂÆÅ‰∏úÈÉ®„ÄÅÂçéÂåó‰∏úÈÉ®ÂíåÂçóÈÉ®„ÄÅÈªÑÊ∑ÆÂ§ßÈÉ®„ÄÅÊ±üÊ∑ÆÂåóÈÉ®Á≠âÂú∞Êúâ‰∏≠Âà∞Â§ßÈõ®ÔºåÈªëÈæôÊ±ü‰∏≠ÈÉ®„ÄÅÂ±±‰∏úË•øÂåóÈÉ®ÂíåË•øÂçóÈÉ®„ÄÅÊ≤≥Âçó‰∏úÈÉ®„ÄÅËãèÁöñÂåóÈÉ®Á≠âÂú∞ÁöÑÈÉ®ÂàÜÂú∞Âå∫ÊúâÊö¥Èõ®ÔºåÂ±ÄÂú∞Â§ßÊö¥Èõ® (100ÔΩû110ÊØ´Á±≥)Ôºå‰∏äËø∞Âú∞Âå∫Âπ∂‰º¥ÊúâÁü≠Êó∂Âº∫ÈôçÊ∞¥„ÄÅÈõ∑Êö¥Â§ßÈ£éÁ≠âÂº∫ÂØπÊµÅÂ§©Ê∞î„ÄÇ""+ 
""1ÔºâÁöÆËÇ§Âú®Êôö10~11ÁÇπËøõÂÖ•‰øùÂÖªÁä∂ÊÄÅÔºåÈïøÊó∂Èó¥ÁÜ¨Â§úÔºå‰∫∫ÁöÑÂÜÖÂàÜÊ≥åÂíåÁ•ûÁªèÁ≥ªÁªüÂ∞±‰ºöÂ§±Ë∞ÉÔºå‰ΩøÁöÆËÇ§Âπ≤Áá•„ÄÅÂºπÊÄßÂ∑Æ„ÄÅÊô¶ÊöóÊó†ÂÖâÔºåÂá∫Áé∞ÊöóÁñÆ„ÄÅÁ≤âÂà∫„ÄÅÈªëÊñëÁ≠âÈóÆÈ¢ò„ÄÇ ""+
""2ÔºâÊôÆÊ¥±Ëå∂Â§ö‰∏∫ÂéãÁ¥ßÂûãÔºåÁ¨¨‰∫åÊ≥°Êó∂Ëå∂Âè∂ÂæÄÂæÄÊ≤°ÊúâÂÆåÂÖ®Â±ïÂºÄÔºåÊâÄ‰ª•Á≤æÂçéË¶Å‰ªéÁ¨¨‰∏âÊ≥°ÂºÄÂßãÔºå‰∏ÄËà¨‰∏âÂà∞‰∫îÊ≥°ÊòØÊôÆÊ¥±Ëå∂Âè£ÊÑüÊúÄÂ•ΩÁöÑ"";
        
       // String co = ""Âú®ËÆ°ÁÆóÊú∫Èü≥ËßÜÈ¢ëÂíåÂõæÂΩ¢ÂõæÂÉèÊäÄÊúØÁ≠â‰∫åÁª¥‰ø°ÊÅØÁÆóÊ≥ïÂ§ÑÁêÜÊñπÈù¢ÁõÆÂâçÊØîËæÉÂÖàËøõÁöÑËßÜÈ¢ëÂ§ÑÁêÜÁÆóÊ≥ïÔºå‰ªäÂ§©ÊòØÊòüÊúüÂ§©ÊàëÂæàÂºÄÂøÉÔºåÂ∏åÊúõ‰∏ã‰∏™ÊòüÊúüÂ§©Êàë‰æùÁÑ∂ÂæàÂºÄÂøÉÔºÅ"";
        occurrence.addAll(co);//**bugÂá∫Áé∞Â§Ñ**
        occurrence.compute();

        Set<Map.Entry<String, TermFrequency>> uniGram = occurrence.getUniGram();
        for (Map.Entry<String, TermFrequency> entry : uniGram)
        {
            TermFrequency termFrequency = entry.getValue();
            System.out.println(termFrequency);
        }

       /* Set<Map.Entry<String, PairFrequency>> biGram = occurrence.getBiGram();
        for (Map.Entry<String, PairFrequency> entry : biGram)
        {
            PairFrequency pairFrequency = entry.getValue();
            if (pairFrequency.isRight())
                System.out.println(pairFrequency);
        }

        Set<Map.Entry<String, TriaFrequency>> triGram = occurrence.getTriGram();
        for (Map.Entry<String, TriaFrequency> entry : triGram)
        {
            TriaFrequency triaFrequency = entry.getValue();
            if (triaFrequency.isRight())
                System.out.println(triaFrequency);
        }*/
    }

![image](https://user-images.githubusercontent.com/22112317/42936929-25bf8680-8b80-11e8-90d0-dc939f62fb41.png)

## Â§çÁé∞ÈóÆÈ¢ò
ËßÅ‰ª£Á†Å
"
WordNetÁ±ªÁöÑinsertÊñπÊ≥ïÊä•Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®ÊâßË°åWordNetÁ±ªÁöÑinsertÊñπÊ≥ïÊä•Exception in thread ""main"" java.util.ConcurrentModificationException

## Â§çÁé∞ÈóÆÈ¢ò
ÊâßË°åTest‰∏≠ÁöÑDemoSentimentAnalysisÁöÑmainÊñπÊ≥ïÂç≥ÂèØÔºåÈáåÈù¢‰ºöË∞ÉÁî®WordNetÁöÑinsertÊñπÊ≥ïÁöÑ„ÄÇ

## ÂÖ∂‰ªñ
ÂÖ∂ÂÆûÈóÆÈ¢òÂ∑≤ÁªèËß£ÂÜ≥‰∫ÜÔºå‰ΩÜÊòØ‰øÆÊîπ‰∫ÜÊ∫êÁ†Å„ÄÇÈîôËØØÂéüÂõ†Â§ßÊ¶ÇÂ∞±ÊòØ‰Ω†‰∏ÄËæπËø≠‰ª£‰∏ÄËæπÂæÄlist‰∏≠Ê∑ªÂä†Êï∞ÊçÆ
"
Áü≠ËØ≠ÊèêÂèñ‰∏∫‰ªÄ‰πàË¶ÅÂéªÂÅúÁî®ËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

Áü≠ËØ≠ÊèêÂèñ‰∏∫‰ªÄ‰πàË¶ÅÂéªÂÅúÁî®ËØçÔºåÂ¶ÇÊûú‰∏çÂéªÂÅúÁî®ËØç

ËøôÊòØ‰∏ÄÁØáÊñáÁ´†Ôºö

Ê¨¢ËøéÊäïÁ®ø ÊñáÁ´†Êù•Ê∫ê ‰ΩúËÄÖ ÂØπÊñ∞ÊâãÊù•ËØ¥ Â≠¶‰π†Êú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÊòØÊØîËæÉÂõ∞ÈöæÁöÑ ÂêÑÁßçÊ∑±Â∫¶Â≠¶‰π†Â∫ì‰πüÊòØÊØîËæÉÈöæÁêÜËß£ ÊâÄ‰ª• ÊàëÂàõÂª∫‰∫ÜËøô‰∏™Êú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÈÄüÊü•Ë°® Â∏åÊúõÂØπÂ§öÂÆ∂ÊúâÂ∏ÆÂä© ÁÉ≠Èó®ÊñáÁ´†Êé®Ëçê È©¨ÂåñËÖæ È©¨‰∫ëÂíåÊùéÂΩ¶ÂÆèÈÉΩÈîô‰∫Ü Âú∫ÊôØÊØîÊï∞ÊçÆÂíåÊäÄÊúØÈÉΩÈáçË¶Å ÊúÄÊñ∞ Ë∞∑Ê≠åËë£‰∫ãÈïø ÊàëÂèØ‰ª•Áõ¥Êé•ÂëäËØâ‰Ω† ‰∫íËÅîÁΩëÂæàÂø´Ê∂àÂ§± ÊñØÂù¶Á¶è Âç∑ÁßØÁ•ûÁªèÁΩëÁªúËßÜËßâËØÜÂà´ËØæÁ®ãËÆ≤‰πâ Áã¨ÂÆ∂ Èù¢ÂØπ‰∫∫Â∑•Êô∫ËÉΩ ‰∏ã‰∏™ÂÉèÊüØÊ¥Å‰∏ÄÊ†∑Âì≠Ê≥£ÁöÑÂèØËÉΩÂ∞±ÊòØ‰Ω† ÊúÄÊñ∞ ÊâéÂÖã‰ºØÊ†º Â¶Ç‰ΩïÂú®Ë¢´ÊäõÂºÉÂíåË¢´Âê¶ÂÆö‰∏≠ÊàêÂ∞±Ëá™Â∑± ÈúáÊÉä Á¶ÅÊ≠¢‰∏≠ÂõΩ‰∫∫ÂèÇÂä† ÁöÑÁ¨¨‰∫åËΩÆÊØîËµõ ÈáçÁ£Ö Êè≠Áßò ÁâàÊú¨ÁöÑÊäÄÊúØËÆæËÆ°ÂíåÊ£ãËâ∫Ê∞¥Âπ≥ ÈáçÁ£Ö Ë∞ÅËÆ©Ëã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§± ‰∫øËøòÁïô‰∏ã‰∏ÄÈÅìÊÄùËÄÉÈ¢ò

Áü≠ËØ≠ÊèêÂèñÂêéÔºö
[Êú∫Âô®Â≠¶‰π†Ê∑±Â∫¶, Ê∑±Â∫¶Â≠¶‰π†, Âç∑ÁßØÁ•ûÁªèÁΩëÁªú, Â≠¶‰π†ÈÄüÊü•Ë°®, ÊùéÂΩ¶ÂÆèÈîô, ÊüØÊ¥ÅÂì≠Ê≥£, Á•ûÁªèÁΩëÁªúËßÜËßâ, È©¨‰∫ëÊùéÂΩ¶ÂÆè, ‰∏ÄÈÅìÊÄùËÄÉÈ¢ò, Ëã±‰ºüËææÊçüÂ§±]

Êàë‰∏çÁü•ÈÅìÁü≠ËØ≠ÊèêÂèñÁî®ÁöÑÂàÜËØçÊó∂Âì™ÁßçÊñπÊ≥ïÔºå‰ΩÜÊòØÊàëÁî®segementÂàÜËØçÂêéÁªìÊûú‰∏∫

[Ê¨¢Ëøé/v, ÊäïÁ®ø/vi, /w, ÊñáÁ´†/n, Êù•Ê∫ê/n, /w, ‰ΩúËÄÖ/nnt, /w, ÂØπ/p, Êñ∞Êâã/n, Êù•ËØ¥/uls, /w, Â≠¶‰π†/v, Êú∫Âô®Â≠¶‰π†/gi, Âíå/cc, Ê∑±Â∫¶/n, Â≠¶‰π†/v, ÊòØ/vshi, ÊØîËæÉ/d, Âõ∞Èöæ/an, ÁöÑ/ude1, /w, ÂêÑÁßç/rz, Ê∑±Â∫¶/n, Â≠¶‰π†/v, Â∫ì/n, ‰πü/d, ÊòØ/vshi, ÊØîËæÉ/d, Èöæ/a, ÁêÜËß£/v, /w, ÊâÄ‰ª•/c, /w, Êàë/rr, ÂàõÂª∫/v, ‰∫Ü/ule, Ëøô‰∏™/rz, Êú∫Âô®Â≠¶‰π†/gi, Âíå/cc, Ê∑±Â∫¶/n, Â≠¶‰π†/v, ÈÄüÊü•Ë°®/n, /w, Â∏åÊúõ/v, ÂØπ/p, Â§ö/a, ÂÆ∂/q, Êúâ/vyou, Â∏ÆÂä©/v, /w, ÁÉ≠Èó®/a, ÊñáÁ´†/n, Êé®Ëçê/v, /w, È©¨ÂåñËÖæ/nr, /w, È©¨‰∫ë/nr, Âíå/cc, ÊùéÂΩ¶ÂÆè/nr, ÈÉΩ/d, Èîô/v, ‰∫Ü/ule, /w, Âú∫ÊôØ/n, ÊØî/p, Êï∞ÊçÆ/n, Âíå/cc, ÊäÄÊúØ/n, ÈÉΩ/d, ÈáçË¶Å/a, /w, ÊúÄÊñ∞/a, /w, Ë∞∑Ê≠å/ntc, Ëë£‰∫ãÈïø/nnt, /w, Êàë/rr, ÂèØ‰ª•/v, Áõ¥Êé•/ad, ÂëäËØâ/v, ‰Ω†/rr, /w, ‰∫íËÅîÁΩë/n, ÂæàÂø´/d, Ê∂àÂ§±/vi, /w, ÊñØÂù¶Á¶è/nrf, /w, Âç∑ÁßØ/gm, Á•ûÁªèÁΩëÁªú/nz, ËßÜËßâ/n, ËØÜÂà´/vn, ËØæÁ®ã/n, ËÆ≤‰πâ/n, /w, Áã¨ÂÆ∂/d, /w, Èù¢ÂØπ/v, ‰∫∫Â∑•Êô∫ËÉΩ/n, /w, ‰∏ã/f, ‰∏™/q, ÂÉè/v, ÊüØÊ¥Å/nr, ‰∏ÄÊ†∑/uyy, Âì≠Ê≥£/vi, ÁöÑ/ude1, ÂèØËÉΩ/v, Â∞±ÊòØ/v, ‰Ω†/rr, /w, ÊúÄÊñ∞/a, /w, ÊâéÂÖã‰ºØÊ†º/nrf, /w, Â¶Ç‰Ωï/ryv, Âú®/p, Ë¢´/pbei, ÊäõÂºÉ/v, Âíå/cc, Ë¢´/pbei, Âê¶ÂÆö/v, ‰∏≠/f, ÊàêÂ∞±/n, Ëá™Â∑±/rr, /w, ÈúáÊÉä/v, /w, Á¶ÅÊ≠¢/v, ‰∏≠ÂõΩ/ns, ‰∫∫/n, ÂèÇÂä†/v, /w, ÁöÑ/ude1, Á¨¨‰∫å/mq, ËΩÆ/qv, ÊØîËµõ/vn, /w, ÈáçÁ£Ö/n, /w, Êè≠Áßò/v, /w, ÁâàÊú¨/n, ÁöÑ/ude1, ÊäÄÊúØ/n, ËÆæËÆ°/vn, Âíå/cc, Ê£ãËâ∫/n, Ê∞¥Âπ≥/n, /w, ÈáçÁ£Ö/n, /w, Ë∞Å/ry, ËÆ©/v, Ëã±‰ºüËææ/nz, ‰∏Ä/m, Â§ú/t, ÊçüÂ§±/n, /w, ‰∫ø/m, Ëøò/d, Áïô‰∏ã/v, ‰∏ÄÈÅì/d, ÊÄùËÄÉÈ¢ò/n]

‚ÄúËã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§±‚ÄùÔºå‚Äù‰∏Ä‚ÄùÊòØÂéªÂÅúÁî®ËØçÂéªÊéâ‰∫ÜÔºå‚ÄúÂ§ú‚Äù‰∏çÁü•ÈÅìÊÄé‰πàÂéªÊéâÁöÑ(Ëøô‰∏çÊòØÈáçÁÇπ)ÔºåÁÑ∂ÂêéÁü≠ËØ≠Â∞±ÊòØËã±‰ºüËææÊçüÂ§±ÔºåÂ¶ÇÊûúËØ¥‰∏çÂéªÂÅúÁî®ËØçÁöÑËØùÔºåÂÅáËÆæÁü≠ËØ≠ÊèêÂèñÂêé‰∏ç‰ºöÂá∫Áé∞‚ÄúËã±‰ºüËææ‰∏ÄÂ§ú‚ÄùÊàñËÄÖ‚Äú‰∏ÄÂ§úÊçüÂ§±‚ÄùÊàñËÄÖ‚ÄúËã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§±"",Âõ†‰∏∫ÊàëÊòØÂ∏åÊúõÁî®ÊèêÂèñÂá∫ÁöÑÁü≠ËØ≠ÂΩìÂÅöÊñáÁ´†ÁöÑÂÖ≥ÈîÆËØçÔºåÊâÄ‰ª•Âêé‰∏â‰∏™Áü≠ËØ≠Ê≠£Â•Ω‰πüÊòØÊàë‰∏çÊÉ≥Ë¶ÅÁöÑÔºåÔºà‰πãÂêéÊàë‰πüÊÉ≥Áî®Áü≠ËØ≠ÂΩìÂÅöÂ≠óÂÖ∏ÈáçÊñ∞ÂØπÊñáÁ´†ÂàÜËØçÔºåÁÑ∂ÂêéËøõË°åÂêéÈù¢ÁöÑÂÆûÈ™åÔºâ
"
Áü≠ËØ≠ÊèêÂèñ‰∏∫‰ªÄ‰πàË¶ÅÂéªÂÅúÁî®ËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

Áü≠ËØ≠ÊèêÂèñ‰∏∫‰ªÄ‰πàË¶ÅÂéªÂÅúÁî®ËØçÔºåÂ¶ÇÊûú‰∏çÂéªÂÅúÁî®ËØç

ËøôÊòØ‰∏ÄÁØáÊñáÁ´†Ôºö

Ê¨¢ËøéÊäïÁ®ø ÊñáÁ´†Êù•Ê∫ê ‰ΩúËÄÖ ÂØπÊñ∞ÊâãÊù•ËØ¥ Â≠¶‰π†Êú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÊòØÊØîËæÉÂõ∞ÈöæÁöÑ ÂêÑÁßçÊ∑±Â∫¶Â≠¶‰π†Â∫ì‰πüÊòØÊØîËæÉÈöæÁêÜËß£ ÊâÄ‰ª• ÊàëÂàõÂª∫‰∫ÜËøô‰∏™Êú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÈÄüÊü•Ë°® Â∏åÊúõÂØπÂ§öÂÆ∂ÊúâÂ∏ÆÂä© ÁÉ≠Èó®ÊñáÁ´†Êé®Ëçê È©¨ÂåñËÖæ È©¨‰∫ëÂíåÊùéÂΩ¶ÂÆèÈÉΩÈîô‰∫Ü Âú∫ÊôØÊØîÊï∞ÊçÆÂíåÊäÄÊúØÈÉΩÈáçË¶Å ÊúÄÊñ∞ Ë∞∑Ê≠åËë£‰∫ãÈïø ÊàëÂèØ‰ª•Áõ¥Êé•ÂëäËØâ‰Ω† ‰∫íËÅîÁΩëÂæàÂø´Ê∂àÂ§± ÊñØÂù¶Á¶è Âç∑ÁßØÁ•ûÁªèÁΩëÁªúËßÜËßâËØÜÂà´ËØæÁ®ãËÆ≤‰πâ Áã¨ÂÆ∂ Èù¢ÂØπ‰∫∫Â∑•Êô∫ËÉΩ ‰∏ã‰∏™ÂÉèÊüØÊ¥Å‰∏ÄÊ†∑Âì≠Ê≥£ÁöÑÂèØËÉΩÂ∞±ÊòØ‰Ω† ÊúÄÊñ∞ ÊâéÂÖã‰ºØÊ†º Â¶Ç‰ΩïÂú®Ë¢´ÊäõÂºÉÂíåË¢´Âê¶ÂÆö‰∏≠ÊàêÂ∞±Ëá™Â∑± ÈúáÊÉä Á¶ÅÊ≠¢‰∏≠ÂõΩ‰∫∫ÂèÇÂä† ÁöÑÁ¨¨‰∫åËΩÆÊØîËµõ ÈáçÁ£Ö Êè≠Áßò ÁâàÊú¨ÁöÑÊäÄÊúØËÆæËÆ°ÂíåÊ£ãËâ∫Ê∞¥Âπ≥ ÈáçÁ£Ö Ë∞ÅËÆ©Ëã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§± ‰∫øËøòÁïô‰∏ã‰∏ÄÈÅìÊÄùËÄÉÈ¢ò

Áü≠ËØ≠ÊèêÂèñÂêéÔºö
[Êú∫Âô®Â≠¶‰π†Ê∑±Â∫¶, Ê∑±Â∫¶Â≠¶‰π†, Âç∑ÁßØÁ•ûÁªèÁΩëÁªú, Â≠¶‰π†ÈÄüÊü•Ë°®, ÊùéÂΩ¶ÂÆèÈîô, ÊüØÊ¥ÅÂì≠Ê≥£, Á•ûÁªèÁΩëÁªúËßÜËßâ, È©¨‰∫ëÊùéÂΩ¶ÂÆè, ‰∏ÄÈÅìÊÄùËÄÉÈ¢ò, Ëã±‰ºüËææÊçüÂ§±]

Êàë‰∏çÁü•ÈÅìÁü≠ËØ≠ÊèêÂèñÁî®ÁöÑÂàÜËØçÊó∂Âì™ÁßçÊñπÊ≥ïÔºå‰ΩÜÊòØÊàëÁî®segementÂàÜËØçÂêéÁªìÊûú‰∏∫

[Ê¨¢Ëøé/v, ÊäïÁ®ø/vi, /w, ÊñáÁ´†/n, Êù•Ê∫ê/n, /w, ‰ΩúËÄÖ/nnt, /w, ÂØπ/p, Êñ∞Êâã/n, Êù•ËØ¥/uls, /w, Â≠¶‰π†/v, Êú∫Âô®Â≠¶‰π†/gi, Âíå/cc, Ê∑±Â∫¶/n, Â≠¶‰π†/v, ÊòØ/vshi, ÊØîËæÉ/d, Âõ∞Èöæ/an, ÁöÑ/ude1, /w, ÂêÑÁßç/rz, Ê∑±Â∫¶/n, Â≠¶‰π†/v, Â∫ì/n, ‰πü/d, ÊòØ/vshi, ÊØîËæÉ/d, Èöæ/a, ÁêÜËß£/v, /w, ÊâÄ‰ª•/c, /w, Êàë/rr, ÂàõÂª∫/v, ‰∫Ü/ule, Ëøô‰∏™/rz, Êú∫Âô®Â≠¶‰π†/gi, Âíå/cc, Ê∑±Â∫¶/n, Â≠¶‰π†/v, ÈÄüÊü•Ë°®/n, /w, Â∏åÊúõ/v, ÂØπ/p, Â§ö/a, ÂÆ∂/q, Êúâ/vyou, Â∏ÆÂä©/v, /w, ÁÉ≠Èó®/a, ÊñáÁ´†/n, Êé®Ëçê/v, /w, È©¨ÂåñËÖæ/nr, /w, È©¨‰∫ë/nr, Âíå/cc, ÊùéÂΩ¶ÂÆè/nr, ÈÉΩ/d, Èîô/v, ‰∫Ü/ule, /w, Âú∫ÊôØ/n, ÊØî/p, Êï∞ÊçÆ/n, Âíå/cc, ÊäÄÊúØ/n, ÈÉΩ/d, ÈáçË¶Å/a, /w, ÊúÄÊñ∞/a, /w, Ë∞∑Ê≠å/ntc, Ëë£‰∫ãÈïø/nnt, /w, Êàë/rr, ÂèØ‰ª•/v, Áõ¥Êé•/ad, ÂëäËØâ/v, ‰Ω†/rr, /w, ‰∫íËÅîÁΩë/n, ÂæàÂø´/d, Ê∂àÂ§±/vi, /w, ÊñØÂù¶Á¶è/nrf, /w, Âç∑ÁßØ/gm, Á•ûÁªèÁΩëÁªú/nz, ËßÜËßâ/n, ËØÜÂà´/vn, ËØæÁ®ã/n, ËÆ≤‰πâ/n, /w, Áã¨ÂÆ∂/d, /w, Èù¢ÂØπ/v, ‰∫∫Â∑•Êô∫ËÉΩ/n, /w, ‰∏ã/f, ‰∏™/q, ÂÉè/v, ÊüØÊ¥Å/nr, ‰∏ÄÊ†∑/uyy, Âì≠Ê≥£/vi, ÁöÑ/ude1, ÂèØËÉΩ/v, Â∞±ÊòØ/v, ‰Ω†/rr, /w, ÊúÄÊñ∞/a, /w, ÊâéÂÖã‰ºØÊ†º/nrf, /w, Â¶Ç‰Ωï/ryv, Âú®/p, Ë¢´/pbei, ÊäõÂºÉ/v, Âíå/cc, Ë¢´/pbei, Âê¶ÂÆö/v, ‰∏≠/f, ÊàêÂ∞±/n, Ëá™Â∑±/rr, /w, ÈúáÊÉä/v, /w, Á¶ÅÊ≠¢/v, ‰∏≠ÂõΩ/ns, ‰∫∫/n, ÂèÇÂä†/v, /w, ÁöÑ/ude1, Á¨¨‰∫å/mq, ËΩÆ/qv, ÊØîËµõ/vn, /w, ÈáçÁ£Ö/n, /w, Êè≠Áßò/v, /w, ÁâàÊú¨/n, ÁöÑ/ude1, ÊäÄÊúØ/n, ËÆæËÆ°/vn, Âíå/cc, Ê£ãËâ∫/n, Ê∞¥Âπ≥/n, /w, ÈáçÁ£Ö/n, /w, Ë∞Å/ry, ËÆ©/v, Ëã±‰ºüËææ/nz, ‰∏Ä/m, Â§ú/t, ÊçüÂ§±/n, /w, ‰∫ø/m, Ëøò/d, Áïô‰∏ã/v, ‰∏ÄÈÅì/d, ÊÄùËÄÉÈ¢ò/n]

‚ÄúËã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§±‚ÄùÔºå‚Äù‰∏Ä‚ÄùÊòØÂéªÂÅúÁî®ËØçÂéªÊéâ‰∫ÜÔºå‚ÄúÂ§ú‚Äù‰∏çÁü•ÈÅìÊÄé‰πàÂéªÊéâÁöÑ(Ëøô‰∏çÊòØÈáçÁÇπ)ÔºåÁÑ∂ÂêéÁü≠ËØ≠Â∞±ÊòØËã±‰ºüËææÊçüÂ§±ÔºåÂ¶ÇÊûúËØ¥‰∏çÂéªÂÅúÁî®ËØçÁöÑËØùÔºåÂÅáËÆæÁü≠ËØ≠ÊèêÂèñÂêé‰∏ç‰ºöÂá∫Áé∞‚ÄúËã±‰ºüËææ‰∏ÄÂ§ú‚ÄùÊàñËÄÖ‚Äú‰∏ÄÂ§úÊçüÂ§±‚ÄùÊàñËÄÖ‚ÄúËã±‰ºüËææ‰∏ÄÂ§úÊçüÂ§±"",Âõ†‰∏∫ÊàëÊòØÂ∏åÊúõÁî®ÊèêÂèñÂá∫ÁöÑÁü≠ËØ≠ÂΩìÂÅöÊñáÁ´†ÁöÑÂÖ≥ÈîÆËØçÔºåÊâÄ‰ª•Âêé‰∏â‰∏™Áü≠ËØ≠Ê≠£Â•Ω‰πüÊòØÊàë‰∏çÊÉ≥Ë¶ÅÁöÑÔºåÔºà‰πãÂêéÊàë‰πüÊÉ≥Áî®Áü≠ËØ≠ÂΩìÂÅöÂ≠óÂÖ∏ÈáçÊñ∞ÂØπÊñáÁ´†ÂàÜËØçÔºåÁÑ∂ÂêéËøõË°åÂêéÈù¢ÁöÑÂÆûÈ™åÔºâ

"
ÂÖ≥‰∫éËÉΩÂê¶ÂÆ¢ËßÇÁªüËÆ°Âá∫ÊñáÁ´†‰∏≠ÂêÑËØçËØ≠ÁöÑÂá∫Áé∞ÁöÑÊ¨°Êï∞,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.6.6
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6


## ÊàëÁöÑÈóÆÈ¢ò
Â¶Ç‰ΩïÂÆ¢ËßÇÁªüËÆ°Âá∫ÊñáÁ´†‰∏≠ÂêÑËØçËØ≠ÁöÑÂá∫Áé∞ÁöÑÊ¨°Êï∞„ÄÇ
‰æãÂ¶ÇÔºö‰ªäÂ§©ÊòØÊòüÊúüÂ§©ÊàëÁã†ÂºÄÂøÉÔºåÂ∏åÊúõ‰∏ã‰∏™ÊòüÊúüÂ§©Êàë‰πü‰æùÁÑ∂ÂºÄÂøÉ„ÄÇ
‰ª•‰∏äÔºöÊòüÊúüÂ§©Ôºö2Ê¨°ÔºõÂºÄÂøÉÔºö2Ê¨°Ôºõ‰ªäÂ§©Ôºö1Ê¨° Á≠â...

## Â§çÁé∞ÈóÆÈ¢ò
Êó†

### Ê≠•È™§
Êó†

### Ëß¶Âèë‰ª£Á†Å
Êó†
### ÊúüÊúõËæìÂá∫
Êó†
### ÂÆûÈôÖËæìÂá∫

Êó†

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Êó†


"
Âú®Áü≠ËØ≠ÊèêÂèñÊó∂ÔºåÊòØÂê¶Â∑≤ÁªèËá™Âä®ËøáÊª§ÂÅúÁî®ËØçÔºåÂ¶ÇÊûúÊòØËá™Âä®ËøáÊª§ÔºåÈÇ£ÊàëÂú®stopwords.txtÊ∑ªÂä†Êñ∞ËØçÊó∂Âç¥Ê≤°ÊúâËøáÊª§Ôºõ Â¶ÇÊûúÊòØÊâãÂä®ËøáÊª§ÔºåÊàëÂ∫îËØ•ÊÄé‰πàË∞ÉÁî®„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

Âú®Áü≠ËØ≠ÊèêÂèñÊó∂ÔºåÊòØÂê¶Â∑≤ÁªèËá™Âä®ËøáÊª§ÂÅúÁî®ËØçÔºåÂ¶ÇÊûúÊòØËá™Âä®ËøáÊª§ÔºåÈÇ£ÊàëÂú®stopwords.txtÊ∑ªÂä†Êñ∞ËØçÊó∂Âç¥Ê≤°ÊúâËøáÊª§Ôºõ
Â¶ÇÊûúÊòØÊâãÂä®ËøáÊª§ÔºåÊàëÂ∫îËØ•ÊÄé‰πàË∞ÉÁî®„ÄÇ


"
ËØÜÂà´Á±ª‰ºº‚ÄúÁéãÂØíÁÇÆËΩ∞Âº†ÂÜ∞ÂÜ∞‚ÄùÁªìÊûÑÁöÑÂè•Â≠êÊó∂ÁªìÊûú‰∏éÈ¢ÑÊúü‰∏çÂêå„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®ÂØπ‚ÄúÁéãÂØíÁÇÆËΩ∞Âº†ÂÜ∞ÂÜ∞‚ÄùÁöÑÁ±ª‰ººÁöÑÂè•Â≠êÂàÜËØçÊó∂ÔºånlpÔºåÊÑüÁü•Êú∫ÂíåcrfÈÉΩ‰ºöÂá∫Áé∞‰∏çÂêåÁ®ãÂ∫¶ÁöÑËØØÂ∑ÆÁé∞Ë±°„ÄÇ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÂàÜÂà´ÂØπ‚ÄúÁéãÂØíÁÇÆËΩ∞Âº†ÂÜ∞ÂÜ∞‚ÄùËøõË°ånlp,ÊÑüÁü•Êú∫ÂàÜËØçÂíåcrfÂàÜËØç
2.Êü•Áúã‰∏âÁßçÊñπÂºèÁöÑÂàÜËØçÁªìÊûú


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
       String testContent = ""ÁéãÂØíÁÇÆËΩ∞Âº†ÂÜ∞ÂÜ∞"";
       //‰∏âÁßçÂàÜËØçÁöÑÊñπÊ≥ï‰∏∫Â∏∏ËßÑ‰ª£Á†ÅÔºåÊöÇÁúÅÁï•
        nlpTokenizerTest(testContent);
        perceptronTest(testContent);
        crfTest(testContent);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
ÁéãÂØí/nrÔºåÁÇÆËΩ∞/vÔºåÂº†ÂÜ∞ÂÜ∞/nr
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
nlpÂàÜËØçÁªìÊûúÔºö[ÁéãÂØíÁÇÆËΩ∞/nr, Âº†ÂÜ∞ÂÜ∞/nr]
ÊÑüÁü•Êú∫ÂàÜËØçÁªìÊûúÔºöÁéãÂØíÁÇÆËΩ∞Âº†ÂÜ∞ÂÜ∞/nr
crfÂàÜËØçÁªìÊûúÔºö[ÁéãÂØíÁÇÆ/nr, ËΩ∞Âº†ÂÜ∞ÂÜ∞/nr]
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![qq 20180711153707](https://user-images.githubusercontent.com/18030444/42557212-724bfa4e-8520-11e8-955d-8bd29747e7e1.png)
"
ÂàÜÂè∑ËØçÊÄßÊ†áÊ≥®‰∏çÂáÜÁ°ÆÔºåÊúüÂæÖÊé®Âá∫ËØ≠‰πâËßíËâ≤ÂàÜÊûêÂíåÊåá‰ª£Ê∂àÂ≤ê„ÄÅÂàùÊ≠•ÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊäΩÂèñÁ≠âÂäüËÉΩ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöÂèëË°åÁâà 1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöÂèëË°åÁâà 1.6.6

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

ÂàÜÂè∑ËØçÊÄßÊ†áÊ≥®‰∏çÂáÜÁ°ÆÔºåÊúüÂæÖÊé®Âá∫ËØ≠‰πâËßíËâ≤ÂíåÊåá‰ª£Ê∂àÂ≤êÁ≠âÂäüËÉΩ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

ËæìÂÖ•‰ª•‰∏ãÂè•Â≠êËøõË°åËØçÊÄßÊ†áÊ≥®Ôºö
‚ÄúËÄåÂèòÈù©ËÄÖÂàôÊèêÂá∫Ëß£ÈáäÁêÜËß£‰∏ñÁïåÁöÑÊñ∞ÊñπÊ≥ïÔºöÂáØÊÅ©ÊñØÂú®ÂØπÊñØÂØÜËøõË°å‰øÆË°•ÔºõÂºóÊ¥õ‰ºäÂæ∑Âè¶ËæüËπäÂæÑÔºõÊØïÂä†Á¥¢ÊåëÊàòÈ©¨ËíÇÊñØÔºõÁà±Âõ†ÊñØÂù¶‰øÆËÆ¢ÁâõÈ°ø‰∏∫Â§ßËá™ÁÑ∂ÁöÑÁ´ãÊ≥ïÔºõÂæ∑È≤ÅÂÖãÂØπÁªÑÁªáÁöÑÁ†îÁ©∂ÔºåÂíå‰ªñÊèêÂá∫ÁöÑ‚ÄúÁü•ËØÜÂ∑•‰∫∫‚Äù‰∏éÂèóÈõáÈò∂Â±Ç„ÄÇ‚Äù

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
       CoNLLSentence sentence =  HanLP.parseDependency(sentence0);

        CoNLLWord[] wordArray = sentence.getWordArray();
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

Ê≠£Á°ÆÁöÑËØçÊÄßÊ†áÊ≥®

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
1	ËÄå	ËÄå	c	c	_	4	Áä∂‰∏≠ÁªìÊûÑ	_	_
2	ÂèòÈù©ËÄÖ	ÂèòÈù©ËÄÖ	n	n	_	4	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
3	Âàô	Âàô	d	d	_	4	Áä∂‰∏≠ÁªìÊûÑ	_	_
4	ÊèêÂá∫	ÊèêÂá∫	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
5	Ëß£Èáä	Ëß£Èáä	v	v	_	10	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
6	ÁêÜËß£	ÁêÜËß£	v	v	_	10	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
7	‰∏ñÁïå	‰∏ñÁïå	n	n	_	6	Âä®ÂÆæÂÖ≥Á≥ª	_	_
8	ÁöÑ	ÁöÑ	u	u	_	6	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
9	Êñ∞	Êñ∞	a	a	_	10	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
10	ÊñπÊ≥ï	ÊñπÊ≥ï	n	n	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
11	ÔºöÂáØ	ÔºöÂáØ	nh	nr	_	12	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
12	ÊÅ©ÊñØ	ÊÅ©ÊñØ	nh	nr	_	21	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
13	Âú®	Âú®	p	p	_	21	Áä∂‰∏≠ÁªìÊûÑ	_	_
14	ÂØπ	ÂØπ	p	p	_	16	Áä∂‰∏≠ÁªìÊûÑ	_	_
15	ÊñØÂØÜ	ÊñØÂØÜ	j	j	_	14	‰ªãÂÆæÂÖ≥Á≥ª	_	_
16	ËøõË°å	ËøõË°å	v	v	_	18	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
17	‰øÆË°•	‰øÆË°•	v	vn	_	16	Âä®ÂÆæÂÖ≥Á≥ª	_	_
18	Ôºõ	Ôºõ	n	n	_	19	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
19	ÂºóÊ¥õ‰ºäÂæ∑	ÂºóÊ¥õ‰ºäÂæ∑	nh	nr	_	13	‰ªãÂÆæÂÖ≥Á≥ª	_	_
20	Âè¶	Âè¶	d	d	_	21	Áä∂‰∏≠ÁªìÊûÑ	_	_
21	Ëæü	Ëæü	v	v	_	23	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
22	ËπäÂæÑ	ËπäÂæÑ	n	n	_	21	Âä®ÂÆæÂÖ≥Á≥ª	_	_
23	Ôºõ	Ôºõ	v	v	_	25	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
24	ÊØïÂä†Á¥¢	ÊØïÂä†Á¥¢	Vg	Vg	_	25	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
25	ÊåëÊàò	ÊåëÊàò	v	v	_	4	Âä®ÂÆæÂÖ≥Á≥ª	_	_
26	È©¨ËíÇÊñØ	È©¨ËíÇÊñØ	nh	nr	_	27	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
27	ÔºõÁà±Âõ†ÊñØÂù¶	ÔºõÁà±Âõ†ÊñØÂù¶	ns	ns	_	28	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
28	‰øÆËÆ¢	‰øÆËÆ¢	v	v	_	25	Âä®ÂÆæÂÖ≥Á≥ª	_	_
29	ÁâõÈ°ø	ÁâõÈ°ø	nh	nr	_	30	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
30	‰∏∫	‰∏∫	v	v	_	34	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
31	Â§ßËá™ÁÑ∂	Â§ßËá™ÁÑ∂	n	n	_	30	Âä®ÂÆæÂÖ≥Á≥ª	_	_
32	ÁöÑ	ÁöÑ	u	u	_	30	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
33	Á´ãÊ≥ï	Á´ãÊ≥ï	v	vn	_	34	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
34	ÔºõÂæ∑È≤ÅÂÖã	ÔºõÂæ∑È≤ÅÂÖã	n	n	_	38	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
35	ÂØπ	ÂØπ	p	p	_	38	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
36	ÁªÑÁªá	ÁªÑÁªá	v	v	_	38	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
37	ÁöÑ	ÁöÑ	u	u	_	36	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
38	Á†îÁ©∂	Á†îÁ©∂	v	vn	_	28	Âä®ÂÆæÂÖ≥Á≥ª	_	_
39	Ôºå	Ôºå	wp	w	_	28	Ê†áÁÇπÁ¨¶Âè∑	_	_
40	Âíå	Âíå	c	c	_	42	Â∑¶ÈôÑÂä†ÂÖ≥Á≥ª	_	_
41	‰ªñ	‰ªñ	r	r	_	42	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
42	ÊèêÂá∫	ÊèêÂá∫	v	v	_	46	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
43	ÁöÑ	ÁöÑ	u	u	_	42	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
44	‚Äú	‚Äú	wp	w	_	46	Ê†áÁÇπÁ¨¶Âè∑	_	_
45	Áü•ËØÜ	Áü•ËØÜ	n	n	_	46	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
46	Â∑•‰∫∫	Â∑•‰∫∫	n	n	_	28	Âπ∂ÂàóÂÖ≥Á≥ª	_	_
47	‚Äù	‚Äù	wp	w	_	46	Ê†áÁÇπÁ¨¶Âè∑	_	_
48	‰∏é	‰∏é	c	c	_	49	Â∑¶ÈôÑÂä†ÂÖ≥Á≥ª	_	_
49	ÂèóÈõá	ÂèóÈõá	v	v	_	46	Âπ∂ÂàóÂÖ≥Á≥ª	_	_
50	Èò∂Â±Ç	Èò∂Â±Ç	n	n	_	49	Âä®ÂÆæÂÖ≥Á≥ª	_	_
51	„ÄÇ	„ÄÇ	wp	w	_	4	Ê†áÁÇπÁ¨¶Âè∑	_	_

ËÄå --(Áä∂‰∏≠ÁªìÊûÑ)--> ÊèêÂá∫
ÂèòÈù©ËÄÖ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊèêÂá∫
Âàô --(Áä∂‰∏≠ÁªìÊûÑ)--> ÊèêÂá∫
ÊèêÂá∫ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##
Ëß£Èáä --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊñπÊ≥ï
ÁêÜËß£ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊñπÊ≥ï
‰∏ñÁïå --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÁêÜËß£
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ÁêÜËß£
Êñ∞ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊñπÊ≥ï
ÊñπÊ≥ï --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÔºöÂáØ
ÔºöÂáØ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊÅ©ÊñØ
ÊÅ©ÊñØ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> Ëæü
Âú® --(Áä∂‰∏≠ÁªìÊûÑ)--> Ëæü
ÂØπ --(Áä∂‰∏≠ÁªìÊûÑ)--> ËøõË°å
ÊñØÂØÜ --(‰ªãÂÆæÂÖ≥Á≥ª)--> ÂØπ
ËøõË°å --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Ôºõ
‰øÆË°• --(Âä®ÂÆæÂÖ≥Á≥ª)--> ËøõË°å
Ôºõ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÂºóÊ¥õ‰ºäÂæ∑
ÂºóÊ¥õ‰ºäÂæ∑ --(‰ªãÂÆæÂÖ≥Á≥ª)--> Âú®
Âè¶ --(Áä∂‰∏≠ÁªìÊûÑ)--> Ëæü
Ëæü --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Ôºõ
ËπäÂæÑ --(Âä®ÂÆæÂÖ≥Á≥ª)--> Ëæü
Ôºõ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊåëÊàò
ÊØïÂä†Á¥¢ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊåëÊàò
ÊåëÊàò --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊèêÂá∫
È©¨ËíÇÊñØ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÔºõÁà±Âõ†ÊñØÂù¶
ÔºõÁà±Âõ†ÊñØÂù¶ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ‰øÆËÆ¢
‰øÆËÆ¢ --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊåëÊàò
ÁâõÈ°ø --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ‰∏∫
‰∏∫ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÔºõÂæ∑È≤ÅÂÖã
Â§ßËá™ÁÑ∂ --(Âä®ÂÆæÂÖ≥Á≥ª)--> ‰∏∫
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ‰∏∫
Á´ãÊ≥ï --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÔºõÂæ∑È≤ÅÂÖã
ÔºõÂæ∑È≤ÅÂÖã --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Á†îÁ©∂
ÂØπ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Á†îÁ©∂
ÁªÑÁªá --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Á†îÁ©∂
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ÁªÑÁªá
Á†îÁ©∂ --(Âä®ÂÆæÂÖ≥Á≥ª)--> ‰øÆËÆ¢
Ôºå --(Ê†áÁÇπÁ¨¶Âè∑)--> ‰øÆËÆ¢
Âíå --(Â∑¶ÈôÑÂä†ÂÖ≥Á≥ª)--> ÊèêÂá∫
‰ªñ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊèêÂá∫
ÊèêÂá∫ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Â∑•‰∫∫
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ÊèêÂá∫
‚Äú --(Ê†áÁÇπÁ¨¶Âè∑)--> Â∑•‰∫∫
Áü•ËØÜ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Â∑•‰∫∫
Â∑•‰∫∫ --(Âπ∂ÂàóÂÖ≥Á≥ª)--> ‰øÆËÆ¢
‚Äù --(Ê†áÁÇπÁ¨¶Âè∑)--> Â∑•‰∫∫
‰∏é --(Â∑¶ÈôÑÂä†ÂÖ≥Á≥ª)--> ÂèóÈõá
ÂèóÈõá --(Âπ∂ÂàóÂÖ≥Á≥ª)--> Â∑•‰∫∫
Èò∂Â±Ç --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÂèóÈõá
„ÄÇ --(Ê†áÁÇπÁ¨¶Âè∑)--> ÊèêÂá∫

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Ë∞¢Ë∞¢ÔºÅÔºÅ"
ÂÖ≥‰∫é‰∫∫ÂêçUVÊãÜÂàÜ‰ºòÂåñÂª∫ËÆÆÔºàÊåâÊ†ºÂºèÈáçÊèêÔºâ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰æã1ÔºöÈæöÂ≠¶Âπ≥Á≠âË°®Á§∫‰ºö‰øùËØÅÈáë‰∫ëÈπèÁöÑÂÆâÂÖ®
‰æã2ÔºöÁéã‰∏≠ÂÜõ‰ª£Ë°®ËìùÈòüÂèëË®Ä
Ê≠§‰∏§ÁßçÊÉÖÂÜµÔºå‰∫∫ÂêçËØÜÂà´ÈîôËØØ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

PersonDictionaryÁöÑparsePatternÂáΩÊï∞ÔºåÂú®ËøõË°åÊãÜÂàÜÊó∂ÔºåÈÅóÊºèËØçËØ≠„ÄÇ
Â¶Ç‰æã1 ÁöÑËØ≠Âè•Ôºå""‰øùËØÅÈáë""ÔºåÂΩìËØÜÂà´Âá∫‰∫∫ÂêçÈáë‰∫ë„ÄÅÈáë‰∫ëÈπèÊó∂ÔºåÈúÄË¶ÅÂ∞Ü‚Äú‰øùËØÅ‚ÄùÊ∑ªÂä†Âà∞ÁªÜÂàÜËØçÂõæ‰∏≠„ÄÇÂêåÁêÜ‰æã2‰∏≠ÁöÑ‚ÄúÂÜõ‰ª£Ë°®‚Äù‰πüÈúÄË¶ÅÂÅöÁõ∏Â∫îÂ§ÑÁêÜ„ÄÇ

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰æã1ÔºöÈæöÂ≠¶Âπ≥Á≠âË°®Á§∫‰ºö‰øùËØÅÈáë‰∫ëÈπèÁöÑÂÆâÂÖ®
[ÈæöÂ≠¶Âπ≥/nr, Á≠â/udeng, Ë°®Á§∫/v, ‰ºö/v, ‰øùËØÅ/v, Èáë‰∫ëÈπè/nr, ÁöÑ/ude1, ÂÆâÂÖ®/an]

‰æã2ÔºöÁéã‰∏≠ÂÜõ‰ª£Ë°®ËìùÈòüÂèëË®ÄÔºåÂàÜËØçÁªìÊûú‰∏∫
[Áéã‰∏≠ÂÜõ/nr, ‰ª£Ë°®/nnt, ËìùÈòü/n, ÂèëË®Ä/vi]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÈæöÂ≠¶Âπ≥/nr, Á≠â/udeng, Ë°®Á§∫/v, ‰ºö/v, ‰øùËØÅÈáë/n, ‰∫ë/vg, Èπè/ng, ÁöÑ/ude1, ÂÆâÂÖ®/an]
[Áéã‰∏≠/nr, ÂÜõ‰ª£Ë°®/nnt, ËìùÈòü/n, ÂèëË®Ä/vi]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊúÄÈïøÂåπÈÖç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.6
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
HanlpÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÂäüËÉΩÔºåÂΩìËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑËØçËæÉÂ§öÊó∂ÔºåÂèØËÉΩ‰ºöÂá∫Áé∞ËæÉÈïøÂ≠óÁ¨¶ÂåÖÂê´ËæÉÁü≠Â≠óÁ¨¶ÁöÑÊÉÖÂÜµÔºåÊØîÂ¶ÇËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÂèØËÉΩÂêåÊó∂Âê´Êúâ‚ÄúÂ¢û‚Äù‰ª•Âèä‚ÄúÊÄ•Â¢û""‰∏§‰∏™ËØçÊù°ÔºåÈÇ£‰πàÂú®ÂØπ‚ÄúÁõàÂà©ÊÄ•Â¢û25‰∫øÂÖÉ‚ÄùËøõË°åÂàÜËØçÊó∂Ôºå‰ºöÂàÜÂá∫‚ÄúÁõàÂà©/vnÔºåÊÄ•/vÔºåÂ¢û/vÔºå25/mÔºå‰∫øÂÖÉ/q‚ÄùÁöÑÁªìÊûúÔºåÊàëÊÉ≥ËøôÂèØËÉΩÂíåTrieÊ†ë‰ª•ÂèäËá™Âä®Êú∫ÁöÑÁÆóÊ≥ïÊúâÂÖ≥ÔºåÁé∞Âú®ÊàëÊÉ≥ÂÆûÁé∞Ëá™ÂÆö‰πâËØçÊù°ÁöÑÊúÄÂ§ßÂåπÈÖçÔºåÊàëÁúãÂà∞CustomDictionaryÁ±ª‰∏≠ÊúâparseLongestTextÁöÑÊñπÊ≥ïÔºå‰∏çËøá‰∏çÁü•ÈÅìÂÖ∂ÂèÇÊï∞ÁöÑÂê´‰πâÂíåÁî®Ê≥ïÔºå‰∏çÁü•ÈÅìËøô‰∏™ÊñπÊ≥ïËÉΩÂê¶ÂÆûÁé∞Ëá™ÂÆö‰πâËØçÂÖ∏ÊúÄÂ§ßÂåπÈÖçÁöÑÂäüËÉΩ„ÄÇÊàñËÄÖHanlpÁöÑÂÜÖÈÉ®ÊúâÊ≤°ÊúâÂÆûÁé∞Á±ª‰ººÂäüËÉΩÁöÑÊé•Âè£Âë¢ÔºüÂ¶ÇÊûúÊ≤°ÊúâÁöÑËØùËÉΩÂê¶ÊåáÁÇπ‰∏ãÂÆûÁé∞ÂàÜËØçÂäüËÉΩÁöÑÁõ∏ÂÖ≥‰ª£Á†ÅÁöÑ‰ΩçÁΩÆÔºüË∞¢Ë∞¢
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
### Ê≠•È™§

1. È¶ñÂÖàÂú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Ê∑ªÂä†‚ÄúÂ¢û‚ÄùÂíå‚ÄúÊÄ•Â¢û‚Äù‰∏§‰∏™ËØçÊù°
2. ÁÑ∂ÂêéÂà†Èô§ÂéüÊù•ÁöÑbinÊñá‰ª∂ÔºåÈáçÊñ∞ËÆ≠ÁªÉÁºìÂ≠òÊñá‰ª∂
3. Êé•ÁùÄÂà©Áî®NÊúÄÁü≠Ë∑ØÂàÜËØçÔºåÂºÄÂêØ‰∫ÜËá™ÂÆö‰πâÂàÜËØçÔºåÂú∞ÂêçËØÜÂà´ÔºåÊú∫ÊûÑËØÜÂà´ÔºåÂπ∂ÂºÄÂêØ‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏Âº∫Âà∂ÂåπÈÖç
4.ÂØπ‚ÄúÁõàÂà©ÊÄ•Â¢û25‰∫øÂÖÉ‚ÄùËøõË°åÂàÜËØçÔºå‰ºöÂàÜÂá∫‚ÄúÁõàÂà©/vnÔºåÊÄ•/vÔºåÂ¢û/vÔºå25/mÔºå‰∫øÂÖÉ/q‚ÄùÁöÑÁªìÊûú

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
           String str = ""ÁõàÂà©ÊÄ•Â¢û25‰∫øÂÖÉ"";          
           Segment nShortSegment = new 
           NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true).enableOffset(true);
           nShortSegment.enableCustomDictionaryForcing(true);
           List<Term> termList = nShortSegment.seg(str);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
ÁõàÂà© ÊÄ•Â¢û 25 ‰∫øÂÖÉ
### ÂÆûÈôÖËæìÂá∫
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
ÁõàÂà© ÊÄ• Â¢û 25 ‰∫øÂÖÉ
Ê≤°ÊúâÂÆûÁé∞Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊúÄÂ§ßÂåπÈÖç

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÜíÂè∑ÁöÑËØÜÂà´Ë≤å‰ºº‰∏çÂ§™Á≤æÂáÜÔºåÂè¶Â§ñÂáØÊÅ©ÊñØ‰πüË¢´ËØÜÂà´‰∏∫ÂáØ+ÊÅ©ÊñØ‰∫Ü,"![b ab6_sl08h1mm kn9 vfz4](https://user-images.githubusercontent.com/32017215/42416259-6c3c8f82-829c-11e8-98e5-3076ddc4a0cd.png)
"
Â∞ÜÁâπÊÆäÁ¨¶Âè∑ÂíåÊ†áÁÇπÁ¨¶Âè∑‰Ωú‰∏∫ÂÅúÁî®ËØçËøáÊª§,"‰∏çÊÉ≥Ë¶ÅÊ†áÁÇπÁ¨¶Âè∑ÂíåÁâπÊÆäÁ¨¶Âè∑ÔºåÂ•ΩÂÉèÊ≤°ÊúâËøô‰∏ÄÈÖçÁΩÆÔºü
Â¶ÇÊûúÊ≤°ÊúâÔºåÊòØÂê¶ËÉΩÂ¢ûÂä†Ëøô‰∏ÄfeatureÔºü"
"HanLP.Config.Normalization = true Êó∂Ôºå‰∏≠ÊñáÁöÑ‚Äú„ÄÅ‚ÄùÈ°øÂè∑‰∏é‚ÄúÔºå‚ÄùÈÄóÂè∑ÈÉΩÂàáÂàÜÊàê‰∫Ü‚Äú,‚Äù","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4 ÈùûportableÁâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂΩì HanLP.Config.Normalization = true Êó∂ÔºåÈªòËÆ§ÂàÜËØçÂ∞Ü‰∏≠ÊñáÁöÑ‚Äú„ÄÅ‚ÄùÈ°øÂè∑‰∏é‚ÄúÔºå‚ÄùÈÄóÂè∑ÂàáÂàÜÁªìÊûúÁõ∏Âêå„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   HanLP.Config.Normalization = true;
   System.out.println(HanLP.segment(""‰∏∫‰ªÄ‰πàÔºåÂè∑‰∏é„ÄÅÂè∑ÁªìÊûú‰∏ÄÊ†∑Ôºü""));
   // ËæìÂá∫‰∏∫Ôºö[‰∏∫‰ªÄ‰πà/ryv, ,/w, Âè∑/q, ‰∏é/cc, ,/w, Âè∑/q, ÁªìÊûú/n, ‰∏ÄÊ†∑/uyy, ?/w]
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
ÂÖ≥‰∫éÂÆû‰ΩìËØÜÂà´ÊñπÈù¢ÁöÑÊï∞ÊçÆÈõÜ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4

Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÑüË∞¢ÂàÜ‰∫´ÔºÅ
ËØ∑ÈóÆÊÇ®‰ΩøÁî®ÁöÑÂÆû‰ΩìËØÜÂà´ÂíåËßíËâ≤ËØ≠‰πâÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÊòØÂì™‰∫õ ‰∏™‰∫∫Á†îÁ©∂ËÄÖËã¶‰∫éÊ≤°ÊúâÊï∞ÊçÆÂèØÁî®„ÄÇ
Â∏åÊúõËÉΩÂàÜ‰∫´ÊÇ®‰ΩøÁî®ÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊúâÂêéÁª≠ÂºÄÂèëÈúÄË¶ÅÂèØÂçèÂä©‰∏ÄËµ∑ÂÆåÊàêÔºå‰∏áÂàÜÊÑüË∞¢ÔºÅ

"
DemoNShortSegmentÊï∞ÁªÑË∂äÁïåÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Áõ¥Êé•checkoutÂ∑•Á®ãÔºåËøêË°åtest/java/com/hankcs/demo/DemoNShortSegmentÊó∂ÔºåÊä•ArrayIndexOutOfBoundsException„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
  public class DemoNShortSegment
{
    public static void main(String[] args)
    {
        Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        Segment shortestSegment = new ViterbiSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        String[] testCase = new String[]{
                ""‰ªäÂ§©ÔºåÂàòÂøóÂÜõÊ°àÁöÑÂÖ≥ÈîÆ‰∫∫Áâ©,Â±±Ë•øÂ•≥ÂïÜ‰∫∫‰∏Å‰π¶ËãóÂú®Â∏Ç‰∫å‰∏≠Èô¢Âá∫Â∫≠ÂèóÂÆ°„ÄÇ"",
                ""Ê±üË•øÁúÅÁõëÁã±ÁÆ°ÁêÜÂ±Ä‰∏é‰∏≠ÂõΩÂ§™Âπ≥Ê¥ãË¥¢‰∫ß‰øùÈô©ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÂçóÊòå‰∏≠ÂøÉÊîØÂÖ¨Âè∏‰øùÈô©ÂêàÂêåÁ∫†Á∫∑Ê°à"",
                ""Êñ∞ÂåóÂïÜË¥∏ÊúâÈôêÂÖ¨Âè∏"",
        };
        for (String sentence : testCase)
        {
            System.out.println(""N-ÊúÄÁü≠ÂàÜËØçÔºö"" + nShortSegment.seg(sentence) + ""\nÊúÄÁü≠Ë∑ØÂàÜËØçÔºö"" + shortestSegment.seg(sentence));
        }
    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 40
	at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:216)
	at com.hankcs.hanlp.seg.common.WordNet.insert(WordNet.java:168)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3779)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3756)
	at com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie.parseText(AhoCorasickDoubleArrayTrie.java:115)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3755)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.recognition(OrganizationRecognition.java:71)
	at com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence(NShortSegment.java:79)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at com.hankcs.demo.DemoNShortSegment.main(DemoNShortSegment.java:36)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â∏åÊúõÁ∑ö‰∏äÊºîÁ§∫ÁöÑÊµÆÂãïË™™ÊòéËÉΩÂ§†Â¢ûÂä†È°ØÁ§∫Ë©≤Ë©ûÊÄßÁöÑ‰∏≠ÊñáË™™Êòé,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

http://hanlp.hankcs.com/

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Â∏åÊúõÁ∑ö‰∏äÊºîÁ§∫ÁöÑÊµÆÂãïË™™ÊòéËÉΩÂ§†Â¢ûÂä†È°ØÁ§∫Ë©≤Ë©ûÊÄßÁöÑ‰∏≠ÊñáË™™Êòé(Ë®òÊÄßÂ§™Â∑ÆÂÆπÊòìÂøòË®òËã±ÊñáË©ûÊÄß‰ª£Ë°®ÊÑèÊÄù)

È†Ü‰æøÂïè‰∏ã Á∑ö‰∏äÊºîÁ§∫ÁöÑÂÄâÂ∫´ÊòØÂì™‰∏ÄÂÄã lol

‰ºº‰πé‰∏çÂú®Ê≠§ÂÄâÂ∫´ÂÖß

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
""Ë¨éÊ†∑""
n
ÂêçËØç
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

![2018-06-26-02-01-59-2](https://user-images.githubusercontent.com/167966/41867392-9b8e543e-78e5-11e8-99ad-dcc47b7edfb0.png)


"
Bug Hanlp.segment() ÂàÜËØçÊä•ÈîôÔºåjava.lang.ArrayIndexOutOfBoundsException: 148,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.4.jar„ÄÅhanlp-1.6.4-sources.jar„ÄÅhanlp.properties„ÄÅdata Áâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂÖàËØ¥Áé∞Ë±°Ôºö
1. Âú®‰ª•‰ª£Á†ÅÁöÑÊñπÂºèËá™ÂÆö‰πâËØçÂÖ∏Âä†ÂÖ•‰∏Ä‰∫õËØçÂêéÔºåÂ¶Ç`CustomDictionary.insert(""Âú∞‰∫ß‰∏öÊàñÊüê‰∏™ËØç"", ""Industry 1"");`ÂêéÔºåÂçïÁã¨ËøêË°åÊµãËØï`HanLP.segment(‚Äú‰ªªÊÑèÂÜÖÂÆπ‚Äù)`Êó∂ÔºåÂàáÂàÜÊ≠£Â∏∏‰∏îÈÉΩ‰∏ç‰ºöÂá∫Áé∞ÈóÆÈ¢ò„ÄÇ
2. ‰ΩÜÁªèËøá‰∏ÄÁ≥ªÂàóÁöÑÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ‰ª•ÂèäËøûÁª≠ÁöÑ`CustomDictionary.insert(""ÊüêËØç"", ""Industry 1"");`Â§ÑÁêÜÂêéÔºåÂÜçËøõË°å`HanLP.segment(‚ÄúÊüê‰∫õÂÜÖÂÆπ‚Äù)`Êó∂Â∞±‰ºöÊä•ÈîôÔºåÈîôÂ¶ÇÔºö
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:106)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:90)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:189)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:325)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:227)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:601)
3. Êä•ÈîôÂêéÂÜç debug Êó∂ÔºåÂä†ÂÖ•ÁöÑÊñ≠ÁÇπÂèØ‰ª•Ë∂äËøáÔºå‰πüÂ∞±ÊòØÂèà‰∏çÊä•Èîô‰∫ÜÔºåÂèØ‰ª•ÊâßË°åÂÆåÔºå‰ΩÜ‰∏ç debug ÊâßË°åÂ∞±‰ºöÊä•Èîô„ÄÇ
4. ÂèçÂ§çÊâßË°åÂêéÂèëÁé∞ÂÅ∂Â∞îÊúâÊó∂ÂÄôÂèØ‰ª•ËøêË°åÊàêÂäüÔºå‰∏î‰∏çÊä•ÈîôÔºå‰ΩÜÂ§öÊï∞Êó∂ÂÄôÊòØ‰ºöÊä•ÈîôÁöÑ„ÄÇ
5. Â∞Ü`CustomDictionary.insert(""ÊüêËØç"", ""Industry 1"");`Êîπ‰∏∫`CustomDictionary.insert(""ÊüêËØç"");`ÂêéÔºåÂéªÊéâËØçÊÄßÂíåËØçÈ¢ëÔºåÂ∞±‰∏ç‰ºöÊä•Èîô‰∫Ü„ÄÇ
==============‰ª•‰∏äÊòØ‰∏ÄÁ±ªÈîôËØØ================
==============‰ª•‰∏ãÊòØ‰∏ÄÁ±ªÈîôËØØ================
1. Âú®Âà©Áî® Spring boot ÂåÖË£Ö HanLP Êàê‰∏∫ service ÁöÑÊó∂ÂÄôÔºåÂä†ËΩΩ‰∏çÂêåÁöÑËØçÂ∫ìÔºå‰∏î`CustomDictionary.insert(""ÂêÑ‰∏™ËØç"", ""Áõ∏Â∫îËØçÊÄß 1"");`ÂêéÔºåÂàáÂàÜÊïàÊûú‰∏çÊ≠£Á°ÆÔºåÂ∞ΩÁÆ°ÂçïÁã¨ÊµãËØïÊòØÊ≠£Á°ÆÁöÑÔºå‰ΩÜservice‰∏≠Á°ÆÂÆû‰∏çÊ≠£Á°ÆÔºåÂ¶ÇÊûúÊîπ‰∏∫`CustomDictionary.insert(""ÂêÑ‰∏™ËØç"");`ÂêéÔºåÂàáÂàÜÊïàÊûúÊ≠£Á°Æ„ÄÇ
==============ÁªºÂêà‰∏äËø∞‰∏§ÁßçÊÉÖÂÜµÁöÑÈîôËØØ================
‰∏™‰∫∫ËÄÉËôë‰∏∫ÂèØËÉΩÊòØÂêå‰∏Ä‰∏™ BUG ÂºïËµ∑ÁöÑÔºå‰∫éÊòØ‰ª£Á†ÅÂè™Â§çÁé∞Á¨¨‰∏ÄÁ±ªÈîôËØØÔºåÁ¨¨‰∫åÁ±ª‰πü‰∏çÂ•ΩÂ§çÁé∞„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    package com.gemantic.Issues;

import com.google.common.collect.Lists;
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.dictionary.CustomDictionary;
import com.hankcs.hanlp.seg.common.Term;
import org.apache.commons.io.FileUtils;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Issue {

    public static void main(String[] args) throws IOException {
        File industryFile = new File(""data/issue.txt"");
        List<String> industries = FileUtils.readLines(industryFile, ""UTF-8"");

        List<String> allIndustryPrefixes = Lists.newArrayList();
        allIndustryPrefixes.add(""ÂÖ∂‰ªñ"");

        List<String> allIndustryPostfixes = Lists.newArrayList();
        allIndustryPostfixes.add(""Ë°å‰∏ö"");

        // step1„ÄÅ‰ªÖ‰ªÖÊü•Áúã‰∫Ü‰∏Ä‰∏ã industries ÁöÑÂàÜËØçÊïàÊûúÔºåÂπ∂Ê≤°ÊúâÂÅöÂÖ∂‰ªñÂ§ÑÁêÜ
        List<String> industriesLooked = afterLooked(industries);
        System.out.println(industries.size());
        System.out.println(industriesLooked.size());

        // step2„ÄÅÈÄêÊ≠•ÂØπ industries ÂÅö‰∫Ü‰∏Ä‰∫õÂà§Êñ≠Ôºå‰∏îÈÄêÊ≠•ÁöÑÂä†ÂÖ•‰∫Ü HanLP ÁöÑ Customdictionary ‰∏≠
        List<String> smoothSegmentCustomDictionary = getSmoothSegmentCustomDictionary(industriesLooked, allIndustryPrefixes, allIndustryPostfixes);
        System.out.println(smoothSegmentCustomDictionary.size());
    }


    public static List<String> getSmoothSegmentCustomDictionary(List<String> allIndustries, List<String> prefixes, List<String> postfixes){

        // ÁªÜÁ≤íÂ∫¶ÂàÜËØçËØçÂÖ∏ CustomDictionary
        List<String> smoothSegmentCustomDictionary = Lists.newArrayList();

        // ÁõÆÁöÑÊòØÂàÜËØçËÉΩÂ§üÂ∞Ü""ÂâçÁºÄ""‰∏é""ÂêéÁºÄ""ÂàÜÂá∫Êù•
        // Ê∑ªÂä†""ÂâçÁºÄ""
        for(String prefix : prefixes){
            if (!CoreDictionary.contains(prefix)) {
                CustomDictionary.insert(prefix);
            }
        }
        // Ê∑ªÂä†""ÂêéÁºÄ""
        for(String postfix : postfixes){
            if (!CoreDictionary.contains(postfix)) {
                CustomDictionary.insert(postfix);
            }
        }

        // Ê∑ªÂä†""ÁâπÊÆäËØç""

        CustomDictionary.insert(""Âú∞‰∫ß‰∏ö"", ""Industry 1"");

        smoothSegmentCustomDictionary.add(""Âú∞‰∫ß‰∏ö"");


        // ÈÅøÂÖçÈáçÂ§çÂ§ÑÁêÜ ‰∏é Âå∫ÂàÜ Á¨¨‰∏ÄËΩÆÁöÑËã±ÊñáÊï∞Â≠ó
        List<String> industryProcessed = new ArrayList<>();

        // Â§ÑÁêÜÂÖ®Ëã±ÊñáÂèäËã±ÊñáÊï∞Â≠óËØç
        // ‰∏≠Ëã±Êñá‰∏çÈúÄÂ§ÑÁêÜÔºåÂêåÂÖ∂‰ªñÊÉÖÂÜµÊòØÂêå‰∏ÄÁßçÊñπÂºè
        for (String industry : allIndustries){

            if (!industryProcessed.contains(industry)){
                Boolean x = industry.matches(""[0-9a-zA-Z]*"");
                if (x == Boolean.TRUE){
                    CustomDictionary.insert(industry);
                    smoothSegmentCustomDictionary.add(industry);
                    industryProcessed.add(industry);
                    System.out.println(industry);
                }
            }
        }

        // Â§ÑÁêÜ1„ÄÅ2Â≠óËØçÔºåËÆ§‰∏∫1„ÄÅ2Â≠óËØç‰∏∫ÊúÄÂü∫Êú¨È¢óÁ≤íÔºå‰∏çÂèØÂÜçÂàáÂàÜ
        for (String industry : allIndustries){

            if (!industryProcessed.contains(industry)){
                if (industry.length() == 1 || industry.length() == 2){
                    if (!CoreDictionary.contains(industry)){
                        CustomDictionary.insert(industry);
                        smoothSegmentCustomDictionary.add(industry);
                        System.out.println(industry);
                    }
                }
            }
        }

        // Â§ÑÁêÜ3Â≠óÂèä‰ª•‰∏äËØç
        int max_length = 50;

        for (int length = 3 ;length < max_length; length ++){

            for (String industry : allIndustries){

                if (!industryProcessed.contains(industry)){

                    if (industry.length() == length){

                        List<Term> segmentsOfIndustry = HanLP.segment(industry);

                        if (segmentsOfIndustry.size() == 1){
                            if (!CoreDictionary.contains(industry)){
                                CustomDictionary.insert(industry);
                                smoothSegmentCustomDictionary.add(industry);
                                System.out.println(industry);
                            }
                        }

                        if (segmentsOfIndustry.size() > 1){
                            // ÈúÄË¶ÅÂà§Êñ≠ËØ•ËØçÊòØÂê¶ÁúüÁöÑÂèØÊãÜÂàÜ

                            // Ëé∑ÂèñËØ•Ë°å‰∏öÊ¶ÇÂøµÁöÑÂàÜËØçÂàóË°®
                            List<String> words = new ArrayList<>();
                            for (Term segment : segmentsOfIndustry){
                                words.add(segment.word);
                            }

                            // ÂèØÊãÜÂàÜÊù°‰ª∂
                            int condition = 0;
                            for (String industryPrefix : prefixes){
                                if (words.contains(industryPrefix)){
                                    condition = condition + 1;
                                }
                            }
                            for (String industryPostfix : postfixes){
                                if (words.contains(industryPostfix)){
                                    condition = condition + 1;
                                }
                            }
                            for (String word : words){
                                for (String industryTemp : allIndustries){
                                    if (industryTemp.equals(word)){
                                        condition = condition + 1;
                                    }
                                }
                            }

                            // ËÆ§‰∏∫ÊòØ‰∏çÂèØÊãÜÂàÜÁöÑËØçÔºåÂ∫îËØ•‰Ωú‰∏∫Êï¥ËØçÂä†ÂÖ• CustomDictionary
                            if (condition == 0 && !CoreDictionary.contains(industry)){
                                CustomDictionary.insert(industry);
                                smoothSegmentCustomDictionary.add(industry);
//                                if(industry.equals(""Â§ßÂÜõÂ∑•"")){
//                                    System.out.println(condition);
//                                    System.out.println(industry);
//                                    System.out.println(words);
//                                }
                                System.out.println(industry);
                            }
                        }

                    }
                }
            }
        }
        // ======  ÁªÜÁ≤íÂ∫¶ÂàÜËØçÂ§ÑÁêÜÂÆåÊØï =========
        return smoothSegmentCustomDictionary;
    }



    public static List<String> afterLooked(List<String> allIndustries) {

        // Âè™Áúã‰∫Ü‰∏ÄÁúºÂàÜËØçÂêéÁöÑÊñ∞ËØçÂÖ∏
        List<String> industriesLooked = Lists.newArrayList();

        for (String industry : allIndustries){

            // Âè™ÊòØÂàÜËØçÁúã‰∏Ä‰∏ã
            List<Term> segmentsOfIndustry = HanLP.segment(industry);
            List<String> segments = Lists.newArrayList();
            for(Term term : segmentsOfIndustry){
                segments.add(term.word);
            }
            System.out.println(segments);

            industriesLooked.add(industry);

        }

        return industriesLooked;
    }
}

```
```
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:106)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:90)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:189)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:325)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:227)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:601)
	at com.gemantic.Issues.Issue.getSmoothSegmentCustomDictionary(Issue.java:107)
	at com.gemantic.Issues.Issue.main(Issue.java:33)
```
```
data/issue.txt
‰∫ë
Â•∂
Â∫ä
Â∫ó
Ê∞¥
ÊµÜ
ÁÇ≠
ÁÖ§
ËΩ¶
ÈÖí
Èì¨
ÈìΩ
ÈîÇ
ÈîÜ
Èîå
Èîë
Èîó
Èî°
ÈïÅ
Èïç
Èïù
Èï®
È∏°
3c
4g
5g
ah
ai
ar
aËÇ°
bËÇ°
hËÇ°
ic
it
mr
sËÇ°
vr
‰∏âÊùø
‰∏ìÂà©
‰∏ìÁßë
‰∏ìÁΩë
‰∏ôÁÉØ
‰∏ôÁÉ∑
‰∏§‰ºö
‰∏§Ëûç
‰∏≠‰ªã
‰∏≠ËçØ
‰∏¥Ê∏Ø
‰πôÁÉØ
‰πôÁÉ∑
‰πôÈÜá
‰π°Èïá
‰π≥‰∏ö
‰π≥ÂìÅ
‰π≥Á≤â
‰∫åËÉé
‰∫åËêú
‰∫ëÁ´Ø
‰∫ïÂè∞
‰∫§Ëøê
‰∫§ÈÄö
‰∫¨Ê¥•
‰ªìÂÇ®
‰ª£Â∑•
‰ª£Ë¥≠
‰ª™Ë°®
‰ºëÈó≤
‰º†Â™í
‰Ωé‰Ωç
‰ΩéÂéã
‰ΩéÁ¢≥
‰ΩèÂÆÖ
‰ΩìÂΩ©
‰ΩìËÇ≤
‰æõÂ∫î
‰æõÊöñ
‰æõÁîµ
‰øùÂÅ•
‰øùËçê
‰øùÈô©
‰ø°ÊÅØ
‰ø°Êâò
‰ø°Áî®
‰ø°Ë¥∑
ÂÄüË¥∑
ÂÅ•Ë∫´
ÂÇ®Ê∞î
ÂÇ®ËÉΩ
ÂÖÉ‰ª∂
ÂÖâ‰ºè
ÂÖâÂ≠¶
ÂÖâÊ£í
ÂÖâÁ∫§
ÂÖâÈÄö
ÂÖ®Â±ã
ÂÖ®ÊÅØ
ÂÖ¨‰ºö
ÂÖ¨ÂÆâ
ÂÖ¨ÂØì
ÂÖ¨Ë∑Ø
ÂÖ≥ËäÇ
ÂÖµÂô®
ÂÖªÊÆñ
ÂÖªËÄÅ
ÂÜÖÈÖØ
ÂÜÖÈïú
ÂÜõ‰∫ã
ÂÜõÂìÅ
ÂÜõÂ∑•
ÂÜõÊú∫
ÂÜõÊ∞ë
ÂÜú‰∏ö
ÂÜúÂåñ
ÂÜúÂû¶
ÂÜúÊú∫
ÂÜúÊùë
ÂÜúÁâß
ÂÜúËçØ
ÂÜ∞Êüú
ÂÜ∞ÁÆ±
ÂÜ∑Èìæ
ÂàÜÈîÄ
Âàõ‰º§
ÂàõÊäï
Âà∂ÂâÇ
Âà∂Á≥ñ
Âà∂ËçØ
Âà∏ÂïÜ
ÂâßÈõÜ
ÂäûÂÖ¨
Âä®‰øù
Âä®ÊÄÅ
Âä®Êº´
Âä®ËΩ¶
ÂãòÊé¢
ÂåÖË£Ö
ÂåñÂ∑•
ÂåñÁ∫§
ÂåñËÇ•
ÂåóÊñó
Âå∫Âüü
Âåª‰øù
ÂåªÊîπ
ÂåªÁñó
ÂåªËçØ
ÂåªÈô¢
ÂçïÊäó
ÂçïÊô∂
ÂçöÂΩ©
Âç°ËΩ¶
Âç´Êòü
Âç´Êµ¥
Âç´ÈÄö
Âç∞Êüì
Âç≥Êúü
Âç∑ÁÉü
ÂéÇÂ∫ì
ÂéãË£Ç
ÂéüÂ•∂
ÂéüÊñô
ÂéüÊ≤π
ÂéüÁ≥ñ
ÂéüËçØ
Âé®Âç´
Âé®Êüú
Âé®Áîµ
Âéø‰π°
ÂèâËΩ¶
ÂèëÁîµ
Âè§‰∫ï
Âè∂Áâá
ÂêäÈ°∂
ÂêåÂüé
ÂêçË°®
ÂêçÈÖí
Âê°Âï∂
Âë≥Á≤æ
ÂîÆÁîµ
ÂïÜÊóÖ
ÂïÜË¥∏
Âï§ÈÖí
Âô®‰ª∂
Âô®ÂÖ∑
ÂúÜÁéØËãó
Âú∞‰∫ß‰∏ö
```

### ÊúüÊúõËæìÂá∫
Ê≠£Â∏∏Êù•ËØ¥Ôºå‰∏çËÆ∫ `CustomDictionary.insert(""‰ªªÊÑèËØç"", ""Áõ∏Â∫îËØçÊÄß 1"");`ÂêéÔºåHanLP.segment() Â∫îËØ•ÈÉΩ‰ºöÊ≠£Â∏∏ÂàáÂàÜÔºåÊúÄÂ§ö‰∏çËøáÊòØÂàáÂàÜÊïàÊûú‰∏çÁêÜÊÉ≥ËÄåÂ∑≤Ôºå‰ΩÜÁé∞Âú®‰∏∫‰ªÄ‰πà‰ºöÊä• HanLP.segment() ‰∏≠ Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148 Âë¢Ôºü
ËØ∑Â§öÊåáÊïôÔºÅ‰∏çËÉúÊÑüË∞¢ÔºÅ
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
JDK9 ÊîØÊåÅ,"Âú® `EnumBuster` ‰∏≠Áî®Âà∞‰∫ÜÂåÖ `sun.reflect`ÔºåÂú® JDK9 Âèä ‰ª•ÂêéÁöÑ JDK ÁâàÊú¨‰∏≠ÔºåÂ∑≤ÁªèÂèòÊàê‰∫Ü `jdk.internal.reflect`  „ÄÇÂèØ‰ª•ËÄÉËôëÂè¶Â§ñÂÆûÁé∞ÔºåÈÅøÂÖçÂÜÖÈÉ®ÂåÖÁöÑÂºïÂÖ•

<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

"
Êõ¥Êîπ CoreNatureDictionary.txt ‰∏îÂà†Èô§ÊâÄÊúâ bin Êñá‰ª∂ÂêéÔºåÊú™ËÉΩÁîüÊïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.4.jar„ÄÅhanlp-1.6.4-sources.jar„ÄÅhanlp.properties„ÄÅdata Áâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1. ÈÖçÁΩÆÂ•Ω 1.6.4 Èùû portable ÁâàÂêéÔºåÂèØ‰ª•ÂàÜËØç‰∏îÊïàÊûúÊ≠£Â∏∏Ôºå‰ΩÜÂú® CoreNatureDictionary.txt ‰∏≠ÊåâÁùÄÂêåÁ≠âÊ†ºÂºèÂä†ÂÖ•`Â§ßÂÆ∂Áîµ n 1000`ÂêéÔºåÊâßË°å`CoreDictionary.contains(""Â§ßÂÆ∂Áîµ"")`ÁªìÊûú‰∏∫`false`„ÄÇ
2. data Êñá‰ª∂Â§π‰∏≠ÁöÑ .bin .tr.txt ÈÉΩÂà†Êéâ‰∫ÜÔºåÂêåÊ†∑Êú™ÂèëÁé∞Êúâ‰ªª‰ΩïÂèòÂåñÔºå‰∏îÂú®`hanlp.properties`‰∏≠ÂèëÁé∞ÔºåËØçÂÖ∏Âú∞ÂùÄ‰πüÊ≤°ÊúâÁî®Âà∞Ëøô‰∫õÂ¶Ç`CoreNatureDictionary.txt.bin`ÁöÑÊñá‰ª∂ÔºåÈÇ£‰πàÂÖ∂Â≠òÂú®ÁöÑ‰ΩúÁî®ÊòØ‰ªÄ‰πàÂë¢ÔºüHanLPÈ¶ñÈ°µËØ¥ÊòØ‰∏∫‰∫ÜÂä†ÈÄüÂä†ËΩΩËØçÂÖ∏ÈÄüÂ∫¶Ôºå‰ΩÜÊ≤°ÊúâÂèëÁé∞Âì™ÈáåÁî®Âà∞Ëøô‰∫õ .bin Êñá‰ª∂ÈáåÂëÄÔºü
3. Êää .bin Êñá‰ª∂Âà†ÊéâÂêéÔºåÈáçÊñ∞ËøêË°åÂàÜËØçÁ®ãÂ∫èÔºå.bin Êñá‰ª∂Â∞±‰∏ç‰ºöÂÜçÂá∫Áé∞‰∫ÜÊòØÂêóÔºüÊàëËøô .bin Êñá‰ª∂Êú™ÂÜçÂá∫Áé∞„ÄÇ 
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.dictionary.CustomDictionary;

public class TempTest {
    public static void main(String[] args) {
        System.out.println(CoreDictionary.contains(""Â§ßÂÆ∂Áîµ""));
    }
}

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
true
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
false
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
Èü©Ê≠£ÂâØÊÄªÁêÜËØÜÂà´Âá∫ÈîôÔºåÊÄªÊòØËØÜÂà´‰∏∫‚ÄúÊ≠£ÂâØ‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ X] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊµãËØïÂè•Â≠êÔºöËÆ§ÁúüËêΩÂÆûÊùéÂÖãÂº∫ÊÄªÁêÜ„ÄÅÈü©Ê≠£ÂâØÊÄªÁêÜÈáçË¶ÅÊâπÁ§∫ÊåáÁ§∫Á≤æÁ•ûÔºå
ÊÄªÊòØËØÜÂà´‰∏∫ÔºöÈü© Ê≠£ÂâØ ÊÄªÁêÜ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    String text = ""ËÆ§ÁúüËêΩÂÆûÊùéÂÖãÂº∫ÊÄªÁêÜ„ÄÅÈü©Ê≠£ÂâØÊÄªÁêÜÈáçË¶ÅÊâπÁ§∫ÊåáÁ§∫Á≤æÁ•ûÔºå"";
    HanLP.newSegment().enableNameRecognize(true)
    CustomDictionary.insert(""Èü©Ê≠£"", ""nr 9999"")
    System.out.println(HanLP.segment(text));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ËÆ§Áúü/ad, ËêΩÂÆû/v, ÊùéÂÖãÂº∫/nr, ÊÄªÁêÜ/nnt, „ÄÅ/w, Èü©Ê≠£/nr, ÂâØ/b, ÊÄªÁêÜ/nnt, ÈáçË¶ÅÊâπÁ§∫/n, ÊåáÁ§∫Á≤æÁ•û/nz, Ôºå/w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ËÆ§Áúü/ad, ËêΩÂÆû/v, ÊùéÂÖãÂº∫/nr, ÊÄªÁêÜ/nnt, „ÄÅ/w, Èü©/b, Ê≠£ÂâØ/b, ÊÄªÁêÜ/nnt, ÈáçË¶ÅÊâπÁ§∫/n, ÊåáÁ§∫Á≤æÁ•û/nz, Ôºå/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Merge pull request #1 from hankcs/master,"ÂÜçÊ¨°ÂêåÊ≠•

ÂêåÊ≠•Â∫ì‰ª£Á†Å

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Â¶Ç‰ΩïÂà©Áî®‰∏çÂêåÁöÑ CustomDictionary ËøõË°å HanLP.segment(),"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.6.4.jar„ÄÅhanlp-1.6.4-sources.jar„ÄÅhanlp.properties„ÄÅdata Áâà

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂÅáËÆæÊúâ‰∏§‰∏™Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåCustomDictionary1.txt ‰∏é CustomDictionary2.txtÔºåÂ¶Ç‰ΩïÂú® HanLP.segment() ÁöÑÂü∫Á°Ä‰∏äÂÜô‰∏Ä‰∏™ÂàÜËØçÂáΩÊï∞ÔºåÈÄöËøá‰º†ÂÖ•ÂèÇÊï∞Êù•ÊéßÂà∂ÂàÜËØçÁöÑÊïàÊûúÔºü
```
Â¶ÇÔºö
Segment segment = HanLP.newSegment();
Segment segment1 = HanLP.newSegment();

HanLP.Config.Normalization = true;
segment.enableOffset(true);
segment1.enableOffset(true);

int availProcessors = Runtime.getRuntime().availableProcessors();
segment.enableMultithreading(availProcessors);
segment1.enableMultithreading(availProcessors);

ÁªÑÂêà segment ‰∏é segment1ÔºåÂπ∂‰∏î‰ªñ‰ª¨ÁöÑÂå∫Âà´ÊòØ CustomDictionary ‰∏çÂêåÔºö
‰∏Ä‰∏™ÊòØ 
CustomDictionary.insert(""ÈõÑÂÆâÊ¶ÇÂøµ‰∫ß‰∏öÈìæ"", ""n 1"");
Âè¶‰∏Ä‰∏™ÊòØ
 CustomDictionary.insert(""ÈõÑÂÆâ"", ""n 1"");
CustomDictionary.insert(""Ê¶ÇÂøµ"", ""n 1"");
CustomDictionary.insert(""‰∫ß‰∏öÈìæ"", ""n 1"");

‰ΩÜÊÄé‰πàÊää CustomDictionary ÂΩìÊàêÂèÇÊï∞‰º†Âà∞‰∏çÂêåÁöÑ segment ‰∏≠Âë¢ÔºüÊàñÊòØËØ¥ÊÄé‰πàÂàõÂª∫‰∏çÂêåÁöÑ CustomDictionary ‰ªéËÄåËÆ© ‰∏çÂêåÁöÑ segment ‰æùËµñ‰∏çÂêåÁöÑ CustomDictionary Âë¢Ôºü
```
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫
```
ÈõÑÂÆâÊ¶ÇÂøµ‰∫ß‰∏öÈìæ = CombineHanLP(parameter = true, normalization, offset, multithreading)
ÈõÑÂÆâ/Ê¶ÇÂøµ‰∫ß/‰∏öÈìæ = CombineHanLP(parameter = false, Normalization, offset, multithreading)
```

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
"
‰ΩøÁî®ÊÑüÁü•Êú∫ËØÜÂà´Êú∫ÊûÑÂÆû‰ΩìÔºåÂú®Á∫øÂ≠¶‰π†ÊúâÁöÑÊó∂ÂÄôÊó†Êïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®ÊÑüÁü•Êú∫ËØÜÂà´Êú∫ÊûÑÂÆû‰ΩìÔºåÂú®Á∫øÂ≠¶‰π†ÊúâÁöÑÊó∂ÂÄôÊó†ÊïàÔºå‰∏çÁü•ÈÅì‰ªÄ‰πàÂéüÂõ†ÔºåÊ†áÊ≥®‰∏çÂØπÂêóÔºü

### Ëß¶Âèë‰ª£Á†Å

```
        String s = ""Â∞±ËØª‰∫éÂÆ∂‰π°Ê±üËãèÁúÅÈïáÊ±üÂ∏ÇÁúÅÁ´ãÂ∏àËåÉÂ≠¶Ê†°Âàù‰∏≠‰∫åÂπ¥Á∫ß"";
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.learn(""[Ê±üËãèÁúÅ/ns ÈïáÊ±üÂ∏Ç/ns ÁúÅÁ´ãÂ∏àËåÉÂ≠¶Ê†°/n]/nt""));
        System.out.println(analyzer.seg(s));
```
### ÊúüÊúõËæìÂá∫
```
true
[Â∞±ËØª/v, ‰∫é/p, ÂÆ∂‰π°/n, Ê±üËãèÁúÅÈïáÊ±üÂ∏ÇÁúÅÁ´ãÂ∏àËåÉÂ≠¶Ê†°/nt, Âàù‰∏≠/n, ‰∫å/m, Âπ¥Á∫ß/n]
```

### ÂÆûÈôÖËæìÂá∫
```
true
[Â∞±ËØª/v, ‰∫é/p, ÂÆ∂‰π°/n, Ê±üËãèÁúÅ/ns, ÈïáÊ±üÂ∏Ç/ns, ÁúÅÁ´ã/v, Â∏àËåÉÂ≠¶Ê†°/l, Âàù‰∏≠/n, ‰∫å/m, Âπ¥Á∫ß/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ


"
ÊÑüÁü•Êú∫Êú∫ÊûÑËØÜÂà´ÔºåÂ¶Ç‰ΩïÂØπÈîôËØØËØÜÂà´ËøõË°åÂπ≤È¢ÑÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®ÊÑüÁü•Êú∫ËøõË°åÊú∫ÊûÑËØÜÂà´ÔºåÊÄª‰ΩìÊØîËæÉÂáÜÁ°ÆÔºå‰ΩÜÊòØÂæàÂ§öËØÜÂà´Âà∞ÁöÑÊú∫ÊûÑÊòØÈîôËØØÁöÑÔºåÈúÄË¶ÅÊéíÈô§ÔºåÂ¶Ç‰ΩïÂà©Áî®Âú®Á∫øÂ≠¶‰π†ËøõË°å‰∫∫Â∑•Âπ≤È¢ÑÂë¢Ôºü
"
‰∏Ä‰∏™‰ª£Á†ÅÁöÑÂ∞èÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®ÈòÖËØª `DoubleArrayTrie` Ê∫êÁ†ÅÁöÑÊó∂ÂÄôÂú®Á¨¨393Ë°åÔºà`build` ÊñπÊ≥ï)ÂÜÖÂèëÁé∞‰∏Ä‰∏™ÂæàÂ∞èÁöÑÈóÆÈ¢òÔºö

```
    if (_keySize > _key.size() || _key == null)
            return 0;
```
 ËøôÈáåÁöÑ `_key` ÊòØÂÖàË¢´‰ΩøÁî®ÁÑ∂ÂêéÂÜçÂà§Êñ≠ÊòØÂê¶‰∏∫Á©∫ÔºåÂΩì `_key` ‰∏∫Á©∫Êó∂ÔºåÊòØÊ≤°Ê≥ïÊâßË°åÂà∞ `_key == null` Ëøô‰∏ÄÊù°‰ª∂ÁöÑÂêß


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§



### Ëß¶Âèë‰ª£Á†Å


### ÊúüÊúõËæìÂá∫



### ÂÆûÈôÖËæìÂá∫



## ÂÖ∂‰ªñ‰ø°ÊÅØ

"
ËØçÂÖ∏‰ºòÂåñ,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÂºÄÂêØNormalization=true‰ª•ÂêéÔºå
‚ÄúËñ∞Ë°£Ëçâ‚ÄùÂàÜËØçÁªìÊûú‰ºöÂèòÊàêÔºö‚ÄúÁÜèË°£Ëçâ‚ÄùÔºåËøôÊòØ‰∏çÊ≠£Á°ÆÁöÑ
Ôºà‰ª£Á†ÅÁöÑcommit ‰ø°ÊÅØ‰∏≠ÂÜôÈîô‰∫ÜÊòØÂéªÊéâ ‚ÄúËñ∞=ÁÜè‚Äù‰∏çÊòØ‚ÄúËñ∞=È¶ôÁÜè‚ÄùÔºâ

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
CRFÂàÜËØçÔºåÊï∞ËØçÂíåÊ†áÁÇπÁ¨¶Âè∑ÈÉΩË¢´ËØÜÂà´Êàênz,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4 
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4 jarÁâàÊú¨ master

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
```
        String name1 = ""ÊÇ®Â•Ω,Â∏ÆÊàëÁúã‰∏Ä‰∏ãÁîµÂΩ±Èô¢Âá†ÁÇπÂºÄÈó®"";
        System.out.println(""Ê†áÂáÜÂàÜËØçÔºö""+ HanLP.segment(name1));
        final Segment segment1 = new CRFSegment();
        List<Term> termList1 = segment1.seg(name1);
        System.out.println(""CRFSegment:""+termList1);
```
log:
Ê†áÂáÜÂàÜËØçÔºö[ÊÇ®Â•Ω/n, ,/w, Â∏Æ/v, Êàë/rr, Áúã/v, ‰∏Ä‰∏ã/m, ÁîµÂΩ±Èô¢/nis, Âá†/d, ÁÇπ/qt, ÂºÄÈó®/vi]
CRFSegment:[ÊÇ®Â•Ω/n, ,/nz, Â∏Æ/v, Êàë/rr, Áúã‰∏Ä‰∏ã/nz, ÁîµÂΩ±Èô¢/nis, Âá†ÁÇπ/nz, ÂºÄÈó®/vi]

-- ‚ÄúÁúã‰∏Ä‰∏ã‚ÄùÂíå‚ÄúÂá†ÁÇπ‚ÄùÔºå‰ª•ÂèäÊ†áÁÇπÁ¨¶Âè∑ÈÉΩË¢´ËØÜÂà´ÊàênzÔºåÊúâÂäûÊ≥ïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÂêóÔºü


"
‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑÊ†áÁ≠æ‰∏∫ÊØè‰∏™Â≠óËøõË°åÊ†áÊ≥®ÔºåÈô§‰∫ÜÈáçÊñ∞ÂÆö‰πâÊ®°ÂûãÊñá‰ª∂ÔºåËøòÈúÄË¶ÅÂÖ∂‰ªñ‰ªÄ‰πàÂ∑•‰ΩúÔºü,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑÊ†áÁ≠æ‰∏∫ÊØè‰∏™Â≠óËøõË°åÊ†áÊ≥®ÔºåÈô§‰∫ÜÈáçÊñ∞ÂÆö‰πâÊ®°ÂûãÊñá‰ª∂ÔºåËøòÈúÄË¶ÅÂÖ∂‰ªñ‰ªÄ‰πàÂ∑•‰ΩúÔºüÊØîÂ¶ÇÊàëÊÉ≥ÂæóÂà∞Â¶Ç‰∏ãÁöÑÁªìÊûúÔºö
ÊÇ£    Bx
ËÄÖ    Ix
Ôºå	D
56	Ba
Â≤Å	Ia
Ôºå    D
Ë°Ä	Bi
Âéã	Ii
‰∏ç	Bis
Á®≥	Iis
ÂÆö	Iis

Ëøô‰∏™Êó∂ÂÄôÈô§Ë¶ÅÂÆö‰πâÊñ∞ÁöÑÊ®°ÂûãÊñá‰ª∂‰πãÂ§ñÔºå‰ª£Á†ÅÈÉ®ÂàÜËøòÈúÄË¶Å‰øÆÊîπÂì™‰∫õÂú∞ÊñπÔºüÊúüÂæÖÂõûÂ§çÔºåË∞¢Ë∞¢ÔºÅ

"
‰ΩøÁî®pythonË∞ÉÁî®hanlpËøõË°åÂä®ÊÄÅÂä†ËΩΩËØçÂÖ∏ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®‰ΩøÁî®pythonÂä®ÊÄÅÂä†ËΩΩËØçÂÖ∏Êó∂ÔºåÊä•Âá∫ÂºÇÂ∏∏ÔºöRuntimeError: No matching overloads found for get in find. at native/common/jp_method.cpp:127

googleÊü•‰∫Ü‰∏ãËß£ÂÜ≥ÊñπÊ≥ïÔºöhttps://stackoverflow.com/questions/7797406/jpype-and-java-util-properties
‰ΩÜ‰æùÊóßÊ≤°ÊúâËß£ÂÜ≥

‰πüËÆ∏‰∏çÊòØhanlpÁöÑÈóÆÈ¢òÔºå‰ΩÜÂÆûÂú®Êâæ‰∏çÂà∞Ëß£ÂÜ≥ÊñπÊ≥ï‰∫Ü„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->



### Ê≠•È™§

1. È¶ñÂÖàÊü•ÁúãÊñáÊ°£ÔºåÈÄöËøápythonË∞ÉÁî®hanlp
2. ÁÑ∂ÂêéËøêË°å
3. Êé•ÁùÄ‚Ä¶‚Ä¶Êä•Èîô‰∫Ü.....

### Ëß¶Âèë‰ª£Á†Å

```
userwords=['java', 'javaÂ∑•Á®ãÂ∏à', 'Êï∞ÊçÆÊåñÊéòÂ∑•Á®ãÂ∏à']
startJVM(getDefaultJVMPath(
), ""-Djava.class.path=/Users/alanlau/Workplace/MacInstallations/Hanlp/hanlp-1.5.3.jar:/Users/alanlau/Workplace/MacInstallations/Hanlp"",
         ""-Xms1g"", ""-Xmx1g"")
CustomDictionary = JClass('com.hankcs.hanlp.dictionary.CustomDictionary')
for userword in userwords:
    CustomDictionary.insert(userword, 'nski 10'))   #Ëøô‰∏ÄË°åÊä•Èîô
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊöÇÊó†ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Traceback (most recent call last):
  File ""transformat1.py"", line 8, in <module>
    from cleaner import slices_text
  File ""/Users/alanlau/Workplace/Bello/projlstm/transformat/cleaner.py"", line 2, in <module>
    from utils import segtools
  File ""/Users/alanlau/Workplace/Bello/projlstm/transformat/utils/segtools.py"", line 14, in <module>
    CustomDictionary.insert(userword, 'nski ' + str(len(userword) + 100))
RuntimeError: No matching overloads found for insert in find. at native/common/jp_method.cpp:127
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Êìç‰ΩúÁ≥ªÁªüÔºöMacOS 10.13.4

java version ""10.0.1"" 2018-04-17
Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10)

Python 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06) 
"
ÂàÜËØçÁªìÊûúÂéªÈô§Â≠óÁ¨¶Ê≠£ÂàôÂåñÂèòÊç¢,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÊÑüÁü•Êú∫ÂàÜËØçÂô®ÂØπÂàÜËØçÁªìÊûúÁöÑÂ≠óÁ¨¶Ê≠£ÂàôÂåñÈóÆÈ¢ò

## Áõ∏ÂÖ≥issue

https://github.com/hankcs/HanLP/issues/844


"
ÊÑüÁü•Êú∫ÂàÜËØçÂô®ÂØπÂàÜËØçÁªìÊûúÁöÑÂ≠óÁ¨¶Ê≠£ÂàôÂåñÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
`AbstractLexicalAnalyzer.analyze()`ÊñπÊ≥ïÔºåÂΩìÊ≤°ÊúâÂä†ËΩΩner.binÊ®°ÂûãÔºà`neRecognizer == null`ÔºâÊó∂ÔºåËøîÂõûÁöÑÂàÜËØçÁªìÊûúÊòØÂ≠óÁ¨¶Ê≠£ÂàôÂåñÂêéÁöÑÂΩ¢ÂºèÔºå‰∏éËæìÂÖ•ÂéüÊñá‰∏ç‰∏ÄËá¥„ÄÇ

‰ªé‰ª£Á†Å‰∏≠ÁöÑË°å‰∏∫Êù•ÁúãÔºå`AbstractLexicalAnalyzer.analyze()`‰∏≠ÁöÑÂàÜËØçËøáÁ®ã‰ºöÈ¶ñÂÖàËøõË°åÂ≠óÁ¨¶Ê≠£ÂàôÂåñÔºå`segmenter`„ÄÅ`posTagger`Âíå`neRecognizer`Áõ¥Êé•Â§ÑÁêÜÁöÑÈÉΩÊòØÁªèÂ≠óÁ¨¶Ê≠£ÂàôÂåñÁöÑÊï∞ÊçÆ„ÄÇ

‰ΩÜÊòØÂú®ËæìÂá∫ÂàÜËØçÁªìÊûúÁöÑÊó∂ÂÄôÔºåÂè™ÊúâÊâßË°å‰∫Ü`neRecognizer`ÁöÑÈÄªËæëÂàÜÊîØÈáåÂ∞ÜÊú™Ê≠£ÂàôÂåñÁöÑËØçÂÜôÂõûÂàÜËØçÁªìÊûúÊï∞ÁªÑÔºö

```
// AbstractLexicalAnalyzer.java Line 190
// if (neRecognizer != null) ÂàÜÊîØ
String[] nerArray = neRecognizer.recognize(wordArray, posArray);
wordList.toArray(wordArray);
```

ÂÖ∂‰ªñÊÉÖÂÜµÁöÑËøîÂõûÂÄºÔºàLine 219 - Line 233Ôºâ‰∏ãÈÉΩÊ≤°ÊúâÊâßË°åËøô‰∏™Êìç‰ΩúÔºåËøîÂõûÁöÑwordArray‰∏≠ÁöÑËØçÊòØË¢´Â≠óÁ¨¶Ê≠£ÂàôÂåñÂ§ÑÁêÜÂêéÁöÑÂΩ¢Âºè„ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) throws Exception {
        // Âä†ËΩΩ nerModel ÁöÑÂÆû‰æã
        PerceptronLexicalAnalyzer analyzer1 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        // Êú™Âä†ËΩΩ nerModel ÁöÑÂÆû‰æã
        PerceptronLexicalAnalyzer  analyzer2 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath);

        // ËæìÂá∫ÂàÜËØçÁªìÊûú
        System.out.println(analyzer1.analyze(""‰∏äÊµ∑Ëá™Êù•Ê∞¥Êù•Ëá™Êµ∑‰∏äÔºåÈªÑÂ±±ËêΩÂè∂ÊùæÂè∂ËêΩÂ±±ÈªÑ„ÄÇ""));
        System.out.println(analyzer2.analyze(""‰∏äÊµ∑Ëá™Êù•Ê∞¥Êù•Ëá™Êµ∑‰∏äÔºåÈªÑÂ±±ËêΩÂè∂ÊùæÂè∂ËêΩÂ±±ÈªÑ„ÄÇ""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰∏äÊµ∑/ns Ëá™Êù•Ê∞¥/n Êù•Ëá™/v Êµ∑‰∏ä/s Ôºå/w ÈªÑÂ±±/ns ËêΩÂè∂/n ÊùæÂè∂/n ËêΩ/v Â±±ÈªÑ/n „ÄÇ/w
‰∏äÊµ∑/ns Ëá™Êù•Ê∞¥/n Êù•Ëá™/v Êµ∑‰∏ä/s Ôºå/w ÈªÑÂ±±/ns ËêΩÂè∂/n ÊùæÂè∂/n ËêΩ/v Â±±ÈªÑ/n „ÄÇ/w
```

### ÂÆûÈôÖËæìÂá∫

Á¨¨‰∫åË°åÁöÑÈÄóÂè∑ÊòØÊ≠£ÂàôÂåñÂêéÁöÑÔºåÊ≤°ÊúâËΩ¨Êç¢ÂõûÊù•„ÄÇ

```
‰∏äÊµ∑/ns Ëá™Êù•Ê∞¥/n Êù•Ëá™/v Êµ∑‰∏ä/s Ôºå/w ÈªÑÂ±±/ns ËêΩÂè∂/n ÊùæÂè∂/n ËêΩ/v Â±±ÈªÑ/n „ÄÇ/w
‰∏äÊµ∑/ns Ëá™Êù•Ê∞¥/n Êù•Ëá™/v Êµ∑‰∏ä/s ,/w ÈªÑÂ±±/ns ËêΩÂè∂/n ÊùæÂè∂/n ËêΩ/v Â±±ÈªÑ/n „ÄÇ/w
```
"
Ê≠£ÂàôÂåñÂ¢ûÂä†Â§ÑÁêÜÂêÑÁßçÁ±ªÂûãÁöÑÁ©∫Ê†ºÔºåÁâπÂà´ÊòØhtml‰∏≠ÁöÑÁ©∫Ê†º,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->
ÊääÂêÑÁßçÁ±ªÂûãÁöÑÁ©∫Ê†ºËΩ¨‰∏∫ÂçäËßíÁ©∫Ê†º
## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Merge pull request #1 from hankcs/master,"update fork

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
ÂèëÂ∏ÉÁöÑÊ≠£ÂàôÂåñÂ≠óÂÖ∏CharTable.txtÂíåmasterÂàÜÊîØ‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÂèëÁé∞masterÂàÜÊîØÁöÑCharTable.txtÂíå‰∏ãËΩΩÁöÑ data-for-1.6.2.zipÈáåÈù¢ÁöÑ‰∏ç‰∏ÄËá¥ÔºåÂ∏åÊúõ data-for-1.6.2.zipËÉΩÂ§üÂçáÁ∫ß‰∏Ä‰∏ãÔºåÊääÊúÄÊñ∞ÁöÑÂ≠óÂÖ∏Á≠â‰øÆÊîπÂêåÊ≠•‰∏Ä‰∏ã„ÄÇ


"
"ÂàÜËØçÁªìÊûú‰∏≠ÂêçËØçÊÄßËØ≠Á¥†ÁöÑËØçÊÄßÊ†áËØÜ‚ÄúNg‚ÄùÂêåNatureÊûö‰∏æÁ±ª‰∏≠ÁöÑÊ†áËØÜ‚Äúng""‰∏ç‰∏ÄËá¥","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®ÊÑüÁü•Êú∫ÂàÜËØçÂô®Ôºà`PerceptronLexicalAnalyzer`ÔºâÁöÑÂàÜËØçÁªìÊûú‰∏≠ÔºåÂêçËØçÊÄßËØ≠Á¥†ÁöÑËØçÊÄßÊ†áËØÜËæìÂá∫ÊòØ""Ng""Ôºå‰ΩÜÊòØNatureÊûö‰∏æÁ±ª‰∏≠ÂØπÂ∫îÁöÑÊòØNature.ngÔºåÂØπÂ∫îÂ≠óÁ¨¶‰∏≤""ng""„ÄÇËøô‰ºöËæìÂá∫‰∏ÄË°åË≠¶ÂëäÔºö
```
‰∫îÊúà 23, 2018 5:06:34 ‰∏ãÂçà com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
Ë≠¶Âëä: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
```

## Â§çÁé∞ÈóÆÈ¢ò
```
String s = ""‰∏™‰∫∫ÂÖ¨ÁßØÈáëÂ¶Ç‰ΩïÂäûÁêÜÔºü"";
Segment segment = new PerceptronLexicalAnalyzer();
segment.seg(s);
```

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰∏™‰∫∫/n ÂÖ¨ÁßØ/n Èáë/ng Â¶Ç‰Ωï/r ÂäûÁêÜ/v Ôºü/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
‰∏™‰∫∫/n ÂÖ¨ÁßØ/n Èáë/Ng Â¶Ç‰Ωï/r ÂäûÁêÜ/v Ôºü/w
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Nature.ng -> ÂêçËØçÊÄßËØ≠Á¥†

"
ËØçÂÖ∏‰ºòÂåñÔºöÂà†Èô§Ê©ôÂ≠êÂíåÊ©òÂ≠êÁöÑ‰∏çÂêàÁêÜÁöÑÁπÅÁÆÄËΩ¨Êç¢,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ËØçÂÖ∏‰ºòÂåñÔºöÂà†Èô§‚ÄúÊ©ôÂ≠ê‚ÄùÂíå‚ÄúÊ©òÂ≠ê‚ÄùÁöÑ‰∏çÂêàÁêÜÁöÑÁπÅÁÆÄËΩ¨Êç¢

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->
#835 

"
PersonRecognitionÁöÑRecognitionÊñπÊ≥ïÂú®1.6.4ÁâàÊú¨Ê≤°ÊúâÈáçÊûÑÊñπÊ≥ïÂêçÔºåÊòØ‰∏çÊòØÈÅóÊºè‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
![image](https://user-images.githubusercontent.com/23119744/40400951-f3762d72-5e76-11e8-94fe-7249fbdc863a.png)



"
ÁπÅ‰ΩìÁÆÄ‰ΩìËΩ¨Êç¢ËØçÂÖ∏ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
dictionary/other/CharTable.txt ‰∏≠Êúâ‰∏ÄË°å
Ê©ô=Ê©ò

Áî±Ê≠§‰∫ßÁîüÁöÑÈóÆÈ¢òÂ∞±ÊòØÂ¶ÇÊûúÂºÄÂêØ‰∫ÜNormalization=true ÔºàÊàë‰ΩøÁî®ÁöÑÂú∫ÊôØ‰∏≠ÂºÄÂêØËøô‰∏™ÂèÇÊï∞ÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØÂ§ßÂ∞èÂÜôÁªü‰∏ÄÔºâ
‚ÄúÊ©ôÂ≠ê‚ÄùÂàÜËØç‰ª•Âêé‰ºö‰∫ßÁîüÁöÑÁªìÊûúÊòØ‚ÄúÊ©òÂ≠ê‚ÄùÔºå‰ΩÜÊòØËøô‰∏§ÁßçÊ∞¥ÊûúÂπ∂‰∏çÊòØÁõ∏ÂêåÁöÑÊ∞¥Êûú„ÄÇ

Âè¶Â§ñ dictionary/tc/t2s.txt ‰∏≠Êúâ‰∏Ä‰∏™ËØç
Ê©ô=Ê©òÂ≠ê
‰∏çÂ§™Á°ÆÂÆöËøô‰∏™ËØçÂÖ∏ÁöÑÁî®ÈÄîÊòØ‰∏çÊòØÁπÅ‰ΩìÂà∞ÁÆÄ‰ΩìÁöÑËΩ¨Êç¢ÂØπÂ∫îÔºåÂ¶ÇÊûúÊòØÁöÑËØùÔºåÊÑüËßâ‰∏çÂ§™ÂØπÔºå‚ÄúÊ©ô‚ÄùÂπ∂‰∏çÁ≠â‰∫éÁÆÄ‰ΩìÁöÑ‚ÄúÊ©òÂ≠ê‚Äù

## Â§çÁé∞ÈóÆÈ¢ò


### Ëß¶Âèë‰ª£Á†Å
ÈÖçÁΩÆÊñá‰ª∂‰∏≠Âä†ÂÖ•ÔºöNormalization=true

HanLP.segment(""Ê©ôÂ≠ê"")

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

Ê©ôÂ≠ê


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

Ê©òÂ≠ê


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
word2vec ËÆ≠ÁªÉÂ∑•ÂÖ∑ÂèÇÊï∞ÊñáÊ°£ÊèèËø∞‰∏çÂáÜÁ°Æ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

È°πÁõÆ‰∏≠Êèê‰æõÁöÑword2vec ËÆ≠ÁªÉÂ∑•ÂÖ∑ÔºåÊñáÊ°£‰∏≠ÂÖ≥‰∫é-hs Âíå-cbow ÁöÑÂèÇÊï∞ÂÆûÈôÖ‰∏äÂøÖÈ°ªË¶ÅÊúâÊï∞ÂÄºÂèÇÊï∞Ôºà1ÊàñÂÖ∂ÂÆÉÔºâÔºåÂê¶ÂàôËøêË°åÊó∂‰ºöÊä•ÂºÇÂ∏∏Ôºö
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
wiki ‰∏≠ÂÖ≥‰∫éËÆ≠ÁªÉÂ∑•ÂÖ∑ÁöÑÂèÇÊï∞Examples‰∏ä‰πüÊúâÂêåÊ†∑ÁöÑÈóÆÈ¢ò„ÄÇ
Áúã‰∫Ü‰∏Ä‰∏ã‰ª£Á†ÅAbstractTrainer.java  ‰∏≠ÁöÑsetConfigÂáΩÊï∞‰∏≠Ëß£ÊûêÂèÇÊï∞Ôºå-cbow ÂêéÈù¢ÂøÖÈ°ªË∑ü‰∏ä1Êâç‰ºö‰ΩøÁî® cbow Ê®°Âûã„ÄÇ
if ((i = argPos(""-cbow"", args)) >= 0) config.setUseContinuousBagOfWords(Integer.parseInt(args[i + 1]) == 1);


## Â§çÁé∞ÈóÆÈ¢ò
java -cp  hanlp-1.6.3.jar com.hankcs.hanlp.mining.word2vec.Train -input input.txt -output output.txt  -cbow

### ÊúüÊúõËæìÂá∫
ÂºÄÂßãËÆ≠ÁªÉ

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:50)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:40)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.setConfig(AbstractTrainer.java:62)
	at com.hankcs.hanlp.mining.word2vec.Train.execute(Train.java:24)
	at com.hankcs.hanlp.mining.word2vec.Train.main(Train.java:38)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëß¶ÂèëÂºÇÂ∏∏  java.lang.AssertionError: ÊûÑÈÄ†Á©∫ÁôΩËäÇÁÇπ‰ºöÂØºËá¥Ê≠ªÂæ™ÁéØÔºÅ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

ÂàáÂàÜ‰ª•‰∏ãÂè•Â≠êÊä•Èîô
```
ÊúÄÂ§ßÂîêÊÑüÂ•≥Â£∞ÂèåÁ¨ôÊú¨‰∫∫ÁúüÊ≠£ÁöÑÁÖßÁâá ÂèåÁ¨ôÂ≠êÊòØË∞ÅÂèåÁ¨ôÁöÑÁúüÂêçÂè´‰ªÄ‰πà
```


### Ëß¶Âèë‰ª£Á†Å

```
   @Test
    public void errTest(){
        String eStr = ""ÊúÄÂ§ßÂîêÊÑüÂ•≥Â£∞ÂèåÁ¨ôÊú¨‰∫∫ÁúüÊ≠£ÁöÑÁÖßÁâá ÂèåÁ¨ôÂ≠êÊòØË∞ÅÂèåÁ¨ôÁöÑÁúüÂêçÂè´‰ªÄ‰πà"";
        Segment seg = HanLP.newSegment().enableMultithreading(8);
        System.out.println(seg.seg(eStr));
    }
```


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
java.lang.AssertionError: ÊûÑÈÄ†Á©∫ÁôΩËäÇÁÇπ‰ºöÂØºËá¥Ê≠ªÂæ™ÁéØÔºÅ

	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:82)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:214)
	at com.hankcs.hanlp.dictionary.nr.PersonDictionary.parsePattern(PersonDictionary.java:109)
	at com.hankcs.hanlp.recognition.nr.PersonRecognition.Recognition(PersonRecognition.java:67)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:78)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
	at hanlp.CutWord.errTest(CutWord.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
wiki ‰∏≠word2vecÁ´†ËäÇÊèê‰æõÁöÑÁª¥Âü∫ÁôæÁßëÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁöÑËÆ≠ÁªÉÂèÇÊï∞,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ËØ∑ÈóÆwiki ‰∏≠word2vecÁ´†ËäÇÊèê‰æõÁöÑ Áª¥Âü∫ÁôæÁßëÁöÑ[È¢ÑËÆ≠ÁªÉÊï∞ÊçÆ](https://pan.baidu.com/s/1qYFozrY) ÁöÑËÆ≠ÁªÉÂèÇÊï∞ÊòØ‰ªÄ‰πàÔºü

"
Ëá™ÂÆö‰πâËØçÂÖ∏ËØÜÂà´Â§±Ë¥•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [Èí©] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ëá™ÂÆö‰πâËØçÂÖ∏ËØªÂèñÂ§±Ë¥•

## Â§çÁé∞ÈóÆÈ¢ò



### Ê≠•È™§

1.È¶ñÂÖà ÔºöÂú®customÊñá‰ª∂Â§π‰∏ãÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑmy.txt ÂÜÖÂÆπÁöÑÁºñÁ†ÅÊ†ºÂºèÊòØutf-8 ÔºåÂÜÖÂÆπÊòØ ÊîªÂüéÁãÆ nz 1024

2.ÁÑ∂ÂêéÔºöÂú®hanlp.properties‰∏≠ Ê∑ªÂä†
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; my.txt;Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt ns;data/dictionary/person/nrf.txt nrf

3.Âà†Èô§customdictionary.txt.bin     ‰ΩÜÊòØÂπ∂Ê≤°ÊúâÁîüÊàêÊñ∞ÁöÑ

4.ÂºÄÂêØËØÜÂà´ ËØªÂèñÂ§±Ë¥•

### Ëß¶Âèë‰ª£Á†Å
HanLP.segment(""ÊîªÂüéÁãÆÊòØ‰∏Ä‰∏™ÊòåÂπ≥Âåó‰∏ÉÂÆ∂ÈïáÂπ≥Ë•øÁéãÂ∫úÊùëÂá∫"");

ÊúüÊúõËæìÂá∫
ÊîªÂüéÁãÆÂ∫îËØ•ÊòØ‰∏Ä‰∏™Êï¥‰Ωì ÔºåÂõ†‰∏∫ÊàëÂú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Ê∑ªÂä†‰∫Ü

ÂÆûÈôÖËæìÂá∫Ôºö
‰ª•‰∏ãÊòØÊàëÁöÑlog
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreDictionary load
‰ø°ÊÅØ: Ê†∏ÂøÉËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.txt
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
‰ø°ÊÅØ: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.txtÂä†ËΩΩÊàêÂäüÔºå153083‰∏™ËØçÊù°ÔºåËÄóÊó∂59ms
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Êîª, ÊîªÂüé]
2:[Âüé]
3:[ÁãÆ]
4:[ÊòØ]
5:[‰∏Ä‰∏™]
6:[‰∏™]
7:[Êòå, ÊòåÂπ≥]
8:[Âπ≥]
9:[Âåó]
10:[‰∏ÉÂÆ∂Èïá]
11:[ÂÆ∂]
12:[Èïá]
13:[Âπ≥]
14:[Ë•ø, Ë•øÁéã]
15:[Áéã, ÁéãÂ∫ú, ÁéãÂ∫úÊùë]
16:[Â∫ú]
17:[Êùë]
18:[Âá∫]
19:[ ]

‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
‰ø°ÊÅØ: ÂºÄÂßãÂä†ËΩΩ‰∫åÂÖÉËØçÂÖ∏/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.ngram.txt.table
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
‰ø°ÊÅØ: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/CoreNatureDictionary.ngram.txt.tableÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂130ms
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
‰ø°ÊÅØ: Ëá™ÂÆö‰πâËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/custom/CustomDictionary.txt
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary <clinit>
‰ø°ÊÅØ: Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ËΩΩÊàêÂäü:391944‰∏™ËØçÊù°ÔºåËÄóÊó∂384ms
Á≤óÂàÜÁªìÊûú[ÊîªÂüé/vi, ÁãÆ/ng, ÊòØ/vshi, ‰∏Ä‰∏™/mq, ÊòåÂπ≥/ns, Âåó‰∏ÉÂÆ∂Èïá/ns, Âπ≥Ë•ø/nrf, ÁéãÂ∫úÊùë/ns, Âá∫/vf]
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
‰ø°ÊÅØ: /Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/person/nr.txtÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂33ms
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][ÊîªÂüé L 1 ][ÁãÆ D 17 E 5 C 1 ][ÊòØ K 2507 L 2504 M 123 C 10 E 1 ][‰∏Ä‰∏™ K 90 L 84 ][ÊòåÂπ≥ Z 41 ][Âåó‰∏ÉÂÆ∂Èïá A 20833310 ][Âπ≥Ë•ø A 20833310 ][ÁéãÂ∫úÊùë A 20833310 ][Âá∫ K 168 L 68 C 1 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,ÊîªÂüé/L ,ÁãÆ/D ,ÊòØ/L ,‰∏Ä‰∏™/K ,ÊòåÂπ≥/Z ,Âåó‰∏ÉÂÆ∂Èïá/A ,Âπ≥Ë•ø/A ,ÁéãÂ∫úÊùë/A ,Âá∫/K , /K]
‰∫îÊúà 16, 2018 5:02:42 ‰∏ãÂçà com.hankcs.hanlp.dictionary.nr.TranslatedPersonDictionary <clinit>
‰ø°ÊÅØ: Èü≥ËØë‰∫∫ÂêçËØçÂÖ∏/Users/zhouqiang/eclipse-workspace/hanlp/data/dictionary/person/nrf.txtÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂26ms


‰ª•‰∏ãÊâìÂç∞ÁªìÊûú‰∏∫Á©∫
System.out.println(""ÊàëÊòØ""+LexiconUtility.getAttribute(""ÊîªÂüéÁãÆ""));
"
"word ""Ëã•Êûú"" includes a unexpected space","<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
I find a small problem in stopwords.txt. I guess ""Ëã•Êûú"" is right, and ""Ëã•Êûú "" may be error.

"
Êñ∞ËØçÂèëÁé∞ÈáåÁ©∫Â≠óÁ¨¶‰∏≤ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Êñ∞ËØçÂèëÁé∞ÈáåËøô‰∏™[Âú∞Êñπ](https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/mining/word/NewWordDiscover.java#L64)ÈôÑËøëÊúâÊõøÊç¢ÂàÜÂâ≤Á¨¶‰∏∫Á©∫‰∏≤ÁöÑÊ≠•È™§ÔºåÂú®pyhanlpÈáåÂæóÂà∞ÁöÑÁªìÊûú‰∏∫ `ÂìàÂìà\x00` ‰πãÁ±ªÔºåËøôÈáåÊòØÂê¶Â∫îËØ•ÊòØ`""""`ÔºàÁ©∫‰∏≤ÔºåÈïøÂ∫¶‰∏∫0ÔºâËÄåÈùû`""\0""`Ôºà0Âè∑Â≠óÁ¨¶ÔºåÈïøÂ∫¶‰∏∫1Ôºâ?

"
ËøêË°ådemoÊó∂ÂèëÁé∞Êèê‰æõÁöÑËØçÂÖ∏‰∏çÂÆåÊï¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöHanLP-1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP-1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®ËøêË°åÊ∫êÁ†Å‰∏≠ÁöÑtestÁ±ªÊó∂ÂèëÁé∞Êúâ‰∫õËØçÂÖ∏ÊòØÂú®data/test/ÁõÆÂΩï‰∏ãÁöÑÔºå‰ΩÜÊòØÊàë‰∏ãËΩΩ‰∏ãÊù•ÁöÑdataÂåÖ‰∏≠‰∏çÂåÖÂê´ÊúâËøôÈÉ®ÂàÜËØçÂÖ∏ÔºåËØ∑ÈóÆÂú®Âì™ÈáåÂèØ‰ª•Ëé∑ÂèñÂà∞Ëøô‰∫õËØçÂÖ∏Ôºü



"
ÂÖ≥‰∫éword2vecÂä†ËΩΩËØçÂêëÈáèÁöÑÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âä†ËΩΩËØçÂêëÈáèÊó∂ÂèëÁîüÊï∞ÁªÑË∂äÁïå„ÄÇ
```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at word2vec.TestWordVec.main(TestWordVec.java:8)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 300
	at com.hankcs.hanlp.mining.word2vec.VectorsReader.readVectorFile(VectorsReader.java:50)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:38)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
	at com.hrtps.qa.tool.WordVecAPI.<clinit>(WordVecAPI.java:23)
	... 1 more
```
## Â§çÁé∞ÈóÆÈ¢ò
Êü•Áúã‰∫ÜÊ∫êÁ†ÅÁöÑ VectorsReaderÁ±ª
ÂèëÁé∞Âä†ËΩΩÊñáÊú¨ËØçÂêëÈáèÊó∂ÂØπÂ≠óÁ¨¶‰∏≤ËøõË°å‰∫Ü trim Êìç‰ΩúÔºåÊòØÂê¶Â≠òÂú®Â∞Ü‰∏çÂèØËßÅËØçÈ°πÂ§ÑÁêÜÊéâÁöÑÊÉÖÂÜµ  ÂØºËá¥ÂêëÈáèÊó†Ê≥ïÊ≠£Â∏∏ËØªÂèñÂë¢Ôºü


### Ëß¶Âèë‰ª£Á†Å

```java
WordVectorModel wordVectorModel = new WordVectorModel(""msr.txt"");
```


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éÊÑüÁü•Êú∫Âú®Á∫øÂ≠¶‰π†ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Êàë‰ΩøÁî®‰∫ÜÊÑüÁü•Êú∫ÁöÑÂú®Á∫øÂ≠¶‰π†ÂäüËÉΩÔºå‰ΩÜÊòØÊ≤°ÊúâËææÂà∞ÊàëË¶ÅÁöÑÊïàÊûú„ÄÇËØ∑Áúã‰∏Ä‰∏ã‰∏ãÈù¢‰ª£Á†Å
### Ëß¶Âèë‰ª£Á†Å

                  PerceptronSegmenter segmenter = new
		  PerceptronSegmenter(Config.CRFCWSModelPath);
		  segmenter.learn(""‰∏ãÈõ®Â§© Âú∞Èù¢ ÁßØÊ∞¥"");
		  System.out.println(segmenter.segment(""‰∏ãÈõ®Â§©Âú∞Èù¢ÁßØÊ∞¥""));
                  ËøôÊ†∑ÂèØ‰ª•Ê≠£Â∏∏ËæìÂá∫Ôºö[‰∏ãÈõ®Â§©, Âú∞Èù¢, ÁßØÊ∞¥]
                 ‰ΩÜÊòØÂ¶ÇÊûúËøôÊ†∑Ôºö
                 PerceptronLexicalAnalyzer segmenter1 = new
		  PerceptronLexicalAnalyzer(Config.CRFCWSModelPath,
		  Config.CRFPOSModelPath, Config.CRFNERModelPath);
		  Sentence sentence = segmenter1.analyze(""‰∏ãÈõ®Â§©Âú∞Èù¢ÁßØÊ∞¥"");
		  System.out.println(sentence);
                  ËæìÂá∫Â∞±Èîô‰∫ÜÔºö[‰∏ãÈõ®ÔºåÂ§©,Âú∞ÔºåÈù¢,ÁßØÔºåÊ∞¥]
‰∏äÈù¢‰ª£Á†Å‰∏çÊòØÂ∑≤ÁªèÂú®Á∫øÂ≠¶‰π†Ëøá‰∫ÜÔºåÁêÜËÆ∫‰∏äÂ∫îËØ•ÂèØ‰ª•Ê≠£Â∏∏ËæìÂá∫‰∫ÜÔºå‰∏∫‰ªÄ‰πàËæìÂá∫ËøòÊòØÈîôÁöÑÔºüÊàëÁúãÂà∞‰Ω†ÂõûÂ§çËøáÊúâÂ∫èÂàóÂåñÂà∞Ê®°ÂûãÔºüËØ∑ÈóÆËøô‰∏™Â∫îËØ•ÊÄé‰πàÂÅöÔºü

"
ÈááÁî®word2vecÁöÑapiÊé•Âè£ËÆ≠ÁªÉÊñ∞Ê®°ÂûãÂÆåÊàêÊó∂Êä•java.lang.ArrayIndexOutOfBoundsException: 200ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÈááÁî®Word2VecTrainerËÆ≠ÁªÉËá™Â∑±ÁöÑÊ®°ÂûãÊó∂ÔºåËôΩÁÑ∂ËÉΩÂ§üÂÆåÊàêËÆ≠ÁªÉÔºå‰ΩÜÊúÄÂêé‰ºöÊï∞ÁªÑË∂äÁïåÈîôËØØÔºåÊàë‰πüÁøªÈòÖ‰∫ÜissuesÂå∫ÁöÑÁõ∏ÂÖ≥Êï∞ÁªÑË∂äÁïåÁöÑËØùÈ¢òÔºå‰ΩÜËøòÊòØÊ≤°ÊúâÊâæÂà∞Ëß£ÂÜ≥ÂäûÊ≥ïÔºõÂú®ÂºïÁî®Êñ∞Ê®°ÂûãËøõË°åÊñáÊú¨ÂàÜÁ±ªÊó∂‰πü‰ºöÊä•Êï∞ÁªÑË∂äÁïåÁöÑÈîôËØØÔºåÊ±ÇÊåáÂØº~

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÊàëÂèÇËÄÉwiki‰∏äÂÜô‰∫Ü‰∏Ä‰∏™Word2VecTrainÁöÑÁ±ªÔºõ
2. ÈáåÈù¢Ê≥®ÂÜå‰∫Ü‰∏Ä‰∏™ÂõûË∞ÉÂáΩÊï∞callbackLPÔºåÁÑ∂ÂêéÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ËÆ≠ÁªÉtrainerBuilderÔºåÂè™ËÆæÁΩÆ‰∫Ü‰∏äÈù¢ÁöÑÂõûË∞ÉÂáΩÊï∞ÂíåÂπ∂Ë°åÁ∫øÁ®ãÊï∞‰∏∫4ÔºàÂú®ÂçïÊú∫‰∏äÊµãËØïÔºåÂ∏åÊúõËÉΩÂ§üÂä†Âø´ËÆ≠ÁªÉÔºâ
3. ÂÖ∑‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ãÔºö

### Ëß¶Âèë‰ª£Á†Å

```
public class Word2VecTrain {

    public static void main(String[] args) {

        TrainingCallback callbackLP = null;
        final long timeStart = System.currentTimeMillis();
        if (callbackLP == null){
            callbackLP = new TrainingCallback() {
                public void corpusLoading(float percent) {
                    System.out.printf(""\rÂä†ËΩΩËÆ≠ÁªÉËØ≠ÊñôÔºö%.2f%%"", percent);
                }

                public void corpusLoaded(int vocWords, int trainWords, int totalWords) {
                    System.out.println();
                    System.out.printf(""ËØçË°®Â§ßÂ∞èÔºö%d\n"", vocWords);
                    System.out.printf(""ËÆ≠ÁªÉËØçÊï∞Ôºö%d\n"", trainWords);
                    System.out.printf(""ËØ≠ÊñôËØçÊï∞Ôºö%d\n"", totalWords);
                }

                public void training(float alpha, float progress) {
                    System.out.printf(""\rÂ≠¶‰π†ÁéáÔºö%.6f  ËøõÂ∫¶Ôºö%.2f%%"", alpha, progress);
                    long timeNow = System.currentTimeMillis();
                    long costTime = timeNow - timeStart + 1;
                    progress /= 100;
                    String etd = Utility.humanTime((long) (costTime / progress * (1.f - progress)));
                    if (etd.length() > 0) System.out.printf(""  Ââ©‰ΩôÊó∂Èó¥Ôºö%s"", etd);
                    System.out.flush();
                }
            };
        }

//      ÊûÑÂª∫ËÆ≠ÁªÉÊñπÊ≥ï
        Word2VecTrainer trainerBuilder = new Word2VecTrainer();

        trainerBuilder.setCallback(callbackLP);
        trainerBuilder.useNumThreads(4);

        WordVectorModel wordVectorModel = trainerBuilder.train
                (""D://DESKTOP//20180427//data-for-1.6.2//data//test//koubei_classify_seg.txt"",
                ""D://DESKTOP//20180427//data-for-1.6.2//data//test//msr_koubei_vectors_default.txt"");
    }
}

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Âä†ËΩΩËÆ≠ÁªÉËØ≠ÊñôÔºö100.00%
ËØçË°®Â§ßÂ∞èÔºö24845
ËÆ≠ÁªÉËØçÊï∞Ôºö2269300
ËØ≠ÊñôËØçÊï∞Ôºö2368390
Â≠¶‰π†ÁéáÔºö0.000163  ËøõÂ∫¶Ôºö99.67%  Ââ©‰ΩôÊó∂Èó¥Ôºö01 s
ËÆ≠ÁªÉÁªìÊùüÔºå‰∏ÄÂÖ±ËÄóÊó∂Ôºö1 m 54 s 
Ê≠£Âú®‰øùÂ≠òÊ®°ÂûãÂà∞Á£ÅÁõò‰∏≠‚Ä¶‚Ä¶
Ê®°ÂûãÂ∑≤‰øùÂ≠òÂà∞Ôºömsr_koubei_vectors_default.txt
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Âä†ËΩΩËÆ≠ÁªÉËØ≠ÊñôÔºö100.00%
ËØçË°®Â§ßÂ∞èÔºö176977
ËÆ≠ÁªÉËØçÊï∞Ôºö32511717
ËØ≠ÊñôËØçÊï∞Ôºö32511717

Â≠¶‰π†ÁéáÔºö0.000005  ËøõÂ∫¶Ôºö100.00%

ËÆ≠ÁªÉÁªìÊùüÔºå‰∏ÄÂÖ±ËÄóÊó∂Ôºö1 h 48 m 15 s 
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 200
	at com.hankcs.hanlp.mining.word2vec.VectorsReader.readVectorFile(VectorsReader.java:50)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.loadVectorMap(WordVectorModel.java:38)
	at com.hankcs.hanlp.mining.word2vec.WordVectorModel.<init>(WordVectorModel.java:32)
	at com.hankcs.hanlp.mining.word2vec.Word2VecTrainer.train(Word2VecTrainer.java:221)
	at com.autohome.Word2VecTrain.main(Word2VecTrain.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

ÂÆûÈôÖËæìÂá∫ÂÖ•‰∏ãÂõæ
```
http://attachbak.dataguru.cn/attachments/album/201805/11/163829w8bczfibvazzcbfu.png


## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
a) ‰∏™‰∫∫Êé®ÊµãÔºöÊòØ‰∏çÊòØÂíåÁºìÂ≠òÊúâÂÖ≥ÔºüÂõ†‰∏∫ÊàëÁ¨¨‰∏ÄÊ¨°ËøêË°åËÆ≠ÁªÉÁ®ãÂ∫èÊó∂ÔºåËÆæÁΩÆ‰∫ÜÁª¥Â∫¶ÊòØ200Ôºå‰ΩÜ‰πüÊä•ÈîôÔºåÂêéÊù•Êîπ‰∏∫ÈªòËÆ§ËÆæÁΩÆÔºàÈªòËÆ§Áª¥Â∫¶Â∫îËØ•ÊòØ100ÔºâÔºå‰πüÊòØÊä•Êï∞ÁªÑË∂äÁïå200ÁöÑÂºÇÂ∏∏Ôºå‰∏çÁü•ÈÅìÊòØÂê¶ÂíåËÆ°ÁÆóÁºìÂ≠òÊúâÂÖ≥Ôºü
b) Âè¶Â§ñÔºåÂú®Ë∞ÉÁî®Ëøô‰∏™Êñ∞Ê®°ÂûãÊó∂‰πü‰ºöÊä•Êï∞ÊçÆË∂äÁïåÁöÑÂºÇÂ∏∏ÔºåÊ≤°ÊúâÂè¶ÂºÄissueÔºåÊàëÊÉ≥ÊòØÊ®°ÂûãÊú¨Ë∫´Ê≤°ÊúâËÆ≠ÁªÉÊàêÂäüÁöÑÁºòÊïÖÔºåÂè™ÊòØÁúãÊú¨Âú∞Êñá‰ª∂Ë≤å‰ºº‰πüÊ≠£Â∏∏ÔºåÊâÄ‰ª•Â∞ùËØïÁùÄË∞ÉÁî®‰∫Ü‰∏Ä‰∏ãÊñ∞Ê®°ÂûãÔºå‰ΩÜÂêåÊ†∑Êä•Êï∞ÊçÆË∂äÁïåÂºÇÂ∏∏„ÄÇ
c) ËøòÊúâ‰∏Ä‰∏™ÈóÆÈ¢òÔºåÂ∞±ÊòØwikiÈáåÂÜôÁöÑËÆ≠ÁªÉÊ®°Âûã‰øùÂ≠òÊñá‰ª∂Ê†ºÂºèÊòØ .binÔºå‰ΩÜÊòØ DemoWord2VecÈáåÂä†ËΩΩÊ®°ÂûãÊòØÁî®ÁöÑÊñá‰ª∂Ê†ºÂºèÊòØ .txtÔºåÊâÄ‰ª•‰∏çÁü•ÈÅìËøô‰∏§‰∏™‰πãÈó¥ÊúâÊ≤°ÊúâÂÖ∂‰ªñÊñπÊ≥ïÂèØ‰ª•ËΩ¨Êç¢ÊàñËÄÖÁîüÊàêÂë¢Ôºü"
Êï∞Èáè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊîØÊåÅ.csv,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

‰ΩøAhoCorasickDoubleArrayTrieSegmentÊîØÊåÅËÆÄÂèñcsv

## Áõ∏ÂÖ≥issue
ÁÑ°

"
Âè•Ê≥ïÂàÜÊûêÂÖ≥ÈîÆËØçÊèêÂèñÂç†ÂéªÂÜÖÂ≠òÂæàÂ§ß,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ÊâìÈí© ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö0.1.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö0.1.41

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊâßË°åget_relations_pos_head(sentence)‰ºöÂç†Áî®ÂÖ≠ÁôæÂ§öÂÖÜÁöÑÂÜÖÂ≠òÔºå ÂØºËá¥Â§öÁ∫øÁ®ãÂá∫Áé∞ÂÜÖÂ≠òÊ∫¢Âá∫Ôºå javaËôöÊãüÊú∫ËÆæÁΩÆÁöÑxms‰∏∫1g,  xmx‰∏∫1536m

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âè•Ê≥ïÂàÜÊûêÊèêÂèñÂÖ≥ÈîÆËØçÔºåÈááÁî®Â§öÁ∫øÁ®ãÔºå‰ºöÂá∫Áé∞ÂÜÖÂ≠òÊ∫¢Âá∫,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ÊâìÈí© ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö0.1.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö0.1.41

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÈááÁî®pyhanlp
Âä†ËΩΩ‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏Ôºå ÈááÁî®Â§öÁ∫øÁ®ãÔºåÂêåÊó∂ÂØπÂØπÂè•ËØùËøõË°åÂè•Ê≥ïÂàÜÊûêÂÖ≥ÈîÆËØçÊèêÂèñÊó∂‰ºöÂá∫Áé∞ÂÜÖÂ≠òÊ∫¢Âá∫ÈóÆÈ¢ò
word_array = HanLP.parseDependency(sentence).getWordArray()
jpype._jexception.java.lang.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: Java heap space


### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
relations, pos, head, words = get_relations_pos_head(sentence)
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºö‰ΩøÁî®NeuralNetworkDependencyParserÂØπÁõ∏ÂêåËØçÊÄßÁªìÊûÑÂè•Â≠êÂæóÂà∞‰∏çÂêå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁªìÊûú,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.3ÔºàÈÄöËøá maven ÈÖçÁΩÆÁöÑÁâàÊú¨Ôºâ

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπ‰∫éÁõ∏ÂêåËØçÊÄßÁªìÊûÑÁöÑÂè•Â≠ê<Êú∫ÊûÑ>ÁöÑ<‰øÆÈ•∞ÊÄßÂêçËØç><‰∫∫Âêç>Ôºå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
Áõ¥Êé•Ë∞ÉÁî®parser NeuralNetworkDependencyParserÔºàÂπ∂Êú™ËøõË°å‰ªª‰Ωï‰øÆÊîπÔºâÔºåÂØπ‰ª•‰∏ãÂè•Â≠êËøõË°å‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºö
1.‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÂÖ¨Âè∏ÁöÑÂëòÂ∑•Èü©Ê¢ÖÊ¢Ö
2.‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÂÖ¨Âè∏ÁöÑËë£‰∫ãÈü©Ê¢ÖÊ¢Ö
ÂæóÂà∞ÁöÑ‰æùÂ≠òÂàÜÊûêÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ‰∏çÁü•ÈÅìÊòØÂê¶ÊúâÊîπËøõÊñπÊ≥ïÔºü

### Ëß¶Âèë‰ª£Á†Å
```
    public void testIssue1234() throws Exception
    {
        IDependencyParser parser = new NeuralNetworkDependencyParser().enableDeprelTranslator(false);
        CoNLLSentence sentenceParsed1 = parser.parse(""‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÂÖ¨Âè∏ÁöÑÂëòÂ∑•Èü©Ê¢ÖÊ¢Ö"");
        CoNLLWord[] arcs1 = sentenceParsed1.word;
        for (CoNLLWord word : arcs1) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
        System.out.println();
        CoNLLSentence sentenceParsed2 = parser.parse(""‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÂÖ¨Âè∏ÁöÑËë£‰∫ãÈü©Ê¢ÖÊ¢Ö"");
        CoNLLWord[] arcs2 = sentenceParsed2.word;
        for (CoNLLWord word : arcs2) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
    }
```
### ÊúüÊúõËæìÂá∫
```
‰∏äÊµ∑(1) --(ATT)--> ÂçéÂÆâ(2)
ÂçéÂÆâ(2) --(ATT)--> ÂÖ¨Âè∏(4)
Â∑•‰∏ö(3) --(ATT)--> ÂÖ¨Âè∏(4)
ÂÖ¨Âè∏(4) --(ATT)--> ÂëòÂ∑•(6)
ÁöÑ(5) --(RAD)--> ÂÖ¨Âè∏(4)
ÂëòÂ∑•(6) --(ATT)--> Èü©Ê¢ÖÊ¢Ö(7)
Èü©Ê¢ÖÊ¢Ö(7) --(HED)--> ##Ê†∏ÂøÉ##(0)

‰∏äÊµ∑(1) --(ATT)--> ÂçéÂÆâ(2)
ÂçéÂÆâ(2) --(ATT)--> ÂÖ¨Âè∏(4)
Â∑•‰∏ö(3) --(ATT)--> ÂÖ¨Âè∏(4)
ÂÖ¨Âè∏(4) --(ATT)--> Ëë£‰∫ã(6) //ÂØπÂ∫î""ÂëòÂ∑•""
ÁöÑ(5) --(RAD)--> ÂÖ¨Âè∏(4)
Ëë£‰∫ã(6) --(ATT)--> Èü©Ê¢ÖÊ¢Ö(7)
Èü©Ê¢ÖÊ¢Ö(7) --(HED)--> ##Ê†∏ÂøÉ##(0)
```

### ÂÆûÈôÖËæìÂá∫
‰ΩÜÂÆûÈôÖËæìÂá∫ÁöÑ‰æùÂ≠òÂàÜÊûêÔºå‰∏äÊµ∑ÂçéÂÆâÂ∑•‰∏öÂÖ¨Âè∏ÁöÑATTË∑ØÂæÑ‰∏ç‰∏ÄËá¥Ôºå1.ÊåáÂêë‚ÄúÂëòÂ∑•‚Äù‰øÆÈ•∞ÊÄßÂêçËØçÔºå2.ÊåáÂêë‚ÄúÈü©Ê¢ÖÊ¢Ö‚ÄùÂÆû‰Ωì„ÄÇ

```
‰∏äÊµ∑(1) --(ATT)--> ÂçéÂÆâ(2)
ÂçéÂÆâ(2) --(ATT)--> ÂÖ¨Âè∏(4)
Â∑•‰∏ö(3) --(ATT)--> ÂÖ¨Âè∏(4)
ÂÖ¨Âè∏(4) --(ATT)--> ÂëòÂ∑•(6)
ÁöÑ(5) --(RAD)--> ÂÖ¨Âè∏(4)
ÂëòÂ∑•(6) --(ATT)--> Èü©Ê¢ÖÊ¢Ö(7)
Èü©Ê¢ÖÊ¢Ö(7) --(HED)--> ##Ê†∏ÂøÉ##(0)

‰∏äÊµ∑(1) --(ATT)--> ÂçéÂÆâ(2)
ÂçéÂÆâ(2) --(ATT)--> ÂÖ¨Âè∏(4)
Â∑•‰∏ö(3) --(ATT)--> ÂÖ¨Âè∏(4)
ÂÖ¨Âè∏(4) --(ATT)--> Èü©Ê¢ÖÊ¢Ö(7) //‰∏çÂØπÂ∫î""ÂëòÂ∑•""
ÁöÑ(5) --(RAD)--> ÂÖ¨Âè∏(4)
Ëë£‰∫ã(6) --(ATT)--> Èü©Ê¢ÖÊ¢Ö(7)
Èü©Ê¢ÖÊ¢Ö(7) --(HED)--> ##Ê†∏ÂøÉ##(0)
```"
Ê†∏ÂøÉËØçÂ∫ìÊñá‰ª∂CoreNatureDictionary.txtÔºåÂ§èÂ§© ËØçÊÄßÊúâÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

![image](https://user-images.githubusercontent.com/23119744/39564110-6f82598c-4ee5-11e8-94f1-cf832429612b.png)
Â§èÂ§©ËØçÊÄßÊòØnsÔºåÂÆûÈôÖÂ∫îËØ•ÊòØt
Êò•Â§©ÔºåÁßãÂ§©ÔºåÂÜ¨Â§©ÈÉΩÊòØËØçÊÄßÈÉΩÊòØt

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫é HanLP.segment() ÁöÑÂá†‰∏™ÈóÆÈ¢òÔºà‰∏ªË¶ÅÂõ¥ÁªïËá™ÂÆö‰πâËØçÂÖ∏„ÄÅËØçÊÄß‰∏éËØçÈ¢ëÔºâ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.6.3 ÔºàÈÄöËøá maven ÈÖçÁΩÆÁöÑÁâàÊú¨Ôºâ

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

1. ÊàëÂ∑≤Áªè‰∫ÜËß£Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Êã•Êúâ‰ª•‰∏ãÁâπÊÄßÔºö
Ôºà1Ôºâ„ÄÅÂä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØç‰∏ç‰∏ÄÂÆö‰ºöÂàÜÂá∫Êù•ÔºåÂõ†‰∏∫HanLP.segment‰ºöÂü∫‰∫éÁªüËÆ°Ê®°Âûã„ÄÅËØ≠Êñô‰∏éÊ†∏ÂøÉËØçÂÖ∏‰∏éËØçÈ¢ëÁ≠â‰ø°ÊÅØÂØπÂè•Â≠êËøõË°åÂàáÂàÜÔºåÂ¶ÇÂ∞Ü‚ÄúÂõΩÊñ∞ËÉΩÊ∫ê‚ÄùÂä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÂêéÔºåÂàÜËØçÂπ∂‰∏ç‰ºöÂ∞Ü‚ÄúÊàëÂõΩÊñ∞ËÉΩÊ∫êË°å‰∏ö‚ÄùÂàÜÊàê‚ÄúÊàë/ÂõΩÊñ∞ËÉΩÊ∫ê/Ë°å‰∏ö‚ÄùÔºåËÄåÊòØÂàÜÊàêÊ≠£Á°ÆÁöÑ‚ÄúÊàëÂõΩ/Êñ∞ËÉΩÊ∫ê/Ë°å‰∏ö‚ÄùÔºå‰ΩÜÂ¶Ç‚ÄúÂõΩÊñ∞ËÉΩÊ∫êËÇ°Á•®Â§ßÊ∂®‚ÄùÂàô‰ºöÂàÜ‰∏∫‚ÄúÂõΩÊñ∞ËÉΩÊ∫ê/ËÇ°Á•®/Â§ß/Ê∂®‚ÄùÔºåËøôÊòØÈùûÂ∏∏ÁêÜÊÉ≥ÁöÑÊïàÊûú„ÄÇ
Ôºà2Ôºâ„ÄÅÂä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØç„ÄÅËØçÊÄß„ÄÅËØçÈ¢ëÔºåÂú®ÂàÜËØçÂêéÔºåÂÖ∂‰∏≠ËØçÊÄß‰ºöË¶ÜÁõñÊéâÂÖ∂ÂéüÊúâÁöÑËØçÊÄß„ÄÇÂ¶ÇÂ∞Ü‚ÄúÂõΩÂÆ∂ Country 1‚ÄùÂä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÂêéÔºå‰ºöÂ∞Ü‚ÄúÂõΩÂÆ∂/n‚ÄùÂèò‰∏∫‚ÄúÂõΩÂÆ∂/Country‚Äù„ÄÇ
Ôºà3Ôºâ„ÄÅËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑËØçÈ¢ëÔºàÂÖ∑‰ΩìÊï∞ÂÄºÔºâËÆæÁΩÆÔºåÂπ∂Ê≤°ËÉΩËµ∑Âà∞ÁêÜÊÉ≥ÁöÑ‰ΩúÁî®ÔºàÊåáË∞ÉÈ´òËØçÈ¢ëÂêéÔºå‰ºöÂ∞ÜÂÖ∂ÂàÜÂá∫ÔºâÔºåÂ¶ÇÊàëÂ∞Ü‚ÄúÂõΩÊñ∞ËÉΩÊ∫ê Stock 9999999‚Äù Âä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂπ∂Êú™ËÉΩÊîπÂèò‚ÄúÊàëÂõΩ/Êñ∞ËÉΩÊ∫ê/Ë°å‰∏ö‚ÄùÁöÑÂàÜËØçÁªìÊûúÔºå‰∏é‚ÄúÂõΩÊñ∞ËÉΩÊ∫ê stock 1‚Äù ÁöÑÊïàÊûúÊòØÁõ∏ÂêåÁöÑÔºåÊâÄ‰ª•‰ªäÂêéÂú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ËÆæÁΩÆËØçÈ¢ëÊó∂ÔºåÂèØ‰ª•ÈÄâÊã© 1 „ÄÇ
‰∏äËø∞Ôºà1Ôºâ„ÄÅÔºà2Ôºâ„ÄÅÔºà3ÔºâÁÇπÊàëÁöÑÁêÜËß£ÊñπÂêëÂ§ßËá¥Ê≠£Á°ÆÂêóÔºüÔºàÁõ∏ÂÖ≥‰ª£Á†ÅÂú®‰∏ãÈù¢Ôºâ

2. ÊàëÂú®ËØïÈ™åÊó∂ÔºåÂ∞Ü‚ÄúÈæô‰∏â  Conception 1‚Äù‰∏é‚ÄúÂõΩÂÖÉËØÅÂà∏ Stock 1‚ÄùÂä†ÂÖ•Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂêéÔºåÂú®‰∏çÂêåÂè•Â≠ê‰∏≠Âπ∂‰∏ç‰∏ÄÂÆöËÉΩÂ∞ÜÂÖ∂ÂàÜÂá∫ÔºåÂ∞ÜÂÖ∂ËØçÈ¢ëË∞ÉÈ´òËá≥9999999‰πü‰∏çËÉΩÂ∞ÜÂÖ∂ÂàÜÂá∫ÔºåÊàë‰∫ÜËß£Âà∞Â¶ÇÊûúÂº∫Âà∂‰ΩøÁî®Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ËÄåÂøΩÁï•ÊéâÊ†∏ÂøÉËØçÂÖ∏ÁöÑËØùÔºåÊòØÂèØ‰ª•ËææÂà∞Â∞ÜËØçÂá∫ÁöÑÊïàÊûúÔºå‰ΩÜÊ≠£Â¶Ç hanks ÊâÄËØ¥ÔºåÂÆÉ‰ºöÂú®Êüê‰∏™Êú™Áü•ÁöÑ‰ΩçÁΩÆÂºïÂèëÂÖ∂ÂÆÉÊú™Áü•ÔºàÊú™ÂèëÁé∞ÔºâÁöÑÈîôËØØ„ÄÇ
Â¶ÇÊûúÊàëÂ∞±ÊÉ≥ÈíàÂØπËøô‰∏™ÂèëÁé∞ÁöÑ‰∏çËÉΩÂàÜÂá∫ÁöÑËØç‰ΩúÂ§ÑÁêÜÔºåÂπ∂‰∏çÊÉ≥Â∞ÜÊâÄÊúâÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØçÈÉΩ‰∏ÄÂÆöÂàÜÂá∫Ôºå‰πüÂ∞±ÊòØ‰ªçÁÑ∂ÈúÄË¶ÅÊ†∏ÂøÉËØçÂÖ∏‰ø°ÊÅØÁ≠âÔºåËØ•Â¶Ç‰ΩïÂ§ÑÁêÜÔºüÔºàÁõ∏ÂÖ≥‰ª£Á†ÅÂú®‰∏ãÈù¢Ôºâ

3. Â¶Ç‰ΩïÂà§Êñ≠‰∏Ä‰∏™ËØçÊòØÂê¶Âú®Â∑≤Áü•ÁöÑËØçÂÖ∏‰∏≠ÔºüÂè™ËÉΩÂéª CoreNatureDictionary.txt ‰∏≠ÊâæÂêóÔºüÂõ†‰∏∫ÊàëÊúâ‰∏Ä‰∏™ÊÉ≥Ê≥ïÊòØÔºåÂú®Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏Êó∂ÔºåÂÖàÂà§Êñ≠Ëøô‰∏™ËØçÊòØÂê¶Â∑≤Âú® HanLP.segment ÁöÑËØçÂÖ∏‰∏≠ÔºåÂ¶ÇÊûú‰∏çÂú®ÔºåÊàëÂÜçÊ∑ªÂä†Âπ∂Â∞ÜËØçÈ¢ëËÆæÁΩÆ‰∏∫1ÔºåËøô‰πàÂÅöÁöÑÁõÆÁöÑÊòØ‰∏çÊÉ≥Êõ¥ÊîπÂéüÊúâÁöÑËØçÊÄßÂíåËØçÈ¢ëÔºåÂØºËá¥‰∏Ä‰∫õ‰∏çÂøÖË¶ÅÁöÑÈîôËØØÔºåÂõ†‰∏∫ÊàëËßâÂæóÔºåHanLP.segment Âú®ÁªùÂ§ßÂ§öÊï∞ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÂàÜËØçÊòØÊ≤°ÊúâÈóÆÈ¢òÁöÑ„ÄÇ

4. Âú®Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏Êó∂ÔºåÂ¶ÇÊûúËøô‰∏™ËØçÂ∑≤ÁªèÂú®ËØçÂÖ∏‰∏≠ÔºåÂ¶Ç‰ΩïÂè™Êõ¥ÊîπÊ≠§ËØçÁöÑËØçÊÄßÔºüËøô‰πàÂÅöÁöÑÁõÆÁöÑÊòØÊÉ≥‰øùÁïôÂéüÊúâËØçÁöÑËØçÈ¢ë„ÄÇÈöæÈÅìÈúÄË¶ÅÂèñÂà∞ËØ•ËØçÂú®ÂéüÊúâËØçÂÖ∏‰∏≠ÁöÑËØçÈ¢ëÔºåÁÑ∂ÂêéÂÜçÂêåÊñ∞ËØçÊÄß‰∏ÄËµ∑Âä†ÂÖ•Âà∞Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÂêóÔºüËøôÊ†∑ÔºåÊñ∞ËØçÊÄßÂ∞±‰ºöË¶ÜÁõñÊéâÂéüÊúâËØçÊÄß‰∫Ü„ÄÇ

5. `CustomDictionary.insert(""ÂõΩÊñ∞ËÉΩÊ∫ê"", ""hy 1"");`‰∏é`CustomDictionary.add(""ÂõΩÊñ∞ËÉΩÊ∫ê"", ""hy 1"");`Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü‚ÄúÂº∫Ë°åÂä†ÂÖ•‚ÄùÂíå‚ÄúÂä†ÂÖ•‚ÄùÔºü

6. `offset`Â±ûÊÄßÂè™ÊúâÂú®`IndexTokenizer.segment`‰∏≠Ëµ∑‰ΩúÁî®ÂêóÔºüÂú®`HanLP.segment`‰∏≠ÂàÜËØçÂêéÊâÄÊúâËØçÁöÑ`offset`ÂÄºÈÉΩ‰∏∫0ÔºüÔºàÁõ∏ÂÖ≥‰ª£Á†ÅÂú®‰∏ãÈù¢Ôºâ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
Âú®ÈÄöËøá maven ÊñπÂºèÈÖçÁΩÆ‰∫Ü hanlp-portable-1.6.3 ÂêéÔºåÂπ∂Êú™ÂÅö‰ªª‰Ωï‰øÆÊîπÔºåÊâÄÊúâÁöÑËá™ÂÆö‰πâËØçÂÖ∏Êìç‰ΩúÈÉΩÊòØÈÄöËøá‰ª£Á†ÅÂä®ÊÄÅÊ∑ªÂä†Êù•ËøõË°åËØïÈ™åÁöÑ„ÄÇ
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å
```
    //ÈóÆÈ¢ò1‰∏ãÁöÑÔºà1Ôºâ
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""ÂõΩÊñ∞ËÉΩÊ∫ê"", ""Stock 1"");
        System.out.println(HanLP.segment(""ÊàëÂõΩÊñ∞ËÉΩÊ∫êË°å‰∏ö""));
        System.out.println(HanLP.segment(""ÂõΩÊñ∞ËÉΩÊ∫êËÇ°Á•®Â§ßÊ∂®""));
    }
    ÊúüÊúõËæìÂá∫‰∏éÂÆûÈôÖËæìÂá∫‰∏ÄËá¥Ôºö
        [ÊàëÂõΩ/n, Êñ∞ËÉΩÊ∫ê/nz, Ë°å‰∏ö/n]
        [ÂõΩÊñ∞ËÉΩÊ∫ê/Stock, ËÇ°Á•®/n, Â§ß/a, Ê∂®/v]
```
```
    //ÈóÆÈ¢ò1‰∏ãÁöÑÔºà2Ôºâ
    public static void main(String[] args) 
    {
        System.out.println(HanLP.segment(""ÂÆ∂Â∫≠„ÄÅÁßÅÊúâÂà∂ÂíåÂõΩÂÆ∂ÁöÑËµ∑Ê∫ê""));
        CustomDictionary.insert(""ÂõΩÂÆ∂"", ""Country 1"");
        System.out.println(HanLP.segment(""ÂÆ∂Â∫≠„ÄÅÁßÅÊúâÂà∂ÂíåÂõΩÂÆ∂ÁöÑËµ∑Ê∫ê""));
    }
    ÊúüÊúõËæìÂá∫‰∏éÂÆûÈôÖËæìÂá∫‰∏ÄËá¥Ôºö
        [ÂÆ∂Â∫≠/n, „ÄÅ/w, ÁßÅÊúâÂà∂/n, Âíå/c, ÂõΩÂÆ∂/n, ÁöÑ/uj, Ëµ∑Ê∫ê/n]
        [ÂÆ∂Â∫≠/n, „ÄÅ/w, ÁßÅÊúâÂà∂/n, Âíå/c, ÂõΩÂÆ∂/Country, ÁöÑ/uj, Ëµ∑Ê∫ê/n]
```
```
    //ÈóÆÈ¢ò1‰∏ãÁöÑÔºà3Ôºâ
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""ÂõΩÊñ∞ËÉΩÊ∫ê"", ""Stock 9999999"");
        System.out.println(HanLP.segment(""Â§ßËµûÊàëÂõΩÊñ∞ËÉΩÊ∫ê""));
    }
    ÂÆûÈôÖËæìÂá∫Ôºö
        [Â§ßËµû/nrf, ÊàëÂõΩ/n, Êñ∞ËÉΩÊ∫ê/nz]
    ÊúüÊúõËæìÂá∫Ôºö
        [Â§ßËµû/nrf, Êàë/r, ÂõΩÊñ∞ËÉΩÊ∫ê/Stock]
```
```
    //ÈóÆÈ¢ò2
    public static void main(String[] args) 
    {
        CustomDictionary.insert(""Èæô‰∏â"", ""Conception 1"");
        CustomDictionary.insert(""ÂõΩÂÖÉËØÅÂà∏"", ""Stock 1"");
        System.out.println(HanLP.segment(""Èæô‰∏âÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏âÂíåÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏â‰∏éÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏âÂèäÂõΩÂÖÉËØÅÂà∏""));

        System.out.println(""\n"");

        CustomDictionary.insert(""Èæô‰∏â"", ""Conception 9999999"");
        CustomDictionary.insert(""ÂõΩÂÖÉËØÅÂà∏"", ""Stock 9999999"");
        System.out.println(HanLP.segment(""Èæô‰∏âÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏âÂíåÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏â‰∏éÂõΩÂÖÉËØÅÂà∏""));
        System.out.println(HanLP.segment(""Èæô‰∏âÂèäÂõΩÂÖÉËØÅÂà∏""));
    }
    ÂÆûÈôÖËæìÂá∫Ôºö
        [Èæô‰∏âÂõΩ/nr, ÂÖÉ/q, ËØÅÂà∏/n]
        [Èæô/n, ‰∏âÂíå/nz, ÂõΩÂÖÉËØÅÂà∏/Stock]
        [Èæô‰∏â/Conception, ‰∏é/p, ÂõΩÂÖÉËØÅÂà∏/Stock]
        [Èæô‰∏â/Conception, Âèä/c, ÂõΩÂÖÉËØÅÂà∏/Stock]
    ÊúüÊúõËæìÂá∫Ôºö
        [Èæô‰∏â/Conception, ÂõΩÂÖÉËØÅÂà∏/Stock]
        [Èæô‰∏â/Conception, Âíå/c, ÂõΩÂÖÉËØÅÂà∏/Stock]
        [Èæô‰∏â/Conception, ‰∏é/p, ÂõΩÂÖÉËØÅÂà∏/Stock]
        [Èæô‰∏â/Conception, Âèä/c, ÂõΩÂÖÉËØÅÂà∏/Stock]
```
```
    //ÈóÆÈ¢ò3„ÄÅ4‰∏∫ÊÄùË∑ØÊÄßÈóÆÈ¢òÔºåÈóÆÈ¢ò5 Â∞Ü CustomDictionary.insert Êç¢‰∏∫ CustomDictionary.add Êó†ÂèòÂåñ
```
```
    //ÈóÆÈ¢ò6
    public static void main(String[] args) 
    {
        List<Term> termList = HanLP.segment(""ÊâìÂ∑•ÊòØ‰∏çÂèØËÉΩÊâìÂ∑•ÁöÑ"");
        System.out.println(termList);
        for (Term term : termList){
            System.out.println(""offset: "" + term.offset);
            System.out.println(""wordLength: "" + term.word.length());
        }
    }
    ÂÆûÈôÖËæìÂá∫Ôºö
        [ÊâìÂ∑•/v, ÊòØ/v, ‰∏ç/d, ÂèØËÉΩ/v, ÊâìÂ∑•/v, ÁöÑ/uj]
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 1
        offset: 0
        wordLength: 1
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 2
        offset: 0
        wordLength: 1
    ÊúüÊúõËæìÂá∫Ôºö
        [ÊâìÂ∑•/v, ÊòØ/v, ‰∏ç/d, ÂèØËÉΩ/v, ÊâìÂ∑•/v, ÁöÑ/uj]
        offset: 0
        wordLength: 2
        offset: 2
        wordLength: 1
        offset: 3
        wordLength: 1
        offset: 4
        wordLength: 2
        offset: 6
        wordLength: 2
        offset: 8
        wordLength: 1
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Â¶Ç‰ΩïÊ†πÊçÆ HanLP.Config.enableDebug() Êù•ÂàÜÊûêÂàÜËØçÁªìÊûúÔºüÂ¶ÇÔºö
```
    //ÂΩì‰ΩøÁî® HanLP.Config.enableDebug();
    public static void main(String[] args) 
    {
       CustomDictionary.insert(""Èæô‰∏â"", ""Conception 1"");
        CustomDictionary.insert(""ÂõΩÂÖÉËØÅÂà∏"", ""Stock 1"");
        HanLP.Config.enableDebug();
        System.out.println(HanLP.segment(""Èæô‰∏âÂíåÂõΩÂÖÉËØÅÂà∏""));
    }
    ÂÆûÈôÖËæìÂá∫Ôºö

        Á≤óÂàÜËØçÁΩëÔºö
        0:[ ]
        1:[Èæô]
        2:[‰∏âÂíå]
        3:[Âíå]
        4:[ÂõΩ]
        5:[ÂÖÉ]
        6:[ËØÅ, ËØÅÂà∏]
        7:[Âà∏]
        8:[ ]

        Á≤óÂàÜÁªìÊûú[Èæô/n, ‰∏âÂíå/nz, ÂõΩÂÖÉËØÅÂà∏/Stock]
        ‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Èæô D 1350 E 924 B 498 C 325 L 17 M 8 K 1 ][‰∏âÂíå A 20833310 ][ÂõΩÂÖÉËØÅÂà∏ A 20833310 ][  K 1 A 1 ]
        ‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Èæô/B ,‰∏âÂíå/A ,ÂõΩÂÖÉËØÅÂà∏/A , /A]
        [Èæô/n, ‰∏âÂíå/nz, ÂõΩÂÖÉËØÅÂà∏/Stock]
```
ÂÖ∂‰∏≠ÔºåÂú®Á≤óÂàÜËØçÁΩë‰∏≠Ôºå‰∏∫‰ªÄ‰πàÊ≤°ÊúâÂèëÁé∞Âä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑ‚ÄúÈæô‰∏â‚ÄùÂíå‚ÄúÂõΩÂÖÉËØÅÂà∏‚ÄùÔºåËøòÊúâ‚Äú0:[ ]""„ÄÅ""8:[ ]""Ë°®Á§∫‰ªÄ‰πàÊÑèÊÄùÂïäÔºü
Â§öË∞¢ÊåáÊïôÔºÅ
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"Fixed #806,TextRankÊèêÂèñÂÖ≥ÈîÆËØçÊèêÂçáÁÆóÊ≥ïÈÄüÂ∫¶","## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÊèêÂçáTextRankÁÆóÊ≥ïÁöÑÈÄüÂ∫¶

## Áõ∏ÂÖ≥issue

#806 

"
‰∫∫ÂêçËØÜÂà´‰∏çÂáÜÁ°Æ,"## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰∫∫ÂêçËØÜÂà´ÊúâËØØÔºö
‚Äú‰ªñË¥üË¥£ÂÜ≤Ë¥ßÔºà‰∏≤Ë¥ßÔºâÁöÑË∞ÉÊü•‰∏éÂ§ÑÁêÜÔºåÂåÖÊã¨Á∫ø‰∏äÁ∫ø‰∏ãÂÜ≤Ë¥ß‚Äù 
=> 
[‰ªñ/r, Ë¥üË¥£/v, ÂÜ≤Ë¥ß/v, Ôºà/w, ‰∏≤Ë¥ß/nr, Ôºâ/w, ÁöÑ/u, Ë∞ÉÊü•/vn, ‰∏é/p, Â§ÑÁêÜ/vn, Ôºå/w, ÂåÖÊã¨/v, Á∫ø/n, ‰∏ä/f, Á∫ø/n, ‰∏ã/f, ÂÜ≤Ë¥ß/n]
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‚Äú‰∏≤Ë¥ß‚ÄùËøô‰∏™ËØçËØØËØÜÂà´‰∏∫‰∫∫Âêç
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

```
//Ëøô‰∏™ÈóÆÈ¢òÊòØÂú®ËøêË°å‰ª•‰∏ã‰ª£Á†ÅÊó∂Âá∫Áé∞Ôºö
NLPTokenizer.ANALYZER
	.enableNameRecognize(true)
	.enableTranslatedNameRecognize(true)
	.enableJapaneseNameRecognize(true)
	.enablePlaceRecognize(true)
	.seg(""‰ªñË¥üË¥£ÂÜ≤Ë¥ßÔºà‰∏≤Ë¥ßÔºâÁöÑË∞ÉÊü•‰∏éÂ§ÑÁêÜÔºåÂåÖÊã¨Á∫ø‰∏äÁ∫ø‰∏ãÂÜ≤Ë¥ß"");
```
"
NLPTokenizer.segment Á∫øÁ®ãÂÆâÂÖ®ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
`NLPTokenizer.segment` Âú®Â§öÁ∫øÁ®ãËøêË°åÊÉÖÂÜµ‰∏ãÔºåÊ¶ÇÁéáÊÄßÂú∞Âá∫Áé∞Â¶Ç‰∏ãÈóÆÈ¢òÔºö

```
SEVERE: Exception while executing runnable io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@721b9773
java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at com.hankcs.hanlp.dictionary.TransformMatrixDictionary.extendSize(TransformMatrixDictionary.java:221)
        at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:62)
        at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
        at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.segSentence(AbstractLexicalAnalyzer.java:321)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
        at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:49)
        at com.company.xushen.Server$NLPImpl.segment(Server.java:71)
        at com.company.xushen.NLPGrpc$MethodHandlers.invoke(NLPGrpc.java:230)
        at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
        at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:698)
        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

ÊàëÊÄÄÁñëÔºåËôΩÁÑ∂ `NLPTokenizer.segment` Â£∞Áß∞ÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÔºå‰ΩÜÊòØÂú®Ë∞ÉÁî®Ê†àÁöÑ `CustomNatureUtility.addNature` Âπ∂‰∏çÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÔºåÊâçÂØºËá¥ËøôÊ†∑ÁöÑÈîôËØØ„ÄÇ
"
ÁºñËØëÂô®Ë≠¶ÂëäÔºöJava SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ªé JDK 8 ÂºÄÂßãÁºñËØëÂô®‰ºöË≠¶Âëä‰ª£Á†Å‰∏≠‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶ÔºåÂú®‰ª•ÂêéÁöÑ Java ÁâàÊú¨‰∏≠ÁªßÁª≠‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶Êàê‰∏∫ÁºñËØëÈîôËØØ„ÄÇÂª∫ËÆÆÊääËøô‰∏™‰∏¥Êó∂ÂèòÈáèÁöÑÂêçÂ≠óÊîπÊàêÂà´ÁöÑ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
`mvn comiple`

### ÊúüÊúõËæìÂá∫
Ê≤°ÊúâËæìÂá∫

### ÂÆûÈôÖËæìÂá∫

```
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,18] '_' Áî®‰ΩúÊ†áËØÜÁ¨¶
  (Java SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,25] '_' Áî®‰ΩúÊ†áËØÜÁ¨¶
  (Java SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/summary/TextRankSentence.java:[95,41] '_' Áî®‰ΩúÊ†áËØÜÁ¨¶
  (Java SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/model/trigram/CharacterBasedGenerativeModel.java:[210,24] '_' Áî®‰ΩúÊ†áËØÜÁ¨¶
  (Java SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶)
[WARNING] /D:/Develop/GitHub/HanLP/src/main/java/com/hankcs/hanlp/model/trigram/CharacterBasedGenerativeModel.java:[212,19] '_' Áî®‰ΩúÊ†áËØÜÁ¨¶
  (Java SE 8 ‰πãÂêéÁöÑÂèëË°åÁâà‰∏≠ÂèØËÉΩ‰∏çÊîØÊåÅ‰ΩøÁî® '_' ‰Ωú‰∏∫Ê†áËØÜÁ¨¶)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
[Small Language Changes in Java SE 9](https://docs.oracle.com/javase/9/language/toc.htm#JSLAN-GUID-16A5183A-DC0D-4A96-B9D8-AAC9671222DD)

"
ËØ≠‰πâÊü•ËØ¢ Ê∑ªÂä†Á±ª‰ºº‚ÄúÂìàÂìàÂìàÂìàÂìà‚ÄùÁöÑÈáçÂ§çÂè†ËØçÊñáÊ°£Êó∂ Êó†Ê≥ïËøîÂõûÊ≠£Á°ÆÁªìÊûú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊèèËø∞
Âú®ËøõË°åËØ≠‰πâÊü•ËØ¢Êó∂, ÊèíÂÖ•ÁöÑÊñáÊ°£Â¶ÇÊûúÊòØÁ±ª‰ºº‚ÄúÂìàÂìàÂìàÂìàÂìàÂìà‚ÄùËøôÁßçÂè†ËØçÈáçÂ§çÁöÑÁªÑÂêàÔºåÊó†Ê≥ïÊ≠£Á°ÆËøîÂõûÁªìÊûúÔºåÊúâBUG„ÄÇ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞
‰ΩøÁî®doc2vectorËøõË°åËØ≠‰πâÊü•ËØ¢ÔºåË∞ÉÁî®addDocumentÊé•Âè£Ê∑ªÂä†‚ÄúÂìàÂìàÂìàÂìàÂìàÂìà‚ÄùËøôÁßçÊñáÊ°£
‰ΩøÁî®ÁöÑËØçÂêëÈáèÊ®°ÂûãÊòØÂÆòÊñπÊèê‰æõÁöÑHANLPÈ¢ÑËÆ≠ÁªÉÂêëÈáè„ÄÇ
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§
Êó†

### Ëß¶Âèë‰ª£Á†Å
Êó†
### ÊúüÊúõËæìÂá∫
Êó†


### ÂÆûÈôÖËæìÂá∫
Êó†

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Êó†
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
TextRankÁÆóÊ≥ïÁöÑ‰ºòÂåñ,"TextRankÁÆóÊ≥ïÈááÁî®Â§öËΩÆËø≠‰ª£Êî∂ÊïõÊù•Ëé∑ÂèñÁªìÊûúÔºåÁõÆÂâçÁöÑÂÆûÁé∞‰∏≠scoreÊ≤°ÊúâËµãÂàùÂÄº„ÄÇ
ÂÆûÈôÖÊµãËØï‰∏≠‰æùÊçÆÂ∑≤Êúâ‰ø°ÊÅØËµãÂàùÂÄºËÉΩÊúâÊïàÁöÑÈôç‰ΩéËø≠‰ª£ËΩÆÊ¨°Ôºå‰ª•‰ΩúËÄÖ
http://www.hankcs.com/nlp/textrank-algorithm-to-extract-the-keywords-java-implementation.html 
ÁöÑ‰æãÂ≠êÔºåÈááÁî®‰∫Ü‰∏§Áßç‰∏çÂêåÁöÑÂàùÂÄºÊñπÊ≥ï
1„ÄÅÂàùÂÄºÈááÁî®sigmoidÂáΩÊï∞Êù•ËµãÂÄºÔºåÂÆûÊµãÁ§∫‰æãËÆ°ÁÆó‰ªé37ËΩÆËø≠‰ª£Èôç‰Ωé‰∏∫9ËΩÆ„ÄÇÊàëÊòØÁî®scalaÊµãËØïÁöÑÔºåJavaÂÆûÁé∞ÂèÇËÄÉ 
Map score = new HashMap(); 
for (Map.Entry<String, Set> entry : words.entrySet()){ score.put(entry.getKey(),sigmoid(entry.getValue().size()); 
}
2„ÄÅÊääsigmoidÂáΩÊï∞Êõ¥Êîπ‰∏∫ 1.0*entry.getValue().size()/words.size()ÔºåÂèØ‰ª•‰ªé37ËΩÆÊ¨°Èôç‰Ωé‰∏∫34ËΩÆÊ¨°

Âª∫ËÆÆÈááÁî®sigmoidÂáΩÊï∞ÊèêÂçáÊïàÁéáÔºåsigmoidÂáΩÊï∞scalaÂÆûÁé∞Â∞±ÊòØ‰∏ÄË°å
def sigmoid(x:Double):Double ={1d/(1d+Math.exp(-x))}"
MathTools.java ‰ª£Á¢ºÁñëÂïè,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö 1.6.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.5.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºåÊàëÊÉ≥Ë´ãÊïô MathTools.java Ë£°ÁöÑ calculateWeight
   
` double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`    

ÈÄôË°å‰ª£Á¢ºÊòØÂèÉËÄÉ‰ªÄÈ∫ºÁÆóÊ≥ïÊàñÂÖ¨ÂºèÊâÄÂØ´ÁöÑ?

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

"
Ê†πÊçÆÁôæÂ∫¶Ê±âËØ≠ÂíåÂú®Á∫øËæûÊµ∑‰øÆÊ≠£ÊãºÈü≥ËØçÂÖ∏,"Ê†πÊçÆ[ÁôæÂ∫¶Ê±âËØ≠](http://hanyu.baidu.com)Âíå[Âú®Á∫øËæûÊµ∑](https://cihai.supfree.net/)ÂØπÊãºÈü≥ËØçÂÖ∏ËøõË°å‰∫Ü‰øÆÊ≠£

‰øÆÊ≠£ËßÑÂàôÔºö
1. Ê†πÊçÆÊãºÈü≥ËØçÂÖ∏‰ªéÁôæÂ∫¶Ê±âËØ≠ÈÄê‰∏ÄÊü•ËØ¢ÔºåËøõË°åÂØπÊØîÔºåÊâæÂà∞‰∏ç‰∏ÄËá¥ÁöÑËØçÊù°Ôºõ
2. Â∞Ü‰∏ç‰∏ÄËá¥ÁöÑËØçÊù°‰ªéÂú®Á∫øËæûÊµ∑Êü•ËØ¢ÔºåÂ¶Ç‰ΩïÊü•ËØ¢ÁªìÊûúÂíåÁôæÂ∫¶Ê±âËØ≠‰∏ÄËá¥ÔºåÂàôËøõË°åÊõøÊç¢„ÄÇ
3. ‰∏ç‰∏ÄËá¥ÁöÑËØçÊù°‰∏≠ÂåÖÂê´‚Äú‰∏Ä‚ÄùÁöÑÔºåÈááÁî®ÁôæÂ∫¶Ê±âËØ≠ÁöÑÁªìÊûú„ÄÇ"
v1.6.2ÊÑüÁü•Êú∫ÁîüÊïà‰∫ÜÔºå‰ΩÜÊòØËÉΩËá™ÂÆö‰πâËØçÊÄßÊ†áÁ≠æ‰πàÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊåâÁÖßhttps://github.com/hankcs/HanLP/wiki/ÁªìÊûÑÂåñÊÑüÁü•Êú∫Ê†áÊ≥®Ê°ÜÊû∂ ËÆ≠ÁªÉËá™Â∑±ÁöÑËØ≠ÊñôÔºåÊ≤°ÊúâËææÂà∞È¢ÑÊÉ≥ÊïàÊûú„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÁîüÊàêËØ≠Êñô
   ‰Ω†Â•Ω/vl	Ëá™Ê≤ªÂéø/n	[‰Ω†Â•Ω/vl	Ëá™Ê≤ªÂéø/n]/company
   ‰∏äË°ó/vi	ÂåóË∑Ø/n	[‰∏äË°ó/vi	ÂåóË∑Ø/n]/company

2. ÂàÜÂà´ËøõË°åÂ¶Ç‰∏ãËÆ≠ÁªÉ
   CWSTrainer PerceptronTrainer  PerceptronTrainer 

3. PerceptronLexicalAnalyzer ÊµãËØï

### Ëß¶Âèë‰ª£Á†Å

```
        Sentence sentence = segmenter.analyze(""‰∏äË°óÂåóË∑Ø"");
        System.out.println(sentence);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

‰∏äË°óÂåóË∑Ø/company

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

‰∏äË°ó/vi	ÂåóË∑Ø/n

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
v1.6.2 Ëá™ÂÆö‰πâËØçÂ∫ì‰∏çÁîüÊïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ëá™ÂÆö‰πâËØçÂ∫ì‰∏çÁîüÊïà
1„ÄÅÊñ∞Âª∫‰∫ÜËá™ÂÆö‰πâËØçÂ∫ìÔºöPlaceDictionary.txt place
    ‰Ω†Â•ΩËá™Ê≤ªÂéø
   ‰∏äË°óÂåóË∑Ø

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
### Ê≠•È™§
1„ÄÅËá™ÂÆö‰πâËØçÂ∫ì PlaceDictionary.txt place
     ËØçÂ∫ìÂÜÖÂÆπ
     ÈÄöÈÅì‰æóÊóèËá™Ê≤ªÂéø
    ‰∏äÂú∞È©¨ËøûÊ¥ºÂåóË∑Ø

2. ÈÖçÁΩÆÊñá‰ª∂
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt ns;data/dictionary/person/nrf.txt nrf, PlaceDictionary.txt place;

3. Âà†Èô§binÔºåÈáçÊñ∞ÊâßË°å

### Ëß¶Âèë‰ª£Á†Å

```
String[] testCase = new String[]{
                ""‰∏äË°óÂåóË∑Ø"",
                ""‰Ω†Â•ΩËá™Ê≤ªÂéø""
        };
        for (String sentence : testCase)
        {
            Segment segment = HanLP.newSegment().enableCustomDictionary(true);
            List<Term> termList = segment.seg(sentence);
            String termString = join(termList,""\t"");
            System.out.println(termString);
        }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
‰∏äË°óÂåóË∑Ø/place
‰Ω†Â•ΩËá™Ê≤ªÂéø/place

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

‰Ω†Â•Ω/vl	Ëá™Ê≤ªÂéø/n	[‰Ω†Â•Ω/vl	Ëá™Ê≤ªÂéø/n]/company
‰∏äË°ó/vi	ÂåóË∑Ø/n	[‰∏äË°ó/vi	ÂåóË∑Ø/n]/company

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
HanLPËøõË°åÊñáÊú¨ËΩ¨Ê±âËØ≠ÊãºÈü≥Êó∂ÈúÄË¶Å‰ºòÂåñ,"Áî®HanLPÁöÑÁî®Êà∑ÁõÆÂâçÂ§ö‰∏∫ÂÅöËØ≠Èü≥ËØÜÂà´Áõ∏ÂÖ≥ÔºåÁõÆÂâçHanLPÂØπÊ±âÂ≠óËΩ¨Ê±âËØ≠ÊãºÈü≥ÁöÑÊîØÊåÅ‰ªÖÈôêÊ±âÂ≠ó„ÄÇ
‰ΩÜÊòØÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåÁî®Êà∑ËæìÂÖ•ÁöÑÊñáÊú¨Âπ∂‰∏çËÉΩÁ°ÆÂÆöÊòØ‰∏≠ÊñáËøòÊòØËã±ÊñáÔºåÈÇ£‰πàÔºåÂú®ËøôÁßçÂú∫ÊôØ‰∏≠Â∫îËØ•ÊîØÊåÅÁöÑÊòØÔºöÂ¶ÇÊûúÊòØ‰∏≠ÊñáÔºåÈÇ£‰πàÂ∞±ËΩ¨ÊàêÊ±âËØ≠ÊãºÈü≥ÔºåÂ¶ÇÊûú‰∏çÊòØÈÇ£‰πàÂ∞±‰øùÁïô„ÄÇ"
ÂÖ≥‰∫éÊÑüÁü•Êú∫‰∏éCRFÂØπÁ©∫Ê†º‰∏éÊ†áÁÇπÁöÑËØçÊÄßËØÜÂà´ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÁªºÂêàÊØîËæÉÂÖ∂‰ªñÂá†ÁßçÂàÜËØçÂô®ÔºåÊÑüÁü•Êú∫ÂØπËØçÊÄßÁöÑÊ†áÊ≥®Áõ∏ÂØπÊõ¥ÂáÜÁ°ÆÔºå‰ΩÜÊòØÂØπ‰∫éÁ©∫Ê†ºÂíåÊ†áÁÇπÁ¨¶Âè∑(Â∞§ÂÖ∂ÊòØËã±ÊñáÊ†áÁÇπ)ÁöÑÊ†áÊ≥®Â≠òÂú®ËÆ∏Â§öÈóÆÈ¢ò„ÄÇ

‰æãÂ¶ÇÂØπ‰ª•‰∏ãËøôÂè•ËØùÁöÑÊ†áÊ≥®Ôºö
`""‰Ω†Â•ΩÔºå ÊàëÊÉ≥Áü•ÈÅìÔºö È£éÊòØ‰ªéÂì™ÈáåÊù•; Èõ∑ÊòØ‰ªéÂì™ÈáåÊù•Ôºõ Èõ®ÊòØ‰ªéÂì™ÈáåÊù•Ôºü""`

perceptron:
```
(‰Ω†/r) (Â•Ω/a) (Ôºå/w) ( /v) (Êàë/r) (ÊÉ≥/v) (Áü•ÈÅì/v) (Ôºö/w) ( È£é/n) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•/v) (; Èõ∑/d) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•/v) (Ôºõ/w) ( Èõ®/n) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•/v) (Ôºü/w) 
```
crf:
```
(‰Ω†Â•Ω/d) (Ôºå/v) ( /v) (Êàë/r) (ÊÉ≥/v) (Áü•ÈÅì/v) (Ôºö/w) ( È£é/n) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•;/v) ( Èõ∑/n) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•/v) (Ôºõ/w) ( Èõ®/n) (ÊòØ/v) (‰ªé/p) (Âì™Èáå/r) (Êù•/v) (Ôºü/v) 
```
viterbi:
```
(‰Ω†Â•Ω/l) (Ôºå/w) ( /w) (Êàë/r) (ÊÉ≥/v) (Áü•ÈÅì/v) (Ôºö/w) ( /w) (È£é/n) (ÊòØ‰ªé/v) (Âì™Èáå/r) (Êù•/v) (;/w) ( /w) (Èõ∑/n) (ÊòØ‰ªé/v) (Âì™Èáå/r) (Êù•/v) (Ôºõ/w) ( /w) (Èõ®/n) (ÊòØ‰ªé/v) (Âì™Èáå/r) (Êù•/v) (Ôºü/w) 
```

ÈáçÁÇπÂÖ≥Ê≥®Á©∫Ê†º‰∏éÊ†áÁÇπÁöÑÂàÜËØçÔºåÁªìÊûúÂèëÁé∞ÔºöÊÑüÁü•Êú∫‰∏éCRFÂØπÊúâÊó∂Á©∫Ê†º‰∏éÊ†áÁÇπËØÜÂà´‰∏∫ÂÖ∂‰ªñËØçÊÄßÔºåÁîöËá≥‰ºö‰∏éÂâçÂêéÁöÑËØçÊàê‰∏∫ÁªÑÂêàÔºåÂèçËÄåÈªòËÆ§ÁöÑviterbiÂØπ‰∫éÊ†áÁÇπÁöÑÂ§ÑÁêÜÊõ¥Â•Ω„ÄÇ

ÊúÄËøëÂºÄÂßãÊé•Ëß¶ËøôÊñπÈù¢ÔºåÂ∞öÊú™‰ªîÁªÜÈòÖËØªÊ∫êÁ†ÅÔºåËØ∑ÈóÆÂØπ‰∫éÁ©∫Ê†º‰∏éÊ†áÁÇπÊòØÂ¶Ç‰ΩïÂ§ÑÁêÜÁöÑÔºåËÉΩÂê¶ÊîπËøõÔºüËØ∑ÊåáÂØº„ÄÇ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->"
‰ºòÂåñCRFÂàÜËØçÊïàÁéáÁöÑÊñπÊ≥ï,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

[CRFÂàÜËØç](https://github.com/hankcs/HanLP#6-crf%E5%88%86%E8%AF%8D)Âíå[ÊÑüÁü•Êú∫ÂàÜËØç](https://github.com/hankcs/HanLP/wiki/%E7%BB%93%E6%9E%84%E5%8C%96%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A0%87%E6%B3%A8%E6%A1%86%E6%9E%B6)ÁöÑÊµÅÁ®ãÁõ∏Â∑Æ‰∏çÂ§ß(ÈÉΩÊòØÊèêÂèñÁâπÂæÅ->Êü•Ê¶ÇÁéá/ÊùÉÈáç->Á¥ØÂä†->Viterbi), 
‰ΩÜWiki‰∏äÈù¢ÁöÑÊµãËØïÁªìÊûúÂ∑ÆË∑ùÂç¥ÂæàÂ§ß„ÄÇËÄå‰∏îHanLPÊó©ÊúüÁöÑCRFÊ®°ÂûãÁâπÂæÅÊ®°ÊùøÊï∞ÈáèÂ∞ë‰∫éÂΩìÂâçÊÑüÁü•Êú∫ÁöÑ‰∏É‰∏™Ê®°Êùø„ÄÇ
Âõ†Ê≠§Êü•Áúã‰∫Ü‰∏Ä‰∏ãHanLPÊûÑÈÄ†CRFÊ®°ÂûãÁöÑÈÄªËæëÔºåÊàëÂèëÁé∞‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºö
CRF++ÁîüÊàêÁöÑÁâπÂæÅÈÉΩÊòØ‰ª•‚ÄúU[0-9]+:‚ÄùÂºÄÂ§¥ÁöÑÔºåËÄåÊ®°Âûã‰ΩøÁî®BinTrieÁ¥¢ÂºïÁâπÂæÅÊ¶ÇÁéáÔºåËøôÂ∞±ÂØºËá¥BinTrieÂä†ÈÄüÁöÑÁ¨¨‰∏ÄÂ±ÇÂè™Êúâ‰∏Ä‰∏™Â≠óÁ¨¶‚ÄúU‚ÄùÔºåÔºåÊâÄÊúâÁöÑÁâπÂæÅÈÉΩÈÉΩËµ∞‰∫Ü‰∫åÂàÜÊü•ÊâæÔºåÈöæÊÄ™ÈÄüÂ∫¶‰ºöÊÖ¢„ÄÇ

## Ëß£ÂÜ≥ÊÄùË∑Ø

ÈúÄË¶ÅËß£ÂÜ≥ÁöÑÊòØÂ¶Ç‰ΩïÊääÊ±âÂ≠óÁ¥¢ÂºïÂà∞Á¨¨‰∏ÄÁ∫ßÂêåÊó∂Âèà‰∏çÂΩ±ÂìçÊïàÁéáÔºåÊàëËßâÂæóÂèØ‰ª•ËÄÉËôëÊãÜËß£ÈáçÁªÑÁâπÂæÅÊ®°ÊùøÂíåÁâπÂæÅKeyÔºåÊàñËÄÖÁõ¥Êé•reverseÂ≠óÁ¨¶‰∏≤„ÄÇ
"
Merge pull request #1 from hankcs/master,"ÂêåÊ≠•Â∫ì‰ª£Á†Å

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
ÂÖ≥‰∫éËá™ÂÆö‰πâRecognitionÂíåÊó•ÊúüÊàñËÄÖÊó∂Èó¥ÁöÑÂàÜËØç‰∏äÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ËØ∑ÈóÆÂú®ÂØπ""ÁÆ°ÁêÜÂëò‰∫é2017-10-12ÂØπËØ•ËÆæÂ§á‰ø°ÊÅØËøõË°å‰∫Ü‰øÆÊîπÊìç‰Ωú""ËøõË°åÊãÜÂàÜÊó∂Ôºå‚Äú2017-10-12‚ÄùËøô‰∏™Êó•Êúü‰ºöË¢´ÂçïÁã¨ÊãÜÂºÄ„ÄÇÊúâÊ≤°ÊúâÂ•ΩÁöÑÊñπÂºèËß£ÂÜ≥ËøôÁÇπÔºåÊâæ‰∫ÜÊâæÁ±ª‰ººÁöÑissue,Ë≤å‰ººÊ≤°ÊâæÂà∞ÂêàÈÄÇÁöÑÊñπÂºè„ÄÇÂè¶Â§ñÁé∞Âú®ÊúâÊèê‰æõRecognitionËá™ÂÆö‰πâ‰πà„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->


### Ëß¶Âèë‰ª£Á†Å

```
     public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer =  new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.seg(""ÁÆ°ÁêÜÂëò‰∫é2017-10-12ÂØπËØ•ËÆæÂ§á‰ø°ÊÅØËøõË°å‰∫Ü‰øÆÊîπÊìç‰Ωú""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÁÆ°ÁêÜÂëò/n, ‰∫é/p, 2017-10-12, ÂØπ/p, ËØ•/r, ËÆæÂ§á/n, ‰ø°ÊÅØ/n, ËøõË°å/v, ‰∫Ü/u, ‰øÆÊîπ/v, Êìç‰Ωú/v
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÁÆ°ÁêÜÂëò/n, ‰∫é/p, 2017-/m, 10/m, -/q, 12/m, ÂØπ/p, ËØ•/r, ËÆæÂ§á/n, ‰ø°ÊÅØ/n, ËøõË°å/v, ‰∫Ü/u, ‰øÆÊîπ/v, Êìç‰Ωú/v


```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ËøêË°åÊà™ÂõæÔºö
![38286722-5e5c1772-37f9-11e8-83b0-eb77566f9c82](https://user-images.githubusercontent.com/18030444/38715484-f8034542-3f0d-11e8-8b38-3fa6236a8e5b.png)
"
ÈªòËÆ§ÂàÜËØçÂô®ÂºÄÂÖ≥ËÆæÁΩÆÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®Âà©Áî®HanLPÈªòËÆ§ÁöÑÁª¥ÁâπÊØîÂàÜËØçÂô®ÂàÜËØçÊó∂ÔºåÁ≥ªÁªüÊäõÂá∫java.util.NoSuchElementException„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->


### Ê≠•È™§

1. ‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÔºåenableNumberQuantifierRecognizeÊâìÂºÄÊó∂‰ºöÊäõÂá∫‰∏äËø∞ÂºÇÂ∏∏„ÄÇ
2. ÊääÂÖ∂‰ªñÂºÄÂÖ≥ÈÉΩÂéªÊéâÔºåÂè™ÊâìÂºÄenableNumberQuantifierRecognizeÊó∂ÔºåÁ®ãÂ∫èÊ≠£Â∏∏ËøêË°å„ÄÇ


### Ëß¶Âèë‰ª£Á†Å

```
 public static void main(String[] args) {
        Segment seg = HanLP.newSegment();
        seg.enablePartOfSpeechTagging(true);
        seg.enableMultithreading(8);
        seg.enableAllNamedEntityRecognize(true);
        seg.enableNameRecognize(true);
        seg.enableOrganizationRecognize(true);
        seg.enablePlaceRecognize(true);
        seg.enableNumberQuantifierRecognize(true);

        String raw=""‰ΩôÂßöÊó•Êä•ÁîµÂ≠êÁâà‰ΩôÂßöÂç´ÁîüËÆ°Áîü‰∫ã‰∏öÂçï‰ΩçÂÖ¨ÂºÄÊãõËÅò121ÂêçÂç´ÊäÄ‰∫∫Âëò2018Âπ¥‰ΩôÂßöÂ∏ÇÂç´ÁîüËÆ°Áîü‰∫ã‰∏öÂçï‰ΩçÂÖ¨ÂºÄÊãõËÅòÂç´ÁîüÊäÄÊúØ‰∫∫ÂëòÁÆÄÁ´†Ê†πÊçÆÊúâÂÖ≥ËßÑÂÆöÂíåÂç´ÁîüËÆ°Áîü‰∫ã‰∏öÂèëÂ±ïÈúÄË¶ÅÔºåÁªèÁ†îÁ©∂ÂÜ≥ÂÆöÔºåÊàëÂ∏ÇÂç´ÁîüËÆ°Áîü‰∫ã‰∏öÂçï‰ΩçÂÖ¨ÂºÄÊãõËÅòÂç´ÊäÄ‰∫∫Âëò121Âêç„ÄÇÁé∞Â∞ÜÊúâÂÖ≥‰∫ãÈ°πÂÖ¨ÂëäÂ¶Ç‰∏ãÔºö‰∏Ä„ÄÅÊãõËÅòËÅå‰ΩçÂèäÊåáÊ†áÊãõËÅòÂçï‰ΩçÂèäÊãõËÅòÊåáÊ†á ËÅå‰ΩçÁºñÁ†Å Âçï‰Ωç ËÅå‰Ωç Â≠¶ÂéÜ Êï∞Èáè ËÅå‰ΩçË¶ÅÊ±ÇÂèäÂÖ∂‰ªñ 1 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ‰∏¥Â∫ä Á†îÁ©∂Áîü 1 ËÇæÂÜÖÁßëÊñπÂêë 2 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏¥Â∫ä Á†îÁ©∂Áîü 1 ÂøÉË°ÄÁÆ°ÂÜÖÁßëÊñπÂêë 3 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏¥Â∫ä Á†îÁ©∂Áîü 1 ÂÜÖÂàÜÊ≥åÁßëÊñπÂêë 4 Â∏Ç‰∫åÈô¢ ‰∏¥Â∫ä Á†îÁ©∂Áîü 1 ÂÑøÁßëÂ≠¶‰∏ì‰∏ö 5 Â∏Ç‰∏âÈô¢ ‰∏¥Â∫ä Á†îÁ©∂Áîü 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÁ≤æÁ•ûÁóÖ‰∏éÁ≤æÁ•ûÂç´ÁîüÂ≠¶‰∏ì‰∏ö 6 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 8 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÁ¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ 7 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÈ∫ªÈÜâÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÈ∫ªÈÜâÂ≠¶‰∏ì‰∏öÔºåÁ¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ 8 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÁ¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ 9 Â∏Ç‰∫åÈô¢ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÂåªÂ∏àËµÑÊ†ºÔºåÊâß‰∏öÊ≥®ÂÜåËåÉÂõ¥‰∏∫ÈáçÁóáÂåªÂ≠¶ 10 Â∏Ç‰∫åÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÁöÆËÇ§ÁßëÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÂåªÂ∏àËµÑÊ†º 11 Â∏Ç‰∫åÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãËßÜÂÖâÈó®ËØäÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÁúºËßÜÂÖâ‰∏ì‰∏ö 12 Â∏Ç‰∏âÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÁ≤æÁ•ûÁßëÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 2 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÁ≤æÁ•ûÂåªÂ≠¶‰∏ì‰∏öÔºåÁî∑ÊÄß 13 Â∏Ç‰∏âÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÁ≤æÁ•ûÁßëÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 2 Á≤æÁ•ûÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÂåªÂ∏àËµÑÊ†ºÔºåÊâß‰∏öÊ≥®ÂÜåËåÉÂõ¥‰∏∫Á≤æÁ•ûÂç´Áîü 14 Â∏ÇÂõõÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÈ™®ÁßëÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÂåªÂ∏àËµÑÊ†º 15 Â∏ÇÂõõÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãÈ∫ªÈÜâÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÂåªÂ∏àËµÑÊ†º 16 Â∏ÇÊÄ•ÊïëÁ´ô ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 2 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÁî∑ÊÄß 17 Â∏ÇÂç´Ê†° ‰∏¥Â∫äÔºà‰ªé‰∫ãËß£ÂâñÊïôÂ≠¶Ôºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 18 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ÂåªÂÖ±‰ΩìÊàêÂëòÂçï‰Ωç ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 3 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 19 Â∏ÇÂõõÈô¢ÈªÑÂÆ∂Âü†ÂàÜÈô¢ÔºàÈªÑÂÆ∂Âü†ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 20 Â∏ÇÂõõÈô¢Ôºà‰∏ãÂ±ûÁ§æÂå∫Âç´ÁîüÊúçÂä°Á´ôÔºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂÑø‰øùÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†º 21 Â∏ÇÂõõÈô¢Ôºà‰∏ãÂ±ûÁ§æÂå∫Âç´ÁîüÊúçÂä°Á´ôÔºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãBË∂ÖÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†º 22 Â∏ÇÊÄ•ÊïëÁ´ô ‰∏¥Â∫ä Â§ß‰∏ìÂèä‰ª•‰∏ä 4 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†ºÔºåÈÄÇÂêàÁî∑ÊÄß 23 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢Ê¢ÅÂºÑÂàÜÈô¢ÔºàÊ¢ÅÂºÑ‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂ¶áÂÑø‰øùÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 24 Â∏ÇÂõõÈô¢ÂåªÂÖ±‰ΩìÊàêÂëòÂçï‰Ωç ‰∏¥Â∫äÔºà‰ªé‰∫ãÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 3 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏ö 25 Â∏ÇÂõõÈô¢ÈªÑÂÆ∂Âü†ÂàÜÈô¢ÔºàÈªÑÂÆ∂Âü†ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫ä Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 26 Â∏Ç‰∏≠ÂåªÂåªÈô¢‰∏â‰∏ÉÂ∏ÇÂàÜÈô¢Ôºà‰∏â‰∏ÉÂ∏ÇÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏ö 27 Â∏Ç‰∏≠ÂåªÂåªÈô¢Â§ßÈöêÂàÜÈô¢ÔºàÂ§ßÈöêÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂÑø‰øùÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†ºÔºåÊâß‰∏öÊ≥®ÂÜåËåÉÂõ¥‰∏∫ÂÖ®ÁßëÂåªÂ≠¶ÊàñÂÑøÁßë 28 Â∏Ç‰∫åÈô¢Â§ßÂ≤öÂàÜÈô¢ÔºàÂ§ßÂ≤öÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏ö 29 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ‰∏¥Â∫äÔºà‰ªé‰∫ãËÄ≥ÈºªÂíΩÂñâÁßëÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫î‰∏ªÊ≤ªÂåªÂ∏àÂèä‰ª•‰∏äËµÑÊ†º 30 Â∏ÇÂõõÈô¢ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫î‰∏ªÊ≤ªÂåªÂ∏àÂèä‰ª•‰∏äËµÑÊ†º 31 Â∏ÇÂõõÈô¢Â∞èÊõπÂ®•ÂàÜÈô¢ÔºàÂ∞èÊõπÂ®•ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫ä Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 32 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢È©¨Ê∏öÂàÜÈô¢ÔºàÈ©¨Ê∏ö‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂ¶á‰øùÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂ•≥ÊÄß 33 Â∏Ç‰∫åÈô¢ÁâüÂ±±ÂàÜÈô¢ÔºàÁâüÂ±±ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫äÔºà‰ªé‰∫ãÂ¶áÂÑø‰øùÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂ•≥ÊÄß 34 Â∏ÇÂõõÈô¢Â∞èÊõπÂ®•ÂàÜÈô¢ÔºàÂ∞èÊõπÂ®•ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏¥Â∫ä Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 35 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Á†îÁ©∂Áîü 1 ‰∏≠ÂåªÂ§ñÁßëÂ≠¶‰∏ì‰∏ö 36 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Á†îÁ©∂Áîü 1 ‰∏≠ÂåªÈ™®‰º§ÁßëÂ≠¶‰∏ì‰∏ö 37 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Á†îÁ©∂Áîü 1 ‰∏≠ÂåªÂ¶áÁßëÂ≠¶‰∏ì‰∏ö 38 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Á†îÁ©∂Áîü 1 ÈíàÁÅ∏Êé®ÊãøÂ≠¶‰∏ì‰∏ö 39 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Á†îÁ©∂Áîü 1 ‰∏≠ÂåªËÇøÁò§Â≠¶ÊñπÂêë 40 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ‰∏≠Âåª Êú¨ÁßëÂèä‰ª•‰∏ä 7 ‰∏≠ÂåªÂ≠¶‰∏ì‰∏öÔºåÁ¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ 41 Â∏ÇÂõõÈô¢ ‰∏≠Âåª Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏≠ÂåªÂ≠¶‰∏ì‰∏ö 42 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢Èò≥ÊòéË°óÈÅìÂàÜÈô¢ÔºàÈò≥ÊòéË°óÈÅìÂç´ÁîüÈô¢Ôºâ ‰∏≠Âåª Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏≠ÂåªÂ≠¶‰∏ì‰∏ö 43 Â∏ÇÂõõÈô¢ÈªÑÂÆ∂Âü†ÂàÜÈô¢ÔºàÈªÑÂÆ∂Âü†ÈïáÂç´ÁîüÈô¢Ôºâ ‰∏≠Âåª Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏≠ÂåªÂ≠¶‰∏ì‰∏ö 44 Â∏ÇÂõõÈô¢ ‰∏≠ÂåªÔºà‰ªé‰∫ãÈ™®‰º§Ôºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºå‰∏≠ÂåªÂ≠¶‰∏ì‰∏ö 45 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ÈíàÁÅ∏Êé®ÊãøÔºà‰ªé‰∫ãÊé®ÊãøÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÈíàÁÅ∏Êé®Êãø‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 46 ÂÖ∂‰ªñÂ∏ÇÁ∫ßÂåªÈô¢ ÈíàÁÅ∏Êé®Êãø Êú¨ÁßëÂèä‰ª•‰∏ä 2 ÈíàÁÅ∏Êé®Êãø‰∏ì‰∏ö 47 Â∏Ç‰∫åÈô¢ Âè£ËÖî Á†îÁ©∂Áîü 1 Âè£ËÖî‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 48 Â∏Ç‰∫åÈô¢ Âè£ËÖî Êú¨ÁßëÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏ö 49 Â∏ÇÂõõÈô¢ Âè£ËÖî Êú¨ÁßëÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏ö 50 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢‰∏à‰∫≠ÂàÜÈô¢Ôºà‰∏à‰∫≠‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ Âè£ËÖî Êú¨ÁßëÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏ö 51 Â∏ÇÂõõÈô¢ÊúóÈúûË°óÈÅìÂàÜÈô¢ÔºàÊúóÈúûË°óÈÅìÂç´ÁîüÈô¢Ôºâ Âè£ËÖî Êú¨ÁßëÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏ö 52 Â∏Ç‰∏≠ÂåªÂåªÈô¢‰∏â‰∏ÉÂ∏ÇÂàÜÈô¢Ôºà‰∏â‰∏ÉÂ∏ÇÈïáÂç´ÁîüÈô¢Ôºâ Âè£ËÖî Â§ß‰∏ìÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†º 53 Â∏Ç‰∫åÈô¢Èπø‰∫≠ÂàÜÈô¢ÔºàÈπø‰∫≠‰π°Âç´ÁîüÈô¢Ôºâ Âè£ËÖî Â§ß‰∏ìÂèä‰ª•‰∏ä 1 Âè£ËÖîÂåªÂ≠¶‰∏ì‰∏öÔºåÂÖ∑ÊúâÁõ∏Â∫îÊâß‰∏öÔºàÂä©ÁêÜÔºâÂåªÂ∏àËµÑÊ†º 54 Â∏ÇÊ¢®Ê¥≤ÂåªÈô¢ÔºàÂ∫∑Â§çÂåªÈô¢Ôºâ Âè£ËÖî Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºåÂè£ËÖîÂåªÂ≠¶‰∏ì‰∏ö 55 Â∏Ç‰∫åÈô¢Â§ßÂ≤öÂàÜÈô¢ÔºàÂ§ßÂ≤öÈïáÂç´ÁîüÈô¢Ôºâ Âè£ËÖî Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºåÂè£ËÖîÂåªÂ≠¶‰∏ì‰∏öÔºåÂπ¥ÈæÑÊîæÂÆΩËá≥1978Âπ¥1Êúà1Êó•Âèä‰ª•ÂêéÂá∫Áîü 56 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ÂåªÂÖ±‰ΩìÊàêÂëòÂçï‰Ωç ÂÖ¨ÂÖ±Âç´Áîü Êú¨ÁßëÂèä‰ª•‰∏ä 3 È¢ÑÈò≤ÂåªÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 57 Â∏ÇÂõõÈô¢ÈªÑÂÆ∂Âü†ÂàÜÈô¢ÔºàÈªÑÂÆ∂Âü†ÈïáÂç´ÁîüÈô¢Ôºâ ÂÖ¨ÂÖ±Âç´Áîü Êú¨ÁßëÂèä‰ª•‰∏ä 1 È¢ÑÈò≤ÂåªÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 58 Â∏Ç‰∫åÈô¢ÂõõÊòéÂ±±ÂàÜÈô¢ÔºàÂõõÊòéÂ±±ÈïáÂç´ÁîüÈô¢Ôºâ ÂÖ¨ÂÖ±Âç´Áîü Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºåÈ¢ÑÈò≤ÂåªÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 59 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 60 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ DSA‰ªãÂÖ•ÂåªÂ∏à Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 61 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 62 Â∏Ç‰∏âÈô¢ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºà‰ªé‰∫ãBË∂ÖÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏ö 63 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ÂåªÂÖ±‰ΩìÊàêÂëòÂçï‰Ωç ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠ Êú¨ÁßëÂèä‰ª•‰∏ä 4 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 64 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢Èò≥ÊòéË°óÈÅìÂàÜÈô¢ÔºàÈò≥ÊòéË°óÈÅìÂç´ÁîüÈô¢Ôºâ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºà‰ªé‰∫ãÂ¶áÁßëBË∂ÖÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂ•≥ÊÄß 65 Â∏ÇÂõõÈô¢ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºà‰ªé‰∫ãÊîæÂ∞ÑÔºâ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºåÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 66 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ÈôÜÂü†ÂàÜÈô¢ÔºàÈôÜÂü†‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Ôºà‰ªé‰∫ãÂ¶áÁßëBË∂ÖÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂÆöÂêëËÅå‰ΩçÔºåÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÔºåÂ•≥ÊÄß 67 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ DSA‰ªãÂÖ•ÊäÄÂ∏à Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 68 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ECTÊäÄÂ∏à Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 69 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ÊîæÁñóÊäÄÂ∏à Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶‰∏ì‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 70 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ‰∏ì‰∏ö 71 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢‰∏à‰∫≠ÂàÜÈô¢Ôºà‰∏à‰∫≠‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ÂåªÂ≠¶Ê£ÄÈ™å Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶Ê£ÄÈ™å„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 72 Â∏ÇÂõõÈô¢Â∞èÊõπÂ®•ÂàÜÈô¢ÔºàÂ∞èÊõπÂ®•ÈïáÂç´ÁîüÈô¢Ôºâ ÂåªÂ≠¶Ê£ÄÈ™å Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶Ê£ÄÈ™å„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 73 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ËçØÂâÇ Á†îÁ©∂Áîü 1 ËçØÂâÇÂ≠¶‰∏ì‰∏ö 74 Â∏Ç‰∏≠ÂåªÂåªÈô¢ ËçØÂâÇ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏¥Â∫äËçØÂ≠¶‰∏ì‰∏ö 75 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢Ê¢ÅÂºÑÂàÜÈô¢ÔºàÊ¢ÅÂºÑ‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ËçØÂâÇ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ËçØÂ≠¶‰∏ì‰∏ö 76 Â∏Ç‰∏≠ÂåªÂåªÈô¢Â§ßÈöêÂàÜÈô¢ÔºàÂ§ßÈöêÈïáÂç´ÁîüÈô¢Ôºâ ËçØÂâÇ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ËçØÂ≠¶‰∏ì‰∏ö 77 Â∏Ç‰∫åÈô¢ ‰∏≠ËçØ Êú¨ÁßëÂèä‰ª•‰∏ä 1 ‰∏≠ËçØÂ≠¶‰∏ì‰∏ö 78 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢Ê¢ÅÂºÑÂàÜÈô¢ÔºàÊ¢ÅÂºÑ‰∏≠ÂøÉÂç´ÁîüÈô¢Ôºâ ‰∏≠ËçØ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 ‰∏≠ËçØÂ≠¶‰∏ì‰∏ö 79 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ Êä§ÁêÜ Êú¨ÁßëÂèä‰ª•‰∏ä 1 Êä§ÁêÜÂ≠¶„ÄÅÂä©‰∫ßÂ£´‰∏ì‰∏ö 80 ÂÖ∂‰ªñÂ∏ÇÁ∫ßÂåªÈô¢ Êä§ÁêÜ Êú¨ÁßëÂèä‰ª•‰∏ä 2 Êä§ÁêÜÂ≠¶„ÄÅÂä©‰∫ßÂ£´‰∏ì‰∏ö 81 Â∏Ç‰∏≠ÂåªÂåªÈô¢ Êä§ÁêÜ Â§ß‰∏ìÂèä‰ª•‰∏ä 1 Êä§ÁêÜÂ≠¶„ÄÅÂä©‰∫ßÂ£´‰∏ì‰∏ö 82 Â∏Ç‰∫åÈô¢ÂåªÂÖ±‰ΩìÊàêÂëòÂçï‰Ωç Êä§ÁêÜ Â§ß‰∏ìÂèä‰ª•‰∏ä 2 Êä§ÁêÜÂ≠¶„ÄÅÂä©‰∫ßÂ£´‰∏ì‰∏öÔºõÂÖ∑ÊúâÊä§Â£´Êâß‰∏öËµÑÊ†ºËÄÖÔºåÂ≠¶ÂéÜÊîæÂÆΩËá≥‰∏≠‰∏ì 83 Â∏Ç‰∏âÈô¢ Êä§ÁêÜÔºà‰ªé‰∫ãÁ≤æÁ•ûÁßëÔºâ Â§ß‰∏ìÂèä‰ª•‰∏ä 2 Êä§ÁêÜÂ≠¶‰∏ì‰∏öÔºåÁî∑ÊÄß 84 Â∏Ç‰∫åÈô¢ Âä©‰∫ß Â§ß‰∏ìÂèä‰ª•‰∏ä 1 Âä©‰∫ßÂ£´‰∏ì‰∏öÊàñÂÖ∑ÊúâÂÖ®Êó•Âà∂Êú¨ÁßëÂ≠¶ÂéÜÁöÑÊä§ÁêÜÂ≠¶ÔºàÂä©‰∫ßÊñπÂêëÔºâ‰∏ì‰∏ö 85 Â∏Ç‰∫∫Ê∞ëÂåªÈô¢ ÊîæÁñóÁâ©ÁêÜÂ∏à Êú¨ÁßëÂèä‰ª•‰∏ä 1 ÂåªÂ≠¶Áâ©ÁêÜ„ÄÅÊ†∏Áâ©ÁêÜ„ÄÅÁâ©ÁêÜÂ≠¶‰∏ì‰∏öÔºåÁ¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ÔºåÈÄÇÂêàÁî∑ÊÄß 86 Â∏Ç‰∫åÈô¢ ÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ã Êú¨ÁßëÂèä‰ª•‰∏ä 2 ÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ã‰∏ì‰∏öÔºåÈ´òÁ©∫‰Ωú‰∏öÔºåÈÄÇÂêàÁî∑ÊÄß 87 Â∏Ç‰∏≠ÂåªÂåªÈô¢ Â∫∑Â§çÂåªÂ≠¶ Êú¨ÁßëÂèä‰ª•‰∏ä 1 Â∫∑Â§çÂåªÂ≠¶„ÄÅ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏ö 88 Â∏ÇÊ¢®Ê¥≤ÂåªÈô¢ÔºàÂ∫∑Â§çÂåªÈô¢Ôºâ Â∫∑Â§çÊ≤ªÁñó Êú¨ÁßëÂèä‰ª•‰∏ä 1 Â∫∑Â§çÊ≤ªÁñóÂ≠¶‰∏ì‰∏ö ÂêàËÆ° 121 Ê≥®Ôºö1.Â∫îËÅòËÄÖÂ∫îÂÖ∑Êúâ‰∏éÊãõËÅòËÅå‰ΩçÂØπÂè£ÊàñÁõ∏Â∫î‰∏ì‰∏öÊñáÂá≠ÔºåÊâÄÂ≠¶‰∏ì‰∏öÈÄÇÂêàÊãõËÅòËÅå‰ΩçÂ∑•‰ΩúÈúÄË¶ÅÔºåÂπ∂‰∏éÂèÇÂä†Êâß‰∏öËµÑÊ†ºËÄÉËØï„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËÄÉËØïÁöÑ‰∏ì‰∏öË¶ÅÊ±ÇÁõ∏ÂåπÈÖçÔºå‰∏çÂº∫Ë∞É‰∏ì‰∏öÂêçÁß∞Â≠óÈù¢ÂÆåÂÖ®‰∏ÄËá¥Ôºõ2.‰∏¥Â∫äËÅå‰ΩçÊ†πÊçÆÂçï‰ΩçÈúÄË¶Å‰ªé‰∫ã‰∏¥Â∫äÁ±ªÂà´ÂêÑ‰∏ì‰∏ö„ÄÇ‰∫å„ÄÅÊãõËÅòÂØπË±°ÂíåÊù°‰ª∂(‰∏Ä)ÊãõËÅòÂØπË±°1.ÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°ÊØï‰∏öÁöÑÁ†îÁ©∂ÁîüÔºå‰ª•ÂèäÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°ÂåªÂ≠¶ÂΩ±ÂÉèÂ≠¶„ÄÅÈ∫ªÈÜâÂ≠¶„ÄÅÂåªÂ≠¶Áâ©ÁêÜ„ÄÅÊ†∏Áâ©ÁêÜ„ÄÅÁâ©ÁêÜÂ≠¶‰∏ì‰∏öÊú¨ÁßëÂ≠¶ÂéÜÁöÑÊØï‰∏öÁîü„ÄÇ2.ÊµôÊ±üÁúÅÁîüÊ∫êÊàñÊµôÊ±üÁúÅÂ∏∏‰ΩèÊà∑Âè£ÁöÑÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°Á¨¨‰∏ÄÊâπÊ¨°ÂÖ•Â≠¶ÁöÑ‰∏¥Â∫äÂåªÂ≠¶‰∏ì‰∏öÊú¨ÁßëÂ≠¶ÂéÜÁöÑÊØï‰∏öÁîü„ÄÅÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°È¢ÑÈò≤ÂåªÂ≠¶„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ‰∏ì‰∏öÊú¨ÁßëÂ≠¶ÂéÜÁöÑÊØï‰∏öÁîü„ÄÅÂÖ∑Êúâ‰∏¥Â∫äÊàñÂÖ¨ÂÖ±Âç´ÁîüÁ±ªÂà´Êâß‰∏öÂåªÂ∏àËµÑÊ†ºÁöÑÊØï‰∏öÁîü„ÄÇ3.ÂÆÅÊ≥¢Â§ßÂ∏ÇÁîüÊ∫ê2018Âπ¥ÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°(Âê´ÊôÆÈÄö‰∏≠Á≠â‰∏ì‰∏öÂ≠¶Ê†°)Â∫îÂ±äÊØï‰∏öÁîüÊàñÂÆÅÊ≥¢Â§ßÂ∏ÇÂ∏∏‰ΩèÊà∑Âè£ÁöÑÂõΩÊ∞ëÊïôËÇ≤Á≥ªÂàóÊØï‰∏öÁîü(ÊàëÂ∏ÇÂÆöÂêëÂüπÂÖª‰∏∫ÂÜúÊùëÁ§æÂå∫ÂåªÁîüÁöÑÊØï‰∏öÁîüÈôêÊä•Èô§‰ΩôÂßöÂ∏ÇÁ∫ßÂåªÁñóÂç´ÁîüÂçï‰Ωç‰ª•Â§ñÁöÑÂçï‰Ωç)„ÄÇ4.Âú®ÊàëÂ∏ÇÂêÑÁ±ªÂåªÁñóÂç´ÁîüËÆ°ÁîüÊú∫ÊûÑËøûÁª≠Â∑•‰ΩúÊª°‰∏§Âπ¥Âπ∂Áé∞Âú®Â≤ó(Â∑•‰ΩúÊó∂Èó¥ÁÆóËá≥2018Âπ¥4Êúà4Êó•Ôºå‰∏ãÂêå)„ÄÅÊåâËßÑÂÆöÁ≠æËÆ¢Âä≥Âä®ÂêàÂêåÂíåÁº¥Á∫≥ÂÖªËÄÅ‰øùÈô©Ôºå‰∏îÂÖ∑ÊúâÊâß‰∏ö(Âä©ÁêÜ)ÂåªÂ∏à„ÄÅÊâß‰∏öÊä§Â£´„ÄÅÂàùÁ∫ß(Â£´)Âèä‰ª•‰∏äËµÑÊ†ºÁöÑ‰∫∫ÂëòÊà∑Á±ç‰∏çÈôê„ÄÇÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°ÊØï‰∏öÁîüÂåÖÊã¨2018Âπ¥Â∫îÂ±äÊØï‰∏öÁîüÂíåÂéÜÂ±äÊØï‰∏öÁîü„ÄÇÊãõËÅòËÅå‰ΩçÂè¶ÊúâËßÑÂÆöÁöÑÔºå‰ªéÂÖ∂ËßÑÂÆöÔºõÂ∏∏‰ΩèÊà∑Âè£ÂÖ•Êà∑Êó∂Èó¥Êà™Ê≠¢2018Âπ¥4Êúà4Êó•„ÄÇ‰ΩôÂßöÂ∏ÇÂç´ÁîüËÆ°ÁîüÁ≥ªÁªüÂú®ÁºñÂíåÂ∑≤Ë¢´Á°ÆÂÆö‰∏∫ÊãüËÅòÁî®„ÄÅÂΩïÁî®‰∫∫Âëò‰∏çÂàóÂÖ•Êú¨Ê¨°ÊãõËÅòÂØπË±°ËåÉÂõ¥„ÄÇ(‰∫å)ÊãõËÅòÊù°‰ª∂1.Êã•Êä§ÂÖöÁöÑË∑ØÁ∫ø„ÄÅÊñπÈíà„ÄÅÊîøÁ≠ñÔºåÈÅµÁ∫™ÂÆàÊ≥ïÔºåÂìÅË°åÁ´ØÊ≠£ÔºåÁà±Â≤óÊï¨‰∏ö„ÄÇ2.1988Âπ¥1Êúà1Êó•Âèä‰ª•ÂêéÂá∫ÁîüÔºåË∫´‰ΩìÂÅ•Â∫∑„ÄÇ3.ÂÖ∑ÊúâÁ¨¶ÂêàÊãõËÅòËÅå‰ΩçË¶ÅÊ±ÇÁöÑÂ≠¶ÂéÜ„ÄÅ‰∏ì‰∏ö„ÄîÊä•ËÄÉËÄÖÊâÄÊåÅÁöÑÂ≠¶ÂéÜËØÅ‰π¶È°ªÁ¨¶ÂêàÊãõËÅòËÅå‰ΩçÂèÇÂä†Áõ∏Â∫îÁöÑÊâß‰∏ö(Âä©ÁêÜ)ÂåªÂ∏àËµÑÊ†ºËÄÉËØï„ÄÅÊâß‰∏öÊä§Â£´ËµÑÊ†ºËÄÉËØï„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËÄÉËØïÂØπÊñáÂá≠ÁöÑÁõ∏ÂÖ≥Ë¶ÅÊ±Ç„Äï„ÄÇ4.ÂõΩÂ§ñ„ÄÅÊ∏ØÊæ≥Âè∞ÁïôÂ≠¶ÂõûÂõΩ(Â¢É)‰∫∫ÂëòÂ∫îËÅòÊó∂ÔºåÈ°ªÂ∑≤ÂèñÂæóÂõΩÂÆ∂ÊïôËÇ≤ÈÉ®ËÆ§ÂÆöÁöÑÂ≠¶ÂéÜ(Â≠¶‰Ωç)ËØÅ‰π¶Ôºå‰∏ì‰∏öÁõ∏ËøëÁöÑ‰ª•ÊâÄÂ≠¶ËØæÁ®ã‰∏ì‰∏öÂêçÁß∞‰∏∫ÂáÜ„ÄÇ5.ÂØπ2016Âπ¥8Êúà31Êó•Âèä‰πãÂâçÊØï‰∏öÁöÑÊä•ËÄÉËÄÖÂ∫îÊåÅÊúâ‰∏éÊãõËÅòËÅå‰ΩçÁõ∏ÂØπÂ∫îÁöÑÊâß‰∏öËµÑÊ†ºËØÅ‰π¶(‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËØÅ‰π¶)„ÄÇ6.ÂØπÊä•ËÄÉÂ∏ÇÁ∫ßÂåªÁñóÂç´ÁîüÂçï‰ΩçÂ≠¶ÂéÜË¶ÅÊ±Ç‰∏∫Êú¨ÁßëÂèä‰ª•‰∏äËÅå‰ΩçËÄÖ(Êä•ËÄÉÂÆöÂêëËÅå‰ΩçËÄÖÈô§Â§ñ)ÔºåË¶ÅÊ±Ç‰∏∫ÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°ÊØï‰∏öÁîüÔºåÂπ∂ÂÖ∑ÊúâÁõ∏Â∫î‰∏ì‰∏öÁöÑÂ≠¶Â£´Âèä‰ª•‰∏äÂ≠¶‰ΩçÂíåÂ§ñËØ≠ÂõΩÂÆ∂ÂõõÁ∫ßÂèä‰ª•‰∏äÊàêÁª©ÂêàÊ†ºÂçï„ÄÇ7.Êä•ËÄÉÂÆöÂêëËÅå‰ΩçËÄÖÔºåÈ°ªÂÖ∑ÊúâÁõ∏Â∫îÁöÑÊâß‰∏ö(Âä©ÁêÜ)ÂåªÂ∏àÁ≠âËµÑÊ†ºÔºåÂÖ∂‰∏≠Êä•ËÄÉÂ∏ÇÁ∫ßÂåªÁñóÂçï‰Ωç‰∏¥Â∫ä„ÄÅ‰∏≠Âåª„ÄÅÂè£ËÖî„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠Á≠âÂÆöÂêëËÅå‰ΩçËÄÖÈ°ªÂÖ∑ÊúâÁõ∏Â∫îÁöÑÊâß‰∏öÂåªÂ∏àËµÑÊ†º„ÄÇ8.ÂØπ‰∏ãÂàóÊä•ËÄÉËÄÖÂπ¥ÈæÑÂèØÊîæÂÆΩÔºöÂÖ∑ÊúâÁ°ïÂ£´Âèä‰ª•‰∏äÂ≠¶‰ΩçÁöÑÂÖ®Êó•Âà∂ÊôÆÈÄöÈ´òÊ†°ÊØï‰∏öÁöÑÁ†îÁ©∂ÁîüÊàñÂÖ∑ÊúâÁõ∏Â∫îÁöÑËµÑÊ†ºÂπ∂Êä•ËÄÉÂÆöÂêëËÅå‰ΩçËÄÖÔºåÂπ¥ÈæÑÂèØÊîæÂÆΩËá≥1983Âπ¥1Êúà1Êó•Âèä‰ª•ÂêéÂá∫ÁîüÔºõÂÖ∑ÊúâÁõ∏Â∫îÂâØ‰∏ª‰ªªÂåªÂ∏à(ËçØÂ∏à„ÄÅÊäÄÂ∏à„ÄÅÊä§Â∏à)Âèä‰ª•‰∏äËµÑÊ†ºËÄÖÔºåÂπ¥ÈæÑÂèØÊîæÂÆΩËá≥1973Âπ¥1Êúà1Êó•Âèä‰ª•ÂêéÂá∫Áîü„ÄÇÊãõËÅòËÅå‰ΩçÂè¶ÊúâËßÑÂÆöÁöÑÔºå‰ªéÂÖ∂ËßÑÂÆö„ÄÇ‰∏â„ÄÅÊãõËÅòÁ®ãÂ∫è„ÄÅÂäûÊ≥ïÊãõËÅòÂ∑•‰ΩúË¥ØÂΩªÂÖ¨ÂºÄ„ÄÅÂπ≥Á≠â„ÄÅÁ´û‰∫â„ÄÅÊã©‰ºòÂéüÂàôÔºåÈááÂèñÂÖ¨ÂºÄÊä•Âêç„ÄÅÁªü‰∏ÄËÄÉËØï„ÄÅ‰ΩìÊ£Ä„ÄÅËÄÉÂØü„ÄÅÊã©‰ºòËÅòÁî®ÁöÑÂäûÊ≥ïËøõË°å„ÄÇÂÖ∑‰ΩìÁ®ãÂ∫èÂíåÂäûÊ≥ïÔºö(‰∏Ä)Êä•ÂêçÊó∂Èó¥Ôºö4Êúà10Êó•‚Äï4Êúà11Êó•(‰∏äÂçà8Ôºö45 ÔΩû11Ôºö45Ôºå‰∏ãÂçà13Ôºö45ÔΩû16Ôºö45)„ÄÇÂú∞ÁÇπÔºöÂ∏ÇÂç´ÁîüËøõ‰øÆÂ≠¶Ê†°ÂÜÖ(‰∏ñÂçóË•øË∑Ø139Âè∑)„ÄÇÂäûÊ≥ïÔºöÊåÅÊú¨‰∫∫Êà∑Âè£Á∞ø(ÊàñÊà∑Á±çËØÅÊòé)„ÄÅË∫´‰ªΩËØÅ„ÄÅÊØï‰∏öËØÅ‰π¶(Â∫îÂ±äÊØï‰∏öÁîüÈúÄÊåÅÂ≠¶ÁîüËØÅ„ÄÅÈ´òÊ†°Ê†∏ÂèëÁöÑÂ∞±‰∏öÊé®ËçêË°®„ÄÅÂ∞±‰∏öÂçèËÆÆ‰π¶)ÂíåËÅå‰ΩçÊãõËÅòÊù°‰ª∂ÊâÄË¶ÅÊ±ÇÁöÑÁõ∏ÂÖ≥ËØÅ‰π¶(ËØÅÊòé)Âéü‰ª∂ÂèäÂ§çÂç∞‰ª∂ÔºåËøëÊúüÂÖçÂÜ†1ÂØ∏ÁÖßÁâá3Âº†ÔºåÂπ∂Â°´ÂÜô„Ää‰ΩôÂßöÂ∏ÇÊãõËÅòÂç´ÁîüÊäÄÊúØ‰∫∫ÂëòÊä•ÂêçÁôªËÆ∞Ë°®„Äã„ÄÇÂà∞Êä•ÂêçÁé∞Âú∫Êä•ÂêçÂπ∂Êé•ÂèóËµÑÊ†ºÂàùÂÆ°ÔºåÂàùÂÆ°ÂêàÊ†ºËÄÖÔºåÁº¥Á∫≥ËÄÉÂä°Ë¥π100ÂÖÉ„ÄÇ‰ªÖÁ¨¶ÂêàÊãõËÅòÂØπË±°Á¨¨4ÁÇπÂç≥Âú®ÊàëÂ∏ÇÂêÑÁ±ªÂåªÁñóÂç´ÁîüËÆ°ÁîüÊú∫ÊûÑËøûÁª≠Â∑•‰ΩúÊª°‰∏§Âπ¥Âπ∂Áé∞Âú®Â≤ó„ÄÅÊåâËßÑÂÆöÁ≠æËÆ¢Âä≥Âä®ÂêàÂêåÂíåÁº¥Á∫≥ÂÖªËÄÅ‰øùÈô©Ôºå‰∏îÂÖ∑ÊúâÊâß‰∏ö(Âä©ÁêÜ)ÂåªÂ∏à„ÄÅÊâß‰∏öÊä§Â£´„ÄÅÂàùÁ∫ß(Â£´)Âèä‰ª•‰∏äËµÑÊ†ºÁöÑÊä•ÂêçÂØπË±°ÔºåËøòÈúÄÊèê‰æõÂ∑•‰ΩúËØÅÊòé„ÄÅÂä≥Âä®ÂêàÂêåÂíåÁº¥Á∫≥ÂÖªËÄÅ‰øùÈô©ËØÅÊòé„ÄÇÊØè‰∫∫ÈôêÊä•‰∏Ä‰∏™ËÅå‰ΩçÔºåÂ§öÊä•Êó†Êïà„ÄÇÂáÜËÄÉËØÅÂèëÊîæÔºöÊä•ÂêçËÄÖÂá≠Êú¨‰∫∫Ë∫´‰ªΩËØÅÂà∞Â∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±ÄÈò≥Êòé‰∏úË∑ØÂäûÂÖ¨Âå∫(Èò≥Êòé‰∏úË∑Ø127Âè∑)ÁªÑÁªá‰∫∫‰∫ãÁßëÈ¢ÜÂèñ„ÄÇÂáÜËÄÉËØÅÈ¢ÜÂèñÊó∂Èó¥Êä•ÂêçÂΩìÂ§©Âè¶Ë°åÂëäÁü•ÔºåÈÄæÊúü‰∏çÈ¢ÜËßÜ‰ΩúÊîæÂºÉËÄÉËØï„ÄÇ(‰∫å)ËÄÉËØï(ÂåÖÊã¨Á¨îËØïÂíåÈù¢ËØï)1.Á¨îËØïÔºöÁ¨îËØïÁßëÁõÆ‰∏∫Âç´ÁîüÁõ∏ÂÖ≥ÁªºÂêàÂü∫Á°ÄÁü•ËØÜÂíåÁõ∏Â∫îËÅå‰Ωç‰∏ì‰∏öÁü•ËØÜ‰∏§Èó®ÔºåÊØèÈó®Êª°ÂàÜÊåâ100ÂàÜËÆ°ÁÆó„ÄÇÂç´ÁîüÁõ∏ÂÖ≥ÁªºÂêàÂü∫Á°ÄÁü•ËØÜÂÜÖÂÆπÂåÖÊã¨Âç´ÁîüÊ≥ïÂæãÊ≥ïËßÑ„ÄÅÂåªÂ≠¶‰º¶ÁêÜÂ≠¶Á≠âÁü•ËØÜÔºõÁõ∏Â∫îËÅå‰Ωç‰∏ì‰∏öÁü•ËØÜÂÜÖÂÆπ‰∏∫ÊãõËÅòËÅå‰ΩçÊâÄË¶ÅÊ±ÇÁöÑÁõ∏ÂÖ≥‰∏ì‰∏öÁü•ËØÜ„ÄÇÁ¨îËØïÈááÂèñÈó≠Âç∑ÂΩ¢Âºè„ÄÇÁ¨îËØïÂÖ∑‰ΩìÊó∂Èó¥„ÄÅÂú∞ÁÇπ‰ª•ÂáÜËÄÉËØÅ‰∏∫ÂáÜ„ÄÇÂ∫îËÅòËÄÖËã•Á¨îËØïÊàêÁª©‰∏çË∂≥48ÂàÜ(ÊäòÂêàÂàÜ)Ôºå‰∏î‰Ωé‰∫éÂêåÁ±ªËÅå‰ΩçÊâÄÊúâÂÆûÈôÖÂèÇÂä†Á¨îËØïÂπ∂ÂèñÂæóÊúâÊïàÂàÜÊï∞‰∫∫ÂëòÂπ≥ÂùáÊàêÁª©(ÊäòÂêàÂàÜ)ÁöÑ90%„ÄîÂ∏ÇÊÄ•ÊïëÁ´ô‰∏¥Â∫äÂ§ß‰∏ìÂèä‰ª•‰∏ä(ËÅå‰Ωç22)ÊîæÂÆΩËá≥60%Ôºå‰øùÁïôÂ∞èÊï∞ÁÇπÂêé‰∏§‰Ωç„ÄïÔºåÂàôÊ∑òÊ±∞ÔºåÂèñÊ∂àÈù¢ËØïËµÑÊ†º„ÄÇ2.Èù¢ËØïÔºöÊåâÁÖßÊãõËÅòËÅå‰ΩçÂÖàÊ†πÊçÆÊäòÁÆóÂêéÁöÑÁ¨îËØïÊàêÁª©‰ªéÈ´òÂàÜÂà∞‰ΩéÂàÜËøõË°åÊéíÂêçÔºåÂÜçÊåâ‰ª•‰∏ãÊØî‰æãÁ°ÆÂÆöÊéíÂêçÂú®ÂâçÁöÑÂ∫îËÅòËÄÖ‰∏∫Èù¢ËØïÂØπË±°ÔºöÊãõËÅòËÅå‰ΩçÈúÄÊ±Ç‰∫∫Êï∞‰∏∫1‰∫∫ÁöÑÊåâËÅå‰ΩçÊåáÊ†á‰ª•1Ôºö3ÁöÑÊØî‰æãÁ°ÆÂÆöÈù¢ËØïÂØπË±°ÔºåÊãõËÅòËÅå‰ΩçÈúÄÊ±Ç‰∫∫Êï∞‰∏∫2Ëá≥4‰∫∫ÁöÑÊåâËÅå‰ΩçÊåáÊ†á‰ª•1Ôºö2ÁöÑÊØî‰æãÁ°ÆÂÆöÈù¢ËØïÂØπË±°ÔºåÊãõËÅòËÅå‰ΩçÈúÄÊ±Ç‰∫∫Êï∞Âú®5‰∫∫Âèä‰ª•‰∏äÁöÑÊåâËÅå‰ΩçÊåáÊ†á‰ª•1Ôºö1.5ÁöÑÊØî‰æãÁ°ÆÂÆöÈù¢ËØïÂØπË±°(ÊåâÂõõËàç‰∫îÂÖ•‰øùÁïôÊï¥Êï∞ÔºåÊØî‰æãÂÜÖÊúÄÂêé‰∏ÄÂêçÂàÜÊï∞Âπ∂ÂàóËÄÖÂÖ®ÈÉ®ÂÖ•Èó±Èù¢ËØï)Ôºå‰∏çË∂≥ËßÑÂÆöÊØî‰æãÁöÑÊåâÂÆûÈôÖ‰∫∫Êï∞Á°ÆÂÆö„ÄÇÊä§ÁêÜËÅå‰ΩçÁöÑÈù¢ËØï‰ª•ÂÆûË∑µËÄÉËØïÂΩ¢ÂºèËøõË°åÔºõÂÖ∂ÂÆÉËÅå‰ΩçÁöÑÈù¢ËØï‰∏ªË¶ÅÊµãËØÑÂ∫îËÅòËÄÖÂè£Â§¥Ë°®ËææËÉΩÂäõ„ÄÅÁé∞Âú∫Â∫îÂèòËÉΩÂäõ„ÄÅÁªºÂêàÂàÜÊûêËÉΩÂäõ„ÄÅÂõûÁ≠îÈóÆÈ¢òÂáÜÁ°ÆÊÄß„ÄÅÁü•ËØÜÈù¢Âíå‰∏æÊ≠¢‰ª™Ë°®Á≠â„ÄÇÈù¢ËØïÊàêÁª©Êª°ÂàÜ‰∏∫100ÂàÜÔºå‰∏çË∂≥60ÂàÜËÄÖÊ∑òÊ±∞„ÄÇÈù¢ËØïÊó∂Èó¥„ÄÅÂú∞ÁÇπÂè¶Ë°åÈÄöÁü•„ÄÇËÄÉËØïÊàêÁª©=Âç´ÁîüÁõ∏ÂÖ≥ÁªºÂêàÂü∫Á°ÄÁü•ËØÜÊàêÁª©√ó10%+Áõ∏Â∫îËÅå‰Ωç‰∏ì‰∏öÁü•ËØÜÊàêÁª©√ó70%+Èù¢ËØïÊàêÁª©√ó20%„ÄÇ(‰∏â)‰ΩìÊ£ÄÊ†πÊçÆËÄÉËØïÊàêÁª©Ôºå‰ªéÈ´òÂàÜÂà∞‰ΩéÂàÜ(Âêå‰∏ÄÊãõËÅòËÅå‰ΩçÂ∫îËÅòËÄÖËÄÉËØïÊàêÁª©Âπ∂ÂàóÁöÑÔºå‰æùÊ¨°ÊåâÁ¨îËØïÊàêÁª©„ÄÅÁõ∏Â∫îËÅå‰Ωç‰∏ì‰∏öÁü•ËØÜÁ¨îËØïÊàêÁª©È´òËÄÖÊéíÂêçÂú®ÂÖà„ÄÇ‰ªç‰∏çËÉΩÂå∫ÂàÜÊéíÂêçÁöÑÔºåÂàôÂè¶Ë°åÂ¢ûÂä†ÊµãËØïÂÜÖÂÆπÔºåÊµãËØïÊàêÁª©È´òËÄÖÊéíÂêçÂú®ÂÖàÔºå‰∏ãÂêå)ÊåâÊãõËÅòËÅå‰ΩçÊåáÊ†á‰ª•1Ôºö1ÁöÑÊØî‰æãÁ°ÆÂÆö‰ΩìÊ£ÄÂØπË±°„ÄÇÂêåÁ±ªËÅå‰ΩçÂÆûÈôÖÂèÇÂä†Á¨îËØï‰ªÖÊúâ1‰∫∫ÁöÑÔºåËã•Â∫îËÅòËÄÖËÄÉËØïÊàêÁª©Êú™ËææÂà∞60ÂàÜÔºåÂèñÊ∂à‰ΩìÊ£ÄËµÑÊ†º„ÄÇ‰ΩìÊ£ÄÂèÇÁÖßÂÆÅÊ≥¢Â∏ÇËÄÉÂΩïÂÖ¨Âä°ÂëòÁöÑÁõ∏ÂÖ≥ÂäûÊ≥ïÂíåÊ†áÂáÜÊâßË°å(Â¶ÇÊúâË°•ÂÖÖËßÑÂÆöÔºåÂàôÂú®‰ΩìÊ£ÄÂâçÂè¶Ë°åÂÖ¨Â∏É)„ÄÇÊîæÂºÉ‰ΩìÊ£ÄÊàñ‰ΩìÊ£ÄÁªìÊûú‰∏çÂêàÊ†ºËÄÖÊ∑òÊ±∞Ôºå‰ΩìÊ£ÄÊó∂Èó¥„ÄÅÂú∞ÁÇπÂè¶Ë°åÈÄöÁü•„ÄÇ(Âõõ)ËÄÉÂØü‰ΩìÊ£ÄÂêàÊ†ºËÄÖ‰∏∫ËÄÉÂØüÂØπË±°Ôºå‰∏ªË¶ÅËÄÉÂØüË¢´ËÄÉÂØüÂØπË±°ÁöÑÂæ∑ÊâçË°®Áé∞ÂíåÂ∫îËÅòËµÑÊ†ºÊù°‰ª∂Á≠âÊÉÖÂÜµÔºåËÄÉÂØüÁªìÊûú‰∏∫‰∏çÂÆúËÅòÁî®‰∏∫‰∫ã‰∏ö‰∫∫ÂëòËÄÖÊ∑òÊ±∞(ËØ¶ËßÅÈôÑ‰ª∂2)„ÄÇËÄÉÂØüÂØπË±°‰∏∫2018Âπ¥Â∫îÂ±äÊØï‰∏öÁöÑÔºåÈ°ªÂú®2018Âπ¥7Êúà30Êó•ÂâçÂ∞ÜÊú¨‰∫∫ÁöÑÊØï‰∏öËØÅ‰π¶„ÄÅÂ≠¶‰ΩçËØÅ‰π¶„ÄÅÂ§ñËØ≠ÊàêÁª©ÂêàÊ†ºÂçï„ÄÅÂÖ•Â≠¶ÊâπÊ¨°ËØÅÊòéÁ≠âÁõ∏Â∫îÊùêÊñô‰∫§Ëá≥‰ΩôÂßöÂ∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±ÄËøõË°åÈ™åËØÅÔºåÈÄæÊúüËßÜ‰∏∫ÊîæÂºÉËÅòÁî®ËµÑÊ†º„ÄÇ(‰∫î)ËÅòÁî®ËÄÉÂØüÂêàÊ†ºËÄÖ‰∏∫ÊãüËÅòÁî®ÂØπË±°ÔºåÁªèÂÖ¨Á§∫7Â§©Êó†ÂºÇËÆÆÁöÑÔºåÊåâÊùÉÈôêÊâπÂáÜÂêé‰∫à‰ª•ËÅòÁî®(Â∑≤ÂèÇÂä†Â∑•‰ΩúÁöÑÊãüËÅòÁî®ÂØπË±°ÔºåÈ°ªÊåâÊúâÂÖ≥ËßÑÂÆöÂäûÁêÜÂ•Ω‰∏éÂéüÂçï‰ΩçÁªàÊ≠¢‰∫∫‰∫ãÂÖ≥Á≥ªÁöÑÊâãÁª≠)ÔºåÂπ∂ÂäûÁêÜÊä•Âà∞ËÅòÁî®ÊâãÁª≠ÔºõÊúâÂºÇËÆÆÁöÑÔºåÁªèÊ†∏ÂÆûÂ¶Ç‰∏çÂÆúËÅòÁî®‰∏∫‰∫ã‰∏ö‰∫∫ÂëòÁöÑÔºåÂèñÊ∂àËÅòÁî®ËµÑÊ†º„ÄÇ‰ΩìÊ£ÄÂêàÊ†º‰∫∫ÂëòÂõ†‰∏™‰∫∫ÂéüÂõ†Âú®ËßÑÂÆöÊó∂Èó¥ÂÜÖ‰∏çËÉΩÊèê‰æõ‰∏™‰∫∫Ê°£Ê°àÊàñÊãüËÅòÁî®‰∫∫ÂëòÈÄæÊúü‰∏çÊåâËßÑÂÆöÊä•Âà∞ÁöÑÔºåÂèñÊ∂àËÅòÁî®ËµÑÊ†º„ÄÇÊú¨Ê¨°ÊãõËÅòÔºåÂõ†ËÄÉÁîü‰∏ªÂä®ÊîæÂºÉ‰ΩìÊ£ÄÂá∫Áé∞ÁöÑÁ©∫Áº∫ÊåâËÄÉËØïÊàêÁª©‰ªéÈ´òÂàÜÂà∞‰ΩéÂàÜ‰æùÊ¨°ÈÄíË°•ÔºåÂÖ∂ÂÆÉÂéüÂõ†Âá∫Áé∞Á©∫Áº∫‰∏çËøõË°åÈÄíË°•ÔºõÂ¶ÇÊó†ÂêàÈÄÇÂØπË±°ÔºåÂÖÅËÆ∏ÊãõËÅò‰∏çË∂≥ÊàñÁ©∫Áº∫„ÄÇÊãõËÅòÂ∑•‰ΩúÁõ∏ÂÖ≥ÈÄöÁü•„ÄÅËÄÉËØïÊàêÁª©„ÄÅ‰ΩìÊ£ÄÁªìÊûúÂèäÊãüËÅòÁî®ÂØπË±°ÂêçÂçïÂ∞ÜÂú®‰ΩôÂßö‰∫∫ÊâçÁΩë( www.yyrc.com )Âíå‰ΩôÂßöÂ∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±ÄÁΩëÁ´ô ( www.yy.gov.cn/col/col73829/index.html) ‰∏äÂÖ¨Â∏É„ÄÇÊñ∞ËÅòÁî®ÁöÑ‰∫ã‰∏öÂçï‰ΩçÂ∑•‰Ωú‰∫∫ÂëòÔºåÊåâÊúâÂÖ≥ËßÑÂÆöÂÆûË°å‰∫∫‰∫ã‰ª£ÁêÜÂíåËØïÁî®Êúü(ËßÅ‰π†Êúü)ÔºåÂπ∂ÊåâËßÑÂÆöÁ≠æËÆ¢ËÅòÁî®ÂêàÂêå„ÄÇÁªèËØïÁî®ÊúüÊàñËßÅ‰π†ÊúüÊª°Âπ∂ÁªèËÄÉÊ†∏ÂêàÊ†ºÂêé‰∫à‰ª•Ê≠£ÂºèËÅòÁî®ÔºåËÄÉÊ†∏‰∏çÂêàÊ†ºËÄÖÂèñÊ∂àËÅòÁî®ËµÑÊ†º„ÄÇ2016Âπ¥9Êúà1Êó•Âèä‰πãÂêéÊØï‰∏öÊú™ÂèñÂæóÊâß‰∏öËµÑÊ†º„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºÁöÑÂ∫îËÅòËÄÖÔºåÊåâÁÖßÊ≠£Â∏∏ÁöÑÊâß‰∏öËµÑÊ†ºËÄÉËØï„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËÄÉËØïÁöÑÊó∂Èó¥ËßÑÂÆöÔºåËÅòÁî®ÂêéÂú®2019Âπ¥12Êúà31Êó•ÂâçÊú™ËÉΩÈÄöËøáÁõ∏Â∫îÁöÑÊâß‰∏öËµÑÊ†ºËÄÉËØï„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËÄÉËØïÁöÑÔºå‰ΩúÂæÖÂ≤óÂ§ÑÁêÜÔºõÂú®2020Âπ¥12Êúà31Êó•Ââç‰ªçÊú™ËÉΩÈÄöËøáÁõ∏Â∫îÁöÑÊâß‰∏öËµÑÊ†ºËÄÉËØï„ÄÅ‰∏ì‰∏öÊäÄÊúØËµÑÊ†ºËÄÉËØïÁöÑÔºåÂàôËß£Èô§ËÅòÁî®ÂêàÂêå„ÄÇÂÆöÂêëËÅå‰ΩçÂΩïÁî®‰∫∫ÂëòÂøÖÈ°ªÂú®ÂΩïÁî®Âçï‰ΩçÊúçÂä°‰∏ÉÂπ¥‰ª•‰∏ä„ÄÇÂ∑•ËµÑÁ¶èÂà©ÂæÖÈÅáÊåâÊúâÂÖ≥ËßÑÂÆöÊâßË°å„ÄÇÊú¨ÁÆÄÁ´†‰∏≠ÊâÄÁß∞ÁöÑÂ∏ÇÁ∫ßÂåªÁñóÂç´ÁîüÂçï‰ΩçÊòØÊåáÂ∏Ç‰∫∫Ê∞ëÂåªÈô¢„ÄÅÂ∏Ç‰∏≠ÂåªÂåªÈô¢„ÄÅÂ∏ÇÁ¨¨‰∫å‰∫∫Ê∞ëÂåªÈô¢„ÄÅÂ∏ÇÁ¨¨‰∏â‰∫∫Ê∞ëÂåªÈô¢„ÄÅÂ∏ÇÁ¨¨Âõõ‰∫∫Ê∞ëÂåªÈô¢„ÄÅÂ∏ÇÊ¢®Ê¥≤ÂåªÈô¢(Â∫∑Â§çÂåªÈô¢)„ÄÅÂ∏ÇÂç´Ê†°„ÄÅÂ∏ÇÊÄ•ÊïëÁ´ô„ÄÇÂÖ∂‰ªñÂ∏ÇÁ∫ßÂåªÈô¢ÊòØÊåáÂ∏Ç‰∏≠ÂåªÂåªÈô¢„ÄÅÂ∏Ç‰∫åÈô¢„ÄÅÂ∏Ç‰∏âÈô¢„ÄÅÂ∏ÇÊ¢®Ê¥≤ÂåªÈô¢(Â∫∑Â§çÂåªÈô¢)„ÄÇÊú¨ÁÆÄÁ´†‰∏≠ÊâÄÁß∞ÁöÑÂêåÁ±ªËÅå‰ΩçÂàÜ‰∏∫‰∏¥Â∫äÁ±ªËÅå‰Ωç„ÄÅ‰∏≠ÂåªÁ±ªËÅå‰Ωç„ÄÅÈíàÁÅ∏Êé®ÊãøÁ±ªËÅå‰Ωç„ÄÅÂè£ËÖîÁ±ªËÅå‰Ωç„ÄÅÂÖ¨ÂÖ±Âç´ÁîüÁ±ªËÅå‰Ωç„ÄÅÂåªÂ≠¶ÂΩ±ÂÉè(DSA‰ªãÂÖ•ÂåªÂ∏à„ÄÅDSA‰ªãÂÖ•ÊäÄÂ∏à„ÄÅECTÊäÄÂ∏à„ÄÅÊîæÁñóÊäÄÂ∏à„ÄÅÂåªÂ≠¶ÂΩ±ÂÉèÊäÄÊúØ)Á±ªËÅå‰Ωç„ÄÅÂåªÂ≠¶Ê£ÄÈ™åÁ±ªËÅå‰Ωç„ÄÅËçØÂâÇÁ±ªËÅå‰Ωç„ÄÅ‰∏≠ËçØÁ±ªËÅå‰Ωç„ÄÅÊä§ÁêÜ(Âä©‰∫ß)Á±ªËÅå‰Ωç„ÄÅÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ã(ÊîæÁñóÁâ©ÁêÜÂ∏à)Á±ªËÅå‰Ωç„ÄÅÂ∫∑Â§çÂåªÂ≠¶Á±ªËÅå‰Ωç„ÄÅÂ∫∑Â§çÊ≤ªÁñóÁ±ªËÅå‰Ωç„ÄÇÊú¨ÁÆÄÁ´†ÂèäÈôÑ‰ª∂‰∏çÂÜçÂú®Êä•ÂêçÁé∞Âú∫ÂèëÊîæÔºåÂ∫îËÅòËÄÖËØ∑Âú®‰ΩôÂßö‰∫∫ÊâçÁΩëÊàñ‰ΩôÂßöÂ∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±ÄÁΩëÁ´ô‰∏äËá™Ë°å‰∏ãËΩΩ„ÄÇÊú¨ÁÆÄÁ´†Áî±‰ΩôÂßöÂ∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±ÄË¥üË¥£Ëß£Èáä„ÄÇÂí®ËØ¢ÁîµËØù(Â∑•‰ΩúÊó•)Ôºö62686886„ÄÅ62672605ÔºõÁõëÁù£ÁîµËØùÔºö62682597„ÄÇ‰ΩôÂßöÂ∏Ç‰∫∫ÂäõËµÑÊ∫êÂíåÁ§æ‰ºö‰øùÈöúÂ±Ä‰ΩôÂßöÂ∏ÇÂç´ÁîüÂíåËÆ°ÂàíÁîüËÇ≤Â±Ä‰∫åÈõ∂‰∏ÄÂÖ´Âπ¥ÂõõÊúàÂõõÊó•"";
        StringBuilder sb = new StringBuilder();
        for (Term term : seg.seg(raw)) {
            sb.append(term.word).append("" "");
        }
        System.out.println(sb.toString());
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊâìÂç∞Âá∫Ê≠£Â∏∏ÁöÑÂàÜËØçÁªìÊûú
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÊäõÂá∫ÂºÇÂ∏∏
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList.getFirst(LinkedList.java:244)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:148)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:507)

"
‰∏Ä‰∏™Ëá™ÂÆö‰πâËØçÂ∫ìÁîüÊàêbinÊñá‰ª∂ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2




## ÊàëÁöÑÈóÆÈ¢ò

Ëá™ÂÆö‰πâËØçÂ∫ìÁîüÊàêbinÊñá‰ª∂ÔºåÈÄöËøáÈÖçÁΩÆhanlp.propertiesÊñá‰ª∂‰∏≠CustomDictionaryPathÔºåÈÖçÁΩÆÂêå‰∏ÄÁõÆÂΩï‰∏ãÂ§ö‰∏™Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÔºåHanlp‰∏çËÉΩ‰∏ÄÊ¨°ÁîüÊàêÊâÄÊúâËá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑbinÊñá‰ª∂ÔºåÊØèÊ¨°Âè™ËÉΩÁîüÊàêCustomDictionaryPathÈÖçÁΩÆÈ°πÁöÑÁ¨¨‰∏Ä‰∏™Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑbinÊñá‰ª∂ÔºåÂØºËá¥Ââ©‰∏ãÁöÑËá™ÂÆö‰πâÂ≠óÂÖ∏‰∏≠ÁöÑËØç‰∏çËÉΩËØÜÂà´ÊãÜÂàÜ„ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò
HadoopÈõÜÁæ§‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏

### Ê≠•È™§

1. È¶ñÂÖàÔºåÂú®Ëá™ÂÆö‰πâÁõÆÂΩïcustom‰∏ãÈù¢Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏(ÊØîÂ¶ÇÔºåÂ≠óÂÖ∏Âêç‰∏∫ÂÜõ‰∫ã.txt; Âä®Ê§çÁâ©.txt; ÂéÜÂè≤.txt; Â®±‰πê.txt; ÊóÖË°å.txt)
2. ÁÑ∂ÂêéÔºåÂú®hanlp.properties‰∏≠Ê∑ªÂä†CustomDictionaryPathÈÖçÁΩÆ(ÊØîÂ¶ÇÔºåCustomDictionaryPath=data/dictionary/custom/ÂÜõ‰∫ã.txt; Âä®Ê§çÁâ©.txt; ÂéÜÂè≤.txt; Â®±‰πê.txt; ÊóÖË°å.txt)
3. Êé•ÁùÄÔºåÊâßË°åHanlpÊãÜÂàÜÔºåÊâßË°åÂÆåÂêéÔºåÂè™ËÉΩÁîüÊàê""ÂÜõ‰∫ã.txt.bin""ÁöÑbinÊñá‰ª∂ÔºåÂä®Ê§çÁâ©„ÄÅÂéÜÂè≤„ÄÅÂ®±‰πê„ÄÅÊóÖË°åÂØπÂ∫îÁöÑbinÊñá‰ª∂‰∏çËÉΩÁîüÊàê

### Ëß¶Âèë‰ª£Á†Å


### ÊúüÊúõËæìÂá∫

ÈÄöËøáÈÖçÁΩÆÊñá‰ª∂ÂèØ‰ª•ÁîüÊàêÊâÄÊúâbinÊñá‰ª∂



### ÂÆûÈôÖËæìÂá∫

Âè™ÊúâÁ¨¨‰∏Ä‰∏™ÈÖçÁΩÆËØçÂÖ∏ÁöÑbinÊñá‰ª∂ÔºåÂØºËá¥ÂêéÁª≠ÂàÜËØçÔºåÈô§Á¨¨‰∏Ä‰∏™‰ª•Â§ñÁöÑÂ≠óÂÖ∏ÈÉΩ‰∏çËÉΩÁî®Êù•Ëá™ÂÆö‰πâÂàÜËØç



<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Merge pull request #1 from hankcs/master,"ÂêåÊ≠•Â∫ì‰ª£Á†Å

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
textrankÁÆóÊ≥ïÂÖ¨ÂºèÊ≤°ÊúâÊ≥®Èáä,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®ÈòÖËØªËøô‰∏™ÂçöÊñáÁöÑÊó∂ÂÄôÔºåÂèëÁé∞textrankÁÆóÊ≥ïÁöÑÂÖ¨ÂºèÊ≤°Ê≥®ÈáäËØ¥ÊòéÔºåÂ∏åÊúõÂçö‰∏ªÂèë‰∏Ä‰ªΩÂÖ¨ÂºèËØ¥ÊòéÔºåÊàëÊúÄËøëÂú®ÊêûÂÖ≥ÈîÆËØçÊèêÂèñÔºåÂ∏åÊúõÂÜçtextrankÁöÑÂü∫Á°Ä‰∏äÊîπËøõÁÆóÊ≥ï
ÂçöÊñáurlÔºö
http://www.hankcs.com/nlp/textrank-algorithm-to-extract-the-keywords-java-implementation.html

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

![image](https://user-images.githubusercontent.com/23119744/38651352-5cafd126-3e33-11e8-8b29-f866538276cb.png)


"
ÂàÜËØçÂá∫Áé∞ÈóÆÈ¢òÔºåÂ∫îËØ•Â¶Ç‰ΩïÊìç‰ΩúÊâçÂèØ‰ª•‰øÆÊ≠£Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
### JarÂíåËØçÂ∫ì
1.6.2 ReleaseÁâàJarÔºõ
1.6.2ÁöÑReleaseËØçÂ∫ìÔºàdata-for-1.6.2.zipÔºâÔºõ
hanlp.properties ‰ªÖ‰øÆÊîπ‰∫ÜrootÂú∞ÂùÄÔºõ

### ÈóÆÈ¢òÂàÜËØç
‰ª•‰∏ãËØ≠Âè•‰ΩøÁî®Ê†áÂáÜÂàÜËØçÔºàHanLP.segmentÔºâÂá∫Áé∞ÈóÆÈ¢òÔºö
#### ËØ≠Âè•
 Â≠¶Áîü‰ªéÁÆÄÁü≠ÁöÑËØóÊ≠å‰∏≠ÊÑüÂèóÂà∞Êú±Âæ∑ÂíåÊàòÂ£´‰ª¨ÁöÑÊ∑±ÂéöÊÉÖË∞ä„ÄÇ
#### ÂàÜËØçÁªìÊûú
Â≠¶Áîü/nnt ‰ªé/p ÁÆÄÁü≠/a ÁöÑ/ude1 ËØóÊ≠å/n ‰∏≠/f ÊÑüÂèóÂà∞/nz Êú±Âæ∑Âíå/nr ÊàòÂ£´/nnt ‰ª¨/k ÁöÑ/ude1 Ê∑±Âéö/a ÊÉÖË∞ä/n „ÄÇ/w

#### ÊèêÂèñÂÖ≥ÈîÆÂ≠óÁªìÊûú
[""ÊÑüÂèóÂà∞"",""Êú±Âæ∑Âíå"",""ËØóÊ≠å"",""ÊàòÂ£´"",""Ê∑±Âéö"",""ÁÆÄÁü≠"",""ÊÉÖË∞ä"",""Â≠¶Áîü""]
#### ÊèêÂèñÁü≠ËØ≠ÁªìÊûú
[""‰∏≠ÊÑüÂèóÂà∞"",""Â≠¶ÁîüÁÆÄÁü≠"",""ÊÑüÂèóÂà∞Êú±Âæ∑Âíå"",""ÊàòÂ£´Ê∑±Âéö"",""Êú±Âæ∑ÂíåÊàòÂ£´"",""Ê∑±ÂéöÊÉÖË∞ä"",""ÁÆÄÁü≠ËØóÊ≠å"",""ËØóÊ≠å‰∏≠""]

### ÊúüÊúõÁªìÊûú
 ÊÉ≥ÊääÊú±Âæ∑ÂàÜÂá∫Êù•ÔºåÂ∫îËØ•Â¶Ç‰ΩïÊìç‰ΩúÔºü
"
‰∫∫ÂêçËØÜÂà´ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4

## ÊàëÁöÑÈóÆÈ¢ò
Âú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Ê∑ªÂä†‰∫Ü‰∫∫ÂêçÔºåÂàÜËØçËøáÁ®ã‰∏≠Ê≤°ÊúâËØÜÂà´Âá∫Êù•„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
### Ê≠•È™§

1. Âú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Ê∑ªÂä†‰∫Ü‚ÄúËãèËãè nr 10‚ÄùÔºåÁÑ∂ÂêéÂà†Èô§Êéâ.binÁöÑÊñá‰ª∂„ÄÇ
2. ÈáçÊñ∞ËøêË°åÂàÜËØç‰ª£Á†ÅÔºåÂè•Â≠ê‰∏∫‚ÄúËãèËãè‰∏≠Á∫ß‰ºöËÆ°‰ªÄ‰πàÊó∂ÂÄôÊõ¥Êñ∞‚ÄùÔºåÂàÜËØçÁªìÊûú‰∏∫‚Äú[ËãèËãè‰∏≠Á∫ß, ‰ºöËÆ°, ‰ªÄ‰πà, Êó∂ÂÄô, Êõ¥Êñ∞]‚Äù„ÄÇ
3. ÊàëÂú®‰ª£Á†Å‰∏≠Ê∑ªÂä†
CustomDictionary.add(""ËãèËãè"");
StandardTokenizer.SEGMENT.enableCustomDictionary(true);
ÊâßË°åÁöÑÁªìÊûúËøòÊòØ‰∏ÄÊ†∑„ÄÇ
4. ÊàëÊâìÂºÄË∞ÉËØïÊ®°ÂºèÔºåHanLP.Config.enableDebug(); ËæìÂá∫ÂàÜËØçËøáÁ®ãÔºåËßÇÂØüÂà∞‚ÄúËØÜÂà´Âá∫‰∫∫ÂêçÔºöËãè‰∏≠Á∫ß XD
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöËãèËãè‰∏≠Á∫ß BXD‚Äú„ÄÇ‰ΩÜÊòØÊàëÂú®‰∫∫ÂêçËØçÂÖ∏‰∏≠Âπ∂Ê≤°ÊúâÂèëÁé∞Êúâ‚ÄúËãè‰∏≠Á∫ß‚ÄùÊàñËÄÖ‚ÄúËãèËãè‰∏≠Á∫ß‚ÄùËøôÊ†∑ÁöÑËØç„ÄÇ
5. ÊàëÂÖ≥Èó≠‰∫∫ÂêçËØÜÂà´ÂêéÔºåËæìÂá∫ÁªìÊûú‰∏çÊòØÊàëÊÉ≥Ë¶ÅÁöÑÔºåÂú®ÂÆûÈôÖ‰ΩøÁî®‰∏≠ËøòÊúâÂæàÂ§öÂè•Â≠êÂåÖÂê´‰∫∫ÂêçÔºåÊâÄ‰ª•ËøôÁßçÊñπÂºèÊàëÂ∞±ÊîæÂºÉ‰∫Ü„ÄÇ
### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        String s = ""ËãèËãè‰∏≠Á∫ß‰ºöËÆ°‰ªÄ‰πàÊó∂ÂÄôÊõ¥Êñ∞"";
        CustomDictionary.add(""ËãèËãè"");
        StandardTokenizer.SEGMENT.enableCustomDictionary(true);
        HanLP.Config.enableDebug();
        System.out.println(HanLP.segment(s));
    }
```
### ÊúüÊúõËæìÂá∫

[ËãèËãè, ‰∏≠Á∫ß, ‰ºöËÆ°, ‰ªÄ‰πà, Êó∂ÂÄô, Êõ¥Êñ∞]

### ÂÆûÈôÖËæìÂá∫

[ËãèËãè‰∏≠Á∫ß, ‰ºöËÆ°, ‰ªÄ‰πà, Êó∂ÂÄô, Êõ¥Êñ∞]

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÈôÑ‰∏äDEBUGÊó•ÂøóÔºö
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Ëãè B 2141 E 105 C 102 D 34 ][Ëãè‰∏≠ X 1 ][Á∫ß D 3 C 1 K 1 ][‰ºöËÆ° K 12 L 1 ][‰ªÄ‰πà L 4 K 3 ][Êó∂ÂÄô K 21 ][Êõ¥Êñ∞ Z 96 L 11 ][  A 20843310 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Ëãè/B ,Ëãè‰∏≠/X ,Á∫ß/D ,‰ºöËÆ°/L ,‰ªÄ‰πà/L ,Êó∂ÂÄô/K ,Êõ¥Êñ∞/Z , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöËãè‰∏≠Á∫ß XD
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöËãèËãè‰∏≠Á∫ß BXD
ÁªÜÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ëãè, ËãèËãè‰∏≠Á∫ß]
2:[Ëãè‰∏≠, Ëãè‰∏≠Á∫ß]
3:[]
4:[Á∫ß]
5:[‰ºöËÆ°]
6:[ËÆ°]
7:[‰ªÄ‰πà]
8:[]
9:[Êó∂ÂÄô]
10:[]
11:[Êõ¥Êñ∞]
12:[]
13:[ ]

"
ÂÖ≥‰∫éÊÑüÁü•Êú∫ÂàÜËØçÁöÑ‰∏ÄÁÇπÁñëÈóÆ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ªøÁÖßÊñáÊ°£‰∏≠ÁöÑÁ§∫‰æãÂèØ‰ª•Ê≠£Á°ÆËØÜÂà´‚Äú‰∏ãÈõ®Â§©Âú∞Èù¢ÁßØÊ∞¥‚ÄùÔºå‰ΩÜÊòØÂØπ‚Äú‰∏≠ÂõΩËÅîÈÄöÊòØÂÖ¨Âè∏‚ÄùÁöÑÂ≠¶‰π†‰∏çÂ¶ÇÈ¢ÑÊúüÔºå‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊàëÂú®‰ΩøÁî®‰∏äÊúâ‰∏çÂΩìÂØºËá¥„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->



### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        PerceptronSegmenter segmenter = new PerceptronSegmenter(HanLP.Config.PerceptronCWSModelPath);
            System.out.println(segmenter.segment(""‰∏ãÈõ®Â§©Âú∞Èù¢ÁßØÊ∞¥""));
            segmenter.learn(""‰∏ãÈõ®Â§© Âú∞Èù¢ ÁßØÊ∞¥"");
            System.out.println(segmenter.segment(""‰∏ãÈõ®Â§©Âú∞Èù¢ÁßØÊ∞¥""));
            System.out.println(segmenter.segment(""‰∏≠ÂõΩËÅîÈÄöÊòØÂÖ¨Âè∏""));
            segmenter.learn(""‰∏≠ÂõΩËÅîÈÄö ÊòØ ÂÖ¨Âè∏"");
            System.out.println(segmenter.segment(""‰∏≠ÂõΩËÅîÈÄöÊòØÂÖ¨Âè∏""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
            [‰∏ãÈõ®, Â§©Âú∞, Èù¢ÁßØ, Ê∞¥]
            [‰∏ãÈõ®Â§©, Âú∞Èù¢, ÁßØÊ∞¥]
            [‰∏≠ÂõΩ, ËÅîÈÄö, ÊòØ, ÂÖ¨Âè∏]
            [‰∏≠ÂõΩËÅîÈÄö, ÊòØ, ÂÖ¨Âè∏]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
            [‰∏ãÈõ®, Â§©Âú∞, Èù¢ÁßØ, Ê∞¥]
            [‰∏ãÈõ®Â§©, Âú∞Èù¢, ÁßØÊ∞¥]
            [‰∏≠ÂõΩ, ËÅîÈÄö, ÊòØ, ÂÖ¨Âè∏]
            [‰∏≠ÂõΩ, ËÅîÈÄö, ÊòØ, ÂÖ¨Âè∏]
```



"
 ÂÆåÂñÑÂúÜÂúàÊï∞Â≠óÂØπÂ∫îÂÖ≥Á≥ª,"
## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÂÆåÂñÑ‰∫ÜÂúÜÂúàÊï∞Â≠óÂØπÂ∫îÂÖ≥Á≥ªÔºå‰æãÂ¶Ç‚ëß=ÂÖ´Ôºõ
Ê≠§Â§ñ‰øÆÊ≠£‰∫Ü‚ë§ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºå‰πãÂâç‚ë§ÂØπÂ∫î‰ºçÔºåÁé∞Âú®Êîπ‰∏∫‰∫î

"
ÂØπ‰∫éÁ±ª‰ººÊó∂Èó¥ÊàñËÄÖÊó•ÊúüÂàÜËØçÊãÜÂàÜÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®ÂØπ""ÁÆ°ÁêÜÂëò‰∫é2017-10-12ÂØπËØ•ËÆæÂ§á‰ø°ÊÅØËøõË°å‰∫Ü‰øÆÊîπÊìç‰Ωú""ËøõË°åÊãÜÂàÜÊó∂Ôºå‚Äú2017-10-12‚ÄùËøô‰∏™Êó•Êúü‰ºöË¢´ÂçïÁã¨ÊãÜÂºÄ„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->



### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer =  new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.seg(""ÁÆ°ÁêÜÂëò‰∫é2017-10-12ÂØπËØ•ËÆæÂ§á‰ø°ÊÅØËøõË°å‰∫Ü‰øÆÊîπÊìç‰Ωú""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÁÆ°ÁêÜÂëò/n, ‰∫é/p, 2017-10-12, ÂØπ/p, ËØ•/r, ËÆæÂ§á/n, ‰ø°ÊÅØ/n, ËøõË°å/v, ‰∫Ü/u, ‰øÆÊîπ/v, Êìç‰Ωú/v
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÁÆ°ÁêÜÂëò/n, ‰∫é/p, 2017-/m, 10/m, -/q, 12/m, ÂØπ/p, ËØ•/r, ËÆæÂ§á/n, ‰ø°ÊÅØ/n, ËøõË°å/v, ‰∫Ü/u, ‰øÆÊîπ/v, Êìç‰Ωú/v
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Êìç‰ΩúÁªìÊûúÔºö

![12](https://user-images.githubusercontent.com/18030444/38286722-5e5c1772-37f9-11e8-83b0-eb77566f9c82.png)


<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
com.hankcs.hanlp.summary.KeywordExtractorÁ±ªÁöÑshouldIncludeÊñπÊ≥ïÔºå‰∏çËØÜÂà´‰∏Ä‰∏™Â≠óÁöÑËØç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.2.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.5.4.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

com.hankcs.hanlp.summary.KeywordExtractorÁ±ªÁöÑshouldIncludeÊñπÊ≥ïÔºå‰∏∫‰ªÄ‰πàÂà§Êñ≠‰∏Ä‰∏™Â≠óÁöÑËØç‰∏çÂú®ËÆ°ÁÆóËåÉÂõ¥ÂÜÖÔºåÂÉè  Èúæ  Ëøô‰∏™Â≠óÊú¨Ë∫´Â∞±ÊòØ‰∏Ä‰∏™ËØçÔºåÊòØ‰∏çÊòØÂ§™Ê≠¶Êñ≠‰∫Ü

## Â§çÁé∞ÈóÆÈ¢ò
### Ëß¶Âèë‰ª£Á†Å

public boolean shouldInclude(Term term)
    {
        // Èô§ÊéâÂÅúÁî®ËØç
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (_**term.word.trim().length() > 1**_ && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }


"
ÂØπÈÉ®ÂàÜ‰∫∫Âêç‰∏≠ÊúâÊï∞ËØçÔºàÂ¶ÇÂº†‰∏â‰∏∞ÔºâÔºåÂú®ÊãÜÂàÜÊó∂‰ºöÂá∫Áé∞‰∏çÂ¶ÇÈ¢ÑÊúüÊÉÖÂÜµ„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Â≠òÂú®Êüê‰∫õ‰∫∫Âêç‰∏≠ÊúâÊï∞ËØçÂá∫Áé∞ÁöÑ‰∫∫ÂêçÊó∂Ôºå‰ºöÂá∫Áé∞ÊãÜÂàÜ‰∏çÂ¶ÇÈ¢ÑÊúüÁöÑÈóÆÈ¢òÔºåÂ¶Ç‚ÄúÂº†‰∏â‰∏∞ÔºåÈªÑ‰∏âÂÖÉÔºåÂàò‰∫îÈÉé‚Äù‰ºöÂá∫Áé∞ÈóÆÈ¢òÔºåËÄå‚ÄúÁéã‰∏âÂº∫ÔºåÈóª‰∏ÄÂ§öÔºåÊùéÂõõÂÖâ‚ÄùÁ≠âÁªèÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÔºå‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÂõ†‰∏∫ÊúâÈóÆÈ¢òÁöÑÂ±û‰∫éÂ∏∏Áî®ËØçÊàñÂïÜÊ†áÂêçÁ≠âÂéüÂõ†ÂØºËá¥„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->



### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) {
        String content1 = ""Âº†‰∏â‰∏∞ÔºåÂàò‰∫îÈÉéÔºåÈªÑ‰∏âÂÖÉÔºåÂº†‰∏ÄÊ•†ÔºåÁéã‰∏âÂº∫Ôºå‰∏Å‰∏ÄÊ•†ÔºåÊùéÂõõÂÖâÔºåÈóª‰∏ÄÂ§öÔºåËµµ‰∏ÄÊ•†ÔºåÊùéÂõõ"";
        System.out.println(NLPTokenizer.segment(content1));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫ÔºöÂº†‰∏â‰∏∞ÔºåÂàò‰∫îÈÉéÔºåÈªÑ‰∏âÂÖÉÔºåÂº†‰∏ÄÊ•†ÔºåÁéã‰∏âÂº∫Ôºå‰∏Å‰∏ÄÊ•†ÔºåÊùéÂõõÂÖâÔºåÈóª‰∏ÄÂ§öÔºåËµµ‰∏ÄÊ•†ÔºåÊùéÂõõ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ÔºöÂº†/q, ‰∏â‰∏∞/nz, Ôºå/w, Âàò/nr, ‰∫îÈÉé/nz, Ôºå/w, ÈªÑ/a, ‰∏âÂÖÉ/nz, Ôºå/w, Âº†‰∏ÄÊ•†/nr, Ôºå/w, Áéã‰∏âÂº∫/nr, Ôºå/w, ‰∏Å‰∏ÄÊ•†/nr, Ôºå/w, ÊùéÂõõÂÖâ/nr, Ôºå/w, Èóª‰∏ÄÂ§ö/nr, Ôºå/w, Ëµµ‰∏ÄÊ•†/nr, Ôºå/w, ÊùéÂõõ/nr]

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÊµãËØïÁªìÊûú
![jg](https://user-images.githubusercontent.com/18030444/38225667-8e0e25c2-3728-11e8-98cd-4d2070edddd1.png)
"
‰∫∫Âêç‰∏≠ÊúâÊï∞ËØçÔºàÂ¶ÇÂº†‰∏âÔºâÂú®ÊñπÂºè‰∏Äportable‰∏éÊñπÂºè‰∫åËá™ÂÆö‰πâÊï∞ÊçÆÂåÖÁöÑÊÉÖÂÜµ‰∏ãÊãÜÂàÜÁªìÊûú‰∏çÂêå,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ √ó] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπ‰∫éÂè•Â≠ê‚ÄúÁÆ°ÁêÜÂëòÂº†‰∏â‚ÄùÁöÑÊãÜÂàÜ‰∏≠Ôºå‰ΩøÁî®ÊñπÂºè‰∏ÄportableÊï¥ÂêàÁâà‰∏éÊñπÂºè‰∫åËá™ÂÆö‰πâÊï∞ÊçÆÂåÖËøô‰∏§ÁßçÊñπÂºèËé∑ÂæóÁöÑÊãÜÂàÜÁªìÊûú‰∏çÂêå
ÊñπÂºè‰∏ÄÔºöÁÆ°ÁêÜÂëò/nr, Âº†‰∏â/nr
ÊñπÂºè‰∫åÔºöÁÆ°ÁêÜÂëò/nnt, Âº†/q, ‰∏â/m
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ÂàÜÂà´‰ΩøÁî®‰∏§ÁßçÊñπÂºèÊãÜÂàÜ‚ÄúÁÆ°ÁêÜÂëòÂº†‰∏â‚Äù

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
       String content1 = ""ÁÆ°ÁêÜÂëòÂº†‰∏â"";
        System.out.println(NLPTokenizer.segment(content1));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
```
ÊúüÊúõËæìÂá∫ÔºöÁÆ°ÁêÜÂëòÔºåÂº†‰∏â
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ÔºöÁÆ°ÁêÜÂëòÔºåÂº†Ôºå‰∏â
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÁªìÊûúÊà™Âõæ
ÊñπÂºè‰∏ÄÔºö
![qq 20180402162824](https://user-images.githubusercontent.com/18030444/38189476-f09255ba-3692-11e8-998a-21ccb11115ca.png)
ÊñπÂºè‰∫åÔºö
![qq 20180402162834](https://user-images.githubusercontent.com/18030444/38189477-f0d07fe8-3692-11e8-81e7-653578c53bc9.png)

"
ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂêéÂàÜËØç‰∏çÁêÜÊÉ≥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö master‰∏ªÂàÜÊîØ

ÊàëÊñ∞Âª∫‰∫ÜÂÆû‰ΩìËØÜÂà´Ê•ºÁõòÂêçÔºåViterbiSegment ÂàÜËØç ‚ÄúÂú∞ÂùÄÊòØÊòüÊµ∑Âüé‰∏âÊúü‚ÄùÔºå
ËØÜÂà´Âá∫Ê•ºÂêç 
[ÊòüÊµ∑Âüé/nbd] Âíå [ÊòüÊµ∑Âüé‰∏âÊúü/nbd]Ôºå‰ΩÜÂàÜËØçÁªìÊûúÂç¥ÊòØ
[Âú∞ÂùÄ/n, ÊòØ/vshi, ÊòüÊµ∑/n, Âüé/n, ‰∏âÊúü/nbdp]

ËØ∑ÈóÆÂ¶Ç‰Ωï‰ΩøÁªìÊûú‰∏∫
[Âú∞ÂùÄ/n, ÊòØ/vshi, ÊòüÊµ∑Âüé‰∏âÊúü/nbd]"
‰æùÂ≠òÂàÜÊûêËÆæÁΩÆ‰ΩøÁî®ÊÑüÁü•Êú∫ÂàÜËØç‰πãÂêéÔºåÊ†áÁÇπÁ¨¶Âè∑ÁöÑ‰æùÂ≠òÂÖ≥Á≥ªÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.1
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaven  ‰∏äÁöÑ portable 1.6.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

       ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂèØ‰ª•ËÆæÁΩÆÂàÜËØçÂô®ÔºåÂú®ËÆæÁΩÆ‰∏∫ÊÑüÁü•Êú∫ÂàÜËØçÂô®‰πãÂêéÔºåÊ†áÁÇπÁ¨¶Âè∑ÁöÑ‰æùÂ≠òÂÖ≥Á≥ª‰∏çÂÜçÊòØ‰πãÂâçÁöÑ **Ê†áÁÇπÁ¨¶Âè∑** ÂÖ≥Á≥ªÔºåËÄåÊòØ **Âä®ÂÆæÂÖ≥Á≥ª** ‰πãÁ±ª„ÄÇ


<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

```java
        String test = ""‰ªÄ‰πàÊòØËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ?"";
        IDependencyParser parser = new NeuralNetworkDependencyParser();
        PerceptronLexicalAnalyzer segmenter = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        parser.setSegment(segmenter);
        CoNLLSentence sentence = parser.parse(test);
        System.out.println(sentence);
        // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
        CoNLLWord[] wordArray = sentence.getWordArray();
        System.out.println(""-------0000----"");
```

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
1	‰ªÄ‰πà	‰ªÄ‰πà	r	ry	_	2	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	ÊòØ	ÊòØ	v	vshi	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ	Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ	nz	nz	_	2	Âä®ÂÆæÂÖ≥Á≥ª	_	_
4	?	?	wp	w	_	2	Ê†áÁÇπÁ¨¶Âè∑	_	_

‰ªÄ‰πà --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊòØ
ÊòØ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##
Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊòØ
? --(Ê†áÁÇπÁ¨¶Âè∑)--> ÊòØ
-------0000----

```


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
1	‰ªÄ‰πà	‰ªÄ‰πà	r	r	_	2	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	ÊòØ	ÊòØ	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
3	Ëá™ÁÑ∂ËØ≠Ë®Ä	Ëá™ÁÑ∂ËØ≠Ë®Ä	n	n	_	4	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
4	Â§ÑÁêÜ	Â§ÑÁêÜ	v	vn	_	2	Âä®ÂÆæÂÖ≥Á≥ª	_	_
5	?	?	n	n	_	4	Âä®ÂÆæÂÖ≥Á≥ª	_	_

‰ªÄ‰πà --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÊòØ
ÊòØ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##
Ëá™ÁÑ∂ËØ≠Ë®Ä --(‰∏ªË∞ìÂÖ≥Á≥ª)--> Â§ÑÁêÜ
Â§ÑÁêÜ --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊòØ
? --(Âä®ÂÆæÂÖ≥Á≥ª)--> Â§ÑÁêÜ
-------0000----
```
## ÂÖ∂‰ªñ‰ø°ÊÅØ

‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁöÑÈªòËÆ§ÂàÜËØçÂô®ÂØπÊ†áÁÇπÁ¨¶Âè∑ÁöÑ‰æùÂ≠òÂàÜÊûêÊòØÊ≠£Á°ÆÁöÑ„ÄÇ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éÊÑüÁü•Êú∫ÂàÜËØçoffset‰∏∫0ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.6.1

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1.6.0ÂêéHanLPÊé®Âá∫‰∫ÜÊÑüÁü•Êú∫ÂàÜËØçÔºåÈùûÂ∏∏ÊÑüË∞¢‰ΩúËÄÖÁöÑËæõËã¶‰ªòÂá∫„ÄÇÊàëÁöÑÈóÆÈ¢òÔºö
1„ÄÅÊÑüÁü•Êú∫ÂàÜËØçÊ≤°ÊúâÂØπTermÂÅöÂÅèÁßªÊ†áËÆ∞ÔºüTerm‰∏≠ÁöÑoffsetÁé∞Âú®ÈÉΩÊòØ0ÔºåÊòØÂÆûÁé∞Ëµ∑Êù•ÂæàÂõ∞ÈöæÂêóÔºü
2„ÄÅÂ¶ÇÊûúÊÑüÁü•Êú∫ÂàÜËØçËÉΩÂÆûÁé∞offsetÔºåÊòØÂê¶ÊúâËÆ°ÂàíÂú®‰∏ã‰∏ÄÁâàÂ¢ûÂä†Ëøô‰∏ÄÂäüËÉΩÔºåÂ§ßËá¥ÊòØ‰ªÄ‰πàÊó∂Èó¥Ôºü
3„ÄÅÊúâÊó†ÂèØËÉΩÈÄöËøáHanLP.newSegment(int segType)Êèê‰æõ‰∏Ä‰∏™Â∑•ÂéÇÊñπÊ≥ïÔºåÊù•ÂàõÂª∫ÊÑüÁü•Êú∫ÂàÜËØçÔºü

### Ëß¶Âèë‰ª£Á†Å

```
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âçï‰∏™ËØçÁöÑËØçÊÄßËØÜÂà´ÔºåÊòØÂê¶ÂèØÊ†áÊ≥®Â§ö‰∏™ËØçÊÄß,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂÖ≥‰∫éËØçÊÄßÊ†áÊ≥®ÔºåÊòØÂê¶‰∏Ä‰∏™ËØçÂèØÊ†áÊ≥®Â§ö‰∏™ËØçÊÄßÔºå‰æãÂ¶ÇÔºöÊ±™Ê¥ãÔºåÂèØ‰ª•ÊòØ‰∫∫Âêç‰πüÂèØ‰ª•ÊòØÊ±™Ê¥ã\n Â§ßÊµ∑\n

"
ÊàëËá™Â∑±‰πüÂú®ÂÅöËØçÂÖ∏ÁöÑÂëΩÂêçÂÆû‰ΩìÔºåÊÉ≥Áü•ÈÅìÂì™ÈáåÊúâÂëΩÂêçÂÆû‰ΩìÁöÑÊ†áÊ≥®ËßÑÂàôÊñáÊ°£ÔºåËøòÊòØËØ¥Ëøô‰∫õÊòØÊ†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÊù•ÂÆö,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫é‚ÄúÊú¨Âë®X‚ÄùÂàÜËØçÈîôËØØ,"## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
1.6.0

### Ëß¶Âèë‰ª£Á†Å

```
        System.out.println(HanLP.segment(""Êú¨Âë®‰∏ÄÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®‰∫åÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®‰∏âÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®ÂõõÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®‰∫îÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®ÂÖ≠ÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
        System.out.println(HanLP.segment(""Êú¨Âë®Êó•ÊàëÂéª‰∏äÁè≠ÁöÑÊó∂ÂÄô""));
```

### ÊúüÊúõËæìÂá∫
```
Êú¨/Âë®‰∏Ä/Êàë/Âéª
```

### ÂÆûÈôÖËæìÂá∫
```
[Êú¨/r, Âë®‰∏ÄÊàë/nr, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®‰∫å/t, Êàë/r, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®‰∏âÊàë/nr, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®ÂõõÊàë/nr, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®‰∫î/t, Êàë/r, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®ÂÖ≠/t, Êàë/r, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
[Êú¨/r, Âë®Êó•Êàë/nr, Âéª/v, ‰∏äÁè≠/v, ÁöÑ/uj, Êó∂ÂÄô/n]
```

"
ÂÖ≥‰∫éhanlpÂàÜËØç‰∏≠/++/.!=\\Á≠âÁ¨¶Âè∑‰ª•ÂèäÁ¨¶Âè∑ÁªÑ~@#$%^&ÁöÑÁ¨¶Âè∑ÂêàÊàêÊñ∞ËØçÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2 / 1.6.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âà©Áî®hanlpÂàÜËØçÁöÑÊó∂ÂÄôÔºåËã±ÊñáÂçäËßíÁ¨¶Âè∑ÁªÑ/,?,!,+,|,\,=,. „ÄÅÁ¨¶Âè∑ÁªÑ~,@,#,$,%,^&ÔºåÈÉΩ‰ºöËá™Âä®ÊãºÊé•Âú®‰∏ÄËµ∑ÔºåÁªÑÊàêÊñ∞ËØç


### Ëß¶Âèë‰ª£Á†Å

startJVM(getDefaultJVMPath(),""-Djava.class.path=C:/Users/reading-magic/Desktop/Work/Cut/hanlp-1.6.0.jar;\
C:/Users/reading-magic/Desktop/Work/Cut"",""-Xms1g"", ""-Xmx1g"")
HanLP = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
hanlp_words=HanLP.segment(""c#/.net,|?.*/\\=!,~@#$%^&"")
for i in hanlp_words:
    print(i)

### ÊúüÊúõËæìÂá∫
ÊúüÊúõËæìÂá∫ÁöÑÁ¨¶Âè∑ËÉΩÂ§üÂàÜÂºÄ
### ÊúüÊúõËæìÂá∫

c#    /nx
/    /w
.    /w
net    /w
,    /w
|    /w
.    /w
‚Äù*‚Äú   /w
?   /w
/    /w
\    /w
\    /w
=   /w
!    /w
,    /w
~     /nx
@   /nx
‚Äú#‚Äù   /nx
$   /nx
%   /nx
^   /nx
&   /nx

### ÂÆûÈôÖËæìÂá∫

ÂÆûÈôÖ‰∏äÊääÂØπÂ∫îËØçÊÄß‰∏∫wÂíånxÁöÑËØçÂàÜÂà´ÂêàÊàêÂú®‰∏ÄËµ∑Ôºå
```
ÂÆûÈôÖËæìÂá∫
```
c#    /nx,
 /.    /w,
 net   /nx,
 ,|?.*/\=!,    /w
  ~@#$%^&     /nx
## ÂÖ∂‰ªñ‰ø°ÊÅØ
‚Äù*‚ÄúË∑ü‚Äù#‚Äú‰ºö‰∏éhtmlÊàñËÄÖcssÂÜ≤Á™Å
Âà©Áî®‰∫ÜpythonË∞ÉÁî®‰∫Üjava Á®ãÂ∫èÔºåÂêåÊó∂Âú®ËØçÂÖ∏ÈáåÊâæ‰∏çÂà∞Áõ∏Â∫îÁöÑ‰ø°ÊÅØ
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
![image](https://user-images.githubusercontent.com/22814402/37819529-baf80648-2eb8-11e8-8e33-3519e5ac91f9.png)

![image](https://user-images.githubusercontent.com/22814402/37819522-b5b7bf20-2eb8-11e8-9969-4b8adf8b1043.png)
"
"ÂàÜËØçÈîôËØØÔºö""‰∏∫‰ªÄ‰πàÊàëÊâîÂá∫ÁöÑÁì∂Â≠êÊ≤°Êúâ‰∫∫ÂõûÂ§çÔºü""","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.6.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπÂè•Â≠ê‚Äú‰∏∫‰ªÄ‰πàÊàëÊâîÂá∫ÁöÑÁì∂Â≠êÊ≤°Êúâ‰∫∫ÂõûÂ§çÔºü‚ÄùÂàÜËØçÈîôËØØ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        List<Term> terms = HanLP.segment(""‰∏∫‰ªÄ‰πàÊàëÊâîÂá∫ÁöÑÁì∂Â≠êÊ≤°Êúâ‰∫∫ÂõûÂ§çÔºü"");
        System.out.println(terms );
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[‰∏∫‰ªÄ‰πà, Êàë, ÊâîÂá∫, ÁöÑ, Áì∂Â≠ê, Ê≤°Êúâ,‰∫∫, ÂõûÂ§ç, Ôºü]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[‰∏∫‰ªÄ‰πà, Êàë, ÊâîÂá∫, ÁöÑ, Áì∂Â≠ê, Ê≤°,Êúâ‰∫∫, ÂõûÂ§ç, Ôºü]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊñáÊú¨ÂàÜÁ±ªÔºöË¥ùÂè∂ÊñØÁªüËÆ°‰∏≠ÁöÑÁâπÂæÅÈÄâÊã©ÂèØÂê¶ÂèØËßÜÂåñÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.6.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Ë¥ùÂè∂ÊñØÁªüËÆ°‰∏≠‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅÔºåÊéßÂà∂Âè∞‰ºöËæìÂá∫Á±ª‰ºº‚Äú‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅ‰∏≠...ËÄóÊó∂ 80 ms,ÈÄâ‰∏≠ÁâπÂæÅÊï∞:5857 / 15110 = 38.76%‚ÄùÁöÑËØ≠Âè•Ôºå‰ΩÜÊòØÊàë‰∏çËÉΩÂ§üÁúãÂà∞ÂÆÉÂà∞Â∫ïÈÄâÊã©‰∫ÜÂì™‰∫õÁâπÂæÅËØçÊ±á„ÄÇ
‰∏æ‰∏™‰æãÂ≠êÔºåÂÅáÂ¶ÇÊàëÂà©Áî®ÊÉÖÊÑüÂàÜÊûêÂØπ‰∏Ä‰∫õËØÑËÆ∫ËøõË°å‰∫ÜÂàÜÁ±ªÔºåÊúâÊ≠£Èù¢ÂíåË¥üÈù¢Ôºå‰ΩÜÂçïÂçïËøôÊ†∑ÂàÜÁ±ªÂØπÊàëÊ≤°ÊúâÂÆûÈôÖ‰ΩúÁî®ÔºåÊàëÊÉ≥Áü•ÈÅìËøô‰∫õÊ≠£Èù¢„ÄÅË¥üÈù¢ËØÑËÆ∫ÊúâÂì™‰∫õÁâπÁÇπÔºåËøô‰∫õË¥üÈù¢ËØÑËÆ∫ÂèàÊòØÂú®‰∏çÊª°‰∫õ‰ªÄ‰πà‰∏úË•øÔºåÊØîÂ¶ÇËØÑËÆ∫‚ÄúÊó©È§êÈÉΩÂáâ‰∫ÜÔºåÊàëÂêÉ‰∫ÜËÇöÂ≠êÁóõ„ÄÇ‚ÄùÊàëÂ∞±Áü•ÈÅìËøô‰∏™ËØÑËÆ∫Âú®‰∏çÊª°Êó©È§êÔºåËøõËÄåÂØπÊó©È§êÂÅöÊîπËøõ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Ê≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÔºå‰ΩÜÊàëÂè™ËÉΩÁúãÂà∞ÊÉÖÊÑüÂàÜÊûêÂíåË¥ùÂè∂ÊñØÁªüËÆ°ÁöÑÁªìÊûúÔºå‰∏çÁü•ÈÅìÊàëÂèØ‰ª•ÁúãÂà∞ÂÖ∂‰∏≠ÁöÑËøáÁ®ãÂêóÔºü
### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

package com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier
....
....
protected BaseFeatureData selectFeatures(IDataSet dataSet)
    {
        ChiSquareFeatureExtractor chiSquareFeatureExtractor = new ChiSquareFeatureExtractor();

        logger.start(""‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅ‰∏≠..."");
        //FeatureStatsÂØπË±°ÂåÖÂê´ÊñáÊ°£‰∏≠ÊâÄÊúâÁâπÂæÅÂèäÂÖ∂ÁªüËÆ°‰ø°ÊÅØ
        BaseFeatureData featureData = chiSquareFeatureExtractor.extractBasicFeatureData(dataSet); //ÊâßË°åÁªüËÆ°

        //Êàë‰ª¨‰º†ÂÖ•Ëøô‰∫õÁªüËÆ°‰ø°ÊÅØÂà∞ÁâπÂæÅÈÄâÊã©ÁÆóÊ≥ï‰∏≠ÔºåÂæóÂà∞ÁâπÂæÅ‰∏éÂÖ∂ÂàÜÂÄº
        Map<Integer, Double> selectedFeatures = chiSquareFeatureExtractor.chi_square(featureData);

        //‰ªéÁªüËÆ°Êï∞ÊçÆ‰∏≠Âà†ÊéâÊó†Áî®ÁöÑÁâπÂæÅÂπ∂ÈáçÂª∫ÁâπÂæÅÊò†Â∞ÑË°®
        int[][] featureCategoryJointCount = new int[selectedFeatures.size()][];
        featureData.wordIdTrie = new BinTrie<Integer>();
        String[] wordIdArray = dataSet.getLexicon().getWordIdArray();
        int p = -1;
        for (Integer feature : selectedFeatures.keySet())
        {
            featureCategoryJointCount[++p] = featureData.featureCategoryJointCount[feature];
            featureData.wordIdTrie.put(wordIdArray[feature], p);
        }
        logger.finish("",ÈÄâ‰∏≠ÁâπÂæÅÊï∞:%d / %d = %.2f%%\n"", featureCategoryJointCount.length,
                      featureData.featureCategoryJointCount.length,
                      featureCategoryJointCount.length / (double)featureData.featureCategoryJointCount.length * 100.);
        featureData.featureCategoryJointCount = featureCategoryJointCount;

        return featureData;
    }
....
....


### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

Â∏åÊúõÁúãÂà∞Ê≠£Èù¢ËØÑËÆ∫‰πãÊâÄ‰ª•ÊòØÊ≠£Èù¢ËØÑËÆ∫ÁöÑ‰æùÊçÆÔºåÊØîÂ¶ÇÁâπÂæÅ‰∏éÂÖ∂ÂàÜÂÄºÔºü
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

Ê≠£Âú®‰ªé data/test/ChnSentiCorpÊÉÖÊÑüÂàÜÊûêÈÖíÂ∫óËØÑËÆ∫ ‰∏≠Âä†ËΩΩÂàÜÁ±ªËØ≠Êñô...
Ê≠£Èù¢ : 2000 ‰∏™ÊñáÊ°£
Ë¥üÈù¢ : 2000 ‰∏™ÊñáÊ°£
ÂºÄÂßãËÆ≠ÁªÉ...
Ê≠£Âú®ÊûÑÈÄ†ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ...[Ê≠£Èù¢]...50.00%...[Ë¥üÈù¢]...100.00%...ËÄóÊó∂ 6448 ms Âä†ËΩΩÂÆåÊØï
ÂéüÂßãÊï∞ÊçÆÈõÜÂ§ßÂ∞è:4000
‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅ‰∏≠...ËÄóÊó∂ 80 ms,ÈÄâ‰∏≠ÁâπÂæÅÊï∞:5857 / 15110 = 38.76%
Ë¥ùÂè∂ÊñØÁªüËÆ°ÁªìÊùü
ËÆ≠ÁªÉËÄóÊó∂Ôºö6689 ms

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‚ÄùÊâéÂøÉ‚ÄúÂ§öÈü≥Â≠óÊãºÈü≥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‚ÄúÊâéÂøÉ‚ÄùÂæóÂà∞ÁöÑÊãºÈü≥ÊòØ‚Äùza xin‚Äú
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å
```
System.out.println (HanLP.convertToPinyinString(""ÊâéÂøÉ"", "" "", true));

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
zha xin
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
za xin
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑ÈóÆÊñ∞ËØçÂèëÁé∞ÁöÑ‰ª£Á†ÅÊòØÂú®Âì™‰∏™Âú∞ÊñπÂë¢Ôºü,ÁúãÂà∞‰ªãÁªçÈáåÊúâÊñ∞ËØçÂèëÁé∞ÂäüËÉΩÔºå‰ΩÜÊòØÂ¶Ç‰ΩïË∞ÉÁî®Âë¢ÔºüÊòØÁõ¥Êé•ÈõÜÊàêÂú®‰∫ÜÂàÜËØçÈáåÈù¢ÂêóÔºüÂèØ‰ª•ÂçïÁã¨ËæìÂá∫ÂêóÔºü
python‰∏≠‰ΩøÁî®hanlpÔºåÈáëÂçéÁöÑËØçÊÄßÔºåËØÜÂà´Âá∫Êù•‰∏∫nrÔºåËÄå‰∏çÊòØns,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.5.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.5.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊàëÂú®python‰∏≠‰ΩøÁî®hanlpÊù•ËØÜÂà´‰∏ÄÊÆµÊñáÂ≠ó‰∏≠Âá∫Áé∞ÁöÑÂú∞Âå∫ÔºåËÄå‰∏îÂ§ßÈÉ®ÂàÜÂú∞Âå∫ÂèØ‰ª•Ê≠£Â∏∏ËØÜÂà´Âá∫Êù•Ôºå‰ΩÜÊòØÈáëÂçéËØÜÂà´Âá∫Êù•ÁöÑËØçÊÄßÊÄªÊòØnr

### Ëß¶Âèë‰ª£Á†Å

```
hanlp = JClass('com.hankcs.hanlp.HanLP')
segment = hanlp.newSegment().enablePlaceRecognize(True)
term_list = segment.seg(u'ÊàëÂú®ÊµôÊ±üÈáëÂçéÂá∫Áîü')

for name in term_list:
    print(name.toString())
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Êàë/r
Âú®/p
ÊµôÊ±ü/ns
ÈáëÂçé/ns
Âá∫Áîü/v
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
ËØÜÂà´ÁöÑÈáëÂçéËØçÊÄß‰∏∫‰∫∫ÂêçÔºànrÔºâËÄå‰∏çÊòØÂú∞ÂêçÔºànsÔºâ
```
Êàë/r
Âú®/p
ÊµôÊ±ü/ns
ÈáëÂçé/nr  
Âá∫Áîü/v
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
‰∏çÁü•ÈÅìÊòØÂê¶ÊòØÂõ†‰∏∫ HanLP/data/dictionary/CoreNatureDictionary.mini.txt
Êñá‰ª∂‰∏≠ÔºåÊ†áËØÜ‰∫ÜÈáëÂçéÁöÑËØçÊÄß‰∏∫nrÂºïËµ∑ÁöÑ
![cosmos](https://user-images.githubusercontent.com/11661935/37083415-5ac2ccbe-21e7-11e8-8020-e6f022640af8.png)


"
Á¥¢ÂºïÊ®°Âºè‰∏ãÂÖ®ÂàáÂàÜ‰∏çÂáÜÁ°Æ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

IndexTokenizerÁ¥¢ÂºïÊ®°ÂºèÂÖ®ÂàáÂàÜÁöÑÈóÆÈ¢òÔºå
Â¶Ç‚ÄúÊàëÁà±‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ‚ÄùÔºå
**ÂÆûÈôÖËæìÂá∫**
‚ÄúÊàë‚ÄùÔºå‚ÄúÁà±‚ÄùÔºå‚Äú‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ‚ÄùÔºå‚Äù‰∏≠Âçé‰∫∫Ê∞ë ‚ÄùÔºå‚Äù‰∏≠Âçé‚ÄùÔºå‚ÄùÂçé‰∫∫‚Äù Ôºå‚Äù‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ ‚ÄùÔºå‚Äù‰∫∫Ê∞ë‚ÄùÔºå‚ÄùÂÖ±ÂíåÂõΩ ‚ÄùÔºå‚ÄùÂÖ±Âíå‚Äù „ÄÇ
‚Äù‰∫∫Ê∞ëÊîøÂ∫ú‚Äù,‚Äù‰∫∫Ê∞ë‚Äù,‚ÄùÊîøÂ∫ú‚Äù,‚ÄùÊ∞ëÊîø‚Äù
**ÊúüÊúõËæìÂá∫**
‚ÄúÊàë‚ÄùÔºå‚ÄúÁà±‚ÄùÔºå‚Äú‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ‚ÄùÔºå‚Äù‰∏≠Âçé‰∫∫Ê∞ë ‚ÄùÔºå‚Äù‰∏≠Âçé‚ÄùÔºå‚Äù‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ ‚ÄùÔºå‚Äù‰∫∫Ê∞ë‚ÄùÔºå‚ÄùÂÖ±ÂíåÂõΩ ‚ÄùÔºå‚ÄùÂÖ±Âíå‚Äù „ÄÇ
‚Äù‰∫∫Ê∞ëÊîøÂ∫ú‚Äù,‚Äù‰∫∫Ê∞ë‚Äù,‚ÄùÊîøÂ∫ú‚Äù

viterbiÂèñÂæóÊúÄ‰ºòËß£‚Äú‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ‚ÄùÂêéÔºåÁÑ∂Âêé**ÂÆåÂÖ®Ê†πÊçÆËØçÂ∫ìËøõË°åÁªÜÂàá**ÁöÑÔºåÊïÖÂàÜÂá∫Êù•‚ÄúÂçé‰∫∫‚ÄùÔºå‚ÄùÊ∞ëÊîø‚ÄùÔºåÁ≠âÊòéÊòæ‰∏∫bad case„ÄÇ
ÂÖ≥‰∫éfixËøô‰∏™ÈóÆÈ¢òÔºåÊÇ®Êúâ‰ªÄ‰πàÂª∫ËÆÆÂêóÔºü



"
ËøêË°åDemoTextClassificationFMeasure. javaÔºåÁúã‰∏çÊáÇËæìÂá∫„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
masterÂàÜÊîØ

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.5.4.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.5.3.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

„ÄêÈóÆÈ¢ò„Äë
ËøêË°åÂºÄÊ∫êÈ°πÁõÆ‰∏≠ÁöÑDemoTextClassificationFMeasure. javaÔºàÊºîÁ§∫‰∫ÜÂàÜÂâ≤ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ,ËøõË°åÊõ¥‰∏•Ë∞®ÁöÑÊµãËØïÔºâ
ÂèØÊòØÊàëÁúã‰∏çÊáÇËæìÂá∫Ôºå‰∏çÁü•ÈÅìËøô‰∏™ÊµãËØïÂíåËøô‰∫õÊï∞ÊçÆÊÑèÂë≥ÁùÄ‰ªÄ‰πàÔºåÂèØ‰ª•Ëß£Èáä‰∏Ä‰∏ãÂêóÔºü

„ÄêÂÆûÈôÖËæìÂá∫„Äë
Ê®°Âºè:ËÆ≠ÁªÉÈõÜ
ÊñáÊú¨ÁºñÁ†Å:UTF-8
Ê†πÁõÆÂΩï:data/test/ChnSentiCorpÊÉÖÊÑüÂàÜÊûêÈÖíÂ∫óËØÑËÆ∫
Âä†ËΩΩ‰∏≠...
[Ê≠£Èù¢]...100.00% 1800 ÁØáÊñáÊ°£
[Ë¥üÈù¢]...100.00% 1800 ÁØáÊñáÊ°£
ËÄóÊó∂ 21797 ms Âä†ËΩΩ‰∫Ü 2 ‰∏™Á±ªÁõÆ,ÂÖ± 3600 ÁØáÊñáÊ°£
ÂéüÂßãÊï∞ÊçÆÈõÜÂ§ßÂ∞è:3600
‰ΩøÁî®Âç°ÊñπÊ£ÄÊµãÈÄâÊã©ÁâπÂæÅ‰∏≠...ËÄóÊó∂ 2504 ms,ÈÄâ‰∏≠ÁâπÂæÅÊï∞:5486 / 14103 = 38.90%
Ë¥ùÂè∂ÊñØÁªüËÆ°ÁªìÊùü
Ê®°Âºè:ÊµãËØïÈõÜ
ÊñáÊú¨ÁºñÁ†Å:UTF-8
Ê†πÁõÆÂΩï:data/test/ChnSentiCorpÊÉÖÊÑüÂàÜÊûêÈÖíÂ∫óËØÑËÆ∫
Âä†ËΩΩ‰∏≠...
[Ê≠£Èù¢]...100.00% 200 ÁØáÊñáÊ°£
[Ë¥üÈù¢]...100.00% 200 ÁØáÊñáÊ°£
ËÄóÊó∂ 1541 ms Âä†ËΩΩ‰∫Ü 2 ‰∏™Á±ªÁõÆ,ÂÖ± 400 ÁØáÊñáÊ°£
     P	          R	           F1	           A	      
 82.63	 88.00	 85.23	 84.75	Ê≠£Èù¢
 87.17	 81.50	 84.24	 84.75	Ë¥üÈù¢
 84.90	 84.75	 84.82	 84.75	avg.
data size = 400, speed = 44444.44 doc/s

ËØ∑ÈóÆ‰∏ãÈù¢ËøôÈÉ®ÂàÜÊÄé‰πàÁêÜËß£ÔºåÊúâ‰ªÄ‰πàÊÑè‰πâÔºü
     P------R-----F1----A	      
 82.63	 88.00	 85.23	 84.75	Ê≠£Èù¢
 87.17	 81.50	 84.24	 84.75	Ë¥üÈù¢
 84.90	 84.75	 84.82	 84.75	avg.
data size = 400, speed = 44444.44 doc/s
"
‰Ω†Â•ΩÔºåËØ∑ÈóÆHanLPÂ¶Ç‰ΩïÂàÜËØçÂπ∂‰∏îÁªüËÆ°ËØçÈ¢ë,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂèØÂê¶Êèê‰æõ‰∏Ä‰∏™ÂëΩ‰ª§Ë°åÂèØÊâßË°åÁöÑjarÂåÖÔºü,Á±ª‰ºº‰∫é[ÊñØÂù¶Á¶èNLP](https://stanfordnlp.github.io/CoreNLP/cmdline.html)ÁöÑ:-)
ËØ∑ÈóÆÊúâÊ≤°ÊúâÂäûÊ≥ïÂÅöÂà∞ËØ≠‰πâ‰∏äÁöÑÁõ∏‰ººÂ∫¶ÂåπÈÖç,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [*] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

## ÊàëÁöÑÈóÆÈ¢ò
Âú®ÂÅöÁü≠ÊñáÊú¨Áõ∏‰ººÂ∫¶ÂåπÈÖçÊó∂ÔºåÊàëÂ∏åÊúõ‰ºòÂÖàÂåπÈÖçËØ≠‰πâÁõ∏ËøëÁöÑ„ÄÇÂ¶ÇÔºö
**‰∏çÂñúÊ¨¢** ÂåπÈÖçÂà∞ **ËÆ®Âéå**ÔºåËÄå‰∏çÊòØÂåπÈÖçÂà∞ **ÂñúÊ¨¢**„ÄÇ
ËØ∑ÈóÆÊúâÊ≤°ÊúâÂäûÊ≥ïÂÆûÁé∞Ôºü

## Â§çÁé∞ÈóÆÈ¢ò

### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

```
    DocVectorModel docVectorModel = new DocVectorModel(new WordVectorModel(modelFileName));

		String[] documents = new String[]{
		        ""ÂÜúÊ∞ëÂú®Ê±üËãèÁßçÊ∞¥Á®ª"",
		        ""Â±±‰∏úËãπÊûú‰∏∞Êî∂"",
		        ""ÊàëÂæàÂñúÊ¨¢ÁØÆÁêÉ"",
		        ""ÊàëÂæàËÆ®ÂéåÁØÆÁêÉ"",
		        ""Â••Ëøê‰ºöÂ•≥ÊéíÂ§∫ÂÜ†"",
		        ""‰∏ñÁïåÈî¶Ê†áËµõËÉúÂá∫""
		};

		for (int i = 0; i < documents.length; i++)
		{
		    docVectorModel.addDocument(i, documents[i]);
		}
	    System.out.println(docVectorModel.nearest(""Êàë‰∏çÂñúÊ¨¢ÁØÆÁêÉ""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
‰∏çÂñúÊ¨¢ËÉΩÂÖàÂåπÈÖçÂà∞ËÆ®Âéå
```
[3=0.99999976, 2=0.87721485
```

### ÂÆûÈôÖËæìÂá∫
‰∏çÂñúÊ¨¢ÂÖàÂåπÈÖçÂà∞‰∫Ü ÂñúÊ¨¢

```
[2=0.99999976, 3=0.87721485
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â∏∏ËßÅËØçÈáçÂè†ÔºåË¢´ËØØËØÜÂà´‰∏∫Êú∫ÊûÑÁöÑ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(false).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
        System.out.println(StandardTokenizer.segment(""ËæñÂå∫Êúâ‰∏≠Â≠¶ÂíåÂ∞èÂ≠¶Â∞èÂ≠¶ÂêÑ1ÊâÄ„ÄÇ""));
    }
```
### ÊúüÊúõËæìÂá∫

[ËæñÂå∫/n, Êúâ/vyou, ‰∏≠Â≠¶/nt, Âíå/cc, Â∞èÂ≠¶/nt, Â∞èÂ≠¶/nt, ÂêÑ/rz, 1ÊâÄ/nt, „ÄÇ/w]

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ËæñÂå∫/n, Êúâ‰∏≠Â≠¶/nt, Âíå/cc, Â∞èÂ≠¶Â∞èÂ≠¶/nt, ÂêÑ/rz, 1ÊâÄ/nt, „ÄÇ/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Êú∫ÊûÑËØÜÂà´Âá∫Â∑ÆÁöÑÈóÆÈ¢òÔºåÁ±ª‰ººÁöÑÂæàÂ§öÔºåÊØîÂ¶ÇÂè•Â≠ê‚ÄúÊàò‰∫â‰∏ã‰ºöÊó†ÁºòÊó†ÊïÖÊâìËµ∑Êù•‚ÄùÔºå‚Äú‰∏ç‚ÄùÂÜôÈîô‰∏∫‚Äú‰∏ã‚ÄùÔºåÊää‚ÄúÊàò‰∫â‰∏ã‰ºö‚ÄùÊãº‰∏∫nt‰∫Ü

"
Êñ∞ËØçÂèëÁé∞,ÊÉ≥ËØ¢ÈóÆ‰∏Ä‰∏ãÁõÆÂâçHanLPÊúâÊ≤°ÊúâÊèê‰æõÊñ∞ËØçÂèëÁé∞ÁöÑÂäüËÉΩ
ËØçÁΩë‰∏éËØçÂõæÁöÑÂå∫Âà´,"ËØ∑Êïô‰∏Ä‰∏ãËØçÁΩëÂíåËØçÂõæËøô‰∏§ÁßçÊï∞ÊçÆÁªìÊûÑÂàÜÂà´ÈÄÇÁî®‰∫éÊÄéÊ†∑ÁöÑÁÆóÊ≥ï„ÄÇ
Âú®ViterbiSegment‰∏≠‰ΩøÁî®‰∫ÜËØçÁΩëÁªìÊûÑÔºåÊØè‰∏ÄË°åÈÉΩÊòØÂâçÁºÄËØçÈìæ„ÄÇÊàëËßâÂæó‰ΩøÁî®ÂÉèDijkstraSegment‰∏≠‰∏ÄÊ†∑ÁöÑËØçÂõæÔºåÁÑ∂Âêé‰æùÊ¨°ÂØπËäÇÁÇπÈÄâÊã©ÊúÄ‰ºòËß£Â∫îËØ•‰πüÂèØ‰ª•ÂÆûÁé∞ViterbiÊúÄÁü≠Ë∑ØÂàáÂàÜ„ÄÇ
ÈÇ£‰πàÔºåÂè¶Â§ñÂèàÂÆö‰πâÁöÑËØçÁΩëÁªìÊûÑÁöÑÊÑè‰πâÊòØ‰ªÄ‰πàÔºåÂÆÉÂú®ÊïàÁéá‰∏äÊòØÊõ¥‰ºòÂêóÔºü"
ËØ∑ÈóÆÊÄé‰πàËá™ÂÆö‰πâÁÆÄÁπÅËΩ¨Êç¢ÁöÑËØçÂÖ∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰Ω†Â•ΩÔºåËØ∑Êïô‰∏™ÈóÆÈ¢òÔºåÂõ†‰∏∫Êàë‰ª¨ÂÖ¨Âè∏ÁöÑÂïÜÊ†áÂè∞ÈôÜÈÄöÔºåÂè∞‰∏çËΩ¨ÁπÅ‰ΩìÔºåÂè™ÊúâÈôÜËΩ¨ÁπÅ‰ΩìÔºåÊâÄ‰ª•ÊÉ≥Áü•ÈÅìÊòØÂê¶ËÉΩÈÄöËøáËá™ÂÆö‰πâÁöÑÊñπÂºèËß£ÂÜ≥ËøôÁßçÁâπÂÆöÊÉÖÂÜµ‰∏ãÁöÑÁÆÄÁπÅËΩ¨Êç¢ÈóÆÈ¢òÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ãÂè∞‰πü‰ºöË¢´ËΩ¨ÊàêÁπÅ‰ΩìÔºå‰ΩÜËøô‰∏çÊòØÊàë‰ª¨Ë¶ÅÁöÑÊïàÊûúÔºåËØ•Â¶Ç‰ΩïËß£ÂÜ≥
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰Ω†Â•ΩÔºåËØ∑ÈóÆËøôÂèØ‰ª•ÂÅö‰∏âÂÖÉÁªÑÊäΩÂèñÂêó?,"‰Ω†Â•ΩÔºåËØ∑ÈóÆËøôÂèØ‰ª•ÂÅö‰∏âÂÖÉÁªÑÊäΩÂèñÂêó?
 ÊØîÂ¶ÇÔºö
Âº†‰∏âÂú®ÊπñÂçóÈïøÂ§ßÔºåÂá∫Áîü‰∫é1999Âπ¥1Êúà‰∏ÄÊó•„ÄÇ
ÊàëÊÉ≥ÊèêÂá∫ÔºöÂº†‰∏âÂá∫Áîü‰∫é1999Âπ¥1Êúà‰∏ÄÊó•

Ëøô‰∏™ËÉΩÊäΩÂèñÂêóÔºüÊàñËÄÖ‰ΩúËÄÖÂèØÂê¶ÊåáÁÇπ‰∏Ä‰∏ãÔºåÁªô‰∏Ä‰∏ãÊÄùË∑ØÔºåË∞¢Ë∞¢„ÄÇ"
ÊúÄÊñ∞ÁâàÊú¨Ôºà1.5.3ÔºâÈóÆÈ¢ò:ÂêåÊ†∑Ë∞ÉÁî®HanLP.segment()ÊñπÂºè‰∏ÄÂíåÊñπÂºè‰∫åÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊúÄÊñ∞ÁâàÊú¨Ôºà1.5.3ÔºâÔºåÊåâÁÖßËØ¥ÊòéÂàÜÂà´ÈÖçÁΩÆÔºåÂàÜËØçÁªìÊûú‰∏ç‰∏ÄÊ†∑
System.out.println(HanLP.segment(""Êää‰∏§Âº†ËΩ¶Á•®Êç¢Êàê‰∏âÂº†""));
ÊñπÂºè‰∏Ä(maven)Ôºö[Êää/p, ‰∏§/m, Âº†/q, ËΩ¶Á•®/n, Êç¢/v, Êàê‰∏âÂº†/nr]
ÊñπÂºè‰∫å(Ëá™ÂÆö‰πâ)Ôºö[Êää/pba, ‰∏§/m, Âº†/q, ËΩ¶Á•®/n, Êç¢Êàê/v, ‰∏â/m, Âº†/q]

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§
1.ÊñπÂºè‰∏ÄÔºöÁõ¥Êé•Áî®maven ÂºïÔºåÂç≥Ôºö
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.5.3</version>
</dependency>
2.ÊñπÂºè‰∫åÔºö‰∏ãËΩΩÊ∫êÁ†ÅÂíådata.zipÊï∞ÊçÆÈÖçÁΩÆ
3.ÊµãËØïÂàÜËØçÔºöSystem.out.println(HanLP.segment(""Êää‰∏§Âº†ËΩ¶Á•®Êç¢Êàê‰∏âÂº†""));

### Ëß¶Âèë‰ª£Á†ÅÔºöSystem.out.println(HanLP.segment(""Êää‰∏§Âº†ËΩ¶Á•®Êç¢Êàê‰∏âÂº†""));

### ÊúüÊúõËæìÂá∫Ôºö[Êää/pba, ‰∏§/m, Âº†/q, ËΩ¶Á•®/n, Êç¢Êàê/v, ‰∏â/m, Âº†/q]

### ÂÆûÈôÖËæìÂá∫Ôºö[Êää/p, ‰∏§/m, Âº†/q, ËΩ¶Á•®/n, Êç¢/v, Êàê‰∏âÂº†/nr]
"
Ëá™ÂÆö‰πâËØçÂÖ∏ ÊåÅ‰πÖÂåñÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1„ÄÅËá™ÂÆö‰πâËØçÂÖ∏ÊåÅ‰πÖÂåñÂ•ΩÂÉèÂÆòÊñπÊ≤°ÊúâÁªôÂá∫ÊúÄ‰ºòÂÅöÊ≥ïÔºåÂèÇËÄÉ ÈóÆÈ¢ò  [#182#](https://github.com/hankcs/HanLP/issues/182)  ÈúÄË¶ÅÁª¥Êä§ CustomDictionary.txt Êñá‰ª∂ÔºåÊòØÈúÄË¶ÅËá™Â∑±ÈÄöËøáIOÊìç‰ΩúÊñá‰ª∂ Ôºõ 

2„ÄÅÂú®ÈÖçÁΩÆÊñá‰ª∂‰∏≠Â¢ûÂä† CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; **# mydict.txt;** Ëá™ÂÆö‰πâËØçÂÖ∏Êñá‰ª∂ÔºåÊ≤°ÊúâÊïàÊûúÔºåÁîüÊàêÁöÑÁºìÂ≠òÊñá‰ª∂‰πüÊ≤°ÊúâÔºåÊòØÂê¶ËøòÈúÄË¶ÅÊúâÂÖ∂‰ªñÊìç‰ΩúÔºåÊàñËÄÖÈúÄË¶ÅÂçïÁã¨ÁîüÁîüÊàê 

 
"
ÊÇ®Â•ΩÔºåËØ∑ÈóÆÊúâÊ≤°ÊúâÈùûÂÖ®Â±ÄÁöÑÁî®Êà∑ËØçÂÖ∏ÔºåÂç≥ÂàÜËØçÊó∂ÂèØÂä®ÊÄÅÊåáÂÆöÊòØÁî®Âì™‰∏™Áî®Êà∑ËØçÂÖ∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.53
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.53

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºåËØ∑ÈóÆÊúâÊ≤°ÊúâÈùûÂÖ®Â±ÄÁöÑÁî®Êà∑ËØçÂÖ∏ÔºåÂç≥ÂàÜËØçÊó∂ÂèØÂä®ÊÄÅÊåáÂÆö‰ΩøÁî®Âì™‰∏™Áî®Êà∑ËØçÂÖ∏ÔºåÁ±ª‰ººANSJÂàÜËØçÊó∂ÂèØÊåáÂÆö
Â≠òÂÇ®ÁùÄËá™ÂÆö‰πâËØçÈõÜÂêàÁöÑForsetÂèÇÊï∞
"
 HanLPÂØπÁü≠ÊñáÊú¨Ôºà30Â≠ó‰ª•ÂÜÖÔºâÂàÜËØçÊïàÁéá‰ΩéÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
1. ‰ΩøÁî®HanLPÂØπÁü≠ÊñáÊú¨Ôºà30Â≠ó‰ª•ÂÜÖÔºâÔºåÂàÜËØçÊïàÁéáÂú®300ms-500ms‰πãÈó¥„ÄÇÂ±û‰∫éÊ≠£Â∏∏ÊÉÖÂÜµÂêóÔºü
2. Â¶ÇÊûúÊàëÊÉ≥ÊèêÈ´òÂàÜËØçÊïàÁéáÂú®50msÂ∑¶Âè≥Ôºà30Â≠óÂ∑¶Âè≥ÁöÑÁü≠ÊñáÊú¨ÔºâÔºå‰ΩÜÂèà‰∏çÊÉ≥Áî®ÊûÅÈÄüÂàÜËØçÁÆóÊ≥ïÔºåÊúâ‰ªÄ‰πàÂÖ∂‰ªñÂäûÊ≥ïÔºüÊòØÈúÄË¶ÅÈÖçÁΩÆÂêóÔºü
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Êàë‰ΩøÁî®‰∫ÜPortableÁâàÊú¨Âíå600Â§öMÁöÑdataÊï∞ÊçÆÈÉΩÂæóÂà∞‰∫ÜÂêåÊ†∑ÁöÑÁªìÊûúÔºåÂàÜËØçÊïàÁéáÈÉΩÂú®300ms-500ms‰πãÈó¥„ÄÇ
### Ê≠•È™§

Â¶Ç‰∏ã‰ª£Á†ÅÊâÄÁ§∫Ôºö

### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args)
    {
        long current = System.currentTimeMillis();
        List<Term> termList = HanLP.segment(""Â∏ÆÊàëÊâæÊâæÊúÄËøë3Â§©Âú®‰∏âÂ≥°Â§ßÂùùÊãçÁöÑÊó†‰∫∫Êú∫ÁÖßÁâá"");
        long end = System.currentTimeMillis();
        System.out.println(end-current);
    }

   ÊàñËÄÖËøôÊ†∑Ôºö
   public static void main(String[] args)
    {
        // TODO Auto-generated method stub
        Segment segment = HanLP.newSegment().enableAllNamedEntityRecognize(true).enableMultithreading(true).enableNumberQuantifierRecognize(true).enablePartOfSpeechTagging(true);
        long current = System.currentTimeMillis();
        List<Term> termList = segment.seg(""Â∏ÆÊàëÊâæÊâæÊúÄËøë3Â§©Âú®‰∏âÂ≥°Â§ßÂùùÊãçÁöÑÊó†‰∫∫Êú∫ÁÖßÁâá"");
        long end = System.currentTimeMillis();
        System.out.println(end-current);
    }

  
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Â∏åÊúõËÄóÊó∂Âú®50ms‰ª•ÂÜÖ„ÄÇ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
300ms-500ms
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‚ÄúÊù•Âº†Âåó‰∫¨ÁöÑËΩ¶Á•®‚Äú ÂàÜËØç‰∏∫ ‚ÄúÂº†Âåó‚Äù ‚Äú‰∫¨‚Äú,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [X ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
portableÁâà
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰Ω†Â•ΩÔºåËøôÂçäÂπ¥‰∏ÄÁõ¥Âú®Áî®‰Ω†ÂºÄÂèëÁöÑÂàÜËØçÂô®ÂÅöÂÆûÈ™åÔºåÊÑüËßâÂæàÂ•ΩÁî®„ÄÇ‰ΩÜÊòØ‰ªäÂ§©ÂèëÁé∞ ‚ÄúÊù•Âº†Âåó‰∫¨ÁöÑËΩ¶Á•®‚ÄùÔºåÊó†ËÆ∫Áî®ÂåÖ‰∏≠ÁöÑÂá†ÁßçÂàÜËØçÂô®ÈÉΩÂàÜ‰∏çÂá∫ ‚Äú Âåó‰∫¨‚Äù  ÔºåÂü∫Êú¨ÈÉΩÂàÜÊàê  ‚Äú‚ÄùÂº†Âåó‚Äú  ‚Äú‰∫¨‚Äù  „ÄÇËÉΩÂê¶ËµêÊïôÊÄé‰πàËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºüÊàëÂ∑≤ÁªèÂêëËá™ÂÆö‰πâËæûÂÖ∏Ê∑ªÂä†‰∫Ü‚ÄúÂåó‰∫¨‚Äú  ‚ÄúÊù•Âº†‚Äú‚ÄùÔºå‰ΩÜÊòØÊó†Êïà„ÄÇË∞¢Ë∞¢ÔºÅ

## Â§çÁé∞ÈóÆÈ¢ò
Ê≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÔºåÁõ¥Êé•Ë∞ÉÁî®ËøôÂá†‰∏™ÂàÜËØçÂô®

### Ê≠•È™§

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
[Êù•Âº†,Âåó‰∫¨,ÁöÑ,ËΩ¶Á•®]
(ÊàëÂøΩÁï•‰∫ÜËØçÊÄß)
### ÂÆûÈôÖËæìÂá∫

[Êù•Âº†Âåó‰∫¨/nr, ÁöÑ/ude1, ËΩ¶Á•®/n]
[Êù•Âº†Âåó‰∫¨/nr, ÁöÑ/ude1, ËΩ¶Á•®/n]
[Êù•Âº†Âåó‰∫¨/nr, ÁöÑ/ude1, ËΩ¶Á•®/n]
[Êù•/null, Âº†Âåó/null, ‰∫¨/null, ÁöÑ/null, ËΩ¶Á•®/null]

```


"
Âêå‰πâËØçËØçÂÖ∏Á¨¨‰∏ÄÂè•ÂºÄÂ§¥ÂåÖÂê´\U+FEFF,"1. Êàë‰ΩøÁî®ÁöÑmave‰æùËµñÂåÖÁâàÊú¨Âè∑
        <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.5.3</version>
        </dependency>

2. ÂèëÁé∞Êúâ‰∏™ÈóÆÈ¢òÊòØÂêå‰πâËØçËØçÂÖ∏ÂºÄÂ§¥Âá∫Áé∞‰∏Ä‰∏™ÈùûÂç†‰ΩçÁ©∫Ê†º\U+FEFF
![image](https://user-images.githubusercontent.com/5870269/35026499-59e0c7c4-fb86-11e7-90ef-d2862f224090.png)
„Äê„Äê ÁñëÈóÆ „Äë„Äë
UTF-8 with BOMËøôÁßçÊ†ºÂºèÁöÑÂêå‰πâËØçËØçÂÖ∏Âú®Âä†ËΩΩÁöÑÊó∂ÂÄôÔºå‰ºöÂ∞ÜÁ¨¨‰∏Ä‰∏™Â≠óÁ¨¶ÂΩìÂÅöargs[0]Êù•ËÆ°ÁÆóidÂÄºÔºåÈÇ£‰πàËøôÊ†∑ËÆ°ÁÆóÂá∫Êù•ÁöÑIDÂÄºÊòØ‰∏çÊòØÂ≠òÂú®ÈóÆÈ¢òÔºü
![image](https://user-images.githubusercontent.com/5870269/35042223-b80fc476-fbc2-11e7-8030-3dedacaf369e.png)

"
HMM-NGramÂàÜËØçÊ®°ÂûãÁ≠âÊïàËØçÊõøÊç¢ÂêéÂ¶Ç‰Ωï‰øùÁïôÁöÑÂéüËØçÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
master
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
1.5.3
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰∫åÂÖÉÊé•Áª≠ËØçÂÖ∏Âú∞Âêç‰∫∫ÂêçÁ≠âË¢´ÊõøÊç¢Êàê‰∫Ü‚ÄúÁ≠âÊïàËØç‚ÄùÔºåËøôÊ†∑Âú®ËÆ≠ÁªÉ1998Âπ¥ËØ≠ÊñôÁöÑÊó∂ÂÄôÂÉè‚Äú‰∏≠ÂõΩ‚ÄùÔºå‚ÄúÂåó‰∫¨‚ÄùÁ≠âÂú∞ÂêçÈÉΩÊàê‰∫ÜÁ≠âÊïàËØç„ÄÇÂ¶Ç‰∏ãÈù¢Ôºö
`Êú™##‰∏≤	x	130296

Êú™##‰∫∫	nr	607718	nrf	113445

Êú™##Âõ¢	nt	112253	ntc	25517	nto	18894	ntu	5426	nth	2556	ntcb	1846	nts	677	ntch	568	ntcf	118

Êú™##Âú∞	ns	595380	nsf	124178

Êú™##ÂÆÉ	xx	1000

Êú™##Êï∞	mq	753456	m	733982

Êú™##Êó∂	t	757118`
ÊàëËÆ≠ÁªÉÂêéÁöÑÁªìÊûú‰πüÂêå‰∏äÈù¢‰∏ÄÊ†∑ÔºåË¢´ÊõøÊç¢ÂêéÂ∑≤ÊúâÁöÑËØç‚Äú‰∏≠ÂõΩ‚ÄùÂ∞±Ê≤°Êúâ‰∫ÜÂëÄÔºå‰ΩÜÊòØ‰Ω†ÁöÑ‰∫åÂÖÉÊé•Áª≠ËØçÂÖ∏ÈáåËøòÊúâÂéüËØçÔºåÊØîÂ¶ÇÔºö
`‰∏≠ÂõΩ	ns	39573`

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   /**
 * Á≠âÊïàËØçÁºñËØëÂô®
 * @author hankcs
 */
public class PosTagCompiler
{
    /**
     * ÁºñËØëÔºåÊØîÂ¶ÇÂ∞ÜËØçÊÄß‰∏∫Êï∞ËØçÁöÑËΩ¨‰∏∫##Êï∞##
     * @param tag Ê†áÁ≠æ
     * @param name ÂéüËØç
     * @return ÁºñËØëÂêéÁöÑÁ≠âÊïàËØç
     */
    public static String compile(String tag, String name)
    {
        if (tag.startsWith(""m"")) return Predefine.TAG_NUMBER;
        else if (tag.startsWith(""nr"")) return Predefine.TAG_PEOPLE;
        else if (tag.startsWith(""ns"")) return Predefine.TAG_PLACE;
        else if (tag.startsWith(""nt"")) return Predefine.TAG_GROUP;
        else if (tag.startsWith(""t"")) return Predefine.TAG_TIME;
        else if (tag.equals(""x"")) return Predefine.TAG_CLUSTER;
        else if (tag.equals(""nx"")) return Predefine.TAG_PROPER;
        else if (tag.equals(""xx"")) return Predefine.TAG_OTHER;
        return name;
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
ÊúüÊúõÊó¢ÊúâÁ≠âÊïàËØçÔºå‰πüÊúâÂú∞ÂêçÁ≠âÂéüËØçÁöÑÊù°‰ª∂ËΩ¨ÁßªËØçÂÖ∏ÔºåÊàëÁúãhankcsÁöÑCoreNatureDictionary.txt‰πüÊòØÊúâÁöÑ
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
CoreNatureDictionary.txtÊú™ËæìÂá∫Ôºö
`‰∏≠ÂõΩ	ns	39573`
**ÊÉ≥ÈóÆ‰∏ãhankcsÁöÑËÆ≠ÁªÉÁöÑËØçÂÖ∏‰∏∫‰ªÄ‰πàÊó¢ÊúâÁ≠âÊïàËØçÔºå‰πüÊúâÂú∞ÂêçÁ≠âÂéüÊù•ÁöÑËØçÔºåÊòØ‰∏Ä‰∏™ÁªºÂêàÁöÑÁªìÊûúÂë¢?
ÊàëËÆ≠ÁªÉÊï∞ÊçÆÂ∞ë‰∫ÜÂì™‰∏ÄÊ≠•Âë¢ÔºåË∑ü‰Ω†ÁöÑÁªìÊûú‰∏ç‰∏ÄÊ†∑**
## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÊàëÊåáÁöÑËØçÂÖ∏‰∏ªË¶ÅÊåáÁöÑÊòØÔºöCoreNatureDictionary.txtÔºåCoreNatureDictionary.ngram.txtÔºåCoreNatureDictionary.tr.txt
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰ΩøÁî®CRFSegmentËøõË°åÂàÜËØçÊó∂ÔºåÂá∫Áé∞java.lang.NullPointerException,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1. ‰∏ãËΩΩdata-for-1.5.3.zipÔºåËß£ÂéãÂêé‰∏ä‰º†dataÁõÆÂΩïÂà∞hdfs‰∏äÔºõ
2. ÁªßÊâøIIOAdapterÊù•ËØªÂèñhdfs‰∏äÁöÑÊñá‰ª∂Ôºõ
3. hanlp.propertiesÊîæÂú®src/main/resourcesÁõÆÂΩï‰∏ãÔºåÂÖ∂‰∏≠ÊåáÂÆö‰∫ÜCRFSegmentModelPathÂíåÂêÑ‰∏™ËØçÂÖ∏ÁöÑË∑ØÂæÑÔºå‰ª•ÂèäIOAdapterÔºõ
4. Á®ãÂ∫èËøêË°åÊó∂ÔºåcrfÊ®°ÂûãÂä†ËΩΩÊàêÂäü(‰∏çÁÑ∂‰ºöÊä•Êâæ‰∏çÂà∞Ê®°ÂûãÊñá‰ª∂error)Ôºå‰ΩÜmapÁÆóÂ≠ê‰∏≠Ë∞ÉÁî®segment()ËøõË°åÂàÜËØçÊó∂ÔºåÊó•Âøó‰∏≠Êä•Á©∫ÊåáÈíàÂºÇÂ∏∏„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
    val segment = new CRFSegment()
          .enableNameRecognize(true)
          .enableTranslatedNameRecognize(true)
          .enableJapaneseNameRecognize(true)
          .enablePlaceRecognize(true)
          .enableOrganizationRecognize(true)
          .enablePartOfSpeechTagging(true)
          .enableCustomDictionary(true)
   segment.seg(text)
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Caused by: java.lang.NullPointerException
    at com.hankcs.hanlp.algorithm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.CharacterBasedGenerativeModelSegment.segSentence(CharacterBasedGenerativeModelSegment.java:81)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:507)

"
ÊÄé‰πàÂ∞ÜCRFÂàÜËØçÊ®°ÂûãÂ≠òÊîæÂú®hdfs‰∏äÔºåÁÑ∂ÂêésparkËøêË°åÊó∂ËøõË°åÂä†ËΩΩÂë¢Ôºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1. ÊàëËøôËæπÂú®spark‰∏äËøêË°åÊó∂ÔºåÂá∫Áé∞CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåÊàëÁü•ÈÅìÊòØÊ≤°ÊúâÂú®hanlp.properties‰∏≠ÈÖçÁΩÆÊ®°ÂûãÊñá‰ª∂Ë∑ØÂæÑ„ÄÇ
2. ÊàëÊÉ≥ËÆ©sparkÈõÜÁæ§Á®ãÂ∫èÂú®ËøêË°åÊó∂Âä†ËΩΩhdfs‰∏äÁöÑcrfÂàÜËØçÊ®°ÂûãÊñá‰ª∂ÔºåÂêéÁª≠Ë¶ÅÊÄé‰πàÂÅöÔºü

"
Â¶Ç‰Ωï‰ΩøÁî®CRFËß£Á†ÅÁî®CRF++ËÆ≠ÁªÉÁîüÊàêÁöÑÊ®°Âûã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®CRF++Ëß£Á†ÅÁî®CRF++ËÆ≠ÁªÉÁîüÊàêÁöÑÊ®°ÂûãËøõË°åÂÆû‰ΩìËØÜÂà´Êó∂ÔºåÂ¶ÇÊûúÂºÄÂêØÂ§öÁ∫øÁ®ãËøêË°åÔºåÂàô‰ºöÊä•ÈîôÔºåÁåúÊµãÊòØÂÖ∂c++Â∫ïÂ±ÇÊúâÈîôËØØ„ÄÇ
HanLP‰ΩøÁî®‰∫ÜCRFËß£Á†ÅÔºå‰ΩÜÊòØÂè™Áî®‰∫éÂàÜËØç„ÄÇ
ËØ∑ÈóÆÂ¶ÇÊûúË¶ÅÁî®CRFËß£Á†ÅÁî®CRF++ËÆ≠ÁªÉÁîüÊàêÁöÑÊ®°ÂûãËøõË°åÂÆû‰ΩìËØÜÂà´ÔºåÂ∫îËØ•ÂèÇËÄÉÂì™‰∫õÈÉ®ÂàÜÔºåÊàñËÄÖÂèØ‰ª•Â§çÁî®CRFËß£Á†ÅÁî®‰∫éÂàÜËØçÁöÑÂì™‰∫õÊÄùË∑ØÂêóÔºü
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->"
‰æùÂ≠òÊñπÊ≥ïÂàÜÊûê‰∏≠Êó∂Èó¥ËØçÁöÑËØçÊÄßÊú™ËÉΩÊ≠£Á°ÆÊ†áÊ≥®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âú®ÂØπ‰ª•‰∏ãÂè•Â≠êËøõË°åÂàÜÊûêÊó∂Ôºö

ÊàëÁöÑÂ§™Â§™Â∞èÂº†‰ªäÂ§©ÊÑüÂÜí‰∫Ü

ÈÄöËøásegmentÂæóÂà∞ËØçÊÄßÂ¶Ç‰∏ãÔºö
 ['Êàë/rr', 'ÁöÑ/ude1', 'Â§™Â§™/n', 'Â∞èÂº†/n', '‰ªäÂ§©/t', 'ÊÑüÂÜí/vi', '‰∫Ü/ule']
ËøôÈáåÁöÑ‰ªäÂ§©Ë¢´Ëß£ÊûêÊàê‰∏∫Êó∂Èó¥ËØçÔºåÊ≠£Á°Æ„ÄÇ

Âú®‰æùÂ≠òÂàÜÊûê‰∏≠Ôºö
   1     Êàë/rr 	 3	ÂÆö‰∏≠ÂÖ≥Á≥ª 
   2     ÁöÑ/ude1	 1	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª
   3    Â§™Â§™/n  	 6	‰∏ªË∞ìÂÖ≥Á≥ª 
   4    Â∞èÂº†/n  	 6	‰∏ªË∞ìÂÖ≥Á≥ª 
   5    ‰ªäÂ§©/n  	 6	‰∏ªË∞ìÂÖ≥Á≥ª 
   6    ÊÑüÂÜí/vi 	 0	Ê†∏ÂøÉÂÖ≥Á≥ª 
   7     ‰∫Ü/ule	 6	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª

ËøôÈáå‰ªäÂ§©Ë¢´Ëß£ÊûêÊàêÂêçËØçÔºåËôΩÊú™Âá∫ÈîôÔºå‰ΩÜ‰∏çÂ§üÁ≤æÁªÜÔºåÂØºËá¥‰∏§ËÄÖ‰∏ç‰∏ÄËá¥ÁöÑÂéüÂõ†ÊòØÔºü

## Â§çÁé∞ÈóÆÈ¢ò
Êàë‰ΩøÁî®pythonÔºåÈÄöËøájpypeÊù•Ë∞ÉÁî® Hanlp

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰æùÂ≠òÊñπÊ≥ïÂàÜÊûêÁöÑÂàùÂßãÂåñÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÈÄöËøáHanLP.parseDependencyË∞ÉÁî®‰æùÂ≠òÊñπÊ≥ïÂàÜÊûêÔºåÁªèÊü•ÁúãÔºåËøôÊòØ‰∏Ä‰∏™ÈùôÊÄÅÊñπÊ≥ïÔºå‰º∞ËÆ°ÊØèÊ¨°Ë∞ÉÁî®Êó∂ÈÉΩË¶ÅÂä†ËΩΩÊ®°Âûã„ÄÇÊúâÊ≤°ÊúâÂäûÊ≥ïÂèØ‰ª•Â∞ÜparserÁöÑÂÆû‰æã‰øùÂ≠òËµ∑Êù•‰ª•ÈÅøÂÖç‰∏ãÊ¨°Ë∞ÉÁî®Êó∂ÂÜçÂä†ËΩΩÊ®°ÂûãÂë¢Ôºü

Ê≥®ÔºöÊàëÈÄöËøáJpype (python)Êù•‰ΩøÁî®HanLP

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ViterbiÂàÜËØçÂô®Ê®°ÂûãÂºÇÂ∏∏,"
1„ÄÅÂæÖÂàÜËØçÊñáÊú¨ÔºöÈô∂Êüê„ÄÅÈô∂ÊüêÊüêË∑ØÁªèËØ•Â§ÑÊó∂
    public static void main(String args[]) {
        HanLP.Config.DEBUG = true;
        Segment segment = new ViterbiSegment();
        List<Term> termList = segment.seg(""Èô∂Êüê„ÄÅÈô∂ÊüêÊüêË∑ØÁªèËØ•Â§ÑÊó∂"");
        System.out.println(termList);
    }
ÁªèÊü•ÁúãÊó•ÂøóÔºåÈô∂Êüê‰∫∫ÂêçÂ∑≤ÁªèËØÜÂà´Âá∫ÔºåÂèØËØÜÂà´ÁªìÊûú‰æùÁÑ∂‰∏∫Ôºö
[Èô∂/ag, Êüê/rz, „ÄÅ/w, Èô∂/ag, Êüê/rz, Êüê/rz, Ë∑ØÁªè/v, ËØ•/rz, Â§Ñ/n, Êó∂/qt]

Á®ãÂ∫èËøêË°åÊó•Âøó‰∏∫Ôºö
Á≤óÂàÜÁªìÊûú[Èô∂/ag, Êüê/rz, „ÄÅ/w, Èô∂/ag, Êüê/rz, Êüê/rz, Ë∑ØÁªè/v, ËØ•/rz, Â§Ñ/n, Êó∂/qt]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Èô∂ B 771 D 30 C 15 E 9 ][Êüê G 2055 C 1082 D 420 L 34 K 7 ][„ÄÅ M 19857 L 5234 K 4094 ][Èô∂ B 771 D 30 C 15 E 9 ][Êüê G 2055 C 1082 D 420 L 34 K 7 ][Êüê G 2055 C 1082 D 420 L 34 K 7 ][Ë∑ØÁªè A 20843310 ][ËØ• K 18 L 13 ][Â§Ñ L 52 K 10 D 9 ][Êó∂ L 228 C 137 D 134 K 106 B 88 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Èô∂/B ,Êüê/G ,„ÄÅ/M ,Èô∂/B ,Êüê/C ,Êüê/L ,Ë∑ØÁªè/A ,ËØ•/K ,Â§Ñ/D ,Êó∂/L , /K]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÈô∂Êüê BG
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÈô∂Êüê BC

ÂíåÈ¢ÑÊúüÁªìÊûú‰∏çÁ¨¶Âêà„ÄÇ


2„ÄÅËøêÁî®ÂÖ¨ÂºèÂèØËÉΩÊúâËØØ
ÁªèÊ£ÄÊü•‰ª£Á†ÅÁâáÊÆµ‰∏≠ÔºåËÆ°ÁÆóËΩ¨ÁßªÊ¶ÇÁéáÂÖ¨ÂºèÔºàËßÅMathTools.calculateWeightÊñπÊ≥ïÔºâÔºö
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

ËØ•ÂÖ¨ÂºèÁªèÊé®Êï≤ÔºåÂ§ö‰πò‰∫Ü‰∏Ä‰∏™frequency ÔºåÁî±‰∫éÊòØÂ∫ïÂ±ÇÁÆóÊ≥ïÔºå‰∏çÁü•ÁêÜËß£ÊòØÂê¶ÊúâËØØ„ÄÇ

ÊàëÂ∞ÜÂÖ¨Âºè‰øÆÊîπ‰∏∫Ôºö
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

ÈáçÊñ∞ËøêË°å‰ª£Á†ÅÂæóÂà∞Â¶Ç‰∏ãÁªìÊûúÔºö
[Èô∂Êüê/nr, „ÄÅ/w, Èô∂ÊüêÊüê/nr, Ë∑ØÁªè/v, ËØ•/rz, Â§Ñ/n, Êó∂/qt]
Êª°Ë∂≥È¢ÑÊúüÁªìÊûú

3„ÄÅÁî±‰∫éÊòØÂ∫ïÂ±ÇÁÆóÊ≥ï‰ª£Á†ÅÔºå‰∏çÁü•ÊòØÂê¶‰øÆÊîπÊúâËØØÔºåËØ∑ÊåáÂØºÔºÅ
"
Êú∫ÊûÑËØÜÂà´ËßíËâ≤Ê†áÊ≥®ÊòØÂê¶‰∏çÂÖ®„ÄÇÊØîÂ¶ÇK„ÄÅPÊ≤°ÊâæÂà∞ÂØπÂ∫îÁöÑÊÑè‰πâ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
![image](https://user-images.githubusercontent.com/30866940/34856575-ca7ea6ae-f780-11e7-9d4d-c9afbfaa567a.png

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éresize(65536 * 32)ÁöÑÊï∞Â≠óÂ§ßÂ∞èÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂú®ÁúãhanlpÁöÑÊ∫êÁ†ÅÊó∂ÔºåÁúãÂà∞Âú®`src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java`‰∏≠`resize(65536 * 32); `„ÄÇ ËøôÊÆµ‰ª£Á†ÅÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÁîüÊàêÂèåÊï∞ÁªÑËÄåËÆæÁΩÆÊï∞ÁªÑÁöÑÂàùÂßãÂ§ßÂ∞è„ÄÇÊàëÊÉ≥ËØ∑ÈóÆ‰∏∫‰ªÄ‰πàË¶ÅËÆæÁΩÆËøô‰πàÂ§ßÂë¢„ÄÇ65536ÊòØÊ†πÊçÆcharÁöÑÊúÄÂ§ßÂÄºËÆæÂÆöÔºåËÄå32ÊòØ‰∏∫‰∫Ü‰ªÄ‰πàÂéüÂõ†ÈÄâ‰∏≠Ê≠§Êï∞ÂÄºÂë¢Ôºü
"
ÂàÜËØçÈóÆÈ¢òÔºöËá™ÂÆö‰πâËØçÂÖ∏ÊúâÊó∂‰ºöÂ§±Êïà,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.3.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Ëá™ÂÆö‰πâËØçÂÖ∏Âú®ÊüêÁßçÊÉÖÂÜµ‰∏ã‰ºöÂ§±ÊïàÔºåÂØπ‚ÄúÊµ∑ÂçóÁúÅÊµ∑Âè£Â∏ÇÈæôÂçéÂå∫Êò•Êù•Êó©Â∏ÇÂú∫‚ÄùËøõË°åÂàÜËØçÔºåÁªìÊûúÊ≠£Â∏∏Ôºå‰ΩÜÊòØÂ¢ûÂä†‰∏Ä‰∏™Êñπ‰ΩçËØçÂÜÖÂ≠ó‰πãÂêé‰ºöÂá∫Áé∞ÂàÜËØçÂºÇÂ∏∏ÁöÑÊÉÖÂÜµ

## Â§çÁé∞ÈóÆÈ¢ò
ÊàëËá™ÂÆö‰πâÁöÑËØçÂÖ∏Ôºö
```
Êµ∑ÂçóÁúÅ dictrict1 1000
Êµ∑Âè£Â∏Ç dictrict2 1000
ÈæôÂçéÂå∫ dictrict3 1000
Êò•Êù•Êó©Â∏ÇÂú∫ resrge 1000
```
```
   List<Term> termList = HanLP.segment(""Êµ∑ÂçóÁúÅÊµ∑Âè£Â∏ÇÈæôÂçéÂå∫Êò•Êù•Êó©Â∏ÇÂú∫"");
    //ÂàÜËØçÁªìÊûúÔºöÊµ∑ÂçóÁúÅ/dictrict1, Êµ∑Âè£Â∏Ç/dictrict2, ÈæôÂçéÂå∫/dictrict3, Êò•Êù•Êó©Â∏ÇÂú∫/resrge
```
Ê≠§Êó∂ËØ¥Êòé ‚ÄúÊò•Êù•Êó©Â∏ÇÂú∫‚ÄùÊòØÊ≠£Á°ÆÂä†ËΩΩÁöÑ‰∫Ü

### Ëß¶Âèë‰ª£Á†Å

```
   List<Term> termList = HanLP.segment(""Êµ∑ÂçóÁúÅÊµ∑Âè£Â∏ÇÈæôÂçéÂå∫Êò•Êù•Êó©Â∏ÇÂú∫ÂÜÖ"");
```
### ÊúüÊúõËæìÂá∫

```
Êµ∑ÂçóÁúÅ/dictrict1, Êµ∑Âè£Â∏Ç/dictrict2, ÈæôÂçéÂå∫/dictrict3, Êò•Êù•Êó©Â∏ÇÂú∫/resrge, ÂÜÖ/s
```

### ÂÆûÈôÖËæìÂá∫

```
Êµ∑ÂçóÁúÅ/dictrict1, Êµ∑Âè£Â∏Ç/dictrict2, ÈæôÂçéÂå∫/dictrict3, Êò•/tg, Êù•/v, Êó©Â∏Ç/n, Âú∫ÂÜÖ/s
```
Â¢ûÂä†‰∫Ü‚ÄúÂÜÖ‚ÄùÂ≠óÔºåÈáçÊñ∞ÂàÜËØçÔºåÂèëÁé∞ÂàÜËØçÊïàÊûú‰∏éÈ¢ÑÊúüÁöÑÂ∑ÆÂà´ÊØîËæÉÂ§ßÔºåËøôÁßçÊÉÖÂÜµÂ∫îËØ•ÊÄé‰πàÂ§ÑÁêÜÂë¢





  
  "
"ÂßìÂêçÂâçÈù¢ÊúâÂè•Âè∑ÔºåÂØºËá¥ÂßìÂêçËØÜÂà´ÈîôËØØÔºö""„ÄÇÊù®Áëû‰∫ëÂëäËØâÂ§ßÂÆ∂ÔºåÊÖ¢ÊÖ¢ÁöÑÔºå""","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂØπÂè•Â≠êÂâçÈù¢ÊúâÂè•Âè∑ÔºåÂàÜËØçËØÜÂà´‰∫∫ÂêçÈîôËØØ„ÄÇÂà†Èô§Âè•Âè∑ÔºåËØÜÂà´Ê≠£Á°Æ„ÄÇÂè•Â≠êÂ¶Ç‰∏ãÔºö
„ÄÇÊù®Áëû‰∫ëÂëäËØâÂ§ßÂÆ∂ÔºåÊÖ¢ÊÖ¢ÁöÑÔºå

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[„ÄÇ/w, Êù®Áëû‰∫ë/nr, ÂëäËØâ/v, Â§ßÂÆ∂/rr, Ôºå/w, ÊÖ¢ÊÖ¢/d, ÁöÑ/ude1, Ôºå/w]
```

### ÂÆûÈôÖËæìÂá∫

```
[„ÄÇ/w, Êù®Áëû/nr, ‰∫ë/vg, ÂëäËØâ/v, Â§ßÂÆ∂/rr, Ôºå/w, ÊÖ¢ÊÖ¢/d, ÁöÑ/ude1, Ôºå/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âè•Ê≥ï‰æùÂ≠òÂàÜÊûêÈîôËØØ‰∏Ä‰æã,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.5.2

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπ‰∏ãÂàóÂè•Â≠êËøõË°å‰æùÂ≠òÂÖ≥Á≥ªÂàÜÊûêÔºö
‰ªñÈÇ£Ê†∑ÊúâË∫´‰ª∑ÁöÑ‰∫∫ËÉΩÂπ≤Âá∫ËøôÊ†∑ÁöÑ‰∫ãÔºü
ÂæóÂà∞Â¶Ç‰∏ãÁªìÊûúÔºö
1	‰ªñ	‰ªñ	r	rr	_	3	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
2	ÈÇ£Ê†∑	ÈÇ£Ê†∑	r	rzv	_	3	Áä∂‰∏≠ÁªìÊûÑ	_	_
3	Êúâ	Êúâ	v	vyou	_	6	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	Ë∫´‰ª∑	Ë∫´‰ª∑	n	n	_	3	Âä®ÂÆæÂÖ≥Á≥ª	_	_
5	ÁöÑ	ÁöÑ	u	ude1	_	3	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
6	‰∫∫	‰∫∫	n	n	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
7	ËÉΩÂπ≤	ËÉΩÂπ≤	a	a	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
8	Âá∫	Âá∫	v	vf	_	7	Âä®Ë°•ÁªìÊûÑ	_	_
9	ËøôÊ†∑	ËøôÊ†∑	r	rzv	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
10	ÁöÑ	ÁöÑ	u	ude1	_	9	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
11	‰∫ã	‰∫ã	n	n	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
12	Ôºü	Ôºü	wp	w	_	11	Ê†áÁÇπÁ¨¶Âè∑	_	_
Ëøô‰∏™ÁªìÊûúÂ∫îËØ•ÊòØÈîôÁöÑÂêßÔºüÊ†∏ÂøÉÂÖ≥Á≥ªÂ∫îËØ•ÊòØ‚ÄùÂπ≤Âá∫‚ÄúÂêßÔºü


"
Âú®Android‰∏ä‰ΩøÁî®Â§ñÊåÇÂ≠óÂÖ∏ÈÅáÂà∞sun.reflect.ReflectionFactoryÁ±ªÊâæ‰∏çÂà∞ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.5

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âú®Android‰ΩøÁî®hanlp.propertiesÈÖçÁΩÆÂ§ñÊåÇÂ≠óÂÖ∏Êó∂ÔºåÈÅáÂà∞ÈóÆÈ¢òÔºö
01-04 21:49:47.031 1196-2513/com.nio.nlu W/HanLP: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
                                                  Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
                                                  
                                                  --------- beginning of crash
01-04 21:49:47.033 1196-2513/com.nio.nlu E/AndroidRuntime: FATAL EXCEPTION: Thread-66
                                                           Process: com.nio.nlu, PID: 1196
                                                           Theme: themes:{}
                                                           java.lang.NoClassDefFoundError: Failed resolution of: Lsun/reflect/ReflectionFactory;
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34)
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43)
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50)
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202)
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505)
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561)
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105)
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java)
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95)
                                                               at java.lang.Thread.run(Thread.java:818)
                                                            Caused by: java.lang.ClassNotFoundException: Didn't find class ""sun.reflect.ReflectionFactory"" on path: DexPathList[[zip file ""/system/app/NluService/NluService.apk""],nativeLibraryDirectories=[/system/app/NluService/lib/arm, /system/app/NluService/NluService.apk!/lib/armeabi-v7a, /vendor/lib, /system/lib]]
                                                               at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:56)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:511)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:469)
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34)¬†
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43)¬†
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)¬†
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306)¬†
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63)¬†
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50)¬†
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202)¬†
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)¬†
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505)¬†
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561)¬†
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105)¬†
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java)¬†
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95)¬†
                                                               at java.lang.Thread.run(Thread.java:818)¬†
                                                           	Suppressed: java.lang.ClassNotFoundException: sun.reflect.ReflectionFactory
                                                               at java.lang.Class.classForName(Native Method)
                                                               at java.lang.BootClassLoader.findClass(ClassLoader.java:781)
                                                               at java.lang.BootClassLoader.loadClass(ClassLoader.java:841)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:504)
                                                               		... 15 more
                                                            Caused by: java.lang.NoClassDefFoundError: Class not found using the boot class loader; no stack trace available



## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÊòØÂê¶ÊòØAndroidÁöÑjavaÂπ≥Âè∞Âπ≥Âè∞Êú¨Ë∫´Â∞±‰∏çÂåÖÂê´Ëøô‰∏™Á±ªÔºåÊòØÂê¶ÂèØ‰ª•Áî®java.lang.reflectÊõø‰ª£sun.reflectÔºü
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"Âª∫ËÆÆÊ†∏ÂøÉËØçÂÖ∏Ê∑ªÂä†""Âú∞ n""Ëøô‰∏™ËØçÊù°","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömater
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂπøÂ∑ûÁ≠âÂú∞ÊîøÂ∫úÂèëË°åÁöÑÂÄ∫Âà∏Â§ßÈù¢ÁßØÂá∫Áé∞‰∫ÜÈóÆÈ¢ò„ÄÇ
""Âú∞""‰ºöËØÜÂà´‰∏∫ude2ËØçÊÄßÔºåÊòéÊòæ‰∏çÂØπ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->


  "
Â∏∏ËßÅÂßìÂêçËØÜÂà´Âá∫ÈîôÔºöËµµÁ∫¢,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂéüÂè•Ôºö‰æ¶Êü•ÂëòÊ†∏ÂÆû‰∫Ü‰ø°Áî®Âç°ÊåÅÊúâ‰∫∫ËµµÁ∫¢ÁöÑÊÉÖÂÜµÂêéÔºå

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‰æ¶Êü•Âëò/nnt, Ê†∏ÂÆû/v, ‰∫Ü/ule, ‰ø°Áî®Âç°/n, ÊåÅÊúâ‰∫∫/nz, ËµµÁ∫¢/nr, ÁöÑ/ude1, ÊÉÖÂÜµ/n, Âêé/f, Ôºå/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
‰æ¶Êü•Âëò/nnt, Ê†∏ÂÆû/v, ‰∫Ü/ule, ‰ø°Áî®Âç°/n, ÊåÅÊúâ‰∫∫/nz, Ëµµ/nz, Á∫¢/a, ÁöÑ/ude1, ÊÉÖÂÜµ/n, Âêé/f, Ôºå/w
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->


  "
Â∏∏ËßÅÁü≠ËØ≠ËØØËØÜÂà´‰∏∫Êú∫ÊûÑÔºö‚ÄúÂÖ¨ÂºÄÊîøÂ∫ú‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÂéüÂè•ÔºöÂ∫îÂΩìÂèäÊó∂„ÄÅÂáÜÁ°ÆÂú∞ÂÖ¨ÂºÄÊîøÂ∫ú‰ø°ÊÅØ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Â∫îÂΩì/v, ÂèäÊó∂/ad, „ÄÅ/w, ÂáÜÁ°Æ/ad, Âú∞/ude2, ÂÖ¨ÂºÄ/v, ÊîøÂ∫ú/n, ‰ø°ÊÅØ/n, „ÄÇ/w
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Â∫îÂΩì/v, ÂèäÊó∂/ad, „ÄÅ/w, ÂáÜÁ°Æ/a, Âú∞/ude2, ÂÖ¨ÂºÄÊîøÂ∫ú/ntc, ‰ø°ÊÅØ/n, „ÄÇ/w
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰æùÂ≠òÂàÜÊûêÊòØÂê¶Á∫øÁ®ãÂÆâÂÖ®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ &radic;] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2 Á¶ªÁ∫ø

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÈóÆÈ¢òÂæàÁÆÄÂçïÔºåÊñáÊ°£ËØ¥‰∫ÜHanLpÁöÑÂàÜËØçÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÔºåÈÇ£‰æùÂ≠òÂàÜÊûêÊòØ‰∏çÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÂë¢Ôºü‰ΩøÁî®Âú®Â§öÁ∫øÁ®ã‰∏≠‰ΩøÁî®‰æùÂ≠òÂàÜÊûêÈúÄË¶ÅÊ≥®ÊÑè‰ªÄ‰πàÔºü



  "
ÂÖ≥‰∫éÂïÜÂìÅÂûãÂè∑ÔºàÂ≠óÊØçÔºåÂàÜÈöîÁ¨¶ÔºåÊï∞ÊçÆÁöÑÁªÑÂêàÔºâÁöÑÂàÜËØçÊïàÊûúÊé¢ËÆ®,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2 Á¶ªÁ∫ø

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÈááÁî®Ê†áÂáÜÂàÜËØçÔºåËøõË°å‰∏Ä‰∫õÂåÖÂê´ÂïÜÂìÅÊèèËø∞ÁöÑËØ≠Âè•ËøõË°åÂàáÂàÜ„ÄÇÂïÜÂìÅÂûãÂè∑ÈÄöÂ∏∏ÊòØ‰ªªÊÑèÁöÑÂ≠óÊØçÔºåÂàÜÈöîÁ¨¶ÔºåÊï∞ÊçÆÁöÑÁªÑÂêà„ÄÇ‰∏çÊòØ‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÊú∫ÊûÑÁ≠â„ÄÇ
ËøõË°åÂàÜËØçÊó∂ÔºåÂèØËÉΩÊúÄÁªàÁªìÊûúÁ≤íÂ∫¶ËøáÁªÜ„ÄÇËÄåÂ∏åÊúõÊòØ‰∏Ä‰∏™Êï¥‰Ωì„ÄÇ
Áî±‰∫éÂûãÂè∑Âú®‰∏çÊñ≠Â¢ûÂä†ÔºåÊØîËæÉÁ¨®ÁöÑÂÅöÊ≥ïÊòØÊó∂ÂÄôÊ∑ªÂä†ËØçÂ∫ìÔºå‰ΩÜÈÇ£Ê†∑Â∑•‰ΩúÈáèÊØîËæÉÂ§ß„ÄÇËæÉ‰∏∫ÁπÅÁêê„ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò

Sample: SAMSUNG ‰∏âÊòü **RS542NCAEWW/SC** 545L È£éÂÜ∑ÂèòÈ¢ëÂØπÂºÄÈó®ÂÜ∞ÁÆ±ÔºåËøòË°åËøòË°åÔºåÂèØ‰ª•ÁúãÁúã;
ÂàÜËØçÁªìÊûúÔºö [SAMSUNG][ ][‰∏âÊòü][ ][RS][542][NCAEWW][/][SC][ ][545][L][ ][È£éÂÜ∑][ÂèòÈ¢ë][ÂØπÂºÄÈó®ÂÜ∞ÁÆ±][\][n][ËøòË°å][ËøòË°å][Ôºå][ÂèØ‰ª•][ÁúãÁúã][;]

### ÊúüÊúõËæìÂá∫

[SAMSUNG][ ][‰∏âÊòü][ ][RS542NCAEWW/]SC[ ][545][L][ ][È£éÂÜ∑][ÂèòÈ¢ë][ÂØπÂºÄÈó®ÂÜ∞ÁÆ±][\][n][ËøòË°å][ËøòË°å][Ôºå][ÂèØ‰ª•][ÁúãÁúã][;]

Âç≥„ÄÇÂ∏åÊúõÊòØ‰∏Ä‰∏™Êï¥‰Ωì ‚Äú**RS542NCAEWW/]SC**‚Äù


## ÂÖ∂‰ªñ‰ø°ÊÅØ

ËøôÁßç‰æãÂ≠êÊúâÂæàÂ§öÔºåÂ¶Ç 
WD Ë•øÈÉ®Êï∞ÊçÆ **WD20EZRZ** Âè∞ÂºèÊú∫Á°¨Áõò ËìùÁõò 2TB

Â∏åÊúõ ‚ÄúWD20EZRZ ‚Äù Êï¥‰ΩìÂàáÂàÜ

NORITZ ËÉΩÁéá **JSQ25-E4/GQ-13E4AFEX** 13ÂçáÁáÉÊ∞îÁÉ≠Ê∞¥Âô®Èò≤ÂÜªÂûã

Â∏åÊúõ ‚Äò‚ÄôJSQ25-E4‚Äú Êï¥‰ΩìÂàáÂàÜ

  "
Ë∞ÉÁî®‰æùÂ≠òËØ≠Ê≥ïÂàÜÊûêÊé•Âè£CRFDependencyParser.compute(sentence)ÔºõÂá∫Áé∞ËøîÂõûÁªìÊûúÊ≠ªÂæ™ÁéØÊÉÖÂÜµ,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.5.2.jar

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Ë∞ÉÁî®‰æùÂ≠òËØ≠Ê≥ïÂàÜÊûêÊé•Âè£CRFDependencyParser.compute(sentence)ÔºõÂá∫Áé∞ËøîÂõûÁªìÊûúÊ≠ªÂæ™ÁéØÊÉÖÂÜµ
ËøîÂõûÁªìÊûú‰∏≠id‰∏∫5ÁöÑCoNLLWordÁöÑhead‰∏∫6Ôºåid‰∏∫6ÁöÑCoNLLWordÁöÑhead‰∏∫5„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
Ê≤°Êúâ‰øÆÊîπÊ∫êÁ†ÅÁõ¥Êé•Ë∞ÉÁî®CRFDependencyParser.compute(sentence)Êé•Âè£Ôºõ

### Ëß¶Âèë‰ª£Á†Å

```
    public void testCompute() throws Exception
    {
        String sentence = ""Áî∑Â≠ê‰ªéÂé¶Èó®Ê∏∏Ê≥≥Âà∞ÈáëÈó®"";
        CoNLLSentence cs = CRFDependencyParser.compute(sentence);
        System.out.println(cs.word[4].HEAD.ID);
        System.out.println(cs.word[5].HEAD.ID);
    }
```
### ÊúüÊúõËæìÂá∫

```
0
5
```

### ÂÆûÈôÖËæìÂá∫

```
6
5
```"
Áî®‰∫éÂßìÂêçËØÜÂà´ÊîπËøõÁöÑËÆ®ËÆ∫Issue,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰πãÂâçÁî±‰∫éÂ∑•‰Ωú‰∏äÈúÄË¶ÅÔºåÁ†îÁ©∂‰∫Ü‰∏Ä‰∏ãHanLPÁöÑÂÆû‰ΩìËØÜÂà´Â≠òÂú®ÁöÑ‰∏çË∂≥„ÄÇ‰∏ªË¶ÅÊòØËØØÂè¨ÂõûÊØîËæÉÈ´ò„ÄÇ‰πüÂ∞ùËØï‰∫Ü‰∏Ä‰∫õÊñπÊ≥ï‰øÆÊîπÔºåËÉΩËß£ÂÜ≥ÈÉ®ÂàÜÈóÆÈ¢ò„ÄÇÂáÜÂ§áÂú®ÊàëÁöÑÂàÜÊîØ‰∏äÈù¢‰øÆÊîπÂÜçPR„ÄÇÊâÄ‰ª•ËøôÈáåÂºÄ‰∏Ä‰∏™IssueËøõË°åËÆ®ËÆ∫„ÄÇÊú¨Issue‰∏çÂ§™Ê∂âÂèäÂÖ∑‰ΩìËØÜÂà´ÈîôËØØÁöÑÊÉÖÂÜµÔºå‰∏ªË¶ÅÁî®‰∫éÊî∂ÈõÜÂèçÈ¶àÂíåËÆ®ËÆ∫„ÄÇ

ÊàëÂú®ÊàëÁöÑForkÊñ∞Âª∫‰∫Ü‰∏Ä‰∏™[Project](https://github.com/TylunasLi/HanLP/projects)Ôºö

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue729() throws Exception
    {
        String[] sentences = new String[]{
                ""Ë¥∑Ê¨æÁöÑÈí±ÊâìÁªôË∞Å"",
                ""‰ªäÂ§©Èí±Êâ£‰∫ÜÊúâÈÄæÊúüÂêó""
        };
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enablePlaceRecognize(true).enableOffset(true);
        for (String sentence : sentences)
                systme.out.println(segment.seg(sentence));
    }
```

### ÊúüÊúõËæìÂá∫


```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫


```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ


"
‰øÆÂ§ç Issue 691 ‰ª•ÂèäË∞ÉËØïÊó∂ÁúãÂà∞ÁöÑ‰∏Ä‰∏™Bug,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

1. Áî®Ë∞ÉËØïÂô®ÂØüÁúãBinTrieÁªìÊûÑÊó∂‰ºöÂèëÁîüÊä•ÈîôÔºåÊµãËØïÂèëÁé∞‰∏ªË¶ÅÂéüÂõ†ÊòØBaseNode‰∏≠ÁöÑtoString()Ê≤°Âà§Á©∫ÔºåÊâÄ‰ª•ËøôÈáå‰øÆÊîπ‰∏Ä‰∏ãÔºõ
2. Issue #691 N-ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØç‰∏≠‰∏ç‰ºöÂ∞ÜÁã¨Á´ãÁöÑÈÄóÂè∑ËØçÊÄßÂèòÊàê /mÔºå‰ΩÜÂèØ‰ª•ËØÜÂà´Â∏¶ÂçÉ‰ΩçÂàÜÈöîÁ¨¶ÁöÑÊï∞Â≠ó„ÄÇ
Ôºà‰øÆÊîπ‰∫ÜÂéüÂ≠êÂàáÂàÜÁöÑÈÄªËæë„ÄÇÂ¶ÇÊûúÂ∏åÊúõ‰∏§‰∏™Êï∞Â≠ó‰∏≠Èó¥ÊúâÈÄóÂè∑Êó∂ÂàÜËØçÂàÜÂºÄÔºåÈúÄË¶Å‰øÆÊîπËøôÊÆµÈÄªËæë„ÄÇÊï∞Â≠óËØÜÂà´ÊúÄÂ•ΩÂÅöÊàêÁä∂ÊÄÅÊú∫„ÄÇÔºâ

Âè¶Â§ñÂæà‰∏çÂ•ΩÊÑèÊÄùÔºåÊàëÊåâÁÖßGithub Help‰∏äÁöÑÊñπÊ≥ïÂêåÊ≠•forkÔºå‰ΩÜÊìç‰ΩúÂá∫‰∫Ü‰∏Ä‰∫õÈóÆÈ¢òÔºåÂØºËá¥ÊÇ®ËøëÊúüÊèê‰∫§ÁöÑIssueÂÖ®Ë¢´Âà∑‰∫Ü‰∏ÄÈÅç„ÄÇÁúãÊù•‰ª•ÂêéËøòÊòØËÄÅÂÆûÁî®PRÂêàÂπ∂Âêß„ÄÇ

## Áõ∏ÂÖ≥issue

issue #691
"
Â¶Ç‰ΩïÂú®Á¥¢ÂºïÂàÜËØç‰∏≠Âè™‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØç,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.3.4

## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºåÊàëÂ∏åÊúõÂú®Á¥¢ÂºïÂàÜËØç‰∏≠Âè™‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑËØçÂÖ∏ËøõË°åÂàÜËØçÔºåÂÖ≥Èó≠ÂÖ∂‰ªñËØçÂÖ∏„ÄÇËØ∑ÈóÆÊúâ‰ΩïÊñπÊ≥ïÂêóÔºü
Âõ†‰∏∫Á¥¢ÂºïÂàÜËØç‰∏≠ÊîØÊåÅÂÖ®ÂàáÂàÜÔºåÂõ†Ê≠§ÊàëÂ∏åÊúõ‰ª•Ê≠§Êù•ÈÅøÂÖçÂü∫‰∫éËØçÂÖ∏ÁöÑÂàÜËØçÊñπÊ≥ïÂè™ËÉΩÂåπÈÖçÊúÄÈïøËØçÁöÑÁº∫ÁÇπ„ÄÇ
ÊúüÂæÖ‰Ω†ÁöÑÂõûÂ§çË∞¢Ë∞¢„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
Â≠óÂÖ∏‰∏≠‰∏∫‚ÄúÂ§èÊ¥õÁâπÁÉ¶ÊÅº n, Â§èÊ¥õ nr‚Äù
List<Term> termList = IndexTokenizer.segment(""Â§èÊ¥õÁâπÁÉ¶ÊÅº"");

### ÊúüÊúõËæìÂá∫

```
Â§èÊ¥õÁâπÁÉ¶ÊÅº/n [0:5]
Â§èÊ¥õ/nr [0:2]
```

### ÂÆûÈôÖËæìÂá∫

```
Â§èÊ¥õÁâπÁÉ¶ÊÅº/n [0:5]
Â§èÊ¥õ/nr [0:2]
Â§èÊ¥õÁâπ/nrf [0:3]
ÁÉ¶ÊÅº/an [3:5]
```
"
‰∫∫ÂêçËØÜÂà´Âá∫ÈîôÔºöÂÖöÂëòÊù•Âà∞ÊùëÊ∞ëÁÑ¶ÁéâËé≤ÂÆ∂‰∏≠,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰∫∫ÂêçËØÜÂà´Âá∫ÈîôÔºöÂÖöÂëòÊù•Âà∞ÊùëÊ∞ëÁÑ¶ÁéâËé≤ÂÆ∂‰∏≠
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true).enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÂÖöÂëò/nnt, Êù•Âà∞/v, ÊùëÊ∞ë/n, ÁÑ¶ÁéâËé≤/nr, ÂÆ∂‰∏≠/s
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÖöÂëò/nnt, Êù•Âà∞/v, ÊùëÊ∞ë/n, ÁÑ¶/ng, ÁéâËé≤/nz, ÂÆ∂‰∏≠/s
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïÂú®python‰∏≠ËØÜÂà´Êó•Êú¨‰∫∫ÂêçÁöÑËØëÂêç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<1.5.2Ôºõmaster>

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<Â¶Ç‰ΩïÂú®python‰∏≠ÂÆûÁé∞ÂØπÊó•Êú¨‰∫∫ÂêçËØëÂêçÁöÑÂàÜËØç>

Êú¨‰∫∫‰ΩøÁî®‰∫Ühttp://www.hankcs.com/nlp/python-calls-hanlp.html‰∏≠Âú®python‰∏≠‰ΩøÁî®HanLPÁöÑÊñπÊ≥ïÔºåÊàêÂäüÂ§çÁé∞ÊâÄÊúâÊñá‰∏≠ÊèêÂà∞ÁöÑÂäüËÉΩÔºåÂ¶Ç‰ΩïÂÆûÁé∞Êó•Êú¨‰∫∫ÂêçËØÜÂà´„ÄÇ
ÊàëÁõÆÂâçÂú®JClass‰∏≠Ë∞ÉÁî®‰∫Ücom.hankcs.hanlp.recognition.nr.JapanesePersonRecognitionÂåÖÔºå‰ΩÜÊòØ‰∏çÁü•ÈÅì‰ΩøÁî®Âì™‰∏Ä‰∏™ÊñπÊ≥ï„ÄÇ

"
ÂèëÁé∞Êñ∞ËØçÁöÑÊñπÊ≥ïÔºåËøáÊª§ÁöÑÊó∂ÂÄôÂª∫ËÆÆshiyonLinkedList,"   /**
     * ÊèêÂèñËØçËØ≠
     *
     * @param reader Â§ßÊñáÊú¨
     * @param size   ÈúÄË¶ÅÊèêÂèñËØçËØ≠ÁöÑÊï∞Èáè
     * @return ‰∏Ä‰∏™ËØçËØ≠ÂàóË°®
     */
    public List<WordInfo> discover(BufferedReader reader, int size) throws IOException

Âú®Line 83Ë°åÔºåArrayListÊõøÊç¢‰∏∫LinkedListÔºåÂÅöÁßªÈô§Êó∂Ôºå‰ºöÂáèÂ∞ëremoveÂ∏¶Êù•ÁöÑlistÂ∑¶ÁßªÁöÑÂ§çÊùÇÂ∫¶ÔºåÈÄüÂ∫¶ÊúâÂæàÂ§ßÊèêÂçá
"
‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁõ∏ÂÖ≥ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºåÊàëÊÉ≥Ê†πÊçÆËá™Â∑±ÁöÑÈ¢ÜÂüüËØ≠ÊñôËÆ≠ÁªÉ‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÂè•Ê≥ïÂàÜÊûêÊ®°ÂûãÔºå‰ΩÜÊòØÊ≤°ÊúâÁúãÂà∞Áõ∏ÂÖ≥ËµÑÊñôÔºåÈ∫ªÁÉ¶ÈóÆ‰∏ãÊÇ®Ôºåhanlp‰∏≠ÊúâÊ†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôÁîüÊàêÁ•ûÁªèÁΩëÁªúÂè•Ê≥ïÂàÜÊûêÊ®°ÂûãÁöÑÊ®°Âùó‰πàÔºüÊâìÊâ∞‰∫ÜÔºåË∞¢Ë∞¢ÔºÅ
‰∏ªË¶ÅÊòØÊúâ‰∏§‰∏™ÈóÆÈ¢ò‰∏çÂ§™ÊòéÁ°ÆÔºö
Ôºà1ÔºâÂè•Ê≥ïÂàÜÊûêÁöÑËØ≠ÊñôÂ∫îËØ•Â¶Ç‰ΩïÊ†áÊ≥®Âë¢ÔºåÊàëÂà©Áî®gateÂèØ‰ª•ËøõË°åÊôÆÈÄöÁöÑËØçÊÄßÂíåÂëΩÂêçÂÆû‰ΩìÊ†áÊ≥®Ôºå‰ΩÜÊòØ‰∏çÊôìÂæóÂè•Ê≥ïÂàÜÊûêÁöÑÊ†áÊ≥®ÊñπÂºèÔºõ
Ôºà2ÔºâÊ†áÊ≥®ÂêéÁöÑËØ≠ÊñôÂ¶Ç‰ΩïËΩ¨Êç¢ÊàêhanlpÊâÄÊîØÊåÅÁöÑÁ•ûÁªèÁΩëÁªúÂè•Ê≥ïÂàÜÊûêÊ®°Âûã„ÄÇ
‰∏áÂàÜÊÑüË∞¢ÔºÅ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËÉΩÂÖ≥Èó≠ÂØπ‰∫éÊ†áÁÇπÁ¨¶Âè∑ÁöÑËØçÊÄßÊ†áÊ≥®ÂêóÔºü,"ÂØπ‰∫éÊ†áÁÇπÁ¨¶Âè∑ÁöÑËØçÊÄßÊ†áÊ≥®‰∏çÂáÜÁ°Æ„ÄÇ
ÊØîÂ¶Ç Ôºö (ËÆ∞ËÄÖÊùéÈë´ËΩ∂ Êï¥ÁêÜ)
ÂàÜËØçÁªìÊûú‰∏∫Ôºö(/w	ËÆ∞ËÄÖ/nnt	ÊùéÈë´/nr	ËΩ∂/ng	 /n	Êï¥ÁêÜ/v	)/w	

Á©∫Ê†ºÊ†áÊ≥®‰∏∫nÔºå‰∏çÁêÜËß£„ÄÇ

ËÉΩÊääÊ†áÁÇπÁöÑËØçÊÄßÊ†áÊ≥®ÂÖ≥Èó≠‰∫ÜÂêóÔºü
"
‰Ω†Â•ΩÔºåÂÖ≥‰∫éÊîØÊåÅËá™ÂÆö‰πâËØçÂ∫ìËã±ÊñáÁ©∫Ê†ºÂàÜÈöîÁ¨¶,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2 ÈùûPortableÁâàÊú¨

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Ëøô‰∏™ÈóÆÈ¢ò‰πãÂâçÊúâÂêåÂ≠¶ÊèêËøáÔºåÊàëÂú®ÂÖ∂‰ªñissueÁúãÂà∞Ëøá„ÄÇ
Â∞±ÊòØ‰∏Ä‰∫õÔºåË•øÊñπËØ≠Ë®Ä„ÄÅÊó•ËØ≠Á≠â‰∏Ä‰∫õËØçÊòØÊúâÁ©∫Ê†º‰Ωú‰∏∫ÂàÜÂâ≤Á¨¶ÁöÑ„ÄÇ
Â¶ÇÔºö
Mystery Ranch nz 1
Altec Lansing Technologies nz 1

## Â§çÁé∞ÈóÆÈ¢ò
Âè™Ë¶ÅËá™ÂÆö‰πâËØçÂÖ∏ÂåÖÂê´ËøôÁßçËØçÔºåÂ∞±Êó†Ê≥ïËß£Êûê‰∫Ü

##Ëß£ÂÜ≥ÂäûÊ≥ï
1. Âä®ÊÄÅinsert Ôºå‰ΩÜÊòØÊÑüËßâËøôÊ†∑ÊØîËæÉÊÖ¢ÔºåÂ∏åÊúõÁªü‰∏ÄÁÆ°ÁêÜ
2. Á≠âÂæÖÊñ∞ÁâàÊú¨‰øÆÂ§ç„ÄÇ ÊàëÁúãËøô‰∏™ÊòØ‰ªäÂπ¥3Êúà‰ªΩÊúâËÆ°ÂàíÔºå‰ΩÜÊòØ‰ºº‰πéÁé∞Âú®ËøòÊ≤°ÊúâÊîØÊåÅÂë¢„ÄÇ
3. Êú¨Ê¨°ÁöÑÊÑèÂõæÔºöÊÉ≥Ëá™Ë°å‰øÆÊîπ‰ª£Á†ÅÔºåËá™Ë°åÁî®Âà´ÁöÑÂàÜÈöîÁ¨¶ËøõË°åËß£Êûê„ÄÇ

"
ËØ∑ÈóÆ‰∏ãhanlpÊ®°ÂûãËá™Â∏¶ÁöÑcustom dictionaryÊòØÊ†πÊçÆ‰ªÄ‰πàÊù•Âà∂ÂÆöÁöÑÔºü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.5.2, data-for-1.3.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.5.2, data-for-1.3.3

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->
ÊàëÂú®‰ΩøÁî®standardtokenizer ÁöÑÊó∂ÂÄôÊúâÈÅáÂà∞‰∏Ä‰∏™ÈóÆÈ¢òÔºå ÂÉè‚ÄúÂ§ß‰ºó‚ÄùÔºå ‚ÄúÁé∞‰ª£‚ÄùÔºå ‚ÄúÂÖâÊòé‚ÄúÔºå‰∏çÁÆ°‰ªª‰ΩïÁä∂ÊÄÅ‰∏ãÈÉΩ‰ºöËØÜÂà´ÊàêntcÔºå ÂêéÊù•Êàë‰ª¨ÊúâËøõÂéªÁúãÂà∞custom dictionaryÊúâÂØπËøô‰∏™Ë∞ÉÊï¥‰∏Ä‰∫õfrequency,ÊÉ≥ËØ∑Êïô‰∏ãËøô‰∏™frequencyÊòØÊÄé‰πàËÆæÂÆöÁöÑ„ÄÇË∞¢Ë∞¢ÔΩûÔΩûÔΩû



"
word2vectorÂäüËÉΩ‰∏≠ÔºåÂä†ËΩΩËØçÂêëÈáèÊ®°Âûã128Áª¥Â∫¶Ë∂ÖÂá∫ÈôêÂà∂,"
## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.2



## ÊàëÁöÑÈóÆÈ¢ò
Âä†ËΩΩ‰∏Ä‰∏™128Áª¥ÁöÑËØçÂêëÈáèÊ®°ÂûãÔºåÊäõÂá∫ÂºÇÂ∏∏
java.lang.ArrayIndexOutOfBoundsException: 128


"
ËÆ≠ÁªÉÊúÄÊñ∞‰∏≠ÊñáwikiÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ËÆ≠ÁªÉÊúÄÊñ∞ÁöÑzh wikiÊäõÂá∫Êï∞ÁªÑË∂äÁïåÂºÇÂ∏∏
## Â§çÁé∞ÈóÆÈ¢ò

[hadoop@LOCAL-202-89 new]$ java -cp hanlp-portable-1.5.2.jar com.hankcs.hanlp.mining.word2vec.Train -input zhwiki-latest-pages-articles.xml.simplified -output zhwiki.txt

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -3
	at com.hankcs.hanlp.mining.word2vec.TextFileCorpus.reduceVocab(TextFileCorpus.java:69)
	at com.hankcs.hanlp.mining.word2vec.TextFileCorpus.learnVocab(TextFileCorpus.java:142)
	at com.hankcs.hanlp.mining.word2vec.Word2VecTraining.trainModel(Word2VecTraining.java:326)
	at com.hankcs.hanlp.mining.word2vec.Train.execute(Train.java:33)
	at com.hankcs.hanlp.mining.word2vec.Train.main(Train.java:38)"
customÊñá‰ª∂Â§π‰∏ãÁöÑËØçÂÖ∏ÈóÆÈ¢òÔºüÊØîÂ¶Ç‰∏äÊµ∑„ÄÅÂÖ®ÂõΩÂú∞ÂêçÁ≠âÁ≠â„ÄÇ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºÅ‰∏çÂ•ΩÊÑèÊÄùÊâìÊâ∞‰∫Ü„ÄÇÊàëÁé∞Âú®Ê≠£Âú®‰ΩøÁî®hanlpÔºåÊÉ≥ËØ∑ÈóÆ‰∏ãcustomÊñá‰ª∂Â§π‰∏ãÁöÑËØçÂÖ∏ÈÉΩÊòØÊÄé‰πàÊù•ÁöÑÔºüÊØîÂ¶Ç‰∏äÊµ∑„ÄÅÂÖ®ÂõΩÂú∞ÂêçÁ≠âÁ≠â„ÄÇË∞¢Ë∞¢~


"
Âü∫‰∫éCRFÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊä•OutOfMemoryErrorÂºÇÂ∏∏,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÊÉ≥Ëá™Â∑±ËÆ≠ÁªÉ‰∏Ä‰∏™‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®ÔºåÁúãÂà∞‰Ω†ÂçöÂÆ¢‰∏äÈù¢CRFÊèê‰æõÊèê‰æõ‰∫ÜÈÄöËøáCRF++ÁöÑËÆ≠ÁªÉÊñπÂºèÔºå
Êàë‰ΩøÁî®CRF++ËÆ≠ÁªÉÁöÑ‰∏Ä‰∏™150MÂ∑¶Âè≥ÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêmodelÔºåÊõøÊç¢ÊéâhanLP‰∏≠ÁöÑCRFDependencyModelMini.txt.binÔºåÂèëÁé∞Êä•Â¶Ç‰∏ãÈîôËØØÔºö
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at com.hankcs.hanlp.model.crf.CRFModel.load(CRFModel.java:334)
	at com.hankcs.hanlp.dependency.CRFDependencyParser$CRFModelForDependency.load(CRFDependencyParser.java:234)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.loadDat(CRFDependencyParser.java:104)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.load(CRFDependencyParser.java:94)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:54)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:67)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:89)
	at dependency.HanLPDependency.main(HanLPDependency.java:10)
"
MutualInformationEntropyPhraseExtractor ÂÆûÁé∞Âª∫ËÆÆ,"ÊàëËßâÂæóÁü≠ËØ≠ÊèêÂèñÂ¢ûÂä†ËÆ≠ÁªÉËØ≠ÊñôÊõ¥ÂêàÈÄÇ„ÄÇÂ¶ÇÊûú‰ªÖÂá≠ËæìÂÖ•ÁöÑÊñáÊ°£ËøõË°åËÆ°ÁÆóÔºå‰∫í‰ø°ÊÅØÁªìÊûúËÇØÂÆö‰∏çÂáÜÔºàÂ∞§ÂÖ∂ÊòØËæìÂÖ•ÊØîËæÉÁü≠ÁöÑÊÉÖÂÜµÔºâ„ÄÇ
ÂÆûÁé∞Ëµ∑Êù•‰πüÁÆÄÂçï„ÄÇÂÖàÈÄöËøáËÆ≠ÁªÉËØ≠ÊñôËÆ°ÁÆóÂá∫Êúâ‰∫í‰ø°ÊÅØÁöÑOccurrenceÂØπË±°„ÄÇ
Âú®ÊèêÂèñÈò∂ÊÆµ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑOccurrenceÂØπË±°„ÄÇÂ¶ÇÊûúÊ≤°ÊúâËÆ≠ÁªÉÔºåÂ∞±‰ΩøÁî®ÂéüÁÆóÊ≥ï„ÄÇ"
Êó∂Èó¥ËØÜÂà´‰∏çÂáÜÁ°ÆÔºåCheckDateElements‰∏≠ÁöÑËßÑÂàôÂ≠òÂú®ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰ΩøÁî®NShortSegmentÂàÜËØçÂô®ËøõË°åÂàÜËØçÔºåÊó∂Èó¥ËØÜÂà´‰∏çÂáÜÁ°ÆÔºåÂ≠òÂú®‰ª•‰∏ãÈóÆÈ¢òÔºö

1. ‰ºöÂ∞Ü‰∏≠Ëã±ÊñáÈÄóÂè∑ËØÜÂà´‰∏∫/mÁ±ªÂûã
2. Â¶ÇÊûú‰∏≠Ëã±ÊñáÈÄóÂè∑ÂêéÁ¥ßË∑üÊï∞Â≠óÔºåÂàÜËØçÊó∂‰ºöÂ∞ÜÈÄóÂè∑ÂíåÊï∞Â≠óÂêàÂπ∂ËØÜÂà´‰∏∫/mÁ±ªÂûã
3. ‰πùÁÇπËøôÁßçÂèØ‰ª•ËØÜÂà´‰∏∫/tÁ±ªÂûãÔºå‰ΩÜ9ÁÇπËØØËØÜÂà´‰∏∫/mÁ±ªÂûã
4. 18:00ËøôÁßçÊ†ºÂºèÁöÑÊó∂Èó¥Ë°®Ëø∞Ôºå‰ºöË¢´ËØØËØÜÂà´‰∏∫/mÁ±ªÂûã

WordBasedGenerativeModelSegment‰∏≠ÁöÑCheckDateElementsÊñπÊ≥ïÔºåÁõ∏ÂÖ≥ËßÑÂàôÂª∫ËÆÆ‰øÆÊîπ
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        Segment segment = new NShortSegment();
        List<Term> list1 = segment.seg(""ÊÉ≥Âá∫ÂéªÁé©Ôºå9ÁÇπË¶Å‰∏äÁè≠Ôºå18:00‰∏ãÁè≠,9ÊàêÁöÑ‰∫∫‰ºöÂä†Áè≠„ÄÇ"");
        System.out.println(list1.toString());
        List<Term> list2 = segment.seg(""ÊÉ≥Âá∫ÂéªÁé©‰ΩÜ9ÁÇπË¶Å‰∏äÁè≠ÔºåÁÑ∂Âêé18:00‰∏ãÁè≠,9ÊàêÁöÑ‰∫∫‰ºöÂä†Áè≠„ÄÇ"");
        System.out.println(list2.toString());
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÊÉ≥/v, Âá∫Âéª/vf, Áé©/v, Ôºå/w, 9ÁÇπ/t, Ë¶Å/v, ‰∏äÁè≠/vi, Ôºå/w, 18:00/t, ‰∏ãÁè≠/vi, ,/w, 9Êàê/m, ÁöÑ/ude1, ‰∫∫/n, ‰ºö/v, Âä†Áè≠/vi, „ÄÇ/w]

[ÊÉ≥/v, Âá∫Âéª/vf, Áé©/v, ‰ΩÜ/c, 9ÁÇπ/t, Ë¶Å/v, ‰∏äÁè≠/vi, Ôºå/w, ÁÑ∂Âêé/c, 18:00/t, ‰∏ãÁè≠/vi, ,/w, 9Êàê/m, ÁöÑ/ude1, ‰∫∫/n, ‰ºö/v, Âä†Áè≠/vi, „ÄÇ/w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÊÉ≥/v, Âá∫Âéª/vf, Áé©/v, Ôºå9ÁÇπ/m, Ë¶Å/v, ‰∏äÁè≠/vi, Ôºå18:00/m, ‰∏ãÁè≠/vi, ,9Êàê/m, ÁöÑ/ude1, ‰∫∫/n, ‰ºö/v, Âä†Áè≠/vi, „ÄÇ/w]

[ÊÉ≥/v, Âá∫Âéª/vf, Áé©/v, ‰ΩÜ/c, 9ÁÇπ/m, Ë¶Å/v, ‰∏äÁè≠/vi, Ôºå/m, ÁÑ∂Âêé/c, 18:00/m, ‰∏ãÁè≠/vi, ,9Êàê/m, ÁöÑ/ude1, ‰∫∫/n, ‰ºö/v, Âä†Áè≠/vi, „ÄÇ/w]

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
1.51ÁâàÊú¨ÂàÜËØçÈóÆÈ¢ò,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
1.51portable

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.52
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.51-portable

## ÊàëÁöÑÈóÆÈ¢ò
ÂØπ‰∫é‚ÄúÊó†Ê≥ï‚ÄùËøô‰∏™ËØçÔºåÂ¶ÇÊûúÂçïÁã¨ÂàÜËØç‰ºöÂàÜÊàê‚ÄúÊó†‚Äù‚ÄúÊ≥ï‚ÄùÔºå‰ΩÜÊòØÊîæÂà∞Âè•Â≠ê‚ÄúÂΩìÂâçËÆ¢ÂçïÊó†Ê≥ïÊ¥æÈÄÅ‚ÄùÈáåÈù¢‰ºöÂàÜÊàê‚ÄúÊó†Ê≥ï‚ÄùÔºåÊúâÊ≤°ÊúâÊñπÊ≥ïËÆ©Ëøô‰∏§ÁßçÊÉÖÂÜµÂæóÂà∞Âêå‰∏Ä‰∏™ÂàÜËØçÁªìÊûú
"
È°πÁõÆÊâìÂåÖdataÁõÆÂΩïÊ≤°ÊâìËøõÂéª,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
1.mvn clean package -DskipTests ÊâìÂåÖÂêédataÁõÆÂΩïÂπ∂Ê≤°ÊúâË¢´ÊâìËøõÂéªÔºåËØ∑ÈóÆ‰ΩúËÄÖ‰Ω†‰ª¨ÊòØÊÄé‰πàÊâìÂåÖÁöÑÔºü
2.ÊàëÈÄöËøá‰øÆÊîπpom.xmlÊñá‰ª∂ÁöÑresourcesÈÖçÁΩÆËøôÁßçÊñπÂºèÂ∞ÜdataÁõÆÂΩïÊâìËøõÂéª‰∫ÜÔºå‰ΩÜÊòØÂú®‰ΩøÁî®ÁöÑÊó∂ÂÄô‰æùÁÑ∂Êä•Êñá‰ª∂Êâæ‰∏çÂà∞ÁöÑÈîôËØØ„ÄÇ

ËØ∑ÊåáÁÇπ‰∏ãÔºåÈùûÂ∏∏ÊÑüË∞¢ÔºÅ


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÊâßË°å mvn clean package -DskipTests 
2. ÁÑ∂ÂêéÊü•Áúãtarget‰∏ãÁöÑjarÂåÖÂÜÖÂÆπ Ê≤°ÊúâÂ∞Ü dataÊâìËøõÂéª

"
ËØ∑Êïô‰ΩúËÄÖÔºå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºåÂØπÂ§çÂêàÂè•Â≠êËß£Êûê‰∏çÂ§üÂêàÁêÜÔºåÊúâ‰ªÄ‰πàÊñπÊ≥ïÊîπËøõÔºü,"ÊØîÂ¶ÇÂØπËøô‰∏™Âè•Â≠êÔºö Â¶ÇÊûú‰∏Ä‰∏™ÁÆóÊ≥ïÊúâÁº∫Èô∑ÔºåÊàñ‰∏çÈÄÇÂêà‰∫éÊüê‰∏™ÈóÆÈ¢òÔºåÊâßË°åËøô‰∏™ÁÆóÊ≥ïÂ∞Ü‰∏ç‰ºöËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ
Êï¥‰ΩìËß£ÊûêÊÑüËßâÊúâÈóÆÈ¢òÔºåÂ¶Ç‰∏ãÔºö

1	Â¶ÇÊûú	Â¶ÇÊûú	c	c	_	4	ADV	_	_
2	‰∏Ä‰∏™	‰∏Ä‰∏™	m	mq	_	3	ATT	_	_
3	ÁÆóÊ≥ï	ÁÆóÊ≥ï	n	n	_	4	SBV	_	_
4	Êúâ	Êúâ	v	vyou	_	0	HED	_	_
5	Áº∫Èô∑	Áº∫Èô∑	n	n	_	4	VOB	_	_
6	,	,	wp	w	_	4	WP	_	_
7	Êàñ	Êàñ	c	c	_	9	LAD	_	_
8	‰∏ç	‰∏ç	d	d	_	9	ADV	_	_
9	ÈÄÇÂêà‰∫é	ÈÄÇÂêà‰∫é	v	v	_	4	COO	_	_
10	Êüê‰∏™	Êüê‰∏™	r	rz	_	11	ATT	_	_
11	ÈóÆÈ¢ò	ÈóÆÈ¢ò	n	n	_	9	VOB	_	_
12	,	,	wp	w	_	4	WP	_	_
13	ÊâßË°å	ÊâßË°å	v	v	_	4	COO	_	_
14	Ëøô‰∏™	Ëøô‰∏™	r	rz	_	15	ATT	_	_
15	ÁÆóÊ≥ï	ÁÆóÊ≥ï	n	n	_	13	VOB	_	_
16	Â∞Ü	Â∞Ü	d	d	_	18	ADV	_	_
17	‰∏ç‰ºö	‰∏ç‰ºö	v	v	_	18	ADV	_	_
18	Ëß£ÂÜ≥	Ëß£ÂÜ≥	v	v	_	13	COO	_	_
19	Ëøô‰∏™	Ëøô‰∏™	r	rz	_	20	ATT	_	_
20	ÈóÆÈ¢ò	ÈóÆÈ¢ò	n	n	_	18	VOB	_	_
21	.	.	wp	w	_	18	WP	_	_


ÊàëÊúüÊúõÁöÑ‰∏≠ÂøÉËØçÊòØ ‚ÄúËß£ÂÜ≥‚Äù „ÄÇ Á•ûÁªèÁΩëÁªúÊ®°ÂûãËøîÂõûÁöÑÊòØ‚ÄúÊúâ‚ÄúÔºåÊù°‰ª∂ÈöèÊú∫Âú∫Ê®°ÂûãËøîÂõûÁöÑÊòØ‚ÄúÈÄÇÂêà‰∫é‚Äù



"
CRFÂàÜËØçÊó∂ÔºåÊ†áÁÇπË¢´ÂàÜÊàênz,"ÁâàÊú¨1.3.3
ÊØîÂ¶ÇËøôÂè•ËØùÔºö
Â¶ÇÊûú‰∏Ä‰∏™ÁÆóÊ≥ïÊúâÁº∫Èô∑ÔºåÊàñ‰∏çÈÄÇÂêà‰∫éÊüê‰∏™ÈóÆÈ¢òÔºåÊâßË°åËøô‰∏™ÁÆóÊ≥ïÂ∞Ü‰∏ç‰ºöËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ
ÂàÜÊàêÔºö

[Â¶ÇÊûú/c, ‰∏Ä‰∏™/mq, ÁÆóÊ≥ï/n, ÊúâÁº∫Èô∑/nz, ,/nz, Êàñ/c, ‰∏ç/d, ÈÄÇÂêà‰∫é/v, Êüê‰∏™/rz, ÈóÆÈ¢ò/n, ,/nz, ÊâßË°å/v, Ëøô‰∏™/rz, ÁÆóÊ≥ï/n, Â∞Ü/d, ‰∏ç‰ºö/v, Ëß£ÂÜ≥/v, Ëøô‰∏™/rz, ÈóÆÈ¢ò/n, ./nz]"
nr.txt ‰∏≠ÁöÑÊï∞ÊçÆÊòØÊÄé‰πàÊù•ÁöÑ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Êâæ‰∏çÂà∞ hanlp.properties Êñá‰ª∂,release ‰∏≠
"ËæÉÂ∏∏ËßÅÂßìÂêçËØÜÂà´Âá∫ÈîôÔºöÊ±ü,Êàê‰∏é","<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
public class DemoChineseNameRecognition
{
.......
       String sentence = ""Â±±‰∏úÁúÅ‰∫§ÈÄöËøêËæìÂéÖÂÖöÁªÑ‰π¶ËÆ∞Ê±üÊàê‰∏éÂ§ßÂÆ∂ËøõË°å‰∫§ÊµÅ"",
       Segment segment = 
        HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
        List<Term> termList = segment.seg(sentence);
        System.out.println(termList);
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Â±±‰∏úÁúÅ/ns, ‰∫§ÈÄöËøêËæì/nz, ÂéÖ/n, ÂÖöÁªÑ‰π¶ËÆ∞/n, Ê±üÊàê/nr, ‰∏é/cc, Â§ßÂÆ∂/rr, ËøõË°å/vn, ‰∫§ÊµÅ/vn]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Â±±‰∏úÁúÅ/ns, ‰∫§ÈÄöËøêËæì/nz, ÂéÖ/n, ÂÖöÁªÑ‰π¶ËÆ∞/n, Ê±ü/n, Êàê‰∏é/nr, Â§ßÂÆ∂/rr, ËøõË°å/vn, ‰∫§ÊµÅ/vn]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÊãºÈü≥ÂàáËØç,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2



## ÊàëÁöÑÈóÆÈ¢ò

‰Ω†Â•ΩÔºåHanLPÊîØÊåÅËá™ÂÆö‰πâÊãºÈü≥ËØçÂ∫ìÂêóÔºüÂõ†‰∏∫ÊàëÊÉ≥‰ΩøÁî®ÊãºÈü≥Êù•ËøõË°åÂàáËØçÔºåÊàë‰ª¨ÊòØÈÄöËøáËØ≠Èü≥ËØÜÂà´ËΩ¨ÊñáÂ≠óÔºåËØÜÂà´ÁöÑÊó∂ÂÄôÂõûÊúâÂêåÈü≥ËØçÔºåÊâÄ‰ª•ÊÉ≥ÈÄöËøáÊãºÈü≥Êù•ÂàáËØç


   CustomDictionary.add(""dakai"");
		CustomDictionary.add(""yiloumenkou"");
		CustomDictionary.add(""deng"");
		List<Term> list = StandardTokenizer.segment(""dakaiyiloumenkoudeng"");
		System.out.println(list);
### ÊúüÊúõËæìÂá∫

dakai yiloumenkou  deng

### ÂÆûÈôÖËæìÂá∫ dakaiyiloumenkoudeng

"
lucene-core-7.0.1‰∏éhanlp1.52 hanlp-indexÂ≠òÂú®ÂàÜËØçÂÖºÂÆπÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.52
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.52

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÈÄöËøáhanlp-ext ÁºñËØëÊï¥ÂêàËøõelasticsearch6.0 ÂÅöÊñáÊ°£Á±ªÂûãÁöÑÂàÜËØçÊò†Â∞ÑÔºåÊò†Â∞ÑÁöÑÂàÜËØçÊòØhanlp-index
elasticsearch6.0 ÊñáÊ°£Êò†Â∞ÑÈÉ®ÂàÜ‰ª£Á†ÅÂ¶Ç‰∏ã
```
PUT test_2017_11_12 
{
    ""mappings"":
    {
        ""meal"":
        {
            ""properties"":
            {
                ""city"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }
                },
                ""name"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""address"":
                {
                    ""type"": ""text"",
                    ""analyzer"": ""hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 512
                        }
                    }
                },
                ""tags"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""price"":
                {
                    ""type"": ""float""
                },
                ""phone"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }

                },
                ""images"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""recommendations"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index""
                },
                ""facilities"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 128
                        }
                    }
                },
                ""uriSign"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""uri"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""createTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""updateTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""syncTime"": 
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                }
            }
        }
    }
}
```
ÈÄöËøágoogleÊêúÁ¥¢Áõ∏ÂÖ≥ËµÑÊñôÊòæÁ§∫ÊòØhanlp-index ÂàÜËØçÁöÑÊñπÂºè‰∏élucene-core-7.0.1Â≠òÂú®ÂÖºÂÆπÈóÆÈ¢ò
DefaultIndexingChain.java:767  lucene ÈªòËÆ§ÁöÑÁ¥¢Â§ÑÁêÜÈìæÊñá‰ª∂
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
ËøôÊòØÈíàÂØπelasticsearchÊñáÊ°£Êò†Â∞ÑÁöÑÂ≠óÊÆµ

Êä•ÈîôÁöÑJavaÈîôËØØÂ†ÜÊ†à‰ø°ÊÅØÂ¶Ç‰∏ã
```
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
	at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:767) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:239) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:481) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1717) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1462) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
```
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂª∫Á´ãelasticsearch ÊñáÊ°£Êò†Â∞Ñ
2. ÁÑ∂ÂêéÈÄöËøáelasticsearchÂ≠òÂÇ®ÊñáresetÊñáÊ°£Êé•Âè£Â≠òÂÇ®ÊñáÊ°£
3. Êé•ÁùÄelasticsearchÊó•ÂøóÊä•ÈîôÔºåÈîôËØØÂ†ÜÊ†àÂ∑≤ÁªèÂÜôÂú®ÊñáÊ°£‰∏äÔºåËßÅ‰∏äÊñá

### Ëß¶Âèë‰ª£Á†Å

```
 PUT test_2017_11_12/meal/5a098a920f3f766cdebf4767
{ 
    ""city"" : ""ÈïøÊ≤ô"", 
    ""name"" : ""ÁõüÈáçÁÉßÁÉ§"", 
    ""address"" : ""ÂÜ¨ÁìúÂ±±Ë£ïÂçóË°ó85Âè∑"", 
    ""tags"" : [
        ""‰π¶Èô¢Ë∑Ø"", 
        ""ÂÖ∂ÂÆÉ""
    ], 
    ""priceText"" : ""68/‰∫∫"", 
    ""price"" : 68, 
    ""phone"" : ""15116157770"", 
    ""images"" : [
        ""http://test.com/host/2017/07/30/1501392633001415.jpg-ssq75""
    ], 
    ""recommendations"" : [
        ""ÊòüÂüéÊúÄÊ≠£ÂÆóÁöÑÊπòË•øÂë≥ÁÉ§‰∏≤"", 
        ""‰∏ªÊâìÁâõÊ≤πÁî±ÊπòË•øÁõ¥Êé•‰æõË¥ß"", 
        ""Êó∂Â∞öÊΩÆ‰∫∫ËÅöÈõÜÁöÑÊ∑±Â§úÈ£üÂ†Ç""
    ], 
    ""facilities"" : [
        ""Wi-Fi"", 
        ""ÈÄÇÂêàÂ∞èËÅö""
    ], 
    ""uriSign"" : ""cd5fb379260756ae50ac9ceb1c2e2331"", 
    ""uri"" : ""http://test/hotel/203381"", 
    ""syncTime"" : 1510669453830, 
    ""createTime"" :1510584054604, 
    ""updateTime"" : 1510584054604
}

```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ≠ÊñôÊúâ‰∫ÜÔºåÂ¶Ç‰ΩïÈáçÊñ∞ËÆ≠ÁªÉCrfSegment  model,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÊÉ≥ÈáçÊñ∞ËÆ≠ÁªÉcrfÂàÜËØçÊ®°ÂûãÔºåÁõ∏ÂÖ≥wikiÈáåÊèêÁ§∫Áî® 
..\..\crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t
‰ΩÜÊòØ‰∏ç‰ºöÁî®Ôºå‰∏äÈù¢Ëøô‰∏™ËØ≠Âè•Âú®Âì™ÈáåÊâßË°åÔºå‰ªÄ‰πàÁéØÂ¢ÉÔºüÂ§öË∞¢ÔºÅÔºÅ

Âè¶Â§ñÔºåÁúã‰∫Ü‰∏Ä‰∫õissue‰πüÊúâÂêåÂ≠¶ÊèêÂá∫ÊØîÂ¶ÇÂ∞Ü‰ø°ÊÅØÁÜµ„ÄÅ‰∫í‰ø°ÊÅØÊàñtfidfÂä†ÂÖ•Âà∞ÁâπÂæÅ‰∏≠Ôºå‰πüÊúâËÆ∫Êñá‰ªãÁªçÂ∞ÜembeddingÁöÑÁâπÂæÅËûçÂÖ•Âà∞crfÊ®°Âûã‰∏≠ ÔºåÂ¶Ç Revisiting Embedding Features for Simple Semi-supervised Learning ÔºåÂ¶ÇÊûúË¶ÅÂä†ÂÖ•Ëøô‰∫õÁâπÂæÅÔºåËØ∑ÈóÆ‰øÆÊîπÂì™‰∫õÁ±ªÂèäÊñπÊ≥ïÂèØ‰ª•ÂÆûÁé∞ÔºåÂ§öË∞¢ÊåáÂØº ÔºÅÔºÅ
"
ËÆ§‰∏∫crfÈáåÁöÑËß£Á†ÅtagÊñπÊ≥ïÊúâËØØ„ÄÇÊàë‰øÆÊîπÂêéÁöÑËØ∑Áúã‰∏Ä‰∏ã„ÄÇ," /**
     * Áª¥ÁâπÊØîÂêéÂêëÁÆóÊ≥ïÊ†áÊ≥®
     *
     * @param table
     */
    public void tag(Table table)
    {
        int size = table.size();
        if (size == 0) return;
        int tagSize = id2tag.length;
        double[][] net = new double[size][tagSize];
        for (int i = 0; i < size; ++i)
        {
            LinkedList<double[]> scoreList = computeScoreList(table, i); //‰∏Ä‰∏™charÂØπÂ∫îÁöÑÁä∂ÊÄÅBMSEÁöÑÊ¶ÇÁéá„ÄÇdouble[]ÔºåÁâπÂæÅ‰∏™Êï∞ÔºÅ
            for (int tag = 0; tag < tagSize; ++tag)//4‰∏™Áä∂ÊÄÅÔºÅ
            {
                net[i][tag] = computeScore(scoreList, tag);
            }
        }

        if (size == 1)
        {
            double maxScore = -1e10;
            int bestTag = 0;
            for (int tag = 0; tag < net[0].length; ++tag)
            {
                if (net[0][tag] > maxScore)
                {
                    maxScore = net[0][tag];
                    bestTag = tag;
                }
            }
            table.setLast(0, id2tag[bestTag]);
            return;
        }

        //viterbiÁöÑÊ†∏ÂøÉÊ≠•È™§ÔºÅ
        //note: ÊîπËøõ‰∫ÜviterbiËß£Á†ÅÁöÑÁÆóÊ≥ï„ÄÇÂèëÁé∞ÂéüÊù•ÁöÑnetÁªìÊûÑ‰∏çÂ§üÊ∏ÖÊô∞„ÄÇ
        int[][] from = new int[size][tagSize];//path
        double pro[][] = new double[size][tagSize];
        for (int i = 1; i < size; ++i)
        {
            for (int now = 0; now < tagSize; ++now)
            {
                double maxScore = -1e10;
                for (int pre = 0; pre < tagSize; ++pre)
                {
                    double score = pro[i-1][pre] + matrix[pre][now] + net[i][now];//?net‰ª£Êõø‰∫ÜÂèëÂ∞ÑÊ¶ÇÁéáÔºåËÇØÂÆöÊòØ‰∏çÂáÜÁ°ÆÁöÑ„ÄÇ
                    if (score > maxScore)
                    {
                        maxScore = score;
                        from[i][now] = pre;
                        pro[i][now] = maxScore; 
                    }
                }
            }
        }
    	   
    	
     
    	
      double maxScore = -1e10;
      int maxTag = 0;
      for (int tag = 0; tag < net[size - 1].length; ++tag)
      {
          if (pro[size - 1][tag] > maxScore)
          {
              maxScore = pro[size - 1][tag];
              maxTag = tag;
          }
      }
    	table.setLast(size-1, id2tag[maxTag]);
    	for (int i = size - 1; i >=1 ; --i)
    	{
          table.setLast(i-1, id2tag[from[i][maxTag]]);
          maxTag = from[i][maxTag];
    	 }
    	
        // ÂèçÂêëÂõûÊ∫ØÊúÄ‰Ω≥Ë∑ØÂæÑ
//        double maxScore = -1e10;
//        int maxTag = 0;
//        for (int tag = 0; tag < net[size - 1].length; ++tag)
//        {
//            if (net[size - 1][tag] > maxScore)
//            {
//                maxScore = net[size - 1][tag];
//                maxTag = tag;
//            }
//        }
//
//        table.setLast(size - 1, id2tag[maxTag]);
//        maxTag = from[size - 1][maxTag];
//        for (int i = size - 2; i > 0; --i)
//        {
//            table.setLast(i, id2tag[maxTag]);
//            maxTag = from[i][maxTag];
//        }
//        table.setLast(0, id2tag[maxTag]);
    }"
Âü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÂè•Ê≥ïÂàÜÊûêÂô®ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
ÊúÄÊñ∞ÁâàÊú¨
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

‰Ω†Â•ΩÔºånndepparserÁöÑ‰ª£Á†ÅÊòØ‰∏çÊòØ‰∏çÂåÖÊã¨ËÆ≠ÁªÉÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÂë¢ÔºüÊàëÊ≤°ÊâæÂà∞Áõ∏Â∫îÁöÑÂáΩÊï∞ÔºåË∞¢Ë∞¢‰∫Ü



"
TextClassifcation ÁöÑ temp.data Â¶Ç‰ΩïÈáçË§á‰ΩøÁî®?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö master branch  https://github.com/hankcs/HanLP/commit/7d35473e4e26631e7362cb861d5a000594f71619
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster branch  https://github.com/hankcs/HanLP/commit/7d35473e4e26631e7362cb861d5a000594f71619
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

DemoTextClassifcationFMeasure.java
Ë£°Èù¢
FileDataSet ÊòØ‰ª• File.createTempFile Âª∫Á´ã TempFile to cache
Â¶Ç‰ΩïÈáçË§á‰ΩøÁî® TempFile ?



### Ëß¶Âèë‰ª£Á†Å

```
    public FileDataSet() throws IOException
    {
        this(File.createTempFile(String.valueOf(System.currentTimeMillis()), "".dat""));
    }
```
"
ÁπÅËΩ¨ÁÆÄ‰∏≠Âá∫Áé∞ÁöÑ‚ÄúÈô∑Èò±‚ÄùË¢´ÁøªËØë‰∏∫‚ÄúÁå´ËÖª‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÂá∫Áé∞ÁÆÄ‰Ωì‚ÄúÈô∑Èò±‚ÄùË¢´ÊõøÊç¢‰∏∫‚ÄúÁå´ËÖª‚Äù„ÄÇ

System.out.println(HanLP.convertToSimplifiedChinese(""Áì¶ÁàæÔºéÈñÉÈõªÈô∑Èò±""));
ËæìÂá∫ÔºöÁì¶Â∞îÔºéÈó™ÁîµÁå´ËÖª

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Ê≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÔºåËØçÂÖ∏ÂíåÊ®°Âûã

### Ê≠•È™§



### Ëß¶Âèë‰ª£Á†Å

```
  System.out.println(HanLP.convertToSimplifiedChinese(""Áì¶ÁàæÔºéÈñÉÈõªÈô∑Èò±""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Áì¶Â∞îÔºéÈó™ÁîµÈô∑Èò±
```


### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Áì¶Â∞îÔºéÈó™ÁîµÁå´ËÖª
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
word2vectorÂáÜÁ°ÆÁéáÊµãËØïÔºåË≤å‰ººÂíåCÊ≤°Êúâ‰ªÄ‰πàÂå∫Âà´,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

1. hanlp‰∏≠word2vectorÁöÑÂèÇÊï∞ÈÖçÁΩÆÈóÆÈ¢ò
2. ÂØπ‰∫éCÁâàÊú¨ÔºåÂáÜÁ°ÆÁéáÊØî‰Ω†ÁöÑÊµãËØïÁªìÊûú‰Ωé‰∫Ü10%
3. ÊàëÂØπword2vectorÁöÑÂêÑÊñπÁâàÊú¨ËøõË°å‰∫ÜÊµãËØïÔºåÂèëÁé∞ÂáÜÁ°ÆÁéáÂ∑ÆÂà´Âπ∂‰∏çÂ§ß

* ÂØπ‰∫é1ÔºåÊ∫êÁ†Å‰∏≠ÂèÇÊï∞Âè™Ë¶ÅÂèëÁé∞ÊúâcbowÂíåhsÔºåÂ∞±Áõ¥Êé•ËÆæ‰∏∫trueÔºåÊó†ÂÖ≥0‰∏é1ÁöÑÂÄºÔºåÊâÄ‰ª•ÂΩìÊµãËØï‰∫Ühs=0ÁöÑÊó∂ÂÄôÔºåÂÖ∂ÂÆûhanlp‰ΩøÁî®hsÔºåËÄåcÁâàÊú¨Ê≤°ÊúâÔºåÂú®[„Ääword2vecÂéüÁêÜÊé®ÂØº‰∏é‰ª£Á†ÅÂàÜÊûê„Äã](http://www.hankcs.com/nlp/word2vec.html)‰∏≠Â∞ΩÁÆ°ÂèÇÊï∞‰∏ÄÊ†∑Ôºå‰ΩÜÂÆûÈôÖËÆ≠ÁªÉËøáÁ®ã‰∏ç‰∏ÄÊ†∑Ôºå‰∏çÁü•ÈÅìËøôÊòØ‰∏çÊòØÈÄ†ÊàêÂáÜÁ°ÆÁéáÂ∑ÆÂà´ÊØîËæÉÂ§ßÁöÑÂéüÂõ†„ÄÇÊàëÂàÜÂà´ÊµãËØï‰∫ÜhanlpÂú®hs=1ÂíåÊ≤°ÊúâÊ∑ªÂä†hsËøô‰∏™ÂèÇÊï∞Êó∂ÁöÑÂáÜÁ°ÆÁéá„ÄÇ

* ÂØπ‰∫é2ÔºåÂØπ‰∫écÁâàÊú¨ÔºåÈááÁî®ÁöÑcËøõË°åËÆ≠ÁªÉÔºågensimËÆ°ÁÆóaccuracyÔºåÊàëÁúãËøáÊ∫êÁ†ÅÂíåË∑ëËøácÁöÑaccuracyÔºå‰∏§‰∏™ÁªìÊûú‰∏ÄËá¥ÔºåÊ≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜÊòØgensimÁöÑÊõ¥Âø´ÔºålogÊõ¥Ê∏ÖÊô∞ÔºåÂ∞±Ë∑ë‰∫ÜgensimÁöÑ„ÄÇ
ËøôÊòØÊµãËØïÁªìÊûúÔºöÊØî[„ÄäAccuracy rate seems to be 10% lower than the original version„Äã](https://github.com/kojisekig/word2vec-lucene/issues/21)‰∏≠ÁöÑc‰Ωé‰∫Ü10%Ôºå‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÔºü

./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 17:29:30,471 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,375 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,436 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 17:29:46,578 : INFO : capital-common-countries: 77.5% (392/506)
2017-11-28 17:30:15,301 : INFO : capital-world: 45.6% (1626/3564)
2017-11-28 17:30:20,082 : INFO : currency: 19.5% (116/596)
2017-11-28 17:30:38,799 : INFO : city-in-state: 41.2% (959/2330)
2017-11-28 17:30:42,157 : INFO : family: 61.7% (259/420)
2017-11-28 17:30:50,121 : INFO : gram1-adjective-to-adverb: 13.8% (137/992)
2017-11-28 17:30:56,214 : INFO : gram2-opposite: 13.1% (99/756)
2017-11-28 17:31:07,010 : INFO : gram3-comparative: 60.6% (807/1332)
2017-11-28 17:31:14,960 : INFO : gram4-superlative: 25.0% (248/992)
2017-11-28 17:31:23,447 : INFO : gram5-present-participle: 38.6% (408/1056)
2017-11-28 17:31:35,607 : INFO : gram6-nationality-adjective: 77.6% (1181/1521)
2017-11-28 17:31:48,147 : INFO : gram7-past-tense: 34.8% (543/1560)
2017-11-28 17:31:58,815 : INFO : gram8-plural: 49.5% (659/1332)
2017-11-28 17:32:05,812 : INFO : gram9-plural-verbs: 30.8% (268/870)
2017-11-28 17:32:05,812 : INFO : total: 43.2% (7702/17827)


* ÂØπ‰∫é3ÔºåÂàÜÂà´ÊµãËØï‰∫ÜgoogleÁöÑcÁâàÊú¨ÔºågensimÔºåhanlpÔºådeeplearning4j„ÄÇ Èô§‰∫Üdeeplearning4jÔºåÊ≤°ÊúâÊµãËØïhs=0ÁöÑÊÉÖÂÜµÔºåÂÖ∂‰ªñÈÉΩÊµãËØï‰∫Ü„ÄÇÁªüËÆ°ÂèëÁé∞‰ΩøÁî®hsÁöÑÂáÜÁ°ÆÁéáÊõ¥Â∑Æ‰∏Ä‰∏ãÔºåÁåúÊµãÊòØÊï∞ÊçÆËæÉÂ∞ëÔºåÂ§™Á®ÄÁñèÂØºËá¥ÁöÑ„ÄÇÂØπ‰∫éhs=0ÔºåÂêÑÂÆ∂Â§ßÊ¶Ç43%Ôºåhs=1ÔºåÂêÑÂÆ∂Â§ßÊ¶Ç35%„ÄÇ

:gensim
model = word2vec.Word2Vec(sentences, size=200, window=8, negative=25, hs=1, sample=0.0001, workers=8, iter=15)
2017-11-29 11:49:46,647 : INFO : loading projection weights from E:/data/word2vec/text8.gensim.word2vec.txt
2017-11-29 11:50:00,520 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.gensim.word2vec.txt
2017-11-29 11:50:00,599 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:50:04,786 : INFO : capital-common-countries: 76.5% (387/506)
2017-11-29 11:50:33,871 : INFO : capital-world: 37.9% (1349/3564)
2017-11-29 11:50:38,687 : INFO : currency: 7.0% (42/596)
2017-11-29 11:50:57,526 : INFO : city-in-state: 40.4% (942/2330)
2017-11-29 11:51:01,313 : INFO : family: 47.4% (199/420)
2017-11-29 11:51:09,776 : INFO : gram1-adjective-to-adverb: 10.8% (107/992)
2017-11-29 11:51:16,038 : INFO : gram2-opposite: 9.0% (68/756)
2017-11-29 11:51:26,976 : INFO : gram3-comparative: 51.4% (685/1332)
2017-11-29 11:51:34,859 : INFO : gram4-superlative: 19.8% (196/992)
2017-11-29 11:51:43,236 : INFO : gram5-present-participle: 25.5% (269/1056)
2017-11-29 11:51:55,519 : INFO : gram6-nationality-adjective: 73.0% (1111/1521)
2017-11-29 11:52:07,953 : INFO : gram7-past-tense: 35.5% (554/1560)
2017-11-29 11:52:18,648 : INFO : gram8-plural: 49.2% (655/1332)
2017-11-29 11:52:25,628 : INFO : gram9-plural-verbs: 21.8% (190/870)
2017-11-29 11:52:25,628 : INFO : total: 37.9% (6754/17827)

model = word2vec.Word2Vec(sentences, size=200, window=8, negative=25, hs=0, sample=0.0001, workers=8, iter=15)
2017-11-29 11:53:14,415 : INFO : loading projection weights from E:/data/word2vec/text8.gensim.word2vec.txt_1
2017-11-29 11:53:27,427 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.gensim.word2vec.txt_1
2017-11-29 11:53:27,505 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:53:31,894 : INFO : capital-common-countries: 72.9% (369/506)
2017-11-29 11:54:01,937 : INFO : capital-world: 51.1% (1822/3564)
2017-11-29 11:54:06,974 : INFO : currency: 18.0% (107/596)
2017-11-29 11:54:26,329 : INFO : city-in-state: 41.5% (966/2330)
2017-11-29 11:54:29,640 : INFO : family: 59.3% (249/420)
2017-11-29 11:54:37,565 : INFO : gram1-adjective-to-adverb: 14.3% (142/992)
2017-11-29 11:54:43,559 : INFO : gram2-opposite: 13.6% (103/756)
2017-11-29 11:54:54,144 : INFO : gram3-comparative: 64.3% (857/1332)
2017-11-29 11:55:02,068 : INFO : gram4-superlative: 23.1% (229/992)
2017-11-29 11:55:10,453 : INFO : gram5-present-participle: 36.0% (380/1056)
2017-11-29 11:55:22,509 : INFO : gram6-nationality-adjective: 73.7% (1121/1521)
2017-11-29 11:55:34,861 : INFO : gram7-past-tense: 34.3% (535/1560)
2017-11-29 11:55:45,290 : INFO : gram8-plural: 49.8% (664/1332)
2017-11-29 11:55:52,154 : INFO : gram9-plural-verbs: 31.5% (274/870)
2017-11-29 11:55:52,155 : INFO : total: 43.9% (7818/17827)


:hanlp
-input E:\data\word2vec\text8 -output E:\data\word2vec\text8.hanlp.word2vec.txt -size 200 -window 8 -negative 25 -hs 0 -cbow 1 -sample 1e-4 -threads 8 -binary 1 -iter 15
2017-11-28 16:53:03,293 : INFO : loading projection weights from E:/data/word2vec/text8.hanlp.word2vec.txt
2017-11-28 16:53:15,493 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.hanlp.word2vec.txt
2017-11-28 16:53:15,553 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:53:19,831 : INFO : capital-common-countries: 69.8% (353/506)
2017-11-28 16:53:49,194 : INFO : capital-world: 30.3% (1079/3564)
2017-11-28 16:53:54,053 : INFO : currency: 4.9% (29/596)
2017-11-28 16:54:12,895 : INFO : city-in-state: 35.7% (831/2330)
2017-11-28 16:54:16,322 : INFO : family: 31.9% (134/420)
2017-11-28 16:54:24,401 : INFO : gram1-adjective-to-adverb: 7.7% (76/992)
2017-11-28 16:54:30,487 : INFO : gram2-opposite: 9.9% (75/756)
2017-11-28 16:54:41,328 : INFO : gram3-comparative: 38.3% (510/1332)
2017-11-28 16:54:49,278 : INFO : gram4-superlative: 13.5% (134/992)
2017-11-28 16:54:58,219 : INFO : gram5-present-participle: 21.6% (228/1056)
2017-11-28 16:55:10,444 : INFO : gram6-nationality-adjective: 72.4% (1101/1521)
2017-11-28 16:55:22,950 : INFO : gram7-past-tense: 28.5% (445/1560)
2017-11-28 16:55:33,730 : INFO : gram8-plural: 45.9% (612/1332)
2017-11-28 16:55:40,694 : INFO : gram9-plural-verbs: 17.1% (149/870)
2017-11-28 16:55:40,696 : INFO : total: 32.3% (5756/17827)

-input E:\data\word2vec\text8 -output E:\data\word2vec\text8.hanlp.word2vec.txt_1 -size 200 -window 8 -negative 25 -cbow 1 -sample 1e-4 -threads 8 -binary 1 -iter 15
2017-11-29 11:15:27,628 : INFO : loading projection weights from E:/data/word2vec/text8.hanlp.word2vec.txt_1
2017-11-29 11:15:42,361 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.hanlp.word2vec.txt_1
2017-11-29 11:15:42,461 : INFO : precomputing L2-norms of word weight vectors
2017-11-29 11:15:47,365 : INFO : capital-common-countries: 80.0% (405/506)
2017-11-29 11:16:20,013 : INFO : capital-world: 46.2% (1647/3564)
2017-11-29 11:16:25,338 : INFO : currency: 14.4% (86/596)
2017-11-29 11:16:46,128 : INFO : city-in-state: 46.4% (1081/2330)
2017-11-29 11:16:49,861 : INFO : family: 53.1% (223/420)
2017-11-29 11:16:58,723 : INFO : gram1-adjective-to-adverb: 15.7% (156/992)
2017-11-29 11:17:05,424 : INFO : gram2-opposite: 9.9% (75/756)
2017-11-29 11:17:17,216 : INFO : gram3-comparative: 51.1% (680/1332)
2017-11-29 11:17:26,082 : INFO : gram4-superlative: 20.0% (198/992)
2017-11-29 11:17:35,536 : INFO : gram5-present-participle: 29.9% (316/1056)
2017-11-29 11:17:49,177 : INFO : gram6-nationality-adjective: 82.4% (1254/1521)
2017-11-29 11:18:03,059 : INFO : gram7-past-tense: 32.5% (507/1560)
2017-11-29 11:18:15,029 : INFO : gram8-plural: 53.7% (715/1332)
2017-11-29 11:18:22,894 : INFO : gram9-plural-verbs: 26.7% (232/870)
2017-11-29 11:18:22,894 : INFO : total: 42.5% (7575/17827)


:deeplearning4j
Word2Vec vec = new Word2Vec.Builder().layerSize(200).windowSize(8).negativeSample(25).minWordFrequency(5).useHierarchicSoftmax(true).sampling(0.0001).workers(8).iterations(15).epochs(15).iterate(iter)
                .elementsLearningAlgorithm(""org.deeplearning4j.models.embeddings.learning.impl.elements.CBOW"")
                .tokenizerFactory(t)
                .build();
2017-11-28 16:46:26,894 : INFO : loading projection weights from E:/data/word2vec/text8.deeplearning4j.word2vec.txt
2017-11-28 16:46:39,391 : INFO : loaded (71290L, 200L) matrix from E:/data/word2vec/text8.deeplearning4j.word2vec.txt
2017-11-28 16:46:39,453 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:46:43,596 : INFO : capital-common-countries: 67.4% (341/506)
2017-11-28 16:47:12,592 : INFO : capital-world: 33.9% (1208/3564)
2017-11-28 16:47:17,515 : INFO : currency: 6.0% (36/596)
2017-11-28 16:47:36,332 : INFO : city-in-state: 36.6% (852/2330)
2017-11-28 16:47:39,834 : INFO : family: 38.3% (161/420)
2017-11-28 16:47:47,898 : INFO : gram1-adjective-to-adverb: 9.0% (89/992)
2017-11-28 16:47:53,953 : INFO : gram2-opposite: 7.0% (53/756)
2017-11-28 16:48:04,632 : INFO : gram3-comparative: 38.7% (515/1332)
2017-11-28 16:48:12,653 : INFO : gram4-superlative: 11.8% (117/992)
2017-11-28 16:48:21,220 : INFO : gram5-present-participle: 23.0% (243/1056)
2017-11-28 16:48:33,519 : INFO : gram6-nationality-adjective: 76.7% (1166/1521)
2017-11-28 16:48:46,165 : INFO : gram7-past-tense: 27.2% (424/1560)
2017-11-28 16:48:56,894 : INFO : gram8-plural: 48.2% (642/1332)
2017-11-28 16:49:03,973 : INFO : gram9-plural-verbs: 19.2% (167/870)
2017-11-28 16:49:03,974 : INFO : total: 33.7% (6014/17827)


:google_c
./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 1 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 16:49:29,132 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt
2017-11-28 16:49:41,848 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt
2017-11-28 16:49:41,914 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 16:49:46,154 : INFO : capital-common-countries: 75.7% (383/506)
2017-11-28 16:50:15,078 : INFO : capital-world: 33.2% (1184/3564)
2017-11-28 16:50:19,993 : INFO : currency: 6.0% (36/596)
2017-11-28 16:50:38,967 : INFO : city-in-state: 36.0% (838/2330)
2017-11-28 16:50:42,348 : INFO : family: 47.4% (199/420)
2017-11-28 16:50:50,315 : INFO : gram1-adjective-to-adverb: 10.6% (105/992)
2017-11-28 16:50:56,355 : INFO : gram2-opposite: 7.8% (59/756)
2017-11-28 16:51:07,065 : INFO : gram3-comparative: 48.3% (644/1332)
2017-11-28 16:51:14,905 : INFO : gram4-superlative: 18.0% (179/992)
2017-11-28 16:51:23,299 : INFO : gram5-present-participle: 29.0% (306/1056)
2017-11-28 16:51:35,345 : INFO : gram6-nationality-adjective: 70.1% (1066/1521)
2017-11-28 16:51:47,733 : INFO : gram7-past-tense: 31.9% (498/1560)
2017-11-28 16:51:58,316 : INFO : gram8-plural: 50.1% (667/1332)
2017-11-28 16:52:05,321 : INFO : gram9-plural-verbs: 20.0% (174/870)
2017-11-28 16:52:05,322 : INFO : total: 35.6% (6338/17827)

./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 8 -binary 0 -iter 15
2017-11-28 17:29:30,471 : INFO : loading projection weights from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,375 : INFO : loaded (71291L, 200L) matrix from E:/data/word2vec/text8.google_c.word2vec.txt_1
2017-11-28 17:29:42,436 : INFO : precomputing L2-norms of word weight vectors
2017-11-28 17:29:46,578 : INFO : capital-common-countries: 77.5% (392/506)
2017-11-28 17:30:15,301 : INFO : capital-world: 45.6% (1626/3564)
2017-11-28 17:30:20,082 : INFO : currency: 19.5% (116/596)
2017-11-28 17:30:38,799 : INFO : city-in-state: 41.2% (959/2330)
2017-11-28 17:30:42,157 : INFO : family: 61.7% (259/420)
2017-11-28 17:30:50,121 : INFO : gram1-adjective-to-adverb: 13.8% (137/992)
2017-11-28 17:30:56,214 : INFO : gram2-opposite: 13.1% (99/756)
2017-11-28 17:31:07,010 : INFO : gram3-comparative: 60.6% (807/1332)
2017-11-28 17:31:14,960 : INFO : gram4-superlative: 25.0% (248/992)
2017-11-28 17:31:23,447 : INFO : gram5-present-participle: 38.6% (408/1056)
2017-11-28 17:31:35,607 : INFO : gram6-nationality-adjective: 77.6% (1181/1521)
2017-11-28 17:31:48,147 : INFO : gram7-past-tense: 34.8% (543/1560)
2017-11-28 17:31:58,815 : INFO : gram8-plural: 49.5% (659/1332)
2017-11-28 17:32:05,812 : INFO : gram9-plural-verbs: 30.8% (268/870)
2017-11-28 17:32:05,812 : INFO : total: 43.2% (7702/17827)
"
Python‰∏ãÊµãËØïNLPÂàÜËØçÂäüËÉΩÊó∂Âá∫Áé∞ÂºÇÂ∏∏‚Äújava.lang.ExceptionInInitializerError‚Äù,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®Python‰∏ãÊµãËØïNLPÂàÜËØçÊó∂Âá∫Áé∞ÂºÇÂ∏∏‚Äújava.lang.ExceptionInInitializerError‚ÄùÔºåÂÖ∂‰ªñÊñπÂºèÊµãËØïÊ≠£Â∏∏
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
print(NLPTokenizer.segment('‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄÁöÑÂÆóÊàêÂ∫ÜÊïôÊéàÊ≠£Âú®ÊïôÊéàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã'))




"
portableÁâàÊú¨ÊØîÂÆåÊï¥ÁâàÂ•Ω ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

‰ΩøÁî®ÈªòËÆ§Ê†áÂáÜÂàÜËØç, ÂèëÁé∞mavenÂºïÁî®portableÁâàÊú¨ÁöÑÂàÜËØçÊïàÊûúÊØî‰∏ãËΩΩ‰∏ãjarÂπ∂ÊåÇËΩΩ‰∫ÜÊï∞ÊçÆÂåÖÁöÑÂ•Ω

## Â§çÁé∞ÈóÆÈ¢ò

Âú®ÂêåÊ†∑ÁöÑËØ≠Êñô‰∏äËøõË°åÂàÜËØçÊµãËØï(sighan bakeoff05ÁöÑmsrÂíåpku)‰∏Ä‰∏ãÂ∞±Ë°å

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

Ëµ∑Á†ÅÂàÜËØçÁªìÊûú‰∏ÄËá¥Âêß? ÊàñËÄÖÊõ¥Êñ∞Êï∞ÊçÆÂåÖ

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂàÜËØçÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""‰øÆÊîπÁ®ø‰ª∂""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[‰øÆÊîπ/v, Á®ø‰ª∂/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[‰øÆÊîπÁ®ø/n, ‰ª∂/q]
```

"
Ê†áÁÇπÁ¨¶Âè∑,"ËØ∑ÈóÆÂ¶Ç‰ΩïÂÖ≥Èó≠ Ê†áÁÇπÁ¨¶Âè∑ÂèòÊõ¥Ôºü
ËæìÂÖ•ÁöÑÊòØ‰∏≠ÊñáÊ†áÁÇπÁ¨¶Âè∑ÔºåÂèòÊàê‰∫ÜËã±ÊñáÊ†áÁÇπÔºåËÄå‰∏îÊúâ‰∏Ä‰∫õÁâπÊÆäÁöÑ‰πüÂèò‰∫ÜÔºåÊØîÂ¶ÇÔºàÂèòÊàê‰∫Ü„ÄäÔºåÂπ∂‰∏îËã±ÊñáÊ†áÁÇπÁöÑËØçÊÄßÈÉΩÊòØm"
Word2VecËÆ≠ÁªÉ1GÁöÑËØ≠ÊñôÂá∫Áé∞java.lang.OutOfMemoryError: Java heap space,ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ê≤°ÊúâÂá∫Áé∞ÈóÆÈ¢òÔºå‰ΩÜËÆ≠ÁªÉÂà∞100%Âêé‰ºöÂèëÁîüËØ•ÈîôËØØÔºå‰∏çÊôìÂæóËÉΩ‰∏çËÉΩ‰ºòÂåñ‰∏Ä‰∏ãÔºå‰ΩøÂæóWord2VecËÉΩÊîØÊåÅÊõ¥Â§ßÁöÑËØ≠ÊñôÁöÑËÆ≠ÁªÉÔºàÂÖ∂ÂÆûÊÑüËßâ1GÂ∫îËØ•‰∏çÁÆóÂæàÂ§ßÔºâ
‰∫∫ÂêçËØÜÂà´ÂØºËá¥ÂàÜËØçÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

Âè•Â≠ê       Ôºö 
```
ÂïÜÊîπÂêéË¶Å‰π∞Âè¶Â§ñÁöÑ‰øùÈô©‰øùË¥πÊÄé‰πàÁÆó
```

ÂàÜËØçÁªìÊûúÔºö
```
[ÂïÜÊîπÂêé/nr, Ë¶Å‰π∞/nz, Âè¶Â§ñ/c, ÁöÑ/ude1, ‰øùÈô©/n, ‰øùË¥π/n, ÊÄé‰πà/ryv, ÁÆó/v]
```

ÂÖ∂‰∏≠ ÂïÜÊîπÂêé Ë¢´ËØÜÂà´‰∏∫‰∏Ä‰∏™‰∫∫Âêç

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(StandardTokenizer.segment(""ÂïÜÊîπÂêéË¶Å‰π∞Âè¶Â§ñÁöÑ‰øùÈô©‰øùË¥πÊÄé‰πàÁÆó""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÂïÜÊîπ/vÔºåÂêé/,  Ë¶Å‰π∞/nz, Âè¶Â§ñ/c, ÁöÑ/ude1, ‰øùÈô©/n, ‰øùË¥π/n, ÊÄé‰πà/ryv, ÁÆó/v]
```

### ÂÆûÈôÖËæìÂá∫

```
[ÂïÜÊîπÂêé/nr, Ë¶Å‰π∞/nz, Âè¶Â§ñ/c, ÁöÑ/ude1, ‰øùÈô©/n, ‰øùË¥π/n, ÊÄé‰πà/ryv, ÁÆó/v]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰∫∫ÂêçËØÜÂà´ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
‰∫∫ÂêçËØÜÂà´ÈîôËØØ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Êú™‰øÆÊîπ‰ªª‰ΩïÊ∫ê‰ª£Á†Å

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
//        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Èü©ÂõΩÂë¢""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Èü©ÂõΩ/nsf Âë¢/y
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Èü©, Èü©ÂõΩ]
2:[ÂõΩ]
3:[Âë¢]
4:[ ]

Á≤óÂàÜÁªìÊûú[Èü©ÂõΩ/nsf, Âë¢/y]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][Èü©ÂõΩ K 55 X 28 L 4 ][Âë¢ L 29 D 1 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,Èü©ÂõΩ/X ,Âë¢/D , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÈü©ÂõΩÂë¢ XD
ÁªÜÂàÜËØçÁΩëÔºö
0:[ ]
1:[Èü©ÂõΩ, Èü©ÂõΩÂë¢]
2:[]
3:[Âë¢]
4:[ ]

Èü©ÂõΩÂë¢/nr
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
N-ÊúÄÁü≠Ë∑ØÂàÜËØç ÂàÜËØçÁªìÊûúËØçÊÄßÊ†áÊ≥®ÈîôËØØ,"## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂæÖÂàÜËØçÁöÑÊñáÊú¨ÂÜÖÂÆπÔºö
""‰ªäÂ§©ÔºåÂàòÂøóÂÜõÊ°àÁöÑÂÖ≥ÈîÆ‰∫∫Áâ©,Â±±Ë•øÂ•≥ÂïÜ‰∫∫‰∏Å‰π¶ËãóÂú®Â∏Ç‰∫å‰∏≠Èô¢Âá∫Â∫≠ÂèóÂÆ°„ÄÇ""
ÂàÜËØçÁªìÊûúÔºö
[‰ªäÂ§©/t, Ôºå/m, ÂàòÂøóÂÜõ/nr, Ê°à/ng, ÁöÑ/ude1, ÂÖ≥ÈîÆ‰∫∫Áâ©/nz, ,/m, Â±±Ë•ø/ns, Â•≥/b, ÂïÜ‰∫∫/nnt, ‰∏Å‰π¶Ëãó/nr, Âú®/p, Â∏Ç‰∫å‰∏≠/n, Èô¢/n, Âá∫Â∫≠/vi, ÂèóÂÆ°/vi, „ÄÇ/w]
ÈóÆÈ¢òÔºö
‰∏≠ÊñáÈÄóÂè∑ÂíåËã±ÊñáÈÄóÂè∑ÁöÑËØçÊÄßÊ†áÊ≥®ÈîôËØØÔºåÂ∫îËØ•‰∏∫wËØçÊÄß

## Â§çÁé∞ÈóÆÈ¢ò
Êú™ÂÅöÂÖ∂ÂÆÉÊîπÂä®ÔºåÁõ¥Êé•‰ΩøÁî®È°πÁõÆREADME.md‰∏≠ÁöÑN-ÊúÄÁü≠Ë∑ØÂàÜËØçÁöÑdemoÊµãËØï


### Ëß¶Âèë‰ª£Á†Å

```
	public static void main(String[] args) {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		System.out.println(nShortSegment.seg(""‰ªäÂ§©ÔºåÂàòÂøóÂÜõÊ°àÁöÑÂÖ≥ÈîÆ‰∫∫Áâ©,Â±±Ë•øÂ•≥ÂïÜ‰∫∫‰∏Å‰π¶ËãóÂú®Â∏Ç‰∫å‰∏≠Èô¢Âá∫Â∫≠ÂèóÂÆ°„ÄÇ""));
	}
```


"
1.3.4ÁâàÊú¨ËøêË°åÂ§ßÊâπÈáèËØ≠ÊñôÂàÜËØçÂêÉÂÜÖÂ≠ò‰∏•Èáç,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
È¶ñÂÖàÊÑüËßâhanlpÂæàÈù†Ë∞±ÔºåËßâÂæóÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Â•ΩÁöÑNLPÂºÄÊ∫êÈ°πÁõÆÔºåÊàëÊãøÊù•ÂØπËØ≠ÊñôÂ∫ìÔºàÂ§ßÁ∫¶8KWÁöÑÊï∞ÊçÆÈõÜÔºâÂÅöÂàÜËØçÔºåÂèëÁé∞ÈùûÂ∏∏ÂêÉÂÜÖÂ≠òÔºåËøôÂùóÊàëËá™ÂÆö‰πâÁöÑËØçÂÖ∏ÈáåÈù¢Âä†ÂÖ•‰∫ÜÂ§ßÁ∫¶120WÂ∑¶Âè≥ÁöÑÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏„ÄÇÈöèÁùÄÁ®ãÂ∫èÁöÑËøêË°åÔºåÂÜÖÂ≠ò‰ΩøÁî®ÈáèÊîÄÈ´òÔºå‰ΩÜÊòØCPU‰ΩøÁî®ÁéáÂü∫Êú¨Ê≤°Êúâ‰ªÄ‰πàÂèòÂåñ„ÄÇËÄå‰∏îÈöèÁùÄÊó∂Èó¥ÁöÑÂ¢ûÈïøÔºåÂÜÖÂ≠ò‰ΩøÁî®ÈáèË∂äÊù•Ë∂äÂ§ßÔºåÈÄºËøëÊï¥‰∏™ÁîµËÑëÁöÑÂ≥∞ÂÄºÔºåÊúÄÂêé‰∏çÂæó‰∏çÁªàÊ≠¢„ÄÇËØ∑ÈóÆÊ•º‰∏ªÂú®ÊµãËØïÂ§ßËØ≠ÊñôÂàÜËØçÁöÑÊó∂ÂÄôÔºåÊòØÂê¶ÊúâËøôÁßçÊÉÖÂÜµÂèëÁîüÔºüË∞¢Ë∞¢

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÊàëÊääËá™Â∑±Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ÂÖ•Âà∞myDictionary.txt‰∏≠ÔºåÂ§ßÁ∫¶120WÂ∑¶Âè≥
2. ÁÑ∂ÂêéÊàëÂÜô‰∫Ü‰∏Ä‰∏™thriftÊúçÂä°ÔºåÊúçÂä°Á´ØÈááÁî®CRFSegment‰∏éHanlp.newSegment()‰∏§ÁßçÊñπÂºèÊµãËØïÔºåÈÉΩÂèëÁé∞ÂêÉÂÜÖÂ≠òÊØîËæÉ‰∏•Èáç„ÄÇÈöèÁùÄÊó∂Èó¥ÁöÑÂ¢ûÈïøÔºåÂÜÖÂ≠ò‰ΩøÁî®Ë∂äÊù•Ë∂äÂ§ö„ÄÇÂΩìÁ®ãÂ∫èÂ§ÑÁêÜ‰∫ÜÂ§ßÁ∫¶1000‰∏áÁöÑÊï∞ÊçÆÁöÑÊó∂ÂÄôÔºåÂèëÁé∞ÊàëÁöÑÁîµËÑëÊâõ‰∏ç‰Ωè‰∫ÜÔºåÊàëÁöÑÁîµËÑëÊòØ8GÁöÑÂÜÖÂ≠òÔºåÂèëÁé∞ÂÜÖÂ≠òÂü∫Êú¨‰∏äË¢´Âç†Êª°‰∫Ü„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Â¶Ç‰ΩïÂú®Á®ãÂ∫è‰∏≠ÈáçÊñ∞Âä†ËΩΩËá™ÂÆö‰πâÁöÑËØçÂÖ∏,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.3.2-portable.jar  „ÄÅ hanlp-lucene-plugin-1.1.2.jar


## ÂÆûÈôÖÂú∫ÊôØ
‰∏Ä‰∏™webÂÆπÂô®‰∏≠Êúâ‰∏§‰∏™webappÔºå‰∏Ä‰∏™ÊòØwebappAÔºåÂè¶‰∏Ä‰∏™ÊòØsolrÔºåsolr‰ΩøÁî®hanlp‰∏∫‰∏≠ÊñáÂàÜËØçÂô®ÔºåÂπ∂ÈÖçÁΩÆ‰∫ÜÁî®Êà∑ËØçÂÖ∏ÔºàÂç≥customDictionaryPathÂ±ûÊÄßÔºâÔºåwebappAÊúâ‰∏Ä‰∏™Âú®Á∫øÁºñËæëËØçÂÖ∏ÁöÑÂäüËÉΩÔºåÂ∏åÊúõÁºñËæëÂÆåÂ≠óÂÖ∏ÔºåsolrËÉΩÂ§üÁúãÂà∞ÊïàÊûúËÄå‰∏çÈúÄË¶ÅÈáçÂêØtomcatÂÆπÂô®„ÄÇ

## Ëß£ÂÜ≥ÊÄùË∑Ø
Âú®hanlp solrÊèí‰ª∂‰∏≠ÁöÑHanLPTokenizerFactoryÂºÄÂêØ‰∏Ä‰∏™ÂÆàÊä§Á∫øÁ®ãÔºåÊØèÈöî‰∏ÄÊÆµÊó∂Èó¥ÂéªÊ£ÄÊü•Â≠óÂÖ∏ÁöÑÊ†°Ê£ÄÁ†ÅÔºåÂ¶ÇÊûúÂèëÁîüÂèòÂåñÂ∞±Âà†Êéâ.binÁºìÂ≠òÊñá‰ª∂ÔºåÂπ∂ÈáçÊñ∞Âä†ËΩΩÂ≠óÂÖ∏„ÄÇ

## ÊàëÁöÑÈóÆÈ¢ò
1„ÄÅÊàëÁõÆÂâçÂú®CustomDictionaryÊ∑ªÂä†‰∫ÜÂ¶Ç‰∏ã‰∏Ä‰∏™ÈùôÊÄÅÊñπÊ≥ïÔºå‰ΩÜÊòØËøôÊ†∑‰ºöÊääÊâÄÊúâÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÈáçÊñ∞Âä†ËΩΩ‰∏ÄÈÅçÔºåÊúâÊ≤°ÊúâÂè™Âä†ËΩΩÊüê‰∏™Êñá‰ª∂ÁöÑÊñπÊ≥ïÂë¢
```
  public static void reloadDic(){
    	trie = null;
    	dat = new DoubleArrayTrie<CoreDictionary.Attribute>();
    	loadMainDictionary(path[0]);
    }
```
2„ÄÅÊâßË°åCustomDictionary.insert()ÊñπÊ≥ïÂêéÔºå‰∏∫‰ªÄ‰πàÊñ∞ËØçÂÖ∏Â∑≤Áªè‰∫ßÁîüÊïàÊûúÔºå‰ΩÜdat.size()Ê≤°ÊúâÂèëÁîüÂèòÂåñ
"
P2PÂíåC2CËøôÁßçËØçÊ≤°ÊúâÂàÜÂá∫Êù•ÔºåÂ∏åÊúõÂä†Âà∞‰∏ªËØçÂ∫ì,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂ∏åÊúõ‚ÄúP2P‚ÄùÂíå‚ÄúC2C‚ÄùËøô‰∫õËØçËÉΩÂ§üÂàÜÊàê‰∏Ä‰∏™ËØçÔºå‰ΩÜÊòØÁé∞Âú®ÊòØÂàÜÊàê‰∫ÜÂ§ö‰∏™„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò


### Ê≠•È™§


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
        List<Term> terms=segment.seg(""P2P C2C"");
        System.out.println(terms);
    }
```
### ÊúüÊúõËæìÂá∫
P2P/n  C2C/n


### ÂÆûÈôÖËæìÂá∫
[P/nx, 2/m, P/nx,  /w, C/nx, 2/m, C/nx]

## ÂÖ∂‰ªñ‰ø°ÊÅØ


"
HanLPÁöÑ‰∫åÂÖÉÊñáÊ≥ïËØçÂÖ∏Â¶Ç‰Ωï‰ΩøÁî®?,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÊàëÂú®data\dictionary\CoreNatureDictionary.ngram.txtÊñá‰ª∂‰∏≠ÂèëÁé∞‰∫Ü""ÁêêÁ¢é@‰∫ãÊÉÖ 2"",  'ÁêêÁ¢é'Âíå'‰∫ãÊÉÖ' ÊòØÂêàÁêÜÁöÑÊé•Áª≠.Êàë‰∏çÁü•ÈÅìËøêË°å‰πãÂêéÊòØÂá∫Áé∞‰ªÄ‰πàÊ†∑ÁöÑÊïàÊûú
### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
       String rawText = ""ÁêêÁ¢éÁöÑ‰∫ãÊÉÖ"";
	List<Term> termList = StandardTokenizer.segment(rawText);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

[ÁêêÁ¢é‰∫ãÊÉÖ]
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

[ÁêêÁ¢é, ÁöÑ, ‰∫ãÊÉÖ]

## ÂÖ∂‰ªñ‰ø°ÊÅØ
‰∏çÁü•ÈÅìÊàëËøôÊ†∑ÁöÑÊÉ≥Ê≥ïÊòØ‰∏çÊòØÈîôËØØÁöÑ.ËøòÊòØÊàëÂì™ÈáåÊìç‰ΩúÊúâÈóÆÈ¢ò
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑ÈóÆÂàÜËØçÁöÑÊó∂ÂÄôËÉΩËØÜÂà´Êó∂Èó¥ÂêóÔºüÂ¶Ç11Êúà22Êó•,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçÊÄß + Ëá™ÂÆö‰πâËØçÊù°Ôºå‰∏é portable Ëá™Â∏¶ËØçÂÖ∏ÂÜ≤Á™Å,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑ 
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.2
Â§áÊ≥®ÔºöwebÈ°πÁõÆÔºåÂü∫‰∫é springboot 1.5.8.RELEASEÔºådocker ÈÉ®ÁΩ≤

## ‰ª£Á†ÅÂ§çÁé∞
ÊàëÊÉ≥Áî® HanLP ÂåπÈÖçËØ≠Âè•‰∏≠ÁöÑÂõΩÂÆ∂ÂüéÂ∏Ç‰ø°ÊÅØÔºåÂõΩÂÆ∂ÂüéÂ∏ÇÂêçÂ≠óÊòØËßÑÂÆöÂ•ΩÁöÑÔºåÂú®Â∫ìÈáåÊúâ‰∏ÄÂ•óÔºåÂú® app ÂêØÂä®Êó∂ÔºåÂ∞ÜÂ∫ìÈáåÁöÑÂõΩÂÆ∂ÂüéÂ∏ÇÊï∞ÊçÆÂà∑ÂÖ• CustomDictionary ‰∏≠Ôºå‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
```java
/**
   * Â∞ÜÂõΩÂÆ∂ÂüéÂ∏ÇÂêçÊ∑ªÂä†Âà∞Áî®Êà∑ËØçÂÖ∏
   */
  @Override
  public void addCountryAndCityNameIntoDict() {
    //Ê∑ªÂä†ÂõΩÂÆ∂
    List<Country> countries = countryRepository.findAll();
    for (Country country : countries) {
      boolean countryInvalid = country.getId() == null || StringUtils.isEmpty(country.getCnName())
              || StringUtils.isEmpty(country.getEnName());
      if (countryInvalid) {
        continue;
      }
      CustomDictionary.insert(country.getCnName(), ""country 1000000"");
      CustomDictionary.insert(country.getEnName().toLowerCase(), ""country 1000000"");
      //Ê†πÊçÆÂõΩÂÆ∂ÔºåÊ∑ªÂä†ÂüéÂ∏Ç
      List<City> cities = cityRepository.findByCountryId(country.getId());
      for (City city : cities) {
        boolean cityInvalid = city.getId() == null || StringUtils.isEmpty(city.getCnName())
                || StringUtils.isEmpty(city.getEnName());
        if (cityInvalid) {
          continue;
        }
        CustomDictionary.insert(city.getCnName(), ""city 1000000"");
        CustomDictionary.insert(city.getEnName().toLowerCase(), ""city 1000000"");
      }
    }
  }
```
ÂüéÂ∏ÇÂêçÁß∞Á§∫‰æãÔºö
```
---------------------
  cn_name   en_name  
---------------------
  ‰∏äÊµ∑       Shanghai 
  Âåó‰∫¨       Beijing     
  Êù≠Â∑û       Hangzhou 
  Âè∞Âåó       Taibei    
  Êµ∑Âçó       Kainan    
  ÈáçÂ∫Ü       Zhongqing
---------------------
```
ÁÑ∂ÂêéÊàëËØï‰∫ÜÂá†‰∏™ÂàÜËØçÂô®ÔºåÂàÜËØçÁªìÊûúÈÉΩÊòØ‰ª• portable ÁâàÊú¨Ëá™Â∏¶ËØçÂÖ∏ÁöÑËØçËØ≠‰∏∫‰∏ªÔºö
```java
@GetMapping(""/test"")
  public Result testHanLP(@RequestParam String query) {
    //ËΩ¨Êç¢Ê†ºÂºè
    query = HanLP.convertToSimplifiedChinese(query.toLowerCase());

    Map<String, String> data = new LinkedHashMap<>(7);

    Segment standardTokenizer = StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""Ê†áÂáÜÂàÜËØç"", standardTokenizer.seg(query).toString());

    data.put(""NLPÂàÜËØç"", NLPTokenizer.segment(query).toString());

    Segment indexTokenizer = IndexTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""Á¥¢ÂºïÂàÜËØç"", indexTokenizer.seg(query).toString());

    Segment nShortSegment = new NShortSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""N-ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØç"", nShortSegment.seg(query).toString());

    Segment shortSegment = new DijkstraSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØç"", shortSegment.seg(query).toString());

    return Results.SUCC.data(data);
  }
```
ÊµãËØïËØ≠Âè•ÔºöÊµ∑ÂçóÁúÅÂåó‰∫¨Â∏Ç‰∏äÊµ∑Â∏ÇÈáçÂ∫ÜÂ∏ÇÂè∞ÊπæÂ≤õÂè∞ÊπæÁúÅÂè∞ÂåóÂ∏ÇÂåó‰∫¨ÊòØÈ¶ñÈÉΩÊµ∑ÂçóÊòØÊóÖÊ∏∏ËÉúÂú∞
ÂàÜËØçÊïàÊûúÔºö
```json
{
  ""code"": 0,
  ""msg"": ""ÊàêÂäü"",
  ""data"": {
    ""Ê†áÂáÜÂàÜËØç"": ""[Êµ∑ÂçóÁúÅ/ns, Âåó‰∫¨Â∏Ç/ns, ‰∏äÊµ∑Â∏Ç/ns, ÈáçÂ∫ÜÂ∏Ç/ns, Âè∞ÊπæÂ≤õ/nz, Âè∞ÊπæÁúÅ/ns, Âè∞ÂåóÂ∏Ç/ns, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]"",
    ""NLPÂàÜËØç"": ""[Êµ∑ÂçóÁúÅ/ns, Âåó‰∫¨Â∏Ç/ns, ‰∏äÊµ∑Â∏Ç/ns, ÈáçÂ∫ÜÂ∏Ç/ns, Âè∞ÊπæÂ≤õ/nz, Âè∞ÊπæÁúÅ/ns, Âè∞ÂåóÂ∏Ç/ns, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]"",
    ""Á¥¢ÂºïÂàÜËØç"": ""[Êµ∑ÂçóÁúÅ/ns, Êµ∑Âçó/ns, Âåó‰∫¨Â∏Ç/ns, Âåó‰∫¨/ns, ‰∏äÊµ∑Â∏Ç/ns, ‰∏äÊµ∑/ns, ÈáçÂ∫ÜÂ∏Ç/ns, ÈáçÂ∫Ü/ns, Âè∞ÊπæÂ≤õ/nz, Âè∞Êπæ/ns, Âè∞ÊπæÁúÅ/ns, Âè∞Êπæ/ns, Âè∞ÂåóÂ∏Ç/ns, Âè∞Âåó/ns, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]"",
    ""N-ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØç"": ""[Êµ∑ÂçóÁúÅ/ns, Âåó‰∫¨Â∏Ç/ns, ‰∏äÊµ∑Â∏Ç/ns, ÈáçÂ∫ÜÂ∏Ç/ns, Âè∞ÊπæÂ≤õ/nz, Âè∞ÊπæÁúÅ/ns, Âè∞ÂåóÂ∏Ç/ns, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]"",
    ""ÊúÄÁü≠Ë∑ØÂæÑÂàÜËØç"": ""[Êµ∑ÂçóÁúÅ/ns, Âåó‰∫¨Â∏Ç/ns, ‰∏äÊµ∑Â∏Ç/ns, ÈáçÂ∫ÜÂ∏Ç/ns, Âè∞ÊπæÂ≤õ/nz, Âè∞ÊπæÁúÅ/ns, Âè∞ÂåóÂ∏Ç/ns, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]""
  }
}
```

### ÊúüÊúõËæìÂá∫
ÁõÆÂâçÊù•ÁúãÔºåÁ¥¢ÂºïÂàÜËØçËøòÁÆóÂáëÂêàÔºåÊàëÊúüÊúõÁöÑÊòØÔºö
```
[Êµ∑ÂçóÁúÅ/ns, Êµ∑Âçó/city, Âåó‰∫¨Â∏Ç/ns, Âåó‰∫¨/city, ‰∏äÊµ∑Â∏Ç/ns, ‰∏äÊµ∑/city, ÈáçÂ∫ÜÂ∏Ç/ns, ÈáçÂ∫Ü/city, Âè∞ÊπæÂ≤õ/nz, Âè∞Êπæ/city, Âè∞ÊπæÁúÅ/ns, Âè∞Êπæ/city, Âè∞ÂåóÂ∏Ç/ns, Âè∞Âåó/city, Âåó‰∫¨/city, ÊòØ/v, È¶ñÈÉΩ/n, Êµ∑Âçó/city, ÊòØ/v, ÊóÖÊ∏∏/vn, ËÉúÂú∞/n]
```

### ÊèêÈóÆ
- ÊàëËØ•Â¶Ç‰ΩïÂú®Áî®ÂÆòÊñπ portable jar ÂåÖÁöÑÂâçÊèê‰∏ãÔºåÂæóÂà∞È¢ÑÊúüÁöÑËæìÂá∫ÔºüÊØïÁ´üËá™Â∑±ÊâìÂåÖÔºåÁÑ∂ÂêéÊîæÂú®È°πÁõÆ‰∏ãÔºå‰∏çÂ§™‰ºòÈõÖÔºàÊàëÁé∞Âú®Ê≤°ÊúâËá™Â∑±ÁöÑ maven ‰ªìÂ∫ìÔºåÂ¶ÇÊûúËá™Â∑±ÊâìÂåÖÁöÑËØùÔºåÂè™ËÉΩÊîæÂú®È°πÁõÆ‰∏ã‰∫ÜÔºâ„ÄÇ
- Âú®‰ΩøÁî®ÂÆòÊñπ portable jar ÂåÖÊó∂ÔºåËÉΩÂê¶Êèê‰æõ‰∏Ä‰∏™Á¶ÅÁî®ÂÖ∂Ëá™Â∏¶ÁöÑÊüê‰∫õËØçÂÖ∏ÁöÑÊñπÊ≥ïÔºüÊÑüËßâËøôÊòØ‰∏ÄÁßçËß£ÂÜ≥ÊÄùË∑Ø„ÄÇ
- ÊàëÊúâËÄÉËôëÂÄüÂä© hanlp.properties Âíå Ëá™ÂÆö‰πâ data ÁõÆÂΩïÔºå‰ΩÜÊòØËøôÊ†∑ÂØπ‰∫é‰∏Ä‰∏™ web È°πÁõÆÊù•ËØ¥ÔºåÈÉ®ÁΩ≤Êó∂Â∞±‰∏çÂ§™Êñπ‰æø„ÄÇ
"
Â∏∏ËßÅÂêçËØçÂíåËøûËØç‚ÄúÂíå‚ÄùË¢´ÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÂêçËØç‰∫Ü,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
HanLP.Config.enableDebug();
Segment segment = new DijkstraSegment();
List<Term> termList = segment.seg(""ÂºÄÂ±ïÂÖ¨ÂÖ±ËµÑÊ∫ê‰∫§ÊòìÊ¥ªÂä®ÁõëÁù£Ê£ÄÊü•Âíå‰∏æÊä•ÊäïËØâÂ§ÑÁêÜ"");
System.out.println(termList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÂºÄÂ±ï/v, ÂÖ¨ÂÖ±ËµÑÊ∫ê/gi, ‰∫§Êòì/vn, Ê¥ªÂä®/vn, ÁõëÁù£/vn, Ê£ÄÊü•/n,Âíå/cc, ‰∏æÊä•/vn, ÊäïËØâ/vn, Â§ÑÁêÜ/vn]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Á≤óÂàÜÁªìÊûú[ÂºÄÂ±ï/v, ÂÖ¨ÂÖ±ËµÑÊ∫ê/gi, ‰∫§Êòì/vn, Ê¥ªÂä®/vn, ÁõëÁù£/vn, Ê£ÄÊü•Âíå/n, ‰∏æÊä•/vn, ÊäïËØâ/vn, Â§ÑÁêÜ/vn]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  K 1 A 1 ][ÂºÄÂ±ï L 11 K 4 ][ÂÖ¨ÂÖ±ËµÑÊ∫ê A 20833310 ][‰∫§Êòì L 2 ][Ê¥ªÂä® L 13 K 3 ][ÁõëÁù£ A 20833310 ][Ê£ÄÊü•Âíå A 20833310 ][‰∏æÊä• K 100 L 33 M 22 ][ÊäïËØâ K 9 L 3 ][Â§ÑÁêÜ L 27 K 12 ][  K 1 A 1 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /K ,ÂºÄÂ±ï/L ,ÂÖ¨ÂÖ±ËµÑÊ∫ê/A ,‰∫§Êòì/L ,Ê¥ªÂä®/L ,ÁõëÁù£/A ,Ê£ÄÊü•Âíå/A ,‰∏æÊä•/K ,ÊäïËØâ/K ,Â§ÑÁêÜ/L , /A]

[ÂºÄÂ±ï/v, ÂÖ¨ÂÖ±ËµÑÊ∫ê/gi, ‰∫§Êòì/vn, Ê¥ªÂä®/vn, ÁõëÁù£/vn, Ê£ÄÊü•Âíå/n, ‰∏æÊä•/vn, ÊäïËØâ/vn, Â§ÑÁêÜ/vn]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
debugÈÉ®ÂàÜËæìÂá∫Ôºö
to: 21, from: 19, weight:11.60, word:Áù£@Ê£ÄÊü•
to: 22, from: 20, weight:11.24, word:Ê£Ä@Êü•
to: 23, from: 21, weight:03.15, word:Ê£ÄÊü•@Âíå
to: 23, from: 22, weight:04.03, word:Êü•@Âíå
to: 24, from: 23, weight:07.47, word:Âíå@‰∏æ
"
Â∏∏ËßÅÂÆòËÅåËØÜÂà´Âá∫Èîô,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Â∏∏ËßÅÂÆòËÅåÂ¶Ç‚ÄúÂ§ñ‰∫§ÈÉ®Èïø‚ÄùËØÜÂà´Âá∫Èîô
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
List<Term> termList = segment.seg(""ÂõΩÈò≤ÈÉ®ÈïøÁéãÊØÖÂêëËÆ∞ËÄÖ‰ªãÁªçÊ≠§ËÆøÊÉÖÂÜµ„ÄÇ"");
System.out.println(termList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÂõΩÈò≤ÈÉ®Èïø/n, ÁéãÊØÖ/nr, Âêë/p, ËÆ∞ËÄÖ/nnt, ‰ªãÁªç/v, Ê≠§/rzs, ËÆø/v, ÊÉÖÂÜµ/n, „ÄÇ/w]
```
[ÂõΩÈò≤ÈÉ®Èïø/n, ÁéãÊØÖ/nr, Âêë/p, ËÆ∞ËÄÖ/nnt, ‰ªãÁªç/v, Ê≠§/rzs, ËÆø/v, ÊÉÖÂÜµ/n, „ÄÇ/w]
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂõΩÈò≤ÈÉ®/nt, Èïø/a, ÁéãÊØÖ/nr, Âêë/p, ËÆ∞ËÄÖ/nnt, ‰ªãÁªç/v, Ê≠§/rzs, ËÆø/v, ÊÉÖÂÜµ/n, „ÄÇ/w]
```
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->
ÊÄª‰ΩìÊÑüËßâmaster‰∏çÂ§üÁ®≥ÂÆö
"
Ëá™ÂÆö‰πâËØçÁöÑ‰ºòÂÖàÁ∫ß‰ª•ÂèäËá™ÂÆö‰πâËØçÊÄßÁöÑÈóÆÈ¢ò,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.0

## ÊàëÁöÑÈóÆÈ¢ò
1. Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÔºåËØçÊÄßÂèØ‰ª•‰ªªÊÑèÂÆöÂêó„ÄÇÊØîÂ¶ÇÊàëÊñ∞ÂàõÈÄ†‰∏ÄÁßç ""ssr"" ÁöÑËØçÊÄßÔºåÁâπÊåáÊàëËá™Â∑±ÁöÑËØç
2. ‰ΩøÁî®‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏ÔºåÂπ∂‰∏îÊâìÂºÄ‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏‰ºòÂÖàÁöÑÂºÄÂÖ≥„ÄÇ‰ΩÜÊòØ‰ªçÁÑ∂Êó†Ê≥ïÂåπÈÖçÂá∫ÊÉ≥Ë¶ÅÁöÑÁªìÊûú„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
		String rawText = ""ÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´ÁãóÔºåËøéÂ®∂ÁôΩÂØåÁæéÔºåËµ∞‰∏ä‰∫∫ÁîüÂ∑ÖÂ≥∞"";
		// ÂéüÂßãÂàÜËØçÊïàÊûúÔºö [ÊîªÂüé/vi, ÁãÆ/ng, ÈÄÜË¢≠/nz, ÂçïË∫´/n, Áãó/n, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞/v,
		// ‰∏ä/f, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]

		// ÊµãËØï1:
		CustomDictionary.insert(""ÁãÆÈÄÜ"", ""ssr 20000"");
		CustomDictionary.insert(""ÁãóÔºåËøé"", ""ssr 2000"");
		CustomDictionary.insert(""Ëµ∞‰∏ä"", ""ssr 2000"");
		StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true);
		System.out.println(HanLP.segment(rawText));

		// ÊïàÊûúÔºö
		// [ÊîªÂüé/vi, ÁãÆ/ng, ÈÄÜË¢≠/nz, ÂçïË∫´/n, ÁãóÔºåËøé/ssr, Â®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/ssr, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
```
### ÊúüÊúõËæìÂá∫

[ÊîªÂüé/vi, ÁãÆÈÄÜ/ssr, Ë¢≠/vi, ÂçïË∫´/n, ÁãóÔºåËøé/ssr, Â®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/ssr, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]

ÂèØ‰ª•ÁúãÂá∫Ôºå ""ÁãóÔºåËøé"" ‰ª•Âèä ""Ëµ∞‰∏ä"" ËøôÁßçÁ°¨ÁîüÁîüÊ∑ªÂä†ÁöÑËØçË¢´ËØÜÂà´Âá∫Êù•‰∫Ü„ÄÇ
‰ΩÜÊòØ ‚Äú ÁãÆÈÄÜ‚Äù Ê≤°ÊúâÂåπÈÖçÂá∫Êù•„ÄÇ
ËØ∑ÈóÆÊòØËØçÊÄß‰∏äÊúâÈôêÂà∂Ôºü ËøòÊòØËØ¥ÈúÄË¶Å‰øÆÊîπÊ†∏ÂøÉÂ≠óÂÖ∏‰ªÄ‰πàÁöÑÔºü 
https://github.com/hankcs/HanLP/issues/393 Ë∑üËøô‰∏™issueÊúâÂÖ≥Âêó„ÄÇportableÁâàÊú¨‰∏çÊòØÂæàÊÉ≥ÂçïÁã¨‰øÆÊîπÊñá‰ª∂
Ë∞¢Ë∞¢ÔºÅ



"
NShortSegmentÂàÜËØçÁÆóÊ≥ïÂÆûÁé∞ÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.5.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.5.2

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
1Ôºâ ‰∏∫‰ªÄ‰πàNShortSegmentÂàÜËØçÈªòËÆ§Âè™ÈÄâÊã©Á¨¨1Êù°Ë∑ØÂæÑËøõË°åÂêéÁª≠ÂàÜÊûêÂíåÂ§ÑÁêÜÔºåÈÇ£ÂÆÉÂâçÈù¢Ê±ÇÂá∫Ëá≥Â∞ë2Êù°ÊúÄÁü≠Ë∑ØÂæÑÁöÑÊÑè‰πâ‰ΩïÂú®ÔºüÔºàÊñá‰ª∂NShortSegment.javaÁ¨¨101Ë°åÔºöList vertexList = coarseResult.get(0)Ôºâ
2Ôºâ ‰∏∫‰ªÄ‰πàNShortSegmentÂàÜËØçÂÆöÊ≠ªnKind = 2ÔºüÊòØ‰∏∫‰∫Ü‰ªÄ‰πàËÄÉËôëÁöÑÔºü


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
‰æãÂ¶ÇÔºöÊàë‰ΩøÁî®NShortSegmentËøõË°åÂàÜËØç‚Äú‰ªñËØ¥ÁöÑÁ°ÆÂÆûÂú®ÁêÜ„ÄÇ‚Äù

ËôΩÁÑ∂‰∏≠Èó¥ÂæóÂà∞‰∏§Êù°ÂàÜËØçÁªìÊûúÔºö
coarseResult = [[ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®, ÁêÜ, „ÄÇ, ], [ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®ÁêÜ, „ÄÇ, ]]

‰ΩÜÊòØÂêéÈù¢ÈªòËÆ§‰ΩøÁî®Á¨¨‰∏Ä‰∏™ÁªìÊûúËøõË°åÂêéÁª≠ÂàÜÊûêÔºàÁ¨¨101Ë°åÔºöList vertexList = coarseResult.get(0)ÔºâÔºåÂØºËá¥ÂÆûÈôÖËæìÂá∫Â∞±ÊòØÁ¨¨‰∏Ä‰∏™ÂàÜËØçÁªìÊûú„ÄÇ

ÈÇ£‰πàËøôÊ†∑ÁöÑËØùÔºåNShortSegmentÂàÜËØçÂá∫‰∏§ÁßçÂàÜËØçÁªìÊûúÔºåÊúâ‰ªÄ‰πàÊÑè‰πâÂë¢ÔºüÁ¨¨‰∫å‰∏™ÂàÜËØçÁªìÊûúÂêéÈù¢Ê†πÊú¨Â∞±Ê≤°Áî®ÔºüËÄå‰∏î‰∏™‰∫∫ËÆ§‰∏∫Á¨¨‰∫å‰∏™ÂàÜËØçÁªìÊûúÁ®çÂæÆÂ•Ω‰∏Ä‰∫õÔºàË∑ØÂæÑÊõ¥Áü≠Ôºâ„ÄÇ

Âè¶Â§ñÔºå‰∏∫‰ªÄ‰πàNSegment.java‰∏≠ÂÆöÊ≠ªnKind=2ÔºåÂç≥ÂèòÊàê2-ÊúÄÁü≠Ë∑ØÂæÑ„ÄÇËøô‰∏™nKindÂÆöÊ≠ª‰∏∫2ÊòØÂá∫‰∫éÂàÜËØçÊïàÁéáËÄÉËôëÂêóÔºüÊòØ‰∏çÊòØÂèØ‰ª•ËÆ©Áî®Êà∑Êù•Á°ÆÂÆönKindÁöÑÂÖ∑‰ΩìÂèñÂÄºÔºü
### Ê≠•È™§

Á®ãÂ∫èËøêË°åÂ¶Ç‰∏ã„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
   	public void testSegment() {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		List<Term> term1 = nShortSegment.seg(""‰ªñËØ¥ÁöÑÁ°ÆÂÆûÂú®ÁêÜ„ÄÇ"");
		System.out.println(term1);
	}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->
```
[ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®ÁêÜ, „ÄÇ, ]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®, ÁêÜ, „ÄÇ, ]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
1.3.2ÁâàÊú¨    ‚ÄúÂÅúÁâåËá™Êü•‚ÄùÂú®Â§Ñ‰∫éÂè•Êú´Êó∂ËøõË°åÂàÜËØçÔºåËØ•ËØç‰ºöÊ∂àÂ§±,"1.3.2ÁâàÊú¨‰∏≠Âá∫Áé∞Ôºö
‰æãÂè•ÔºöÊ≠¶Ê±âÂá°Ë∞∑9Êó•ÂÅúÁâåËá™Êü•
ËØçËØ≠‚ÄúÂÅúÁâåËá™Êü•‚ÄùÂú® segment.seg(sentence)Êìç‰ΩúÂêéÊ∂àÂ§±Ôºå‰ªÖÂá∫Áé∞‚ÄòÊ≠¶Ê±âÂá°Ë∞∑‚Äò „ÄÅ‚Äô9Êó•‚ÄôÔºõ
Èô§Ê≠§‰πãÂ§ñËøòÊúâ‰∫õÂÖ∂ÂÆÉÂä®ËØçÂú®Âè•Êú´Êó∂‰ºöÊòæÁé∞Ê≠§ÁßçÊÉÖÂÜµ„ÄÇ
ÊúõËß£ÂÜ≥„ÄÇ

"
Ëá™ÂÆö‰πâËØçÂ∫ì‰ºòÂÖàÁ∫ßÈóÆÈ¢ò,"CustomDictionary.add(""ÊîøÁ≠ñ"");
StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true);
System.out.println(HanLP.segment(""Êñ∞ÊîøÁ≠ñ""));

ÊÉ≥Ë¶ÅÂàáÂá∫ÁöÑÊòØ‚ÄúÊîøÁ≠ñ‚ÄùËøîÂõûÁöÑÊòØÊñ∞ÊîøÁ≠ñ"
ÈùûÂ∏∏Â•áÊÄ™ÁöÑÂßìÂêçËØÜÂà´ÈîôËØØ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºömaster

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
Âè•Â≠ê‰∏≠ÂßìÂêçËØÜÂà´Âá∫ÈîôÔºåÊåâÂ∏∏ÁêÜÊé®ËÆ∫‰∏çÂ∫îËØ•ÁöÑ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
   Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
   List<Term> termList = segment.seg(sentence);
   System.out.println(termList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
‰∏â‰∫öÂ∏ÇÊïôËÇ≤Â±Ä/nt, Âê¥Ëêç/nr, Â±ÄÈïø/nnt
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
‰∏â‰∫öÂ∏ÇÊïôËÇ≤Â±Ä/nt, Âê¥/tg, Ëêç/nz, Â±ÄÈïø/nnt
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éÂàÜËØçÁöÑÊïàÁéáÈóÆÈ¢òÔºåÊ†áÂáÜÂàÜËØçÂíåSpeedTokenizerÈÉΩÂ≠òÂú®ÔºåËØ∑ÊïôÔºÅ,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0 portable
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.5.0 portable

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
HanLpÁöÑÂàÜËØçÊïàÁéáÂ¶Ç‰ΩïÔºåÂè™ÁúãÂà∞ËøáÂÖ≥‰∫éÊÄ•ÈÄüÂàÜËØçÁöÑdemo„ÄÇ‰ΩÜÊòØÈÇ£‰∏™demoÊòØÈáçÂ§ç‰∫Ü1Áôæ‰∏áÊ¨°Áõ∏ÂêåÁöÑÊñáÊú¨ÔºåÊÄÄÁñëÂÜÖÈÉ®ÊúâÁºìÂ≠òÁ≠âÔºåÁúãËµ∑Êù•ÂàÜËØçÈÄüÂ∫¶ÂæàÈ´ò„ÄÇËÄåÊàëËá™Â∑±ËøõË°å‰∫ÜÁÆÄÂçïÁöÑ5‰∏™ÊñáÊú¨ÁöÑÊµãËØï„ÄÇÊïàÊûú‰∏çÊòØÂæàÁêÜÊÉ≥„ÄÇ

## Â§çÁé∞‰ª£Á†Å
```java
	@Test
	public void testCustomizeDict() {
		String[] sentences = new String[] { ""ËøôÊòØ‰∏Ä‰∏™‰º∏Êâã‰∏çËßÅ‰∫îÊåáÁöÑÈªëÂ§ú„ÄÇÊàëÂè´Â≠ôÊÇüÁ©∫ÔºåÊàëÁà±Âåó‰∫¨ÔºåÊàëÁà±PythonÂíåC++„ÄÇ"", ""Êàë‰∏çÂñúÊ¨¢Êó•Êú¨ÂíåÊúç„ÄÇ"",
				""Èõ∑Áå¥ÂõûÂΩí‰∫∫Èó¥„ÄÇ"", ""Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú"", ""ÁªìÊûúÂ©öÁöÑÂíåÂ∞öÊú™ÁªìËøáÂ©öÁöÑ"" };
		// ËøõË°åËá™ÂÆö‰πâËØçÊÄßÁöÑÈÖçÁΩÆ
		CustomDictionary.insert(""ÊïèÊÑüËØç"", ""ssr 1024"");
		CustomDictionary.insert(""ÁôΩÊ¥Å"", ""ssr 1024"");
		CustomDictionary.insert(""ÁôΩÂ∞èÊ¥Å"", ""ssr 1024"");
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			HanLP.segment(sentence);
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
**// Â§ßÊ¶ÇÊ∂àËÄó 300~400ms**
	}

	@Test
	public void testCustomizeSpeed() {
		String[] sentences = new String[] { ""ËøôÊòØ‰∏Ä‰∏™‰º∏Êâã‰∏çËßÅ‰∫îÊåáÁöÑÈªëÂ§ú„ÄÇÊàëÂè´Â≠ôÊÇüÁ©∫ÔºåÊàëÁà±Âåó‰∫¨ÔºåÊàëÁà±PythonÂíåC++„ÄÇ"", ""Êàë‰∏çÂñúÊ¨¢Êó•Êú¨ÂíåÊúç„ÄÇ"",
				""Èõ∑Áå¥ÂõûÂΩí‰∫∫Èó¥„ÄÇ"", ""Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú"", ""ÁªìÊûúÂ©öÁöÑÂíåÂ∞öÊú™ÁªìËøáÂ©öÁöÑ"" };
		// ËøõË°åËá™ÂÆö‰πâËØçÊÄßÁöÑÈÖçÁΩÆ
		CustomDictionary.insert(""ÊïèÊÑüËØç"", ""ssr 1024"");
		CustomDictionary.insert(""ÁôΩÊ¥Å"", ""ssr 1024"");
		CustomDictionary.insert(""ÁôΩÂ∞èÊ¥Å"", ""ssr 1024"");
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			SpeedTokenizer.segment(sentence);
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
	}
**// Â§ßÊ¶ÇÊ∂àËÄó80ms**
```
Áõ∏ÊØî‰πã‰∏ãÔºåÁ°ÆÂÆûÊÄ•ÈÄüÂàÜËØçÂø´ÂæàÂ§öÔºå‰ΩÜÊòØËøòÊ≤°ÊúâËææÂà∞ÁêÜÊÉ≥Áä∂ÊÄÅ„ÄÇÊÄ•ÈÄüÂàÜËØçÊ†πÊçÆÊÇ®Êèê‰æõÁöÑdemoÊù•ÁúãÔºå
ÂàÜËØçÈÄüÂ∫¶Ôºö11068068.62Â≠óÊØèÁßí „ÄÇ ÁÑ∂ËÄåÂÆûÈôÖ‰∏äÂ§ö‰∏™‰∏çÂêåÁöÑËøõË°åÂàÜËØçÔºåÊÑüËßâÊïàÁéáÂæà‰Ωé„ÄÇ
**ËØ∑ÈóÆÊòØÊàëÁöÑ‰ª£Á†ÅÂì™ÈáåÂÜôÁöÑ‰∏çÂØπÂêóÔºüËøòÊòØËØ¥ÁéØÂ¢É‰∏äÊúâÂì™‰∫õ‰∏çÊ≠£Á°ÆÁöÑÈÖçÁΩÆ„ÄÇ**

Âè¶Â§ñÔºå**HanLpÊòØÊàëÈ¶ñÈÄâÁöÑÔºå‰ΩÜÊòØÁúãÂà∞jiebaÂàÜËØçÁöÑjavaÁâà„ÄÇÁõ∏ÂêåÁöÑÊµãËØïÁî®‰æãÔºåÂàÜËØçÊÄªËÆ°ÊâçÊ∂àËÄó8ms„ÄÇ**
ÊØîHanlpÊÄ•ÈÄüÂàÜËØçËøòÂø´10ÂÄçÔºåÊÑüËßâËøôÊâçÊòØÊ≠£Â∏∏ÁöÑÊ∞¥Âπ≥„ÄÇ
ÁéØÂ¢ÉÈÉΩÊòØÊú¨Âú∞ÂºÄÂèëÊú∫ÔºåÂú®Intellij‰∏äËøõË°åËøêË°åÔºåi5,8gÂÜÖÂ≠ò„ÄÇ

```
	public static void main(String[] args) throws Exception {
		String[] sentences = new String[] { ""ËøôÊòØ‰∏Ä‰∏™‰º∏Êâã‰∏çËßÅ‰∫îÊåáÁöÑÈªëÂ§ú„ÄÇÊàëÂè´Â≠ôÊÇüÁ©∫ÔºåÊàëÁà±Âåó‰∫¨ÔºåÊàëÁà±PythonÂíåC++„ÄÇ"", ""Êàë‰∏çÂñúÊ¨¢Êó•Êú¨ÂíåÊúç„ÄÇ"",
				""Èõ∑Áå¥ÂõûÂΩí‰∫∫Èó¥„ÄÇ"", ""Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú"", ""ÁªìÊûúÂ©öÁöÑÂíåÂ∞öÊú™ÁªìËøáÂ©öÁöÑ"" };
		JiebaSegmenter segmenter = new JiebaSegmenter();
		long startTime = System.currentTimeMillis();
		for (String sentence : sentences) {
			System.out
					.println(segmenter.process(sentence, JiebaSegmenter.SegMode.INDEX).toString());
		}
		long endTime = System.currentTimeMillis();
		System.out.println(endTime - startTime);
	}
**//Â§ßÊ¶ÇÊ∂àËÄó8ms**
```
"
Java1.9Â∑≤Áªè‰∏çÂºÄÂßãÊîØÊåÅÂä®ÊÄÅÂ¢ûÂä†Êûö‰∏æÁöÑÈÉ®ÂàÜÊñπÊ≥ï,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
[
![qq 20171116103726](https://user-images.githubusercontent.com/13550295/32871119-3df4c0f8-caba-11e7-9018-9d3e3bac5196.png)
](url)
![package](https://user-images.githubusercontent.com/13550295/32871723-aa166f90-cabd-11e7-818c-10b494657c50.png)
JDK1.9ÈáçÊûÑ‰∫ÜÈÉ®ÂàÜ‰∏úË•øÔºå‰∏îÊîæÂà∞‰∫Ü‰∏çÊîØÊåÅÁöÑÂåÖÔºåÂèÇËßÅ‰∏äÂõæ
‰∏çÁü•ÈÅìÊúâ‰ªÄ‰πà‰ªÄ‰πàÂ•ΩÊñπÊ≥ïÂèØ‰ª•Ëß£ÂÜ≥"
ÂßìÂêçÂâçÈù¢ÊúâÁ©∫Ê†ºÔºàÂÖ®ËßíÂíåÂçäËßíÈÉΩÁÆóÔºâÔºåÂØºËá¥ÂßìÂêçÊó†Ê≥ïËØÜÂà´,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºömaster
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp:portable-1.5.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
ÂßìÂêçÂâçÈù¢ÊúâÁ©∫Ê†ºÔºàÂÖ®ËßíÂíåÂçäËßíÈÉΩÁÆóÔºâÔºåÂØºËá¥ÂßìÂêçÊó†Ê≥ïËØÜÂà´
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
hanlpÁöÑDemoChineseNameRecognition.javaÈáåÔºåÂßìÂêçÂâçÈù¢Âä†‰∏™Á©∫Ê†ºÔºåÂßìÂêçÂ∞±Êó†Ê≥ïËØÜÂà´Ôºö ÂéüÂè•Ôºö"" ÁéãÊÄªÂíåÂ∞è‰∏ΩÁªìÂ©ö‰∫Ü"", ËØÜÂà´ÁªìÊûúÔºö[ /w, Áéã/n, ÊÄª/b, Âíå/cc, Â∞è‰∏Ω/nr, ÁªìÂ©ö/vi, ‰∫Ü/ule]
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÊàëÊ≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÔºåÊâÄÊúâÁöÑhanlpÁâàÊú¨ÈÉΩÊúâËøô‰∏™ÈóÆÈ¢ò
### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    Segment segment = HanLP.newSegment().enableNameRecognize(true);
    List<Term> termList = segment.seg(‚Äú ËµµÂøóËæâÂêåÂøó‰ªªÂåó‰∫¨Â∏ÇÈ°∫‰πâÂå∫Ë¥®ÈáèÊäÄÊúØÁõëÁù£Â±ÄÂÖöÁªÑÊàêÂëò‚Äù);
    System.out.println(termList);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
[ /w, ËµµÂøóËæâ/nr, ÂêåÂøó/n, ‰ªª/v, Âåó‰∫¨Â∏Ç/ns, È°∫‰πâÂå∫/ns, Ë¥®Èáè/n, ÊäÄÊúØ/n, ÁõëÁù£Â±Ä/nis, ÂÖöÁªÑ/nis, ÊàêÂëò/nnt]
### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
[ /w, Ëµµ/nz, Âøó/n, Ëæâ/ng, ÂêåÂøó/n, ‰ªª/v, Âåó‰∫¨Â∏Ç/ns, È°∫‰πâÂå∫/ns, Ë¥®Èáè/n, ÊäÄÊúØ/n, ÁõëÁù£Â±Ä/nis, ÂÖöÁªÑ/nis, ÊàêÂëò/nnt]
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ê∑ªÂä†Áî®Êà∑Ëá™ÂÆö‰πâËØçÂêéÊä•viterbiÊï∞ÁªÑË∂äÁïå,"HiÔºö
ÊàëÂú®ÂàÜËØçÁöÑÊó∂ÂÄôÈÅáÂà∞‰∫ÜÊ∑ªÂä†Áî®Êà∑Ëá™ÂÆö‰πâËØçÂêéÊä•viterbiÊï∞ÁªÑË∂äÁïåÁöÑÈóÆÈ¢òÔºåÈ∫ªÁÉ¶Â∏ÆÂøôÁúã‰∏ã„ÄÇ
Êä•Èîô‰ø°ÊÅØÔºö
**java.lang.ArrayIndexOutOfBoundsException: 72
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:154)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:521)
	at com.hankcs.hanlp.tokenizer.NotionalTokenizer.segment(NotionalTokenizer.java:48)
	at com.hankcs.hanlp.tokenizer.NotionalTokenizer.segment(NotionalTokenizer.java:37)
	at com.mig.ml.mlplatform.segment$$anonfun$2.apply(segment.scala:398)
	at com.mig.ml.mlplatform.segment$$anonfun$2.apply(segment.scala:341)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)**

Ëß¶ÂèëÁâàÊú¨ÔºömasterÂàÜÊîØÊúÄÊñ∞Áâà**1.5.0**
Ëß¶Âèë‰ª£Á†ÅÔºö
```
 public static String readFile(String filePath) throws IOException {
        StringBuffer sb = new StringBuffer();
        fileUtils.readToBuffer(sb, filePath);
        return sb.toString();
    }
public static void readToBuffer(StringBuffer buffer, String filePath) throws IOException {
        InputStream is = new FileInputStream(filePath);
        String line; // Áî®Êù•‰øùÂ≠òÊØèË°åËØªÂèñÁöÑÂÜÖÂÆπ
        BufferedReader reader = new BufferedReader(new InputStreamReader(is));
        line = reader.readLine(); // ËØªÂèñÁ¨¨‰∏ÄË°å
        while (line != null) { // Â¶ÇÊûú line ‰∏∫Á©∫ËØ¥ÊòéËØªÂÆå‰∫Ü
            buffer.append(line); // Â∞ÜËØªÂà∞ÁöÑÂÜÖÂÆπÊ∑ªÂä†Âà∞ buffer ‰∏≠
            buffer.append(""\n""); // Ê∑ªÂä†Êç¢Ë°åÁ¨¶
            line = reader.readLine(); // ËØªÂèñ‰∏ã‰∏ÄË°å
        }
        reader.close();
        is.close();
    }

     try {
            String dic_word = readFile(""d:\\dic_word"");
            String[] split_dic_word = dic_word.split(""\n"");
            for(String sp: split_dic_word){
                CustomDictionary.add(sp);
            }

        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println(NotionalTokenizer.segment(""„Äê‰ªÅÂàõ„ÄëÂÄíËÆ°Êó∂1Â§© Ê≠¶Ê±âÁ≥ñÈÖí‰ºöÂú®Ê±âÂè£Ê≠¶Â±ïÊòéÂ§©ÁõõÂ§ßÂºÄÂπïÔºåÊÇ®ÂáÜÂ§áÂ•Ω‰∫ÜÂêóÔºüÊâæ‰∫ßÂìÅ„ÄÅ‰∫ÜËß£Â∏ÇÂú∫ÔºåÈ´òËßÑÊ†ºË°å‰∏öÊ¥ªÂä®Á≠â‰Ω†Êù•Ôºå‰∏çÂÆπÈîôËøá18171482672ÈªÑ""));
```

ËØçÂ∫ìdic_wordÊñá‰ª∂Â¶Ç‰∏ãÔºö


"
ÂàõÂª∫Âêå‰πâËØçÂÖ∏Êú™ÂÆö‰πâËØçÂêéË∞ÉÁî®CommonSynonymDictionary.SynonymItem.toStringÊñπÊ≥ïÁ©∫ÊåáÈíàÂºÇÂ∏∏,"## ÁâàÊú¨Âè∑
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.5.0

## ÊàëÁöÑÈóÆÈ¢ò
ÂàõÂª∫ËØçÂÖ∏Êú™ÂÆö‰πâËØçË∞ÉÁî®CommonSynonymDictionary.SynonymItem.toStringÊñπÊ≥ïÁ©∫ÊåáÈíàÂºÇÂ∏∏

### Ëß¶Âèë‰ª£Á†Å

```
    SynonymItem item = CommonSynonymDictionary.SynonymItem.createUndefined(""ÊµãËØïÁâà"");
    System.out.println(item.toString());
```

### ÂÆûÈôÖËæìÂá∫
Á©∫ÊåáÈíàÂºÇÂ∏∏

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÂàõÂª∫Âêå‰πâËØçÂÖ∏‰∏≠Êú™Âá∫Áé∞ÁöÑËØçÊó∂ com.hankcs.hanlp.corpus.synonym.Synonym ÁöÑType‰∏∫null

"
Python Ë∞ÉÁî®Êó∂Â¶Ç‰Ωï enable debug ‰ø°ÊÅØÔºü,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0 
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.5.0 portable

‰ΩøÁî® Python Ë∞ÉÁî®Êó∂Â∏åÊúõÊâìÂºÄË∞ÉËØï‰ø°ÊÅØÊü•ÁúãËØçÂÖ∏ÊòØÂê¶Âä†ËΩΩÊàêÂäüÔºåÈòÖËØª‰∫Ü jpype ÁöÑÁõ∏ÂÖ≥Ë∞ÉÁî®ÊñáÊ°£ÔºåÂèëÁé∞Áî±‰∫é inner class ÁöÑÈóÆÈ¢òÂπ∂‰∏çËÉΩÊåâÂ∏∏ËßÑÊñπÂºèË∞ÉÁî®
https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/HanLP.java#L53

`Config = JClass('com.hankcs.hanlp.HanLP$Config')`

ËØ∑ÈóÆÂ∫îËØ•Â¶Ç‰ΩïÊâìÂºÄË∞ÉËØï‰ø°ÊÅØÂë¢Ôºü

PS:

```
Config = JClass('com.hankcs.hanlp.HanLP$Config')
Config.enableDebug()
```

Êõ¥Êñ∞Âà∞1.5.0‰πãÂêéÈóÆÈ¢òÂ∑≤Ëß£ÂÜ≥Ôºå‰ΩÜÊòØÂèØËÉΩÂπ∂‰∏çÊòØÁâàÊú¨ÁöÑÈóÆÈ¢òÔºåÊòØËá™Â∑±ÊüêÂ§ÑË∞ÉÁî®Ê≤°ÊúâÂÜôÂØπ„ÄÇ







"
NShortSegmentÂàÜËØçÁÆóÊ≥ïÂÆûÁé∞ÈóÆÈ¢ò,"ÂéüÊñá‰ª∂ÔºöHanLP/src/main/java/com/hankcs/hanlp/seg/NShort/NShortSegment.java

Á¨¨101Ë°åÔºö List<Vertex> vertexList = coarseResult.get(0);

ÈóÆÈ¢òÔºö
1Ôºâ ‰∏∫‰ªÄ‰πàÈªòËÆ§Âè™ÈÄâÊã©Á¨¨1Êù°Ë∑ØÂæÑËøõË°åÂêéÁª≠ÂàÜÊûêÂíåÂ§ÑÁêÜÔºåÈÇ£ÂÆÉÂâçÈù¢Ê±ÇÂá∫Ëá≥Â∞ë2Êù°ÊúÄÁü≠Ë∑ØÂæÑÁöÑÊÑè‰πâ‰ΩïÂú®Ôºü
2Ôºâ ‰∏∫‰ªÄ‰πàÂÆöÊ≠ªnKind = 2ÔºüÊòØ‰∏∫‰∫Ü‰ªÄ‰πàËÄÉËôëÁöÑÔºü

‰æãÂ¶ÇÔºöÊàë‰ΩøÁî®NShortSegmentËøõË°åÂàÜËØç‚Äú‰ªñËØ¥ÁöÑÁ°ÆÂÆûÂú®ÁêÜ„ÄÇ‚Äù

‰∏≠Èó¥ÂæóÂà∞‰∏§Êù°ÂàÜËØçÁªìÊûúÔºö
coarseResult = [[ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®, ÁêÜ, „ÄÇ,  ], [ , ‰ªñ, ËØ¥, ÁöÑ, Á°ÆÂÆû, Âú®ÁêÜ, „ÄÇ,  ]]

ÊúÄÂêéËæìÂá∫Ôºö
[‰ªñ/rr, ËØ¥/v, ÁöÑ/ude1, Á°ÆÂÆû/ad, Âú®/p, ÁêÜ/n, „ÄÇ/w]

‰∏∫‰ªÄ‰πàÂêéÈù¢Âè™ÈÄâÊã©‰∫ÜÁ¨¨1‰∏™ÂàÜËØçÁªìÊûúËøõË°åÂàÜÊûêÔºüËÄåÂøΩÁï•‰∫ÜÁ¨¨2‰∏™ÂàÜËØçÁªìÊûúÔºü
ÊåâÁêÜÊù•ËØ¥ÔºåÁ¨¨2‰∏™ÂàÜËØçÁªìÊûú‰∏çÂ∫îËØ•Êõ¥Â•ΩÂêóÔºü"
ËØ∑ÈóÆÔºå‰∏∫‰ªÄ‰πà‰∏çÊîØÊåÅÂàÜËØçÁªìÊûúÊúÄÂ∞èÈ¢óÁ≤íÂ∫¶Ôºü,"
## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.5
<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ËØ∑ÈóÆÔºå‰∏∫‰ªÄ‰πà‰∏çÊîØÊåÅÂàÜËØçÁªìÊûúÊúÄÂ∞èÈ¢óÁ≤íÂ∫¶„ÄÇ
Âú®luceneÂú∫ÊôØ‰∏≠ÂàõÂª∫Êõ¥‰∏∞ÂØåÁöÑÁ¥¢ÂºïÔºå‰ª•‰æøÊîØÊåÅÊõ¥‰∏∞ÂØåÁöÑÊ£ÄÁ¥¢Âú∫ÊôØ„ÄÇ
ÊàëÁøªÈòÖ‰∫ÜÈÉ®ÂàÜ‰ª£Á†ÅÂíåÊñáÊ°£ÂèëÁé∞Ôºå‰ª•Âèä‰ΩøÁî®‰Ωú‰∏∫
com.hankcs.lucene.HanLPTokenizerFactory
Á¥¢ÂºïÂàÜËØçÂô®„ÄÇÈúÄË¶ÅÂàÜËØçÁöÑÁü≠ËØ≠‰∏∫:‚ÄúÂºπÁ∞ßÂ∫ä‚ÄùÔºåÂæóÂà∞ÁöÑÂàÜËØçÁªìÊûú‰∏∫Ôºö‚ÄúÂºπÁ∞ß‚Äù+‚ÄúÂ∫ä‚Äù„ÄÇ
ÈÇ£‰πàÊ£ÄÁ¥¢Âô®Êó†ËÆ∫ÊÄéÊ†∑ËÆæÁΩÆÔºåÊòØÊó†Ê≥ïÈÄöËøáÔºö‚ÄúÂºπ‚ÄùÔºåÊàñËÄÖÁõ∏ÂÖ≥Ê£ÄÁ¥¢Â≠óËØçÊü•ËØ¢Âà∞ÁªìÊûúÔºü

ÂàöÊé•Ëß¶ÊêúÁ¥¢Áõ∏ÂÖ≥Áü•ËØÜÔºåÂ¶ÇÊúâ‰∏çËÆ§ËØÜ‰∏çÂë®ÁöÑÔºåËØ∑ÂåÖÊ∂µ„ÄÇ


"
ÂÖ≥‰∫éVectorÁ±ª‰ª•ÂèäÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
1. ÂØπ‰∫éVectorÁ±ªÈáåÈù¢ÁöÑcosineÊñπÊ≥ïË°®Á§∫‰∏çÁêÜËß£
‰∏∫‰ªÄ‰πàËÆ°ÁÆódotÁöÑÁªìÊûúÂÜçÂºÄÂπ≥ÊñπÔºåÂä†ËΩΩËØçÂêëÈáèÁöÑÊó∂ÂÄôÂ∑≤ÁªèÊòØÂçï‰ΩçÂêëÈáèÔºå‰πüÂ∞±ÊòØËØ¥ËÆ°ÁÆócosineÁõ¥Êé•ËøîÂõûÁªìÊûúÂç≥ÂèØ„ÄÇÊ≤°ÊòéÁôΩ‰∏∫‰ªÄ‰πàËøô‰πàÂÅöÔºü

2. ÂØπ‰∫éDocVectorModelÁöÑqueryÊñπÊ≥ïÁöÑ‰∏çÁêÜËß£ÔºàÊñáÊ°£ÂêëÈáèÁîüÊàêÔºâ
ÁªìÊûúÈô§‰ª•nÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫Ü‰ªÄ‰πàÔºåÊ±ÇÊúüÊúõÔºüÂ¶ÇÊûúÊòØÊ±ÇÊúüÊúõÔºåÊòØ‰∏çÊòØËÆ°ÁÆóÂÆåÊàê‰πãÂêéÂ∫îËØ•ËÆ°ÁÆóÂçï‰ΩçÂêëÈáè„ÄÇ‰ª•‰∏ãË¥¥‰∏ÄÊÆµÊàëÁöÑÊµãËØï‰ª£Á†ÅÔºö
<pre><code>  
    public static void main(String[] args) {
        List<String> one = new ArrayList<>();
        one.add(""Êàë"");
        one.add(""ÊòØ"");
        one.add(""Á®ãÂ∫èÂëò"");

        List<String> other = new ArrayList<>();
        other.add(""Á®ãÂ∫èÂëò"");
        other.add(""Âæà"");
        other.add(""Ëã¶ÈÄº"");

        float[] senOne = new float[200];
        for (String s : one) {
            //ÈÄöËøáansjÊèê‰æõÁöÑÊ∫êÁ†ÅÂä†ËΩΩÁöÑ‰∫åËøõÂà∂ËØçÂêëÈáèÊñá‰ª∂
            float[] floats = WordSimilarity.getWordMap().get(s);
            if (floats == null) {
                System.out.println(s + ""Êú™ÊâæÂà∞ËØçÂêëÈáè"");
                continue;
            }
            for (int i = 0; i < floats.length; i++) {
                senOne[i] += floats[i];
            }
        }
        float[] senTwo = new float[200];
        for (String s : other) {
            //ÈÄöËøáansjÊèê‰æõÁöÑÊ∫êÁ†ÅÂä†ËΩΩÁöÑ‰∫åËøõÂà∂ËØçÂêëÈáèÊñá‰ª∂
            float[] floats = WordSimilarity.getWordMap().get(s);
            if (floats == null) {
                System.out.println(s + ""Êú™ÊâæÂà∞ËØçÂêëÈáè"");
                continue;
            }
            if (floats == null) {
                System.out.println(s + ""Êú™ÊâæÂà∞ËØçÂêëÈáè"");
                continue;
            }
            for (int i = 0; i < floats.length; i++) {
                senTwo[i] += floats[i];
            }
        }
        System.out.println(""ËØçÂêëÈáèÁõ¥Êé•Áõ∏Âä† + cosine Ôºö"" + cosine(senOne, senTwo));
        System.out.println(""DocVectorModel.query + Vector.cosineÔºö"" + query(one).cosine(query(other)));
        System.out.println(""DocVectorModel.query + cosineÔºö"" + cosine(query(one).elementArray, query(other).elementArray));
    }

    public static double cosine(float[] x, float[] y) {
        double xx = 0;
        double xy = 0;
        double yy = 0;
        for (int i = 0; i < x.length; i++) {
            xx += x[i] * x[i];
            xy += x[i] * y[i];
            yy += y[i] * y[i];
        }
        return xy / Math.sqrt(xx * yy);
    }


    /**
     * Áõ¥Êé•copyÁöÑDocVectorModelÁöÑqueryÊñπÊ≥ï
     * @param words ÂàÜËØçÁªìÊûú
     * @return Âè•ÂêëÈáè
     */
    public static Vector query(List<String> words) {
        if (words == null || words.size() == 0) return null;
        Vector result = new Vector(200);
        int n = 0;
        for (String word : words) {
            float[] floats = WordSimilarity.getWordMap().get(word);

            if (floats == null) {
                continue;
            }
            result.addToSelf(new Vector(floats));
            ++n;
            if (n == 0) {
                return null;
            }
        }
        result.divideToSelf(n);
        return result;
    }
</code></pre>
## ÊµãËØïÁªìÊûú
ËØçÂêëÈáèÁõ¥Êé•Áõ∏Âä† + cosine Ôºö0.7249490543441774
DocVectorModel.query + Vector.cosineÔºö0.5651151
DocVectorModel.query + cosineÔºö0.7249490499784463

## ÁªìËÆ∫
‰∏™‰∫∫ÊÑüËßâ1.ÊñáÊ°£ÂêëÈáèÂ∫îËØ•Èô§‰ª•ÂêëÈáèÁöÑÊ®°Ôºå2.Vector.cosineÊñπÊ≥ïÂ∫îËØ•Áõ¥Êé•ËøîÂõûdotÁªìÊûú„ÄÇ
‰∏çÁü•ÈÅìÊàëËØ¥ÁöÑÂØπ‰∏çÂØπ„ÄÇÂ∏åÊúõÊåáÊ≠£„ÄÇ"
Readme‰∏≠ÊâÄËØ¥ÁöÑÂ∏¶Á©∫Ê†ºÁöÑÁ∫ØÊñáÊú¨csvËá™ÂÆö‰πâËØçÂÖ∏Êä•Èîô,"`.txt`ËØçÂÖ∏Êñá‰ª∂ÁöÑÂàÜÈöîÁ¨¶‰∏∫Á©∫Ê†ºÊàñÂà∂Ë°®Á¨¶ÔºåÊâÄ‰ª•‰∏çÊîØÊåÅÂê´ÊúâÁ©∫Ê†ºÁöÑËØçËØ≠„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÊîØÊåÅÁ©∫Ê†ºÔºåËØ∑‰ΩøÁî®Ëã±ÊñáÈÄóÂè∑`,`ÂàÜÂâ≤ÁöÑ**Á∫ØÊñáÊú¨**`.csv`Êñá‰ª∂„ÄÇÂú®‰ΩøÁî®ExcelÁ≠âÂØåÊñáÊú¨ÁºñËæëÂô®Êó∂ÔºåÂàôËØ∑Ê≥®ÊÑè‰øùÂ≠ò‰∏∫**Á∫ØÊñáÊú¨**ÂΩ¢Âºè„ÄÇ<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
1.5.0 Êñ∞ËØçÂèëÁé∞ÁâπÊÄßOutOfMemoryÔºådiscoveryÂáΩÊï∞ËØªÂèñËØ≠ÊñôÂ≠óÁ¨¶‰∏≤ÔºåÊòØÂê¶ÂèØ‰ª•Êèê‰æõÊµÅÂºèÁÆ°ÈÅì‰Ωú‰∏∫ÂèÇÊï∞ÁöÑdiscoveryÂáΩÊï∞,"<!--
Ê≥®ÊÑè‰∫ãÈ°πÂíåÁâàÊú¨Âè∑ÂøÖÂ°´ÔºåÂê¶Âàô‰∏çÂõûÂ§ç„ÄÇËã•Â∏åÊúõÂ∞ΩÂø´ÂæóÂà∞ÂõûÂ§çÔºåËØ∑ÊåâÊ®°ÊùøËÆ§ÁúüÂ°´ÂÜôÔºåË∞¢Ë∞¢Âêà‰Ωú„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.5.0
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.5.0

<!--‰ª•‰∏äÂ±û‰∫éÂøÖÂ°´È°πÔºå‰ª•‰∏ãÂèØËá™Áî±ÂèëÊå•-->

## ÊàëÁöÑÈóÆÈ¢ò
‰ΩøÁî®1.5.0ÁâàÊú¨Êñ∞Â¢ûÂä†ÁöÑÁâπÊÄßÔºöÊñ∞ËØçÂèëÁé∞„ÄÇËæìÂÖ•ËØ≠ÊñôÊñá‰ª∂Â§ßÂ∞è90MÔºåÂá∫Áé∞OutOfMemory

## Â§çÁé∞ÈóÆÈ¢ò
		NewWordDiscover nd = new NewWordDiscover(4, 0.00005f, .6f, 0.2f, true);
		try {
			List<com.hankcs.hanlp.mining.word.WordInfo> newWords =nd.discovery(this.read(""/content/discuss_content.txt""), 100);
			
			System.out.println(newWords);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

### Ê≠•È™§

1. È¶ñÂÖàÔºåÊûÑÈÄ†NewWordDiscoverÂØπË±°ÔºåÂèÇÊï∞Ôºö4, 0.00005f, .6f, 0.2f, true
2. ÁÑ∂ÂêéÔºå‰ªéÊñá‰ª∂‰∏≠ËØªÂèñËØ≠ÊñôÔºåËØ≠ÊñôÊñá‰ª∂Á∫¶90M
```
	private String read(String fileName) throws IOException {
		StringBuffer result = new StringBuffer();
		BufferedReader  br = new BufferedReader(new FileReader(new File(fileName)));
		
		String s = null;
		while((s = br.readLine())!=null){
            result.append(System.lineSeparator()+s);
        }
		br.close(); 
		return result.toString();
	}
```
3. Êé•ÁùÄÔºåËøêË°å10ÂàÜÈíüÂ∑¶Âè≥ÂêéÊä•OutOfMemory

### Ëß¶Âèë‰ª£Á†Å
```
    private static void increaseFrequency(char c, Map<Character, int[]> storage)
    {
        int[] freq = storage.get(c);
        if (freq == null)
        {
            freq = new int[]{1};
            storage.put(c, freq);  // <-- Ëß¶ÂèëOutOfMemory
        }
        else
        {
            ++freq[0];
        }
    }
```
### ÊúüÊúõËæìÂá∫

Ê≠£Â∏∏ËæìÂá∫Êñ∞ËØçÁªìÊûúÔºå‰∏çÊä•Èîô

### ÂÆûÈôÖËæìÂá∫
ÊäõÂá∫OutOfMemoryÂºÇÂ∏∏

## ÂÖ∂‰ªñ‰ø°ÊÅØ
JVMÂÜÖÂ≠òÂ§ßÂ∞èËÆæÁΩÆÔºö-Xmx4096M -Xms4096M

## Âª∫ËÆÆ
ÊúüÊúõÂú®ËØ≠ÊñôËæÉÂ§ßÊó∂ÔºåËÉΩÈÄöËøáÊµÅËØªÂèñËØ≠ÊñôÊñá‰ª∂ÔºåÈò≤Ê≠¢‰∏ÄÊ¨°ÊÄßÂä†ËΩΩÊíëÁàÜÂÜÖÂ≠ò

"
Ëß£ÊûêtxtÊ†ºÂºèÁöÑ‰∏™‰∫∫Ê±ÇËÅåÁÆÄÂéÜ,"Ê•º‰∏ªÂ•ΩÔºåÊúâ‰∏™ÈóÆÈ¢òÊÉ≥ËØ∑Êïô‰∏ãÔºåÊàëÁé∞Âú®ÊúâËøô‰πà‰∏™ÈúÄÊ±ÇÔºö
ÈúÄË¶Å**Ëß£ÊûêtxtÊ†ºÂºèÁöÑ‰∏™‰∫∫Ê±ÇËÅåÁÆÄÂéÜ**
È¶ñÂÖàÊàëÈÄöËøáÂàÜËØçÊù•Ëé∑ÂèñÊ±ÇËÅåËÄÖÁöÑÂßìÂêç„ÄÅÊÄßÂà´„ÄÅ‰ΩèÂùÄ„ÄÅÈÇÆÁÆ±Á≠âÂü∫Êú¨‰ø°ÊÅØÊ≤°Êúâ‰ªÄ‰πàÈóÆÈ¢òÔºå
**‰ΩÜÊòØÁé∞Âú®ÈúÄË¶ÅÊèêÂèñÂá∫ÁÆÄÂéÜ‰∏≠ÁöÑÊ±ÇËÅåËÄÖÁöÑÈ°πÁõÆÂ∑•‰ΩúÁªèÈ™åÔºåËøô‰∏™Â•ΩÂÉèÂæà‰∏çÂ•ΩÊêûÔºåÊ≤°ÊâæÂà∞Â•ΩÁöÑÂäûÊ≥ïËÉΩÂ§üÂÆåÊï¥ÁöÑÊèêÂèñÂá∫È°πÁõÆÁªèÈ™åÁöÑ‰ø°ÊÅØÊù•ÔºåÊ•º‰∏ªÊúâÂï•ÂäûÊ≥ïÂ∏ÆÂøôÊåáÂØº‰∏ã‰πàÔºü**
ÊÑüË∞¢ÔºÅÔºÅÔºÅ"
Êää‰ª£Á†Å‰∏≠ÁöÑSystem.exitÊõøÊç¢ÊàêRuntimeExceptionÂºÇÂ∏∏,"Âú®‰ΩøÁî®ËøáÁ®ã‰∏≠ÊàëÂèëÁé∞ÔºåÂú®ÈÅáÂà∞ÂºÇÂ∏∏ÊÉÖÂÜµÁöÑÊó∂ÂÄô‰ª£Á†ÅÂü∫Êú¨ÈÉΩÈááÁî®System.exitÊù•Âº∫Âà∂ÈÄÄÂá∫JVMÔºåËøôÊ†∑ÁªôHanLP‰ΩøÁî®Â∏¶Êù•ÂæàÂ§ö‰∏çÊñπ‰æøÁöÑÂú∞Êñπ„ÄÇ

ÂàÜËØçÂá∫Èîô‰ªÖ‰ªÖÊòØHanLPÂ∫ìÁöÑÈîôËØØÔºå‰∏çËÉΩËÆ©Êï¥‰∏™Â∫îÁî®ÈÄÄÂá∫ÔºåÂ∫îÁî®ÁöÑÈÄÄÂá∫Â∫îËØ•‰∫§ÁªôÂ∫îÁî®Á®ãÂ∫èÁöÑÂºÄÂèëËÄÖÊù•ÂÜ≥ÂÆö„ÄÇ

Âª∫ËÆÆÂÆö‰πâ‰∏Ä‰∏™ÊàñÂ§ö‰∏™RuntimeExceptionÁöÑÂ≠êÁ±ªÔºåÁÑ∂ÂêéÂú®HanLPÈÅáÂà∞Ëá¥ÂëΩÈîôËØØÁöÑÊó∂ÂÄôÊäõÂá∫Ëøô‰∏™ÂºÇÂ∏∏ÔºåËÄå‰∏çÊòØÁõ¥Êé•ÈÄÄÂá∫JVMÁªôÂ∫îÁî®ÂºÄÂèëÈÄ†Êàê‰∏ç‰æø„ÄÇ"
"ÁπÅ‰ΩìÁÆÄ‰ΩìËΩ¨Êç¢ËØçÂÖ∏‰∏≠""Á´ãÈ´î=‰∏âÁ∂≠""ÁöÑ‰∏Ä‰∏™bug","t2s.txt‰∏≠
Á´ãÈ´î=‰∏âÁ∂≠

‰ΩÜÊòØ""‰∏âÁ∂≠""ÁöÑ‚ÄúÁ∂≠‚Äù‰πüÊòØ‰∏Ä‰∏™ÁπÅ‰ΩìÂ≠óÔºåÂ∫îËØ•ÊòØ‚Äú‰∏âÁª¥‚Äù"
ËÉΩÂê¶ÊîØÊåÅËßÇÁÇπÊäΩÂèñ,"@hankcs    ‰Ω†Â•Ω   
ÊàëÊÉ≥‰∫ÜËß£‰∏Ä‰∏ãHanLPËÉΩÂê¶ÊîØÊåÅËßÇÁÇπÊäΩÂèñ
ÊØîÂ¶Ç:ÔºÇÊúçÂä°ÊÄÅÂ∫¶ÂæàÂ•ΩÔºåÁéØÂ¢É‰πü‰∏çÈîôÔºåÂ∞±ÊòØÁÇπÊ≠åÁ≥ªÁªü‰∏çÂ§™Â•ΩÁî®„ÄÇÔºÇ
Ê≠£ÂêëËßÇÁÇπ : ÊúçÂä°ÊÄÅÂ∫¶ÂæàÂ•Ω ÁéØÂ¢É‰∏çÈîô
Ë¥üÂêëËßÇÁÇπ : ÁÇπÊ≠åÁ≥ªÁªüËêΩÂêé
"
Âú®spark‰∏≠‰ΩøÁî®ÂàÜËØçÂô®Êúâ‰∫õÈóÆÈ¢ò,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
Âú®spark‰∏≠‰ΩøÁî®ÂàÜËØçÂô®Êó∂ÔºåÊä•Êâæ‰∏çÂà∞ËØçÂÖ∏ÔºÅ
ËØ∑ÈóÆÊÄéÊ†∑ËÆ©Á®ãÂ∫èÂä†ËΩΩÊîæÂú®hdfs‰∏äÁöÑdataÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂ÔºåÊàñËÄÖËØ¥ÊÇ®ÊúâÊ≤°ÊúâÂàÜËØçÂô®Âú®ÂàÜÂ∏ÉÂºèËÆ°ÁÆóÊ°ÜÊû∂‰∏≠ÁöÑ‰∏Ä‰∫õÂ•ΩÁöÑÂÆûË∑µÔºüË∞¢Ë∞¢
"
CQueue.java ‰∏§‰∏™doubleÂûãÊï∞ÊçÆÁõ¥Êé•ÊØîËæÉÂ§ßÂ∞èÔºü,"Âéü‰ª£Á†ÅÔºö

Á¨¨32Ë°åÔºö
```java
 while (pCur != null && pCur.weight < newElement.weight)
{
            pPre = pCur;
            pCur = pCur.next;
}
```

‰∏ãÈù¢Ëøô‰∏ÄÂè•Ôºö
```
 pCur.weight < newElement.weight
```
ÊòØÂê¶Â∫îËØ•ÊîπÊàêÔºö```Double.compare(pCur.weight, newElement.weight) < 0```

"
ÂèØ‰ª•ÂÖàÂàÜËØçÔºåÂêéÁøªËØëÂêó?,"## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.5
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.5

## ÊàëÁöÑÈóÆÈ¢ò
HanLP ÂàÜËØçÊ≠£Á°ÆÔºåÂèØÊòØÁπÅÁ∞°ËΩ¨Êç¢ÁªìÊûúÈîôËØØÔºåÊü•Êâæ‰∏Ä‰∏ãÔºåÂèëÁé∞ HanLP Â•ΩÂÉèÂÖ∂‰∏≠Áî®‰∫Ü OpenCC ÂêåÊ†∑ËØçÂ∫ìÔºåÊâÄ‰ª•‰∏§ËæπÊúâÂÖ±ÂêåÈîôËØØÔºå‰∏çÁü•ÊòØÂê¶ÂèØ‰ª•‰ª•ÂàÜËØçÊù•ÂØπÊØîËØçÂ∫ì?

ÊØîÂ¶ÇËΩ¨Êç¢ ""ËÆ°ÁÆóÂèëÁé∞"" Âà∞ÁπÅ‰Ωì:
https://github.com/BYVoid/OpenCC/issues/272

https://github.com/BYVoid/OpenCC/issues/224"
Â¶Ç‰ΩïËÆ©‰π¶ÂêçÂè∑ÂÜÖÁöÑËØçËØ≠‰∏çËøõË°åÂàáÂàÜÔºüÁî®CustomDictionary.insert() Ôºå‰∏ç‰∏ÄÂÆöËÉΩËµ∑‰ΩúÁî®ÔºåÈ∫ªÁÉ¶„ÄÇÊúâÊ≤°ÊúâÊúâ‰∏Ä‰∏™ÊñπÊ≥ïÔºåËÉΩÂº∫Âà∂ÊÄßÁªô‰∫à‰∏çÂàáÂàÜËØ•ËØçÔºü,Â¶Ç‰ΩïËÆ©‰π¶ÂêçÂè∑ÂÜÖÁöÑËØçËØ≠‰∏çËøõË°åÂàáÂàÜÔºüÁî®CustomDictionary.insert() Ôºå‰∏ç‰∏ÄÂÆöËÉΩËµ∑‰ΩúÁî®ÔºåÈ∫ªÁÉ¶„ÄÇÊúâÊ≤°ÊúâÊúâ‰∏Ä‰∏™ÊñπÊ≥ïÔºåËÉΩÂº∫Âà∂ÊÄßÁªô‰∫à‰∏çÂàáÂàÜËØ•ËØçÔºüÊ±ÇÊåáÊïô„ÄÇ
Êâæ‰∏çÂà∞ÈÖçÁΩÆÊñá‰ª∂Ôºü,"_![Uploading image.png‚Ä¶]()
Á§∫‰æãÈÖçÁΩÆÊñá‰ª∂:hanlp.properties Âú®GitHubÁöÑÂèëÂ∏ÉÈ°µ‰∏≠Ôºåhanlp.properties‰∏ÄËà¨ÂíåjarÊâìÂåÖÂú®Âêå‰∏Ä‰∏™zipÂåÖ‰∏≠„ÄÇ
ÈÖçÁΩÆÊñá‰ª∂ÁöÑ‰ΩúÁî®ÊòØÂëäËØâHanLPÊï∞ÊçÆÂåÖÁöÑ‰ΩçÁΩÆÔºåÂè™ÈúÄ‰øÆÊîπÁ¨¨‰∏ÄË°å_
 Âπ∂Êú™ÊâæÂà∞ÈÖçÁΩÆÊñá‰ª∂ÔºåjarÂåÖ‰∏ãËΩΩ‰∏ç‰∫Ü"
ÂØπ‰∫éÊèêÂèñ‰∏çÂêåÁöÑËØçÔºåÈÄâÂèñÂì™ÁßçÂàÜËØçÊñπÊ≥ïÂõûÊõ¥Â•ΩÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöjarÂåÖ-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöjarÂåÖ-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºÅ
ÊàëÁé∞Âú®ÈúÄË¶ÅÂØπ‰∏ÄÁØá300Â≠óÂ∑¶Âè≥ÁöÑÊñ∞ÈóªËøõË°åÂÖ≥ÈîÆËØçÊèêÂèñÔºåÂÖ≥ÈîÆËØçÁöÑÂàÜÂà´‰∏∫Ôºö
```
Êó∂Èó¥
Á§æ‰ºöÂú∞ÁÇπÔºàegÔºöÂåªÈô¢„ÄÅÂ≠¶Ê†°Ôºâ
Ëá™ÁÑ∂Âú∞ÁÇπÔºàegÔºöÊ±üËãè„ÄÅÊù≠Â∑ûÔºâ
Á§æ‰ºöÂú∞‰ΩçÔºàegÔºöÁúÅÈïø„ÄÅÊÄª‰π¶ËÆ∞Ôºâ
Ë∫´‰ªΩÔºàegÔºöÂ≠ïÂ¶á„ÄÅÂåªÁîüÔºâ
‰∫∫Âêç
Ë°å‰∏∫Âä®ËØçÔºà‰∏ªË¶ÅÊòØvÂíåviÔºåegÔºöÊä•Âëä„ÄÅÂÆâÊÖ∞Ôºâ
```
ÁªèËøáÂ∞ùËØï‰ª•ÂêéÂèëÁé∞‰∏çÂêåÁöÑÂàÜËØçÊñπÊ≥ïÂØπ‰∫éÁõ∏ÂêåÁöÑÊñáÊú¨‰ºöÊúâ‰∏çÂêåÁöÑÂàÜËØçÁªìÊûúÔºå‰æãÂ¶ÇÊúâ‰∫õÂ∞±‰ºöËØÜÂà´Êõ¥Á≤æÂáÜÁöÑË°åÊîøÊú∫ÊûÑÔºåËÄåÊúâ‰∫õÂàôÁõ∏ÊØîËæÉÂ∑ÆÔºåÂõ†‰∏∫ÂØπ‰∫éNLPÊé•Ëß¶Êó∂Èó¥Âπ∂‰∏çÊòØÂæàÈïøÔºåÊâÄ‰ª•ÂØπ‰∫éÂêÑÁßçÂàÜËØçÂéüÁêÜ‰∫ÜËß£Âπ∂‰∏çÊòØÂæàÊ∏ÖÊô∞„ÄÇ
ËØ∑ÈóÆÊÇ®ËÉΩÂê¶ÂØπ‰∫éËøô‰∫õ‰∏çÂêåÁöÑÂÖ≥ÈîÆËØçÊèêÂèñÔºåÊèê‰æõÂú®HanLPÁé∞Â≠òÂàÜËØçÊñπÊ≥ï‰∏≠ÔºåËæÉÂ•ΩÁöÑÂàÜËØçÊñπÂºèÔºåÂç≥ÂØπ‰∫é‰∏çÂêåÁöÑËØçÁöÑÊèêÂèñÔºåÈááÂèñÊúÄÂ•ΩÁöÑÂàÜËØçÊñπÂºèÔºü
Ë∞¢Ë∞¢ÔºÅ"
ËøêË°åDemo‰∏≠ÁöÑDemoCustomNature.javaÂá∫Áé∞NoSuchMethodErrorÈîôËØØ,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöjarÂåÖ-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöjarÂåÖ-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºÅ
ÊàëÂú®ËøêË°ådemo‰∏≠ÁöÑDemoCustomNature.javaÊó∂ÔºåÂá∫Áé∞‰∫ÜÂú®‰∏ãÈù¢‚ÄúÂÆûÈôÖËæìÂá∫‚Äù‰∏≠ÁöÑÊä•Èîô„ÄÇ
Êìç‰ΩúÁ≥ªÁªüÊòØarchlinuxÔºåjavaÁâàÊú¨1.9Âíå1.8ÈÉΩÂ∞ùËØïËøáÔºåÂùáÂá∫Áé∞Êä•Èîô„ÄÇ
ÂèÇËÄÉËøá[#279](!https://github.com/hankcs/HanLP/issues/279#ref-issue-170997234)ÁöÑÈóÆÈ¢òÔºå‰ΩÜÊòØÊó†Ê≥ïËß£ÂÜ≥„ÄÇ
ËØ∑ÈóÆÂ¶Ç‰ΩïÊâçËÉΩÊ≠£Â∏∏ËøêË°å‰ª£Á†ÅÔºüË∞¢Ë∞¢ÔºÅ

## Â§çÁé∞ÈóÆÈ¢ò
Áõ¥Êé•ËøêË°å‰∫Üdemo

### Ëß¶Âèë‰ª£Á†Å
demo‰ª£Á†Å‰∏≠ÁöÑDemoCustomNature.java

### ÂÆûÈôÖËæìÂá∫

```
n
null
10Êúà 22, 2017 11:08:47 ‰∏äÂçà com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
Ë≠¶Âëä: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
Exception in thread ""main"" java.lang.NoSuchMethodError: sun.reflect.ReflectionFactory.newConstructorAccessor(Ljava/lang/reflect/Constructor;)Lsun/reflect/ConstructorAccessor;
	at com.hankcs.hanlp.corpus.util.EnumBuster.findConstructorAccessor(EnumBuster.java:255)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:92)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
	at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
	at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
	at demo.DemoCustomNature.main(DemoCustomNature.java:41)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:131)

```"
ÂÉè‚ÄúÊå∫Â•Ω‚ÄùÔºå‚ÄúÊ≤°ÊÉ≥Âà∞‚ÄùÁ≠âÂú®Â≠óÂÖ∏Êñá‰ª∂CoreNatureDictionary.txtÊñá‰ª∂‰∏≠ÈÉΩË¢´Ê†áÊ≥®Êàê‰∫Ünz,"## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

ÂÉè‚ÄúÊå∫Â•Ω‚ÄùÔºå‚ÄúÊ≤°ÊÉ≥Âà∞‚ÄùÁ≠âÂú®Â≠óÂÖ∏Êñá‰ª∂CoreNatureDictionary.txtÊñá‰ª∂‰∏≠ÈÉΩË¢´Ê†áÊ≥®Êàê‰∫ÜnzÔºå‰ΩÜÊòØÂÆûÈôÖ‰∏äÂÆÉ‰ª¨ÈÄöÂ∏∏ÈÉΩ‰∏çÊòØ‰∏ìÊúâÂêçËØç„ÄÇÂØπËØçÊÄßÂàÜÊûêÊó∂ÈÄ†ÊàêÂΩ±Âìç„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

‰æãÂ¶ÇÔºå‚ÄúÁúüÊòØÊ≤°ÊÉ≥Âà∞ÂëÄ„ÄÇ‚ÄùÂ∞±‰ºöÂæóÂá∫‚ÄúÊ≤°ÊÉ≥Âà∞/nz‚ÄùÔºå‚ÄúËøô‰∏™‰∏úË•øÊå∫Â•Ω‚ÄùÂ∞±‰ºöÂæóÂá∫‚ÄúÊå∫Â•Ω/nz‚Äù„ÄÇ

"
Êó†ËÆ∫ÊîπÂèò‰ªÄ‰πàÈÉΩÂàÜ‰∏çÂºÄÔºåÂåÖÊã¨‰øÆÊîπËØ≠ÊñôÊú¨Ë∫´,"![default](https://user-images.githubusercontent.com/17618881/31812053-555d8bfa-b5b4-11e7-982b-01aaa6531a3b.png)

‰∏äÁ¨¨‰∏Ä   ÔºåËøô‰∏™ËØçÂ∞±ÊòØÂàÜ‰∏çÂºÄÔºåÁÑ∂ÂêéÊàëÁúãÂéü‰ª£Á†ÅÂèëÁé∞Ôºå‰ª£Á†Å‰∏≠Â∞Ü m mÁªìÊûÑÁªôÂêàÂπ∂„ÄÇÊàëÂ∞ùËØïÂéªÊîπÂèòÔºå‰∏äÁöÑËØçÊÄß‰∏∫  ‰∏ä f 64454 Ôºå ‰ΩÜÊú∫Âô®Âç¥Ê†áÊ≥®‰∏∫ mËØçÊÄßÔºü ‰∏∫‰ªÄ‰πà‰∏äÂ≠óËØçÊÄßË¶ÅÁªôÊàëÊ†á‰∏ä m Ëøô‰∏™ËØçÊÄß„ÄÇ ÊàëÂéãÊ†πÈÉΩÊ≤°ÊúâÊ†á
"
Â¶Ç‰ΩïËé∑ÂèñÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÊâÄÊúâÂèØËÉΩÂàÜËØç,"ÊàëÁõÆÂâçÈááÁî®ÁöÑÊòØ1.3.1ÁâàÊú¨Ôºå‰ΩøÁî®Áî®Êà∑Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÔºåÂ≠óÂÖ∏‰∏≠ÂåÖÂê´ÔºöÁâõ‰ªîÈ©¨Áî≤„ÄÅ Áâõ‰ªî„ÄÅ È©¨Áî≤„ÄÅÁâõ‰ªîÈ©¨Áî≤Â§ñÂ•ó„ÄÅÈ©¨Áî≤„ÄÇ
ÈíàÂØπÊ∫êÂ≠óÁ¨¶‰∏≤Ôºö‚ÄúÂ∏ÖÊ∞îÊó†Ë¢ñËøûÂ∏ΩÁâõ‰ªîÈ©¨Áî≤Â§ñÂ•óÂ•≥Áü≠Ê¨æ2017ÁßãË£ÖÊñ∞Ê¨æÈü©ÁâàÂ§çÂè§Áâõ‰ªîÈ©¨Â§π‚Äù ËøõË°åÂàÜËØçÊó∂ÔºåÈááÁî®ÁöÑÊòØÁ¥¢ÂºïÂàÜËØçÂô®ÔºåÂæóÂà∞ÁöÑÁªìÊûúÊòØÔºö‚ÄúÁâõ‰ªîÈ©¨Áî≤Â§ñÂ•óÔºåÁâõ‰ªîÔºåÈ©¨Áî≤‚ÄùÔºåÊó†Ê≥ïÂàÜËØçÂæóÂá∫‚ÄúÁâõ‰ªîÈ©¨Áî≤‚Äù„ÄÇhanlp‰∏≠ÊòØÂê¶ÊúâÂäûÊ≥ïËé∑ÂèñÂà∞ÊâÄÊúâÂèØËÉΩÂ≠òÂú®ÁöÑÂàÜËØçÔºü"
ViterbiSegment ‰∏≠Á≤óÂàáÁÆóÊ≥ïÊúâ‰∫õÁñëÈóÆ,"ViterbiSegment.viterbi(WordNet wordNet) 
Ëøô‰∏™ÁÆóÊ≥ïÊàëÂ§ßËá¥Áúã‰∫ÜÔºå ‰ΩÜÊÑüËßâ‰∏çÂÉèÁª¥ÁâπÊØîÁÆóÊ≥ïÔºå ÊàëÁúãÁÆóÊ≥ï‰∏≠Âè™ÊòØ‰ªéÂêéÂêëÂâçÂèñ‰∫ÜÊØè‰∏™ËØçÁöÑÂà∞ÂâçÈù¢ÈÇ£‰∏™ËØçÁöÑÊúÄË∑ùÁ¶ªÊúÄÁü≠ÁöÑÈÇ£‰∏™ËØç„ÄÇ

ÊØîÂ¶Ç  ""ÊàëÁà±Ê±âËØ≠Â§ÑÁêÜ""  ÔºåÂÖàÂèñÂæóÂ§ÑÁêÜÔºå Êé•ÁùÄ‰ªéÂ§ÑÁêÜ‰∏≠ÂèñÂæó
‚Äò‚ÄôÊ±âËØ≠‚Äò‚Äô ËÄå‰∏çÊòØ ‚ÄòËØ≠‚Äô Ôºå Âõ†‰∏∫‰ªñ‰ª¨‰ø©ÁöÑË∑ùÁ¶ªÊúÄÁü≠„ÄÇ  Êé•ÁùÄÂÜç‰ªé ‚ÄúÊ±âËØ≠‚Äù ÂèñÂà∞ 'Áà±' ËÄå‰∏çÊòØ‚ÄúÊàëÁà±‚Äù Âõ†‰∏∫‰ªñ‰ª¨‰ø©ÁöÑË∑ùÁ¶ªÊúÄÁü≠„ÄÇ 

‰ΩÜÊòØÁª¥ÁâπÊØîÁÆóÊ≥ï‰∏çÊòØË¶ÅËÄÉËôëÂà∞ÊâÄÊúâÂâçÈù¢ÁöÑÊúÄÁü≠Ë∑ØÂæÑÔºå ËÄåÊàëÁúãËøô‰∏™ÁÆóÊ≥ïÂè™ÊòØÂèñ‰∫ÜÂΩìÂâçËØçÁöÑÂà∞ÂâçÈù¢ÈÇ£‰∏™ËØçÊúÄÁü≠Ë∑ØÂæÑÊâÄÂØπÂ∫îËØç„ÄÇ
"
ÂèØ‰ª•ÊúâËá™ÂÆö‰πâÂÅúËØçÂ∫ìÂêó,ÊúÄÊñ∞ÁâàÊú¨ÂèØ‰ª•‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂ∫ìÔºå‰∏çÁü•ÈÅìÊòØÂê¶ÊîØÊåÅËá™ÂÆö‰πâÂÅúËØçÂ∫ì„ÄÇËØï‰∫Ü‰∏Ä‰∏ãÁõ¥Êé•Âú®ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂÅúËØçpathÂêéÈù¢Âä†Ë∑ØÂæÑ‰∏çÊîØÊåÅ
CRFDependencyParser.computeÂá∫Áé∞bugÔºåÊúâ‰∏§‰∏™ËØç‰∫í‰∏∫ÂØπÊñπÁöÑHEAD,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.2.8-sources


## ÊàëÁöÑÈóÆÈ¢ò

Âú®‰ΩøÁî®CRFDependencyParser.compute(sentence)Êó∂Ôºå‰æùÂ≠òÂÖ≥Á≥ªÂá∫Áé∞‰∫ÜÁéØÁöÑÈóÆÈ¢òÔºåÊØîÂ¶Ç‚Äú‰∏çÂ∞ëiPhoneÈìÅÊùÜÁ≤â‰∏ùÈÄâÊã©Ë∑≥ËøáiPhone8Á≥ªÂàóÔºåÁ≠âÂæÖiPhoneXÁöÑ‰∏äÂ∏Ç„ÄÇ‚ÄùÔºåÁªìÊûúÂ¶Ç‰∏ãÔºà[word.ID, word.NAME, word.HEAD.ID, word.DEPREL]ÔºâÔºö
1	Êú™##Êï∞	2	Êï∞Èáè
2	iphone	3	ÈôêÂÆö
3	ÈìÅÊùÜ	4	ÈôêÂÆö
4	Á≤â‰∏ù	6	Âèó‰∫ã
5	ÈÄâÊã©	6	Âπ∂Âàó
6	Ë∑≥Ëøá	7	ÈôêÂÆö
7	iphone8	8	ÈôêÂÆö
8	Á≥ªÂàó	10	Âèó‰∫ã
9	Êú™##‰∏≤	10	Âèó‰∫ã
10	Á≠âÂæÖ	13	ÈôêÂÆö
11	iphonex	10	ÂÜÖÂÆπ
12	ÁöÑ	11	‚ÄúÁöÑ‚ÄùÂ≠ó‰æùÂ≠ò
13	‰∏äÂ∏Ç	10	ÂÜÖÂÆπ
      ÁªìÊûú‰∏≠""Á≠âÂæÖ""Âíå""‰∏äÂ∏Ç""‰∫í‰∏∫HEADÔºåËøôÂ∫îËØ•ÊòØ‰∏çÊ≠£Â∏∏ÁöÑÔºåËøôÊòØÂíãÂõû‰∫ãÂÑøÊçèÔºü
    ÔºàÊàëÂè™ÊòØÂú®Ëá™ÂÆö‰πâËØçÂ∫ì‰∏≠Âä†‰∫Üiphone, iphone8, iphonexËøô‰∫õËØçÔºâ

"
ËØ∑Ê±ÇËøõË°å‰∏ÄÊ¨°Â∏∏ËßÑÊÄßÊõ¥Êñ∞,Ë∑ùÁ¶ª‰∏ä‰∏ÄÊ¨°releaseÂèëÂ∏ÉÂ∑≤ÁªèÊúâÂ•ΩÂá†‰∏™Êúà‰∫ÜÔºåÂèØÂê¶Â∞ÜËøëÂá†‰∏™ÊúàÁöÑÂ∞èÂπÖ‰øÆÊ≠£Êï¥ÁêÜÂèëÂ∏É‰∏Ä‰∏™Êñ∞ÁâàÊú¨ÁöÑreleaseÔºåÂêåÊó∂Êõ¥Êñ∞‰∏Ä‰∏ãMavenÁâàÊú¨„ÄÇ
Ëá™ÂÆö‰πâËØçÂÖ∏ËØªÂèñÂ§±Ë¥•,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.2.8


## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÁöÑÈÖçÁΩÆÊñá‰ª∂ÈáåÂè™ÊîπÂèò‰∫ÜrootË∑ØÂæÑÔºåÁªèÊµãËØïÊó†ËØØ
‰ΩÜÊòØËá™ÂÆö‰πâÂàÜËØçÊó∂Ê∑ªÂä†‰∫ÜËá™Â∑±ÁöÑËØçÂÖ∏my.txtÂêéÊÄªÊòØÂá∫Áé∞Ë≠¶ÂëäËØªÂèñÂ§±Ë¥•‰∏çËÉΩÊòæÁ§∫ÂàÜËØçÁªìÊûúÔºåÂΩìÂú®Ëá™ÂÆö‰πâËØçÂÖ∏Ë∑ØÂæÑÈáå‰∏çÂä†ÂÖ•my.txtÊó∂ËÉΩÊòæÁ§∫Âá∫ÈªòËÆ§ÂàÜËØçÁªìÊûú

## Â§çÁé∞ÈóÆÈ¢ò
Êú™ÊîπÂèò‰ªª‰ΩïÂ∑≤ÊúâËØçÂÖ∏ÂíåÊ®°ÂûãÔºåÂè™Ê∑ªÂä†‰∫ÜËá™Â∑±ÁöÑËØçÂÖ∏ÔºàUTF-8ÔºâÁºñÁ†Å
‰∏îÊØèÊ¨°ÈÉΩÂà†Êéâ‰∫ÜÁºìÂ≠òÊñá‰ª∂
### Ê≠•È™§
È¶ñÂÖàÊåâÁÖßÊ≠•È™§ÊääÈÖçÁΩÆÊñá‰ª∂ÂÜÖÂÆπÂ§çÂà∂Â•ΩÔºåÊõ¥ÊîπrootË∑ØÂæÑÔºåÂú®Ëá™ÂÆö‰πâËØçÂÖ∏Ë∑ØÂæÑÂ§ÑÊ∑ªÂä†‰∫Ümy.txt(Â∑≤Ê≥®ÊÑè‰∫ÜÂâçÈù¢ÁöÑÁ©∫Ê†ºÂíåÂêéÈù¢ÁöÑÂàÜÂè∑)

### Ëß¶Âèë‰ª£Á†Å
import java.util.List;
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;

public class hanlp
{  public static void main (String[] args)
	{
       String  testline=""ÊµãËØïËá™ÂÆö‰πâÂàÜËØçËØçÂàÜ‰πâÂÆöËá™ËØïÊµã"";
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
       
       List<Term>termlist =segment.seg(testline);
       for(Term term:termlist)
       {
    	   System.out.println(term.toString());
       }
	}
}

### ÊúüÊúõËæìÂá∫


### ÂÆûÈôÖËæìÂá∫

Êó†ÂàÜËØçÁªìÊûú
 
ÊèêÁ§∫ÔºöÂçÅÊúà 11, 2017 5:28:24 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadDat
Ë≠¶Âëä: ËØªÂèñÂ§±Ë¥•ÔºåÈóÆÈ¢òÂèëÁîüÂú®java.lang.ArrayIndexOutOfBoundsException: 1056291
	at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToInt(ByteUtil.java:239)
	at com.hankcs.hanlp.corpus.io.ByteArray.nextInt(ByteArray.java:68)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:323)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:66)
	at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:53)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:199)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)
	at hanlp.main(hanlp.java:12)


ÂΩìÂà†ÂéªËá™ÂÆö‰πâËØçÂÖ∏Ë∑ØÂæÑÈáåmy.txt Êó∂ËæìÂá∫
ÊµãËØï/vn
Ëá™ÂÆö‰πâ/nz
ÂàÜËØç/n
ËØç/n
ÂàÜ/qt
‰πâÂÆö/nr
Ëá™/p
ËØï/v
Êµã/v

Ëá™ÂÆö‰πâËØçÂÖ∏my.txtÁöÑÂÜÖÂÆπÔºö
ËØïÊµã"
#623Â∞öÊú™‰øÆÊîπÂÆåÂÖ®ÁöÑÈÉ®ÂàÜÔºöÁ°Æ‰øù‰∏≠ÊñáÊï∞Â≠óÁöÑËØçÊÄß‰∏∫m,"
## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Á°Æ‰øù‰∏≠ÊñáÊï∞Â≠óÁöÑËØçÊÄß‰∏∫m
Â≠óÁ¨¶Á±ªÂûãÁªü‰∏Ä‰ΩøÁî®CharTypeÁ±ª‰∏≠ÁöÑÂ∏∏ÈáèÔºå‰ΩÜTextUtility‰∏≠ÁöÑ‰øùÁïô‰ª•‰æøÂâ•Á¶ª‰ΩøÁî®

## Áõ∏ÂÖ≥issue

#623"
‰Ω†Â•ΩÔºåÊàëÂ∞ÜÊàëËá™Â∑±ËÆ≠ÁªÉÂ•ΩÁöÑÊñá‰ª∂ÊîæÂà∞ÊõøÊç¢‰∫ÜÂéüÊúâÊ®°ÂûãÊñá‰ª∂ÔºåÊàëÁöÑÊ®°ÂûãÂú®crf++‰∏≠Êúâ97ÁöÑÊ≠£Á°ÆÁéáÔºå‰ΩÜÊòØÂú®ËøôÈáåÂè™Êúâ30Ôºå‰πüÊòØ4tagÔºåÊàë‰∏çÁü•ÈÅìÊòØÊ®°ÂûãÂä†ËΩΩÂá∫Áé∞‰∫ÜÈóÆÈ¢òÔºåËøòÊòØËß£Á†ÅËøáÁ®ã‰∏≠Âá∫Áé∞‰∫ÜÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑Ê±ÇÂíåÂª∫ËÆÆÔºàËøùÁ¶ÅËØç„ÄÅÊïèÊÑüËØçÂåπÈÖçÔºâÔºÅ,"ÊÇ®Â•ΩÔºÅ
ÊÑüË∞¢ÁôæÂøô‰∏≠ÂõûÂ§çÔºÅ
Âè™ÊòØ‰∏Ä‰∏™ËØ∑Ê±Ç„ÄÇ
ÁõÆÂâçÊúâÁ±ª‰ººÁöÑÈúÄÊ±ÇÔºö
**Â∞Ü‰∏ÄÊÆµÊñáÊú¨‰∏≠ÁöÑËøùÁ¶ÅËØçÔºàÂíåË∞êËØçÔºâËøõË°åÂåπÈÖçÂíåËøáÊª§**
ÁõÆÂâçÁªìÂêàHanLPÔºåÊàëËßâÂæóÂÖàÂØπÊñáÊú¨ËøõË°åÂàÜËØçÔºåÂ∞ÜÂàÜËØçÂêéÁöÑËØçÂíåÊïèÊÑüËØçÂ∫ìËøõË°åÂØπÊØîÔºàÈááÁî®hashÊàñËÄÖtrieÁ≠âÔºâ
ËÉΩËææÂà∞ n O(1) ÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºàÊØîÂ¶ÇÁõ¥Êé•‰ΩøÁî®hashmapÔºâ„ÄÇ
‰∏çÁü•HanLPÊúâÊ≤°ÊúâÊõ¥Â§ßÁöÑÂäûÊ≥ïÔºåËÉΩÂ§üÂú®ÂàÜËØçËøáÁ®ã‰∏≠Áõ¥Êé•ÂåπÈÖçÊüê‰∏™ÁâπÊÆäÂ≠êËØçÂ∫ìÁöÑÂäüËÉΩÔºàËøùÁ¶ÅËØçÂ∫ìÔºâ
Ë∞¢Ë∞¢ÔºÅ

"
Âí®ËØ¢HanLP‰∏éLTPÁöÑÂÖ≥Á≥ªÔºåÂèäLicenseÁõ∏ÂÖ≥ÈóÆÈ¢ò,"HiÔºå
  ËØ∑ÈóÆËØ•È°πÁõÆÂíåLTPÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºåÂì™‰∫õÊòØÁßªÊ§çÔºåÂì™‰∫õÊòØÈáçÊñ∞ÁºñÂÜôÂë¢ÔºüÂ¶ÇÊûúÂïÜÁî®ÔºåÊòØÂê¶ÈúÄË¶ÅËÅîÁ≥ªLTPÂèñÂæóÊéàÊùÉÂë¢Ôºü
  Âõ†‰∏∫ÁúãÂà∞ÂÖ∂‰ªñissue‰∏≠ÊèêÂà∞‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊ®°ÂûãËÆ≠ÁªÉÊòØÈÄöËøáLTPËÆ≠ÁªÉÂá∫ÁöÑ„ÄÇ

THX

"
‰∫∫Ê∞ëÊó•Êä•ÁöÑÊ†áÊ≥®È¢ÑÊñôÂ∫ìÊòØ‰∫∫Â∑•ÁöÑËøòÊòØÊú∫Âô®ÂÅöÁöÑÂë¢Ôºü,"
## ÊàëÁöÑÈóÆÈ¢ò

‰∫∫Ê∞ëÊó•Êä•ÁöÑÊ†áÊ≥®È¢ÑÊñôÂ∫ìÊòØ‰∫∫Â∑•ÁöÑËøòÊòØÊú∫Âô®ÂÅöÁöÑÂë¢Ôºü

"
pythonË∞ÉÁî®HanLP.parseDependencyÁöÑÊó∂ÂÄôÊä•Èîô,"
Êä•Èîô‰ø°ÊÅØÔºöjpype._jexception.LinkageErrorPyRaisable: java.lang.ExceptionInInitializerErrorÔºåÊ±ÇÂä©ÊÄé‰πàËß£ÂÜ≥
"
Â≠óÊ†áÊ≥®ÂàÜËØçÊ®°ÂùóÊõ¥Êñ∞Ôºö‰ºòÂåñ2Èò∂HMMÂàÜËØçÔºåCRFÊï∞Â≠óÂ≠óÊØçËØçÊ†áÊ≥®,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

1. ‰∏∫2Èò∂HMMÂàÜËØçÔºàÂü∫‰∫éÂ≠óÁöÑÁîüÊàêÂºèÊ®°ÂûãÔºâÂ¢ûÂä†‰∫ÜÁä∂ÊÄÅËΩ¨ÁßªÁü©ÈòµÔºåÁî®‰∫éÊêúÁ¥¢Ë∑ØÂæÑÂâ™Êûù„ÄÇÈÄüÂ∫¶ÊèêÈ´òËøë‰∏§ÂÄç„ÄÇ‰∏îÊïàÊûúÂæóÂà∞ËæÉÂ§ßÊèêÂçá„ÄÇ#566
2. ÈÄöËøáÂíåCRFÂÖ±Áî®VertexËΩ¨Êç¢‰ª£Á†ÅÔºåHMMÂàÜËØç‰πüÊîØÊåÅËØçÊÄßÊ†áÊ≥®ÂíåËá™ÂÆö‰πâËØçÂÖ∏ÂêàÂπ∂„ÄÇ
3. CRFÂàÜËØçÁªìÊûú‰∏≠ÁöÑÊï∞Â≠óÂíåÂ≠óÊØç‰∏çÂÜçÊ†áÊ≥®‰∏∫""nz'„ÄÇÂàÜÂà´Ê†áÊ≥®‰∏∫‚Äòm‚ÄôÂíå‚Äònx‚Äô ÔºåËøôÊ†∑Âú®Áî®‰∫éÊñ∞ËØçÊèêÂèñÊó∂ÂèØ‰ª•‰ªÖ‰ªÖÊãøÂà∞Êñ∞ËØç„ÄÇ #196 

## Áõ∏ÂÖ≥issue




"
java webÈ°πÁõÆ‰∏≠dataÊñá‰ª∂Â§πÂíåhanlp.propertiesÂ∫îËØ•ÊîæÂú®Âì™Èáå,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò
ËØ∑ÈóÆÂ¶Ç‰ΩïÂú®Java webÈ°πÁõÆ‰∏≠ÊîæÁΩÆdataÊñá‰ª∂Â§πÂíåhanlp.propertiesÂë¢ÔºüÂ∫îËØ•ÊîæÂú®Âì™ÈáåÊâçËÉΩÂä†ËΩΩÂà∞
"
ÂèØ‰ª•Â¢ûÂä†ÈíàÂØπelasticsearchÁöÑÊîØÊåÅÂêó,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

ËÉΩÈíàÂØπelasticsearch 5.xÂÅöÊîØÊåÅÂêó

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‚Äú‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°ÈõÜÂõ¢‚Äù Âíå ‚Äú‰∏≠ÂõΩÁßªÂä®‚Äù 2‰∏™ËØçÁöÑÂàÜËØçËØçÂ∫è‰∏ç‰∏ÄÊ†∑,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöc57895f14801df54312e8b656ea8ac2eeef72f99
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöc57895f14801df54312e8b656ea8ac2eeef72f99

## ÊàëÁöÑÈóÆÈ¢ò
‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°ÈõÜÂõ¢ Âíå ‰∏≠ÂõΩÁßªÂä® 2‰∏™ËØçÁöÑÂàÜËØçËØçÂ∫è‰∏ç‰∏ÄÊ†∑ÔºåÂØºËá¥solrÂÆåÂÖ®ÂåπÈÖçÊêúÁ¥¢ÁöÑÊó∂ÂÄôÊêú‰∏çÂà∞Êï∞ÊçÆ

## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
        List<Term> termList = IndexTokenizer.segment(""‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°ÈõÜÂõ¢"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
        List<Term> termList2 = IndexTokenizer.segment(""‰∏≠ÂõΩÁßªÂä®"");
        for (Term term : termList2) {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

```
### ÂÆûÈôÖËæìÂá∫

```
‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°ÈõÜÂõ¢/nt [0:8]
‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°/nz [0:6]
‰∏≠ÂõΩÁßªÂä®/nz [0:4]
‰∏≠ÂõΩ/ns [0:2]
ÁßªÂä®ÈÄö‰ø°/nz [2:6]
ÁßªÂä®/vn [2:4]
ÈÄö‰ø°/vn [4:6]
ÈõÜÂõ¢/nis [6:8]

‰∏≠ÂõΩÁßªÂä®/nz [0:4]
‰∏≠ÂõΩ/ns [0:2]
ÁßªÂä®/vn [2:4]
```

‚Äù‰∏≠ÂõΩÁßªÂä®ÈÄö‰ø°ÈõÜÂõ¢‚ÄúÁöÑÂàÜËØçÁªìÊûú‰∏≠ ‚Äú‰∏≠ÂõΩ‚ÄùÂíå‚ÄúÁßªÂä®‚Äù‰πãÈó¥Â§ö‰∫Ü‰∏Ä‰∏™‚ÄúÁßªÂä®ÈÄö‰ø°‚ÄùÔºåÂØºËá¥Âíå‚Äú‰∏≠ÂõΩÁßªÂä®‚ÄùÁöÑÂàÜËØçÁªìÊûúÈ°∫Â∫è‰∏çÂåπÈÖç„ÄÇËøôÊ†∑Âú®solr‰∏≠‰ΩøÁî®text:""‰∏≠ÂõΩÁßªÂä®""ËøôÁßçÂÖ®ËØçÂåπÈÖçÊ®°ÂºèÊêúÁ¥¢ÁöÑËØùÔºåÊòØÊêú‰∏çÂà∞Êï∞ÊçÆÁöÑ

‰πãÂâçÂõ†‰∏∫ËØçÂ∫èÁöÑ‰∫ãÊÉÖÂ∑≤ÁªèÈ∫ªÁÉ¶Ëøá‰Ω†‰∫ÜÔºåÁªèËøá‰∏äÊ¨°ÁöÑ‰øÆÊîπÔºåÂàÜËØçÁöÑËØçÂ∫èÂ∑≤ÁªèË∂ã‰∫éÁ®≥ÂÆöÔºå‰ΩÜËøòÊúâÂ∞ëÊï∞ÊúâÈóÆÈ¢òÔºåËøô‰∏™Â∞±ÊòØ‰∏Ä‰∏™Áâπ‰æãÔºåÂ∏åÊúõËÉΩÂ§üÂæóÂà∞‰øÆÊ≠£ÔºåÂÜçÊ¨°ÊÑüË∞¢ÔºÅ
"
111,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
DoubleArrayTrie‰∏≠ÁöÑ‰∏Ä‰∏™bug,"/**
     * Á≤æÁ°ÆÊü•ËØ¢
     *
     * @param keyChars ÈîÆÁöÑcharÊï∞ÁªÑ
     * @param pos      charÊï∞ÁªÑÁöÑËµ∑Âßã‰ΩçÁΩÆ
     * @param len      ÈîÆÁöÑÈïøÂ∫¶
     * @param nodePos  ÂºÄÂßãÊü•ÊâæÁöÑ‰ΩçÁΩÆÔºàÊú¨ÂèÇÊï∞ÂÖÅËÆ∏‰ªéÈùûÊ†πËäÇÁÇπÊü•ËØ¢Ôºâ
     * @return Êü•Âà∞ÁöÑËäÇÁÇπ‰ª£Ë°®ÁöÑvalue IDÔºåË¥üÊï∞Ë°®Á§∫‰∏çÂ≠òÂú®
     */
    public int exactMatchSearch(char[] keyChars, int pos, int len, int nodePos)
    {
        int result = -1;

        int b = base[nodePos];
        int p;

#Â¶ÇÊûúlenÊòØÈîÆÁöÑÈïøÂ∫¶ÈÇ£‰πàÔºåÂæ™ÁéØÂ∫îËØ•ÊòØ  for (int i = pos; i < pos+len; i++)
        for (int i = pos; i < len; i++)   

        {
            p = b + (int) (keyChars[i]) + 1;
            if (b == check[p])
                b = base[p];
            else
                return result;
        }"
Ê∑ªÂä†Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂêéÊ≤°Ëµ∑‰ΩúÁî®,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable 1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶ Ê∑ªÂä†‰∫ÜÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂåÖÊã¨nova,p10Á≠âËØçÔºåËØçÊÄß‰∏∫pr
2. ÁÑ∂Âêé‚Ä¶‚Ä¶‰ΩøÁî®HanlpËøõË°åÂàÜËØçÔºåÊú™ÂæóÂà∞ÁêÜÊÉ≥ÊïàÊûú
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        List<Term> termList = HanLP.segment(""novap10"");
        System.out.println(termList);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```nova/pr,p10/m 

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->
   
```
ÂÆûÈôÖËæìÂá∫
```novap/nx,10/m
   novap‰Ωú‰∏∫‰∏Ä‰∏™Ëã±ÊñáËØçËæìÂá∫Ôºå10‰Ωú‰∏∫‰∏Ä‰∏™Êï∞Â≠óËØçËæìÂá∫

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Merge pull request #1 from hankcs/master,"update from origin

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
ÂàÜËØçÁñëÈóÆÔºö‚ÄúÈí±ÁÆ°ÂÆ∂‰∏≠ÊÄé‰πàÁªëÂÆöÁΩëÈì∂‚Äù,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
ÂàÜËØçÁñëÈóÆÔºåÊ∑ªÂä†‰∫ÜËá™ÂÆö‰πâËØçÔºå‰ΩÜÊòØÊ≤°ÊúâÂàÜËØçÂá∫Êù•ÔºåËÄåÊòØ‰∫∫ÂêçËØÜÂà´‰æùÁÑ∂Ëµ∑ÊïàÊûúÔºå‰ΩÜÊÑüËßâ‰∫∫ÂêçËØÜÂà´‰πü‰∏çÂØπ„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Èí±ÁÆ°ÂÆ∂"");
        System.out.println(HanLP.segment(""Èí±ÁÆ°ÂÆ∂‰∏≠ÊÄé‰πàÁªëÂÆöÁΩëÈì∂""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Èí±ÁÆ°ÂÆ∂/n,  ‰∏≠/, ÊÄé‰πà/ryv, ÁªëÂÆö/gi, ÁΩëÈì∂/n]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Èí±ÁÆ°/nr, ÂÆ∂‰∏≠/s, ÊÄé‰πà/ryv, ÁªëÂÆö/gi, ÁΩëÈì∂/n]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

‰ªéÂàÜËØçÁªìÊûúÁúãÔºå`Èí±ÁÆ°`ÂàÜËØç‰∏∫‰∏Ä‰∏™‰∫∫Âêç‰∫ÜÔºåÊ∑ªÂä†Ëá™ÂÆö‰πâÂàÜËØçÂêéÔºåÊ≤°Êúâ‰ªª‰ΩïÂΩ±Âìç„ÄÇ
ÁÑ∂ÂêéÔºåÊàëÂÖ≥Èó≠‰∫∫ÂêçËØÜÂà´ÂäüËÉΩÔºåÂèëÁé∞ËøòÊòØÊ≤°Êúâ‰ΩúÁî®ÔºåÂøÖÈ°ªË¶ÅÂêëCoreNatureDictionary.ngram.txt‰∏≠
Ê∑ªÂä†
```
Èí±ÁÆ°ÂÆ∂@‰∏≠ 10
```
Âπ∂‰∏îÔºåÂ∞Ü`Èí±ÁÆ°ÂÆ∂`Ê∑ªÂä†Âà∞CoreNatureDictionary.txt‰∏≠ÊâçË°å„ÄÇ



"
ÊòØÂê¶ËÄÉËôëÂá∫‰∏™pythonÁâàÔºüÊàñËÄÖÊèê‰æõpythonÊé•Âè£Ôºü,ÂêëpythonÂºÄÊîæÂ∫îËØ•ÊòØ‰∏™Â•ΩÁöÑÊñπÂêë„ÄÇ
Hanlp‰ΩøÁî®,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®‰ΩøÁî®HanlpÂàÜËØçÁöÑÊó∂ÂÄôÂèëÁé∞ÊïàÊûú‰∏çÈîôÔºåÊâÄ‰ª•ÊÉ≥Â∫îÁî®Âà∞Êüê‰∏Ä‰∏™ÁâπÂÆöÈ¢ÜÂüüÔºà‰æãÂ¶ÇÊñ∞ÈóªÔºâ„ÄÇËã¶‰∫éÂØπÊï¥‰∏™Â∑•Á®ãÁöÑÂéüÁêÜÂíåÂ∑•Á®ãÂÆûÁé∞‰∏çÊòØÂæàÊ∏ÖÊ•ö„ÄÇ‰ΩúËÄÖËÉΩÂê¶Âá∫‰∏™‰∏ìÈ¢òÔºàÊàñËÄÖ‰π¶Á±çÔºâÂØπÂπøÂ§ßREADER‰ªãÁªç‰∏Ä‰∏ãËØ≠ÊñôÂ∫ìÁöÑÊî∂ÈõÜÔºå‰ΩøÁî®ÔºåÂ§ÑÁêÜÔºõÊ®°ÂûãÁöÑËÆ≠ÁªÉÔºåË∞É‰ºòÊµãËØïÔºõ‰ª•ÂèäÂêéÁª≠ÁöÑÁª¥Êä§Á≠â‰∏ªÈ¢ò

@hankcs 
"
test,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂàÜËØçËøáÁ®ãÁªìÊûúÁöÑ‰∏çÁêÜËß£,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÂú®Ë∞ÉËØïÊ†áÂáÜÂàÜËØçÂíåN-shortÊúÄÁü≠Ë∑ØÂàÜËØçÁöÑËøáÁ®ã‰∏≠ÈÅáÂà∞Â¶Ç‰∏ãÈóÆÈ¢òÔºö
String testSentence = ""ÈáçÂ∫ÜÈÇÆÁîµÂ§ßÂ≠¶ÊòØ‰∏ÄÊâÄÂ∑•ÁßëÂ§ßÂ≠¶ÔºåÂùêËêΩÂú®Áæé‰∏ΩÁöÑÂçóÂ±±‰∏ä„ÄÇËøôÈáåÊòØÊï∞Â≠óÈÄö‰ø°ÂèëÊ∫êÂú∞„ÄÇ"";
        //Ê†áÂáÜÂàÜËØç
        List<Term> standList = HanLP.segment(testSentence);
        System.out.println(""Ê†áÂáÜÂàÜËØçÔºö"" + standList);
ËæìÂá∫ÁªìÊûúÊòØÔºö
Ê†áÂáÜÂàÜËØçÔºö[ÈáçÂ∫ÜÈÇÆÁîµÂ§ßÂ≠¶/ntu, ÊòØ/vshi, ‰∏ÄÊâÄ/n, Â∑•ÁßëÂ§ßÂ≠¶/l, Ôºå/w, ÂùêËêΩ/vi, Âú®/p, Áæé‰∏Ω/a, ÁöÑ/ude1, ÂçóÂ±±/ns, ‰∏ä/f, „ÄÇ/w, ËøôÈáå/rzs, ÊòØ/vshi, Êï∞Â≠óÈÄö‰ø°/nz, ÂèëÊ∫êÂú∞/n, „ÄÇ/w]
‚ÄúÈáçÂ∫ÜÈÇÆÁîµÂ§ßÂ≠¶‚ÄùËøô‰∏™‰∏ìÊúâÂêçËØçÊòØÂú®Êú∫ÊûÑÂêçËØçÂÖ∏ÈáåÈù¢ÁöÑÔºåÊ†áÂáÜÂàÜËØçÂπ∂Ê≤°ÊúâÂºÄÂêØÊú∫ÊûÑÂêçËØçÂÖ∏Ëøô‰∏™Ê®°ÂûãÔºåËæìÂá∫ÁªìÊûúÂ∫îËØ•ÊòØ‚ÄúÈáçÂ∫Ü‚Äù+‚ÄúÈÇÆÁîµÂ§ßÂ≠¶‚Äù„ÄÇÂΩìÂú®Êú∫ÊûÑÂêçËØçÂÖ∏ÂéªÊéâ‚ÄúÈáçÂ∫ÜÈÇÆÁîµÂ§ßÂ≠¶‚ÄùËøô‰∏™ËØçËØ≠ÂêéÔºåÂàÜËØçÁªìÊûúÂíåÈ¢ÑÊÉ≥ÁöÑ‰∏ÄÊ†∑ÔºåÊòØ‚ÄúÈáçÂ∫Ü‚Äù+‚ÄúÈÇÆÁîµÂ§ßÂ≠¶‚Äù„ÄÇËØ¥ÊòéË∞ÉÁî®Ê†áÂáÜÂàÜËØçÁöÑÊó∂ÂÄôÁî®Âà∞‰∫ÜÊú∫ÊûÑÂêçËØçÂÖ∏ÈáåÈù¢ÁöÑÊï∞ÊçÆÔºåËøôÂíå‰ª£Á†ÅÈÄªËæë‰∏ç‰∏ÄÊ†∑ÔºåÈ∫ªÁÉ¶Â∏ÆÂøôËß£Á≠î‰∏Ä‰∏ã"
ËØªÂèñcrfÊ®°ÂûãÁöÑÊó∂ÂÄôÂæóÂà∞ÁöÑÊùÉÈáçÂÄº‰∏çË¶Å‰∫∫Â∑•ÂéªËÆæÂÆöÔºõÂÖ∑‰Ωì‰ΩçÁΩÆÂú®Á¨¨‰∏Ä‰∏™Â≠ó‰∏çÂèØËÉΩMÊàñËÄÖE,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

‰∫ã‰æãÔºöU00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-1,0]/%x[0,0]
U06:%x[0,0]/%x[1,0]
"
Ëá™ÂÆö‰πâËØçÊÄßÁöÑÁî®Êà∑ËØçÂÖ∏ÂØºÂÖ•,"## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
Ê†πÊçÆ[issue243](https://github.com/hankcs/HanLP/issues/243)

ÈÇ£ÂèØÂê¶Âú®Âä†ËΩΩÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Êñá‰ª∂ÂâçÔºåÂÖàÁî®CustomNatureUtility.addNature(‚ÄúÊñ∞ËØçÊÄß‚Äù);ÁÑ∂ÂêéÂÜçÁî®ACDoubleArrayTrieSegment.loadDistionaryÊù•ÂØºÂÖ•Â∏¶ÊúâËá™ÂÆö‰πâËØçÊÄßÁöÑÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Êñá‰ª∂Ôºü
ÊàëÂÅö‰∫ÜÂ¶Ç‰∏ãÊìç‰Ωú:
```
CustomNatureUtility.addNature(‚ÄúÊñ∞ËØçÊÄß1‚Äù);
CustomNatureUtility.addNature(""Êñ∞ËØçÊÄß2"");
AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment()
                .loadDictionary(HanLP.Config.CustomDictionaryPath[1]);
```
ÂèëÁé∞Êä•ÈîôÔºö
java.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.Êñ∞ËØçÊÄß2

Ë∑üË∏™‰∫Ü‰∏Ä‰∏ã‰ª£Á†ÅÂèëÁé∞‰∏äËø∞‰ª£Á†ÅÊ∑ªÂä†ËØçÊÄß‰πãÂêéÔºåËØçÊÄßÂ∏∏ÈáèÈáåÈù¢ÔºåÂè™Ê∑ªÂä†ÊàêÂäü‰∫ÜÊñ∞ËØçÊÄß1ÔºåÊ≤°ÊúâÊñ∞ËØçÊÄß2.

Â¶ÇÊûúÂè™Ê∑ªÂä†‰∏Ä‰∏™Êñ∞ËØçÊÄß1Âπ∂‰∏îËØçÂÖ∏Êñá‰ª∂‰∏≠Âè™ÊúâÊñ∞ËØçÊÄß1ÁöÑËØùÔºåÂ∞±ÂèØ‰ª•Ê≠£Á°ÆÂØπÂàÜËØçÁªìÊûúÊ†áÊ≥®Áî®Êà∑ÂÆö‰πâÁöÑÊñ∞ËØçÊÄß1ÔºåÊ∑ªÂä†‰∏§‰∏™Êñ∞ËØçÊÄßÔºåÂ∞±‰∏çÂèØ‰ª•Ôºå‰∏çÁü•ÊòØÂê¶Êúâ‰∫∫ÈÅáÂà∞Á±ª‰ººÈóÆÈ¢òÔºüË∞¢Ë∞¢„ÄÇ



"
ËØ∑ÈóÆÊòØÂê¶ÂèØ‰ª•ÊèêÂèñÊåáÂÆöÁöÑÂÖ≥ÈîÆÂ≠ó,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÊàëÁöÑÈóÆÈ¢ò

ÂÅáÂ¶ÇËØ¥ÊàëÊúâ‰∏Ä‰∏™ÂÖ≥ÈîÆÂ≠óÂàóË°® (10000+)ÔºõÊàëÊÉ≥ÊèêÂèñ‰∏Ä‰∏™ÊñáÁ´†ÁöÑÊâÄÊúâÁöÑÂèØ‰ª•ÂåπÈÖçÁöÑÂÖ≥ÈîÆÂ≠ó(Ê≤°ÊúâÂåπÈÖçÁöÑÂÖ®ÈÉ®ÂâîÈô§)ÔºõËØ∑ÈóÆ HanLP ÂèØ‰ª•ÂÆûÁé∞ÂêóÔºü

ËØ∑ÈóÆÂ§ßÂÆ∂ÊòØÂê¶ÊúâÁõ∏‰ººÁöÑÔºåÈ´òÊÄßËÉΩÁöÑÂºÄÊ∫êÊâ©Â±ïÂ∫ìÔºå‰πüÂèØ‰ª•Êé®Ëçê‰∏Ä‰∏ã„ÄÇ‰∏ªË¶ÅÁî®‰∫éÊèêÂèñÊåáÂÆöÂÖ≥ÈîÆËØçÂàóË°®ÁöÑÂÖ≥ÈîÆËØç (Ëøô‰∏™Ê†áÂÖ≥ÈîÆËØçË°®ÊúâÁÇπÂ§ß)„ÄÇ

ÊØîÂ¶ÇÔºö  
ShellÊòØLinuxËøêÁª¥‰∏≠ÂøÖ‰∏çÂèØÂ∞ëÁöÑËÑöÊú¨ËØ≠Ë®ÄÔºåCentOSÊòØ‰∏ÄÊ¨æ‰ºüÂ§ßÁöÑÊìç‰ΩúÁ≥ªÁªü„ÄÇ  
ÊàëÂè™ÈúÄË¶ÅÔºöShellÔºåLinuxÔºåLinuxËøêÁª¥ÔºåËÑöÊú¨ËØ≠Ë®ÄÔºåCentOSÔºåÊìç‰ΩúÁ≥ªÁªü  
Ëøô‰∫õÂÖ≥ÈîÆÂ≠ó(Ê†áÁ≠æ)ÊòØÊàë‰∫ãÂÖàÂÆö‰πâÂ•ΩÁöÑ„ÄÇ  
"
‰∏∫TermÁ±ªÊ∑ªÂä†equalÊñπÊ≥ï,"
## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

‰∏∫TermÁ±ªÊ∑ªÂä†‰∫ÜequalÊñπÊ≥ïÔºå‰∏ªË¶ÅÁî®‰∫éËß£ÂÜ≥‰ª•ÂæÄÂà§Êñ≠TermÊòØÂê¶Áõ∏Á≠âÔºåÂßãÁªàËøîÂõûfalseÁöÑÈóÆÈ¢òÔºõ
ÁõÆÂâç‰ΩøÁî®‰∫ÜËØçÊÄßÂíåËØçËØ≠ÂÜÖÂÆπÊù•Ê†°È™åÊòØÂê¶Áõ∏Á≠â„ÄÇ



"
fix chinese numbers link with roman numbers,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

ÁõÆÂâçÁöÑÁâàÊú¨ÂØπ‰∏≠ÊñáÊï∞Â≠óÂíåÁΩóÈ©¨Êï∞Â≠óÊ≤°ÊúâÂàÜÂºÄÔºå‰æãÂ¶ÇÔºö‚ÄúËµµÂõõ158ÂºÄÂ§¥ÁöÑÂè∑Á†Å‚Äù‰ºöÊää‚ÄúÂõõ158‚ÄùÂàÜÂà∞‰∏ÄËµ∑„ÄÇÂéüÂõ†ÊòØÊûÑÂª∫ËØçÂõæÁöÑÊó∂ÂÄôÂ∞±Ê≤°ÊúâÂàáÂºÄÔºåÈÄöËøáÊîπËØçÂÖ∏Âπ∂‰∏çËÉΩËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ

Êñ∞ÁâàÊú¨Êñ∞Â¢ûCT_CNUMÁ±ªÂûãË°®Á§∫‰∏≠ÊñáÊï∞Â≠óÁ±ªÔºåÁªèÊµãËØïÔºåÊûÑÂª∫ËØçÂõæÊó∂Ôºå‰∏≠ÊñáÊï∞Â≠óÂíåÁΩóÈ©¨Êï∞Â≠ó‰ºöÂàáÂºÄÔºåÂÖ∂‰ªñÈÄªËæë‰∏çÂèò„ÄÇ‚ÄúËµµÂõõ158ÂºÄÂ§¥ÁöÑÂè∑Á†Å‚Äù‰ºöÊää‚ÄúËµµÂõõ‚ÄùÔºå‚Äú156‚ÄùÂçïÁã¨ÂàÜÂºÄ„ÄÇ

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Âè•Â≠ê‚Äù‰ªçÊúâÂæàÈïøÁöÑË∑ØË¶ÅËµ∞‚ÄúÂàÜËØçÈîôËØØ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## Âè•Â≠ê‚Äù‰ªçÊúâÂæàÈïøÁöÑË∑ØË¶ÅËµ∞‚ÄúÂàÜËØçÈîôËØØ

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
Ê≤°Êúâ‰øÆÊîπ

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        System.out.println(HanLP.segment(""‰ªçÊúâÂæàÈïøÁöÑË∑ØË¶ÅËµ∞""));
    }
```
### ÊúüÊúõËæìÂá∫

```
[‰ªç/d, Êúâ/vyou, ÂæàÈïø/d, ÁöÑ/ude1, Ë∑Ø/n, Ë¶Å/v, Ëµ∞/v]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[‰ªç/d, Êúâ/vyou, ÂæàÈïø/d, ÁöÑ/ude1, Ë∑ØË¶Å/nr, Ëµ∞/v]

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

## ÂàÜÊûê
‰ªéÁªìÊûúÁúãÔºåÂ•ΩÂÉèÊòØ‰∫∫ÂêçËØÜÂà´ÂΩ±Âìç‰∫Ü„ÄÇ
‰ΩÜËøô‰∏™Âè•Â≠êÔºå‰∏çÂ∫îËØ•Êúâ‰∫∫ÂêçÁöÑÔºåËøôÈáåÈîôËØØÂ∞Ü Ë∑ØË¶Å ÂàÜÊàê‰∫Ü‰∫∫Âêç‰∫Ü„ÄÇ

"
Êó∂Èó¥ËÉΩÂê¶‰Ωú‰∏∫‰∏Ä‰∏™Êï¥‰ΩìËØÜÂà´Âá∫Êù•Âë¢Ôºü,"Âú®ÂàÜËØçÁöÑÊó∂ÂÄôÊàëÂèëÁé∞Êó∂Èó¥ÈÉΩÊòØÂàÜÂºÄ‰∫ÜÔºåÊØîÂ¶ÇËØ¥2017Âπ¥9Êúà8Êó•ÔºåÂàÜËØçÁªìÊûú‰∏∫Ôºö
2017 Âπ¥ 9 Êúà 8 Êó• 
ËÉΩ‰∏çËÉΩÊää‰ªñ‰ª¨‰Ωú‰∏∫‰∏Ä‰∏™Êï¥‰ΩìËØÜÂà´Âá∫Êù•Âë¢ÔºüËÄå‰∏çÊòØÂàÜÂºÄÁöÑÔºåÂèØ‰ª•Âú®Áî®Êà∑ËØçÂÖ∏ÈáåÈù¢Âä†ÂÖ•Ê≠£ÂàôË°®ËææÂºèÊù•ÊääÊó∂Èó¥‰Ωú‰∏∫‰∏Ä‰∏™Êï¥‰ΩìËøõË°åÂàÜÂâ≤ÂêóÔºü"
Êú∫ÊûÑÂêçËØÜÂà´Âá∫ÂêéÊ∑ªÂä†Âà∞ËØçÁΩë‰∏≠Ôºå Êú™##Âõ¢ ËØçÊÄßËØªÂèñÈîô‰π±„ÄÇ,"<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Êú∫ÊûÑÂêçËØÜÂà´Ôºå
Â∑≤ÁªèËØÜÂà´Âá∫ÁöÑÊú∫ÊûÑÂêçÂàõÂª∫Êñ∞ÁöÑËäÇÁÇπÂπ∂ inset Âà∞ËØçÁΩë‰∏≠Ôºå
Ê†πÊçÆ Êú™##Âõ¢ Ëé∑ÂæóÁöÑ ATTRIBUTEËØªÂèñÈîô‰π±Ôºå
‰ª•‰∏ãÊòØ debug Êà™ÂõæÔºåÂíåÊàëÊ†πÊçÆËØçÈ¢ëÊêúÁ¥¢Âá∫ÁöÑÂîØ‰∏ÄÁªìÊûúÔºå

![31caf6d28e7006c994a7a6fc554953c](https://user-images.githubusercontent.com/26315728/30159424-19cfb804-93fb-11e7-9e55-aa2e3809785e.png)

ÂèØ‰ª•ÁúãÂà∞ÔºåËØçÈ¢ëÈ°∫Â∫èÊòØÂØπÁöÑÔºåËØçÊÄßÈîô‰π±‰∫Ü„ÄÇ
Âà†Èô§ÁºìÂ≠òÊñá‰ª∂ÂêéÈóÆÈ¢òÂæóÂà∞Ëß£ÂÜ≥„ÄÇ
ËøôÁßçÊÉÖÂÜµÂÅ∂Â∞îÂèëÁîü„ÄÇ
ÂΩìÁÑ∂ÔºåÊàë‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºåËÄå‰∏îÊúâÁÇπÂ§ö„ÄÇ
ËÉΩÊÉ≥Âà∞ÁöÑÂ∞±ÊòØÂú®ËØªÂèñÊ†∏ÂøÉËØçÂÖ∏ÁöÑÊó∂ÂÄôÈîô‰π±‰∫ÜÔºå
Â∏åÊúõÁªôÂá∫Âª∫ËÆÆ„ÄÇ
Ë∞¢Ë∞¢„ÄÇ

"
pku_training.bmes.txtËøô‰∏™Êñá‰ª∂Âú®Âì™ÈáåÔºü,"‰ΩøÁî®Ëøô‰∏™ÂëΩ‰ª§Êó∂Ôºå`crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t`
ÈúÄË¶Å pku_training.bmes.txtÊñá‰ª∂Ôºå‰ΩÜÊòØÈ°πÁõÆÈáåÊ≤°ÊúâÔºåÊòØ‰ªéÂì™ÈáåÂèØ‰ª•ÊâæÂà∞‰πà"
ÁÆóÊ≥ï‰∫∫ÂêçËØÜÂà´ÊúâÁÇπÂ•áÊÄ™Ôºü,"‰ªñÂè´Â∞èÊòéÂç¥ËÉΩÂàáÂá∫Â∞èÊòé„ÄÇ
![default](https://user-images.githubusercontent.com/17618881/30004018-4e074046-90fa-11e7-8a8e-993775e333c0.png)

Â∞èËêçÂ∞±‰∏çÂØπÔºåÂàáÊàêÂ∞è Ëêç„ÄÇ
![default](https://user-images.githubusercontent.com/17618881/30004013-286dc738-90fa-11e7-8107-67c5aeb6b0e2.png)

Â∞èËä±Âç¥ÂèàÈîô„ÄÇ

ÊàëÂ¶Ç‰ΩïËá™ÂÆö‰πâ‰∏Ä‰∏ãÔºå‰ΩÜÊàë‰∏çÂèØËÉΩÊâÄÊúâÁöÑÂêçÂ≠óÊúâÂ∞è #‰ªÄ‰πàÁöÑ‰∫∫ÂêçÈÉΩÂÆö‰πâÁöÑÔºåÊàëÂ¶Ç‰ΩïËÆ©ÊâÄÊúâÂè´@‰∫∫## Â∞±ËÉΩÂåπÈÖçÂà∞Â∞è ÂêéÈù¢ÁöÑÂÖ®ÈÉ®‰∫∫ÂêçÈÉΩÂàáÂØπÔºü    

"
Â¢ûÂä†Ê†∏ÂøÉÊï∞ÊçÆÔºåÂà∞40Â§öMÂêéÔºåÂ§ßÊ¶ÇÊòØ100‰∏áÊù°ËØçÊù°ÂêéÔºåÂá∫Áé∞ java.lang.OutOfMemoryError: Java heap space,"‰Ω†Â•ΩÔºåÊú¨‰∫∫Êù•Ëá™Âåó‰∫¨Ëà™Á©∫Ëà™Â§©Â§ßÂ≠¶‰∏ÄÂêçÂ≠¶ÁîüÔºåÂèëÁé∞Á®ãÂ∫èÂèØËÉΩÂ≠òÂú®ÁöÑ‰∏Ä‰∏™BUGÔºåÊÉ≥ËØ∑Êïô‰∏Ä‰∏ã„ÄÇ‰∏∫‰ΩïÂ¢ûÂä†Ê†∏ÂøÉÊï∞ÊçÆÔºåÂà∞40Â§öMÂêéÔºåÂ§ßÊ¶ÇÊòØ100‰∏áÊù°ËØçÊù°ÂêéÔºåÂá∫Áé∞ java.lang.OutOfMemoryError: Java heap space
![default](https://user-images.githubusercontent.com/17618881/29923940-c1204f9c-8e8d-11e7-887f-0422be6bcfe3.png)

ËØ∑ÈóÆ‰Ω†ÊòØÂê¶ÈÇ£ÈáåÂÜôÈîôÔºüJAVAÂ∫îËØ•‰∏ç‰ºöËøô‰πàÁÉÇÁöÑÔºåÊ∏¥ÊúõËÉΩÂæóÂà∞‰Ω†ÁöÑÂõûÁ≠î„ÄÇË∞¢Ë∞¢
"
hanlpËÆ°ÁÆóÂêå‰πâËØçÁõ∏‰ººÂ∫¶‰∏≠ÔºåÂêå‰∏ÄÁ±ªÂà´ÁöÑËØçÁõ∏‰ººÂ∫¶‰∏∫‰ªÄ‰πà‰∏çÈ´ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
ÊàëÂú®ËÆ°ÁÆóÂêå‰∏ÄÁ±ª‰∏≠Âá∫Áé∞ÁöÑ‰∏§‰∏™ËØç‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Êó∂ÔºåÊú¨‰ª•‰∏∫Áõ∏‰ººÂ∫¶‰ºöÂæàÈ´òÔºåËÄåÂÆûÈôÖËÆ°ÁÆóÁªìÊûúÂç¥‰∏çÁêÜÊÉ≥„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊØîÂ¶ÇËØ¥ÔºåÊàëËÆ°ÁÆó‚ÄúÂ≠§‚ÄùÂíå‚ÄúÂØ°‰∫∫‚Äù‰∏§‰∏™ËØçÁöÑÁõ∏‰ººÂ∫¶Êó∂ÔºåÂèëÁé∞Áõ∏‰ººÂ∫¶‰∏çÂ§üÔºåÊâÄ‰ª•ÊàëÊúâÁÇπÁñëÈóÆÔºåÂΩì‰∏Ä‰∏™ËØçÂ≠òÂú®Â§ö‰∏™Á±ªÂà´Êó∂ÔºåÂØπ‰∫éËØ•Á±ªÂà´ÈÄâÊã©ÁöÑÊú∫ÁêÜÊòØ‰ªÄ‰πà?
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
String[] wordArray = new String[]
            {
                ""Â≠§"", ""ÂØ°‰∫∫""
            };
        System.out.printf(""%-5s\t%-5s\t%-10s\t%-5s\n"", ""ËØçA"", ""ËØçB"", ""ËØ≠‰πâË∑ùÁ¶ª"", ""ËØ≠‰πâÁõ∏‰ººÂ∫¶"");
        for (String a : wordArray) {
            for (String b : wordArray) {
                System.out.printf(""%-5s\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
            }
        }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ËØçA   	ËØçB   	ËØ≠‰πâË∑ùÁ¶ª      	ËØ≠‰πâÁõ∏‰ººÂ∫¶
Â≠§    	Â≠§    	0              	1.0000000000
Â≠§    	ÂØ°‰∫∫   	0              	1.0000000000
ÂØ°‰∫∫   	Â≠§    	0              	1.0000000000
ÂØ°‰∫∫   	ÂØ°‰∫∫   	0              	1.0000000000
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ËØçA   	ËØçB   	ËØ≠‰πâË∑ùÁ¶ª      	ËØ≠‰πâÁõ∏‰ººÂ∫¶
Â≠§    	Â≠§    	0              	1.0000000000
Â≠§    	ÂØ°‰∫∫   	28319444208    	0.6188445149
ÂØ°‰∫∫   	Â≠§    	28319444208    	0.6188445149
ÂØ°‰∫∫   	ÂØ°‰∫∫   	0              	1.0000000000
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‰Ω†Â•ΩÔºåËØ∑ÈóÆËøô‰∏™È°πÁõÆÊúâÊ≤°ÊúâËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÂ≠êÈ°πÁõÆÊàñËÄÖÊîπ‰∏∫ËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑËÆ°ÂàíÂíåÈ°πÁõÆÊé®ËçêÂêóÔºü,‰Ω†Â•ΩÔºåËØ∑ÈóÆËøô‰∏™È°πÁõÆÊúâÊ≤°ÊúâËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÂ≠êÈ°πÁõÆÊàñËÄÖÊîπ‰∏∫ËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑËÆ°ÂàíÂíåÈ°πÁõÆÊé®ËçêÂêó
ËÉΩÂ§üÂ∞ÜËØÜÂà´Âá∫Êù•ÁöÑÊú∫ÊûÑÂêçÁß∞ËøõË°åÊèêÂèñÂêóÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÊàëÁöÑÈóÆÈ¢ò
ÊÇ®Â•ΩÔºÅÊàëÂú®‰ΩøÁî®ÊÇ®ÁöÑHanLPÁöÑÊó∂ÂÄôÂèëÁé∞Êú∫ÊûÑÂëΩÂêçËØÜÂà´ÁéáÂæàÈ´ò„ÄÇ
‰ΩÜÊòØÊúâ‰ªÄ‰πàÂ•ΩÁöÑÊñπÊ≥ïÂ∞ÜËØÜÂà´Âá∫Êù•ÁöÑÊú∫ÊûÑÂêçÁß∞ÊèêÂèñÂá∫Êù•Ôºü





"
solr‰∏≠Âº∫Âà∂ËØ•Ê¨°ËØ∑Ê±Ç‰∏ç‰ΩøÁî®ÂàÜËØç,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•ÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

Âú®solrÊü•ËØ¢ÁöÑËøáÁ®ã‰∏≠ÔºåÊàëÊÉ≥Âº∫Âà∂ÊüêÊü•ËØ¢Â≠óÊÆµËØ•Ê¨°ËØ∑Ê±Ç‰∏ç‰ΩøÁî®ÂàÜËØçÔºå‰∏ÄÁõ¥Ê≤°ÊúâÊâæÂà∞Â∫îËØ•‰øÆÊîπÂì™Èáå„ÄÇ

‰æãÂ¶ÇÔºö
solrÂú®Êü•ËØ¢Êó∂Ôºå‰ºöÂØπ‰º†ÂÖ•ÁöÑÂÄºÂàÜËØçÔºå ‰∏≠ÂõΩ‰∫∫ÊúâÂèØËÉΩ‰ºöË¢´ÂàÜÊàê‚Äú‰∏≠ÂõΩ‚ÄùÔºå‚ÄúÂõΩ‰∫∫‚ÄùÔºå‚Äú‰∏≠ÂõΩ‰∫∫‚Äù„ÄÇ
ÂåÖÂê´ÁùÄ‰∏â‰∏™Áü≠ËØ≠ÁöÑËØ≠Âè•ÈÉΩ‰ºöË¢´ÊêúÂà∞„ÄÇÂº∫Âà∂Ë¶ÅÊ±Çsolr‰∏çÂàÜËØçÔºåÁõ¥Êé•ÂåπÈÖç‰∏≠ÂõΩÂ∞±Â•Ω

"
Ê†∏ÂøÉËØçÂÖ∏CoreNatureDictionary.txtÂä†ËΩΩÂ§±Ë¥•,"

Âú®‰ΩøÁî®hanlpÂàÜËØçÊó∂Ôºå‰∏ÄÁõ¥ÊòæÁ§∫Ê†∏ÂøÉËØçÂÖ∏CoreNatureDictionary.txtÂä†ËΩΩÂ§±Ë¥•Ôºå‰ªîÁªÜÁ°ÆËÆ§Ëøáhanlp.properties‰∏≠ÁöÑË∑ØÂæÑÔºåÂä†ËΩΩ‰∫ÜÁõ∏ÂÖ≥jarÊñá‰ª∂Ôºå‰ªçÁÑ∂Êó†Ê≥ïÊâßË°åÁ®ãÂ∫è„ÄÇ 
"
ÂÖ≥‰∫élicenseÈóÆÈ¢òÔºåhankcs ‰Ω†Â•ΩÔºåÊàëÊúâ‰∏Ä‰∏™ÂàÜËØçÈ°πÁõÆÔºå‰ΩøÁî®‰∫ÜhanLPÂàÜËØçÁÆóÊ≥ïÂíå‰ª£Á†Å,"hankcs ‰Ω†Â•ΩÔºåÊàëÊúâ‰∏Ä‰∏™ÂàÜËØçÈ°πÁõÆÔºå‰ΩøÁî®‰∫ÜhanLPÂàÜËØçÁÆóÊ≥ïÂíå‰ª£Á†Å„ÄÇ

‰∏ªË¶ÅËøõË°å‰∫ÜÂ§ßÈáèÁöÑÈáçÊûÑÂíåÁªìÊûÑÁöÑË∞ÉÊï¥Ôºå‰øÆÊîπÈù¢ÁßØËøáÂ§ßÊ≤°Ê≥ïÊèê‰∫§pull 

Â¶ÇÊûú‰πüÂú®github‰∏äÂºÄÊ∫êÔºåÊòØÂê¶ÂèØ‰ª•

Â¶ÇÊûúÂèØ‰ª•ÔºåÂú®licenseÊÄé‰πàÂ£∞ÊòéÔºåÂõ†‰∏∫ÊàëÁúã‰πãÂâçÊÇ®ÁöÑÊñáÊ°£ÂÜôÊòé‰∫ÜÁâàÊùÉÂíå‰∏äÊµ∑ÊûóÊ∫êÂÖ¨Âè∏ÊúâÂÖ≥ÔºåÊòØ‰∏çÊòØÂΩ±ÂìçlicenseÈÄâÊã©"
Êô∫ËÉΩÊé®ËçêÁÆóÊ≥ïÂéüÁêÜÊòØ‰ªÄ‰πàÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.2.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.2.8


## ÊàëÁöÑÈóÆÈ¢ò

1.‰ΩúËÄÖÂ§ß‰∫∫ÔºÅÊàëÁúãËøáHanLPÁöÑÊñáÊ°£1.1.2ÔºåËøôÈáåÂØπÊô∫ËÉΩÊé®ËçêÁöÑÂéüÁêÜÊ≤°Êúâ‰ªãÁªçÔºåËÉΩÂõûÂ§ç‰∏Ä‰∏ãÊòØÁî®‰ªÄ‰πàÁÆóÊ≥ïÂÅöÁöÑÂêóÔºüÊàñËÄÖÂéüÁêÜÂêçÂ≠óÊàñËÄÖÈìæÊé•‰ªÄ‰πàÈÉΩË°å
2.‰ΩúËÄÖÂ§ß‰∫∫ÔºåÊàë‰πüÊù•ËØ¥‰∏™È¢òÂ§ñËØùÔºåÂ∏åÊúõÊÇ®ËÉΩÁúãËßÅÔºå‰πãÂâçÂú®Âà´ÁöÑissue‰∏≠ÁúãÂà∞ËøáÊÇ®ËØ¥ÂæàÂ§ö‰∫∫ÈóÆÈóÆÈ¢ò‰πãÂâçÈÉΩÊ≤°ÊúâÂ•ΩÂ•ΩÁúãÊñáÊ°£ÔºåÂÖ∂ÂÆûÊàëÁúüÁöÑÊúâËÆ§ÁúüÁúãÊñáÊ°£ÔºåÂπ∂‰∏îÊääÊÇ®ÈôÑÁöÑÈìæÊé•ÈÉΩÊâìÂºÄÁúã‰∫ÜÔºå‰ΩÜÊòØÊàëÊØîËæÉÂÖ≥Ê≥®ÁÆóÊ≥ïÔºåÂ∏åÊúõËÉΩÂºÑÊáÇÁÆóÊ≥ïÔºåÁÑ∂ÂêéÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÂéª‰øÆÊîπÁ®ãÂ∫è„ÄÇÊàëÁé∞Âú®ÁúãÂà∞Êô∫ËÉΩÊé®ËçêËøôÈáåÔºåÁúüÁöÑÊ≤°ÊúâÂÜôÁÆóÊ≥ïÊòØ‰ªÄ‰πà„ÄÇ„ÄÇ„ÄÇ
3.ÊàëÁöÑÂ∫üËØùÂÆå‰∫ÜÔºå‰∏äÂ§©‰øù‰ΩëÔºå‰ΩúËÄÖÂ§ßÂ§ßËÉΩÁúãÂà∞ÊàëÁöÑÁïôË®Ä


"
ËØ≠‰πâË∑ùÁ¶ª‰∏≠ÁöÑÂêå‰πâËØçËØçÊûóÊÄé‰πàÊÄé‰πà‰øÆÊîπ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.2.8
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.2.8


## ÊàëÁöÑÈóÆÈ¢ò

ÊÇ®Â•ΩÔºåÊàëÂú®‰ΩøÁî®HanLP‰∏≠ÁöÑËØ≠‰πâË∑ùÁ¶ªÊó∂ÂèëÁé∞ÊÇ®ÊòØÂà©Áî®Âêå‰πâËØçËØçÊûóÁöÑ‰∏ÄÁª¥Êò†Â∞ÑÂÆûÁé∞ÁöÑÔºåÂπ∂‰∏îÂú®‰ª•ÂæÄÁöÑissue‰∏≠ÁúãÂà∞ÊÇ®ËØ¥ÔºåÂèØ‰ª•Ëá™Â∑±Ê†πÊçÆÂêå‰πâËØçËØçÊûóÁöÑÊ†ºÂºèÂ¢ûÂä†Êñ∞ËØçÔºåÈÇ£ÊàëÊÉ≥Áü•ÈÅìÊÄé‰πàÊ∑ªÂä†ÔºüÊ∑ªÂä†ÂÆåËøòË¶ÅÊåâÁÖßÊÇ®ÁöÑÊò†Â∞ÑÊñπÂºèËøõË°åÊò†Â∞ÑÂØπÂêóÔºüËÉΩÂê¶ÂÉèÂàÜËØçÂäüËÉΩÁöÑcustomDictionary‰∏ÄÊ†∑Ôºü


"
Ê∑ªÂä†‰∫ÜÁ±ªÂ∫èÂàóÂåñÊé•Âè£ÔºåÂèØÊîØÊåÅsparkÈõÜÁæ§Ë∞ÉÁî®,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->
Ê∑ªÂä†‰∫ÜÁ±ªÂ∫èÂàóÂåñÊé•Âè£ÔºåÂèØ‰ª•Âú®SparkÈõÜÁæ§‰∏äÂπ∂Ë°åÂåñ‰ΩøÁî®HanLPÔºõ
‰ΩøÁî®ÊñπÊ≥ï:
1. Âú®com.hankcs.hanlp.utility.Config‰∏≠‰øÆÊîπÊîæÁΩÆËØçÂÖ∏ÁöÑhdfsË∑ØÂæÑ
2. ÈáçÊñ∞ÊâìjarÂåÖ
3. Ë∞ÉÁî®ÊñπÊ≥ï
```
import com.hankcs.hanlp.Config;
import com.hankcs.hanlp.seg.Viterbi.ViterbiSegment;
import com.hankcs.hanlp.corpus.io.IIOAdapter
import java.io._;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.net.URI;
 
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }   
 
    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}

val a = sc.parallelize(Seq(""‰∫íËÅîÁΩëÊêúÁ¥¢ÁÆóÊ≥ï‰πüÊàê‰∏∫ÂΩì‰ªäÁöÑÁÉ≠Èó®ÊñπÂêë„ÄÇ"",""ÁÆóÊ≥ïÂ∑•Á®ãÂ∏àÈÄêÊ∏êÂæÄ‰∫∫Â∑•Êô∫ËÉΩÊñπÂêëÂèëÂ±ï„ÄÇ""))
val test = a.map(e=> {
    Config.IOAdapter = new HadoopFileIoAdapter(); 
    val vs = new ViterbiSegment();
    vs.seg(e)
    })

test take 10
```


## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->
#588 "
Â¶Ç‰ΩïÂè™‰ΩøÁî®Ëá™ÂÆö‰πâÁîüÊàêÁöÑËØçÂÖ∏ÂàÜËØçÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
HancksÔºå‰Ω†Â•ΩÔºö
       Âõ†‰∏∫‰∏öÂä°ÈúÄË¶ÅÔºåÁõÆÂâçÂè™ÊÉ≥‰ΩøÁî®Ëá™Â∑±ÁîüÊàêÁöÑËØçÂÖ∏ËøõË°åÂàÜËØçÔºåËØ∑ÈóÆÊúâÂÖ≥Èó≠Á≥ªÁªüËØçÂÖ∏ÁöÑÊñπÊ≥ïÂêóÔºü



"
ÁÆÄÁπÅËΩ¨Êç¢ÈîôËØØÔºöÂßìÂêç‚ÄúÊ¢ÅÈπè‚ÄùËΩ¨‰∏∫ÁπÅ‰ΩìÂêéÂèòÊàê‰∫Ü‚ÄùÊ®ëÈπè‚Äù,"ÁâàÊú¨Ôºöportable-1.3.4

"
ÊÄéÊ†∑ÊãøÂèñÂåπÈÖçÂà∞ÁöÑÊâÄÊúâËØçÊÄßÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
ÊàëËá™ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ËØçÊÄß‚ÄúÊ∑±Âú≥‚Äù‰∏∫ shenzhenÔºåËÄåÊ∑±Âú≥ÁöÑÂü∫Á°ÄËØçÂ∫ìÈáåÁöÑËØçÊÄßÊòØ ns Âú∞ÂêçËØçÊÄßÔºåÊàë‰ª£Á†ÅÈáåÊãøÂèñÊ∑±Âú≥ÁöÑËØçÊÄßÂè™ËÉΩÊãøÂà∞ shenzhen ËØçÊÄß‰∫ÜÔºåÊúâÂäûÊ≥ïËÉΩÊãøÂà∞Ê∑±Âú≥Ëøô‰∏™ËØçÊâÄÂåπÈÖçÁöÑÊâÄÊúâËØçÊÄßÂêóÔºü

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
       System.out.println(HanLP.segment(""Ê∑±Âú≥""));
        CustomDictionary.insert(""Ê∑±Âú≥"",""shenzhen 1"");
        System.out.println(HanLP.segment(""Ê∑±Âú≥""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Ê∑±Âú≥/ns,/shenzhen]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Ê∑±Âú≥/ns]
[Ê∑±Âú≥/shenzhen]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
OrganizationRecognize,"ËØ∑Êïô‰∏Ä‰∏ãÔºåÂºÄÂêØOrganizationRecognizeÂêé‰∏∫‰ªÄ‰πàË¶ÅÈáçÊñ∞ÁîüÊàêÂàáÂàÜÂõæÔºü Êàë‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØçÔºåÂú®ËøôÊ¨°ÈáçÊñ∞ÁîüÊàêÂêéÂ∞±Â§±Êïà‰∫Ü„ÄÇ

"
root ÂÜôÁõ∏ÂØπË∑ØÂæÑ‰ΩÜÊòØËØªÂèñÊñá‰ª∂‰∏ÄÁõ¥ÊúâÂºÇÂ∏∏,"Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.3.4    master

ÊàëÁöÑÈóÆÈ¢ò
ÊàëÊääÂÜôÂ•ΩÁöÑÁ®ãÂ∫èÈõÜÊàêÂà∞webÁ≥ªÁªü‰∏äÔºådataÊîæÂú®‰∫Üweb/asset/HanLP-master‰∏ãÈù¢ÔºåÊúâÁúãÂà∞‰∏Ä‰∏™issueÈáåÊèêÂà∞ÁöÑÔºåÊâÄ‰ª•ÊàëÂÜô‰∫ÜSystem.out.println(new File(""."").getAbsolutePath());ËæìÂá∫ÁöÑÊòØD:\jboss\jboss-4.2.2.GA - FJ\bin\.   ÊàëÂÜôÊàêroot=../assets/HanLP-master/ÊòØ‰∏çË°åÁöÑÔºåÈÇ£ËØ•ÊÄé‰πàÂÜôÂë¢

 ÊúüÊúõËæìÂá∫
ËØªÂèñdataËøõË°åÂàÜËØç

ÂÆûÈôÖËæìÂá∫
ËØªÂèñ../assets/HanLP-master/data/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException:




"
ÂÖ≥‰∫éÂîêËØó‰∫∫ÂêçÁöÑÁÆÄÁπÅ„ÄÅÊãºÈü≥‰ΩìËΩ¨Êç¢‰ª•ÂèäconvertToPinyinStringÂáΩÊï∞ÁöÑÂª∫ËÆÆ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

1. ÂîêËØóËØó‰∫∫‰∫∫ÂêçÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÂá∫Áé∞‰π±Á†Å
2. Êüê‰∫õ‰∫∫Âêç‰ºº‰πéÂèØ‰∏çËΩ¨
3.Â§öÈü≥Â≠ó‰∫∫Âêç‰∏çÊ≠£Á°Æ
4.convertToPinyinStringÂáΩÊï∞Âª∫ËÆÆ


 

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. ‰∏ãËΩΩjarÔºåÂä†ÂÖ•Â∑•Á®ãÔºåËøêË°å‰ª£Á†Å
2. ÁÑ∂Âêé‰ªé https://github.com/chinese-poetry/chinese-poetry Ëé∑Âæójson
3. Êé•ÁùÄËΩ¨Êç¢jsonÔºåÊâìÂç∞ËØó‰∫∫‰∫∫Âêç„ÄÅÁÆÄ‰Ωì„ÄÅÊãºÈü≥

### Ëß¶Âèë‰ª£Á†Å

```
     Log.d(TAG, HanLP.convertToSimplifiedChinese(""Ë£¥Ë´¥""));
    Log.d(TAG, HanLP.convertToSimplifiedChinese(""ÊãæÂæó""));
    List<Pinyin> pinyinList = HanLP.convertToPinyinList(""ÊõæÊâà"") ;
    String pyname="""";
    for (Pinyin pinyin : pinyinList)
    {
      pyname+= pinyin.getPinyinWithToneMark()+"" "";
    }
    Log.d(TAG, pyname);
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
Ë£¥Ë´¥ ÊãæÂæó zeng hu
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Ë£¥ÔøΩÔøΩ   ÂçÅÂæó c√©ng h√π 
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

Âè¶Â§ñ‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàconvertToPinyinString(String text, String separator, boolean remainNone)Ëøô‰πàËÆæËÆ°ÔºåËÉΩ‰∏çËÉΩÂä†‰∏™convertToPinyinString(String text, int start,int len)? Ë∞¢Ë∞¢„ÄÇ"
Ëá™ÂÆö‰πâËØçÂÖ∏Ê≤°ÊúâËµ∑Âà∞‰ΩúÁî®Ôºü,"
## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
 ÊàëÊòØÈÄöËøámavenÁõ¥Êé•ÂºïÂÖ•ÁöÑ
   <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.3.4</version>
   </dependency>

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

Ëá™ÂÆö‰πâËØçÂÖ∏‰∏çËµ∑‰ΩúÁî®

## Â§çÁé∞ÈóÆÈ¢ò

Ëá™ÂÆö‰πâËØçÂÖ∏‰∏çËµ∑‰ΩúÁî®

### Ê≠•È™§

```
ÊàëÂú®ÈÖçÁΩÆÊñá‰ª∂hanlp.properties‰∏≠Âä†ÂÖ•‰∫Ü
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CustomDictionary/Ëá™ÂÆö‰πâËØçÂÖ∏.txt
```
Ëá™ÂÆö‰πâËØçÂÖ∏Ê†ºÂºèÂ¶Ç‰∏ãÔºö

```
ÁΩóÊ∞èÂ©¥ÂÑøÈÖçÊñπÁ≤â n 1000
ÊåÇËä±Â§ßÂ§¥Ëèú n 1000
ÈªÑÊØõÁ±Ω n 1000
ÈùíË±Ü n 1000
ÂÑøÁ´•Ëê•ÂÖªÈ•ºÂπ≤ n 1000
Ê±§Ëèú n 1000
ÈùíËêùÂçú n 1000
```

ÂàÜËØçÁªìÊûúÔºö

### Ëß¶Âèë‰ª£Á†Å

```
String str = ""ÁΩóÊ∞èÂ©¥ÂÑøÈÖçÊñπÁ≤âÊòØ‰ªÄ‰πà?"";
        CoNLLSentence sentence = new NeuralNetworkDependencyParser().enableDeprelTranslator(false).parse(str);
        // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s--%s --(%s)--> %s\n"",word.ID, word.LEMMA, word.DEPREL, word.CPOSTAG);
        }
```
### ÊúüÊúõËæìÂá∫

```
1--ÁΩóÊ∞èÂ©¥ÂÑøÈÖçÊñπÁ≤â --(SBV)-->n
2--ÊòØ --(HED)--> v
3--‰ªÄ‰πà --(VOB)--> r
4--? --(WP)--> wp
```

### ÂÆûÈôÖËæìÂá∫

```
1--ÁΩóÊ∞è --(ATT)--> nz
2--Â©¥ÂÑø --(ATT)--> n
3--ÈÖçÊñπ --(ATT)--> n
4--Á≤â --(SBV)--> a
5--ÊòØ --(HED)--> v
6--‰ªÄ‰πà --(VOB)--> r
7--? --(WP)--> wp
```



"
solr‰∏≠ÁâπÊÆäÁ¨¶Âè∑ÂØºËá¥‰∫ÜÈ´ò‰∫Æ‰ΩçÁΩÆÂÅèÂ∑Æ,"## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.3.4
hanlp-solr-plugin-1.1.2
solrÁâàÊú¨Ôºö6.3.0
tomcatÁâàÊú¨Ôºö8.5.15

## ÊàëÁöÑÈóÆÈ¢ò
ÈÉ®ÂàÜÁâπÊÆäÁ¨¶Âè∑ÂØºËá¥È´ò‰∫Æ‰ΩçÁΩÆÊúâÂèòÂ∑Æ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
ÈÖçÁΩÆÂ•ΩËØçÂÖ∏ÔºåsolrcloudÂ∫îÁî®ÊàêÂäüÔºåÊü•ËØ¢ÂàÜËØçÔºåËÆæÁΩÆÈ´ò‰∫ÆÔºåÈÄöËøásolrÁöÑhttpÊé•Âè£ÁúãÂà∞ÔºåÈ´ò‰∫ÆÊ†áËÆ∞ÁöÑ‰ΩçÁΩÆ‰∏çÂØπ
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

### ÂÆûÈôÖËæìÂá∫
Ê≠£Â∏∏ÊÉÖÂÜµÂ∫îËØ•ÊòØÊääÊïÖ‰∫ãÈ´ò‰∫ÆÔºå‰ΩÜÊòØ‰∏ÄÊó¶ÊúâÔºüÁöÑÂ≠òÂú®ÔºåÂ∞±‰ºöÂØºËá¥È´ò‰∫Æ‰ΩçÁΩÆÂèòÂåñÔºåÊ†áËÆ∞Â§±Ë¥•ÔºåÂ¶Ç‰∏ãhighlightingÂ≠óÊÆµÁöÑËøîÂõûÔºöid=4444444444444ÊòØÊ≠£Á°ÆÁöÑÔºåÂÖ∂ÂÆÉÈÉΩÊòØÂõ†‰∏∫ÁâπÊÆäÁ¨¶Âè∑ÔºüËÄåÂØºËá¥ÂàÜËØçÂá∫Áé∞ÂÅèÂ∑Æ„ÄÇ
ÁõÆÂâçÊµãËØïÂèëÁé∞Ôºü‰ºöÂØºËá¥ËøôÁßçÊÉÖÂÜµÔºåÊöÇÊó∂Ê≤°ÊúâÂèëÁé∞ÂÖ∂ÂÆÉÂ≠óÁ¨¶ÂØºËá¥ËøôÁßçÊÉÖÂÜµ




<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
{
  ""responseHeader"":{
    ""zkConnected"":true,
    ""status"":0,
    ""QTime"":2011,
    ""params"":{
      ""q"":""name:ÊïÖ‰∫ã"",
      ""hl"":""on"",
      ""indent"":""on"",
      ""hl.fl"":""name"",
      ""wt"":""json"",
      ""_"":""1501643817193""}},
  ""response"":{""numFound"":3,""start"":0,""maxScore"":0.6395861,""docs"":[
      {
        ""id"":""22222222222"",
        ""name"":""È≠ØË±´ÊúâÁ¥Ñ: ÔºüÂá∫‰Ω†ÁöÑÊïÖ‰∫ã"",
        ""_version_"":1574587208305737728},
      {
        ""id"":""33333333333"",
        ""name"":""È≠ØË±´ÊúâÁ¥Ñ:ÔºüÂá∫‰Ω†ÁöÑÊïÖ‰∫ã"",
        ""_version_"":1574587388861087744},
      {
        ""id"":""4444444444444"",
        ""name"":""È≠ØË±´ÊúâÁ¥Ñ:Âá∫‰Ω†ÁöÑÊïÖ‰∫ã"",
        ""_version_"":1574587451757821952}]
  },
  ""highlighting"":{
    ""22222222222"":{
      ""name"":[""È≠ØË±´Êúâ<em>Á¥Ñ:</em> ÔºüÂá∫‰Ω†ÁöÑÊïÖ‰∫ã""]},
    ""33333333333"":{
      ""name"":[""È≠ØË±´Êúâ<em>Á¥Ñ:</em>ÔºüÂá∫‰Ω†ÁöÑÊïÖ‰∫ã""]},
    ""4444444444444"":{
      ""name"":[""È≠ØË±´ÊúâÁ¥Ñ:Âá∫‰Ω†ÁöÑ<em>ÊïÖ‰∫ã</em>""]}}}
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÂÖ∑‰ΩìÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
managed-schema‰∏≠ÈÖçÁΩÆ
```
<field name=""name"" type=""hanlp_cn"" indexed=""true"" stored=""true"" multiValued=""false"" />

   <fieldType name=""hanlp_cn"" class=""solr.TextField"">
      <analyzer type=""index"" enableCustomDictionary=""true"">
          <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" 
	             enableIndexMode=""true"" 
		     enableCustomDictionary=""true"" 
		     customDictionaryPath=""Ë∑ØÂæÑÈöêËóè"" 
		     stopWordDictionaryPath=""Ë∑ØÂæÑÈöêËóè""
		     enableTraditionalChineseMode=""true""/>
          <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
          <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
      <analyzer type=""query"">
          <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" 
		    enableIndexMode=""false""
		    enableCustomDictionary=""true""
                    customDictionaryPath=Ë∑ØÂæÑÈöêËóè""
                    stopWordDictionaryPath=""Ë∑ØÂæÑÈöêËóè""
                    enableTraditionalChineseMode=""true""/>
          <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" />
          <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms.txt"" ignoreCase=""true"" expand=""true""/>
          <filter class=""solr.LowerCaseFilterFactory""/>
      </analyzer>
  </fieldType>

```
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
1.3.4ÁâàÊú¨Ëá™ÂÆö‰πâËØçÂÖ∏ÂèëÁé∞ÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.2.8


## ÊàëÁöÑÈóÆÈ¢ò
ÂΩìÊàë‰ΩøÁî®1.3.4ÁâàÊú¨ÁöÑÊó∂ÂÄôÔºåÂú®hanlp.properties‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑË∑ØÂæÑÔºàÂè™ÂÜô‰∏Ä‰∏™CustomDictionaryPathÈÖçÁΩÆÔºåÂÖ∂‰ªñÈÉΩ‰∏çÂÜôÔºâÔºåÂ∞±‰ºöÊä•Âá∫Êâæ‰∏çÂà∞Ê†∏ÂøÉËØçÂÖ∏ÁöÑÈîôËØØÔºåÂΩìÊç¢Âà∞1.2.8ÁâàÊú¨‰πãÂêéÔºåÂêåÊ†∑ÁöÑÈÖçÁΩÆÊñá‰ª∂‰ΩøÁî®Ëµ∑Êù•Ê≤°Êúâ‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ‰∏çÁü•ÈÅìÊòØÊàëÈÖçÁΩÆÁöÑÂéüÂõ†ÔºåËøòÊòØÊú¨Ë∫´È°πÁõÆÁöÑbug
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà
ÊàëÁöÑÈÖçÁΩÆÊñá‰ª∂ÂÜÖÂÆπ‰∏∫Ôºö
root=F:/hanlp/
CustomDictionaryPath=data/dictionary/custom/xxx.txt;
2. ÁÑ∂Âêé
ËøêË°åÁ®ãÂ∫è‰ºöÊä•Âá∫
Ë≠¶Âëä: ËØªÂèñF:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂÖ´Êúà 08, 2017 4:20:46 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreDictionary load
Ë≠¶Âëä: Ê†∏ÂøÉËØçÂÖ∏F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt‰∏çÂ≠òÂú®ÔºÅjava.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂÖ´Êúà 08, 2017 4:20:46 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
‰∏•Èáç: Ê†∏ÂøÉËØçÂÖ∏F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txtÂä†ËΩΩÂ§±Ë¥•
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËÉΩ‰∏çËÉΩÂÄüÈòÖ‰∏Ä‰∏ã‰Ω†‰ª¨ÁîüÊàêCRFSegmentModel.txt.binÂéüÊñá‰ª∂CRFSegmentModel.txt,"ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

 1„ÄÅÊàëÁé∞Âú®ËØïÁùÄÊÉ≥Âä†ÂÖ•ËØçÊÄßÔºåÈáçÊñ∞ÊûÑÈÄ†ÁâπÂæÅÊ®°ÁâàÔºåÂèØ‰∏çÂèØ‰ª•ÂèÇËÄÉ‰∏Ä‰∏ã‰Ω†‰ª¨ÁöÑ
"
Â¢ûÂä†‰∏§‰∏™Êñ∞ÁöÑÊñáÊú¨ÊëòË¶ÅÊäΩÂèñÊñπÊ≥ïÔºåÂú®ÊäΩÂèñÊñáÊú¨ÊëòË¶ÅÊó∂ÊåáÂÆöÂè•Â≠êÂàÜÂâ≤Á¨¶ÔºåÊîπÂèòÈªòËÆ§ÁöÑÂè•Â≠êÂàÜÂâ≤Á¨¶,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
ÂéüÊù•ÁöÑÊñáÊú¨ÊëòË¶ÅÊäΩÂèñÊñπÊ≥ïÊó†Ê≥ïÊåáÂÆöÂè•Â≠ê‰πãÈó¥ÁöÑÂàÜÂâ≤Á¨¶ÔºåÂõ†‰∏∫ÈÄóÂè∑ÊòØÈªòËÆ§ÂàÜÂâ≤Á¨¶‰πã‰∏ÄÔºå‰ºöÈÄ†ÊàêÊäΩÂèñÁªìÊûúËØ≠‰πâÁ†¥Á¢é„ÄÇÂ¢ûÂä†‰∏§‰∏™Êñ∞ÁöÑÊñáÊú¨ÊëòË¶ÅÊäΩÂèñÊñπÊ≥ïÔºåÂú®ÊäΩÂèñÊñáÊú¨ÊëòË¶ÅÊó∂ÊåáÂÆöÂè•Â≠êÂàÜÂâ≤Á¨¶„ÄÇÂéüÊù•ÁöÑÊäΩÂèñÊñπÊ≥ï‰ªçÁÑ∂‰ºöÈááÁî®ÈªòËÆ§ÁöÑÂè•Â≠êÂàÜÂâ≤Á¨¶„ÄÇ

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue
Êó†„ÄÇ
ÊµãËØïÊñπÊ≥ï‰∏∫Ôºösrc/test/java/com/hankcs/test/other/TestExtractSummary.java
<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Áé∞Âú®elasticsearch Â¶ÇÊó•‰∏≠Â§©ÔºåË∞ÅËÉΩÊèê‰æõ‰∏Ä‰∏™ elasticsearch‰∏é HanLPÁªìÂêàÁöÑÂàÜËØçÊèí‰ª∂ÔºüË∞¢Ë∞¢,
ÂÖ≥ÈîÆËØçÊèêÂèñÂèØËÉΩ‰ºöÊääËá™ÂÆö‰πâËØçÊÄßÁöÑËØçÂÖ®ÈÉ®ËøáÊª§Êéâ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂÖ≥ÈîÆËØçÊèêÂèñÊó∂‰ºöÂéªÊéâÂÅúÁî®ËØçÔºåÂç≥ÈÄöËøá KeywordExtractorÁöÑshouldInclude()ÊñπÊ≥ïÔºå
‰ªª‰ΩïËá™ÂÆö‰πâËØçÊÄßÂ¶ÇÊûúÈ¶ñÂ≠óÊØçÊª°Ë∂≥switchÊù°‰ª∂ÔºåÈÉΩ‰ºöË¢´‰Ωú‰∏∫ÂÅúÁî®ËØçÂéªÊéâÔºåÊó†Ê≥ïÊèêÂèñ‰ªª‰ΩïËá™ÂÆö‰πâËØçÊÄßÁöÑËØçÊ±á‰Ωú‰∏∫ÂÖ≥ÈîÆËØç„ÄÇÊàëÂú®Â§çÁé∞Ê≠•È™§Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑcase„ÄÇ
shouldInclude ÁöÑ‰ª£Á†ÅÂ¶Ç‰∏ã:
```
public boolean shouldInclude(Term term)
    {
        // Èô§ÊéâÂÅúÁî®ËØç
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)  
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (term.word.trim().length() > 1 && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }
```
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->


### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç"", ""mZiDingYi 1111"");
        String msg = ""ÊµãËØï‰∏Ä‰∏ã,ÊàëÊòØËá™ÂÆö‰πâÁöÑËØçÊàëÊòØËá™ÂÆö‰πâÁöÑËØçÊàëÊòØËá™ÂÆö‰πâÁöÑËØçÊàëÊòØËá™ÂÆö‰πâÁöÑËØç,ÊµãËØï‰∏Ä‰∏ã"";
        System.out.println(HanLP.segment(msg));
        System.out.println(TextRankKeyword.getKeywordList(msg, 100));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[ÊµãËØï/vn, ‰∏Ä‰∏ã/m, ,/w, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ,/w, ÊµãËØï/vn, ‰∏Ä‰∏ã/m]
[ÊµãËØï, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÊµãËØï/vn, ‰∏Ä‰∏ã/m, ,/w, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ÊàëÊòØËá™ÂÆö‰πâÁöÑËØç/mZiDingYi, ,/w, ÊµãËØï/vn, ‰∏Ä‰∏ã/m]
[ÊµãËØï]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Âú®ÊèêÂèñÂÖ≥ÈîÆËØçÁöÑËøáÁ®ã‰∏≠ÔºåÊ†πÊçÆËØçÊÄßËøáÊª§ÂæóÂá∫ÁöÑÂÖ≥ÈîÆËØç,"Âú®ÊèêÂèñÂÖ≥ÈîÆËØçÁöÑËøáÁ®ã‰∏≠ÔºåÊàëÊÉ≥Ê†πÊçÆËØçÊÄßËøáÊª§ÔºåÂè™Áïô‰∏ãËØçÊÄß‰∏∫ÂêçËØçÁöÑÂÖ≥ÈîÆËØç„ÄÇ
ÊàëÁöÑÊÉ≥Ê≥ïÊòØÂú®ÂàÜËØçÁöÑÊ≠•È™§‰∏≠ÔºåÂ∞±ÊåâÁÖßËØçÊÄßÂéªËøáÊª§ÔºåÂè™Áïô‰∏ãÂêçËØçÔºå‰ΩÜÊòØÂú®Ê†áÂáÜÂàÜËØçÊ∫êÁ†ÅÈÉ®ÂàÜÔºåÂπ∂Ê≤°ÊúâÊâæÂà∞ÂÖ≥‰∫éËØçÊÄßÁöÑ‰ª£Á†ÅÔºåhankcsËÉΩÂê¶ÊåáÁÇπ‰∏Ä‰∏ãÂ∞èÂºüÂë¢Ôºü‰∏çËÉúÊÑüÊøÄ
"
ÂàÜËØçÁ≤íÂ∫¶ÊòØÂê¶ÂèØ‰ª•ÈÖçÁΩÆ,"hanlpÁöÑÂàÜËØçÁ≤íÂ∫¶ÂèØÂê¶ÈÖçÁΩÆÔºü
ÊØîÂ¶Ç""Âõ¢Ë¥≠ÁΩëÁ´ôÁöÑÊú¨Ë¥®ÊòØ‰ªÄ‰πàÔºü""ËÉΩÂê¶ÈÄöËøáÈÖçÁΩÆÁ≤íÂ∫¶ÔºåÂàÜËØçÁªìÊûú‰øùÂ≠ò‰∏çÂêåÁ≤íÂ∫¶ÁöÑÁªìÊûú‚ÄúÂõ¢Ë¥≠ÁΩëÁ´ô/Âõ¢Ë¥≠/Âõ¢Ë¥≠ÁΩë/ÁΩëÁ´ô/ÁöÑ/Êú¨Ë¥®/ÊòØ/‰ªÄ‰πà‚Äù "
Add docker image links in README,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->
Âø´ÈÄüÈõÜÊàêHanLPÊúçÂä°ÔºåÂÆπÂô®ÂåñÔºåÂæÆÊúçÂä°ÔºåÂáèÂ∞ë‰æùËµñÂíåÈÉ®ÁΩ≤ÊàêÊú¨„ÄÇ




"
Wiki È°πÁõÆ‰ªãÁªçËØ∑Â∏ÆÂøô‰øÆÊîπ‰∏Ä‰∏ã,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

Ë°çÁîüÈ°πÁõÆ‰∏≠Ôºå‰πãÂâçÊàëÊèèËø∞ÁöÑ‰∏çÂáÜÁ°ÆÔºå**Â∫îËØ•Â∞Ü (HanLP Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ for nodejs)  ÊîπÊàê (HanLP Docker Image: Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ)**„ÄÇ

https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE

![image](https://user-images.githubusercontent.com/3538629/28705638-52d03156-73a3-11e7-8111-83bdd253211a.png)

"
ÁåúÊµãÊúÄÂèØËÉΩÁöÑËØçÊÄßÊñπÊ≥ïÊï∞ÁªÑË∂äÁïå,"com.hankcs.hanlp.seg.common.Vertex
 public Nature guessNature() {
        return attribute.nature[0];
    }


Ëøô‰∏™ÊñπÊ≥ïÊï∞ÁªÑË∂äÁïå"
‰øÆÂ§çÊï∞ËØçÂà§Êñ≠ÁöÑÈîôËØØ,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

ÁõÆÂâçÊàë‰ª¨Ê≠£Âú®Âü∫‰∫éHanLPËøõË°åÂàÜËØçÂêéÊï∞ÈáèË°®ËææÂºèËØÜÂà´ÁöÑ‰∫åÊ¨°ÂºÄÂèëÔºå
Âú®ÂºÄÂèë‰∏≠ÂèëÁé∞‰∫ÜHanLPÂü∫Êú¨Êï∞Â≠óËØÜÂà´‰∏≠Â≠òÂú®‰∏çÂ∞ëÈóÆÈ¢òÔºå‰∏îÈóÆÈ¢òÂú®ËæÉ‰∏∫Â∫ïÂ±ÇÁöÑUtilityÂåÖ‰∏≠„ÄÇ
Âõ†Ê≠§ËøõË°å‰∫Ü‰øÆÊîπÂπ∂ÂèëËµ∑ËØ•PR„ÄÇ

## Áõ∏ÂÖ≥issue

ÔºàÊó†Ôºâ
ÂèÇÁÖßTestTextUtility.javaÊµãËØï"
Âú®SparkÂçïÊú∫‰ΩøÁî®Ê≠£Â∏∏ÔºåmapÂà∞ËäÇÁÇπ‰∏ä‰ΩøÁî®Âá∫Áé∞ÈîôËØØ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

@hankcs ÊÇ®Â•ΩÔºåÊåâÁÖßÊÇ®ËØ¥ÁöÑÊñπÊ≥ïÔºå‰øÆÊîπÈÖçÁΩÆ
root=hdfs://localhost:9000/hanlpdata/
IOAdapter=com.xxx.HDFSIOAdapter
ËÄå‰∏îÂú®scalaÈáçÂÜô‰∫ÜÊé•Âè£Ôºö
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }    

    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}
HanLP.Config.IOAdapter = new HadoopFileIoAdapter();

‰πüÂè™ËÉΩÂú®ÂçïÊú∫spark‰∏ãËøêË°åÂàÜËØç
scala> System.out.println(HanLP.segment(""‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®HanLPÊ±âËØ≠Â§ÑÁêÜÂåÖÔºÅ""));
[‰Ω†Â•Ω/vl, Ôºå/w, Ê¨¢Ëøé/v, ‰ΩøÁî®/v, HanLP/nx, Ê±âËØ≠/gi, Â§ÑÁêÜ/vn, ÂåÖ/v, ÔºÅ/w]


ÂàÜÂèëÂà∞ËäÇÁÇπÁöÑÊó∂ÂÄôÂ∞±Âá∫Èîô‰∫ÜÔºåËØ∑ÈóÆ‰Ω†‰ª¨Âú®ÈõÜÁæ§‰∏ä‰ΩøÁî®ÁöÑÊó∂ÂÄôÊòØÂê¶ÈÅáÂà∞ËøáËøôÁßçÈóÆÈ¢òÔºüÈúÄË¶ÅÊ≥®ÊÑèÂì™‰∫õÂùëÂë¢Ôºü
val a = sc.parallelize(Seq(""‰∏≠ÂõΩÁöÑÁ•ûÂ®ÅÂ§™Êπñ‰πãÂÖâËÆ°ÁÆóÊú∫Ë¢´Áî®‰∫éÂ§©Ê∞îÈ¢ÑÊä•"",""Âà∂ËçØÁ†îÁ©∂ÂíåÂ∑•‰∏öËÆæËÆ°Á≠âÈ¢ÜÂüü„ÄÇ""))
val res = a.map(e=>{
        HanLP.segment(e).toString
    })
res take 2 foreach println

Exit code: 255
Stack trace: ExitCodeException exitCode=255:
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:578)
	at org.apache.hadoop.util.Shell.run(Shell.java:489)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:755)
	at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:297)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)"
‰ª£Á†ÅË¥®ÈáèÂ§™Â∑Æ,"Ëøô‰∏™È°πÁõÆÊòØ‰∏™Â•Ω‰∏úË•øÔºåÂèØÊÉú‰ª£Á†ÅË¥®ÈáèÂ§™Â∑Æ
ÁâπÂà´ÊòØ IOËØªÂÜôÁöÑÂú∞ÊñπÔºåÂæàÂ§öinputStreamÔºåoutputStreamÊâìÂºÄ‰∫ÜÂ∞±Ê≤°ÂÖ≥Èó≠ËøáÔºå‰ºöÈÄ†ÊàêËµÑÊ∫êÊ≥ÑÈú≤„ÄÇ

‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÔºåbr.readLine()ÊäõÂºÇÂ∏∏ÂêéÔºåËøô‰∏™ÊµÅÂ∞±Ê∞∏Ëøú‰∏ç‰ºöË¢´ÂÖ≥Èó≠„ÄÇ„ÄÇ„ÄÇ
ÊàëÂ∞ùËØï‰øÆÊîπËøô‰∏™È°πÁõÆÁöÑ‰ª£Á†ÅÔºåÂèëÁé∞ËøôÊ†∑‰ΩéË¥®Èáè‰ª£Á†ÅÂÆûÂú®Â§™Â§öÊ†πÊú¨Êîπ‰∏çËøáÊù•
try
        {
            BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(path), ""UTF-8""));
            while ((line = br.readLine()) != null)
            {
                Map.Entry<String, Map.Entry<String, Integer>[]> args = EnumItem.create(line);
                EnumItem<NR> nrEnumItem = new EnumItem<NR>();
                for (Map.Entry<String, Integer> e : args.getValue())
                {
                    nrEnumItem.labelMap.put(NR.valueOf(e.getKey()), e.getValue());
                }
                valueList.add(nrEnumItem);
            }
            br.close();
        }
        catch (Exception e)
        {
            logger.severe(""ËØªÂèñ"" + path + ""Â§±Ë¥•["" + e + ""]\nËØ•ËØçÂÖ∏Ëøô‰∏ÄË°åÊ†ºÂºè‰∏çÂØπÔºö"" + line);
            return null;
        }"
"""Â§ßÂ≠¶ÂüéÈáå""ÂàÜËØçÈîôËØØÁöÑÈóÆÈ¢ò","## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

ÂØπ‰ª•‰∏ãÂè•Â≠êËøõË°åÂàÜËØç""Ê∑±Âú≥Â§ßÂ≠¶ÂüéÈáåÂêÑÂ§ßÂõΩÂÜÖÂ§ñÁü•ÂêçÈ´òÊ†°ÂêàÂäûÂ≠¶Ê†°Ê†°ÁâåÂú®Èò≥ÂÖâ‰∏ãÁÜ†ÁÜ†ÂèëÂÖâ„ÄÇ""
ÂèëÁé∞‚ÄúÊ∑±Âú≥Â§ßÂ≠¶ÂüéÈáå‚ÄùÂàÜËØçÈîôËØØÔºåÂèØ‰ª•Âú®CoreNatureDictionary.ngram.txt‰∏≠Ê∑ªÂä†""Â§ßÂ≠¶Âüé@Èáå 10""Ëß£ÂÜ≥ËØ•ÈóÆÈ¢ò„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
public class DemoSegment
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""Ê∑±Âú≥Â§ßÂ≠¶ÂüéÈáåÂêÑÂ§ßÂõΩÂÜÖÂ§ñÁü•ÂêçÈ´òÊ†°ÂêàÂäûÂ≠¶Ê†°Ê†°ÁâåÂú®Èò≥ÂÖâ‰∏ãÁÜ†ÁÜ†ÂèëÂÖâ„ÄÇÊØóÈÇªÊ∑±Âú≥Â§ßÂ≠¶ÂüéÁöÑÊòØ‰ª•ÂàõÊñ∞‰∫ß‰∏ö‰∏∫ÂèëÂ±ïÊ†∏ÂøÉ""
        };
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            System.out.println(termList);
        }
    }
}

```
### ÊúüÊúõËæìÂá∫

```
Ê∑±Âú≥Â§ßÂ≠¶/ntu, ÂüéÈáå/s, ÂêÑ/rz, Â§ß/a, ÂõΩÂÜÖÂ§ñ/s, Áü•Âêç/a, È´òÊ†°/n
```

### ÂÆûÈôÖËæìÂá∫

```
Ê∑±Âú≥/ns, Â§ßÂ≠¶Âüé/nz, Èáå/f, ÂêÑ/rz, Â§ß/a, ÂõΩÂÜÖÂ§ñ/s, Áü•Âêç/a, È´òÊ†°/n
```
"
‚ÄúÂπøÂ∑ûÂ§ßÂ≠¶Âüé‚Äù‰∏é‚ÄúÊ∑±Âú≥Â§ßÂ≠¶Âüé‚ÄùÂàÜËØçÁªìÊûú‰∏çÂêåÁöÑÈóÆÈ¢ò,"## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

ÂØπÂåÖÂê´‚ÄúÊ∑±Âú≥Â§ßÂ≠¶Âüé‚ÄùÂíå‚ÄúÂπøÂ∑ûÂ§ßÂ≠¶Âüé‚ÄùÁöÑÂè•Â≠êËøõË°åÂàÜËØçÔºåÂèëÁé∞ÂàÜËØçÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```
public class DemoSegment
{
    public static void main(String[] args)
    {
        String[] testCase = new String[]{
                ""Ê∑±Âú≥Â§ßÂ≠¶ÂüéÂíåÂπøÂ∑ûÂ§ßÂ≠¶ÂüéÂêÑÂ§ßÂõΩÂÜÖÂ§ñÁü•ÂêçÈ´òÊ†°ÂêàÂäûÂ≠¶Ê†°Ê†°ÁâåÂú®Èò≥ÂÖâ‰∏ãÁÜ†ÁÜ†ÂèëÂÖâ„ÄÇÊØóÈÇªÊ∑±Âú≥Â§ßÂ≠¶ÂüéÁöÑÊòØ‰ª•ÂàõÊñ∞‰∫ß‰∏ö‰∏∫ÂèëÂ±ïÊ†∏ÂøÉ""
        };
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            System.out.println(termList);
        }
    }
}
```
### ÊúüÊúõËæìÂá∫

```
Ê∑±Âú≥/ns, Â§ßÂ≠¶Âüé/nz, Âíå/cc, ÂπøÂ∑û/ns, Â§ßÂ≠¶Âüé/nz
```

### ÂÆûÈôÖËæìÂá∫
```
Ê∑±Âú≥Â§ßÂ≠¶Âüé/nz, Âíå/cc, ÂπøÂ∑û/ns, Â§ßÂ≠¶Âüé/nz
```
"
ÂÅúÊ≠¢ËØç‰∏≠Ê∑ªÂä†ÊéßÂà∂Â≠óÁ¨¶ÁöÑÈóÆÈ¢ò,"
## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ


ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

ÊàëÂ∏åÊúõÂú®ÂÅúÁî®ËØç‰∏≠Ê∑ªÂä† ""\t""  ËøôÊ†∑ÁöÑÊéßÂà∂Â≠óÁ¨¶ ,Âõ†‰∏∫ÂàáÂàÜ ""ËøêÂä®Áî®ÈÄî	Áî®Ê≥ï""(‰∏≠Èó¥ÊòØ""\t"")Ëøô‰∏™ËØçÁöÑÊó∂ÂÄôÁªìÊûúÊòØ:
ËøêÂä®/vn
Áî®ÈÄî/n
	/nx
Ê≥ï/n

‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÊääÂà∂Ë°®Á¨¶ËÆ§‰∏∫Êàê‰∫ÜÂ≠óÊØç‰∏ìÂêç.
ÁÑ∂ÂêéÊàëÂÜçstopword.txt‰∏≠Ê∑ªÂä†""	"" ÂΩìËØªÂèñÂÅúÊ≠¢ËØçËØçÂÖ∏Â∞±Êä•‰∏ãËæπÁöÑÈîô:

‰∏•Èáç: ËΩΩÂÖ•ÂÅúÁî®ËØçËØçÂÖ∏D:/Idea_workplace/HanLP/data/dictionary/stopwords.txtÂ§±Ë¥•java.lang.StringIndexOutOfBoundsException: String index out of range: 0
	at java.lang.String.charAt(String.java:658)
	at com.hankcs.hanlp.collection.MDAG.MDAG.replaceOrRegister(MDAG.java:541)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:220)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:170)
	at com.hankcs.hanlp.collection.MDAG.MDAGSet.<init>(MDAGSet.java:42)
	at com.hankcs.hanlp.dictionary.stopword.StopWordDictionary.<init>(StopWordDictionary.java:44)
	at com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary.<clinit>(CoreStopWordDictionary.java:42)
	at com.hankcs.demo.DemoCustomDictionary.main(DemoCustomDictionary.java:79)

‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊàëÁöÑÁî®Ê≥ïÊúâÈóÆÈ¢ò?

ÊÑüË∞¢Â§ßÁ•ûÁöÑÂºÄÊ∫ê,Êñπ‰æøÊàëÂæàÂ§ö  :)"
"""‰ªôÂâëÂ•á‰æ†‰º†""Â§öÈü≥Â≠óÊãºÈü≥ÔºàchuanÔºâ","<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö
```
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.3.4</version>
</dependency>	
```


## ÊàëÁöÑÈóÆÈ¢ò
‚Äú‰ªôÂâëÂ•á‰æ†‰º†‚ÄùÂæóÂà∞ÁöÑÊãºÈü≥ÊòØ‚Äúxian,jian,qi,xia,chuan‚Äù
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

### Ëß¶Âèë‰ª£Á†Å

```java
@Test
public void testPinyin() {
    String word = ""‰ªôÂâëÂ•á‰æ†‰º†"";
    System.out.println(word + "": "" + HanLP.convertToPinyinString(word, "","", false));
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
xian,jian,qi,xia,zhuan
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
xian,jian,qi,xia,chuan
```


"
ÂØπ‚ÄúËÆ≠ÁªÉHMM-NGramÂàÜËØçÊ®°Âûã‚Äù‰∫ßÁîüÁöÑÈóÆÈ¢òÂíåÁêÜËß£,"ÂØπ‰Ω†Âú® [ËÆ≠ÁªÉÂàÜËØçÊ®°Âûã](https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B) ‰∏≠ÊèêÂà∞ÁöÑHMM-NGramÂàÜËØçÊ®°Âûã‰∫ßÁîü‰∫ÜÂá†‰∏™ÈóÆÈ¢ò

1„ÄÅHMMÊòØÁî±ËæìÂá∫ÈáèÊ±ÇËæìÂÖ•ÈáèÁöÑÊ®°ÂûãÔºåËØ∑ÈóÆËøôÈáåÁöÑÂàÜËØçËæìÂá∫ÈáèÊòØ‰ªÄ‰πàÔºåÊòØÊåáÂàÜËØçÁöÑÂ∫èÂàóÂêóÔºüÈÇ£ËæìÂÖ•ÈáèÂèàÊòØ‰ªÄ‰πàÂë¢Ôºü

HanLP‰∏≠‰ΩøÁî®‰∫Ü‰∏ÄÈò∂ÈöêÈ©¨Ê®°ÂûãÔºåÂú®Ëøô‰∏™ÈöêÈ©¨Â∞îÂèØÂ§´Ê®°Âûã‰∏≠ÔºåÈöêÁä∂ÊÄÅÊòØËØçÊÄßÔºåÊòæÁä∂ÊÄÅÊòØÂçïËØç„ÄÇËßÅ[ËØçÊÄßÊ†áÊ≥®](http://www.hankcs.com/nlp/part-of-speech-tagging.html)


2„ÄÅËÆ≠ÁªÉÂêé‰∏ÄÂÖ±ÂæóÂá∫3‰∏™Êñá‰ª∂Ôºö

CoreNatureDictionary.txtÔºöÂçïËØçËØçÊÄßËØçÂÖ∏
CoreNatureDictionary.ngram.txtÔºö‰∫åÂÖÉÊé•Áª≠ËØçÂÖ∏
CoreNatureDictionary.tr.txtÔºöËØçÊÄßËΩ¨ÁßªÁü©Èòµ

ËØ∑ÈóÆËøôÂá†‰∏™Â≠óÂÖ∏ÁöÑ‰ΩúÁî®Ôºü‰Ω†ËÉΩÁÆÄÂçïÂú∞Áî®ÊúÄÁü≠ÂàÜËØçÁªìÂêà‰∏äÈù¢ÁöÑÂ≠óÂÖ∏ÂÅö‰∏Ä‰∏™Ê¢≥ÁêÜÂêóÔºåÂç≥ÊòØÂ¶Ç‰ΩïÂàÜËØçÔºåÂèàÊòØÂ¶Ç‰ΩïÂÅöËØçÊÄßÊ†áÊ≥®ÁöÑÔºü

ÊàëÁöÑÁêÜËß£Ôºö

CoreNatureDictionary.txtÂíåCoreNatureDictionary.ngram.txtÊòØÁî®Êù•ÂàÜËØçÁöÑÔºåCoreNatureDictionary.tr.txtÊòØÁî®Êù•ÂÅöËØçÊÄßÊ†áÊ≥®ÁöÑ„ÄÇÂú®S-segment‰∏≠ÔºåÂÖàÈÄöËøáÂä®ÊÄÅËßÑÂàíÔºåÂç≥Áî®Â≠óÂÖ∏CoreNatureDictionary.txtÂíåCoreNatureDictionary.ngram.txtÁîüÊàêËØçÂõæÔºàÂä®ÊÄÅËßÑÂàíË∑ØÂæÑÂõæÔºâÔºåÁÑ∂ÂêéÈÄâÊã©ÊúÄÁü≠ÁöÑË∑ØÂæÑ„ÄÇËøôÈáåÂè™ÊòØÁî®Âà∞‰∫ÜMMÔºåÂπ∂Ê≤°ÊúâÁî®Âà∞HMMÔºåÂ±û‰∫éÊú∫Ê¢∞ÂºèËßÑÂàô+ÁªüËÆ°ÁöÑÂàÜËØçÊñπÊ≥ï„ÄÇ
ËÄåCoreNatureDictionary.tr.txtÊ≠£Â¶ÇÈóÆÈ¢ò1ÁöÑËØ¥ÁöÑÔºåÊ†πÊçÆËæìÂÖ•ÁöÑÂàÜËØçÂ∫èÂàóÊù•Âà§Êñ≠ËØçÊÄßÁöÑÂ∫èÂàó„ÄÇ

ÊúâËµÑÊñô‰∏≠ËØ¥Âà∞ÔºåÂàÜËØçÊñπÊ≥ïÁöÑÊºîËøõÔºåÂèØÂàÜ‰∏∫
1„ÄÅÊú∫Ê¢∞ÂºèËßÑÂàô
2„ÄÅËßÑÂàô+ÁªüËÆ°
3„ÄÅMM+Viterbi
4„ÄÅÁî±Â≠óÊûÑËØç
5„ÄÅÁ•ûÁªèÁΩëÁªú

ÊàëËÆ§‰∏∫3ÂÖ∂ÂÆû‰πüÊòØ2ÔºåMMÈ¶ñÂÖà‰πüË¶ÅÂ∞ÜÂè•Â≠êËøõË°åÂä®ÊÄÅËßÑÂàíËøõË°åÂàÜËØçÔºàÊú∫Ê¢∞ËßÑÂàôÔºâÔºå‰πüÂ∞±ÊòØÊü•Â≠óÂÖ∏ÔºåÂÖàÂ∞ÜÂè•Â≠ê‰∏≠ÊâÄÊúâÂèØ‰ª•Êü•Âà∞ÁöÑËØçÔºàÊü•Ê†∏ÂøÉÂ≠óÂÖ∏ÔºâÔºåÂÖàÂàíÂàÜÂá∫Êù•ÔºåÁÑ∂ÂêéÊåâÊó∂ËØçÂà∞ËØçÂá∫Áé∞ÁöÑÈ°∫Â∫èÔºà‰∫åÂÖÉËøûÁª≠Â≠óÂÖ∏ÔºâÁîüÊàêË∑ØÂæÑÁöÑÊùÉÈáçÔºå‰πüÂ∞±ÊòØÊûÑÈÄ†ËØçÂõæ„ÄÇ"
Ëã±ÊñáÊï∞ÈáèËØçÁöÑËØÜÂà´ÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.3


## ÊàëÁöÑÈóÆÈ¢ò

Âú®ÂºÄÂêØÊï∞ÈáèËØçËØÜÂà´ÂêéÔºåËã±ÊñáÁöÑÊï∞ÈáèÊ≤°ÊúâËØÜÂà´

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ëß¶Âèë‰ª£Á†Å

```
Segment segment = HanLP.newSegment()
              .enablePlaceRecognize(true)
              .enableIndexMode(false)
              .enableOrganizationRecognize(true)
              .enableNumberQuantifierRecognize(true);
System.out.println(segment.seg(""18kÈáëÈ°πÈìæ""));
System.out.println(segment.seg(""18k""));
System.out.println(segment.seg(""18kg""));
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[18k/mq, ÈáëÈ°πÈìæ/nz]
[18k/mq]
[18kg/mq]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[18/m, k/nx, ÈáëÈ°πÈìæ/nz]
[18/m, k/nx]
[18/m, kg/nx]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

ÊàëÂú®CoreNatureDictionary.txt‰∏≠Ê∑ªÂä†‰∫Ü‰∏ãÈù¢ËÆ∞ÂΩïÂêéÂÄíÊòØÂæóÂà∞‰∫ÜÊÉ≥Ë¶ÅÁöÑÁªìÊûú
```
k q 1000
kg q 1000
```

ÈóÆÈ¢òÊòØ‰∏∫‰ªÄ‰πàÊ†∏ÂøÉËØçÂÖ∏ÈáåÊ≤°ÊúâËøô‰∫õËØçÁöÑÈÖçÁΩÆÔºåÊòØÊúâ‰ªÄ‰πàÁâπÊÆäÂéüÂõ†ÂêóÔºüÔºüÔºü

"
ngramËØçÂÖ∏ÈáçÊñ∞Âä†ËΩΩ,"## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö 1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö 1.3.3


## ÊàëÁöÑÈóÆÈ¢ò

Áé∞Âú®ngramËØçÂÖ∏ÂæàÂ§öÊ†áËØÜÁöÑ‰∏çÊòØÂæàÂáÜÁ°ÆÔºåËÄå‰∏î‰πüÊúâÂæàÂ§öÁº∫Â§±ÁöÑÔºàÂΩìÁÑ∂ÔºåÂæàÂ§ß‰∏ÄÈÉ®ÂàÜÁöÑÂéüÂõ†ÊòØÂõ†‰∏∫Êàë‰ª¨‰∏öÂä°ÈúÄÊ±ÇÁöÑÈóÆÈ¢òÔºâÔºåÊâÄ‰ª•Áé∞Âú®ÊâìÁÆó‰∫∫Â∑•‰øÆÊîπËØçÂÖ∏ÔºåÁÑ∂ÂêéÂÅö‰∏Ä‰∏™ngramËØçÂÖ∏ÁöÑÁÉ≠Êõ¥Êñ∞ÔºåÈóÆÈ¢òÊòØÔºö

**Â¶ÇÊûúË¶ÅÈáçÊñ∞Âä†ËΩΩngramËØçÂÖ∏ÔºåÊòØÂê¶Âà†Èô§ÁºìÂ≠òÁÑ∂ÂêéË∞ÉÁî®com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary#loadËøô‰∏™ÊñπÊ≥ïÂç≥ÂèØÔºü**




"
http://ictclas.nlpir.org/nlpir/ ËØ∑ÈóÆË∑üËøô‰∏™ÊØîÔºåÂå∫Âà´Âú®Âì™,
‰øÆÊ≠£ÂÖ®ËßíÂπ¥‰ªΩËØÜÂà´‰∏≠Â≠óÁ¨¶‰∏≤ÈïøÂ∫¶ÈîôËØØ,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Âú®MSRËØ≠ÊñôÁöÑÂàÜËØçÊµãËØï‰∏≠ÂèëÁé∞„ÄÇÂπ¥‰ªΩÂà§Êñ≠‰∏≠Êúâ‰∏ÄÂ§ÑÈîôËØØÔºåÂØºËá¥‰∏çËÉΩËØÜÂà´ÂÖ®ËßíÂ≠óÁ¨¶Âπ¥‰ªΩÔºå

## Áõ∏ÂÖ≥issue

ÔºàÊó†Ôºâ"
TextRank ÈáåÈù¢ max_iter = 200 Â¶ÇÊûúÊñáÊú¨Â≠óÊï∞Â§öÔºåÊÄßËÉΩÊúâÂΩ±Âìç,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

com.hankcs.hanlp.summary.TextRankKeyword#max_iter = 200 ÊòØ protect ÁöÑÔºåÊàëÂèëÁé∞ÊàëÂú®Â§ÑÁêÜÂ§ßÈáèÊñáÊú¨Êó∂ÂÄôÔºåÂá∫Áé∞ÊÄßËÉΩÈóÆÈ¢òÔºå‰∏ÄÊ¨°Ë∞ÉÁî®ÈúÄË¶ÅÊ∂àËÄó 200-300ms ÔºåÂèØ‰ª•ÊääËøô‰∏™ÂèòÈáèÊîπÊàê public ÁöÑÔºåÊñπ‰æøÊàëÂú®ÂåÖÂ§ñ‰øÆÊîπ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ËØ∑ÈóÆ‰∏Ä‰∏ãÔºåÊÇ®ËøôÈáåÁöÑËá™Âä®ÊëòË¶ÅÊ®°ÂùóÊîØÊåÅËã±ÊñáÂêó,ËØ∑ÈóÆ‰∏Ä‰∏ãÔºåÊÇ®ËøôÈáåÁöÑËá™Âä®ÊëòË¶ÅÊ®°ÂùóÊîØÊåÅËã±ÊñáÂêó
ÂèëÁé∞CustomDictionaryÂ≠óÂÖ∏ÔºåÂè™ÊúâÊúÄÂêéadd()Êàñinsert()ÁöÑËØçÊù°ÔºåÊâç‰ºöÂú®ÂàÜËØç‰∏≠ÁîüÊïàÔºüÔºÅ,"# HanLPÁâàÊú¨Âè∑
ÁéØÂ¢É1ÔºöÂ≠óÂÖ∏ v1.3.3ÔºàfullÔºâ+jarÂåÖ v1.3.4
ÁéØÂ¢É2ÔºöÂ≠óÂÖ∏ v1.2.8ÔºàminiÔºâ+jarÂåÖ v1.2.8
# ÊàëÁöÑÈóÆÈ¢ò
ÂÆûÈ™åÂèëÁé∞Ôºå‰∏çËÆ∫ÊòØ‰øÆÊîπ.txtÂπ∂Âà∑Êñ∞.binÂ≠óÂÖ∏Êñá‰ª∂ÔºåËøòÊòØÈÄîÁªèCustomDictionary.add()|insert()ÊñπÊ≥ïÔºåÈÉΩÊòØÂè™ÊúâÊúÄÂêéÈôÑÂä†‰∏äÂéªÁöÑËá™ÂÆö‰πâËØçÊù°ÔºåÂú®ÂàÜËØçÊó∂ÊâçÁîüÊïàÔºüÔºÅ
# Â§çÁé∞ÈóÆÈ¢ò
ÂéüÊñáÔºöÂåó‰∫¨Êµ∑Ê∑ÄÂå∫ÔºåË•øÂÖ´ÈáåÂ∫ÑÔºåË£ïÂèãÂ§ßÂé¶3Â±ÇÔºõ
Â≠óÂÖ∏Ôºö
Ë£ïÂèãÂ§ßÂé¶ ns 100
Ë•øÂÖ´ÈáåÂ∫Ñ ns 100
ÂàÜËØçÁªìÊûúÔºöÂåó‰∫¨/ns Êµ∑Ê∑Ä/ns Âå∫/n Ôºå/w Ë•øÂÖ´ÈáåÂ∫Ñ/ns Ôºå**/w Ë£ï/vg Âèã/ng Â§ßÂé¶/n** 3Â±Ç/mq Ôºõ/w
Â≠óÂÖ∏ÔºöÔºà‰ªÖË∞ÉÊç¢Ëá™ÂÆö‰πâËØçÊù°ÁöÑÊ¨°Â∫èÔºâ
Ë•øÂÖ´ÈáåÂ∫Ñ ns 100
Ë£ïÂèãÂ§ßÂé¶ ns 100
ÂàÜËØçÁªìÊûúÔºöÂåó‰∫¨/ns Êµ∑Ê∑Ä/ns Âå∫/n Ôºå/w **Ë•ø/f ÂÖ´ÈáåÂ∫Ñ/ns** Ôºå/w Ë£ïÂèãÂ§ßÂé¶/ns 3Â±Ç/mq Ôºõ/w
# ÁªßÁª≠ËØïÈ™å
ÈôÑÂä†2‰∏™Êó†ÊÑè‰πâÁöÑËØçÊù°Ôºà‚ÄúË•øÂÖ´Èáå‚ÄùÔºå‚ÄúË£ïÂèã‚ÄùÔºâÔºåÂ∞±Â•Ω‰∫ÜÔºÅ
Â≠óÂÖ∏Ôºö
**Ë•øÂÖ´Èáå** ns 100
Ë•øÂÖ´ÈáåÂ∫Ñ ns 100
**Ë£ïÂèã** ns 100
Ë£ïÂèãÂ§ßÂé¶ ns 100
ÂàÜËØçÁªìÊûúÊ≠£Â∏∏Ôºö
Âåó‰∫¨/ns Êµ∑Ê∑ÄÂå∫/ns Ôºå/w Ë•øÂÖ´ÈáåÂ∫Ñ/ns Ôºå/w Ë£ïÂèãÂ§ßÂé¶/ns 3Â±Ç/mq Ôºõ/w
# ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ≥ïÔºö
Â∞ÜÊú™ËÉΩÊàêÂäüÁªÑÂêàÂàÜËØçÁöÑËØçÊù°ÔºåÂú®Ëá™ÂÆö‰πâÂ≠óÂÖ∏‰∏≠ÔºåÊã∑Ë¥ù2Ê¨°ÔºÅ
Â≠óÂÖ∏Ôºö
Ë•øÂÖ´ÈáåÂ∫Ñ ns 1
**Ë•øÂÖ´ÈáåÂ∫Ñ** ns 1
Ë£ïÂèãÂ§ßÂé¶ ns 1
**Ë£ïÂèãÂ§ßÂé¶** ns 1
ÂàÜËØçÁªìÊûúÊ≠£Â∏∏Ôºö
Âåó‰∫¨/ns Êµ∑Ê∑ÄÂå∫/ns Ôºå/w **Ë•øÂÖ´ÈáåÂ∫Ñ/ns** Ôºå/w **Ë£ïÂèãÂ§ßÂé¶/ns** 3Â±Ç/mq Ôºõ/w
# ËØ∑Êïô
@hackers
1ÔºöÊúâÊó†Ê≠£ËßÑÁöÑËß£ÂÜ≥ÊñπÊ≥ïÔºü
2ÔºöËÉΩÂê¶ÊØîËæÉÊ∏ÖÊô∞Âú∞ÔºåÂ∞ÜÊúÄÂ∏∏Áî®ÁöÑViterbiSegment‰ΩøÁî®Â≠óÂÖ∏ÂàÜËØçÊó∂ÁöÑ‰ºòÂÖàÁ∫ßÁêÜËß£ÔºåÊòéÁ°ÆÂ£∞Êòé‰∏Ä‰∏ãÔºü
3Ôºö‰ªÄ‰πàÊ†∑ÁöÑËá™ÂÆö‰πâÂàÜËØçÂèØ‰ª•ÈÄöËøá‰øÆÊîπËá™ÂÆö‰πâËØçÊù°Ëß£ÂÜ≥Ôºå‰ªÄ‰πàÊ†∑ÁöÑËá™ÂÆö‰πâÂàÜËØçÂàôÂøÖÈ°ª‰øÆÊîπÂÖ∂ÂÆÉÔºà‰∏ìÂ±ûÔºâÂ≠óÂÖ∏Ôºü"
"Âçï‰æãÊ®°Âºè,‰ΩøÁî®‰∏ÄÊÆµÊó∂Èó¥(1~2Â§©Â∑¶Âè≥)Â∞±Â≠òÂú®ÂÜÖÂ≠òÊ∫¢Âá∫,ÂÜÖÂ≠òÈÖçÁΩÆ‰∫Ü2G","ËØ∑ÈóÆ‰∏ã  ÊàëËøôÊ†∑‰ΩøÁî®ÊúâÊ≤°Êúâ‰ªÄ‰πàÈóÆÈ¢ò
ÊàëÊà™ÂèñÈÉ®ÂàÜ‰ª£Á†Å

@Component
@Service(""aiServiceHit"")
public class AiServiceByHit implements IAIService {
        private Segment segment = new NShortSegment().enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).enablePartOfSpeechTagging(true);

	@Override
	public boolean answerService(RequestDto requestDto) {
                   ËøôÈáåÈÉ®ÂàÜ‰ª£Á†ÅËøôÊ†∑‰ΩøÁî® 
                  
                        List<Term> seg = segment.seg(text);
			LOG.info(""ÂàÜËØçÁÆóÊ≥ï:"" + JSONArray.toJSONString(seg));
			List<String> getNsV = new ArrayList<String>();//ÂêçËØçÈõÜÂêà
			for(Term t : seg){
				switch (t.nature) {
				case n://ÂêçËØç
				case nx:
				case ns:
				case nz:
				case nf:
				case nt:
				case m:
				case mq:
				case Mg:
					getNsV.add(t.word.toLowerCase());
					break;
				default:
					break;
				}
			}
                    ‰∏ãÈù¢Â∞±ÊòØÈÉ®ÂàÜ‰∏öÂä°
        }
}


Áé∞Âú®ÁöÑÂºÇÂ∏∏ÊèêÈÜíÔºö
J 7395 C1 com.hankcs.hanlp.recognition.nr.TranslatedPersonRecognition.Recognition(Ljava/util/List;Lcom/hankcs/hanlp/seg/common/WordNet;Lcom/hankcs/hanlp/seg/common/WordNet;)V (238 bytes) @ 0x00007f3c6dc7fe44 [0x00007f3c6dc7f720+0x724]
J 7488 C1 com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence([C)Ljava/util/List; (420 bytes) @ 0x00007f3c6d46a9b4 [0x00007f3c6d469960+0x1054]
J 7618 C1 com.hankcs.hanlp.seg.Segment.seg(Ljava/lang/String;)Ljava/util/List; (458 bytes) @ 0x00007f3c6d42d12c [0x00007f3c6d42a9c0+0x276c]



ÊòØ‰∏çÊòØÊàëÁöÑ‰ΩøÁî®ÊñπÊ≥ïÊúâËØØÂë¢Ôºü"
Fix bug on println(CoNLLWord),"ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì
Ê≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
[x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

ÂΩìÂàÜÊûê‰æùÂ≠òÂÖ≥Á≥ªÊó∂ÔºåÈúÄË¶ÅËÄÉËôëÊØè‰∏™CoNLLWordÁöÑ‰æùÂ≠òÂºßÊåáÂêëÁöÑCoNLLWord„ÄÇÂΩìËØ•CoNLLWordÊú¨Ë∫´‰∏∫rootÊó∂ÔºåËæìÂá∫ËØ•ÁâáÊÆµÁöÑHEAD‰ºö
Êä•Èîô„ÄÇ

Êä•Èîô‰ª£Á†ÅÊÆµÂ¶Ç‰∏ãÔºö
CoNLLSentence sentence = HanLP.parseDependency(""ÂùöÂÜ≥ÊÉ©Ê≤ªË¥™Ê±°Ë¥øËµÇÁ≠âÁªèÊµéÁäØÁΩ™"");
for (CoNLLWord word : sentence) {
System.out.println(word.HEAD);
}

ÂàÜÊûêÂéüÂõ†‰∏∫Ôºö
rootÁöÑÂàùÂßãÂåñ‰ª£Á†Å‰∏∫
public static final CoNLLWord ROOT = new CoNLLWord(0, ""##Ê†∏ÂøÉ##"", ""ROOT"",
""root"");
ÂÖ∂HEADÂÄº‰∏∫Á©∫ÔºåÂú®toStringÊñπÊ≥ï‰∏≠Ë∞ÉÁî®HEAD.IDÊó∂Êä•Èîô„ÄÇ

‰øÆÊîπÊñπÊ≥ï‰∏∫Ôºö
Âú®CoNLLWordÁöÑtoStringÊñπÊ≥ï‰∏≠Âà§Êñ≠ÊòØÂê¶‰∏∫Ê†πËäÇÁÇπ(root)ÊàñÁ©∫ÁôΩËäÇÁÇπ(null)ÔºåËã•ÊòØÂàôÂ∞ÜHEAD.IDÊõøÊç¢‰∏∫‰∏ãÂàíÁ∫ø„ÄÇ"
JDK1.6 portableÂåÖ‰∏ãÊä•Èîô,"## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable-1.3.4.jar
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable-1.3.4.jar

## ÊàëÁöÑÈóÆÈ¢ò

Áõ¥Êé•Êñ∞Âª∫È°πÁõÆÔºåÂä†ÂÖ•ÂºïÁî®hanlp-portable-1.3.4.jarÔºåÂú®jdk1.6ÁéØÂ¢É‰∏ã‰ª•‰∏ã‰ª£Á†Å‰ºöÊä•Èîô
ÈîôËØØ‰ø°ÊÅØ
```
2017-6-29 9:31:53 com.hankcs.hanlp.dictionary.TransformMatrixDictionary load
Ë≠¶Âëä: ËØªÂèñdata/dictionary/CoreNatureDictionary.tr.txtÂ§±Ë¥•java.lang.NullPointerException: Inflater has been closed
Exception in thread ""main"" java.lang.NullPointerException
	at com.hankcs.hanlp.algorithm.Viterbi.compute(Viterbi.java:121)
	at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:533)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:120)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)
	at Main.main(Main.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
```

### Ëß¶Âèë‰ª£Á†Å

```
        Segment segment = new ViterbiSegment(){{
            enableIndexMode(false);
            enableOffset(true);
            enableNumberQuantifierRecognize(false);
            enableOrganizationRecognize(true);
            enableCustomDictionary(true);
            enablePlaceRecognize(true);
            enableNameRecognize(true);
            enableJapaneseNameRecognize(false);
            enableTranslatedNameRecognize(false);
            enablePartOfSpeechTagging(true);
        }};
        System.out.println(segment.seg(""Áúã‰∏Ä‰∏ãÂ∞èÁéãÁöÑÊó•Êä•""));
```
## ÂÖ∂‰ªñ‰ø°ÊÅØ
1. Ê≥®Èáä`enablePlaceRecognize(true);`ÂèØÊ≠£Â∏∏ËøêË°åÔºõ
2. ‰ΩøÁî®portableÊ∫êÁ†ÅË∞ÉËØïÊó†Êä•Èîô„ÄÇ


"
[Not a bug] HanLP API Docker Image,"ÂêÑ‰Ωç

ÊàëÂ∞ÅË£Ö‰∫Ü‰∏Ä‰∏™docker imagesÔºåÂèØ‰ª•Áî®Êù•Âø´ÈÄüÂºÄÂßã‰ΩøÁî®HanLP APIÔºåÂ∞§ÂÖ∂ÊòØÂú®Ë∑®ËØ≠Ë®ÄÂíåÂπ≥Âè∞‰∏äÔºåÈõÜÊàêHanLPÊúçÂä°„ÄÇ

http://nlp.chatbot.io/public/index.html

Âø´ÈÄüÂºÄÂßã: https://hub.docker.com/r/samurais/hanlp-api/"
ÂÖ≥‰∫éAndroidÁöÑ‰ΩøÁî®,"‰Ω†Â•ΩÔºåÂæàÊÑüË∞¢‰Ω†‰ª¨ËÉΩÊèê‰æõÂ¶ÇÊ≠§‰ºòÁßÄÁöÑÂºÄÊ∫êÂ∫ìÔºåÊàëÊÉ≥ËØ¢ÈóÆ‰∏Ä‰∏ãÂÖ≥‰∫éÂÆâÂçìÂ¶Ç‰Ωï‰ΩøÁî®Ëøô‰∏™Â∫ìÔºå ÊàëÁî®ÁöÑIDEÊòØandroid studio,‰æãÂ¶ÇHanLP.propertiesÂ∫îËØ•ÊîæÂì™ÂÑøÔºåÊÑüË∞¢Ëß£Á≠î"
Ê†πÊçÆ‰∏ªÂàÜÊîØ‰ª£Á†ÅÊõ¥Êñ∞protable‰ª£Á†Å,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü
Ê†πÊçÆ‰∏ªÁâàÊú¨ÁöÑ‰ª£Á†ÅÊõ¥Êñ∞‰∫ÜprotableÂàÜÊîØÁöÑ‰ª£Á†Å

## Áõ∏ÂÖ≥issue



"
ÂàÜËØçÈîôËØØÔºåÈóÆÈ¢òÔºö‰Ω†Â•ΩÔºåËØ∑ÈóÆ‰Ω†‰ª¨ÊúâÂÆùÈ©¨carÁöÑ‰ª∑Ê†ºÂêóÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.2.9


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÂàÜËØçÈóÆÈ¢òÔºö‚Äú‰Ω†Â•ΩÔºåËØ∑ÈóÆ‰Ω†‰ª¨ÊúâÂÆùÈ©¨carÁöÑ‰ª∑Ê†ºÂêóÔºü‚ÄùÔºåÂàÜËØçÁªìÊûúÔºö[‰Ω†Â•Ω/l, Ôºå/w, ËØ∑ÈóÆ/v, `‰Ω†/r, ‰ª¨/k`, Êúâ/v, ÂÆùÈ©¨/nz, car/nx, ÁöÑ/uj, ‰ª∑Ê†º/n, Âêó/y, Ôºü/w]ÔºåÂÖ∂‰∏≠ËØç ‰Ω†‰ª¨ Ë¢´ÂàÜÊãÜÂºÄ‰∫ÜÔºåÂ∫îËØ•ÂàÜ‰∏∫‚Äú‰Ω†‰ª¨‚Äù„ÄÇ

ËÄåÁõ¥Êé•ÂàÜËØçÔºö‚Äú‰Ω†‰ª¨ÊúâÂÆùÈ©¨carÁöÑ‰ª∑Ê†ºÂêóÔºü‚ÄùÔºåÊòØÊ≤°ÊúâÈóÆÈ¢òÁöÑÔºåÂè™Êúâ‚ÄúËØ∑ÈóÆ‰Ω†‰ª¨‚ÄùÔºåËøôÁßçÊâç‰ºöÊúâÈóÆÈ¢ò„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Ê≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÂèäÊ®°Âûã„ÄÇ

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        List<Term> terms = HanLP.segment(""‰Ω†Â•ΩÔºåËØ∑ÈóÆ‰Ω†‰ª¨ÊúâÂÆùÈ©¨carÁöÑ‰ª∑Ê†ºÂêóÔºü"");
        System.out.println(terms);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[‰Ω†Â•Ω/l, Ôºå/w, ËØ∑ÈóÆ/v, ‰Ω†‰ª¨/r, Êúâ/v, ÂÆùÈ©¨/nz, car/nx, ÁöÑ/uj, ‰ª∑Ê†º/n, Âêó/y, Ôºü/w]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[‰Ω†Â•Ω/l, Ôºå/w, ËØ∑ÈóÆ/v, ‰Ω†/r, ‰ª¨/k, Êúâ/v, ÂÆùÈ©¨/nz, car/nx, ÁöÑ/uj, ‰ª∑Ê†º/n, Âêó/y, Ôºü/w]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
"CoreDictionary‰∏≠Êúâ‰∏Ä‰∏™""Êú∫Êî∂""ÁöÑËØçÔºåÂØºËá¥‚ÄúÊâãÊú∫Êî∂ÈÇÆ‰ª∂‚ÄùÂàÜËØçÁªìÊûú‰∏∫‚ÄúÊâã Êú∫Êî∂ ÈÇÆ‰ª∂‚Äù ","<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.2


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
Âú®ÂàÜËØçÁöÑÊó∂ÂÄôÂèëÁé∞ÂØπ‰∏Ä‰∏™Âè•Â≠ê‚ÄúÊâãÊú∫Êî∂ÈÇÆ‰ª∂ÁöÑÈóÆÈ¢ò‚ÄùËøõË°åÂàÜËØçÔºåÁªìÊûúÊòØ‚ÄúÊâã Êú∫Êî∂ ÈÇÆ‰ª∂ ÁöÑ ÈóÆÈ¢ò‚ÄùÔºåÂç≥‰ΩøÂ∞Ü‚ÄúÊâãÊú∫‚ÄùÂä†Âà∞CustomDictionary‰∏≠‰πüËøòÊòØËøôÊ†∑Â≠êÁöÑÁªìÊûú„ÄÇÂ∞ùËØï‰∫ÜÂêÑ‰∏™ÂàÜËØçÁ±ªÔºöNotionalTokenizer,HanLP.segment(),HanLP.newSegment() ÈÉΩÂá∫Áé∞Ëøô‰∏™ÈóÆÈ¢ò
ÂÆö‰ΩçÂèëÁé∞CoreDictionary‰∏≠Êúâ‰∏Ä‰∏™""Êú∫Êî∂""ÁöÑËØçÔºåÂØºËá¥‚ÄúÊâãÊú∫Êî∂ÈÇÆ‰ª∂‚ÄùÂàÜËØçÁªìÊûú‰∏∫‚ÄúÊâã Êú∫Êî∂ ÈÇÆ‰ª∂‚Äù 

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
static void testSeg(){
   Segment segment = HanLP.newSegment().enableCustomDictionary(true);
    String str = ""ÊâãÊú∫Êî∂ÈÇÆ‰ª∂ÁöÑÈóÆÈ¢ò"";
    List<Term> res = segment .seg(str);
    StringBuilder sb = new StringBuilder();
    for(Term term:res ){
      sb.append(term.word).append(""\t"");
    }
    System.out.println(sb.toString());
  }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊâãÊú∫ Êî∂ ÈÇÆ‰ª∂ ÁöÑ ÈóÆÈ¢ò
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
Êâã Êú∫Êî∂ ÈÇÆ‰ª∂ ÁöÑ ÈóÆÈ¢ò
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂÖ≥‰∫éCharacterBasedGenerativeModel ÂéüÂßãËØ≠ÊñôÊó†Ê≥ïÊâæÂà∞ÁöÑÈóÆÈ¢ò,"Âü∫‰∫é2Èò∂HMM ÂàÜËØçÂô® HMMSegment‰∏≠‰ΩøÁî®Âà∞ÁöÑÊ®°ÂûãÔºå‰ΩÜÊòØÊó†Ê≥ïÊâæÂà∞‰∏é‰πãÁõ∏ÂØπÂ∫îÁöÑÂéüÂßãËØ≠Êñô„ÄÇËøòÊúõÂ§ßÁ•ûËÉΩÂ§üÊèê‰æõ‰∏é‰πãÁõ∏ÂØπÂ∫îÁöÑÂéüÂßãËØ≠ÊñôÊñπ‰æøÊàë‰ª¨Â≠¶‰π†ÂíåËøõË°ådebug
"
HanLP Ê±âÂ≠óËΩ¨Êç¢ÊãºÈü≥Êó∂ÔºåÂ§öÈü≥Â≠óÈóÆÈ¢òÔºå‚ÄúËøòÊ¨æ‚Äù ÁªìÊûú‰∏∫Ôºö‚Äúhai kuan‚ÄùÔºåÊ±ÇÊõ¥Ê≠£„ÄÇ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
HanLP Ê±âÂ≠óËΩ¨Êç¢ÊãºÈü≥Êó∂ÔºåÂ§öÈü≥Â≠óÈóÆÈ¢òÔºå‚ÄúËøòÊ¨æ‚Äù ÁªìÊûú‰∏∫Ôºö‚Äúhai kuan‚Äù
## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
 public static String convertToPinyinString(String text, String separator, boolean remainNone) {
        List pinyinList = PinyinDictionary.convertToPinyin(text, true);
        int length = pinyinList.size();
        StringBuilder sb = new StringBuilder(length * (5 + separator.length()));
        int i = 1;

        for(Iterator var7 = pinyinList.iterator(); var7.hasNext(); ++i) {
            Pinyin pinyin = (Pinyin)var7.next();
            if(pinyin == Pinyin.none5 && !remainNone) {
                sb.append(text.charAt(i - 1));
            } else {
                sb.append(pinyin.getPinyinWithoutTone());
            }

            if(i < length) {
                sb.append(separator);
            }
        }

        return sb.toString();
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
huan kuan

### ÂÆûÈôÖËæìÂá∫
hai kuan
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
hanlpÊîØÊåÅ‰ªéÁî®Êà∑ËØ≠ÊñôÁªßÁª≠ËÆ≠ÁªÉÂòõÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

Êàë‰ªéÊñáÊ°£‰∏äÁúãÂà∞hanlpÂèØ‰ª•Â∏ÆÂä©Áî®Êà∑ËÆ≠ÁªÉËá™Â∑±ÁöÑËØ≠Êñô„ÄÇÈÇ£‰πàÂÅáËÆæÊàëÊúâËá™Â∑±ÁöÑÂàÜËØçËØ≠ÊñôÔºåÊàëÈúÄË¶ÅÊÄé‰πàÂéªËÆ≠ÁªÉÂë¢Ôºü
"
hanlp ËÉΩÂê¶Âä†ÂÖ•‰∏Ä‰∏™ËøúÁ®ãËØçÂÖ∏Êõ¥Êñ∞ÁöÑÂäüËÉΩ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÂÜç‰ΩøÁî® [elasticsearch-ik](https://github.com/medcl/elasticsearch-analysis-ik)ÁöÑÊó∂ÂÄôÔºåÂèëÁé∞Ëøô‰∏™ËøúÁ®ãÊõ¥Êñ∞ÁöÑÂäüËÉΩÈùûÂ∏∏‰∏çÈîôÔºå‰πüÂæàÁÆÄÂçïÔºåÂ∞±ÊòØÈÄöËøáËøúÁ®ãÁöÑ‰∏Ä‰∏™ txt Êñá‰ª∂ÔºåÂà§Êñ≠ ETag ÊòØÂê¶Êõ¥Êñ∞ÔºåÁÑ∂ÂêéÈáçÊñ∞ reload ËØçÂÖ∏ÔºåÊàëËá™Â∑±Âú®‰ΩøÁî®ÁöÑÈ°πÁõÆÈáåÈù¢Â∑≤ÁªèÂèÇËÄÉ ik ÁöÑÊú∫Âà∂ÔºåÂÜô‰∫Ü‰∏Ä‰∏™Ëá™Âä®Êõ¥Êñ∞ÁöÑÂäüËÉΩ„ÄÇ

BTW: CustomDictionary ÈáåÈù¢ËÉΩ‰∏çËÉΩÂä†ÂÖ•‰∏Ä‰∏™ reload ÁöÑÂäüËÉΩÔºåÊàëÁî®ËøúÁ®ãÊõ¥Êñ∞ÁöÑÊó∂ÂÄôÈúÄË¶ÅÈáçÊñ∞Âä†ËΩΩËøô‰∏™ËØçÂÖ∏„ÄÇÊàëÊú¨Âú∞ÊµãËØï‰∫Ü‰∏Ä‰∏™Áõ¥Êé•Êää CustomDictionary.BinTrie = nullÔºåËøêË°åËøá‰∫Ü‰∏ÄÊÆµÊó∂Èó¥ÔºåÂ•ΩÂÉèÊ≤°ÊúâÂØπË±°ËøáÂ§öÁöÑÂÜÖÂ≠òÊ≥ÑÈú≤Ôºå‰ΩÜÊòØÊ≤°ÊúâËØ¶ÁªÜÊµãËØïËøá„ÄÇ

"
ËØÜÂà´‰∫∫ÂêçÂá∫Èîô,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4
## ÊàëÁöÑÈóÆÈ¢ò
 ‰ΩøÁî®NLPTokenizer.segmentÔºàÔºâÊñπÊ≥ïË∞ÉÁî® ÂèëÁé∞‰ΩøÁî®""ÂàòÈ£ûÂú®ÊâìÁîµËØù"" ÊääÂÖ∂‰∏≠ÁöÑ‚ÄúÂàòÈ£ûÂú®‚Äù ÂÆöÊÄß‰∏∫nr

### Ëß¶Âèë‰ª£Á†Å

```
public class DemoNLPSegment
{
    public static void main(String[] args)
    {
        HanLP.Config.enableDebug();
        Segment segment = HanLP.newSegment().enableOrganizationRecognize(false);
        List<Term> termList = NLPTokenizer.segment(""ÂàòÈ£ûÂú®ÊâìÁîµËØù"");
        System.out.println(termList);
    }
}
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
‚ÄúÂàòÈ£û/nr‚Äù ""Âú®/p"" ""ÊâìÁîµËØù/vi""
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[ÂàòÈ£ûÂú®/nr, ÊâìÁîµËØù/vi]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Á•ûÁªèÁΩëÁªúÂè•Ê≥ïÂàÜÊûêÁöÑÊó∂ÂÄôÂä†ËΩΩÊúâÈóÆÈ¢òÔºåCoNLLWordÁ±ª toString ÊñπÊ≥ïÊâìÂç∞‰∏§Ê¨°ÂêçÁß∞,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [‚àö] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

ÈááÁî®NeuralNetworkDependencyParser.computeÁöÑËøõË°åÂè•Ê≥ïÂàÜÊûêÁöÑÊó∂ÂÄôÔºåÈªòËÆ§‰ºöË∞ÉÁî® jar ÂåÖÁöÑË∑ØÂæÑÔºå‰ºöÊä•Èîô„ÄÇÈúÄË¶ÅÂÖàÂ∞Ü HanLP.Config.IOAdapter = null; ËøôÊ†∑‰ºö‰ªéÊñá‰ª∂ËØªÂèñ„ÄÇËÄå‰∏îCoNLLWordÁ±ªÁöÑ toString ÊñπÊ≥ïÊääËØçÂÜô‰∫Ü‰∏§Ê¨°„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
### Ê≠•È™§

### Ëß¶Âèë‰ª£Á†Å

```
     CoNLLSentence sentence = NeuralNetworkDependencyParser.compute(""ÂæêÂÖàÁîüËøòÂÖ∑‰ΩìÂ∏ÆÂä©‰ªñÁ°ÆÂÆö‰∫ÜÊääÁîªÈõÑÈπ∞„ÄÅÊùæÈº†ÂíåÈ∫ªÈõÄ‰Ωú‰∏∫‰∏ªÊîªÁõÆÊ†á„ÄÇ"");
        System.out.println(sentence);
        // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
1	ÂæêÂÖàÁîü	nh	nr	_	4	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
```

### ÂÆûÈôÖËæìÂá∫

```
1	ÂæêÂÖàÁîü	ÂæêÂÖàÁîü	nh	nr	_	4	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
consider empty emit for more stable result,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

‰øÆÊ≠£Á¢∫Ë™çÈóúÈçµÂ≠óÊôÇÂèäÊó©ÈÄÄÂá∫ÁöÑÊ¢ù‰ª∂

## Áõ∏ÂÖ≥issue

<!-- None -->


"
Early abandoning ac trie,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Á¢∫Ë™çÊñáÊú¨‰∏≠ÊòØÂê¶ÂåÖÂê´Âª∫Á´ãtrieÊôÇ‰ΩøÁî®ÁöÑÊ®°ÂºèÔºå‰∏ÄÊó¶ÊâæÂà∞Â∞±ÂèØÂèäÊó©ÈÄÄÂá∫„ÄÇ
ÈÅ©Áî®ÊñºÂè™ÈúÄÁ¢∫Ë™ç‰ªª‰∏ÄÊ®°ÂºèÂ≠òÂú®ÔºåËÄå‰∏çÈúÄË¶ÅÁµ±Ë®àÈ†ªÁéáÁöÑÊÉÖÂ¢É„ÄÇ

## Áõ∏ÂÖ≥issue

<!-- None -->


"
ÂÖ≥‰∫ésolr‰∏≠Ê∑ªÂä†hanlpÁöÑÂàÜËØçÊèí‰ª∂‰∏≠ÔºåÈÖçÁΩÆhanlp.propertiesÁöÑrootÁõ∏ÂØπË∑ØÂæÑ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-portable.jar1.3.4Âíåhanlp-solr-plugin.jar1.1.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-portable.jar1.3.4Âíåhanlp-solr-plugin.jar1.1.2


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
ÊàëÊòØÊÉ≥Â∞ÜhanlpÁöÑÂàÜËØçÊèí‰ª∂Âä†ËΩΩÂà∞solr‰∏≠Êù•ÔºåÂÖàÂ∑≤ÁªèÊäähanlpÁöÑÈÖçÁΩÆÊñá‰ª∂hanlp.propertiesÊîæËøõÊù•‰∫ÜÔºådataÂåÖÊîæÂà∞solrÈ°πÁõÆÂ∑•Á®ãÁöÑwebContent‰∏ã‰∫ÜÔºåÂ¶ÇÊûúrootÂÜôÁöÑÁªùÂØπË∑ØÂæÑÊòØÂèØ‰ª•Ê≠£Â∏∏Âä†ËΩΩÁöÑÔºåÂ¶ÇÊûúÂÜôÊàêÈ°πÁõÆÁöÑÁõ∏ÂØπË∑ØÂæÑÔºåÂ∞±Êó†Ê≥ïÊâæÂà∞Ë∑ØÂæÑÔºåÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÔºårootÂèØ‰ª•ÈÖçÁΩÆÊàêÁõ∏ÂØπË∑ØÂæÑÂêóÔºüËØ•Â¶Ç‰ΩïÈÖçÁΩÆÔºüÔºåËøòÊòØËØ¥dataÂåÖ‰∏çËÉΩÊîæÂà∞solrÈ°πÁõÆÂ∑•Á®ã‰∏ãÁöÑwebContent‰∏ãÔºüÊàëÁúã‰∫Ü‰∏Ä‰∏ãÈáåÈù¢‰πãÂâçÂà´‰∫∫ÁöÑÊèêÈóÆ ÔºåÊàëÁúãÂà∞ËØ¥ÈÖçÁΩÆrootË∑ØÂæÑÈáåÔºåÊòØ../ÁöÑÊñπÂºèÈÖçÁΩÆÁöÑÔºåËØ∑Â§ßÁ•ûËß£Á≠îÔºåË∞¢Ë∞¢ÔºÅ
Âè¶ÔºåÂú®solr‰∏≠ÁöÑschemaÊñá‰ª∂ÈáåÈÖçÁΩÆËá™ÂÆö‰πâËØçÂ∫ìÊó∂Â∫îËØ•‰πüÂèØ‰ª•ÂÜôÈ°πÁõÆÁöÑÁõ∏ÂØπË∑ØÂæÑÂêóÔºüÂÜôÊ≥ïÊòØ‰∏çÊòØÂíårootÈÖçÁΩÆÊòØ‰∏ÄËá¥ÁöÑÔºü


## Ëß¶Âèë‰ª£Á†Å
```
 Áõ¥Êé•ÂêØÂä®solrÈ°πÁõÆ
```


## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÂ∞Ühanlp.propertiesÊñá‰ª∂ÊîæÂà∞‰∫ÜsolrÈ°πÁõÆÂ∑•Á®ãÁöÑsrcË∑ØÂæÑ‰∏ãÔºåÈÄöËøáÁºñËØëÂèØÁîüÊàêclasspath‰∏ã
2. ÁÑ∂ÂêéÂú®WebContent‰∏ãÊñ∞Âª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫hanlpÁöÑÊñá‰ª∂Â§π
3. Êé•ÁùÄÂ∞ÜdataÂåÖÊîæÂà∞solrÈ°πÁõÆÂ∑•Á®ãÁöÑWebContent‰∏≠ÁöÑhanlp‰∏ãÔºå
4. ÊúÄÂêé‰øÆÊîπhanlp.propertiesÈáåÁöÑrootË∑ØÂæÑÔºåÊàëÊÉ≥‰øÆÊîπÊàêÁõ∏ÂØπË∑ØÂæÑÔºåroot=../È°πÁõÆÂêçÁß∞/hanlp/

### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõÂèØ‰ª•ÈÄöËøáÁõ∏ÂØπË∑ØÂæÑÊâæÂà∞dataÂåÖ
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫ÔºåÊó†Ê≥ïËΩΩÂÖ•dataÂåÖ
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
Ëá™ÂÆö‰πâËØçËØ≠Êó†Ê≥ïË¢´Ê≠£Á°ÆÂàÜÂá∫,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

Ëá™ÂÆö‰πâËØçËØ≠ÂàÜËØçÈîôËØØ

## Â§çÁé∞ÈóÆÈ¢ò


### Ê≠•È™§

ÂØπ‚Äú2016Âπ¥6ÊúàÂ§ßÂ≠¶Ëã±ËØ≠ÂÖ≠Á∫ßËÄÉËØïÁúüÈ¢ò‚ÄùËøõË°åÂàÜËØç

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
		String[] testCase = new String[]{
                ""2016Âπ¥6ÊúàÂ§ßÂ≠¶Ëã±ËØ≠ÂÖ≠Á∫ßËÄÉËØïÁúüÈ¢ò"",
        };
		
		CustomDictionary.insert(""Ëã±ËØ≠ÂÖ≠Á∫ß"", ""nz 99999"");
        for (String sentence : testCase)
        {
            List<Term> termList = HanLP.segment(sentence);
            for(Term t : termList){
            	System.out.println(t.word + "":"" + t.getFrequency());
            }
        }
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

ÂåÖÂê´
‚Ä¶‚Ä¶
Ëã±ËØ≠ÂÖ≠Á∫ßÔºö99999
‚Ä¶‚Ä¶
ÁöÑËæìÂá∫

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

2016:0
Âπ¥:14340
6:0
Êúà:7097
Â§ß:27275
Â≠¶Ëã±ËØ≠:16
ÂÖ≠:0
Á∫ß:3240
ËÄÉËØï:3281
ÁúüÈ¢ò:12

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÂΩìCustomDictionary.insert(""Â§ßÂ≠¶Ëã±ËØ≠ÂÖ≠Á∫ß"");Êó∂
Âç¥ÂèØ‰ª•Ê≠£Á°ÆÂàÜÂá∫
‚Ä¶‚Ä¶
Â§ßÂ≠¶Ëã±ËØ≠ÂÖ≠Á∫ßÔºö1
‚Ä¶‚Ä¶
ÁöÑÁªìÊûú


"
ÂÖ≥‰∫éhanlp.properties ÁöÑrootË∑ØÂæÑÁöÑÁõ∏ÂØπË∑ØÂæÑÈÖçÁΩÆÈóÆÈ¢ò,"ÊàëÊòØÊÉ≥Â∞ÜhanlpÁöÑÂàÜËØçÊèí‰ª∂Âä†ËΩΩÂà∞solr‰∏≠Êù•ÔºåÂÖàÂ∑≤ÁªèÊäähanlpÁöÑÈÖçÁΩÆÊñá‰ª∂hanlp.propertiesÊîæËøõÊù•‰∫ÜÔºådataÂåÖÊîæÂà∞solrÈ°πÁõÆÂ∑•Á®ãÁöÑwebContent‰∏ã‰∫ÜÔºåÂ¶ÇÊûúrootÂÜôÁöÑÁªùÂØπË∑ØÂæÑÊòØÂèØ‰ª•Ê≠£Â∏∏Âä†ËΩΩÁöÑÔºåÂ¶ÇÊûúÂÜôÊàêÈ°πÁõÆÁöÑÁõ∏ÂØπË∑ØÂæÑÔºåÂ∞±Êó†Ê≥ïÊâæÂà∞Ë∑ØÂæÑÔºåÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÔºårootÂèØ‰ª•ÈÖçÁΩÆÊàêÁõ∏ÂØπË∑ØÂæÑÂêóÔºüËØ•Â¶Ç‰ΩïÈÖçÁΩÆÔºüÔºåËøòÊòØËØ¥dataÂåÖ‰∏çËÉΩÊîæÂà∞solrÈ°πÁõÆÂ∑•Á®ã‰∏ãÁöÑwebContent‰∏ãÔºüÊàëÁúã‰∫Ü‰∏Ä‰∏ãÈáåÈù¢‰πãÂâçÂà´‰∫∫ÁöÑÊèêÈóÆ ÔºåÊàëÁúãÂà∞ËØ¥ÈÖçÁΩÆrootË∑ØÂæÑÈáåÔºåÊòØ../ÁöÑÊñπÂºèÈÖçÁΩÆÁöÑÔºåËØ∑Â§ßÁ•ûËß£Á≠îÔºåË∞¢Ë∞¢ÔºÅ
Âè¶ÔºåÂú®solr‰∏≠ÁöÑschemaÊñá‰ª∂ÈáåÈÖçÁΩÆËá™ÂÆö‰πâËØçÂ∫ìÊó∂Â∫îËØ•‰πüÂèØ‰ª•ÂÜôÈ°πÁõÆÁöÑÁõ∏ÂØπË∑ØÂæÑÂêóÔºüÂÜôÊ≥ïÊòØ‰∏çÊòØÂíårootÈÖçÁΩÆÊòØ‰∏ÄËá¥ÁöÑÔºü"
HanLP.properties ËøòÊòØhanlp.properties,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

Âü∫‰∫émavenÈ°πÁõÆÔºåÈÖçÁΩÆHanLP.propertiesÔºå ÈÄöËøáIDEË∞ÉËØïÔºåÂèëÁé∞ËÉΩÊ≠£Á°ÆÂä†ËΩΩÊåáÂÆöÁõÆÂΩïÁöÑÂ≠óÂÖ∏Â∫ì„ÄÇ‰ΩÜÊòØÊâìÂåÖÊàêjarÔºåÂ∞±ÂèëÁé∞ÔºåÊØèÊ¨°ÈÉΩÊòØÂä†ËΩΩÁõ∏ÂØπÁõÆÂΩïÔºåÂç≥hanlpËá™Â∏¶jarÈáåÁöÑÂ≠óÂÖ∏Â∫ì„ÄÇ‰∏çÁÆ°HanLP.properties ÁöÑÈÖçÁΩÆÂ¶Ç‰ΩïË∞ÉÊï¥ÔºåÈÉΩÊó†Êïà„ÄÇÊîπÊàêÂÖ®ÈÉ®Â∞èÂÜôÔºöhanlp.properties  Âç≥ÂèØ„ÄÇ
ËôΩÁÑ∂Êìç‰ΩúÁ≥ªÁªü‰∏ä HanLP.properties Âíåhanlp.properties ÈÉΩÊòØÊåáÂêå‰∏Ä‰∏™Êñá‰ª∂Ôºå‰ΩÜÊ∫êÁ†ÅÈáåÊåáÊòé‰∫ÜÊòØÔºöhanlp.properties „ÄÇ


## Â§çÁé∞ÈóÆÈ¢ò
HanLP.properties ÊîπÊàêhanlp.properties ÂêéÈóÆÈ¢òËß£ÂÜ≥„ÄÇ
Â¶ÇÊûúÊîπÊàêHanLP.properties ÔºåÂàôÂèà‰ºöËØªÂèñÁõ∏ÂØπÁõÆÂΩï„ÄÇ



"
Á≥ªÁªüÂä†ËΩΩÁöÑÊó•ÂøóËÉΩÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰πàÔºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
ÊúÄËøëÂá†Â§©ÂÜçÁî®hanlpÂàÜËØçÔºåÊó•ÂøóÊâìÂç∞Êó∂ÔºåË¶Å‰πàÈááÁî®debugÊ®°ÂºèÔºå‰ΩÜËøôÊ†∑‰ºöÊâìÂç∞ÂæàÂ§öÊó•Âøó„ÄÇËÉΩÂê¶Â∞ÜÁ≥ªÁªüÂä†ËΩΩÊó∂ÔºåÂä†ËΩΩÁöÑÂì™‰∫õÊñá‰ª∂‰Ωú‰∏∫Ê†∏ÂøÉÊó•ÂøóÊâìÂá∫Âë¢ÔºüÂõ†‰∏∫ÊàëÂàÜËØçÊñá‰ª∂Âá†ÁôæÂÖÜÔºåÂÖ∂ÂÆûÈ¶ñÂÖàÊòØÊúüÊúõÈÖçÁΩÆÂä†ËΩΩÊòØÂê¶Ê≠£Á°ÆÔºåÂπ∂‰∏çÈúÄË¶ÅÊØè‰∏™ËØçÁöÑÂàÜËØçÁªÜËäÇ„ÄÇ‰ΩÜÁõÆÂâçÊù•ÁúãÔºåËøòË°åÂè™Êúâ‰∏Ä‰∏™ÈÄâÊã©ÔºöHanLP.Config.enableDebug();
Âè¶Â§ñÔºåÈááÁî®slfjÔºålog4j ÁöÑÈÖçÁΩÆÔºåÂç≥‰ΩøËÆæÂÆö‰∫ÜÊó•ÂøóÁ∫ßÂà´‰∏∫debugËøòÊòØÊó†Êïà„ÄÇÊòØÂê¶‰πüËÉΩËÄÉËôë‰∏ãÂçáÁ∫ß„ÄÅÔºü
![image](https://cloud.githubusercontent.com/assets/4981629/26762138/47a4f694-496f-11e7-9cf8-fac1c237f822.png)



"
Ëá™ÂÆö‰πâÁöÑËØçÊÄßÊó†Ê≥ïËØÜÂà´,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4

## ÊàëÁöÑÈóÆÈ¢ò
Âú®Èõ∂ÈÖçÁΩÆÁöÑÊÉÖÂÜµ‰∏ãÔºåËá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑËØçÊÄßËØÜÂà´‰∏çÊ≠£Á°Æ

### Ê≠•È™§
### Ëß¶Âèë‰ª£Á†Å

```
    public static void main(String[] args) {
        CustomDictionary.insert(""Âà´ÂÖã"",""brand 1"");
        CustomDictionary.insert(""Ëã±Êúó"",""family 1"");

        String text1=""Âà´ÂÖãËã±Êúó"";
        String text2 = ""Âà´ÂÖã Ëã±Êúó"";
        StandardTokenizer.SEGMENT.enableAllNamedEntityRecognize(false);

        List<Term> terms1 = StandardTokenizer.segment(text1);
        System.out.println(terms1);

        List<Term> terms2 = StandardTokenizer.segment(text2);
        System.out.println(terms2);
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
[Âà´ÂÖã/brand,  Ëã±Êúó/family]
[Âà´ÂÖã/brand,  /w, Ëã±Êúó/family]
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
[Âà´ÂÖãËã±Êúó/nrf]
[Âà´ÂÖã/brand,  /w, Ëã±Êúó/family]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ



"
Â≠óÂÖ∏ÁºìÂ≠ò*.binÂà†Èô§ÂêéÊó†Ê≥ïÂÜçÊ¨°‰∫ßÁîü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->


Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP-1.3.2


## ÊàëÁöÑÈóÆÈ¢ò
windows ÁéØÂ¢ÉÔºåÊØèÊ¨°ÂàÜËØçÂêØÂä®Âêé‰ºöÁîüÊàêbin ÁºìÂ≠ò„ÄÇÂ¶ÇÊûúÊõ¥Êñ∞Â≠óÂÖ∏ÊàñÂà†Èô§ÁºìÂ≠òÔºå‰∏ãÊ¨°ÂêØÂä®Âêé‰ºöÂÜçÊ¨°ÁîüÊàê„ÄÇ
‰ΩÜÂú®linux ‰∏äÔºåÈ¶ñÊ¨°ËøêË°åÁîüÊàê‰∫ÜbinÔºåÊâãÂä®Âà†ÊéâÂêéÔºå‰∏ãÊ¨°Â∞±Ê≤°Êúâbin Êñá‰ª∂‰∫ÜÔºåËøôÊòØ‰ªÄ‰πàÊÉÖÂÜµÔºü‚Äô

"
‰æùÂ≠òÊñáÊ≥ïËøêË°åÊïàÁéáÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò
Áî®scalaÂÅöNLPÁöÑÂÖ∏ÂûãÊÑèËßÅÔºåÊúüÂæÖÊïàÊûúÂ¶Ç‰∏ãÔºö
![qq 20170531101801](https://cloud.githubusercontent.com/assets/10076707/26613211/3dd00be4-45ec-11e7-98f3-151ff9f90789.png)
ÊàëÁöÑËß£ÂÜ≥ÊñπÊ°àÔºö‰æùÂ≠òÊñáÊ≥ï + ÊèêÂèñÊ†∏ÂøÉ„ÄÅ‰∏ªË∞ì„ÄÅÂÆö‰∏≠ + ÂΩ¢ÊàêÊÑèËßÅ
‰ΩÜÊòØHanLPÁöÑ‰æùÂ≠òÊñáÊ≥ïËøêË°å Êó∂Èó¥ÂæàÈïøÔºåspark‰∏ä‰æùÂ≠ò 500 Êù° ËÆ∞ÂΩïÈúÄË¶Å40sÔºå‰∏çËÉΩÊª°Ë∂≥Ë¶ÅÊ±Ç
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
Ê≤°Êúâ‰øÆÊîπ‰ª£Á†ÅÔºåË∞ÉÁî®Ôºö
``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```
Âè™ÂÆûÁé∞‰∫ÜHdfsIOAdapterÔºö
``` scala
import java.io.{InputStream, OutputStream}
import com.hankcs.hanlp.corpus.io.IIOAdapter
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
/**
  * @author zzzhy
  *         2017-05-18
  */
object HdfsIOAdapter {
  private val conf = new Configuration()
}
class HdfsIOAdapter extends IIOAdapter {

  import HdfsIOAdapter._
  private val fs = FileSystem.newInstance(conf)

  override def create(path: String): OutputStream = {
    val fsDataOutputStream = fs.create(new Path(path))
    fsDataOutputStream.getWrappedStream
  }

  override def open(path: String): InputStream = {
    val fsDataInputStream = fs.open(new Path(path.replace(""\\"", ""/"")))
    fsDataInputStream.getWrappedStream
  }
}
```

### Ëß¶Âèë‰ª£Á†Å

``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```

Êúâ‰Ωï‰ΩøÁî®‰∏çÂΩìÊàñËÄÖÊúâ‰ΩïÊîπËøõÊñπÂºèÔºü
"
‰Ω†ËÆ≠ÁªÉ‰∫∫ÂêçËØÜÂà´ÊòØÁî®‰∫Ü‰∏ÄÂπ¥ÁöÑ‰∫∫Ê∞ëÊó•Êä•ÁöÑËØ≠ÊñôÂêóÔºü2014Âπ¥ÁöÑÂêóÔºüÊÄªÂÖ±ÊúâÂ§öÂ∞ëÁØáÊñáÁ´†Ôºü,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂàÜËØçËÉΩÂê¶ÂÆûÁé∞ËæìÂá∫Â§ö‰∏™ÂàÜËØçÁªìÊûú,"ÊØîÂ¶ÇÔºö „ÄéË¥≠Áâ©Â∞±‰∏äÂ§©Áå´„Äè  
ËøôÂè•ËØù‰ºöÁî®ÂêÑÁßçÂàÜËØçÊñπÊ≥ïÈÉΩÊòØËøôÊ†∑ÁöÑÁªìÊûúÔºö
[Ë¥≠Áâ©/vn, Â∞±/d, ‰∏äÂ§©/vi, Áå´/n]

Â§©Áå´Ëøô‰∏™Âú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÔºåÂàáËØçÈ¢ëËÆæÁΩÆÂæàÈ´ò‰πüÊ≤°ÊúâÁî®„ÄÇ  
HanlpËÉΩÂê¶ÂÆûÁé∞ÊääÂ§öÁßçÂàÜËØçÂèØËÉΩÈÉΩËæìÂá∫Ôºå ÊØîÂ¶Ç‰∏äËØâÊÉÖÂÜµÔºåËøîÂõû‰∏§ÁßçÁªìÊûúÔºö  
[[Ë¥≠Áâ©/vn, Â∞±/d, ‰∏äÂ§©/vi, Áå´/n],[Ë¥≠Áâ©/vn, Â∞±/d, ‰∏ä/vÂ§©Áå´/ntc]]"
TextRankKeyword ÊèêÂèñÁ™óÂè£Áõ∏ËøëËØçÁöÑÂº∫Âåñ,"<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->
ÊÄßËÉΩÂº∫Âåñ
## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->
[TextRankKeyword ÊèêÂèñÁ™óÂè£Áõ∏ËøëËØçÁöÑÂº∫Âåñ](https://github.com/hankcs/HanLP/issues/546)

"
Update CharTable.txt,"* [x] ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* [x] ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* [x] ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* [x] ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Ê∑ªÂä†‰∫ÜÂÖ¨Âºè‰ª•ÂèäÂåñÂ≠¶Âºè‰∏ä‰∏ãÊ†á‰∏éÊï∞Â≠óÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ΩøÁöÑ‰∏ä‰∏ãÊ†áÂèØ‰ª•Ë¢´Ê≠£Á°ÆËØÜÂà´

## Áõ∏ÂÖ≥issue
[#543](https://github.com/hankcs/HanLP/issues/543)



"
TextRankKeyword ÊèêÂèñÁ™óÂè£Áõ∏ËøëËØçÁöÑÂº∫Âåñ,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.0ÔºåÂíå1.3.4Ê∫êÁ†Å‰∏ÄÊ†∑


## ÊàëÁöÑÈóÆÈ¢ò

Â∞èÂ∞èÁöÑÂäüËÉΩÂº∫ÂåñÔºå
Ê∫êÁ†ÅÔºöÊèêÂèñ‰∏¥ËøëÁ™óÂè£ËØçÁöÑÂ§çÊùÇÂ∫¶‰∏∫o(n^2)Ôºån‰∏∫Á™óÂè£Â§ßÂ∞è
Âª∫ËÆÆÔºöÊèêÂèñ‰∏¥ËøëÁ™óÂè£ËØçÁöÑÂ§çÊùÇÂ∫¶‰∏∫o(n)Ôºån‰∏∫Á™óÂè£Â§ßÂ∞è

ÊÉ≥Ê≥ïÔºöÁî±‰∫éÊòØÊñáÁ´†ÔºåÂÅáËÆæËØçÁöÑÊÄªÊï∞‰∏∫CÔºåÊâÄ‰ª•Êó∂Èó¥Âèà‰ºöË¢´Á∫øÊÄßÊîæÂ§ßCÂÄçÔºåÂª∫ËÆÆËøòÊòØÂèØ‰ª•Â§ÑÁêÜ‰∏Ä‰∏ã
Âú∞ÂùÄÔºöcom.hankcs.hanlp.summary.TextRankKeyword.getRank(List<Term> termList) ÊñπÊ≥ï
Ê∫êÁ†ÅÔºö
<pre><code>            que.offer(w);
            if (que.size() > 5) {
                que.poll();
            }

            for (String w1 : que)
            {
                for (String w2 : que)
                {
                    if (w1.equals(w2))
                    {
                        continue;
                    }

                    words.get(w1).add(w2);
                    words.get(w2).add(w1);
                }
            }
</code></pre>
Âª∫ËÆÆÔºö
<pre><code>     
            // Â§çÊùÇÂ∫¶O(n-1)
            if (que.size() >= 5) {
                que.poll();
            }            
            for (String qWord : que) {
                if (w.equals(qWord)) {
                    continue;
                }
                //Êó¢ÁÑ∂ÊòØÈÇªÂ±Ö,ÈÇ£‰πàÂÖ≥Á≥ªÊòØÁõ∏‰∫íÁöÑ,ÈÅçÂéÜ‰∏ÄÈÅçÂç≥ÂèØ
                words.get(w).add(qWord);
                words.get(qWord).add(w);
            }
            que.offer(w);
</code></pre>
## ÊµãËØïÁªìÊûúÔºö
* ÊµãËØï‰∏ÄÔºö
  - Âè•Â≠êÔºö
Á®ãÂ∫èÂëòÊòØ‰ªé‰∫ãÁ®ãÂ∫èÂºÄÂèë„ÄÅÁª¥Êä§ÁöÑ‰∏ì‰∏ö‰∫∫Âëò„ÄÇ‰∏ÄËà¨Â∞ÜÁ®ãÂ∫èÂëòÂàÜ‰∏∫Á®ãÂ∫èËÆæËÆ°‰∫∫ÂëòÂíåÁ®ãÂ∫èÁºñÁ†Å‰∫∫ÂëòÔºå‰ΩÜ‰∏§ËÄÖÁöÑÁïåÈôêÂπ∂‰∏çÈùûÂ∏∏Ê∏ÖÊ•öÔºåÁâπÂà´ÊòØÂú®‰∏≠ÂõΩ„ÄÇËΩØ‰ª∂‰ªé‰∏ö‰∫∫ÂëòÂàÜ‰∏∫ÂàùÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÈ´òÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÁ≥ªÁªüÂàÜÊûêÂëòÂíåÈ°πÁõÆÁªèÁêÜÂõõÂ§ßÁ±ª„ÄÇ
  - Ê∫êÁ†ÅÔºö
{‰∏ì‰∏ö=[‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], ‰∏≠ÂõΩ=[‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫èÂëò, ËΩØ‰ª∂, ÈùûÂ∏∏], ‰∫∫Âëò=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ÂàÜ‰∏∫, ÂºÄÂèë, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§, ÁºñÁ†Å, ËÆæËÆ°, ËΩØ‰ª∂, ÈùûÂ∏∏, È´òÁ∫ß], ‰ªé‰∫ã=[‰∏ì‰∏ö, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], ÂàÜ‰∏∫=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ‰∫∫Âëò, ÁâπÂà´, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Á≥ªÁªü, Áª¥Êä§, ËÆæËÆ°, ËΩØ‰ª∂, È´òÁ∫ß], ÂàÜÊûêÂëò=[Á®ãÂ∫èÂëò, Á≥ªÁªü, ÁªèÁêÜ, È°πÁõÆ, È´òÁ∫ß], ÂºÄÂèë=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], Ê∏ÖÊ•ö=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÁâπÂà´, ÁïåÈôê, ÁºñÁ†Å, ËΩØ‰ª∂, ÈùûÂ∏∏], ÁâπÂà´=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁïåÈôê, ËΩØ‰ª∂, ÈùûÂ∏∏], ÁïåÈôê=[‰∏≠ÂõΩ, ‰∫∫Âëò, Ê∏ÖÊ•ö, ÁâπÂà´, Á®ãÂ∫è, ÁºñÁ†Å, ÈùûÂ∏∏], Á®ãÂ∫è=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, ÁïåÈôê, Á®ãÂ∫èÂëò, Áª¥Êä§, ÁºñÁ†Å, ËÆæËÆ°, ÈùûÂ∏∏], Á®ãÂ∫èÂëò=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂàÜÊûêÂëò, ÂºÄÂèë, Á®ãÂ∫è, Á≥ªÁªü, ÁªèÁêÜ, Áª¥Êä§, ËÆæËÆ°, ËΩØ‰ª∂, È°πÁõÆ, È´òÁ∫ß], Á≥ªÁªü=[ÂàÜ‰∏∫, ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, ÁªèÁêÜ, È°πÁõÆ, È´òÁ∫ß], ÁªèÁêÜ=[ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, È°πÁõÆ], Áª¥Êä§=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò], ÁºñÁ†Å=[‰∫∫Âëò, Ê∏ÖÊ•ö, ÁïåÈôê, Á®ãÂ∫è, ËÆæËÆ°, ÈùûÂ∏∏], ËÆæËÆ°=[‰∫∫Âëò, ÂàÜ‰∏∫, Á®ãÂ∫è, Á®ãÂ∫èÂëò, ÁºñÁ†Å], ËΩØ‰ª∂=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁâπÂà´, Á®ãÂ∫èÂëò, ÈùûÂ∏∏, È´òÁ∫ß], ÈùûÂ∏∏=[‰∏≠ÂõΩ, ‰∫∫Âëò, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫è, ÁºñÁ†Å, ËΩØ‰ª∂], È°πÁõÆ=[ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, ÁªèÁêÜ, È´òÁ∫ß], È´òÁ∫ß=[‰∫∫Âëò, ÂàÜ‰∏∫, ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, ËΩØ‰ª∂, È°πÁõÆ]}
  - Âª∫ËÆÆÔºö
{‰∏ì‰∏ö=[‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], ‰∏≠ÂõΩ=[‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫èÂëò, ËΩØ‰ª∂, ÈùûÂ∏∏], ‰∫∫Âëò=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ÂàÜ‰∏∫, ÂºÄÂèë, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§, ÁºñÁ†Å, ËÆæËÆ°, ËΩØ‰ª∂, ÈùûÂ∏∏, È´òÁ∫ß], ‰ªé‰∫ã=[‰∏ì‰∏ö, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], ÂàÜ‰∏∫=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ‰∫∫Âëò, ÁâπÂà´, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Á≥ªÁªü, Áª¥Êä§, ËÆæËÆ°, ËΩØ‰ª∂, È´òÁ∫ß], ÂàÜÊûêÂëò=[Á®ãÂ∫èÂëò, Á≥ªÁªü, ÁªèÁêÜ, È°πÁõÆ, È´òÁ∫ß], ÂºÄÂèë=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, Á®ãÂ∫è, Á®ãÂ∫èÂëò, Áª¥Êä§], Ê∏ÖÊ•ö=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÁâπÂà´, ÁïåÈôê, ÁºñÁ†Å, ËΩØ‰ª∂, ÈùûÂ∏∏], ÁâπÂà´=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁïåÈôê, ËΩØ‰ª∂, ÈùûÂ∏∏], ÁïåÈôê=[‰∏≠ÂõΩ, ‰∫∫Âëò, Ê∏ÖÊ•ö, ÁâπÂà´, Á®ãÂ∫è, ÁºñÁ†Å, ÈùûÂ∏∏], Á®ãÂ∫è=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, ÁïåÈôê, Á®ãÂ∫èÂëò, Áª¥Êä§, ÁºñÁ†Å, ËÆæËÆ°, ÈùûÂ∏∏], Á®ãÂ∫èÂëò=[‰∏ì‰∏ö, ‰∏≠ÂõΩ, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂàÜÊûêÂëò, ÂºÄÂèë, Á®ãÂ∫è, Á≥ªÁªü, ÁªèÁêÜ, Áª¥Êä§, ËÆæËÆ°, ËΩØ‰ª∂, È°πÁõÆ, È´òÁ∫ß], Á≥ªÁªü=[ÂàÜ‰∏∫, ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, ÁªèÁêÜ, È°πÁõÆ, È´òÁ∫ß], ÁªèÁêÜ=[ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, È°πÁõÆ], Áª¥Êä§=[‰∏ì‰∏ö, ‰∫∫Âëò, ‰ªé‰∫ã, ÂàÜ‰∏∫, ÂºÄÂèë, Á®ãÂ∫è, Á®ãÂ∫èÂëò], ÁºñÁ†Å=[‰∫∫Âëò, Ê∏ÖÊ•ö, ÁïåÈôê, Á®ãÂ∫è, ËÆæËÆ°, ÈùûÂ∏∏], ËÆæËÆ°=[‰∫∫Âëò, ÂàÜ‰∏∫, Á®ãÂ∫è, Á®ãÂ∫èÂëò, ÁºñÁ†Å], ËΩØ‰ª∂=[‰∏≠ÂõΩ, ‰∫∫Âëò, ÂàÜ‰∏∫, Ê∏ÖÊ•ö, ÁâπÂà´, Á®ãÂ∫èÂëò, ÈùûÂ∏∏, È´òÁ∫ß], ÈùûÂ∏∏=[‰∏≠ÂõΩ, ‰∫∫Âëò, Ê∏ÖÊ•ö, ÁâπÂà´, ÁïåÈôê, Á®ãÂ∫è, ÁºñÁ†Å, ËΩØ‰ª∂], È°πÁõÆ=[ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, ÁªèÁêÜ, È´òÁ∫ß], È´òÁ∫ß=[‰∫∫Âëò, ÂàÜ‰∏∫, ÂàÜÊûêÂëò, Á®ãÂ∫èÂëò, Á≥ªÁªü, ËΩØ‰ª∂, È°πÁõÆ]}

* ÊµãËØï‰∫åÔºö
  - Âè•Â≠êÔºö
ÁÆóÊ≥ïÂèØÂ§ßËá¥ÂàÜ‰∏∫Âü∫Êú¨ÁÆóÊ≥ï„ÄÅÊï∞ÊçÆÁªìÊûÑÁöÑÁÆóÊ≥ï„ÄÅÊï∞ËÆ∫ÁÆóÊ≥ï„ÄÅËÆ°ÁÆóÂá†‰ΩïÁöÑÁÆóÊ≥ï„ÄÅÂõæÁöÑÁÆóÊ≥ï„ÄÅÂä®ÊÄÅËßÑÂàí‰ª•ÂèäÊï∞ÂÄºÂàÜÊûê„ÄÅÂä†ÂØÜÁÆóÊ≥ï„ÄÅÊéíÂ∫èÁÆóÊ≥ï„ÄÅÊ£ÄÁ¥¢ÁÆóÊ≥ï„ÄÅÈöèÊú∫ÂåñÁÆóÊ≥ï„ÄÅÂπ∂Ë°åÁÆóÊ≥ï„ÄÅÂéÑÁ±≥ÂèòÂΩ¢Ê®°Âûã„ÄÅÈöèÊú∫Ê£ÆÊûóÁÆóÊ≥ï„ÄÇ
ÁÆóÊ≥ïÂèØ‰ª•ÂÆΩÊ≥õÁöÑÂàÜ‰∏∫‰∏âÁ±ªÔºå
‰∏ÄÔºåÊúâÈôêÁöÑÁ°ÆÂÆöÊÄßÁÆóÊ≥ïÔºåËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑ‰∏ÄÊÆµÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇ‰ªñ‰ª¨ÂèØËÉΩË¶ÅËä±ÂæàÈïøÊó∂Èó¥Êù•ÊâßË°åÊåáÂÆöÁöÑ‰ªªÂä°Ôºå‰ΩÜ‰ªçÂ∞ÜÂú®‰∏ÄÂÆöÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇËøôÁ±ªÁÆóÊ≥ïÂæóÂá∫ÁöÑÁªìÊûúÂ∏∏ÂèñÂÜ≥‰∫éËæìÂÖ•ÂÄº„ÄÇ
‰∫åÔºåÊúâÈôêÁöÑÈùûÁ°ÆÂÆöÁÆóÊ≥ïÔºåËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫é‰∏Ä‰∏™ÔºàÊàñ‰∏Ä‰∫õÔºâÁªôÂÆöÁöÑÊï∞ÂÄºÔºåÁÆóÊ≥ïÁöÑÁªìÊûúÂπ∂‰∏çÊòØÂîØ‰∏ÄÁöÑÊàñÁ°ÆÂÆöÁöÑ„ÄÇ
‰∏âÔºåÊó†ÈôêÁöÑÁÆóÊ≥ïÔºåÊòØÈÇ£‰∫õÁî±‰∫éÊ≤°ÊúâÂÆö‰πâÁªàÊ≠¢ÂÆö‰πâÊù°‰ª∂ÔºåÊàñÂÆö‰πâÁöÑÊù°‰ª∂Êó†Ê≥ïÁî±ËæìÂÖ•ÁöÑÊï∞ÊçÆÊª°Ë∂≥ËÄå‰∏çÁªàÊ≠¢ËøêË°åÁöÑÁÆóÊ≥ï„ÄÇÈÄöÂ∏∏ÔºåÊó†ÈôêÁÆóÊ≥ïÁöÑ‰∫ßÁîüÊòØÁî±‰∫éÊú™ËÉΩÁ°ÆÂÆöÁöÑÂÆö‰πâÁªàÊ≠¢Êù°‰ª∂„ÄÇ

  - Ê∫êÁ†ÅÔºö
{‰∏ÄÂÆö=[‰ªªÂä°, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÁÆóÊ≥ï, ÁªàÊ≠¢], ‰∫ßÁîü=[ÂÆö‰πâ, Êú™ËÉΩ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å, ÈÄöÂ∏∏], ‰ªªÂä°=[‰∏ÄÂÆö, ÂèØËÉΩ, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÁÆóÊ≥ï, ÁªàÊ≠¢], Âá†‰Ωï=[Âä®ÊÄÅ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ËßÑÂàí, ËÆ°ÁÆó], ÂàÜ‰∏∫=[Âü∫Êú¨, Â§ßËá¥, ÂÆΩÊ≥õ, Êï∞ÊçÆ, ÊúâÈôê, Ê£ÆÊûó, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªìÊûÑ], ÂàÜÊûê=[Âä†ÂØÜ, Âä®ÊÄÅ, ÊéíÂ∫è, Êï∞ÂÄº, ÁÆóÊ≥ï, ËßÑÂàí], Âä†ÂØÜ=[ÂàÜÊûê, Âä®ÊÄÅ, ÊéíÂ∫è, Êï∞ÂÄº, Ê£ÄÁ¥¢, ÁÆóÊ≥ï, ËßÑÂàí], Âä®ÊÄÅ=[Âá†‰Ωï, ÂàÜÊûê, Âä†ÂØÜ, Êï∞ÂÄº, ÁÆóÊ≥ï, ËßÑÂàí, ËÆ°ÁÆó], ÂèñÂÜ≥‰∫é=[ÂæóÂá∫, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÂèòÂΩ¢=[Âπ∂Ë°å, Ê£ÆÊûó, Ê®°Âûã, ÁÆóÊ≥ï], ÂèØËÉΩ=[‰ªªÂä°, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÊúâÈôê, ÁÆóÊ≥ï, ÁªàÊ≠¢], Âü∫Êú¨=[ÂàÜ‰∏∫, Â§ßËá¥, Êï∞ÊçÆ, ÁÆóÊ≥ï, ÁªìÊûÑ], Â§ßËá¥=[ÂàÜ‰∏∫, Âü∫Êú¨, Êï∞ÊçÆ, ÁÆóÊ≥ï], ÂÆö‰πâ=[‰∫ßÁîü, Êï∞ÊçÆ, Êó†Ê≥ï, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÂÆΩÊ≥õ=[ÂàÜ‰∏∫, ÊúâÈôê, Ê£ÆÊûó, Ê®°Âûã, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï], Âπ∂Ë°å=[ÂèòÂΩ¢, Ê£ÄÁ¥¢, Ê£ÆÊûó, Ê®°Âûã, ÁÆóÊ≥ï], ÂæóÂá∫=[‰∏ÄÂÆö, ÂèñÂÜ≥‰∫é, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÊâßË°å=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèØËÉΩ, ÊåáÂÆö, Êó∂Èó¥, ÁªàÊ≠¢], ÊåáÂÆö=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèØËÉΩ, ÊâßË°å, Êó∂Èó¥, ÁªàÊ≠¢], ÊéíÂ∫è=[ÂàÜÊûê, Âä†ÂØÜ, Êï∞ÂÄº, Ê£ÄÁ¥¢, ÁÆóÊ≥ï], Êï∞ÂÄº=[ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, ÊéíÂ∫è, Êó∂Èó¥, ÊúâÈôê, Ê≤°Êúâ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËßÑÂàí], Êï∞ÊçÆ=[ÂàÜ‰∏∫, Âü∫Êú¨, Â§ßËá¥, ÂÆö‰πâ, Êï∞ËÆ∫, Êó†Ê≥ï, Êù°‰ª∂, Êª°Ë∂≥, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªìÊûÑ, ËæìÂÖ•, ËøêË°å], Êï∞ËÆ∫=[Âá†‰Ωï, Êï∞ÊçÆ, ÁÆóÊ≥ï, ÁªìÊûÑ, ËÆ°ÁÆó], Êó†Ê≥ï=[ÂÆö‰πâ, Êï∞ÊçÆ, Êù°‰ª∂, Êª°Ë∂≥, ÁªàÊ≠¢, ËæìÂÖ•], Êó∂Èó¥=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êï∞ÂÄº, ÊúâÈôê, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö], ÊúâÈôê=[ÂàÜ‰∏∫, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂÆΩÊ≥õ, ÂæóÂá∫, Êï∞ÂÄº, Êó∂Èó¥, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËæìÂÖ•], Êú™ËÉΩ=[‰∫ßÁîü, ÂÆö‰πâ, Êù°‰ª∂, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÈÄöÂ∏∏], Êù°‰ª∂=[ÂÆö‰πâ, Êï∞ÊçÆ, Êó†Ê≥ï, Êú™ËÉΩ, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, ÁªàÊ≠¢, ËæìÂÖ•], Ê£ÄÁ¥¢=[Âä†ÂØÜ, Âπ∂Ë°å, ÊéíÂ∫è, ÁÆóÊ≥ï], Ê£ÆÊûó=[ÂàÜ‰∏∫, ÂèòÂΩ¢, ÂÆΩÊ≥õ, Âπ∂Ë°å, Ê®°Âûã, ÁÆóÊ≥ï], Ê®°Âûã=[ÂèòÂΩ¢, ÂÆΩÊ≥õ, Âπ∂Ë°å, Ê£ÆÊûó, ÁÆóÊ≥ï], Ê≤°Êúâ=[ÂÆö‰πâ, Êï∞ÂÄº, Êù°‰ª∂, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢], Êª°Ë∂≥=[Êï∞ÊçÆ, Êó†Ê≥ï, Êù°‰ª∂, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], Á°ÆÂÆö=[‰∫ßÁîü, ÂèñÂÜ≥‰∫é, ÂÆö‰πâ, ÂæóÂá∫, Êï∞ÂÄº, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËæìÂÖ•, ÈÄöÂ∏∏], Á°ÆÂÆöÊÄß=[ÂàÜ‰∏∫, ÂÆΩÊ≥õ, Êó∂Èó¥, ÊúâÈôê, ÁÆóÊ≥ï], ÁÆóÊ≥ï=[‰∏ÄÂÆö, ‰∫ßÁîü, ‰ªªÂä°, Âá†‰Ωï, ÂàÜ‰∏∫, ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, ÂèñÂÜ≥‰∫é, ÂèòÂΩ¢, ÂèØËÉΩ, Âü∫Êú¨, Â§ßËá¥, ÂÆö‰πâ, ÂÆΩÊ≥õ, Âπ∂Ë°å, ÂæóÂá∫, ÊéíÂ∫è, Êï∞ÂÄº, Êï∞ÊçÆ, Êï∞ËÆ∫, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Ê£ÄÁ¥¢, Ê£ÆÊûó, Ê®°Âûã, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁªàÊ≠¢, ÁªìÊûÑ, ÁªôÂÆö, ËßÑÂàí, ËÆ°ÁÆó, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], ÁªàÊ≠¢=[‰∏ÄÂÆö, ‰∫ßÁîü, ‰ªªÂä°, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂÆö‰πâ, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êï∞ÂÄº, Êï∞ÊçÆ, Êó†Ê≥ï, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªôÂÆö, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], ÁªìÊûÑ=[ÂàÜ‰∏∫, Âü∫Êú¨, Êï∞ÊçÆ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ËÆ°ÁÆó], ÁªôÂÆö=[Êï∞ÂÄº, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢], ËßÑÂàí=[Âá†‰Ωï, ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, Êï∞ÂÄº, ÁÆóÊ≥ï], ËÆ°ÁÆó=[Âá†‰Ωï, Âä®ÊÄÅ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ÁªìÊûÑ], ËæìÂÖ•=[ÂèñÂÜ≥‰∫é, ÂÆö‰πâ, ÂæóÂá∫, Êï∞ÊçÆ, Êó†Ê≥ï, ÊúâÈôê, Êù°‰ª∂, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å], ËøêË°å=[‰∫ßÁîü, Êï∞ÊçÆ, Êª°Ë∂≥, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•, ÈÄöÂ∏∏], ÈÄöÂ∏∏=[‰∫ßÁîü, Êú™ËÉΩ, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å]}
  - Âª∫ËÆÆÔºö
{‰∏ÄÂÆö=[‰ªªÂä°, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÁÆóÊ≥ï, ÁªàÊ≠¢], ‰∫ßÁîü=[ÂÆö‰πâ, Êú™ËÉΩ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å, ÈÄöÂ∏∏], ‰ªªÂä°=[‰∏ÄÂÆö, ÂèØËÉΩ, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÁÆóÊ≥ï, ÁªàÊ≠¢], Âá†‰Ωï=[Âä®ÊÄÅ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ËßÑÂàí, ËÆ°ÁÆó], ÂàÜ‰∏∫=[Âü∫Êú¨, Â§ßËá¥, ÂÆΩÊ≥õ, Êï∞ÊçÆ, ÊúâÈôê, Ê£ÆÊûó, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªìÊûÑ], ÂàÜÊûê=[Âä†ÂØÜ, Âä®ÊÄÅ, ÊéíÂ∫è, Êï∞ÂÄº, ÁÆóÊ≥ï, ËßÑÂàí], Âä†ÂØÜ=[ÂàÜÊûê, Âä®ÊÄÅ, ÊéíÂ∫è, Êï∞ÂÄº, Ê£ÄÁ¥¢, ÁÆóÊ≥ï, ËßÑÂàí], Âä®ÊÄÅ=[Âá†‰Ωï, ÂàÜÊûê, Âä†ÂØÜ, Êï∞ÂÄº, ÁÆóÊ≥ï, ËßÑÂàí, ËÆ°ÁÆó], ÂèñÂÜ≥‰∫é=[ÂæóÂá∫, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÂèòÂΩ¢=[Âπ∂Ë°å, Ê£ÆÊûó, Ê®°Âûã, ÁÆóÊ≥ï], ÂèØËÉΩ=[‰ªªÂä°, ÊâßË°å, ÊåáÂÆö, Êó∂Èó¥, ÊúâÈôê, ÁÆóÊ≥ï, ÁªàÊ≠¢], Âü∫Êú¨=[ÂàÜ‰∏∫, Â§ßËá¥, Êï∞ÊçÆ, ÁÆóÊ≥ï, ÁªìÊûÑ], Â§ßËá¥=[ÂàÜ‰∏∫, Âü∫Êú¨, Êï∞ÊçÆ, ÁÆóÊ≥ï], ÂÆö‰πâ=[‰∫ßÁîü, Êï∞ÊçÆ, Êó†Ê≥ï, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÂÆΩÊ≥õ=[ÂàÜ‰∏∫, ÊúâÈôê, Ê£ÆÊûó, Ê®°Âûã, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï], Âπ∂Ë°å=[ÂèòÂΩ¢, Ê£ÄÁ¥¢, Ê£ÆÊûó, Ê®°Âûã, ÁÆóÊ≥ï], ÂæóÂá∫=[‰∏ÄÂÆö, ÂèñÂÜ≥‰∫é, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•], ÊâßË°å=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèØËÉΩ, ÊåáÂÆö, Êó∂Èó¥, ÁªàÊ≠¢], ÊåáÂÆö=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèØËÉΩ, ÊâßË°å, Êó∂Èó¥, ÁªàÊ≠¢], ÊéíÂ∫è=[ÂàÜÊûê, Âä†ÂØÜ, Êï∞ÂÄº, Ê£ÄÁ¥¢, ÁÆóÊ≥ï], Êï∞ÂÄº=[ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, ÊéíÂ∫è, Êó∂Èó¥, ÊúâÈôê, Ê≤°Êúâ, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËßÑÂàí], Êï∞ÊçÆ=[ÂàÜ‰∏∫, Âü∫Êú¨, Â§ßËá¥, ÂÆö‰πâ, Êï∞ËÆ∫, Êó†Ê≥ï, Êù°‰ª∂, Êª°Ë∂≥, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªìÊûÑ, ËæìÂÖ•, ËøêË°å], Êï∞ËÆ∫=[Âá†‰Ωï, Êï∞ÊçÆ, ÁÆóÊ≥ï, ÁªìÊûÑ, ËÆ°ÁÆó], Êó†Ê≥ï=[ÂÆö‰πâ, Êï∞ÊçÆ, Êù°‰ª∂, Êª°Ë∂≥, ÁªàÊ≠¢, ËæìÂÖ•], Êó∂Èó¥=[‰∏ÄÂÆö, ‰ªªÂä°, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êï∞ÂÄº, ÊúâÈôê, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö], ÊúâÈôê=[ÂàÜ‰∏∫, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂÆΩÊ≥õ, ÂæóÂá∫, Êï∞ÂÄº, Êó∂Èó¥, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËæìÂÖ•], Êú™ËÉΩ=[‰∫ßÁîü, ÂÆö‰πâ, Êù°‰ª∂, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÈÄöÂ∏∏], Êù°‰ª∂=[ÂÆö‰πâ, Êï∞ÊçÆ, Êó†Ê≥ï, Êú™ËÉΩ, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, ÁªàÊ≠¢, ËæìÂÖ•], Ê£ÄÁ¥¢=[Âä†ÂØÜ, Âπ∂Ë°å, ÊéíÂ∫è, ÁÆóÊ≥ï], Ê£ÆÊûó=[ÂàÜ‰∏∫, ÂèòÂΩ¢, ÂÆΩÊ≥õ, Âπ∂Ë°å, Ê®°Âûã, ÁÆóÊ≥ï], Ê®°Âûã=[ÂèòÂΩ¢, ÂÆΩÊ≥õ, Âπ∂Ë°å, Ê£ÆÊûó, ÁÆóÊ≥ï], Ê≤°Êúâ=[ÂÆö‰πâ, Êï∞ÂÄº, Êù°‰ª∂, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢], Êª°Ë∂≥=[Êï∞ÊçÆ, Êó†Ê≥ï, Êù°‰ª∂, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], Á°ÆÂÆö=[‰∫ßÁîü, ÂèñÂÜ≥‰∫é, ÂÆö‰πâ, ÂæóÂá∫, Êï∞ÂÄº, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, ÁÆóÊ≥ï, ÁªàÊ≠¢, ÁªôÂÆö, ËæìÂÖ•, ÈÄöÂ∏∏], Á°ÆÂÆöÊÄß=[ÂàÜ‰∏∫, ÂÆΩÊ≥õ, Êó∂Èó¥, ÊúâÈôê, ÁÆóÊ≥ï], ÁÆóÊ≥ï=[‰∏ÄÂÆö, ‰∫ßÁîü, ‰ªªÂä°, Âá†‰Ωï, ÂàÜ‰∏∫, ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, ÂèñÂÜ≥‰∫é, ÂèòÂΩ¢, ÂèØËÉΩ, Âü∫Êú¨, Â§ßËá¥, ÂÆö‰πâ, ÂÆΩÊ≥õ, Âπ∂Ë°å, ÂæóÂá∫, ÊéíÂ∫è, Êï∞ÂÄº, Êï∞ÊçÆ, Êï∞ËÆ∫, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Ê£ÄÁ¥¢, Ê£ÆÊûó, Ê®°Âûã, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, Á°ÆÂÆöÊÄß, ÁªàÊ≠¢, ÁªìÊûÑ, ÁªôÂÆö, ËßÑÂàí, ËÆ°ÁÆó, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], ÁªàÊ≠¢=[‰∏ÄÂÆö, ‰∫ßÁîü, ‰ªªÂä°, ÂèñÂÜ≥‰∫é, ÂèØËÉΩ, ÂÆö‰πâ, ÂæóÂá∫, ÊâßË°å, ÊåáÂÆö, Êï∞ÂÄº, Êï∞ÊçÆ, Êó†Ê≥ï, Êó∂Èó¥, ÊúâÈôê, Êú™ËÉΩ, Êù°‰ª∂, Ê≤°Êúâ, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªôÂÆö, ËæìÂÖ•, ËøêË°å, ÈÄöÂ∏∏], ÁªìÊûÑ=[ÂàÜ‰∏∫, Âü∫Êú¨, Êï∞ÊçÆ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ËÆ°ÁÆó], ÁªôÂÆö=[Êï∞ÂÄº, Êó∂Èó¥, ÊúâÈôê, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢], ËßÑÂàí=[Âá†‰Ωï, ÂàÜÊûê, Âä†ÂØÜ, Âä®ÊÄÅ, Êï∞ÂÄº, ÁÆóÊ≥ï], ËÆ°ÁÆó=[Âá†‰Ωï, Âä®ÊÄÅ, Êï∞ËÆ∫, ÁÆóÊ≥ï, ÁªìÊûÑ], ËæìÂÖ•=[ÂèñÂÜ≥‰∫é, ÂÆö‰πâ, ÂæóÂá∫, Êï∞ÊçÆ, Êó†Ê≥ï, ÊúâÈôê, Êù°‰ª∂, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å], ËøêË°å=[‰∫ßÁîü, Êï∞ÊçÆ, Êª°Ë∂≥, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËæìÂÖ•, ÈÄöÂ∏∏], ÈÄöÂ∏∏=[‰∫ßÁîü, Êú™ËÉΩ, Êª°Ë∂≥, Á°ÆÂÆö, ÁÆóÊ≥ï, ÁªàÊ≠¢, ËøêË°å]}
[ÊµãËØïÁªìÊûú.txt](https://github.com/hankcs/HanLP/files/1028723/default.txt)


"
Ê†∏ÂøÉÂ≠óÂÖ∏Â¢ûÂä†Ëá™ÂÆö‰πâËØçÊÄß,"ÁâàÊú¨1.3.2
Â¢ûÂä†‰∫Ü‰∏Ä‰∏™Áß∞Ë∞ìËØçÊÄß ncw

        modified:   src/main/java/com/hankcs/hanlp/corpus/tag/Nature.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/common/Node.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/common/POSUtil.java
        modified:   src/main/java/com/hankcs/hanlp/dependency/nnparser/util/PosTagUtil.java

Âú®‰∏äÈù¢ÁöÑÊñá‰ª∂Âä†ÂÖ•‰∫Üncw


Âú®ÊµãËØï‚ÄúÂ§ñÂÖ¨‚ÄùÔºàÂ§ñÂÖ¨ ncw 10)Êó∂Âá∫Èîô

‰∫îÊúà 25, 2017 11:16:40 ‰∏äÂçà com.hankcs.hanlp.dictionary.CoreDictionaryTransformMatrixDictionary <clinit>
‰ø°ÊÅØ: Âä†ËΩΩÊ†∏ÂøÉËØçÂÖ∏ËØçÊÄßËΩ¨ÁßªÁü©Èòµ/C:/code/github/segmentation/src/main/resources/hanlp/data/dictionary/core/CoreNatureDictionary.tr.txtÊàêÂäüÔºåËÄóÊó∂Ôºö60 ms
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 151
	at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
	at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
	at com.hankcs.hanlp.seg.NShort.NShortSegment.segSentence(NShortSegment.java:133)
	at com.hankcs.hanlp.seg.Segment.segAndCheckNature(Segment.java:588)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:566)
	at com.twsz.creative.seg.service.HanlpOriginalTest.main(HanlpOriginalTest.java:173)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

ÁúãÁùÄÊòØËΩ¨ÁßªÁü©ÈòµÂá∫‰∫ÜÈîôÔºåCoreNatureDictionary.tr.txtÊ≤°ÊúâncwÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºå[ËÆ≠ÁªÉÂàÜËØçÊ®°Âûã](https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B) ‰∏≠ËØ¥Âà∞ÈÄöËøáËÆ≠ÁªÉËæìÂá∫ÁöÑËøô‰∏™Êñá‰ª∂Ôºå‰ΩÜÊàëÊ≤°ÊúâËØ≠ÊñôÔºåÊó†Ê≥ïËÆ≠ÁªÉ„ÄÇÊúâ‰ªÄ‰πàÂà´ÁöÑÂäûÊ≥ïÂèØ‰ª•Â§ÑÁêÜÂêóÔºüÔºà‰∏çÂêØÁî®Ëá™ÂÆö‰πâÂ≠óÂÖ∏Êù°‰ª∂‰∏ãÔºâ

"
Â≠óÁ¨¶Ë°®Âä†ËΩΩÂ§±Ë¥•ÂØºËá¥Á®ãÂ∫èÈÄÄÂá∫,"## ÁâàÊú¨Âè∑ 1.3.4

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

Â≠óÁ¨¶Ë°®Âä†ËΩΩÂ§±Ë¥•ÔºåÂØºËá¥‰∏ªÁ®ãÂ∫èÊï¥‰∏™Á®ãÂ∫èÈÄÄÂá∫„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

com.hankcs.hanlp.dictionary.other.CharType
```
           try
            {
                byteArray = generate();
            }
            catch (IOException e)
            {
                e.printStackTrace();
                logger.severe(""Â≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°® "" + HanLP.Config.CharTypePath + "" Âä†ËΩΩÂ§±Ë¥•Ôºö "" + TextUtility.exceptionToString(e));
                System.exit(-1);
            }
```
### ÊúüÊúõÁªìÊûú

Â≠óÁ¨¶Ë°®ÂàùÂßãÂåñÂ§±Ë¥•ÂêéÊäõÂá∫Áõ∏Â∫îÂºÇÂ∏∏Ôºå‰ªéËÄåÁî®Êà∑ÂèØ‰ª•ÂÅöÁõ∏Â∫îÂ§ÑÁêÜÔºåËÄå‰∏çÂ∫îËØ•ÂØºËá¥Êï¥‰∏™Á®ãÂ∫èÈÄÄÂá∫„ÄÇ



"
ÂÖ≥‰∫éÂ≠óÊØçÊï∞Â≠óÁªÑÂêàËØçÁöÑÂª∫ËÆÆ,"1. Âú®Êï∞Â≠¶Ë°®ËææÂºè‰ª•ÂèäÂåñÂ≠¶Ë°®ËææÂºèÈáåÈù¢ÊúâÂæàÂ§öÁöÑ‰∏ä‰∏ãÊ†áÔºåËøô‰∫õÁ¨¶Âè∑Âú®Â≠óÁ¨¶Ë°®ÈáåÈù¢Ê≤°ÊúâÂÆö‰πâÔºåÂ∏∏ËßÅÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂ¶Ç‰∏ãÔºö
```
¬∫=0
¬π=1
¬≤=2
¬≥=3
‚Å¥=4
‚Åµ=5
‚Å∂=6
‚Å∑=7
‚Å∏=8
‚Åπ=9
‚ÇÄ=0
‚ÇÅ=1
‚ÇÇ=2
‚ÇÉ=3
‚ÇÑ=4
‚ÇÖ=5
‚ÇÜ=6
‚Çá=7
‚Çà=8
‚Çâ=9
‚Åø=n
```
2. ÂØπ‰∫éÂ≠óÊØçÊï∞Â≠óÁöÑÁªÑÂêàÔºåÁõÆÂâçÁ≥ªÁªüÊòØÁõ¥Êé•ËøõË°åÂàÜÂâ≤ÁöÑÔºå‰æãÂ¶ÇÔºöÂåñÂ≠¶Ë°®ËææÂºèH2OÔºåÁõÆÂâçÁöÑÂàÜËØçÁªìÊûú‰∏∫H/2/OÔºõË∞ÉËØïÂèëÁé∞Âú®ÁîüÊàêËØçÁΩëÁöÑÊó∂ÂÄôÂ∞±Â∑≤ÁªèË¢´ÂàÜÂºÄ‰∫ÜÔºåÂª∫ËÆÆËÉΩÂ§üÂú®ÁîüÊàêËØçÁΩëÁöÑÊó∂ÂÄô‰∏çÂàÜÂºÄÔºåÁÑ∂ÂêéÂú®Á¥¢ÂºïÊ®°ÂºèÁöÑÁªÜÂàáÂàÜÊ®°Âºè‰∏≠ËøõË°å‰∫åÊ¨°ÂàáÂàÜÔºåÊïàÊûú‰ºöÊõ¥Â•Ω‰∏Ä‰∫õÔºõ‰æãÂ¶Çqq2017ÁöÑÂàÜËØçÁªìÊûú‰ºöÂèò‰∏∫Ôºöqq2017„ÄÅqq„ÄÅ2017ÔºõH2OÁöÑÁªìÊûú‰∏∫h20„ÄÅh„ÄÅ2„ÄÅoÔºõÂΩìÁÑ∂ÂèØ‰ª•Âú®‰∫åÊ¨°ÂàáÂàÜÁöÑÊó∂ÂÄôÂà§Êñ≠‰∏Ä‰∏ãÂ≠êËØçÁöÑÈïøÂ∫¶ÔºåÂ¶ÇÊûúÂ∞è‰∫éÁ≠â‰∫é1ÂàôËøõË°åÂøΩÁï•Ôºõ"
ArrayIndexOutOfBoundsExceptionÂºÇÂ∏∏,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4

## ÊàëÁöÑÈóÆÈ¢ò
ÂêåÊó∂ÊâìÂºÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂíåÊï∞ÈáèËØçËØÜÂà´ÔºåÂú®ÂàáÂàÜ‚Äú‰∏ÄÂàÜÈíüÂ∞±Á¥Ø‰∫Ü‚ÄùÊó∂‰ºöÂØºËá¥ÂºÇÂ∏∏Ôºö
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 8
        at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
        at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
ÂêåÊó∂ÊâìÂºÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂíåÊï∞ÈáèËØçËØÜÂà´Ôºå‰ºöÂØºËá¥ÂºÇÂ∏∏„ÄÇÂ¶Ç‰∏ãÈù¢‰ª£Á†ÅÔºö
        Segment seg = HanLP.newSegment();
        seg.enableAllNamedEntityRecognize(true);
        seg.enableNumberQuantifierRecognize(true);
        List<Term> list = seg.seg(""‰∏ÄÂàÜÈíüÂ∞±Á¥Ø‰∫Ü"");
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
‚ÄúÁ≥ä‚ÄùÁπÅ‰ΩìËΩ¨Êç¢ÂêéÁöÑÁªìÊûú‰∏∫‚ÄúÁÖ≥‚ÄúÔºåbug,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->
""Á≥ä""Âú®ËøõË°åÁÆÄ‰ΩìËΩ¨ÁπÅ‰ΩìÁªìÊûúÊòØ‚ÄúÁ≥ä‚ÄùÔºå‰ªéÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÊó∂ÁªìÊûúÊòØ‚ÄúÁÖ≥‚Äù
‰∫íÈÄÜÁöÑÊìç‰ΩúÔºåÁªìÊûú‰∏çÂêå„ÄÇÂ∫îÂ±û‰∫ébug

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->
ÊàëÊ≤°Êúâ‰øÆÊîπ‰ªª‰Ωï‰ª£Á†ÅÂíåËØçÂÖ∏

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
System.out.println(HanLP.convertToTraditionalChinese(""Á≥äÊ∂ÇÊûú""));
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
System.out.println(HanLP.convertToSimplifiedChinese(""Á≥äÂ°óÊûú""));
3. Êé•ÁùÄ‚Ä¶‚Ä¶
Âá∫ÁªìÊûú
### Ëß¶Âèë‰ª£Á†Å
System.out.println(HanLP.convertToTraditionalChinese(""Á≥äÊ∂ÇÊûú""));
		System.out.println(HanLP.convertToSimplifiedChinese(""Á≥äÂ°óÊûú""));
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```
Á≥äÂ°óÊûú
Á≥äÊ∂ÇÊûú

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```
Á≥äÂ°óÊûú
ÁÖ≥Ê∂ÇÊûú
## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
removeËá™ÂÆö‰πâÊ∑ªÂä†ËØçÁöÑÊó∂ÂÄôÈÅáÂà∞‰∏Ä‰∫õÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.4
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.4


## ÊàëÁöÑÈóÆÈ¢ò

Âú®Ëá™ÂÆö‰πâÊ∑ªÂä†Áî®Êà∑Â≠óÂÖ∏‰πãÂêéÔºåÊâßË°åÂà†Èô§Êìç‰ΩúÔºåÂèëÁé∞Êúâ‰∫õËØçÂà†‰∏çÊéâÔºåÊØîÂ¶Ç‚ÄúÂäõÂ∏Ü‚ÄùÔºå‚ÄúÊë©Ê†π‚Äù„ÄÇ
ÊàëÂÅö‰∫ÜÂá†Ê¨°ÊµãËØïÁªìÊûúÊòØÔºö
1.ÂçïÁã¨remove ‚ÄúÊë©Ê†π‚Äù  --ÊàêÂäü„ÄÇ
2.remove ‚ÄúÊë©Ê†π‚ÄùÔºå‚Äú123Ôºà123Ë°®Á§∫‰ªªÊÑè‰∏Ä‰∏™Â≠óÁ¨¶ÔºåÂç≥ÊàëÊ∑ªÂä†‰∫Ü2‰∏™ËØçÔºåÁé∞Âú®Ë¶ÅremoveËøô‰∏§‰∏™ËØçÔºâ Ê≠§Êó∂Â§±Ë¥•„ÄÇ
3.remove ‚ÄúÊë©Ê†π‚ÄùÔºå‚ÄúÂäõÂ∏Ü‚Äù Ê≠§Êó∂ÂèàÊòØÊàêÂäü„ÄÇ

Êú¨‰∫∫ÊòØÊñ∞ÊâãËèúÈ∏üÔºåÊÑüËßâÊòØ‰∏çÊòØÊüê‰∫õËØçÁõ¥Êé•ÁöÑÂÖ≥ËÅîÂÖ≥Á≥ªÂØºËá¥ÁöÑÔºü‰∏çÁü•ÈÅìÊ≠§Áé∞Ë±°ÊòØ‰∏∫‰ΩïÂéüÂõ†ÔºåÂèàÊòØÂ¶Ç‰ΩïÂéªËß£ÂÜ≥ÔºåÊúõÂ§ßÁ•ûÊåáÁÇπ„ÄÇ

ÈôÑ‰∏äÈÉ®ÂàÜ‰ª£Á†ÅÔºö
CustomDictionary.add(""123"");
		CustomDictionary.add(""Êë©Ê†π"");
		CustomDictionary.remove(""123"");
		CustomDictionary.remove(""Êë©Ê†π"");
Ê≠§Êó∂ÊúÄÂêé‰∏ÄË°å removeÊë©Ê†πÊä•Èîô„ÄÇ
		CustomDictionary.add(""ÂäõÂ∏Ü"");
		CustomDictionary.add(""Êë©Ê†π"");
		CustomDictionary.remove(""ÂäõÂ∏Ü"");
		CustomDictionary.remove(""Êë©Ê†π"");

Ê≠§Êó∂Á®ãÂ∫èÊâßË°åÊ≠£Â∏∏„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
HanLP.propertiesÂ¶Ç‰ΩïÂºïÁî®JavaÁ≥ªÁªüÂèÇÊï∞,"ÈóÆÈ¢ò‰∫ßÁîüÁöÑÊÉÖÊôØÊòØÔºöÊàëÊúâ‰∏Ä‰∏™‰ΩøÁî®Âà∞hanlpÁöÑwebappÈ°πÁõÆÂ∑•Á®ãÔºåÈúÄË¶ÅÈÉ®ÁΩ≤Âà∞windowsÂíålinuxÁ≠â‰∏çÂêåÁöÑÁ≥ªÁªüÁéØÂ¢É‰∏≠Ôºå‰ΩÜÊòØHanLPÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑË∑ØÂæÑÊØèÊ¨°ÈÉΩÈúÄË¶ÅÊâãÂä®‰øÆÊîπÔºåÊàëÊÉ≥ÂèØ‰∏çÂèØ‰ª•‰ΩøÁî®Á≥ªÁªüÂèÇÊï∞Êù•‰ª£ÊõøÂë¢
‰æãÂ¶ÇÔºö‰øÆÊîπ‰πãÂâçHanLP.properties‰∏≠ÁöÑÈÖçÁΩÆÊòØËøôÊ†∑ÁöÑ
```
    root=D:/dic/
```

‰øÆÊîπ‰πãÂêéÔºåÊúüÊúõÂèØ‰ª•ÂºïÁî®jvmÂèÇÊï∞Ôºà-Dhanlp.dic=D:/dic/ÔºâÔºå
```
root=${hanlp.dic}
```
Â¶ÇÊûúhanlpÊ≤°ÊúâÊ≠§ÂäüËÉΩÔºåÊàëÂ∫îËØ•ÊÄé‰πàÊâ©Â±ïÂë¢ÔºåÈúÄË¶Å‰øÆÊîπÂì™‰∫õ‰ª£Á†ÅÔºü




"
‰∏™Âà´Âè•Â≠êÂè•Ê≥ï‰æùÂ≠òÁöÑÊ†∏ÂøÉËØçÊúâËØØ,"ÁâàÊú¨1.3.2
ÂØπ‰∫éÂè•Ê≥ï‰æùÂ≠òÁöÑÁªìÊûúÔºåÊúâÁßçÊó†‰ªé‰øÆÊîπÁöÑÊÑüËßâÔºåÊòØ‰∏çÊòØÂè™ËÉΩÈáçÊñ∞ËÆ≠ÁªÉ‰∏Ä‰∏™Ê®°ÂûãÂë¢Ôºü

**""ÂÖÉÊó¶‰∏ãÂçà8ÁÇπ03ÂàÜÊèêÈÜíÊàëÂéªÂêéÊµ∑ÂíåËÄÅÊúãÂèãËÅö‰ºö"",**
Ê†∏ÂøÉËØç‰∏∫‚ÄúËÅö‰ºö‚ÄùÔºåÂ∫îËØ•‰∏∫‚ÄúÊèêÈÜí‚ÄùÔºåËÅö‰ºöÊú¨Êù•ÊòØÂä®ËØçÁöÑÔºåÊàë‰øÆÊîπÊàê‰∫ÜÂêçËØç„ÄÇ

1	ÂÖÉÊó¶	nt	t	_	2	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
2	‰∏ãÂçà	nt	t	_	3	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
3	8ÁÇπ	nt	t	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	03ÂàÜ	nt	t	_	5	Áä∂‰∏≠ÁªìÊûÑ	_	_
5	ÊèêÈÜí	v	v	_	8	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
6	Êàë	r	rr	_	5	ÂÖºËØ≠	_	_
7	Âéª	v	vf	_	5	Âä®ÂÆæÂÖ≥Á≥ª	_	_
8	Âêé	nd	f	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
9	Êµ∑ÂíåËÄÅ	nh	nr	_	10	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
10	ÊúãÂèã	n	n	_	11	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
11	ËÅö‰ºö	n	n	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_

ÂÖÉÊó¶ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ‰∏ãÂçà
‰∏ãÂçà --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> 8ÁÇπ
8ÁÇπ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> 03ÂàÜ
03ÂàÜ --(Áä∂‰∏≠ÁªìÊûÑ)--> ÊèêÈÜí
ÊèêÈÜí --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Âêé
Êàë --(ÂÖºËØ≠)--> ÊèêÈÜí
Âéª --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊèêÈÜí
Âêé --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ËÅö‰ºö
Êµ∑ÂíåËÄÅ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊúãÂèã
ÊúãÂèã --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ËÅö‰ºö
ËÅö‰ºö --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##

**‚ÄúÊØèÊòüÊúüÊó•‰∏≠Âçà‰∏ÄÁÇπÂçäÊèêÈÜíÊàëÂá∫ÂéªÁé©ÁöÑÊó∂Èó¥Âà∞‰∫Ü""**
Ê†∏ÂøÉËØç‰∏∫‚ÄúÂà∞‚ÄùÔºåÂ∫îËØ•‰∏∫‚ÄúÊèêÈÜí‚Äù

1	ÊØèÊòüÊúüÊó•	nt	t	_	2	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
2	‰∏≠Âçà	nt	t	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
3	‰∏ÄÁÇπ	nt	t	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	Âçä	m	mq	_	5	Áä∂‰∏≠ÁªìÊûÑ	_	_
5	ÊèêÈÜí	v	v	_	10	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
6	Êàë	r	rr	_	5	ÂÖºËØ≠	_	_
7	Âá∫Âéª	v	vf	_	5	Âä®ÂÆæÂÖ≥Á≥ª	_	_
8	Áé©	v	v	_	7	Âπ∂ÂàóÂÖ≥Á≥ª	_	_
9	ÁöÑ	u	ude1	_	7	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
10	Êó∂Èó¥	n	n	_	11	‰∏ªË∞ìÂÖ≥Á≥ª	_	_
11	Âà∞	v	v	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_
12	‰∫Ü	u	ule	_	11	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_

ÊØèÊòüÊúüÊó• --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ‰∏≠Âçà
‰∏≠Âçà --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Âçä
‰∏ÄÁÇπ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Âçä
Âçä --(Áä∂‰∏≠ÁªìÊûÑ)--> ÊèêÈÜí
ÊèêÈÜí --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Êó∂Èó¥
Êàë --(ÂÖºËØ≠)--> ÊèêÈÜí
Âá∫Âéª --(Âä®ÂÆæÂÖ≥Á≥ª)--> ÊèêÈÜí
Áé© --(Âπ∂ÂàóÂÖ≥Á≥ª)--> Âá∫Âéª
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> Âá∫Âéª
Êó∂Èó¥ --(‰∏ªË∞ìÂÖ≥Á≥ª)--> Âà∞
Âà∞ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##
‰∫Ü --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> Âà∞"
ÁîüÊàêCoreNatureDictionary.txt.binÁöÑÊ®°ÂùóÊòØÂì™‰∏™Ôºü,"   ÁîüÊàêCoreNatureDictionary.txt.binÁöÑÊ®°ÂùóÊòØÂì™‰∏™ÔºüÊàëÁî®NatureDictionaryMakerÊù•Âà∂‰Ωú‰∫ÜËá™Â∑±ÁöÑËØçÂÖ∏„ÄÇ‰ΩÜÊòØÁî®Ëá™Â∑±ÂÅöÁöÑËøô‰∏™ËØçÂÖ∏ÂéªËøõË°åÂàÜËØçÊòØÁ≥ªÁªüÊ≤°ÊúâÁîüÊàê*.txt.bin Êä•ÈîôÊó†Ê≥ïËøõË°åÂàÜËØç„ÄÇ
   ÊàëÂú®ÂÅöÁöÑ‰∏çÊòØ‰∏≠ÊñáÔºåÊòØÂ∞èÊï∞Ê∞ëÊóèÊñáÂ≠ó„ÄÇÊâÄ‰ª•ÊàëÊÉ≥Ë¶ÅÊòØÁîüÊàê*.txt.bin Êñá‰ª∂ÁöÑÊ®°ÂùóÊúâÁºñÁ†ÅÈôêÂà∂ÔºåÂØπ‰ªñËøõË°å‰øÆÊîπ„ÄÇ  
ÊàëÂú®Áî®ÁöÑÁâàÊú¨ÊòØ1.2.8"
ÂÖ≥‰∫éIIOAdapterÁöÑÁñëÈóÆ,"IIOAdapter‰∏≠Êúâ‰∏§‰∏™Êé•Âè£ÈúÄË¶ÅÂÆûÁé∞Ôºöopen‰∏écreateÔºõ
Áé∞Âú®ÊàëÊÉ≥Â∞ÜËØçÂ∫ìÊîæÂú®HDFS‰∏äÔºåËØªÂèñÂæàÂ•ΩÂÆûÁé∞ÔºåÂè™ÊòØÁé∞Âú®‰∏çÂ§ßÊ∏ÖÊ•öcreateÊñπÊ≥ïÁöÑÂÖ∑‰Ωì‰ΩúÁî®Ôºõ
Áé∞Âú®ÈúÄË¶ÅÊòéÁ°ÆÁöÑÊòØÔºö
1. createÊòØÂê¶ÊòØÁîüÊàêbinÊñá‰ª∂‰ΩøÁî®ÁöÑÔºõ
2. Âú®Linux‰∏äË≤å‰ººÊòØ‰∏çÁîüÊàêbinÊñá‰ª∂ÁöÑÔºåÈÇ£‰πàÊòØ‰∏çÊòØÂèØ‰ª•ËÆ©createÊñπÊ≥ïÂè™Êé•ËøîÂõûNullÔºõ
3. Â¶ÇÊûúÂú®ËØçÂ∫ìÂàùÂßãÂåñÁöÑÊó∂ÂÄôÈúÄË¶ÅÁîüÊàêbinÊàñËÄÖÂÖ∂‰ªñÊñá‰ª∂ÔºåÈÇ£‰πàÂú®‰ΩøÁî®HDFSËØçÂ∫ìÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ§öËäÇÁÇπÂêåÊó∂ÂàùÂßãÂåñÁöÑÊó∂ÂÄôÊòØ‰∏çÊòØ‰ºöÈÄ†ÊàêÂÜôÂÖ•ÂÜ≤Á™ÅÈóÆÈ¢ò




"
Merge pull request #1 from hankcs/master,"136da834f0a1a92987d733e67d0560c4a80a144e

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
Merge pull request #1 from hankcs/master,"update

<!--
ÊÑüË∞¢‰Ω†ÂØπÂºÄÊ∫ê‰∫ã‰∏öÁöÑË¥°ÁåÆÔºÅËøôÊòØ‰∏Ä‰ªΩÊ®°ÊùøÔºåÊñπ‰æøËÆ∞ÂΩï‰Ω†ÂÅöÂá∫ÁöÑÂäüÁª©ÔºåË∞¢Ë∞¢ÔºÅ
-->

## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

<!-- ‰Ω†ÁöÑË°•‰∏ÅËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºåÁªôÂ§ßÂÆ∂Â∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü -->

## Áõ∏ÂÖ≥issue

<!-- Â¶ÇÊûúË∑üÂ∑≤ÊúâissueÁõ∏ÂÖ≥ÁöÑËØùÔºåÈ∫ªÁÉ¶Âàó‰∏Ä‰∏ã -->


"
"""ÁÉ§ÁæäÊéí""ËøõË°åÂàÜËØç‰∏∫""ÁÉ§""Âíå""ÁæäÊéí""","ÈóÆÈ¢ò
ÂØπËØçËØ≠""ÁÉ§ÁæäÊéí""ËøõË°åÂàÜËØçÔºå
ÂæóÂà∞ÁªìÊûú: ÁÉ§ÁæäÊéí [nf]Ôºå ÊúüÊúõËÉΩÊää ÁÉ§ Âíå ÁæäÊéí Âå∫ÂàÜÂºÄ

ÁâàÊú¨
Á®ãÂ∫èÁâàÊú¨Âè∑Ôºöhanlp-portable-1.2.7
Êï∞ÊçÆÁâàÊú¨Âè∑Ôºödata-for-1.3.2.zip

Ëß¶Âèë‰ª£Á†Å
`HanLP.segment(""ÁÉ§ÁæäÊéí"");`

‰ª£Á†ÅËæìÂá∫
`ÁÉ§ÁæäÊéí [nf]`

Â∞ùËØï
CoreNatureDictionary.txt‰∏≠Êúâ ÁÉ§„ÄÅÁæäÊéí„ÄÅÁÉ§ÁæäÊéí‰∏â‰∏™ËØçËØ≠ÔºåÊàëÂú®ËØçÂ∫ìCoreNatureDictionary.ngram.txt‰∏≠Â¢ûÂä† ÁÉ§@ÁæäÊéí 2000Ôºå‰ΩÜÊòØËøòÊòØÊ≤°ÊúâÊïàÊûúÔºåÈ∫ªÁÉ¶Â∏ÆÂøôÁúã‰∏Ä‰∏ãÔºåÈùûÂ∏∏ÊÑüË∞¢„ÄÇ"
Êï¥ÂêàLDA ‰∏ªË∞ìÂÆæÊèêÂèñ,"Êï¥ÂêàLDA ‰∏ªË∞ìÂÆæÊèêÂèñ Êñπ‰æø‰∏éÂÖ∂‰ªñÂàÜËØç‰∏ÄËµ∑‰ΩøÁî®
"
ÂÖ≥‰∫éËØ≠‰πâË∑ùÁ¶ªËÆ°ÁÆóÁöÑÈóÆÈ¢ò,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöhanlp-1.3.3-release
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöhanlp-1.3.3-release


## ÊàëÁöÑÈóÆÈ¢ò
Âú®Ë∞ÉÁî®[ËØ≠‰πâË∑ùÁ¶ª](https://github.com/hankcs/HanLP#20-ËØ≠‰πâË∑ùÁ¶ª)ÂíåËØ≠‰πâÁõ∏‰ººÂ∫¶ÊñπÊ≥ïÊó∂ÔºåÂá∫Áé∞ËØ≠‰πâÁõ∏‰ººÂ∫¶‰∏∫0„ÄÇ
<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖàÔºåÊàëÈÄöËøá[ÂÖ≥ÈîÆËØçÊèêÂèñ](https://github.com/hankcs/HanLP#14-ÂÖ≥ÈîÆËØçÊèêÂèñ)Ëé∑Âèñ‰∫ÜÊüê‰∏™Âè•Â≠êÁöÑÂÖ≥ÈîÆËØç„ÄÇ‰æãÂ¶ÇÔºå[Á®ãÂ∫èÂëò, Á®ãÂ∫è, ÂàÜ‰∏∫, ‰∫∫Âëò, ËΩØ‰ª∂]„ÄÇ
2. ÁÑ∂ÂêéÔºåÊàëÂØπËé∑ÂæóÁöÑÂÖ≥ÈîÆËØçËøõË°åËØ≠‰πâË∑ùÁ¶ªÂíåËØ≠‰πâÁõ∏‰ººÂ∫¶ÁöÑËÆ°ÁÆó„ÄÇËÆ°ÁÆóÁªìÊûú‰∏≠Ôºå‰∏é‚ÄúÁ®ãÂ∫èÂëò‚Äù‰∏ÄËØçÁõ∏ÂÖ≥ÁöÑËÆ∞ÂΩïÔºåËØ≠‰πâË∑ùÁ¶ª‰∏∫Êó†Á©∑ÔºåËØ≠‰πâÁõ∏‰ººÂ∫¶‰∏∫0„ÄÇ

Êñ∞ÊâãÔºåÂàöÊé•Ëß¶ÂàÜËØçÔºå‰ª•‰∏ãÊòØËá™Â∑±ÂÅöÂá∫ÁöÑ‰∏Ä‰∫õÂ∞ùËØï‰∏éÁåúÊÉ≥ÔºåÂ¶ÇÊúâÊèèËø∞‰∏çÂΩìÔºåËøòËØ∑ÊâπËØÑÊåáÊ≠£„ÄÇ
1.ÁåúÊÉ≥ÈúÄË¶ÅÊ∑ªÂä†‚ÄúÁ®ãÂ∫èÂëò‚ÄùÂà∞Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂ∞ùËØï‰∫ÜÂä®ÊÄÅÂ¢ûÂä†ËØçËØ≠‚ÄúÁ®ãÂ∫èÂëò‚ÄùÔºåÁªìÊûúÊó†ÂèòÂåñ„ÄÇ
2.ÁåúÊÉ≥Âú®„ÄäÂêå‰πâËØçËØçÊûóÊâ©Â±ïÁâà„Äã‰∏≠Ê≤°Êúâ‰∏é‚ÄúÁ®ãÂ∫èÂëò‚ÄùÁõ∏ÂÖ≥ÁöÑËÆ∞ÂΩïÔºåÊïÖÊó†Ê≥ïËÆ°ÁÆóËØ≠‰πâË∑ùÁ¶ªÂíåËØ≠‰πâÁõ∏‰ººÂ∫¶„ÄÇ

ËØ∑ÈóÆÔºåÂÖ∑‰ΩìÊòØÂì™‰∏ÄÁßçÈóÆÈ¢òÂØºËá¥‰∏Ä‰∏™ËØçËØ≠ÂØπËá™Â∑±Êú¨Ë∫´ÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶‰∏∫0ÔºüÂØπ‰∫éÁõ∏Â∫îÁöÑÈóÆÈ¢òÔºåÊàëÂèØ‰ª•ÊÄé‰πàËß£ÂÜ≥ÔºåÊàñÂèÇËÄÉ‰ªÄ‰πàÊ†∑‰æãÂéªËß£ÂÜ≥Ôºü

### Ëß¶Âèë‰ª£Á†Å

```
String content = ""Á®ãÂ∫èÂëò(Ëã±ÊñáProgrammer)ÊòØ‰ªé‰∫ãÁ®ãÂ∫èÂºÄÂèë„ÄÅÁª¥Êä§ÁöÑ‰∏ì‰∏ö‰∫∫Âëò„ÄÇ‰∏ÄËà¨Â∞ÜÁ®ãÂ∫èÂëòÂàÜ‰∏∫Á®ãÂ∫èËÆæËÆ°‰∫∫ÂëòÂíåÁ®ãÂ∫èÁºñÁ†Å‰∫∫ÂëòÔºå‰ΩÜ‰∏§ËÄÖÁöÑÁïåÈôêÂπ∂‰∏çÈùûÂ∏∏Ê∏ÖÊ•öÔºåÁâπÂà´ÊòØÂú®‰∏≠ÂõΩ„ÄÇËΩØ‰ª∂‰ªé‰∏ö‰∫∫ÂëòÂàÜ‰∏∫ÂàùÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÈ´òÁ∫ßÁ®ãÂ∫èÂëò„ÄÅÁ≥ªÁªüÂàÜÊûêÂëòÂíåÈ°πÁõÆÁªèÁêÜÂõõÂ§ßÁ±ª„ÄÇ"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);
System.out.printf(""%-5s\t\t%-5s\t\t%-10s\t\t%-5s\n"", ""ËØçA"", ""ËØçB"", ""ËØ≠‰πâË∑ùÁ¶ª"", ""ËØ≠‰πâÁõ∏‰ººÂ∫¶"");
for (String a : keywordList)
{
	
    for (String b : keywordList)
    {
        System.out.printf(""%-5s\t\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
    }
}
```
### ÊúüÊúõËæìÂá∫
(ËæìÂá∫ÁªìÊûúÊú´Â∞æÂä†""*""‰∏∫Êñπ‰æøÊèêÁ§∫ÂÖ≥ÈîÆË°å)
<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ËØçA   		ËØçB   	ËØ≠‰πâË∑ùÁ¶ª      	ËØ≠‰πâÁõ∏‰ººÂ∫¶
Á®ãÂ∫èÂëò  		Á®ãÂ∫èÂëò  	0              	1.0000000000*
Á®ãÂ∫èÂëò  		Á®ãÂ∫è   	(ÈùûÊó†Á©∑)		(Èùû0)*
Á®ãÂ∫èÂëò  		ÂàÜ‰∏∫   	(ÈùûÊó†Á©∑)		(Èùû0)*
Á®ãÂ∫èÂëò  		‰∫∫Âëò   	(ÈùûÊó†Á©∑)		(Èùû0)*
Á®ãÂ∫èÂëò  		ËΩØ‰ª∂   	(ÈùûÊó†Á©∑)		(Èùû0)*
Á®ãÂ∫è   		Á®ãÂ∫èÂëò  	(ÈùûÊó†Á©∑)		(Èùû0)*
Á®ãÂ∫è   		Á®ãÂ∫è   	0              	1.0000000000
Á®ãÂ∫è   		ÂàÜ‰∏∫   	28631246094    	0.6146479284
Á®ãÂ∫è   		‰∫∫Âëò   	21054522402    	0.7166241456
Á®ãÂ∫è   		ËΩØ‰ª∂   	14295982706    	0.8075883064
ÂàÜ‰∏∫   		Á®ãÂ∫èÂëò  	(ÈùûÊó†Á©∑)		(Èùû0)*
ÂàÜ‰∏∫   		Á®ãÂ∫è   	28631246094    	0.6146479284
ÂàÜ‰∏∫   		ÂàÜ‰∏∫   	0              	1.0000000000
ÂàÜ‰∏∫   		‰∫∫Âëò   	49685768496    	0.3312720740
ÂàÜ‰∏∫   		ËΩØ‰ª∂   	42927228800    	0.4222362347
‰∫∫Âëò   		Á®ãÂ∫èÂëò  	(ÈùûÊó†Á©∑)		(Èùû0)*
‰∫∫Âëò   		Á®ãÂ∫è   	21054522402    	0.7166241456
‰∫∫Âëò   		ÂàÜ‰∏∫   	49685768496    	0.3312720740
‰∫∫Âëò   		‰∫∫Âëò   	0              	1.0000000000
‰∫∫Âëò   		ËΩØ‰ª∂   	6758539696     	0.9090358392
ËΩØ‰ª∂   		Á®ãÂ∫èÂëò  	(ÈùûÊó†Á©∑)		(Èùû0)*
ËΩØ‰ª∂   		Á®ãÂ∫è   	14295982706    	0.8075883064
ËΩØ‰ª∂   		ÂàÜ‰∏∫   	42927228800    	0.4222362347
ËΩØ‰ª∂   		‰∫∫Âëò   	6758539696     	0.9090358392
ËΩØ‰ª∂   		ËΩØ‰ª∂   	0              	1.0000000000
```

### ÂÆûÈôÖËæìÂá∫
(ËæìÂá∫ÁªìÊûúÊú´Â∞æÂä†""*""‰∏∫Êñπ‰æøÊèêÁ§∫ÂÖ≥ÈîÆË°å)
<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ËØçA   		ËØçB   		ËØ≠‰πâË∑ùÁ¶ª      	ËØ≠‰πâÁõ∏‰ººÂ∫¶
Á®ãÂ∫èÂëò  		Á®ãÂ∫èÂëò  	9223372036854775807	0.0000000000*
Á®ãÂ∫èÂëò  		Á®ãÂ∫è   	9223372036854775807	0.0000000000*
Á®ãÂ∫èÂëò  		ÂàÜ‰∏∫   	9223372036854775807	0.0000000000*
Á®ãÂ∫èÂëò  		‰∫∫Âëò   	9223372036854775807	0.0000000000*
Á®ãÂ∫èÂëò  		ËΩØ‰ª∂   	9223372036854775807	0.0000000000*
Á®ãÂ∫è   		Á®ãÂ∫èÂëò  	9223372036854775807	0.0000000000*
Á®ãÂ∫è   		Á®ãÂ∫è   	0              		1.0000000000
Á®ãÂ∫è   		ÂàÜ‰∏∫   	28631246094    		0.6146479284
Á®ãÂ∫è   		‰∫∫Âëò   	21054522402    		0.7166241456
Á®ãÂ∫è   		ËΩØ‰ª∂   	14295982706    		0.8075883064
ÂàÜ‰∏∫   		Á®ãÂ∫èÂëò  	9223372036854775807	0.0000000000*
ÂàÜ‰∏∫   		Á®ãÂ∫è   	28631246094    		0.6146479284
ÂàÜ‰∏∫   		ÂàÜ‰∏∫   	0              		1.0000000000
ÂàÜ‰∏∫   		‰∫∫Âëò   	49685768496    		0.3312720740
ÂàÜ‰∏∫   		ËΩØ‰ª∂   	42927228800    		0.4222362347
‰∫∫Âëò   		Á®ãÂ∫èÂëò  	9223372036854775807	0.0000000000*
‰∫∫Âëò   		Á®ãÂ∫è   	21054522402    		0.7166241456
‰∫∫Âëò   		ÂàÜ‰∏∫   	49685768496    		0.3312720740
‰∫∫Âëò   		‰∫∫Âëò   	0              		1.0000000000
‰∫∫Âëò   		ËΩØ‰ª∂   	6758539696     		0.9090358392
ËΩØ‰ª∂   		Á®ãÂ∫èÂëò  	9223372036854775807	0.0000000000*
ËΩØ‰ª∂   		Á®ãÂ∫è   	14295982706    		0.8075883064
ËΩØ‰ª∂   		ÂàÜ‰∏∫   	42927228800    		0.4222362347
ËΩØ‰ª∂   		‰∫∫Âëò   	6758539696     		0.9090358392
ËΩØ‰ª∂   		ËΩØ‰ª∂   	0              		1.0000000000
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
Ë°•ÂÖÖ‰∏Ä‰∏ãËá™Â∑±ÂêåÊó∂ÁªìÂêàË∞ÉÁî®‰∏§ÁßçÊñπÊ≥ïÔºà[ËØ≠‰πâË∑ùÁ¶ª](https://github.com/hankcs/HanLP#20-ËØ≠‰πâË∑ùÁ¶ª)Âíå[ÂÖ≥ÈîÆËØçÊèêÂèñ](https://github.com/hankcs/HanLP#14-ÂÖ≥ÈîÆËØçÊèêÂèñ)ÔºâÁöÑÂàùË°∑Ôºö
ÊàëÊúüÊúõÈÄöËøáËæìÂÖ•ÂÖ≥ÈîÆËØçÔºåËÆ°ÁÆóÂÖ≥ÈîÆËØç‰∏éÂè•Â≠êÁöÑÂÖ≥ËÅîÂ∫¶Ôºà‰ª•‰∏ãÁÆÄÁß∞ ÂÖ≥ËÅîÂ∫¶ÔºâÔºå‰ª•ËææÂà∞‚ÄúÂÖ≥ÈîÆËØç-Âè•Â≠ê-ÂÖ≥ËÅîÂ∫¶‚ÄùÁöÑÁªìÊûú„ÄÇ
ËÄåÂú®ËÆ°ÁÆóÂÖ≥ËÅîÂ∫¶Êó∂ÔºåÊàëËÆ°ÂàíÔºö
ÂÖàÊèêÂèñÂè•Â≠êÂÖ≥ÈîÆËØçÔºåÂ¶ÇÔºå[Á®ãÂ∫èÂëò, Á®ãÂ∫è, ÂàÜ‰∏∫, ‰∫∫Âëò, ËΩØ‰ª∂]Ôºõ
ÂÜçËæìÂÖ•ÂÖ≥ÈîÆËØçÔºåÂ¶ÇÔºå‚ÄúÁ®ãÂ∫èÂëò‚ÄùÔºå‚ÄúÁºñÁ®ã‚ÄùÁ≠âÔºåËÆ°ÁÆóËæìÂÖ•ÁöÑÂÖ≥ÈîÆËØç‰∏éÂè•Â≠êÂÖ≥ÈîÆËØç‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶Ôºõ
ÊúÄÂêéÂà©Áî®ÂÖ≥ÈîÆËØçÁöÑÁõ∏‰ººÂ∫¶Ôºå‰ªé‰∏ÄÂÆöÁöÑÁ®ãÂ∫¶ÂèçÊò†ÂÖ≥ËÅîÂ∫¶„ÄÇ

‰ª•‰∏äÊòØÊàëÁõÆÂâçÊúüÊúõÁöÑÈúÄÊ±ÇÔºåÂ¶ÇÊûú‰ΩúËÄÖÊúâÈÅáÂà∞ÊàñËß£ÂÜ≥ËøáÁ±ª‰ººÈóÆÈ¢òÔºåËøòËØ∑Â§öÂ§öÊåáÊïô„ÄÇ
Ë∞¢Ë∞¢ÔºÅ
<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
stopwordËØçÂÖ∏Âä†ËΩΩÈóÆÈ¢ò,"ÊàëÁé∞Âú®Áî®ÁöÑÊòØhanlp 1.3.0ÁâàÊú¨. Âú®ÂàÜÊûêCoreStopWordDictionary.javaÂèëÁé∞‰ª•‰∏ãËØçÂÖ∏Âä†ËΩΩËØ≠Âè•Ôºö
dictionary = new StopWordDictionary(new File(HanLP.Config.CoreStopWordDictionaryPath));

‰πãÂâçÁöÑÊ†∏ÂøÉËØçÂÖ∏ÔºåÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Á≠âÂùáÈááÁî®‰ª•‰∏ãÊñπÂºè„ÄÇ‰ª•Ê†∏ÂøÉËØçÂÖ∏‰∏∫‰æãÔºöCoreDictionary.java
br = new BufferedReader(new InputStreamReader(IOUtil.newInputStream(path), ""UTF-8""));
ÊòØÈááÁî®IOUtilÁöÑÁªü‰∏ÄÊé•Âè£„ÄÇ
ËÄåStopWordDictionaryÁõ¥Êé•‰ΩøÁî®‰∫ÜFileÊù•ÂÅöÔºåÈÄ†Êàê‰∫Ü‰∏çÁªü‰∏Ä„ÄÇÊòØÂê¶ËÄÉËôëÂØπCoreStopWordDictionaryÂª∫Á´ãÁªü‰∏ÄÊÄßÔºü
Âõ†‰∏∫ÊàëËá™Â∑±ÂÆö‰πâÁöÑJarIOAdapter.javaÔºö
public class JarIOAdapter implements IIOAdapter
{
    @Override
    public InputStream open(String path) throws FileNotFoundException
    {
        /*
        ÈááÁî®Á¨¨‰∏ÄË°åÁöÑÊñπÂºèÂä†ËΩΩËµÑÊñô‰ºöÂú®ÂàÜÂ∏ÉÂºèÁéØÂ¢ÉÊä•Èîô
        ÊîπÁî®Á¨¨‰∫åË°åÁöÑÊñπÂºè
         */
        //return ClassLoader.getSystemClassLoader().getResourceAsStream(path);
        return JarIOAdapter.class.getClassLoader().getResourceAsStream(path);
    }

    @Override
    public OutputStream create(String path) throws FileNotFoundException
    {
        return new FileOutputStream(path);
    }
}
ËøôÈáåÊòØÂÆûÁé∞‰ª£Á†Å‰∏éËØçÂÖ∏Êï∞ÊçÆÁöÑÂàÜÁ¶ªÔºåÂçïÁã¨Êäähanlp.properties‰∏édataÁõÆÂΩïÂÅöÊàê‰∏Ä‰∏™jar„ÄÇ‰ΩÜÁî±‰∫éCoreStopDictionary.javaËØªÊñá‰ª∂Êé•Âè£‰∏çÁªü‰∏ÄÔºåÂØºËá¥ËØª‰∏çÂà∞ÂÅúÁî®ËØçÂÖ∏Êñá‰ª∂„ÄÇ
‰ΩúËÄÖÊòØÂê¶ÊúâÊÑèÊää‰ª£Á†Å‰∏éËØçÂÖ∏Êï∞ÊçÆÂàÜÊàê‰∏§‰∏™jarÂåÖÔºåÊàëËøôËæπÂ∑≤Â∑Æ‰∏çÂ§öÂÆåÊàêÔºåÂèØ‰ª•Êèê‰∫§‰ª£Á†Å"
ÈúÄË¶ÅÊÄé‰πàÂ§ÑÁêÜ‰∏çÂéªÂä†ËΩΩÂÜÖÈÉ®ÁöÑËßíËâ≤ÁªÑÂíåÂàóË°®,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.2


## ÊàëÁöÑÈóÆÈ¢ò
 trie = new AhoCorasickDoubleArrayTrie<String>();
        TreeMap<String, String> patternMap = new TreeMap<String, String>();
        addKeyword(patternMap, ""CCCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCD"");
        addKeyword(patternMap, ""CCCCCCGD"");
        addKeyword(patternMap, ""CCCCCCICCCCD"");
        addKeyword(patternMap, ""CCCCCCPD"");
        addKeyword(patternMap, ""CCCCCD"");
        addKeyword(patternMap, ""CCCCCDD"");
ÈúÄË¶ÅÊÄé‰πàÂ§ÑÁêÜÊâçËÉΩÈÅøÂºÄËÆ∞ËΩΩ‰Ω†‰ª¨ÂÜÖÈÉ®ÁöÑËøô‰∏™ËßíËâ≤Ê†áÊ≥®ÂàóË°®ÔºåÊàëÊúâËá™Â∑±ÁöÑËßíËâ≤Ê†áÊ≥®ÂàóË°®„ÄÇ
Êú∫ÊûÑÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1169909 ][Ê≤àÈò≥ G 97524 ][Á°ï P 7 ][Ê∂¶ C 116 ][Êú∫Ê¢∞ÁîµÂ≠ê F 1000 ][ËÆæÂ§á C 273 B 2 ][ÊúâÈôêÂÖ¨Âè∏ K 1000 D 1000 ][  B 8425 ]
Êú∫ÊûÑÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /S ,Ê≤àÈò≥/G ,Á°ï/P ,Ê∂¶/C ,Êú∫Ê¢∞ÁîµÂ≠ê/F ,ËÆæÂ§á/C ,ÊúâÈôêÂÖ¨Âè∏/D , /S]
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöÊú∫Ê¢∞ÁîµÂ≠êËÆæÂ§áÊúâÈôêÂÖ¨Âè∏ FCD
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöÊ∂¶Êú∫Ê¢∞ÁîµÂ≠êËÆæÂ§áÊúâÈôêÂÖ¨Âè∏ CFCD
ËØÜÂà´Âá∫Êú∫ÊûÑÂêçÔºöËÆæÂ§áÊúâÈôêÂÖ¨Âè∏ CD
###ÊúüÊúõËæìÂá∫
Êú∫Ê¢∞ÁîµÂ≠êËÆæÂ§áÊúâÈôêÂÖ¨Âè∏ FCD
ËæìÂá∫Ëøô‰∏™ÁªìÊûúÁöÑÂéüÂõ†ÊòØÊàëÁöÑÂàóË°®‰∏≠‰∏çÂ≠òÂú®CÂºÄÂ§¥ÁöÑËßíËâ≤Ê†áÊ≥®ÁªÑÂêà
### ÂÆûÈôÖËæìÂá∫
[Ê≤àÈò≥/ns, Á°ï/ag, Ê∂¶Êú∫Ê¢∞ÁîµÂ≠êËÆæÂ§áÊúâÈôêÂÖ¨Âè∏/nt]   Ê≥®ÊÑèÁ¨¨‰∫å‰∏™  Ê∂¶Êú∫Ê¢∞ÁîµÂ≠êËÆæÂ§áÊúâÈôêÂÖ¨Âè∏ CFCD

ÊØèÊ¨°Ë∞ÉÁî®ÈÉΩ‰ºöÈªòËÆ§ËÆ∞ËΩΩÂÜÖÈÉ®ÁöÑËßíËâ≤Ê†áÊ≥®ÂàóË°®
"
com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes ‰∏≠  int availableBytes = is.available();,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [ ] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.3
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.2


## ÊàëÁöÑÈóÆÈ¢ò

Âú®resin‰∏ãÈÉ®ÁΩ≤HanLP Âä†ËΩΩËØçÂÖ∏Êñá‰ª∂Êó∂, Âá∫Áé∞Á©∫ÊåáÈíàÂºÇÂ∏∏

	... 53 more
Caused by: java.lang.NullPointerException
	at com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes(ByteArrayOtherStream.java:72)
	at com.hankcs.hanlp.corpus.io.ByteArrayStream.nextInt(ByteArrayStream.java:56)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.load(DoubleArrayTrie.java:531)
	at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.load(DoubleArrayTrie.java:516)
	at com.hankcs.hanlp.dictionary.common.CommonDictionary.loadDat(CommonDictionary.java:90)
	at com.hankcs.hanlp.dictionary.common.CommonDictionary.load(CommonDictionary.java:44)
	at com.hankcs.hanlp.dictionary.nr.PersonDictionary.<clinit>(PersonDictionary.java:57)

## Â§çÁé∞ÈóÆÈ¢ò

ÊØèÊ¨°ÈÉΩËÉΩÂ§çÁé∞

### Ê≠•È™§

Âú®resinÁéØÂ¢É‰∏ã,  Ë∞ÉÁî®Â¶Ç‰∏ã‰ª£Á†Å, ‰ºö‰∫ßÁîüÂºÇÂ∏∏
        List<com.hankcs.hanlp.seg.common.Term> segment = HanLP.segment(episodeName);

### Ëß¶Âèë‰ª£Á†Å

com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes(ByteArrayOtherStream.java:72)

ÁªèËøáË∞ÉËØï, ÂèëÁé∞ÊòØÁî±‰∫éresinÂØπInputStreamÁöÑÂÆûÁé∞is.available()ÊØèÊ¨°ÊúÄÂ§öËØªÂèñ8192‰∏™Â≠óËäÇ, ËÄå‰∏çÊòØÂ∞ÜÊâÄÊúâÁöÑÊï∞ÊçÆÈÉΩËØªÂèñ.  Á®ãÂ∫èÁ¨¨‰∏ÄÊ¨°Âú®ËØªÂèñ8192Â≠óËäÇ, Ê≤°ÊúâËØªÂèñÂÆåÊï¥Êï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ã, Â∞ÜInputStreamÊµÅÈîôËØØÂÖ≥Èó≠, Á¨¨‰∫åÊ¨°ËØªÂèñÊó∂Êä•‰∏äËø∞ÈîôËØØ
                if (readBytes == availableBytes)
                {
                    is.close();
                    is = null;
                }

‰∏ãÈù¢ÊòØInputStream available ÁöÑÊñáÊ°£
 /**
     * Returns an estimate of the number of bytes that can be read (or
     * skipped over) from this input stream without blocking by the next
     * invocation of a method for this input stream. The next invocation
     * might be the same thread or another thread.  A single read or skip of this
     * many bytes will not block, but may read or skip fewer bytes.
     *
    * ËøôÈáåËØ¥Êòé‰∫ÜÂèØËÉΩ‰∏ç‰ºöËøîÂõûÊâÄÊúâÂ≠óËäÇ*
     *** <p> Note that while some implementations of {@code InputStream} will return
     * the total number of bytes in the stream, many will not.  It is
     * never correct to use the return value of this method to allocate
     * a buffer intended to hold all data in this stream.**
     *
     * <p> A subclass' implementation of this method may choose to throw an
     * {@link IOException} if this input stream has been closed by
     * invoking the {@link #close()} method.
     *
     * <p> The {@code available} method for class {@code InputStream} always
     * returns {@code 0}.
     *
     * <p> This method should be overridden by subclasses.
     *
     * @return     an estimate of the number of bytes that can be read (or skipped
     *             over) from this input stream without blocking or {@code 0} when
     *             it reaches the end of the input stream.
     * @exception  IOException if an I/O error occurs.
     */

Âª∫ËÆÆ‰ª£Á†Å‰øÆÊîπ:
              if(is.available()==0)
              {
                    is.close();
                    is = null;
                }

### ÊúüÊúõËæìÂá∫


### ÂÆûÈôÖËæìÂá∫


## ÂÖ∂‰ªñ‰ø°ÊÅØ


"
"‰∏∫‰ªÄ‰πà""good*tvb""ÂàÜÊàê[good, *, tvb]Ôºå‰ΩÜÊòØ""good#tvb""ÂàÜÊàêÊï¥‰∏™""good#tvb""? Â¶Ç‰ΩïÂÆûÁé∞ÂàÜÊàê[good, #, tvb]?","<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->

ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö


## ÊàëÁöÑÈóÆÈ¢ò

<!-- ËØ∑ËØ¶ÁªÜÊèèËø∞ÈóÆÈ¢òÔºåË∂äËØ¶ÁªÜË∂äÂèØËÉΩÂæóÂà∞Ëß£ÂÜ≥ -->

## Â§çÁé∞ÈóÆÈ¢ò
<!-- ‰Ω†ÊòØÂ¶Ç‰ΩïÊìç‰ΩúÂØºËá¥‰∫ßÁîüÈóÆÈ¢òÁöÑÔºüÊØîÂ¶Ç‰øÆÊîπ‰∫Ü‰ª£Á†ÅÔºü‰øÆÊîπ‰∫ÜËØçÂÖ∏ÊàñÊ®°ÂûãÔºü-->

### Ê≠•È™§

1. È¶ñÂÖà‚Ä¶‚Ä¶
2. ÁÑ∂Âêé‚Ä¶‚Ä¶
3. Êé•ÁùÄ‚Ä¶‚Ä¶

### Ëß¶Âèë‰ª£Á†Å

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""Áî®Êà∑ËØçËØ≠"");
        System.out.println(StandardTokenizer.segment(""Ëß¶ÂèëÈóÆÈ¢òÁöÑÂè•Â≠ê""));
    }
```
### ÊúüÊúõËæìÂá∫

<!-- ‰Ω†Â∏åÊúõËæìÂá∫‰ªÄ‰πàÊ†∑ÁöÑÊ≠£Á°ÆÁªìÊûúÔºü-->

```
ÊúüÊúõËæìÂá∫
```

### ÂÆûÈôÖËæìÂá∫

<!-- HanLPÂÆûÈôÖËæìÂá∫‰∫Ü‰ªÄ‰πàÔºü‰∫ßÁîü‰∫Ü‰ªÄ‰πàÊïàÊûúÔºüÈîôÂú®Âì™ÈáåÔºü-->

```
ÂÆûÈôÖËæìÂá∫
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

<!-- ‰ªª‰ΩïÂèØËÉΩÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Êà™Âõæ„ÄÅÊó•Âøó„ÄÅÈÖçÁΩÆÊñá‰ª∂„ÄÅÁõ∏ÂÖ≥issueÁ≠âÁ≠â„ÄÇ-->

"
ÂèëÁé∞TextUtility.isAllSingleByteÁöÑbugÔºå‰∏çÁü•ÈÅìÂàÜËØçËøáÁ®ã‰ºöË∞ÉÁî®Âêó," while (i < len && b[i] < 128)
            {
                i++;
            }

----------------

b[i] < 128 Â∫îËØ•ÊîπÊàê b[i] >0.

byteÁöÑÂèñÂÄºËåÉÂõ¥ÊòØ-128Âà∞127 Âíåchar‰∏ç‰∏ÄÊ†∑

ËøòÊúâÊàëËßâÂæóÂ∫îËØ•Ê≤°ÂøÖË¶ÅËΩ¨Êç¢ÊàêgbkÔºåÁõ¥Êé•Áî®charAt(i)ÊòØÂê¶Â∞è‰∫é128Âç≥ÂèØÂà§Êñ≠„ÄÇ



"
ÂÖ≥‰∫éÊ∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØç‰∏éËØçÊÄßÊ†áÊ≥®ÈóÆÈ¢ò,"
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöHanLP1.3.2
## ÊàëÁöÑÈóÆÈ¢ò

Âú®ËØçÊÄßÊ†áÊ≥®‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰∫ÜÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂèëÁé∞Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑÂ§ßÈÉ®ÂàÜËØçËØ≠ËÉΩÊåâÁî®Êà∑Ëá™ÂÆö‰πâËÆ∞Âè∑ËøõË°åÊ†áÊ≥®ÔºåËÄåÂ≠òÂú®Â∞ëÈÉ®ÂàÜËØçËØ≠Êó†Ê≥ïÊåâËá™ÂÆö‰πâÊ†áÊ≥®ËøõË°åËØçÊÄßÊ†áÊ≥®„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

Â¶ÇÔºöÁî®Êà∑Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏userdict.txt Â≠òÂú®Â¶Ç‰∏ãÂá†‰∏™ËØçËØ≠Ôºö
ÊÄßÂà´  sex
Áî∑ man
Â•≥ woman
ÊïÖÊÑè‰º§ÂÆ≥ÁΩ™ anyou

ÂÖ∂‰∏≠Ôºå‚ÄúÊÄßÂà´‚Äù„ÄÅ‚ÄúÁî∑‚Äù„ÄÅ‚ÄúÂ•≥‚ÄùËøô‰∏â‰∏™ËØçÂèØ‰ª•ÊåâËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊ†áÊ≥®ËøõË°åÊ†áÊ≥®ÔºåÁªìÊûú‰∏∫ ÔºöÊÄßÂà´/sex„ÄÅÁî∑/man„ÄÅÂ•≥/ womanÔºåËÄå‚Äú ÊïÖÊÑè‰º§ÂÆ≥ÁΩ™‚ÄùËøô‰∏™ËØçËØ≠ËØçÊÄßÊ†áÊ≥®‰∏∫‚Äúnt‚Äù,ÁªìÊûú‰∏∫ÔºöÊïÖÊÑè‰º§ÂÆ≥ÁΩ™/nt„ÄÇ

### ÊúüÊúõËæìÂá∫

Êàë‰ª¨ÁöÑÊúüÊúõËæìÂá∫ÊòØÔºö
ÊÄßÂà´ / sex
Áî∑ /man
Â•≥ /woman
ÊïÖÊÑè‰º§ÂÆ≥ÁΩ™/ anyou



"
"‰øÆÊ≠£‰∫ÜÊ†∏ÂøÉÂ≠óÂÖ∏ÁöÑ‚ÄùÊØèxx""ËØçÊÄß","
"
v1.3.3ÂçáÁ∫ßÊåáÂçó,":loudspeaker: v1.3.3‰æùÁÑ∂‰øùËØÅ‰∫ÜÊâÄÊúâÊé•Âè£ÁöÑÂÖºÂÆπÊÄß„ÄÇ‰ΩÜÂÅö‰∫ÜÂ¶Ç‰∏ã‰∏§‰∏™ÂæÆÂ∞èÊîπÂä®Ôºö

## com.hankcs.hanlp.algorithm ÂåÖÂêçÊãºÂÜôÈîôËØØÊîπÊ≠£
Ëøô‰∏™ÂåÖ‰∏ãÈù¢ÂÆûÁé∞‰∫Ü‰∏Ä‰∫õÁÆóÊ≥ïÔºåÂπ∂Ê≤°ÊúâÂú®ÊñáÊ°£‰∏≠ÂÖ¨ÂºÄ„ÄÇÂ¶ÇÊûú‰Ω†Ë∞ÉÁî®‰∫ÜËØ•ÂåÖ‰∏ãÈù¢ÁöÑÁÆóÊ≥ïÔºåÂèØËÉΩÈúÄË¶ÅÊâãÂä®‰øÆÊ≠£‰∏Ä‰∏ãÂåÖÂêç„ÄÇ

## portableÁâà‰∏çÂÜçÊ£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
Êú™ÂÆûÁé∞IOAdapterÁöÑÁî®Êà∑ÂèØ‰ª•ÂøΩÁï•ËøôÊù°‰ø°ÊÅØ„ÄÇ

Âú®v1.3.2‰πãÂâçÔºåportable‰ºöÊ£ÄÊü•ÈÖçÁΩÆÈ°π‰∏≠ÊåáÂÆöÁöÑË∑ØÂæÑÊòØÂê¶Â≠òÂú®„ÄÇÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôËØ•ÈÖçÁΩÆÈ°π‰∏çÁîüÊïàÔºåÂΩ¢Êàê‰∏Ä‰∏™‚ÄúÈò≤ÈîôËÆæËÆ°‚Äù„ÄÇ‰ΩÜËá™‰ªéÂºïÂÖ•‰∫ÜIOAdapterÊú∫Âà∂ÂêéÔºåË∑ØÂæÑÂèØ‰ª•ÊåáÂêë‰ªª‰ΩïÂú∞ÊñπÔºàËøúÁ®ãÔºâ„ÄÇÊâÄ‰ª•‰∏çÂÜçÂèØËÉΩ‰∏∫Áî®Êà∑Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶Â≠òÂú®„ÄÇ

ÂéªÊéâËØ•Ê£ÄÊü•ÈÄªËæëÂêéÔºåÁõÆÂâçportableÁâàÁöÑcom.hankcs.hanlp.HanLP.Config#KeyPath‰∏éÈùûportableÁâàÂäüËÉΩ‰∏ÄËá¥ÔºåÈÉΩÊòØKeyPath=root+KeyValue „ÄÇ

ÊâÄ‰ª•Â¶ÇÊûú‰Ω†ÂÆûÁé∞‰∫ÜËá™Â∑±ÁöÑIOAdapterÔºåopenÂíåcreateÊé•Êî∂ÁöÑpathÂèÇÊï∞Â∞Ü‰ºö‰ª•rootÂºÄÂ§¥ÔºåÊï¨ËØ∑Ê≥®ÊÑè„ÄÇÊ≠§Êó∂ÂèØ‰æõÂèÇËÄÉÁöÑÊé™ÊñΩÊúâÔºö

- Â∞ÜÈÖçÁΩÆÊñá‰ª∂ÁöÑrootËÆæ‰∏∫```""""```
- ÊàñÁ®çÂæÆË∞ÉÊï¥‰∏Ä‰∏ãËá™Â∑±ÁöÑIOAdapter



## ÁªìËØ≠
Ëøô‰∫õÊîπÂä®ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫Ü‰ΩøÈ°πÁõÆÊõ¥Âä†ËßÑËåÉÔºåÈÄ†Êàê‰∏ç‰æøËøòÊúõÊµ∑Ê∂µ„ÄÇÊúâ‰ªª‰ΩïÈóÆÈ¢òÊ¨¢ËøéÂú®Ê≠§Â§ÑÁïôË®ÄÔºåË∞¢Ë∞¢ÔºÅ"
Âú®Ëá™ÂÆö‰πâËØçÂ∫ì‰∏≠Â¢ûÂä†‰∏Ä‰∏™ËØçÂêéÔºåÂú®Êüê‰∫õÁâπÊÆäÂ≠óÁ¨¶‰∏≤ÂàÜËØçÊó∂Êä•ÈîôArrayIndexOutOfBoundsException,"<!--
ËøôÊòØHanLPÁöÑissueÊ®°ÊùøÔºåÁî®‰∫éËßÑËåÉÊèêÈóÆÈ¢òÁöÑÊ†ºÂºè„ÄÇÊú¨Êù•Âπ∂‰∏çÊâìÁÆóÁî®Ê≠ªÊùøÁöÑÊ†ºÂºèÈôêÂà∂Â§ßÂÆ∂Ôºå‰ΩÜissueÂå∫ÂÆûÂú®ÊúâÁÇπÊ∑∑‰π±„ÄÇÊúâÊó∂ÂÄôËØ¥‰∫ÜÂçäÂ§©ÊâçÊêûÊ∏ÖÊ•öÂéüÊù•ÂØπÊñπÁî®ÁöÑÊòØÊóßÁâà„ÄÅËá™Â∑±Êîπ‰∫Ü‰ª£Á†Å‰πãÁ±ªÔºåÊµ™Ë¥πÂèåÊñπÂÆùË¥µÊó∂Èó¥„ÄÇÊâÄ‰ª•ËøôÈáåÁî®‰∏Ä‰∏™ËßÑËåÉÁöÑÊ®°ÊùøÁªü‰∏Ä‰∏Ä‰∏ãÔºåÈÄ†Êàê‰∏ç‰æøÊúõÊµ∑Ê∂µ„ÄÇÈô§‰∫ÜÊ≥®ÊÑè‰∫ãÈ°πÂ§ñÔºåÂÖ∂‰ªñÈÉ®ÂàÜÂèØ‰ª•Ëá™Ë°åÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂÅöÈÄÇÈáè‰øÆÊîπ„ÄÇ
-->

## Ê≥®ÊÑè‰∫ãÈ°π
ËØ∑Á°ÆËÆ§‰∏ãÂàóÊ≥®ÊÑè‰∫ãÈ°πÔºö

* ÊàëÂ∑≤‰ªîÁªÜÈòÖËØª‰∏ãÂàóÊñáÊ°£ÔºåÈÉΩÊ≤°ÊúâÊâæÂà∞Á≠îÊ°àÔºö
  - [È¶ñÈ°µÊñáÊ°£](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hankcs/HanLP/wiki/FAQ)
* ÊàëÂ∑≤ÁªèÈÄöËøá[Google](https://www.google.com/#newwindow=1&q=HanLP)Âíå[issueÂå∫Ê£ÄÁ¥¢ÂäüËÉΩ](https://github.com/hankcs/HanLP/issues)ÊêúÁ¥¢‰∫ÜÊàëÁöÑÈóÆÈ¢òÔºå‰πüÊ≤°ÊúâÊâæÂà∞Á≠îÊ°à„ÄÇ
* ÊàëÊòéÁôΩÂºÄÊ∫êÁ§æÂå∫ÊòØÂá∫‰∫éÂÖ¥Ë∂£Áà±Â•ΩËÅöÈõÜËµ∑Êù•ÁöÑËá™Áî±Á§æÂå∫Ôºå‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªªÊàñ‰πâÂä°„ÄÇÊàë‰ºöÁ§ºË≤åÂèëË®ÄÔºåÂêëÊØè‰∏Ä‰∏™Â∏ÆÂä©ÊàëÁöÑ‰∫∫Ë°®Á§∫ÊÑüË∞¢„ÄÇ
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## ÁâàÊú¨Âè∑
<!-- ÂèëË°åÁâàËØ∑Ê≥®ÊòéjarÊñá‰ª∂ÂêçÂéªÊéâÊãìÂ±ïÂêçÁöÑÈÉ®ÂàÜÔºõGitHub‰ªìÂ∫ìÁâàËØ∑Ê≥®ÊòémasterËøòÊòØportableÂàÜÊîØ -->
‰ΩøÁî®ÁöÑMaven‰ªìÂ∫ìÔºö
    <dependency>
      <groupId>com.hankcs</groupId>
      <artifactId>hanlp</artifactId>
      <version>portable-1.3.2</version>
    </dependency>

JDK1.8.0_20
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöportable-1.3.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöportable-1.3.2

## ÊàëÁöÑÈóÆÈ¢ò
Â¢ûÂä†Ëá™ÂÆö‰πâËØçÂ∫ì‰πãÂêéÔºåÂπ∂ÂºÄÂêØ‰∫∫ÂêçËØÜÂà´ÔºåÂú®Êüê‰∫õÊñáÊú¨ÁöÑÂàÜËØçÊó∂‰ºöÊä•ÈîôArrayIndexOutOfBoundsException„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Â¢ûÂä†‰∏Ä‰∏™ËØçÔºö
CustomDictionary.add(""Á¨¨‰∏ÄÂ≠£"", ""n 1"")

Âπ∂‰∏çÊòØÊâÄÊúâÁöÑËØçÈÉΩ‰ºöËß¶ÂèëËøô‰∏™ExceptionÔºåÂêéÈù¢ÁöÑ‰ª£Á†Å‰∏≠ÊòØÊâæÂà∞ÁöÑ‰∏Ä‰∏™„ÄÇ

### Ê≠•È™§
ÊâßË°å‰∏ãÈù¢ÁöÑËß¶Âèë‰ª£Á†ÅÂç≥ÂèØ„ÄÇ

### Ëß¶Âèë‰ª£Á†Å

```

import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.dictionary.CustomDictionary;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;


import java.util.List;

public class Main {
    public static void main(String[] args){
        CustomDictionary.add(""Á¨¨‰∏ÄÂ≠£"", ""n 1"");

        Segment seg = HanLP.newSegment().enableIndexMode(true)
                .enableNameRecognize(true)
                .enableNumberQuantifierRecognize(false)
                .enableCustomDictionary(true)
                .enableTranslatedNameRecognize(false)
                .enableJapaneseNameRecognize(false)
                .enableOrganizationRecognize(false)
                .enablePlaceRecognize(false);

        HanLP.Config.enableDebug();
        List<Term> termList = seg.seg(""‰πòÈæôÊÄ™Â©øÁ¨¨‰∏ÄÂ≠£"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
    }
}
```


### ÂÆûÈôÖËæìÂá∫
```
May 05, 2017 5:55:11 PM com.hankcs.hanlp.dictionary.CoreDictionary load
INFO: Ê†∏ÂøÉËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:data/dictionary/CoreNatureDictionary.mini.txt
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
INFO: data/dictionary/CoreNatureDictionary.mini.txtÂä†ËΩΩÊàêÂäüÔºå85585‰∏™ËØçÊù°ÔºåËÄóÊó∂232ms
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[‰πò]
2:[Èæô]
3:[ÊÄ™]
4:[Â©ø]
5:[Á¨¨, Á¨¨‰∏Ä]
6:[]
7:[Â≠£]
8:[ ]

May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
INFO: ÂºÄÂßãÂä†ËΩΩ‰∫åÂÖÉËØçÂÖ∏data/dictionary/CoreNatureDictionary.ngram.mini.txt.table
Á≤óÂàÜÁªìÊûú[‰πò/v, Èæô/n, ÊÄ™/a, Â©ø/ng, Á¨¨‰∏ÄÂ≠£/n]
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
INFO: data/dictionary/CoreNatureDictionary.ngram.mini.txt.tableÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂136ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.common.CommonDictionary load
INFO: Âä†ËΩΩÂÄºdata/dictionary/person/nr.txt.value.datÊàêÂäüÔºåËÄóÊó∂64ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.common.CommonDictionary load
INFO: Âä†ËΩΩÈîÆdata/dictionary/person/nr.txt.trie.datÊàêÂäüÔºåËÄóÊó∂78ms
May 05, 2017 5:55:12 PM com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
INFO: data/dictionary/person/nr.txtÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂274ms
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][‰πò K 15 C 7 L 5 E 1 M 1 ][Èæô D 1350 E 924 B 498 C 325 L 17 M 8 K 1 ][ÊÄ™ D 10 E 1 K 1 L 1 ][Â©ø E 2 K 1 ][Á¨¨‰∏ÄÂ≠£ K 1 U 1 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,‰πò/K ,Èæô/B ,ÊÄ™/E ,Â©ø/E ,Á¨¨‰∏ÄÂ≠£/U , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÈæôÊÄ™ BE
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÊÄ™Â©ø EE
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÈæôÊÄ™Â©ø BEE
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 9
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:142)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.douban.solr.Main.main(Main.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

```

"
ÂÖ≥‰∫éÂ±ÇÂè†HMM‰∏≠ÊñáÂÆû‰ΩìËØÜÂà´ÁöÑËøáÁ®ã,"hankcs‰Ω†Â•ΩÔºÅ
ÊàëÂú®Áî®pythonÂÅö‰∏≠ÊñáÂÆû‰ΩìËØÜÂà´ÁöÑÂàÜËØçÂô®ÔºåÊÉ≥ËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰Ω†ÁöÑÂ±ÇÂè†HMM‰∏≠ÊñáÂÆû‰ΩìËØÜÂà´‰∏≠Ôºå‰∏çÂêåÁöÑÂÆû‰ΩìËØÜÂà´‰πãÈó¥ÊòØÂ¶Ç‰Ωï‰º†ÈÄíÁöÑ„ÄÇÊàëÁöÑÁ≠ñÁï•ÊòØÂÖàËøõË°å‰∫∫ÂêçËØÜÂà´ÔºåÂ∞ÜÁªìÊûú‰º†ÈÄíÔºåÂÜçËøõË°åÂú∞ÂêçËØÜÂà´ÔºåÂÜçÂ∞ÜÁªìÊûú‰º†ÈÄíÔºåÊúÄÂêéËøõË°åÊú∫ÊûÑÂêçËØÜÂà´„ÄÇ‰ΩÜÊòØÂΩìÂ§ö‰∏™ÂÆû‰ΩìËØÜÂà´ÂêåÊó∂ÂºÄÂêØÊó∂Ôºå‰ºöÂá∫Áé∞‰∏ÄÁÇπÈóÆÈ¢ò„ÄÇ
ÊØîÂ¶ÇÔºö
`s = 'ÂçóÁøîÂêëÂÆÅÂ§èÂõ∫ÂéüÂ∏ÇÂΩ≠Èò≥ÂéøÁ∫¢Ê≤≥ÈïáÈªëÁâõÊ≤üÊùëÊçêËµ†‰∫ÜÊåñÊéòÊú∫'`

ÂΩìÂè™ÂºÄÂêØÂú∞ÂêçËØÜÂà´Êó∂ÔºåÂèØ‰ª•Ê≠£Á°ÆÂàÜËØçÔºö
`['ÂçóÁøî', 'Âêë', 'ÂÆÅÂ§è', 'Âõ∫ÂéüÂ∏Ç', 'ÂΩ≠Èò≥Âéø', 'Á∫¢Ê≤≥Èïá', 'ÈªëÁâõÊ≤üÊùë', 'ÊçêËµ†', '‰∫Ü', 'ÊåñÊéòÊú∫']`

‰ΩÜÊòØÂΩìÊàëÂêåÊó∂ÂºÄÂêØ‰∫∫ÂêçËØÜÂà´ÂíåÂú∞ÂêçËØÜÂà´Êó∂ÔºåÂàÜËØçÁªìÊûúÂ∞±‰∏çÂ§™Â•Ω‰∫Ü„ÄÇ
‰∫∫ÂêçËØÜÂà´ÁªìÊûúÔºö
 `['ÂçóÁøî', 'Âêë', 'ÂÆÅÂ§è', 'Âõ∫ÂéüÂ∏Ç', 'ÂΩ≠Èò≥Âéø', 'Á∫¢Ê≤≥Èïá', 'Èªë', 'ÁâõÊ≤üÊùë', 'ÊçêËµ†', '‰∫Ü', 'ÊåñÊéòÊú∫']`

Â∞ÜÁªìÊûú‰Ωú‰∏∫‰º†ÈÄíÁªôÂú∞ÂêçËØÜÂà´ÔºåÁªìÊûú‰∏çËÉΩÊ≠£Á°ÆÂàÜÂá∫`'ÈªëÁâõÊ≤üÊùë'`
Âú∞ÂêçËØÜÂà´ÁªìÊûúÔºö
 `['ÂçóÁøî', 'Âêë', 'ÂÆÅÂ§è', 'Âõ∫ÂéüÂ∏Ç', 'ÂΩ≠Èò≥Âéø', 'Á∫¢Ê≤≥Èïá', 'Èªë', 'ÁâõÊ≤üÊùë', 'ÊçêËµ†', '‰∫Ü', 'ÊåñÊéòÊú∫']`

ÊàëÁî®HanLPËØï‰∫Ü‰∏ãÔºåÂèëÁé∞ÔºåÂΩìÂè™ÂºÄÂêØ‰∫∫ÂêçËØÜÂà´Êó∂ÔºåÂíåÊàëÈîôËØØÁöÑÁªìÊûú‰∏ÄËá¥Ôºö
`[ÂçóÁøî/ns, Âêë/p, ÂÆÅÂ§è/ns, Âõ∫ÂéüÂ∏Ç/ns, ÂΩ≠Èò≥Âéø/ns, Á∫¢Ê≤≥Èïá/ns, Èªë/a, ÁâõÊ≤üÊùë/nr, ÊçêËµ†/v, ‰∫Ü/ule, ÊåñÊéòÊú∫/n]`

‰ΩÜÊòØÂêåÊó∂ÂºÄÂêØ‰∫∫ÂêçÂíåÂú∞ÂêçËØÜÂà´ÔºåÂàôÂèØ‰ª•ÂæóÂà∞Ê≠£Á°ÆÁöÑÁªìÊûúÔºö
`[ÂçóÁøî/ns, Âêë/p, ÂÆÅÂ§è/ns, Âõ∫ÂéüÂ∏Ç/ns, ÂΩ≠Èò≥Âéø/ns, Á∫¢Ê≤≥Èïá/ns, ÈªëÁâõÊ≤üÊùë/ns, ÊçêËµ†/v, ‰∫Ü/ule, ÊåñÊéòÊú∫/n]`

ÊàëÊÉ≥ËØ∑Êïô‰∏ãÔºåÂΩìÂêåÊó∂ÂºÄÂêØ‰∫Ü‰∫∫ÂêçÂíåÂú∞ÂêçËØÜÂà´Êó∂Ôºå‰Ω†ÊòØÂ¶Ç‰ΩïÂ§ÑÁêÜÁöÑ„ÄÇ




"
ÂèëÁé∞‰∏™bug,"Â§™Â∑ÆÂä≤‰∫Ü!Ëä±‰∫ÜÂá†ÁôæÂùóÈí±‰ªÄ‰πàÊØõÁóÖÈÉΩÊ≤°ÁúãÂá∫Êù•ÔºÅÂêÑÁßçÂ•óË∑Ø‰Ω†‰ªòÈí±ÁªôÁãóÁãóÁúãÁóÖÁªìÊûúÂ±ÅÈÉΩÁúã‰∏çÂá∫Êù•ÔºÅÊÉ≥ÁªôÁãóÁãóÁúãÁóÖÁöÑÊàëÂäù‰Ω†‰ª¨ÂçÉ‰∏á‰∏çË¶ÅÂéªÈÇ£ÂÆ∂ÂåªÈô¢ÔºÅÂ¶àÁöÑÂùëÁöÑ‰∏ÄÊâπÔºÅÁ•ùÂ∏ÆÊàëÂÆ∂ÁãóÁãóÁúãÁóÖÁöÑÈÇ£‰∏™Êà¥ÁúºÈïúÁöÑËÉñÂ≠êÂÖ®ÂÆ∂ÁàÜÁÇ∏.md



ÂØπ‰∏äÈù¢ËøôÊÆµÊñáÊú¨ËøõË°åÂàÜËØçÁúãÁúãÔºå‰ª•md‰∏∫ÁªìÂ∞æÁöÑ‰ºöÊä•Èîô"
Á¥¢ÂºïÂàÜËØçÁº∫Â∞ëËØç,"hi hankcs, ÂèàÈÅáÂà∞‰∏™ÈóÆÈ¢òÊÉ≥Âêë‰Ω†ËØ∑Êïô„ÄÇ
## ÁâàÊú¨Âè∑
masterÂàÜÊîØ
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºö221d2a9e152763afe77b6ffc966b570ee308fb6e
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºö221d2a9e152763afe77b6ffc966b570ee308fb6e

## ÊàëÁöÑÈóÆÈ¢ò
‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùÁöÑÁ¥¢ÂºïÂàÜËØç‰∏≠Â∏¶Êúâ‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùËøô‰∏™ËØçÔºå‰ΩÜÊòØ‚ÄúÂïÜÊ†áËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùÁöÑÁ¥¢ÂºïÂàÜËØçÂ∞±Ê≤°Êúâ‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùËøô‰∏™ËØç„ÄÇ‰Ωú‰∏∫ÂÖ®ÂàáÂàÜÁöÑÁ¥¢ÂºïÊ®°ÂºèÔºåÊàëËßâÂæóÂØπ‰∫é‚ÄúÂïÜÊ†áËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùÁöÑÂàáÂàÜÂ∫îËØ•Â∞Ü‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚Äù‰πüÂàáÂá∫Êù•„ÄÇÂê¶ÂàôÊêúÁ¥¢""ËØÑÂÆ°ÂßîÂëò‰ºö""Â∞Ü‰ºöÊêú‰∏çÂà∞„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
        List<Term> termList = IndexTokenizer.segment(""ËØÑÂÆ°ÂßîÂëò‰ºö"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
ËØÑÂÆ°ÂßîÂëò‰ºö/nt [0:5]
ËØÑÂÆ°/vn [0:2]
ÂßîÂëò‰ºö/nis [2:5]
ÂßîÂëò/nnt [2:4]

        List<Term> termList2 = IndexTokenizer.segment(""ÂïÜÊ†áËØÑÂÆ°ÂßîÂëò‰ºö"");
        for (Term term : termList2)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
ÂïÜÊ†áËØÑÂÆ°ÂßîÂëò‰ºö/nt [0:7]
ÂïÜÊ†á/n [0:2]
ËØÑÂÆ°/vn [2:4]
ÂßîÂëò‰ºö/nis [4:7]
ÂßîÂëò/nnt [4:6]

```

## ÂÖ∂‰ªñ‰ø°ÊÅØ
ÊàëÊü•Áúã‰∫Ü‰∏Ä‰∏ãÔºåÊÑüËßâÂ∫îËØ•ÊòØÂú®‚Äú‰ΩøÁî®Áî®Êà∑ËØçÂÖ∏ÂêàÂπ∂Á≤óÂàÜÁªìÊûú‚ÄùËøô‰∏™ÈÉ®ÂàÜÂá∫ÁöÑÈóÆÈ¢ò„ÄÇÂú®Segment.javaÁöÑ384Ë°åÂ∞ÜÂ∑≤ÁªèÂêàÂπ∂ÁöÑËØçÁΩÆÁ©∫‰∫ÜÔºåÁÑ∂ÂêéÂú®Segment.javaÁöÑ302Ë°åÂ∞ÜÂæ™ÁéØÂèòÈáèÁõ¥Êé•Ë∑≥Ëøá‰∫ÜÂ∑≤ÂêàÂπ∂ÁöÑÂá†È°πÔºåÂØºËá¥‰∫Ü‚ÄúÂïÜÊ†áËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùËøô‰∏™ËØçÊèêÂá∫Êù•‰πãÂêéÔºåÊ≤°ÊúâÂÜçÁªßÁª≠Âà§Êñ≠ÊòØÂê¶Êúâ‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùÔºåÊâÄ‰ª•ÂêàÂπ∂ÂÆåÁî®Êà∑ËØçÂÖ∏ÂêéÔºåËØçÂõæÈáåÈù¢Ê≤°Êúâ‚ÄúËØÑÂÆ°ÂßîÂëò‰ºö‚ÄùËøô‰∏™ËØçÔºåÂêéÈù¢ÁöÑÁ¥¢ÂºïÊ®°ÂºèÂÖ®ÂàáÂàÜÊñπÊ≥ïdecorateResultForIndexMode‰πüÊ≤°ÂäûÊ≥ïËØÜÂà´Ëøô‰∏™ËØç„ÄÇ

ÂêåÁêÜÔºåÂè™Ë¶ÅÊòØÁî®Êà∑ËØçÂÖ∏‰∏≠Â≠òÂú®‰∏§‰∏™‰∫íÁõ∏ÂåÖÂê´ÁöÑËØçÔºåÂ∞±Âè™Êúâ‰∏Ä‰∏™ËÉΩÂá∫Êù•Ôºå‰æãÂ¶ÇÔºö
```
ÈìÅÈÅìÈÉ®ËøêËæìÂ±ÄËê•ËøêÈÉ®Ë¥ßËøêËê•ÈîÄËÆ°ÂàíÂ§Ñ/nt [0:16]
ÈìÅÈÅìÈÉ®/nis [0:3]
ÈìÅÈÅì/n [0:2]
ËøêËæìÂ±Ä/nis [3:6]
ËøêËæì/vn [3:5]
Ëê•ËøêÈÉ®/nz [6:9]
Ëê•Ëøê/vn [6:8]
Ë¥ßËøê/n [9:11]
ËøêËê•/vn [10:12]
Ëê•ÈîÄ/vn [11:13]
ËÆ°ÂàíÂ§Ñ/nis [13:16]
ËÆ°Âàí/n [13:15]

ÈìÅÈÅìÈÉ®ËøêËæìÂ±Ä/nto [0:6]
ÈìÅÈÅìÈÉ®/nis [0:3]
ÈìÅÈÅì/n [0:2]
ËøêËæìÂ±Ä/nis [3:6]
ËøêËæì/vn [3:5]

```

ÊàëËØïÁùÄÊîπ‰∫ÜÂá†Ë°å‰ª£Á†ÅÔºå‰ΩÜÊòØÁõ¥Êé•Â∞±Êä•Êï∞ÁªÑË∂äÁïå‰∫ÜÔºåÊâÄ‰ª•ËøòÊòØÊù•Âêë‰Ω†Ê±ÇÂä©‰∫Ü„ÄÇ"
ÂàÜËØçËØçÊÄßÈáçËÆ≠ÁªÉ‰∏≠Êüê‰∫õËØç‰∏çÂú®CoreNatureDictionaryËØçË°®‰∏≠,"ÊàëÂú®ËÆ≠ÁªÉÈõÜ‰∏≠ÊúâËøôÊ†∑‰∏ÄÂè•ËØù      Êàë/rr ÂñúÊ¨¢/v ËìùÂÆùÁü≥ÈÖíÂ∫ó/nt ÂèØÊòØ/c Â∑≤Áªè/d Âõû‰∏çÂéª/vf ‰∫Ü/ule
ÁîüÊàêÁöÑÊ†∏ÂøÉËØçÂÖ∏CoreNatureDictionaryÁöÑËØçÊù°‰∏≠Ê≤°Êúâ   ËìùÂÆùÁü≥ÈÖíÂ∫óËøô‰∏™ËØçÊù°   ÊÄé‰πàËÉΩ‰ΩøÂæóÁîüÊàêÁöÑÊ†∏ÂøÉËØçË°®‰∏≠ÊúâËøô‰∏™ËØç„ÄÇ"
Â≠óÊØçÊï∞Â≠óÊ∑∑ÂêàÁöÑÁºñÂè∑ÂàÜËØçÈîôËØØ,"HanLPÁâàÊú¨`1.3.2`

ÊµãËØïÁî®‰æãÂ¶Ç‰∏ãÔºå
```java
@Test public void wordSegment2() {
        HanLP.Config.enableDebug();
        assertThat(new NShortSegment().enableCustomDictionary(true).seg(""1000000438L01"").stream().map(w -> w.word))
                .containsExactly(""1000000438L01"");
    }
```

HanLPÊåâËØçÊÄßÊääÁºñÂè∑ÊãÜÂàÜ‰∫Ü„ÄÇÂ¶ÇÊûúÊÉ≥‰øùÁïôÂÆåÊï¥ÁºñÂè∑ÔºåÂ∫îËØ•Â¶Ç‰ΩïËÆæÁΩÆÔºü

> ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
> to:  1, from:  0, weight:04.56, word:Âßã##Âßã@Êú™##Êï∞
> to:  2, from:  1, weight:07.60, word:Êú™##Êï∞@Êú™##‰∏≤
> to:  3, from:  2, weight:07.06, word:Êú™##‰∏≤@Êú™##Êï∞
> to:  4, from:  3, weight:03.69, word:Êú™##Êï∞@Êú´##Êú´
> 
> Á≤óÂàÜÁªìÊûú[1000000438/m, L/nx, 01/m]"
Âú®C#‰∏≠‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÊñ∞Âª∫NatureÂ§±Ë¥•,"ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöv1.3.2
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöv1.3.2


## ÊàëÁöÑÈóÆÈ¢ò

ÊàëÊÑèÂõæÂú®C#‰∏≠‰ΩøÁî®HanLPÔºåÊåâÁÖßÊïôÁ®ã[Âú®CSharp‰∏≠Ë∞ÉÁî®HanLP](http://www.hankcs.com/nlp/call-hanlp-in-csharp.html)ÊàëÂ∑≤ÁªèÊàêÂäü‰ΩøÁî®iKVMÊâìÂåÖÔºåÂπ∂Âú®C#ËøêË°åÁî®‰æã„ÄÇ‰∏çËøáÊàëÊÉ≥Êñ∞Âª∫Áî®Êà∑Â≠óÂÖ∏Âπ∂Êñ∞Âª∫NatureÔºå‰ª•Êª°Ë∂≥ÊàëËØÜÂà´Ëá™ÂÆö‰πâÁ±ªÂûãÂÆû‰ΩìÁöÑË¶ÅÊ±ÇÔºåÂ¶ÇÊñ∞Âª∫Âêç‰∫∫Á±ªÂûãÔºönrmy„ÄÇ

Âú®Java‰∏≠ÈÄöËøá‰øÆÊîπhanlp.propertiesÂèØ‰ª•Ê∑ªÂä†Êñ∞ÁöÑÁ±ªÂûãnrmyÔºåÂú®ÂÜô‰ª£Á†ÅÁöÑÊó∂ÂÄôÊ∑ªÂä†Nature.create(""nrmy"")‰πüÊ≤°ÈóÆÈ¢ò„ÄÇ‰ª•‰∏ä‰∏§Ê≠•ÂÅöÂÆå‰πãÂêéJava‰ª£Á†ÅÂèØ‰ª•ËØÜÂà´Êñ∞Ê∑ªÂä†ÁöÑnrmyÂÆû‰Ωì„ÄÇ

‰∏çËøáC#‰ª£Á†Å‰ºöÊòæÁ§∫Êó†Ê≥ï‰ªéhanlp.properties‰∏≠ËØªÂèñÊñ∞ÁöÑÁ±ªÂûãnrmy„ÄÇÊñ∞ÁöÑÁ±ªÂûãÂÆåÂÖ®Â§±Êïà„ÄÇ

## Â§çÁé∞ÈóÆÈ¢ò
```Java
        static void Main(string[] args)
        {
            java.lang.System.getProperties().setProperty(""java.class.path"", ""mypath"");
            Nature.create(""nrmy"");
            CustomDictionary.insert(""ÁôΩÂØåÁæé"", ""nrmy 1024"");
            Console.WriteLine(HanLP.segment(""Ê¨¢ËøéÂú®CSharp‰∏≠Ë∞ÉÁî®HanLPÁöÑAPI""));
            Console.ReadKey();
        }
```

## Êä•ÈîôÔºö
WARNING: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª

Unhandled Exception: java.lang.IllegalArgumentException: Could not set the enum ---> java.lang.IllegalAccessException: Can not set static final [Lcom.hankcs.hanlp.corpus.tag.Nature; field com.hankcs.hanlp.corpus.tag.Nature.$VALUES to [Lcom.hankcs.hanlp.corpus.tag.Nature;
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value, FieldAccessor`1 acc)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.ObjectField.set(Object obj, Object value)
   at com.hankcs.hanlp.corpus.util.ReflectionHelper.setStaticFinalField(Field field, Object value)
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   --- End of inner exception stack trace ---
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(String name)
   at com.hankcs.hanlp.corpus.tag.Nature.create(String name)
   at HanLPSharp.Program.Main(String[] args) in mypath
"
Á≠âÊïàËØç‰∏≤ÈÉΩÊúâÂì™‰∫õ,"[BigramÂàÜËØç‰∏≠ÁöÑÁ≠âÊïàËØç‰∏≤](http://www.hankcs.com/nlp/segment/bigram-word-equivalent-word-in-string.html)
‰Ω†ÁöÑÊñáÁ´†Âàó‰∏æ‰∫Ü‰∏Ä‰∫õÁ≠âÊïàËØçÊÄßÔºåËØ∑ÈóÆÈô§‰∫Ü‰∏ãÈù¢Ëøô‰∫õÔºåËøòÊúâÂì™‰∫õÂë¢?
Êú™##Âú∞, Âßã##Âßã, Êú™##ÂÆÉ, Êú™##Âõ¢, Êú™##Êï∞, Êú™##‰∏ì, Êú™##Êó∂, Êú™##‰∏≤, Êú´##Êú´, Êú™##‰∫∫

ËøôÈáåÁöÑÊú™Êúâ‰ªÄ‰πàÂê´‰πâÂêóÔºüÊú™ÁôªÂΩïËØçÔºüËøôÊ†∑‰ºö‰∏ç‰ºöÂΩ±ÂìçÂà∞Â∑≤ÁªèÁôªÂΩïÁöÑÂ≠óËØçÂë¢Ôºü

eg:ÁßªÈô§ÊØèÂ§©Êôö‰∏ä‰∫åÂçÅ‰∏ÄÁÇπÁöÑÊó•Á®ã
Âú®Âä†ÂÖ•‚ÄùÁßªÈô§  v 100""ÂêéÔºåÁßªÂíåÈô§ËøòÊòØÂàÜÂºÄÁöÑ

> ÁâàÊú¨1.3.2ÔºåNSÂàÜËØç

************START**************
ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.60, word:Âßã##Âßã@Áßª
to:  2, from:  0, weight:04.60, word:Âßã##Âßã@ÁßªÈô§
to:  3, from:  1, weight:02.31, word:Áßª@Èô§
to:  4, from:  2, weight:11.25, word:ÁßªÈô§@ÊØè
to:  4, from:  3, weight:05.70, word:Èô§@ÊØè
to:  5, from:  2, weight:11.25, word:ÁßªÈô§@Êú™##Êó∂
to:  5, from:  3, weight:03.40, word:Èô§@Êú™##Êó∂
to:  6, from:  4, weight:10.58, word:ÊØè@Â§©
to:  7, from:  5, weight:04.76, word:Êú™##Êó∂@Êôö
to:  7, from:  6, weight:10.04, word:Â§©@Êôö
to:  8, from:  5, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to:  8, from:  6, weight:04.07, word:Â§©@Êú™##Êó∂
to:  9, from:  7, weight:10.68, word:Êôö@‰∏ä
to: 10, from:  8, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to: 10, from:  9, weight:04.09, word:‰∏ä@Êú™##Êó∂
to: 14, from: 11, weight:05.57, word:Êú™##Êó∂@ÁÇπ
to: 15, from: 10, weight:03.04, word:Êú™##Êó∂@ÁöÑ
to: 15, from: 12, weight:03.04, word:Êú™##Êó∂@ÁöÑ
to: 15, from: 13, weight:03.04, word:Êú™##Êó∂@ÁöÑ
to: 15, from: 14, weight:02.98, word:ÁÇπ@ÁöÑ
to: 16, from: 15, weight:05.62, word:ÁöÑ@Êó•
to: 17, from: 15, weight:05.67, word:ÁöÑ@Êó•Á®ã
to: 18, from: 16, weight:10.10, word:Êó•@Á®ã
to: 19, from: 17, weight:03.52, word:Êó•Á®ã@Êú´##Êú´
to: 19, from: 18, weight:06.15, word:Á®ã@Êú´##Êú´

Á≤óÂàÜÁªìÊûú[Áßª/v, Èô§/p, ÊØèÂ§©/t, Êôö‰∏ä/t, ‰∫åÂçÅ‰∏ÄÁÇπ/t, ÁöÑ/ude1, Êó•Á®ã/n]"
Ê†∏ÂøÉÂ≠óÂÖ∏‰∏≠‚ÄúÊØèxx‚ÄùÁöÑËØçÊÄßÊòØ‰∏çÊòØÊúâËØØÔºü,"ÁâàÊú¨1.3.2
ÊØé	nz	10  // ÂÖ∂‰ªñ‰∏ìÂêçÔºü
ÊØè	rz	4131 // ÊåáÁ§∫‰ª£ËØçÔºü
ÊØè‰∏ÄÂπ¥	nz	31 //Â∫îËØ•‰∏∫t?
ÊØè‰∏ÄÊñπ	nz	1
ÊØè‰∏ÄÊ¨°	d	188
ÊØè‰∏ÄÊ≠•	d	44
ÊØè‰∏ÉÂπ¥	nz	1 //Â∫îËØ•‰∏∫t?
ÊØè‰∏™	r	2135   //Â∫îËØ•‰∏∫m?
ÊØè‰∫îÂπ¥	nz	4 //Â∫îËØ•‰∏∫t?
ÊØè‰∫©	r	117 //Â∫îËØ•‰∏∫m?
ÊØè‰∫∫	r	904
ÊØè‰ª∂	d	61
ÊØè‰ªΩ	r	17 //Â∫îËØ•‰∏∫m?
ÊØè‰Ωç	r	201 
ÊØèÂÜµÊÑà‰∏ã	vl	22
ÊØèÂåÖ	nz	16 //Â∫îËØ•‰∏∫m?
ÊØèÂåπ	nz	2 //Â∫îËØ•‰∏∫m?
ÊØèÂçÅÂπ¥	nz	5 //Â∫îËØ•‰∏∫t?
ÊØèÂçäÂπ¥	nz	24 //Â∫îËØ•‰∏∫t?
ÊØèÂê®	q	123 //Â∫îËØ•‰∏∫m?
ÊØèÂë®	r	603 //Â∫îËØ•‰∏∫t?
ÊØèÂë®‰∏â	nz	21 //Â∫îËØ•‰∏∫t?
ÊØèÂõõÂπ¥	nz	10  //Â∫îËØ•‰∏∫t?
ÊØèÂõû	nz	8
ÊØèÂú∫	r	45
ÊØèÂùó	r	16
ÊØèÂ§ú	nz	14 //Â∫îËØ•‰∏∫t?
ÊØèÂ§©	r	5072 //Â∫îËØ•‰∏∫t?
ÊØèÂ§¥	nz	15
ÊØèÂ•ó	r	23
ÊØèÂÆ∂	r	117
ÊØèÂ±Ä	r	2
ÊØèÂ±Ç	d	27
ÊØèÂπ¥	r	3400 //Â∫îËØ•‰∏∫t?
ÊØèÂº†	r	116
ÊØèÂΩì	p	185
ÊØèÊà∑	r	120
ÊØèÊâπ	nz	7
ÊØèÊéí	nz	17
ÊØèÊîØ	r	16
ÊØèÊñ§	nz	141
ÊØèÊó•	r	1636 //Â∫îËØ•‰∏∫t?
ÊØèÊó¨	nz	1
ÊØèÊó∂ÊØèÂàª	bl	15
ÊØèÊòüÊúü	d	17
ÊØèÊôö	r	248 //Â∫îËØ•‰∏∫t?
ÊØèÊúà	r	1329 //Â∫îËØ•‰∏∫t?
ÊØèÊúü	r	36
ÊØèÊù°	d	54
ÊØèÊ†∑	nz	5
ÊØèÊ°∂	r	14
ÊØèÊ¨°	r	1501
ÊØèÊÆµË∑Ø	nz	1
ÊØèÊØè	d	59
ÊØèÁâá	d	17
...
"
Á¥¢ÂºïÂàÜËØçÂá∫Áé∞‰∫ÜÈáçÂ§çÁöÑËØç,"hi hankcsÔºåÊúâ‰∏™ÈóÆÈ¢òËøòÈúÄË¶ÅÈ∫ªÁÉ¶‰Ω†ÔºåÂÖ≥‰∫éËøô‰∏™ÈóÆÈ¢òÊàëÁúã‰∫Ü‰∏ã‰ª£Á†ÅÔºå‰ΩÜÂÖ∑‰ΩìÁÆóÊ≥ïÊ≤°ÂºÑÊáÇ„ÄÇ
## ÁâàÊú¨Âè∑
masterÂàÜÊîØ
ÂΩìÂâçÊúÄÊñ∞ÁâàÊú¨Âè∑ÊòØÔºöd37f97c8d54acda7ca1c6a8baae5cf0ebbd6a775
Êàë‰ΩøÁî®ÁöÑÁâàÊú¨ÊòØÔºöd37f97c8d54acda7ca1c6a8baae5cf0ebbd6a775


## ÊàëÁöÑÈóÆÈ¢ò
Á¥¢ÂºïÂàÜËØçÂá∫Áé∞‰∫ÜÈáçÂ§çÁöÑËØç

## Â§çÁé∞ÈóÆÈ¢ò

### Ëß¶Âèë‰ª£Á†Å

```
        List<Term> termList = IndexTokenizer.segment(""Âçó‰∫¨Â∏ÇÈïøÊ±üÂ§ßÊ°•"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
```

### ÂÆûÈôÖËæìÂá∫
Âá∫Áé∞‰∫Ü‰∏§‰∏™ÈïøÊ±üÂ§ßÊ°•
```
Âçó‰∫¨Â∏Ç/ns [0:3]
Âçó‰∫¨/ns [0:2]
ÈïøÊ±üÂ§ßÊ°•/nz [1:5]
Â∏ÇÈïø/nnt [2:4]
ÈïøÊ±üÂ§ßÊ°•/nz [3:7]
ÈïøÊ±ü/ns [3:5]
Â§ßÊ°•/n [5:7]
```

## ÂÖ∂‰ªñ‰ø°ÊÅØ

Áúã‰∫Ü‰∏ã‰ª£Á†ÅÔºåÊòØÂú®ViterbiSegmentËøô‰∏™Á±ªÁöÑ56Ë°åÔºåË∞ÉÁî®Ëøô‰∏™ÊñπÊ≥ïÂêéÂØºËá¥ÁöÑcombineByCustomDictionary(vertexList, wordNetAll);
Ë∞ÉÁî®ÂÆåÂêéÔºåËØçÂõæË≤å‰ººÂ∞±ÊúâÈóÆÈ¢ò‰∫Ü
![qq20170502-115123](https://cloud.githubusercontent.com/assets/5125893/25603470/a55b1b3e-2f2e-11e7-8751-529a30eecafe.jpg)

"
ÂàÜÂºÄ‰∏çÂàÜÂºÄÔºåÊâìÊ≠ª‰πü‰∏çÂàÜÂºÄ^_^,"ÁâàÊú¨1.3.2
**Êñ∞Â¢û‰∏Ä‰∏™ÊòüÊúü‰∫î‰∏ãÂçà4ÁÇπ15ÂàÜÂºÄ‰∏ãÂçàËÆ®ËÆ∫‰ºöÁöÑÊó•Á®ã**

> Â§ÑÁêÜ

> Âà†Êéâ

Êú™##Êï∞@ÂàÜÂºÄ 58

> Âä†ÂÖ•

ÂºÄ@‰∏äÂçà 100
ÂºÄ@‰∏≠Âçà 100
ÂºÄ@‰∏ãÂçà 100
ÂºÄ@‰∏äÂçà‰ºö 100
ÂºÄ@‰∏≠Âçà‰ºö 100
ÂºÄ@‰∏ãÂçà‰ºö 100
ÂºÄ@ËÆ®ËÆ∫‰ºö 100
ÂºÄ@‰∏äÂçàËÆ®ËÆ∫‰ºö 100
ÂºÄ@‰∏≠ÂçàËÆ®ËÆ∫‰ºö 100
ÂºÄ@‰∏ãÂçàËÆ®ËÆ∫‰ºö 100

> ‰øÆÊîπ

Êú™##Êï∞@ÂàÜ 363 -->Êú™##Êï∞@ÂàÜ 10000

> DEGUG

ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.56, word:Âßã##Âßã@Êñ∞
to:  2, from:  0, weight:04.60, word:Âßã##Âßã@Êñ∞Â¢û
to:  3, from:  1, weight:09.57, word:Êñ∞@Â¢û
to:  4, from:  2, weight:01.07, word:Êñ∞Â¢û@Êú™##Êï∞
to:  4, from:  3, weight:01.16, word:Â¢û@Êú™##Êï∞
to:  6, from:  4, weight:05.12, word:Êú™##Êï∞@Êòü
to:  6, from:  5, weight:08.77, word:‰∏™@Êòü
to:  7, from:  4, weight:05.09, word:Êú™##Êï∞@ÊòüÊúü
to:  7, from:  5, weight:06.62, word:‰∏™@ÊòüÊúü
to:  8, from:  4, weight:04.59, word:Êú™##Êï∞@Êú™##Êó∂
to:  8, from:  5, weight:05.18, word:‰∏™@Êú™##Êó∂
to:  9, from:  6, weight:11.26, word:Êòü@Êúü
to: 10, from:  8, weight:05.77, word:Êú™##Êó∂@‰∏ã
to: 11, from:  8, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to: 12, from: 10, weight:09.41, word:‰∏ã@Âçà
to: 13, from: 11, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to: 13, from: 12, weight:11.60, word:Âçà@Êú™##Êó∂
to: 15, from: 13, weight:03.44, word:Êú™##Êó∂@Êú™##Êï∞
to: 15, from: 14, weight:02.68, word:ÁÇπ@Êú™##Êï∞
to: 16, from: 15, weight:00.11, word:Êú™##Êï∞@ÂàÜ
to: 17, from: 15, weight:09.93, word:Êú™##Êï∞@ÂàÜÂºÄ
to: 18, from: 16, weight:10.66, word:ÂàÜ@ÂºÄ
to: 19, from: 17, weight:11.45, word:ÂàÜÂºÄ@‰∏ã
to: 19, from: 18, weight:06.89, word:ÂºÄ@‰∏ã
to: 20, from: 17, weight:04.73, word:ÂàÜÂºÄ@Êú™##Êó∂
to: 20, from: 18, weight:06.89, word:ÂºÄ@Êú™##Êó∂
to: 21, from: 19, weight:09.41, word:‰∏ã@Âçà
to: 22, from: 20, weight:05.78, word:Êú™##Êó∂@ËÆ®
to: 22, from: 21, weight:11.60, word:Âçà@ËÆ®
to: 23, from: 20, weight:05.79, word:Êú™##Êó∂@ËÆ®ËÆ∫
to: 23, from: 21, weight:11.60, word:Âçà@ËÆ®ËÆ∫
to: 24, from: 20, weight:05.80, word:Êú™##Êó∂@ËÆ®ËÆ∫‰ºö
to: 24, from: 21, weight:11.60, word:Âçà@ËÆ®ËÆ∫‰ºö
to: 25, from: 22, weight:11.39, word:ËÆ®@ËÆ∫
to: 26, from: 23, weight:10.83, word:ËÆ®ËÆ∫@‰ºö
to: 26, from: 25, weight:11.45, word:ËÆ∫@‰ºö
to: 27, from: 24, weight:02.67, word:ËÆ®ËÆ∫‰ºö@ÁöÑ
to: 27, from: 26, weight:04.51, word:‰ºö@ÁöÑ
to: 28, from: 27, weight:05.62, word:ÁöÑ@Êó•
to: 29, from: 27, weight:05.67, word:ÁöÑ@Êó•Á®ã
to: 30, from: 28, weight:10.10, word:Êó•@Á®ã
to: 31, from: 29, weight:03.52, word:Êó•Á®ã@Êú´##Êú´
to: 31, from: 30, weight:06.15, word:Á®ã@Êú´##Êú´

**Á≤óÂàÜÁªìÊûú[Êñ∞Â¢û/v, ‰∏Ä‰∏™/mq, ÊòüÊúü‰∫î/t, ‰∏ãÂçà/t, 4ÁÇπ/t, 15/m, ÂàÜÂºÄ/vi, ‰∏ãÂçà/t, ËÆ®ËÆ∫‰ºö/n, ÁöÑ/ude1, Êó•Á®ã/n]**
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Êñ∞Â¢û A 22202445 ][‰∏Ä‰∏™ K 90 L 84 ][ÊòüÊúü‰∫î A 22202445 ][‰∏ãÂçà L 15 K 12 ][4ÁÇπ A 22202445 ][15 L 8 ][ÂàÜÂºÄ L 4 ][‰∏ãÂçà L 15 K 12 ][ËÆ®ËÆ∫‰ºö A 22202445 ][ÁöÑ L 15411 K 11354 M 96 C 1 ][Êó•Á®ã A 22202445 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Êñ∞Â¢û/A ,‰∏Ä‰∏™/K ,ÊòüÊúü‰∫î/A ,‰∏ãÂçà/K ,4ÁÇπ/A ,15/L ,ÂàÜÂºÄ/L ,‰∏ãÂçà/K ,ËÆ®ËÆ∫‰ºö/A ,ÁöÑ/K ,Êó•Á®ã/A , /A]

**Á≤óÂàÜÁªìÊûú[Êñ∞Â¢û/v, ‰∏Ä‰∏™/mq, ÊòüÊúü‰∫î/t, ‰∏ãÂçà/t, 4ÁÇπ/t, 15ÂàÜ/t, ÂºÄ/v, ‰∏ãÂçà/t, ËÆ®ËÆ∫‰ºö/n, ÁöÑ/ude1, Êó•Á®ã/n]**
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Êñ∞Â¢û A 22202445 ][‰∏Ä‰∏™ K 90 L 84 ][ÊòüÊúü‰∫î A 22202445 ][‰∏ãÂçà L 15 K 12 ][4ÁÇπ A 22202445 ][15ÂàÜ A 22202445 ][ÂºÄ C 865 L 71 D 53 K 13 E 7 ][‰∏ãÂçà L 15 K 12 ][ËÆ®ËÆ∫‰ºö A 22202445 ][ÁöÑ L 15411 K 11354 M 96 C 1 ][Êó•Á®ã A 22202445 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Êñ∞Â¢û/A ,‰∏Ä‰∏™/K ,ÊòüÊúü‰∫î/A ,‰∏ãÂçà/K ,4ÁÇπ/A ,15ÂàÜ/A ,ÂºÄ/K ,‰∏ãÂçà/L ,ËÆ®ËÆ∫‰ºö/A ,ÁöÑ/K ,Êó•Á®ã/A , /A]
ÂõõÊúà 28, 2017 5:07:00 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreDictionaryTransformMatrixDictionary <clinit>

**[Êñ∞Â¢û/v, ‰∏Ä‰∏™/mq, ÊòüÊúü‰∫î/t, ‰∏ãÂçà/t, 4ÁÇπ/t, 15/m, ÂàÜÂºÄ/vi, ‰∏ãÂçà/t, ËÆ®ËÆ∫‰ºö/n, ÁöÑ/ude1, Êó•Á®ã/n]**

"
CoreDictionaryÁöÑtotalFrequency ÊòØ‰∏™Â∏∏Èáè,"ÁâàÊú¨1.3.2
public static final int totalFrequency = 221894;
‰∏∫‰ªÄ‰πàË¶ÅÂÆö‰∏∫Â∏∏ÈáèÂë¢ÔºüÊ†∏ÂøÉÂ≠óÂÖ∏‰∏çÊòØ‰πüÂèØ‰ª•‰øÆÊîπÂêóÔºü"
NSÂàÜËØçÁõ¥Êé•Â∞±ÂèñÁ¨¨‰∏Ä‰∏™ÂàÜËØçÁªìÊûú,"ÁâàÊú¨1.3.2
Â¶Ç List<Vertex> vertexList = coarseResult.get(0);

**eg:ÂàõÂª∫‰∏Ä‰∏™‰∏ãÂçà3ÁÇπ‰∏æÂäû‰∏ãÂçàËå∂ÁöÑÊèêÈÜí**

‚Äù‰∏ãÂçàËå∂‚ÄúË¢´ÂàÜÊàê‚Äù‰∏ãÂçà Ëå∂‚Äú
ÊàëÂ∞ùËØïÂ¢ûÂ§ß‚Äù‰∏ãÂçàËå∂‚ÄúÈ¢ëÁéá  Âíå  Âà†Èô§‚ÄùÊú™##Êó∂@Ëå∂‚Äú‰πüÊ≤°Êúâ‰ΩúÁî®
Âè™ÂæóÊöÇÊó∂Â¢ûÂä†‚Äù‰∏æÂäû@‰∏ãÂçàËå∂ 1‚ÄúÊù•Ëß£ÂÜ≥

ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.60, word:Âßã##Âßã@Âàõ
to:  2, from:  0, weight:04.60, word:Âßã##Âßã@ÂàõÂª∫
to:  3, from:  1, weight:11.32, word:Âàõ@Âª∫
to:  4, from:  2, weight:03.01, word:ÂàõÂª∫@Êú™##Êï∞
to:  4, from:  3, weight:02.21, word:Âª∫@Êú™##Êï∞
to:  6, from:  4, weight:05.10, word:Êú™##Êï∞@‰∏ã
to:  6, from:  5, weight:08.38, word:‰∏™@‰∏ã
to:  7, from:  4, weight:04.59, word:Êú™##Êï∞@Êú™##Êó∂
to:  7, from:  5, weight:05.18, word:‰∏™@Êú™##Êó∂
to:  8, from:  6, weight:09.41, word:‰∏ã@Âçà
to:  9, from:  7, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to:  9, from:  8, weight:11.60, word:Âçà@Êú™##Êó∂
to: 11, from:  9, weight:05.80, word:Êú™##Êó∂@‰∏æ
to: 11, from: 10, weight:09.78, word:ÁÇπ@‰∏æ
to: 12, from:  9, weight:05.69, word:Êú™##Êó∂@‰∏æÂäû
to: 12, from: 10, weight:09.78, word:ÁÇπ@‰∏æÂäû
to: 13, from: 11, weight:11.40, word:‰∏æ@Âäû
to: 14, from: 12, weight:10.87, word:‰∏æÂäû@‰∏ã
to: 14, from: 13, weight:06.25, word:Âäû@‰∏ã
to: 15, from: 12, weight:03.78, word:‰∏æÂäû@Êú™##Êó∂
to: 15, from: 13, weight:05.21, word:Âäû@Êú™##Êó∂
to: 16, from: 12, weight:10.87, word:‰∏æÂäû@‰∏ãÂçàËå∂
to: 16, from: 13, weight:10.63, word:Âäû@‰∏ãÂçàËå∂
to: 17, from: 14, weight:09.41, word:‰∏ã@Âçà
to: 18, from: 15, weight:05.80, word:Êú™##Êó∂@Ëå∂
to: 18, from: 17, weight:11.60, word:Âçà@Ëå∂
to: 19, from: 16, weight:02.38, word:‰∏ãÂçàËå∂@ÁöÑ
to: 19, from: 18, weight:02.61, word:Ëå∂@ÁöÑ
to: 20, from: 19, weight:05.67, word:ÁöÑ@Êèê
to: 21, from: 19, weight:05.64, word:ÁöÑ@ÊèêÈÜí
to: 22, from: 20, weight:11.05, word:Êèê@ÈÜí
to: 23, from: 21, weight:03.00, word:ÊèêÈÜí@Êú´##Êú´
to: 23, from: 22, weight:04.25, word:ÈÜí@Êú´##Êú´

Á≤óÂàÜÁªìÊûú[ÂàõÂª∫/v, ‰∏Ä‰∏™/mq, ‰∏ãÂçà/t, 3ÁÇπ/t, ‰∏æÂäû/v, ‰∏ãÂçà/t, Ëå∂/n, ÁöÑ/ude1, ÊèêÈÜí/v]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][ÂàõÂª∫ L 16 ][‰∏Ä‰∏™ K 90 L 84 ][‰∏ãÂçà L 15 K 12 ][3ÁÇπ A 22202445 ][‰∏æÂäû K 11 L 9 ][‰∏ãÂçà L 15 K 12 ][Ëå∂ D 19 C 2 E 2 L 2 ][ÁöÑ L 15411 K 11354 M 96 C 1 ][ÊèêÈÜí L 123 K 14 M 2 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,ÂàõÂª∫/L ,‰∏Ä‰∏™/K ,‰∏ãÂçà/L ,3ÁÇπ/A ,‰∏æÂäû/K ,‰∏ãÂçà/L ,Ëå∂/D ,ÁöÑ/L ,ÊèêÈÜí/K , /A]
Á≤óÂàÜÁªìÊûú[ÂàõÂª∫/v, ‰∏Ä‰∏™/mq, ‰∏ãÂçà/t, 3ÁÇπ/t, ‰∏æÂäû/v, ‰∏ãÂçàËå∂/nf, ÁöÑ/ude1, ÊèêÈÜí/v]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][ÂàõÂª∫ L 16 ][‰∏Ä‰∏™ K 90 L 84 ][‰∏ãÂçà L 15 K 12 ][3ÁÇπ A 22202445 ][‰∏æÂäû K 11 L 9 ][‰∏ãÂçàËå∂ A 22202445 ][ÁöÑ L 15411 K 11354 M 96 C 1 ][ÊèêÈÜí L 123 K 14 M 2 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,ÂàõÂª∫/L ,‰∏Ä‰∏™/K ,‰∏ãÂçà/L ,3ÁÇπ/A ,‰∏æÂäû/K ,‰∏ãÂçàËå∂/A ,ÁöÑ/K ,ÊèêÈÜí/L , /A]
[ÂàõÂª∫/v, ‰∏Ä‰∏™/mq, ‰∏ãÂçà/t, 3ÁÇπ/t, ‰∏æÂäû/v, ‰∏ãÂçà/t, Ëå∂/n, ÁöÑ/ude1, ÊèêÈÜí/v]
ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.60, word:Âßã##Âßã@Âàõ
to:  2, from:  0, weight:04.60, word:Âßã##Âßã@ÂàõÂª∫
to:  3, from:  1, weight:11.32, word:Âàõ@Âª∫
to:  4, from:  2, weight:03.01, word:ÂàõÂª∫@Êú™##Êï∞
to:  4, from:  3, weight:02.21, word:Âª∫@Êú™##Êï∞
to:  6, from:  4, weight:05.10, word:Êú™##Êï∞@‰∏ã
to:  6, from:  5, weight:08.38, word:‰∏™@‰∏ã
to:  7, from:  4, weight:04.59, word:Êú™##Êï∞@Êú™##Êó∂
to:  7, from:  5, weight:05.18, word:‰∏™@Êú™##Êó∂
to:  8, from:  6, weight:09.41, word:‰∏ã@Âçà
to:  9, from:  7, weight:02.52, word:Êú™##Êó∂@Êú™##Êó∂
to:  9, from:  8, weight:11.60, word:Âçà@Êú™##Êó∂
to: 11, from:  9, weight:05.80, word:Êú™##Êó∂@‰∏æ
to: 11, from: 10, weight:09.78, word:ÁÇπ@‰∏æ
to: 12, from:  9, weight:05.69, word:Êú™##Êó∂@‰∏æÂäû
to: 12, from: 10, weight:09.78, word:ÁÇπ@‰∏æÂäû
to: 13, from: 11, weight:11.40, word:‰∏æ@Âäû
to: 14, from: 12, weight:10.87, word:‰∏æÂäû@‰∏≠
to: 14, from: 13, weight:08.25, word:Âäû@‰∏≠
to: 15, from: 12, weight:03.78, word:‰∏æÂäû@Êú™##Êó∂
to: 15, from: 13, weight:05.21, word:Âäû@Êú™##Êó∂
to: 16, from: 14, weight:08.17, word:‰∏≠@Âçà
to: 17, from: 15, weight:05.80, word:Êú™##Êó∂@Ëå∂
to: 17, from: 16, weight:11.60, word:Âçà@Ëå∂
to: 18, from: 17, weight:02.61, word:Ëå∂@ÁöÑ
to: 19, from: 18, weight:05.67, word:ÁöÑ@Êèê
to: 20, from: 18, weight:05.64, word:ÁöÑ@ÊèêÈÜí
to: 21, from: 19, weight:11.05, word:Êèê@ÈÜí
to: 22, from: 20, weight:03.00, word:ÊèêÈÜí@Êú´##Êú´
to: 22, from: 21, weight:04.25, word:ÈÜí@Êú´##Êú´
"
ÂèòÈáèÂêç‰øÆÊ≠£,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

‰øÆÊ≠£ÂèòÈáèÂêç




"
ÁßªÈô§ÈÄªËæëÈáçÂ§çÁöÑËØ≠Âè•,"## Ê≥®ÊÑè‰∫ãÈ°π

* ËøôÊ¨°‰øÆÊîπÊ≤°ÊúâÂºïÂÖ•Á¨¨‰∏âÊñπÁ±ªÂ∫ì„ÄÇ
* ‰πüÊ≤°Êúâ‰øÆÊîπJDKÁâàÊú¨Âè∑
* ÊâÄÊúâÊñáÊú¨ÈÉΩÊòØUTF-8ÁºñÁ†Å
* ‰ª£Á†ÅÈ£éÊ†º‰∏ÄËá¥
* [x] ÊàëÂú®Ê≠§Êã¨Âè∑ÂÜÖËæìÂÖ•xÊâìÈí©Ôºå‰ª£Ë°®‰∏äËø∞‰∫ãÈ°πÁ°ÆËÆ§ÂÆåÊØï„ÄÇ

## Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºüÂ∏¶Êù•‰∫Ü‰ªÄ‰πàÂ•ΩÂ§ÑÔºü

Âà†Èô§‰∫ÜÂ§ö‰ΩôÁöÑÊìç‰ΩúÔºåÊèêÈ´òÊïàÁéá„ÄÇ



"
‰∏≠ÊñáÂàÜËØçbug,"		System.out.println(NLPTokenizer.segment(""Ëä±‰∫Ü38ÂÖÉ‰∫∫Ê∞ëÂ∏Å""));
		System.out.println(HanLP.segment(""Ëä±‰∫Ü38ÂÖÉ‰∫∫Ê∞ëÂ∏Å""));
[Ëä±/n, ‰∫Ü/ule, 38/m, ÂÖÉ/q, ‰∫∫Ê∞ëÂ∏Å/n]
[Ëä±/n, ‰∫Ü/ule, 38/m, ÂÖÉ/q, ‰∫∫Ê∞ëÂ∏Å/n]"
ÂàÜËØçÈóÆÈ¢ò,"**ÈóÆÈ¢ò**
ÂØπÂè•Â≠ê""ÊàëÊÉ≥ÊèêÁÇπÈí±Âá∫Êù•""ËøõË°åÂàÜËØçÔºå
ÂæóÂà∞ÁªìÊûú: [Êàë/rr, ÊÉ≥/v, Êèê/v, ÁÇπÈí±/nz, Âá∫Êù•/vf]ÔºåÂàÜËØç‰∏≠‚ÄúÁÇπÈí±‚ÄùÂàáÂàÜÈîôËØØÔºü

**ÁâàÊú¨**
- Á®ãÂ∫èÁâàÊú¨Âè∑Ôºöhanlp-portable-1.2.7
- Êï∞ÊçÆÁâàÊú¨Âè∑Ôºödata-for-1.3.2.zip

**Ëß¶Âèë‰ª£Á†Å**
```java
HanLP.segment(""ÊàëÊÉ≥ÊèêÁÇπÈí±Âá∫Êù•"");
```
**‰ª£Á†ÅËæìÂá∫**
``` 
[Êàë/rr, ÊÉ≥/v, Êèê/v, ÁÇπÈí±/nz, Âá∫Êù•/vf]
```

**Â∞ùËØï**
ÂàÜËØçÁªìÊûúÊúâÈóÆÈ¢òÔºåÊää`ÁÇπÈí±`‰Ωú‰∏∫‰∏Ä‰∏™ËØçÂàáÂàÜÂá∫Êù•ÔºåËÄå‰∏çÊòØÂàáÂàÜ‰∏∫`ÊèêÁÇπ/v Èí±/n`„ÄÇ
ÊàëÂú®ËØçÂ∫ìCoreNatureDictionary.txt‰∏≠ÂèëÁé∞ ‚ÄúÁÇπÈí± nz 110‚ÄùÔºåËÄåÊ≤°Êúâ‚ÄúÊèêÁÇπ‚ÄùËØçÔºåÊâÄ‰ª•ÊàëÁî®CustomDictionaryÂ∞Ü‚ÄúÊèêÁÇπ‚ÄùÂä†ÂÖ•Ôºå‰ΩÜÊòØËøòÊòØÊ≤°ÊúâÊïàÊûúÔºåÈ∫ªÁÉ¶Â∏ÆÂøôÁúã‰∏Ä‰∏ãÔºåÈùûÂ∏∏ÊÑüË∞¢„ÄÇ"
Âè•Ê≥ï‰æùÂ≠ò‚Äî‚ÄîÂü∫‰∫éÁ•ûÁªèÁΩëÁªú,"‚ÄúÂª∫‰∏Ä‰∏™‰∏ãÂçà7ÁÇπ3ÂàªÁöÑÊèêÈÜí‚Äù
1	Âª∫	v	v	_	7	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
2	‰∏Ä‰∏™	m	mq	_	3	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
3	‰∏ãÂçà	n	n	_	4	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
4	7ÁÇπ	nt	t	_	5	ÂÆö‰∏≠ÂÖ≥Á≥ª	_	_
5	3Âàª	nt	t	_	1	Âä®ÂÆæÂÖ≥Á≥ª	_	_
6	ÁöÑ	u	ude1	_	5	Âè≥ÈôÑÂä†ÂÖ≥Á≥ª	_	_
7	ÊèêÈÜí	v	vn	_	0	Ê†∏ÂøÉÂÖ≥Á≥ª	_	_

Âª∫ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ÊèêÈÜí
‰∏Ä‰∏™ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ‰∏ãÂçà
‰∏ãÂçà --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> 7ÁÇπ
7ÁÇπ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> 3Âàª
3Âàª --(Âä®ÂÆæÂÖ≥Á≥ª)--> Âª∫
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> 3Âàª
ÊèêÈÜí --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##

ÊàëËÆ§‰∏∫ÊòØÊèêÈÜíÁöÑËØçÊÄßÂΩ±Âìç‰∫ÜÁªìÊûúÔºåÂàÜËØçÁî®ÁöÑÊòØNSÂàÜËØçÂô®ÔºåÂè™ÊòØÊàëÊ≤°ÊúâÊÉ≥Âà∞‰ªÄ‰πàÂäûÊ≥ï‰øÆÊîπÊèêÈÜíÁöÑËØçÊÄß‰∏∫n„ÄÇËøôÈáåÁöÑËØçÊÄßÂ∫îËØ•ÊòØÁî®HMMSegmentModel.binÊù•Ê†áÊ≥®ÁöÑÂêßÔºÅËØ∑ÈóÆÈô§‰∫ÜÈáçÊñ∞ËÆ≠ÁªÉ‰∏Ä‰∏™Ê®°ÂûãÔºåËøòÊúâ‰ªÄ‰πàÊñπÊ≥ïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò"
ÂèëÁé∞‰∏™‰∫∫ÂêçÂàÜËØçÁöÑbug,ÊâÄÊúâÂßìÂÜØ„ÄÅ‰∫é„ÄÅÊñáÁöÑÂêçÂ≠óÂàÜËØçÈÉΩÊòØÈîôËØØÁöÑÂÜØ‰ºöË¢´ÂàÜ‰∏∫nzÔºàÂÖ∂‰ªñ‰∏ìÂêçÔºâÔºõ‰∫éÔºå‰ºöË¢´ÂàÜ‰∏∫pÔºà‰ªãËØçÔºâÔºõÊñáÔºå‰ºöË¢´ÂàÜ‰∏∫ngÔºàÂêçËØçÊÄßËØ≠Á¥†Ôºâ
ÊÉÖÊÑüÂàÜÊûê,Hanlp‰∏∫‰ªÄ‰πàÊ≤°ÊúâÂÆûÁé∞ÊÉÖÊÑüÂàÜÊûêÁõ∏ÂÖ≥ÁöÑÂäüËÉΩÂë¢Ôºü
ÂèëÁé∞‰∏™ÂàÜËØçbugÔºöÂØπ‚ÄúÈòøÈáå‚ÄùÔºå‚ÄúÂçé‰∏∫‚ÄùÂàÜËØçÁªìÊûúËøîÂõûËØçÊÄß‰∏∫nsÔºàÂú∞ÂêçÔºâ,ÂØπ‚ÄúÈòøÈáå‚ÄùÔºå‚ÄúÂçé‰∏∫‚ÄùÂàÜËØçÁªìÊûúËøîÂõûËØçÊÄß‰∏∫nsÔºàÂú∞ÂêçÔºâ
pythonÂ¶Ç‰ΩïË∞ÉÁî®model‰∏ãÈù¢ÁöÑÊ®°Âûã,"pythonÂ¶Ç‰ΩïË∞ÉÁî®model‰∏ãÈù¢ÁöÑÊ®°Âûã,Hanlp 1.3.2"
ÂÖ≥‰∫émodelÔºàÊ®°ÂûãÔºâÂú®‰∏ãËΩΩÁöÑÊó∂ÂÄôÁôæÂ∫¶‰∫ëÂÆûÊïàËøòÊòØÁªôÂ¢ô‰∫ÜÔºü,"‰Ω†Â•ΩÔºö
        ÊàëÂú®‰∏ãËΩΩdata-1.28-fullÔºàÂ§ßËá¥ÔºâÁöÑÊó∂ÂÄôÔºåÁôæÂ∫¶‰∫ëËøûÊé•Â§±ÊïàÔºåËØ∑ÈóÆËøòÊúâ‰ªÄ‰πàË∑ØÂæÑÊàñËÄÖÈìæÊé•ÂêóÔºüÊàëÂ∞ùËØï‰∫ÜÁøªÂ¢ôÂéªÈìæÊé•ËøòÊòØ‰∏çÂèØ‰ª•ÁöÑ„ÄÇË∞¢Ë∞¢"
CrfÂàÜËØçÁªìÂêàngramËØçÂÖ∏‰∏çËÉΩÂèñÂæóÁêÜÊÉ≥ÁªìÊûú,"@hanks, ‰Ω†Â•Ω„ÄÇ
ÂÖ≥‰∫é‰∏ãÂàóËæìÂÖ•
CRFSegment segment = new CRFSegment().;
segment.enableCustomDictionary(true);
		List<Term> seg = segment.seg(""Âì™‰∫õËçØ‰∫ßËá™‰∏äÊµ∑"");

‰∏ÄÁõ¥Âá∫Áé∞‰∏çÁêÜÊÉ≥ÁöÑÂàÜËØçÁªìÊûú

[Âì™‰∫õ/r, ËçØ‰∫ß/nz, Ëá™/p, ‰∏äÊµ∑/ns]

ÁêÜÊÉ≥ÁöÑÁªìÊûúÊòØ [Âì™‰∫õ/r, ËçØ/n,‰∫ßËá™/v, ‰∏äÊµ∑/ns]

Â∑≤ÁªèÊåâÁÖßÊÇ®Âú®ÂÖ∂‰ªñissueÈáåÈù¢ËØ¥ÁöÑ‰øÆÊîπ‰∫ÜÊ†∏ÂøÉËØçÂÖ∏CoreNatureDictionary.mini.txt
ÂÖ∂‰∏≠ÂåÖÂê´Ôºà‰∫ßËá™	v	999ÔºâÊñ∞Â¢ûÔºåÔºàËçØ	n	41	v	1ÔºâËçØËøô‰∏™ËØçÊòØÂéüÊù•Â∞±ÊúâÁöÑ
‰πü‰øÆÊîπ‰∫ÜCoreNatureDictionary.ngram.mini.txt
ÔºàËçØ@‰∫ßËá™ 10000ÔºâÊñ∞Â¢ûÔºå
Âπ∂‰∏îÂú®Ëá™ÂÆö‰πâËØçÂÖ∏CustomDictionary.txt‰∏≠Êñ∞Â¢ûÔºà‰∫ßËá™ v 999Ôºâ
Âà†Èô§ÊâÄÊúâÁºìÂ≠òÔºåÂπ∂‰∏îËøêË°åÔºåËøòÊòØÂá∫Áé∞[Âì™‰∫õ/r, ËçØ‰∫ß/nz, Ëá™/p, ‰∏äÊµ∑/ns]ÁªìÊûú

‰∏çÁü•ÈÅìÊàëÂì™ÈáåÂÅöÁöÑ‰∏çÂØπÔºåÊàñËÄÖÁº∫Â∞ë‰∫ÜÂì™‰∫õÊ≠•È™§ÔºåËøòËØ∑Âä°ÂøÖÂëäÁü•
ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ
@TylunasLi"
ÂÖ≥‰∫éÂÅúÁî®ËØçÁöÑÈóÆÈ¢ò,"ÊàëÂú®stopwordsÈáåÂä†‰∫ÜÂÅúÁî®ËØç ‚ÄòÔºå‚Äô,ÁÑ∂ÂêéÂú®‰ª£Á†ÅÈáåÂä†‰∫ÜÂÅúÁî®ËØç‚ÄòËøëÊó•‚ÄôÔºåÁªìÊûúÈÉΩÊ≤°Ëµ∑‰ΩúÁî®
`      Segment segment = HanLP.newSegment();

        segment.enablePartOfSpeechTagging(true);
        segment.enableCustomDictionary(true);
        System.out.println(CoreStopWordDictionary.contains(""Ôºå""));
        CoreStopWordDictionary.add(""ËøëÊó•"");
        CustomDictionary.insert(""‰Ωç‰∫éÈªÑÈ™ÖÁªèÊµéÂºÄÂèëÂå∫"");
        CustomDictionary.add(""‰Ωç‰∫éÈªÑÈ™ÖÁªèÊµéÂºÄÂèëÂå∫"");
        System.out.println(segment.seg(""ËøëÊó•Ôºå‰Ωç‰∫éÈªÑÈ™ÖÁªèÊµéÂºÄÂèëÂå∫ÁöÑÊ≤≥ÂåóÂÆèÊ≥∞‰∏ìÁî®Ê±ΩËΩ¶ÊúâÈôêÂÖ¨Âè∏Áîü‰∫ßÁöÑÊñ∞ÂûãÈìùÂêàÈáëÁΩêÂºèËΩ¶„ÄÇ""));`
ÁªìÊûúÊòØÔºö[ËøëÊó•, Ôºå, ‰Ωç‰∫éÈªÑÈ™ÖÁªèÊµéÂºÄÂèëÂå∫, ÁöÑ, Ê≤≥Âåó, ÂÆèÊ≥∞, ‰∏ìÁî®Ê±ΩËΩ¶, ÊúâÈôêÂÖ¨Âè∏, Áîü‰∫ßÁöÑ, Êñ∞Âûã, ÈìùÂêàÈáë, ÁΩêÂºè, ËΩ¶, „ÄÇ]"
ËØ∑ÈóÆ‰∏Ä‰∏ãÊ±âÂ≠óÁπÅÁÆÄ‰ΩìËΩ¨Êç¢ÁöÑÈóÆÈ¢òÔºü,"‰Ω†Â•ΩÔºåÊàëÊúÄËøë‰ΩøÁî®ÁöÑ‰Ω†ÁöÑHanLP-1.2.8ÁâàÊú¨‰∏≠ÁöÑÊ±âÂ≠óÁπÅÁÆÄ‰ΩìËΩ¨Êç¢„ÄÇ
ÊàëÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊúâ‰∫õÁπÅ‰ΩìÂà∞ÁÆÄ‰ΩìÁöÑËΩ¨Êç¢Ôºå‰∏çÊòØÊåâÂéüÂ≠óËΩ¨Êç¢ÁöÑÔºåËÄåÊòØÊç¢‰∫Ü‰∏Ä‰∏™ËØçÔºåËøôÁªôÊàëÂ∏¶Êù•ÂæàÂ§ßÈóÆÈ¢ò„ÄÇ
ÊØîÂ¶ÇÔºöÊñáÂ≠ó=ÊñáÊú¨
ÂÜÖÂÆπ=Â±ûÊÄß
Âú®‰Ω†ÁöÑËØçÂÖ∏Êñá‰ª∂TraditionalChinese.txt‰∏≠ÂèØ‰ª•Êü•Âà∞„ÄÇÁ±ª‰ººÁöÑÊÉÖÂÜµÊúâÂæàÂ§ö„ÄÇ

ËØ∑ÈóÆÔºåËøô‰∏™ÈóÆÈ¢òÂèØ‰ª•ÈÄöËøáÈÖçÁΩÆËß£ÂÜ≥ÂêóÔºüÂèØ‰ª•‰∏•Ê†ºÊåâÂéüÂ≠óËΩ¨Êç¢ÂêóÔºü

Âè¶Â§ñÔºå‰Ω†ÁöÑËΩ¨Êç¢ËØçÂÖ∏Êúâ‰∫õÈÅóÊºèÁöÑËΩ¨Êç¢‰∏ç‰∫ÜÔºåÊØîÂ¶ÇÔºöÂãêÊà≥Ôºå Ê∏õËÇ•

ÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÊàëÂèØ‰ª•Âú®TraditionalChinese.txt‰∏≠Ê∑ªÂä†Êñ∞ÁöÑËΩ¨Êç¢ËØçË°®Ôºå‰ΩÜÊòØ‰∏çÁü•ÈÅìÊÄé‰πà‰∫ßÁîübinÊñá‰ª∂Ôºå‰∏çËÉΩËµ∑‰ΩúÁî®ÔºåÂèØ‰ª•ËÆ©Áî®Êà∑‰øÆÊîπËøô‰∏™ËΩ¨Êç¢ËØçÂÖ∏ÂêóÔºüÊÄé‰πà‰∫ßÁîübinÊñá‰ª∂Ôºü

Ë∞¢Ë∞¢Ôºå‰Ω†ÁöÑÂàÜËØçÂ∑•ÂÖ∑ÂÅöÁöÑ‰∏çÈîô„ÄÇËøô‰∏™ÈóÆÈ¢òÔºåÂ∏åÊúõËÉΩ‰∏ÄËµ∑Â§ÑÁêÜÔºåË∞¢Ë∞¢„ÄÇ"
‰ΩøÁî®solrÊêúÁ¥¢ÂàÜËØçÂêéÁöÑÁªìÊûúÊêú‰∏çÂà∞ÔºåÈ∫ªÁÉ¶Ëß£Á≠î‰∏Ä‰∏ã„ÄÇ,"‰ΩøÁî®hanlpÂàÜËØç,Á¥¢ÂºïÂÜÖÂÆπÔºö‚Äú‰∏≠ÂåªËçØÂ§ßÂ≠¶ÈôÑÂ±ûÂåªÈô¢‚ÄùÔºåsolrÊµãËØïÂ≠óÊÆµÁî®ÁöÑÊòØ‚Äúhanlp‚Äù„ÄÇ
Êêúhanlp:""‰∏≠Âåª""ËÉΩÊêúÂà∞ÔºåÊêúhanlp:""‰∏≠ÂåªËçØ""Âç¥Êêú‰∏çÂà∞„ÄÇÂÖ∑‰ΩìÈóÆÈ¢òÊà™ÂõæÂú®ËøôÈáåÔºöhttps://www.oschina.net/question/145106_2239234
È∫ªÁÉ¶‰ΩúËÄÖÂ∏ÆÂøôÁúã‰∏ãÔºåÂ§öË∞¢ÔºÅ"
ÊúâÂÖ≥Âä®ÊÄÅÊ∑ªÂä†ÂçïËØçÁöÑÈóÆÈ¢ò,"hanksÔºå‰Ω†Â•ΩÔºå
Âú®ÁªèËøáÂä®ÊÄÅÊ∑ªÂä†ÂçïËØçÔºåÊàñËÄÖÂçïËØç+ËØçÊÄß
CustomDictionary.insert(word);
CustomDictionary.add(word);
‰πãÂêéÔºåÂ∫îËØ•ÈÄöËøá‰ªÄ‰πàÊñπÊ≥ïÊâçËÉΩËé∑ÂæóËØ•ËØçÁöÑÂéüÂàÜËØçÁªìÊûúÂíåËØçÊÄß
ÊØîÊñπËØ¥ÔºåÂº∫Âà∂Ê∑ªÂä†‰∫Ü‰∏Ä‰∏™Êñ∞ËØçCustomDictionary.insert(‚ÄúÁù°‰∏çÁùÄËßâ xx 1‚Äù);
‰πãÂêéÊÉ≥Ëé∑ÂæóËØ•ËØçÁöÑÂéüÊù•ÁöÑÂàÜËØçÁªìÊûú
[Áù°/v, ‰∏ç/d, ÁùÄ/uz, Ëßâ/v
ÂèØÊúâÊñπÊ≥ïÔºü"
ÊúâÂÖ≥CRFÂàÜËØçËØ≠ÊñôÁöÑÊ†ºÂºèÈóÆÈ¢òÔºü,"@hankcs Ôºå‰Ω†Â•Ω„ÄÇ
ÊàëÊÉ≥ÂÅö‰∏Ä‰∏™Ëá™Â∑±ÁöÑÂàÜËØçÊ®°ÂûãÔºåÊÉ≥Áü•ÈÅìÔºå‰Ω†ÂΩìÂàùËÆ≠ÁªÉmodel‰ΩøÁî®ÁöÑÊ®°ÂûãÊòØÂì™‰∏ÄÁßçÔºü
‰æã1Ôºö
Êâç d S
Âèë v B
Ëßâ v E
Â∑≤ d S
Ëø∑ v B
Â§± v E
‰∫Ü u S
Êù• n B
Ë∑Ø n E
„ÄÇ w S


‰æã2Ôºö
ÊçÆ	S
‰∫Ü	B
Ëß£	E
Ôºå	S
ÁõÆ	B
Ââç	E
ÁúÅ	S
ÂÜú	B
‰∏ö	M
ÂéÖ	E
ÊâÄ	B
Êúâ	E
Ë°å	B
Êîø	E


ÈùûÂ∏∏Â∏åÊúõÊÇ®ËÉΩÂ§üÂëäÁü•ÔºåË∞¢Ë∞¢ÔºÅ
"
ÂèëÁé∞‰∏Ä‰∏™ËØçÊÄßÂ§ÑÁêÜÁöÑÈóÆÈ¢ò ËØçÊÄß‰∏∫wÁöÑÂÜÖÂÆπ‰ºöÊúâ‰π±Á†ÅÔºåËøô‰∏™ÊòØ‰ªÄ‰πàÊÉÖÂÜµ,"Á§∫‰æãÔºö
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ÷±ÔøΩÔøΩÔøΩ«ªÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ⁄ΩÔøΩÔøΩœµÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ¬µƒΩÔøΩÔøΩÔøΩ»∑ µÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‹ªÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ∆§ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ«øÔøΩ“µƒ∏…µƒ∏ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ»∂»°ÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ»ªÔøΩÔøΩÔøΩ€£ÔøΩÔøΩÔøΩÔøΩÔøΩÃ´ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÃºÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ≥ÔøΩƒ∑ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÀ≥ÔøΩÔøΩœ∏ÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‘µƒ≥ÔøΩÔøΩÔøΩÔøΩ£øøÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‘£ÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ„ÄÄ„ÄÄ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ«≤ÔøΩÔøΩ«øÔøΩÔøΩ≈æÔøΩÔøΩÔøΩÔøΩ„µΩ“ªÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ≥ÔøΩÿ£ÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ“©ÔøΩ≈µƒ¥ÔøΩÔøΩÔøΩÔøΩ∆∑ÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ„ÄÄ„ÄÄ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÀ£ÔøΩ≈ÆÔøΩÀµÔøΩÔøΩÔøΩ—πÔøΩÔøΩÔøΩÔøΩ”≥ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ“ªÔøΩø¥øÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ¬πÔøΩ“∂ÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩŒ¢ÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ„ÄÄ„ÄÄ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ/ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ⁄ΩÔøΩÕ∑ÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ w 0
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‡£¨ÔøΩÔøΩ ≥ÔøΩÔøΩÔøΩƒªÔøΩÔøΩÔøΩ w 0


ËØ∑ÈóÆËøôÊ†∑ÁöÑÈóÆÈ¢òËØ•Â¶Ç‰ΩïÂ§ÑÁêÜÔºåË∞¢Ë∞¢ÔºÅ"
ÂÖ≥ÈîÆËØçÊèêÂèñ Â¶Ç‰ΩïËÆ©ÂÖ∂ËØÜÂà´Â§ñÂõΩ‰∫∫Âêç,"‰ª£Á†ÅÔºö
        List<String> keyWords = 
                        HanLP.extractKeyword(""‰∫®Âà©¬∑Á¶èÁâπÔºàHenryFordÔºå1863Âπ¥7Êúà30Êó•‚Äî1947Âπ¥4Êúà8Êó•ÔºâÔºåÁæéÂõΩÊ±ΩËΩ¶Â∑•Á®ãÂ∏à‰∏é‰ºÅ‰∏öÂÆ∂ÔºåÊ±ΩËΩ¶ÂÖ¨Âè∏ÁöÑÂª∫Á´ãËÄÖ„ÄÇ"",30);
        keyWords.forEach(System.out::println);

ÁªìÊûúÔºö
Ê±ΩËΩ¶ ÁæéÂõΩÊ±ΩËΩ¶Â∑•Á®ãÂ∏à ‰ºÅ‰∏öÂÆ∂ HenryFord ÂÖ¨Âè∏ Á¶èÁâπ Âª∫Á´ã

ËØ∑ÈóÆ Â¶ÇÊûúÊÉ≥ËÆ©ÂÖ≥ÈîÆËØç ÂêØÁî®‰∫∫ÂêçËØÜÂà´ ÊÄé‰πàÂêØÁî®Ôºü ‰∏ªË¶ÅÊòØÁõÆÁöÑ ‰∏çÊòØËÆ©ÂÖ∂Ëß£ÊûêÂá∫‚ÄúÁ¶èÁâπ‚Äù ËÄåÊòØÁî®ÂÖ≥ÈîÆËØçËß£ÊûêÂá∫‚Äú‰∫®Âà©¬∑Á¶èÁâπ‚Äù"
ÈÉΩËØØÊ†áÊàê‰∫∫Âêç   nr.txt‰∏≠L„ÄÅB„ÄÅDÂê´‰πâ,Áé∞Âú®ÈÅáÂà∞ÈóÆÈ¢òÔºöÈÉΩ‰π∞/nr    ÈÉΩÁ¢∞/nr  ÈÉΩÂ•ΩÂ§ß/nr  ÈÉΩÂíåË∞Å/nrÁ≠âÔºåÊÉ≥ÊîπÈÉΩ‰Ωú‰∏∫ÂßìÊ∞èÁöÑÈ¢ëÊï∞„ÄÇË∞ÅËÉΩÂëäËØâÊàënr.txt‰∏≠ÁöÑL„ÄÅB„ÄÅDÁ≠âÈÉΩË°®Á§∫‰ªÄ‰πàÊÑèÊÄù       ÈÉΩ L 587 B 308 D 32 C 16 E 6 K 3
‰øÆÊ≠£nodesÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†Â§ßÂ∞è‰∏∫0Êó∂ÈÄ†ÊàêÁöÑÂºÇÂ∏∏,
Áª¥ÁâπÊØîÁÆóÊ≥ïÊúâ‰∏ÄÂ§ÑBUG,"#### ViterbiSegment.java#148 Ë°å
`Vertex from = nodes[nodes.length - 1].getFirst();`
**ÊµãËØï‰ª£Á†ÅÔºö**

```java
System.out.println(new ViterbiSegment()
               .enableNameRecognize(true)
               .enableIndexMode(true)
               .enablePlaceRecognize(true)
               .enableOrganizationRecognize(true)
               .enableCustomDictionary(true).seg(""ÁôåÁóáÊ≠ª‰∫°Áéá‰∏éÂèëÁóÖÁéáÈÄêÂπ¥Â¢ûÈ´ò,Â∑≤Êàê‰∏∫‰∏ñÁïåËåÉÂõ¥ÂÜÖÁöÑ‰∏ÄÂ§ßÂÅ•Â∫∑ÈóÆÈ¢ò„ÄÇÂ§öË•ø‰ªñËµõ(docetaxel,DTX)ÊòØÁ¥´ÊùâÁÉ∑Á±ªÊäóÁôåËçØ,ÂÖ∂‰ΩúÁî®Êú∫Âà∂ÊòØÊäëÂà∂ÂæÆÁÆ°ËõãÁôΩËß£ËÅö,‰ªéËÄåÈòªÊñ≠ËÇøÁò§ÁªÜËÉûÂ¢ûÊÆñ„ÄÇÂ§öË•ø‰ªñËµõÊäóÁôåË∞±ËæÉÂπø,ÂèØ‰ª•Áî®Êù•Ê≤ªÁñóËÇ∫Áôå„ÄÅ‰π≥ËÖ∫Áôå„ÄÅÂâçÂàóËÖ∫Áôå„ÄÅÂçµÂ∑¢Áôå,Ê≤ªÁñóÊïàÊûúÊòØÁ¥´ÊùâÈÜáÁöÑ2-4ÂÄç„ÄÇ‰ΩÜÊòØÂÖ∂Ê∞¥Ê∫∂ÊÄßËæÉÂ∑Æ,‰∏¥Â∫äÊ≥®Â∞ÑÊ∂≤Â∫îÁî®ÂêêÊ∏©80Â¢ûÊ∫∂,ÊòìÂºïËµ∑ÊØíÂâØÂèçÂ∫îÔºõÂÖ∂ÂàÜÂ∏ÉÊó†ÁâπÂºÇÊÄß,ÊòìÂºïËµ∑ÂÖ®Ë∫´ÊØíÊÄß„ÄÇÊ≠§Â§ñ,Áî±‰∫éÁ¥´ÊùâÁÉ∑Á±ªËçØÁâ©ÁöÑÂ§ßÈáè‰ΩøÁî®,ÂØºËá¥ÁôåÁóáÁªÜËÉûÂØπÁ¥´ÊùâÁÉ∑Á±ªËçØÁâ©‰∫ßÁîü‰∫ÜÂ§öËçØËÄêËçØ(multi-drug resistance,MDR)ÊÄß,ËøôÊàê‰∏∫DTX‰ΩøÁî®‰∏≠ÁöÑÂè¶‰∏ÄÂ§ßÈóÆÈ¢ò„ÄÇÁôåÁóáÁªÜËÉû‰∫ßÁîüËÄêËçØÊÄßÁöÑ‰∏ªË¶ÅÂéüÂõ†ÊòØÁªÜËÉûËÜúËΩ¨ËøêËõãÁôΩp-glycoprotein(p-gp)ÁöÑËøáÈáèË°®Ëææ„ÄÇÁî±‰∫éÈ´òÂàÜÂ≠êÁßëÂ≠¶ÁöÑÂø´ÈÄüÂèëÂ±ï,Â∫îÁî®ËÅöÂêàÁâ©È´òÂàÜÂ≠ê‰Ωú‰∏∫ËçØÁâ©ËΩΩ‰ΩìÂåÖËΩΩÁñèÊ∞¥ÊÄßËçØÁâ©‰ª•Â¢ûÂä†ÁñèÊ∞¥ËçØÁâ©Ê∫∂Ëß£ÊÄßÂíåÊ≤ªÁñóÊïàÊûúÁöÑÁ†îÁ©∂Êàê‰∏∫ÁÉ≠ÁÇπ„ÄÇ‰∏§‰∫≤ÊÄßÂµåÊÆµËÅöÂêàÁâ©ÂèØ‰ª•Âú®Ê∞¥‰∏≠ËÅöÈõÜ‰∏∫ËÉ∂ÊùüÔºõËÉ∂ÊùüÂÖ∑Êúâ‰∫≤-ÁñèÊ∞¥Ê†∏Â£≥ÁªìÊûÑ,ÂÜÖÊ†∏ÁñèÊ∞¥ËÄåÂ§ñÂ£≥‰∫≤Ê∞¥,ÂÖ∂ÁñèÊ∞¥ÂÜÖÊ†∏ÂèØ‰ª•ÂåÖËΩΩÁñèÊ∞¥ÊÄßËçØÁâ©„ÄÇËÅöÂêàÁâ©ËÉ∂ÊùüÂèØ‰ª•ÈÄöËøáÁâ©ÁêÜÊÄßÂåÖÂüãÂíåÂåñÂ≠¶ÁªìÂêà‰∏§ÁßçÊñπÂºèÂåÖËΩΩËçØÁâ©„ÄÇÂåñÂ≠¶ÁªìÂêàËΩΩËçØÊòØÂ∞ÜËçØÁâ©ÂíåËÅöÂêàÁâ©ÂµåÊÆµÈÄöËøáÂåñÂ≠¶ÈîÆÂåñÂ≠¶ÁªìÂêàÂΩ¢ÊàêËÅöÂêàÁâ©-ËçØÁâ©ÁªìÂêàÁâ©(polymer-drug conjugate),ËçØÁâ©Êàê‰∏∫ËÅöÂêàÁâ©ÁöÑ‰∏ÄÈÉ®ÂàÜ,ÂèÇ‰∏éËÉ∂ÊùüÁöÑÁªÑË£Ö„ÄÇÊ≠§ÊñπÊ≥ïÂèØ‰ª•Â¢ûÂä†ËÉ∂ÊùüÁöÑËΩΩËçØÈáèÂíåÁ®≥ÂÆöÊÄßÔºõËøòÂèØÂà©Áî®ÁéØÂ¢ÉÊïèÊÑüÁöÑÂåñÂ≠¶ÈîÆÂÖ±‰ª∑ËøûÊé•ËçØÁâ©ÂíåËÅöÂêàÁâ©,Ëµ∑Âà∞Âú®ËÇøÁò§ÈÉ®‰ΩçÈõÜ‰∏≠ÈáäÊîæËçØÁâ©,ÊèêÈ´òËçØÁâ©ÁñóÊïàÁöÑ‰ΩúÁî®,Ëøô‰πüÊòØÊ≠§Á±ªËçØÁâ©‰º†ÈÄíÁ≥ªÁªüÁöÑ‰∏ÄÂ§ßÂàõÊñ∞ÁÇπ„ÄÇËÇøÁò§ÁªÑÁªáÂÜÖÂÖ∑ÊúâËæÉ‰ΩéÁöÑpHÂÄº,ÁâπÊÆäÈÖ∂ÂíåËæÉÂº∫ÁöÑËøòÂéüÊÄßÁéØÂ¢É,Âü∫‰∫éÊ≠§ÁâπÁÇπ,ÂèØ‰ª•ÈááÁî®pHÊïèÊÑü„ÄÅÈÖ∂ÊïèÊÑüÂíåÊ∞ßÂåñËøòÂéüÊïèÊÑüÁöÑÂåñÂ≠¶ÈîÆËøûÊé•ËçØÁâ©,‰ΩøËÅöÂêàÁâ©ÂÖ∑ÊúâÁéØÂ¢ÉÊïèÊÑüÊÄßË¥®„ÄÇÊú¨ËØæÈ¢òÊàêÂäüÂêàÊàê‰∫ÜÂÖ∑ÊúâÊ∞ßÂåñËøòÂéüÊïèÊÑüÁöÑËÅöÂêàÁâ©-ËçØÁâ©ÁªìÂêàÁâ©,‰ΩøÂÖ∂Âú®Ê∞¥‰∏≠Ëá™ÁªÑË£Ö,Âπ∂Ëøõ‰∏ÄÊ≠•Áî®‰∫éÂåÖËΩΩÂÖ∂‰ªñËçØÁâ©„ÄÇÈÄâÊã©‰∏§‰∫≤ÊÄßËÅöÂêàÁâ©Áî≤Ê∞ßÂü∫ËÅö‰πô‰∫åÈÜá-ËÅö‰∏ô‰∫§ÈÖØ‰πô‰∫§ÈÖØ(mPEG-PLGA,PP)‰Ωú‰∏∫Â§ßÂàÜÂ≠êÈ™®Êû∂,ÈÄöËøáÊ∞ßÂåñËøòÂéüÊïèÊÑüÁöÑ‰∫åÁ°´ÈîÆËøûÊé•DTX,ÂêàÊàêÊïèÊÑüÁöÑnPEG-PLGA-SS-DTX (PP-SS-DTX)ÁªìÂêàÁâ©„ÄÇÂà©Áî®Ê≠§ÁªìÂêàÁâ©Ëøõ‰∏ÄÊ≠•ÂåÖËΩΩÂ§öË•ø‰ªñËµõÊàñÁª¥ÊãâÂ∏ïÁ±≥(verapamil, VRP),ÂÆûÁé∞ËΩΩËçØ‰ΩìÁ≥ªÂäüËÉΩÂ§öÊ†∑Âåñ„ÄÇÊú¨ËØæÈ¢òÁ†îÁ©∂ÂåÖÊã¨‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö1„ÄÅPP-SS-DTXÁªìÂêàÁâ©ÁöÑÂêàÊàêÂíåË°®ÂæÅÔºöÈÄâÊã©Âê´Êúâ‰∫åÁ°´ÈîÆÁöÑ‰∫åÁ°´‰ª£‰∫å‰∏ôÈÖ∏(Âç≥DTDP)‰Ωú‰∏∫ËøûÊé•ÂàÜÂ≠ê,ÈÄöËøá‰∏âÊ≠•ÂåñÂ≠¶ÂèçÂ∫îÊàêÂäüÂêàÊàê‰∫ÜPP-SS-DTXÁªìÂêàÁâ©,Âπ∂ÈÄöËøáÁÜîÁÇπÊµãÂÆö„ÄÅÊ†∏Á£ÅÂÖ±ÊåØÊ∞¢Ë∞±(1H-NMR);ÂíåÂÇÖÁ´ãÂè∂ÂèòÊç¢Á∫¢Â§ñËâ≤Ë∞±(FT-IR)È™åËØÅÂÖ∂ÁªìÊûÑ„ÄÇÊ≠§Â§ñ,ÈÄöËøáËäòÊé¢ÈíàÊ≥ïÊµãÂÆöÂÖ∂‰∏¥ÁïåËÅöÈõÜÊµìÂ∫¶(critical aggregation concentration, CAC),ÂèëÁé∞PP-SS-DTXÁªìÂêàÁâ©ÁöÑCACÂÄºÂæàÂ∞è,‰∏∫10.20 Œºmol/L„ÄÇ2„ÄÅÊ∞ßÂåñËøòÂéüÊïèÊÑüÂûãÂèåÊ®°ÂºèËΩΩÂ§öË•ø‰ªñËµõPP-SS-DTX/DTXËÉ∂ÊùüÁ≥ªÁªüÁöÑËØÑ‰ª∑ÔºöÈÄöËøáÈÄèÊûêÊ≥ïÂà∂Â§áÂèåÊ®°ÂºèËΩΩÂ§öË•ø‰ªñËµõPP-SS-DTX/DTXËÉ∂Êùü,ÂèåÊ®°ÂºèÊòØÊåáÂêå‰∏Ä‰ΩìÁ≥ª‰∏≠ÈÄöËøáÂåñÂ≠¶ÁªìÂêàÂíåÁâ©ÁêÜÂåÖÂüã‰∏§ÁßçÊ®°ÂºèÂåÖËΩΩDTX„ÄÇÈÄöËøáTEMÂíåDLSÊµãÂÆöËÉ∂ÊùüÁöÑÂΩ¢ÊÄÅÂíåÁ≤íÂæÑ,ÊâÄÂà∂Â§áÁöÑPP-SS-DTX/DTXËÉ∂ÊùüÂëàÁêÉÂΩ¢,Á≤íÂæÑ‰∏∫112.3 nm;PP-SS-DTX/DTXËÉ∂ÊùüÂÖ∑ÊúâËæÉÈ´òÁöÑËΩΩËçØÈáè,‰∏∫(14.65¬±0.71)%„ÄÇÈÄöËøáÊ∫∂Ë°ÄËØïÈ™åÂàùÊ≠•Âà§Êñ≠PP-SS-DTX/DTXËÉ∂ÊùüÂÖ∑Êúâ‰∏ÄÂÆöÁîüÁâ©Áõ∏ÂÆπÊÄß„ÄÇ‰∏∫È™åËØÅPP-SS-DTX/DTXËÉ∂ÊùüÈáäËçØÊòØÂê¶ÂÖ∑ÊúâÊ∞ßÂåñËøòÂéüÊïèÊÑüÊÄß,Âú®ÈáäÊîæ‰ªãË¥®‰∏≠Âä†ÂÖ•ËøòÂéüÂûãÁâ©Ë¥®DTT„ÄÇÈáäÊîæÂÆûÈ™åÁªìÊûúË°®Êòé,‰∏éDTXÂéüÊñôËçØÊ∫∂Ê∂≤Áõ∏ÊØî,PP-SS-DTX/DTXËÉ∂ÊùüÂÖ∑ÊúâËçØÁâ©ÁºìÈáä„ÄÅÊ∞ßÂåñËøòÂéüÊïèÊÑüÊÄßÈáäËçØÂíåÁ®ãÂ∫èÊÄßÈáäËçØÁöÑÁâπÁÇπ„ÄÇÈÄâÊã©‰π≥ËÖ∫ÁôåMCF-7ÁªÜËÉûÁ≥ªÂíåÈªëËâ≤Á¥†Áò§B16F10ÁªÜËÉûÁ≥ªËøõË°å‰ΩìÂ§ñÁªÜËÉûÊØíÊÄßÂÆûÈ™å‰∏éÁªÜËÉûÊëÑÂèñÂÆûÈ™å„ÄÇÂú®‰∏§ÁßçÁªÜËÉûÁ≥ª‰∏≠,PP-SS-DTX/DTXËÉ∂ÊùüÁöÑÊØíÊÄß‰ΩúÁî®ÂùáÊòéÊòæÂº∫‰∫éDTXÊ∫∂Ê∂≤„ÄÇÁî±‰∫éDTX‰∏çËÉΩÂèëËçßÂÖâ,Âà©Áî®È¶ôË±ÜÁ¥†-6(coumarin-6, C-6)Ê®°ÊãüÁñèÊ∞¥ËçØÁâ©DTX,Âà∂Â§á‰∫ÜPP-SS-DTX/C-6ËÉ∂Êùü,ÈÄöËøáËçßÂÖâÂÄíÁΩÆÊòæÂæÆÈïúÊäÄÊúØÂíåÊµÅÂºèÁªÜËÉûÊúØÂÆöÊÄß„ÄÅÂÆöÈáèÁöÑÁ†îÁ©∂ÁªÜËÉûÂØπPP-SS-DTX/C-6ËÉ∂ÊùüÁöÑÊëÑÂèñ„ÄÇÁªìÊûúË°®Êòé,MCF-7ÂíåB16F10‰∏§ÁßçÁªÜËÉûÂØπPP-SS-DTX/C-6ËÉ∂ÊùüÁöÑÊëÑÂèñÊïàÁéáÈ´ò‰∫éC-6Ê∫∂Ê∂≤„ÄÇÁªÜËÉûÂØπËÉ∂ÊùüÂà∂ÂâÇÁöÑÈ´òÊëÑÂèñÊïàÁéá,‰øùËØÅ‰∫ÜËçØÁâ©ÊµìÈõÜ‰∫éÁóÖÂèòÁªÜËÉû,ÊòØÊèêÈ´òÊäóËÇøÁò§ÊïàÊûúÁöÑÈáçË¶ÅÂéüÂõ†„ÄÇ3„ÄÅPP-SS-DTX/VRPÂèåÊ®°ÂºèÂèåËΩΩËçØËÉ∂ÊùüÁ≥ªÁªüÁöÑÊäóËÇøÁò§Â§öËçØËÄêËçØÁöÑËØÑ‰ª∑ÔºöÈááÁî®Êé¢Â§¥Ë∂ÖÂ£∞Ê≥ïÂà∂Â§áPP-SS-DTX/VRPËÉ∂Êùü,ÂèåÊ®°ÂºèÊòØÊåáÂàÜÂà´ÈÄöËøáÂåñÂ≠¶ÁªìÂêàÂíåÁâ©ÁêÜÂåÖÂüãÊñπÂºèËΩΩËçØ,ÂèåËΩΩËçØÊòØÊåá‰∏Ä‰∏™ËÉ∂Êùü‰ΩìÁ≥ªÂêåÊó∂ÂåÖËΩΩ‰∏§ÁßçËçØÁâ©‚Äî‚ÄîÂ§öË•ø‰ªñËµõÂíåÁª¥ÊãâÂ∏ïÁ±≥„ÄÇÁª¥ÊãâÂ∏ïÁ±≥ÊòØÊúâÊïàÁöÑp-gpÁöÑÊãÆÊäóÂâÇ,ÂèØ‰∏ép-gpÁªìÂêàÂπ∂‰ΩøÂÖ∂Â§±Ê¥ª,ÈÄÜËΩ¨ÁôåÁóáÁªÜËÉûÁöÑÂ§öËçØËÄêËçØ„ÄÇÂú®Âà∂Â§áËÉ∂ÊùüËøáÁ®ã‰∏≠,ËÄÉÂØüÊé¢Â§¥Ë∂ÖÂ£∞ÁöÑË∂ÖÂ£∞Êó∂Èó¥ÂØπÁª¥ÊãâÂ∏ïÁ±≥ÁöÑÂåÖÂ∞ÅÁéá‰∏éËΩΩËçØÈáèÁöÑÂΩ±Âìç,ÂèëÁé∞Êé¢Â§¥Ë∂ÖÂ£∞ÁöÑÊó∂Èó¥Ë∂äÁü≠,Áª¥ÊãâÂ∏ïÁ±≥ÁöÑËΩΩËçØÈáèÂíåÂåÖÂ∞ÅÁéáË∂äÈ´ò„ÄÇÈÄöËøáTEMÂíåDLSÊµãÂÆöËÉ∂ÊùüÁöÑÂΩ¢ÊÄÅÂíåÁ≤íÂæÑ,PP-SS-DTXËÉ∂ÊùüÂíåPP-SS-DTX/VRPËÉ∂ÊùüÂùáÂëàÁêÉÂΩ¢,Á≤íÂæÑÂàÜÂà´‰∏∫(79.3¬±1.2)nmÂíå(78.3¬±4.6)nm„ÄÇPP-SS-DTX/VRPËÉ∂Êùü‰∏≠DTXËΩΩËçØÈáè‰∏∫(8.94¬±0.72)%,Áª¥ÊãâÂ∏ïÁ±≥ÁöÑËΩΩËçØÈáè‰∏∫(5.66¬±1.60)%,ÂåÖÂ∞ÅÁéá‰∏∫(53.49¬±10.96)%„ÄÇ‰∏∫È™åËØÅËÉ∂ÊùüÁöÑ‰ΩìÂ§ñÈáäÊîæÊòØÂê¶ÂÖ∑ÊúâÊïèÊÑüÊÄß,Âú®ÈáäÊîæ‰ªãË¥®‰∏≠Âä†ÂÖ•DTT,ÂÆûÈ™åÂèëÁé∞DTXÂíåVRPÁöÑÈáäÊîæÂùáÂÖ∑ÊúâÊ∞ßÂåñËøòÂéüÊïèÊÑüÊÄß„ÄÇ‰∏∫Á†îÁ©∂ËÉ∂ÊùüÁöÑÊäóËÇøÁò§Â§öËçØËÄêËçØÁöÑ‰ΩúÁî®,ÈÄâÊã©‰π≥ËÖ∫ÁôåMCF-7ÁªÜËÉûÂíåËÄêËçØÁöÑ‰π≥ËÖ∫ÁôåMCF-7/ADRÁªÜËÉû,ËøõË°åDTXÊ∫∂Ê∂≤„ÄÅPP-SS-DTXËÉ∂ÊùüÂíåPP-SS-DTX/VRPËÉ∂ÊùüÁöÑÁªÜËÉûÊØíÊÄßÂÆûÈ™å„ÄÅÁªÜËÉûÊëÑÂèñÂÆûÈ™åÂíåÁªÜËÉûÂáã‰∫°ÂÆûÈ™å„ÄÇÁªÜËÉûÊØíÊÄßÂÆûÈ™åË°®Êòé,PP-SS-DTXËÉ∂ÊùüÂíåPP-SS-DTX/VRPËÉ∂ÊùüÂØπ‰∏§ÁßçÁªÜËÉûÁ≥ªÁöÑÁªÜËÉûÊØíÊÄß‰ΩúÁî®ÂùáÂ•Ω‰∫éDTXÊ∫∂Ê∂≤ÔºõÊ≠§Â§ñ,PP-SS-DTX/VRPËÉ∂ÊùüÂØπMCF-7/ADRÁªÜËÉûÁöÑÁªÜËÉûÊØíÊÄß‰ΩúÁî®Âº∫‰∫éPP-SS-DTXËÉ∂Êùü„ÄÇÁî±‰∫éDTXÂíåVRP‰∏çËÉΩÂèëËçßÂÖâ,Âõ†Ê≠§ÈÄâÊã©p-gpÂ∫ïÁâ©ÁΩó‰∏πÊòé123(rhodamine 123,RH 123)‰Ωú‰∏∫ËçßÂÖâÂàÜÂ≠êÂØπËÉ∂ÊùüËøõË°åÊ†áËÆ∞Á†îÁ©∂ÁªÜËÉûÊëÑÂèñÊÉÖÂÜµÂíåp-gpÊäëÂà∂ÊïàÊûú„ÄÇÁªìÊûúË°®Êòé,MCF-7ÂØπPP-SS-DTXËÉ∂ÊùüÂíåPP-SS-DTX/VRPËÉ∂ÊùüÁöÑÁªÜËÉûÊëÑÂèñÊïàÁéáÁõ∏‰ºº,‰∏îÂùáÂ§ß‰∫éRH 123Ê∫∂Ê∂≤ÔºõMCF-7/ADRÁªÜËÉûÂØπRH 123Âá†‰πéÊó†ÊëÑÂèñ,ÂØπPP-SS-DTX/VRPËÉ∂ÊùüÁöÑÊëÑÂèñÂ§ß‰∫éÂØπPP-SS-DTXËÉ∂ÊùüÁöÑÊëÑÂèñ„ÄÇÁªÜËÉûÂáã‰∫°ÂÆûÈ™åË°®Êòé,PP-SS-DTXËÉ∂ÊùüÂíåPP-SS-DTX/VRPËÉ∂ÊùüÂüπÂÖªÁöÑÁªÜËÉûÂáã‰∫°Êï∞ÁõÆÂ§ö‰∫éDTXÊ∫∂Ê∂≤ÁªÑ„ÄÇ‰ª•‰∏äÂÆûÈ™åÁªìÊûúË°®Êòé,PP-SS-DTXÂåÖËΩΩVRPÂêé,ÂèØ‰ª•ÊäëÂà∂p-gpÁöÑ‰ΩúÁî®,Â¢ûÂº∫ËÄêËçØÁªÜËÉûÂØπËÉ∂ÊùüÁöÑÊëÑÂèñ,Âä†Âø´ÁªÜËÉûÂáã‰∫°ÁöÑËøõÁ®ã,‰ªéËÄåÊèêÈ´òÂ§öË•ø‰ªñËµõÁöÑÊäóËÇøÁò§ÊïàÂ∫î„ÄÇ4„ÄÅPP-SS-DTXËÉ∂Êùü‰ΩìÂÜÖËçØÂä®Â≠¶Á†îÁ©∂ÔºöÂÆûÈ™åÂä®Áâ©ÈÄâÊã©WistarÂ§ßÈº†,ÈÄöËøáÂ∞æÈùôËÑâÊ≥®Â∞ÑÁªôËçØ,Âà©Áî®HPLCÊ≥ïÊ£ÄÊµãÂ§ßÈº†Ë°ÄÊµÜ‰∏≠DTXÁöÑÂê´Èáè„ÄÇ‰ª•DTXÂéüÊñôËçØÊ∫∂Ê∂≤‰ΩúÂØπÁÖß,ËßÇÂØüPP-SS-DTXËÉ∂ÊùüÁöÑ‰ΩìÂÜÖËçØÂä®Â≠¶ËøáÁ®ã,ÂæóÂà∞Ë°ÄËçØÊµìÂ∫¶-Êó∂Èó¥Êõ≤Á∫ø,Âπ∂ÈÄöËøáDSA 2.0ËΩØ‰ª∂ÂØπË°ÄËçØÊµìÂ∫¶-Êó∂Èó¥Êõ≤Á∫øÊãüÂêà,ÂæóPP-SS-DTXËÉ∂ÊùüÊàøÂÆ§Ê®°ÂûãÂíåËçØÂä®Â≠¶ÂèÇÊï∞„ÄÇÂ∞ÜDTXÂà∂Â§áÊàêPP-SS-DTXËÉ∂ÊùüÂêé,Ë°ÄËçØÊµìÂ∫¶-Êó∂Èó¥Êõ≤Á∫øÂèòÂπ≥ÁºìÔºõÈÄöËøáÂØπÊõ≤Á∫øÊãüÂêà,ÂèëÁé∞DTXÊ∫∂Ê∂≤ÂíåPP-SS-DTXËÉ∂ÊùüÂùáÁ¨¶Âêà‰∫åÂÆ§Ê®°Âûã„ÄÇÂ∞ÜDTXÂà∂Â§áÊàêPP-SS-DTXËÉ∂ÊùüÂêé,‰ΩìÂÜÖÊ∏ÖÈô§ÁéáÈôç‰Ωé,Ê∂àÈô§ÂçäË°∞ÊúüÂíå‰ΩìÂÜÖÊªûÁïôÊó∂Èó¥Âª∂Èïø,ÊúâÂà©‰∫éËçØÁâ©ÁºìÈáäÂπ∂‰øùËØÅËçØÁâ©ÁñóÊïà„ÄÇÁªº‰∏äÊâÄËø∞,Êú¨Á†îÁ©∂È¶ñÊ¨°ÂêàÊàê‰∫ÜÂÖ∑ÊúâÊ∞ßÂåñËøòÂéüÊïèÊÑüÁöÑËÅöÂêàÁâ©-ËçØÁâ©ÁªìÂêàÁâ©PP-SS-DTX,Ê≠§ÁªìÂêàÁâ©ÂèØ‰ª•Ëá™ÁªÑË£Ö‰∏∫ËÉ∂Êùü,ÂåÖËΩΩDTXÊàñVRP,ÊèêÈ´òDTXÁöÑÊ∫∂Ëß£Â∫¶ÂíåÊäóËÇøÁò§ÊïàÊûú,Âπ∂ËÉΩÂ§üÈÄÜËΩ¨DTXÁöÑMDR„ÄÇÊú¨ËØæÈ¢òÁ†îÁ©∂ÂÖ∑ÊúâÈáçË¶ÅÁöÑÊÑè‰πâ,‰∏∫ÊèêÈ´òÁñèÊ∞¥ÊÄßËçØÁâ©Ê∫∂Ëß£Â∫¶„ÄÅÊèêÈ´òËçØÁâ©ÁñóÊïà„ÄÅËß£ÂÜ≥ËÇøÁò§ÁªÜËÉûÂ§öËçØËÄêËçØÁöÑÁ†îÁ©∂Êèê‰æõ‰∏ÄÂÆöÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇ""));
```

**Êä•Èîô‰ø°ÊÅØÂ¶Ç‰∏ãÔºö**
```
Exception in thread ""main"" java.util.NoSuchElementException
	at java.util.LinkedList.getFirst(LinkedList.java:242)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:148)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at TestMain.main(TestMain.java:11)
```

**ÂÖ∂‰ªñËØ¥ÊòéÔºö**
>‰∏ç‰ΩøÁî®HanLpËá™Â∏¶ÁöÑËá™ÂÆö‰πâËØçÂÖ∏‰∏ç‰ºöÊä•Èîô

**ÊîπËøõÂêéÁöÑ‰ª£Á†ÅÔºö**
```java
LinkedList<Vertex> node = null;
int index = 1;
while (node == null || node.size() <= 0){
     node = nodes[nodes.length - index];
     index++;
}
Vertex from = node.getFirst();
```"
HanLp ÊúâÊïèÊÑü‰ø°ÊÅØËØÜÂà´ÂäüËÉΩÔºü,
HanLp ÊúâÊïèÊÑü‰ø°ÊÅØËØÜÂà´ÂäüËÉΩ,
chartypeËØÜÂà´ËÉΩËá™ÂÆö‰πâ‰πàÔºü,Âú®ËøõË°åÂàÜËØçË∞ÉËØïËøáÁ®ã‰∏≠ÔºåÂ≠óÁ¨¶Á±ªÂûãÊòØCharType.dat.yes‰ªéÂä†ËΩΩÁöÑ„ÄÇËÉΩÂê¶Ëá™ÂÆö‰πâchartypeÁ±ªÂûãÂë¢ÔºüÂÆûÁé∞Êõ¥Á≤æÁªÜÁöÑÂ≠óÁ¨¶Á±ªÂûãÊéßÂà∂Ôºü
Ëá™ÂÆö‰πâËØçÂÖ∏ÈááÁî®AhoCorasickDoubleArrayTrieSegmentÂèëÁîü‰∫ÜÈáçÂ§çÂàÜËØçÁöÑÈóÆÈ¢ò,"hankcsÔºå‰Ω†Â•Ω„ÄÇ
Âè™Áî®‰∏Ä‰∏™Ëá™ÂÆö‰πâËØçÂÖ∏‚Äúmydict.txt‚ÄùÔºåÈáåÈù¢ÂÜÖÂÆπÊúâ‚ÄúÂåó‰∫¨‚Äù„ÄÅ‚ÄúÂåó‰∫¨Â∏Ç‚Äù„ÄÇÂØπ‚ÄúÂåó‰∫¨Â∏ÇÈïøÂÆâË°ó‚ÄùÁî®AhoCorasickDoubleArrayTrieSegmentÂàÜËØçÔºåÁªìÊûú‰∏∫„ÄêÂåó‰∫¨ÔºåÂåó‰∫¨Â∏Ç„ÄëÔºå‰∏çÁ¨¶ÂêàÊàëÁöÑÈ¢ÑÊúü„ÄÇ
ÊàëÂ∏åÊúõËææÂà∞ÁöÑÊïàÊûúÊòØËØçÂÖ∏‰∏≠‰∏Ä‰∏™ËØçÂåÖÂê´Âè¶‰∏Ä‰∏™ËØçÊó∂ÔºåÂè™ÈááÁî®ÊúÄÈïøÁöÑÈÇ£‰∏™ËØçÂåπÈÖçÔºåÂç≥ÂàÜËØçÁªìÊûú‰∏∫„ÄêÂåó‰∫¨Â∏Ç„Äë„ÄÇËØ∑ÈóÆÊàëÊç¢‰∏Ä‰∏™ÁÆóÊ≥ïËøòÊòØÊÄé‰πàÂÅöÔºüË∞¢Ë∞¢„ÄÇ"
Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Ê†á‰∏∫ntcÁöÑËØçÂàÜËØç‰ºöÊòæÁ§∫‰∏∫nt,"ÊØîÂ¶Ç‰æãÂè•""ÊïôÊéàÂú®ÊïôÊéàËØæÁ®ã""ÔºåÂ¶ÇÊûúÊàëÂú®Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ÂÖ•‚ÄúÊïôÊéàËØæÁ®ã ntc 1000‚ÄùÂàôÈªòËÆ§ÂàÜËØçHanLP.newsegmentËÉΩÂ§üÊ≠£Á°ÆÂàÜËØçÔºå‰ΩÜ‰ºöÂ∞Ü‚ÄúÊïôÊéàËØæÁ®ã‚ÄùÊ†á‰∏∫nt 
ÊïôÊéà/nnt,Âú®/p,ÊïôÊéàËØæÁ®ã/nt
ÁÑ∂ËÄåÊïôÊéàËØæÁ®ã‰∏çÊòØÂè™ÊúâËá™ÂÆö‰πâÁöÑntcËøô‰∏™ËØçË°åÂêóÔºü‰∏∫‰Ωï‰ºöËØÜÂà´‰∏∫nt
‰ΩÜÊòØÂ¶ÇÊûúËá™ÂÆö‰πâËØçÂÖ∏‰∏≠Â∞ÜÊïôÊéàËØæÁ®ãÊîπ‰∏∫ÂÖ®Êñ∞ÁöÑËØçÊÄßÔºåÊàñÊòØ‰∏Ä‰∫õËæÉ‰∏∫Â∏∏ËßÅÁöÑÔºåÂ¶Çn,aÔºåv‰πãÁ±ªÁöÑÂ∞±Ê≤°ÊúâÈóÆÈ¢ò
ÈùûÂ∏∏Âõ∞Êâ∞ÔºåÂú®Ê≠§ÂÖàÊÑüË∞¢Êãâ"
ÈÇÆÁÆ±ÂàÜËØç,"ÊµãËØïÁî®‰æã‰∏≠ÈááÁî®sunny800629@sina.comËøõË°åÂäüËÉΩÊµãËØïÔºåÂàÜËØçÁöÑÊïàÊûúÊòØËøôÊ†∑ÁöÑÔºö[0:5 1] sunni/nx
[5:11 1] 800629/m
[11:16 1] @sina/nx
[16:17 1] ./w
[17:20 1] com/nx
ËøôÈáåÈù¢ÁöÑ@sinaÔºåÊàëÊÉ≥ÊãÜÊàê@Âíåsina‰∏§‰∏™ËØç„ÄÇÁªèËøáË∞ÉËØïÂú®ÁîüÊàêÂõæÁΩëÁöÑÊó∂ÂÄôÂ∞±Â∑≤ÁªèÊòØËøô‰∏™Ê†∑Â≠ê‰∫Ü„ÄÇËØ∑ÈóÆÔºåÊúâ‰ªÄ‰πàËß£ÂÜ≥ÂäûÊ≥ï‰πàÔºü"
‰øÆÊ≠£‰∏Ä‰∏™ÊãºÈü≥,
hankcsÂ•ΩÔºåÊàëÁúãgithub‰∏äËØ¥Êúâ1.4ÁâàÊú¨ÁöÑËÆ°ÂàíÔºåËØ∑ÈóÆ1.4È¢ÑËÆ°‰ªÄ‰πàÊó∂Èó¥‰ºöÂá∫Âë¢ÔºüÂèØÊúâ‰∏Ä‰∏™Â§ßÊ¶ÇÂá∫ÁöÑÊó∂Èó¥Âå∫Èó¥Ôºü,RT
model.txtÂ¶Ç‰ΩïËΩ¨Êç¢‰∏∫model.txt.bin,"ÊÇ®Â•ΩÔºåÊàëÂú®[issue#45](https://github.com/hankcs/HanLP/issues/45)‰∏≠ÁúãÂà∞ÊÇ®ÊèêÂà∞Â∞ÜCRF++ËÆ≠ÁªÉÂá∫ÁöÑmodel.txtËΩ¨Êç¢Êàê‰∫Ümodel.txt.binÔºåÁÑ∂ÂêéÂ∞Ümodel.txt.binÊñá‰ª∂‰Ωú‰∏∫HanLPÁöÑCRFÂàÜËØçÊ®°ÂûãÔºåÊàëÊÉ≥ËØ∑ÈóÆ‰∏Ä‰∏ãËøô‰∏™ËΩ¨Êç¢ËøáÁ®ãÊòØÂ¶Ç‰ΩïÂÆåÊàêÁöÑÔºåÁõÆÁöÑÊòØ‰ªÄ‰πàÔºüËøòÊúõ‰∏çÂêùËµêÊïôÔºåË∞¢Ë∞¢ÔºÅ
Âè¶ÔºåÊàëÁöÑÁõÆÁöÑÊòØÊÉ≥Ë¶Å‰ΩøÁî®ÊàëËá™Â∑±Áî®CRF++ËÆ≠ÁªÉÂá∫Êù•ÁöÑÊ®°ÂûãÊõøÊç¢ÊÇ®HanLPÊèê‰æõÁöÑÊ®°ÂûãÊù•ÂÆöÂà∂CRFÊ®°Âûã„ÄÇ"
ËØ∑ÈóÆËøô‰∏™ÁöÑCRF ‰ΩøÁî®ÁöÑmodel ÂíåCRF++ ËÆ≠ÁªÉÂá∫Êù•ÁöÑmodelÈÄöÁî®ÂêóÔºü,"ÊàëÊåâÁÖßÊÇ®ÁöÑÊïôÁ®ãÔºåÁî®CRF++ËÆ≠ÁªÉÂá∫Ê®°ÂûãÔºåÁÑ∂ÂêéÊõøÊç¢ÊéâHanLPÁöÑCRFÊ®°ÂûãÔºåÊä•ÈîôÔºö
`Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space`

ÊâÄ‰ª•ÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºå‰ΩøÁî®CRF++ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®HanLP‰∏≠ÈÄöÁî®ÂêóÔºü"
Êâ©Â±ïËØÜÂà´Âô®,ÊúâÊ≤°ÊúâÊâ©Â±ïËØÜÂà´Âô®ÁöÑÊé•Âè£ÂÆûÁé∞ÂïäÔºüÊÉ≥ÂÆûÁé∞‰∏Ä‰∫õËá™ÂÆö‰πâÁöÑËØÜÂà´ÂÆûÁé∞„ÄÇ
HanLP Elasticsearch 5.x Êèí‰ª∂,"Êàë‰ª¨ÂÆûÁé∞‰∫Ü elasticsearch 5.x ÁöÑÊèí‰ª∂Ôºöhttps://github.com/hualongdata/hanlp-ext „ÄÇ
‰∏çËøáÁé∞Âú®ÈÅáÂà∞‰∏™Â∞èÈóÆÈ¢òÈúÄË¶ÅÊâãÂä®ËÆæÁΩÆ -Djava.security.policy Âíå ES_CLASSPATHÔºåÂ∏åÊúõÊúâ‰∫∫ËÉΩÂ∏ÆÊàëËß£ÂÜ≥„ÄÇ"
‰ΩøÁî®CustomDictionary.getËé∑ÂèñÊ†∏ÂøÉËØçÂÖ∏ÂçïËØç‰∏∫nullÔºåÂºÄÂêØË∞ÉËØïÊòæÁ§∫ËØçÂ∫ìÁöÜÂ∑≤Âä†ËΩΩ,"‰ª•‰∏ãÊØèÊ¨°ÊµãËØïÂâçÈÉΩÂà†Èô§‰∫ÜbinÊñá‰ª∂
‰ª•‰∏ãÊµãËØïÂùá‰ΩøÁî®Ê∫êÁ†ÅÂíåjarÂåÖÂØºÂÖ•‰∏§ÁßçÂΩ¢ÂºèÊµãËØïËøáÔºåÁªìÊûú‰∏ÄËá¥
ÊúõhankcsËß£ÊÉë

1.CustomDictionary.getÂáΩÊï∞Ëé∑ÂèñÊ†∏ÂøÉËØçÂÖ∏ËøîÂõû‰∏∫Null
ÁªìÊûú‰∏∫null
‰ΩÜHanLP.Config.enableDebug()ÂºÄÂêØË∞ÉËØïÂêéÔºåË∞ÉÁî®ÂàÜËØçÂáΩÊï∞HanLP.extractPhrase(""Êõ¥Â§öÁ≤æÂΩ©ËØ∑ÊêúÁ¥¢ÁéãËÄÖËç£ËÄÄ"",10);ÁªìÊûúÂèØ‰ª•ÁúãÂà∞ ÊêúÁ¥¢/vn
Ê≠§Êó∂Ë∞ÉÁî®CustomDictionary.get(‚ÄúÊêúÁ¥¢‚Äù)Ëé∑Âèñ‰ø°ÊÅØ‰ªçÁÑ∂ÊòØNull

2.Ëá™ÂÆöËØçÂ∫ìÊú™ËΩΩÂÖ•
HanLP.Config.enableDebug()ÂºÄÂêØË∞ÉËØïÂêéÊèêÁ§∫Âä†ËΩΩ‰∫ÜËá™ÂÆö‰πâËØçÂÖ∏ÔºåËØ•ËØçÂÖ∏Âè™Êúâ‰∏ÄÊù°ËÆ∞ÂΩïÔºàÁéãËÄÖËç£ËÄÄ nz 1ÔºâÔºå‰ΩÜÊòØË∞ÉÁî®ÂàÜËØçÂáΩÊï∞HanLP.extractPhrase(""Êõ¥Â§öÁ≤æÂΩ©ËØ∑ÊêúÁ¥¢ÁéãËÄÖËç£ËÄÄ"",10);ÁªìÊûúÂèØ‰ª•ÁúãÂà∞ ÁéãËÄÖ/n, Ëç£ËÄÄ/an.‰ΩøÁî®CustomDictionary.get(‚ÄúÁéãËÄÖËç£ËÄÄ‚Äù)ËøîÂõû‰∏∫null

3.‰øÆÊîπÂ∑≤ÊúâÁöÑËá™ÂÆö‰πâËØçÂÖ∏Êú™ÁîüÊïà
‰ΩøÁî®CustomDictionary.get(‚ÄúbË∂Ö‚Äù)Ëé∑ÂèñÂà∞‰∫Ü‚ÄúÁé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt‚Äù‰∏≠ÁöÑÁ¨¨‰∏ÄÊù°ËÆ∞ÂΩïÔºå‰∫éÊòØÊàëÂ∞Ü""ÁéãËÄÖËç£ËÄÄ nz 1""Ê∑ªÂä†Âú®Ëøô‰∏™ËØçÂÖ∏ÁöÑÂºÄÂ§¥ÔºàÂ§±Ë¥•ÂêéÂèàÂ∞ùËØïÊîæÂú®ÁªìÂ∞æÔºå‰ªçÁÑ∂Â§±Ë¥•Ôºâ,‰ΩøÁî®CustomDictionary.get(‚ÄúÁéãËÄÖËç£ËÄÄ‚Äù)ËøîÂõû‰∏∫nullÔºåCustomDictionary.get(‚ÄúbË∂Ö‚Äù)‰ªçÁÑ∂ÊúâËøîÂõûÂÄº„ÄÇ"
ÂÖ≥‰∫é‰∏ÄËØçÂ§öËØçÊÄßÁöÑËé∑ÂèñÈóÆÈ¢ò,"ÊàëËá™ÂÆö‰πâ‰∏§‰∏™Â≠óÂÖ∏Ôºå
test1.txt
Ê±ΩËΩ¶ qchy 1
test2.txt
Ê±ΩËΩ¶ kxp 1
ÊàëÊÉ≥ÈÄöËøá‚ÄúÊ±ΩËΩ¶‚ÄùËøô‰∏™ËØçËØ≠Ëé∑ÂæóÂà∞ qchy Âíå kxp ‰∏§‰∏™ËØçÊÄß„ÄÇÊàëÂú®‰ΩøÁî®CustomDictionary.getÔºà‚ÄúÊ±ΩËΩ¶‚Äù)ÔºåÂè™ËÉΩËé∑Âæó
kxp Ëøô‰∏™‰πà‰∏Ä‰∏™„ÄÇÊàëÁü•ÈÅìËøòÊúâ‰∏ÄÁßçÂ∞±ÊòØ ÊääËøô‰∏§‰∏™ËØçÊÄßÂÜôÂÖ•Âà∞Âêå‰∏ÄË°åÔºå‰ΩÜÊòØÂà∞Êó∂ÂÄôÂà§Êñ≠ËØçÊÄßÁöÑÊó∂ÂÄôÂ∞±Âæó‰ΩøÁî®indexOf(""kxp"")Á≠âËøôÁßçÊñπÂºèÈÅçÂéÜ‰∫Ü„ÄÇ
ËØ∑ÈóÆÊúâÊ≤°ÊúâÊõ¥Â•ΩÁöÑÂäûÊ≥ïËß£ÂÜ≥Ôºü
------  Â•îÊ∫É‰∫ÜÔºå‰∏ÄÁõ¥Êèê‰∫§‰∏ç‰∏äÂéªÔºå‰∏çÁü•ÈÅì‰ªÄ‰πàÊÉÖÂÜµÔºåÊâì‰∫ÜÂ•ΩÂá†ÈÅç-------"
‰∏çËß£ÔºöÂÅúÁî®ËØçËØçÂÖ∏‰∏≠‰∏∫‰ªÄ‰πàÊúâÂæàÂ§öÈáçÂ§çÁöÑËØç,"since
since
since
sincere
six
six
sixty
so
so
some
some
somehow
somehow
someone
something
sometime
sometimes
somewhere
still
still
such
such
system
take
ten
ten
than
that
that
that
that
that
the
the
the
their
their
their"
ÂçáÁ∫ßÊúÄÊñ∞ÁâàÊú¨ÂêéËßíËâ≤Ê†áÊ≥®ËØÜÂà´ÁöÑÊú∫ÊûÑÂêçÁß∞ÊÄé‰πàÂÖ®ÈÉ®ÂåπÈÖç‰∫Ü,"ÊàëÁé∞Âú®Âè™ÊòØÊÉ≥ËØÜÂà´Âá∫‰∏Ä‰∏™Êú∫ÊûÑÂêçÁß∞ÔºöÈôïË•øÁúÅÊ∏≠ÂçóÂ∏ÇËí≤ÂüéËøéÂÆæÂïÜÂüéÊúâÈôêÂÖ¨Âè∏
![image](https://cloud.githubusercontent.com/assets/20895017/24889354/13b04210-1e9c-11e7-891f-339a38b8a62a.png)
‰πãÂâçÊòØÊ†πÊçÆÈïøÂ∫¶‰ºòÂÖàÂåπÈÖçÔºåÊòæÁ§∫ÁöÑÁªìÊûúÂè™Êúâ‰∏Ä‰∏™ÔºåÁé∞Âú®ÁªìÊûúÊòØÂ§ö‰∏™ÔºåÈúÄË¶ÅÊÄé‰πàË∞ÉÊï¥Âè™ËÆ©ÂÆÉÊòæÁ§∫ÁöÑÁªìÊûúÂè™Êúâ‰∏Ä‰∏™
ÊúÄÂêéÊòæÁ§∫ÁöÑÁªìÊûú[ÈôïË•øÁúÅ/ns, Ê∏≠ÂçóÂ∏ÇËí≤ÂüéËøéÂÆæÂïÜÂüéÊúâÈôêÂÖ¨Âè∏/nt]  ‰πãÂâçÊúÄÁªàÁöÑÁªìÊûúÊòØ[ÈôïË•øÁúÅÊ∏≠ÂçóÂ∏ÇËí≤ÂüéËøéÂÆæÂïÜÂüéÊúâÈôêÂÖ¨Âè∏/nt]"
ÂÖ≥‰∫éÁî®CRFËøõË°åËØçÊÄßÊ†áÊ≥®ÁöÑÂá†‰∏™ÈóÆÈ¢ò,"Âú®‰ΩøÁî®CRF++Êó∂ÔºåÂèëÁé∞exampleÁöÑ‰æãÂ≠êÊòØÊúâ3ÂàóÁöÑÔºå‰∏≠Èó¥ÂàóÊòØËØçÊÄß
ÊàëÂÅö‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑËØ≠Êñô
                ""ÊîæÂë®Êù∞‰º¶"",
                ""ÊîæÂë®Êù∞‰º¶ÁöÑÊ≠å"",
                ""ÊîæÁîµÂè∞"",
                ""Êí≠ÁîµÂè∞"",
                ""Êí≠ÁîµÂè∞Âåó‰∫¨ÊÄÄÊóßÈáëÊõ≤"",
                ""Âê¨ÁîµÂè∞"",
                ""Ë¶ÅÂê¨ÁîµÂè∞"",
                ""ÊàëÊÉ≥Âê¨ÁîµÂè∞"",
                ""ÊàëË¶ÅÂê¨ÁîµÂè∞"",
                ""ÊîæÊúâÂ£∞‰π¶"",
                ""ÊàëÊÉ≥Âê¨Áõ∏Â£∞"",
                ""Êí≠ÈºìÊõ≤"",
                ""ÊàëÊÉ≥Âê¨ÈºìÊõ≤"",
                ""ÊîæË∂äÂâß"",
                ""Êí≠Ë∂äÂâß"",
                ""Êî∂Âê¨ÂøÉÂä®FM"",
                ""Êî∂Âê¨Ê∑±Âú≥Âä®Âê¨102"",

Êîæ      v       S
Âë®      nt      B
Êù∞      nt      M
‰º¶      nt      E
Êîæ      v       S
Âë®      ns      B
Êù∞      ns      M
‰º¶      ns      E
ÁöÑ      ude1    S
Ê≠å      n       S
Âè∞      n       E
Êí≠      v       S
Áîµ      a       B
Âè∞      n       E
Âê¨      v       S
Áîµ      a       B
Âè∞      n       E
Êàë      rr      S
ÊÉ≥      v       S
Âê¨      v       S
Áîµ      a       B
...

**Q1: ‚ÄúÂë®Êù∞‰º¶‚ÄùÊòØ‰∏ÄÈ¶ñÊ≠åÊõ≤ËøòÊòØ‰∏Ä‰∏™‰∫∫Âêç**
ÂÖ∂‰∏≠ ‚ÄúÊîæÂë®Êù∞‰º¶‚ÄùËØçÊÄßÊ†áÊ≥®‰∏∫ ‚ÄúÊîæ/v, Âë®/nt,  Êù∞/nt, ‰º¶/nt‚Äù
ÂÖ∂‰∏≠ ‚ÄúÊîæÂë®Êù∞‰º¶ÁöÑÊ≠å‚ÄùËØçÊÄßÊ†áÊ≥®‰∏∫ ‚ÄúÊîæ/v, Âë®/ns,  Êù∞/ns, ‰º¶/ns, ÁöÑ/ude1, Ê≠å/n‚Äù
ËøôÈáåÁöÑÂë®Êù∞‰º¶ÂàÜÂà´‰∏∫nt,nsÂ±ûÊÄßÔºåntÂíånsÂè™ÊòØÁî®Êù•ÊµãËØïËÄåÂ∑≤ÔºåÂÖ∂ÁõÆÁöÑÊòØËÉΩÊ†πÊçÆ‰∏ä‰∏ãÊñá‰ª∂Á°ÆÂÆö‚ÄúÂë®Êù∞‰º¶‚ÄùÊòØ‰∏ÄÈ¶ñÊ≠åÊõ≤ËøòÊòØ‰∏Ä‰∏™‰∫∫Âêç„ÄÇ

**QA:‚ÄúÂë®Êù∞‰º¶‚ÄùË¢´ÊãÜÂàÜÊàê‚ÄúÂë® Êù∞ ‰º¶‚Äù‰∏â‰∏™Â≠óÂêéÔºåÊØè‰∏™Â≠óÂ∫îËØ•Ê†áÊ≥®‰ªÄ‰πàËØçÊÄßÔºü**
Âë®Êù∞‰º¶Êú¨Ë∫´ÊòØ‰∏Ä‰∏™‰∫∫ÂêçÔºåÊãÜÂàÜÊàê‚ÄúÂë® Êù∞ ‰º¶‚Äù‰∏â‰∏™Â≠óÂêéÔºåÂ∫îËØ•Â¶Ç‰ΩïÂØπÊØè‰∏™Â≠óËøõË°åËØçÊÄßÊ†áÊ≥®Âë¢Ôºü

**Q3:Âú®hanlpÂú®ÔºåÈÄöËøáCRFËøõË°åËØçÊÄßÊ†áÊ≥®‰ºö‰∏ç‰ºöÁîüÊïàÔºü**
ÊàëÂèëÁé∞hanlpÁöÑËØçÊÄßÊ†áÊ≥®ÊòØÈÄöËøáËá™ÂÆö‰πâÂ≠óÂÖ∏ÂÆûÁé∞ÁöÑÔºåCRFÊú¨Ë∫´Âè™ÊòØÂàÜËØçÔºåÂπ∂Ê≤°ÊúâËøõË°åËØçÊÄßÊ†áÊ≥®„ÄÇ
1ÔºâÂ¶ÇÊûúÊàëÂØπCRFÁöÑÊ®°ÂûãÊú¨Ë∫´Â∑≤ÁªèÂä†ÂÖ•ËØçÊÄß‰∏ÄÂàóËøõË°åËÆ≠ÁªÉÔºåÈÇ£ÈÄöËøáhanlpÁöÑCRFËÉΩÂê¶Ê†áÊ≥®Âá∫ËØçÊÄßÔºåËØï‰∫Ü‰∏Ä‰∏ãÔºåÂèëÁé∞ÊòØÊ≤°ÊúâÁöÑÔºåÁî®ÁöÑÊòØ‰∏äÈù¢ÁöÑ‰æãÂ≠êÔºåÊâÄÊúâÂ≠óËØçËØçÊÄßÈÉΩÊòØnull
2ÔºâÊàëÁêÜËß£CRFÊú¨Ë∫´ÊòØ‰∏çÈúÄË¶ÅËØçÂÖ∏ÁöÑÔºàÊàñËÄÖËØ¥ËØçÂÖ∏Êú¨Ë∫´Â∞±Âú®ËÆ≠ÁªÉÁöÑËØ≠Êñô‰∏≠Ôºâ,ËÄåhanlp‰∏≠ÁöÑCRF‰ºº‰πé‰ΩøÁî®‰∫ÜcoreÂ≠óÂÖ∏„ÄÇ
3ÔºâhanlpÁöÑCRFÂàÜËØçÂô®Ê≤°Êúâ‰ΩøÁî®ÂÆû‰ΩìËØÜÂà´ÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢Ôºü"
‰ΩÜÊòØËã±ÊñáËøòÊúâ‰∏Ä‰∏™ËØçÊ†πËøòÂéüÁöÑÂäüËÉΩÔºåÂ∞±ÊòØwentËÉΩËØÜÂà´Âá∫go„ÄÇËøôÁßçÊÉÖÂÜµhanlpÊúâ‰πà,
Python Â¶Ç‰ΩïË∞ÉÁî®ËØçÂÖ±Áé∞ÁªüËÆ°Ôºü,"ÂØπ java ‰∏çÁîöÁÜüÊÇâÔºåÂèÇËÄÉ https://github.com/hankcs/HanLP/blob/master/src/test/java/com/hankcs/demo/DemoOccurrence.java
ÊäòËÖæ‰∏ÄÊÆµÊó∂Èó¥‰πãÂêéÊó†Êûú„ÄÇ„ÄÇ
Ê±ÇÊåáÊïô :)"
pythonÂÆûÁé∞Âú∞ÁêÜÂëΩÂêçÂÆû‰ΩìËØÜÂà´,ÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÂØπ‰∫éÂú∞ÁêÜÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºåÊàëÊÄé‰πàÊûÑÂª∫Ëá™Â∑±ÁöÑÂàÜËØçËØçÂÖ∏„ÄÇ„ÄÇ„ÄÇÊàëÂÖàÁî®pythonÊù•ÂÆûÁé∞Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ„ÄÇ
Âêå‰πâËØçÊòØÁî®‰ªÄ‰πàÁÆóÊ≥ïÔºü,"‰Ω†Â•ΩÔºåhanksÔºÅ
ÊàëÂèëÁé∞‰Ω†ÁöÑÂêå‰πâËØçÂáÜÁ°ÆÁéáÊå∫È´òÁöÑÔºåÊÉ≥Áü•ÈÅì‰Ω†ÊòØÁî®‰ªÄ‰πàÁÆóÊ≥ïÔºåÂ¶ÇÊûúËÉΩÂ§üÊúâËØ•ÁÆóÊ≥ïÁöÑËÆ∫ÊñáÂ∞±Êõ¥Â•Ω‰∫ÜÔºåÂ§™ÊÑüË∞¢ÔºÅ"
ËÉΩÂê¶ÂØπËã±ÊñáÊñáÁåÆËøõË°åÂ§ÑÁêÜÂêóÔºü,ÊÉ≥ÂØπËã±ÊñáÂÜÖÂÆπËøõË°åÊëòË¶ÅÔºåÂÖ≥ÈîÆËØçÊü•ÊâæÔºåËØÑÂàÜÁ≠âÂäüËÉΩ
DoubleArrayTrieÁöÑÈîôËØØ,"debug DemoNLPSegmentÊó∂ÔºåÂèëÁé∞ÂæóÂà∞‚ÄúÂßã##Âßã‚ÄùÁöÑwordIDÊòØ42996ÔºåÊòØÊ≠£Á°ÆÁöÑ„ÄÇ‰ΩÜÊòØ""Êú´##Êú´""ÁöÑwordIDÊòØ75262ÔºåÊòØÈîôËØØÁöÑ„ÄÇÊ≠£Á°ÆÁöÑIDÂ∫îËØ•ÊòØ75261„ÄÇËøôÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü"
Â¶Ç‰Ωï‰ΩøÁî®Ëá™ÂÆö‰πâÂÅúÁî®ËØçÔºåÂéªÈô§ÈªòËÆ§ÁöÑÂÅúÁî®ËØç,"ÈúÄË¶ÅÂØπ‰∏Ä‰∫õÊó†ÊÑè‰πâÁöÑËØçËøõË°åÂÅúÁî®Ôºå‰ΩÜÊòØÂèëÁé∞Â¶ÇÊûú‰ΩøÁî®ÈªòËÆ§ÁöÑÂÅúÁî®ËØçÂ∞±‰ºöÊääÊàëÂÖ≥Ê≥®ÁöÑ‰∏Ä‰∫õ‰ø°ÊÅØ‰πüËøáÊª§Êéâ‰∫ÜÔºõ
ÊàëÁî®ÊòØÔºöNotionalTokenizer.segment(sentence)Ôºõ"
Dependency viewer,ÂèØ‰ª•Áªô‰∏Ä‰∏™Dependency viewer‰∏ãËΩΩÈìæÊé•ÂêóÔºüË∞¢Ë∞¢Ê•º‰∏ª‰∫Ü„ÄÇ
support custom dictionary with csv,
„ÄäËá™Âä®ÊëòË¶Å„ÄãÊñáÁ´†Â≠óÊï∞ËææÂà∞‰∏ÄÂÆöÈáèÂØºËá¥ÂÜÖÂ≠òÊ∫¢Âá∫," java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:74)
    	at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:253)
    	at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:484)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.getTextRankSummary(AutoSummaryUtil.java:299)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.truncExcerptContent(AutoSummaryUtil.java:133)

ÊàëÁî®‰∏Ä‰∏ÄÁØá ÊñáÁ´† Â§ßÊ¶ÇÊúâ16wÂ≠óÂ∑¶Âè≥Ôºådebug Áúã‰∫Ü ÈÇ£‰∏™ ÂàáÂâ≤ÂêéÁöÑÂè•Â≠êÁöÑ sentences size Êúâ1w‰ª•‰∏äÔºåÁÑ∂Âêé Âú®Ë∞ÉÁî® ÂàÜËØçÊéíÂ∫èÁÆóÊ≥ïÊó∂ TextRankSentence textRank = new TextRankSentence(docs);Â∞±ÊäõÂá∫ÂÜÖÂ≠òÊ∫¢Âá∫ÔºåÊ±ÇÂ§ßÁ•û Â∏ÆÂ∏ÆËß£Èáä‰∏ãÔºåËøô‰∏™Â§ßÊ¶ÇÊ∂àËÄóÂÜÖÂ≠ò ÊòØÂ§öÂ∞ëÔºåÂ§ÑÁêÜÁöÑÈáèËåÉÂõ¥ÔºõË∞¢Ë∞¢

public TextRankSentence(List<List<String>> docs)
    {
        this.docs = docs;
        bm25 = new BM25(docs);
        D = docs.size();
        weight = new double[D][D];
        weight_sum = new double[D];
        vertex = new double[D];
        top = new TreeMap<Double, Integer>(Collections.reverseOrder());
        solve();
    `}```
weight = new double[D][D]; Ëøô‰∏™ÂΩì  docs.size();Ë∂ÖËøá1wÊòØÂ∞±Êä•ÂÜÖÂ≠òÊ∫¢Âá∫Ôºõ"
‚ÄúÂê¨ÁîµÂè∞Âåó‰∫¨ÊÄÄÊóßÈáëÊõ≤‚ÄùVS‚ÄúÂê¨ÁîµÂè∞Âåó‰∫¨Èü≥‰πêÈ¢ëÈÅì‚Äù,"Ë∞ÉÁî®Segment segment = new NShortSegment().enableCustomDictionary(true);

1. **Âê¨ÁîµÂè∞Âåó‰∫¨ÊÄÄÊóßÈáëÊõ≤-->[Âê¨/v, Áîµ/n, Âè∞Âåó/ns, ‰∫¨/b, ÊÄÄÊóß/vn, ÈáëÊõ≤/nz]**
ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.60, word:Âßã##Âßã@Âê¨
to:  2, from:  1, weight:05.49, word:Âê¨@Áîµ
to:  3, from:  1, weight:10.85, word:Âê¨@ÁîµÂè∞
to:  4, from:  2, weight:10.35, word:Áîµ@Âè∞
to:  5, from:  2, weight:02.90, word:Áîµ@Êú™##Âú∞
to:  6, from:  3, weight:11.44, word:ÁîµÂè∞@Âåó
to:  6, from:  4, weight:10.57, word:Âè∞@Âåó
to:  7, from:  3, weight:04.54, word:ÁîµÂè∞@Êú™##Âú∞
to:  7, from:  4, weight:05.39, word:Âè∞@Êú™##Âú∞
to:  8, from:  5, weight:04.28, word:Êú™##Âú∞@‰∫¨
to:  8, from:  6, weight:10.84, word:Âåó@‰∫¨
to:  9, from:  7, weight:07.56, word:Êú™##Âú∞@ÊÄÄ
to:  9, from:  8, weight:11.18, word:‰∫¨@ÊÄÄ
to: 10, from:  7, weight:09.53, word:Êú™##Âú∞@ÊÄÄÊóß
to: 10, from:  8, weight:11.18, word:‰∫¨@ÊÄÄÊóß
to: 11, from:  9, weight:11.50, word:ÊÄÄ@Êóß
to: 12, from: 10, weight:11.56, word:ÊÄÄÊóß@Èáë
to: 12, from: 11, weight:11.27, word:Êóß@Èáë
to: 13, from: 10, weight:04.24, word:ÊÄÄÊóß@ÈáëÊõ≤
to: 13, from: 11, weight:11.27, word:Êóß@ÈáëÊõ≤
to: 14, from: 12, weight:10.94, word:Èáë@Êõ≤
to: 15, from: 13, weight:11.58, word:ÈáëÊõ≤@Êú´##Êú´
to: 15, from: 14, weight:03.15, word:Êõ≤@Êú´##Êú´

Á≤óÂàÜÁªìÊûú[Âê¨/v, Áîµ/n, Âè∞Âåó/ns, ‰∫¨/b, ÊÄÄÊóß/vn, ÈáëÊõ≤/nz]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Âê¨ L 75 K 22 C 5 M 2 D 1 ][Áîµ E 20 K 19 C 14 L 6 D 3 ][Âè∞Âåó K 1 ][‰∫¨ C 305 E 274 D 97 B 24 ][ÊÄÄÊóß L 4 ][ÈáëÊõ≤ A 22202445 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Âê¨/K ,Áîµ/E ,Âè∞Âåó/K ,‰∫¨/B ,ÊÄÄÊóß/L ,ÈáëÊõ≤/A , /A]
Á≤óÂàÜÁªìÊûú[Âê¨/v, ÁîµÂè∞/nis, Âåó‰∫¨/ns, ÊÄÄÊóß/vn, ÈáëÊõ≤/nz]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Âê¨ L 75 K 22 C 5 M 2 D 1 ][ÁîµÂè∞ K 3 ][Âåó‰∫¨ K 37 L 28 ][ÊÄÄÊóß L 4 ][ÈáëÊõ≤ A 22202445 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Âê¨/K ,ÁîµÂè∞/K ,Âåó‰∫¨/K ,ÊÄÄÊóß/L ,ÈáëÊõ≤/A , /A]
[Âê¨/v, Áîµ/n, Âè∞Âåó/ns, ‰∫¨/b, ÊÄÄÊóß/vn, ÈáëÊõ≤/nz]

ÂÖ∂‰∏≠Êúâ‰∏Ä‰∫õËæπÊòØÊâæ‰∏çÂà∞ÁöÑÔºåËøô‰∏™Êï∞ÂÄºÂì™Êù•ÁöÑÂë¢
‚Äú ‚Äù@Âê¨ 181
Âê¨@Áîµ 12 
Âê¨@ÁîµÂè∞ 0
Áîµ@Âè∞ 0
Áîµ@Âè∞Âåó 354
ÁîµÂè∞@Âåó 0
ÁîµÂè∞@Âåó‰∫¨ 5 **Êü•‰∏çÂà∞Ëøô‰∏™Ë∑ØÂæÑ**
Âè∞@Âåó 0
Âè∞@Âåó‰∫¨ 21 **Êü•‰∏çÂà∞Ëøô‰∏™Ë∑ØÂæÑ**
Âè∞Âåó@‰∫¨ 8 **Êü•‰∏çÂà∞Ëøô‰∏™Ë∑ØÂæÑ**
Âåó@‰∫¨ 0
Âåó‰∫¨@ÊÄÄ 8  **Êü•‰∏çÂà∞Ëøô‰∏™Ë∑ØÂæÑ**
Âåó‰∫¨@ÊÄÄÊóß 0
‰∫¨@ÊÄÄÊóß 0
ÊÄÄ@Êóß 0
ÊÄÄÊóß@Èáë 0
ÊÄÄÊóß@ÈáëÊõ≤ 2
Êóß@ÈáëÊõ≤ 0
Èáë@Êõ≤ 0
ÈáëÊõ≤@‚Äú ‚Äù 0
Êõ≤@‚Äú ‚Äù 9


2. **Âê¨ÁîµÂè∞Âåó‰∫¨Èü≥‰πêÈ¢ëÈÅì-->[Âê¨/v, ÁîµÂè∞/nis, Âåó‰∫¨/ns, Èü≥‰πê/n, È¢ëÈÅì/n]**
ÊâìÂç∞ËØçÂõæÔºö========ÊåâÁªàÁÇπÊâìÂç∞========
to:  1, from:  0, weight:04.60, word:Âßã##Âßã@Âê¨
to:  2, from:  1, weight:05.49, word:Âê¨@Áîµ
to:  3, from:  1, weight:10.85, word:Âê¨@ÁîµÂè∞
to:  4, from:  2, weight:10.35, word:Áîµ@Âè∞
to:  5, from:  2, weight:02.90, word:Áîµ@Êú™##Âú∞
to:  6, from:  3, weight:11.44, word:ÁîµÂè∞@Âåó
to:  6, from:  4, weight:10.57, word:Âè∞@Âåó
to:  7, from:  3, weight:04.54, word:ÁîµÂè∞@Êú™##Âú∞
to:  7, from:  4, weight:05.39, word:Âè∞@Êú™##Âú∞
to:  8, from:  5, weight:04.28, word:Êú™##Âú∞@‰∫¨
to:  8, from:  6, weight:10.84, word:Âåó@‰∫¨
to:  9, from:  7, weight:05.53, word:Êú™##Âú∞@Èü≥
to:  9, from:  8, weight:11.18, word:‰∫¨@Èü≥
to: 10, from:  7, weight:05.12, word:Êú™##Âú∞@Èü≥‰πê
to: 10, from:  8, weight:11.18, word:‰∫¨@Èü≥‰πê
to: 11, from:  9, weight:11.52, word:Èü≥@‰πê
to: 12, from: 10, weight:11.02, word:Èü≥‰πê@È¢ë
to: 12, from: 11, weight:11.33, word:‰πê@È¢ë
to: 13, from: 10, weight:04.64, word:Èü≥‰πê@È¢ëÈÅì
to: 13, from: 11, weight:11.33, word:‰πê@È¢ëÈÅì
to: 14, from: 12, weight:11.46, word:È¢ë@ÈÅì
to: 15, from: 13, weight:05.94, word:È¢ëÈÅì@Êú´##Êú´
to: 15, from: 14, weight:04.92, word:ÈÅì@Êú´##Êú´

Á≤óÂàÜÁªìÊûú[Âê¨/v, ÁîµÂè∞/nis, Âåó‰∫¨/ns, Èü≥‰πê/n, È¢ëÈÅì/n]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Âê¨ L 75 K 22 C 5 M 2 D 1 ][ÁîµÂè∞ K 3 ][Âåó‰∫¨ K 37 L 28 ][Èü≥‰πê L 9 ][È¢ëÈÅì K 19 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Âê¨/K ,ÁîµÂè∞/K ,Âåó‰∫¨/K ,Èü≥‰πê/L ,È¢ëÈÅì/K , /A]
Á≤óÂàÜÁªìÊûú[Âê¨/v, Áîµ/n, Âè∞Âåó/ns, ‰∫¨/b, Èü≥‰πê/n, È¢ëÈÅì/n]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Âê¨ L 75 K 22 C 5 M 2 D 1 ][Áîµ E 20 K 19 C 14 L 6 D 3 ][Âè∞Âåó K 1 ][‰∫¨ C 305 E 274 D 97 B 24 ][Èü≥‰πê L 9 ][È¢ëÈÅì K 19 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Âê¨/K ,Áîµ/E ,Âè∞Âåó/K ,‰∫¨/B ,Èü≥‰πê/L ,È¢ëÈÅì/K , /A]
[Âê¨/v, ÁîµÂè∞/nis, Âåó‰∫¨/ns, Èü≥‰πê/n, È¢ëÈÅì/n]"
‰Ω†Â•ΩÔºåDoubleArrayTrieÂπ∂Ê≤°ÊúâÁúãÂà∞ÊúâÊ∑ªÂä†ÊàñÂà†Èô§Êüê‰∏™Â≠óËØçÁöÑÊé•Âè£,ÊØîÂ¶Ç‚ÄúÊàëË¶ÅÂê¨Èõ™Êº´È¢ëÈÅì‚ÄùÂ∞±Ë¢´ÂàÜÊàê‚ÄúÊàë Ë¶Å Âê¨Èõ™ Êº´ È¢ëÈÅì‚ÄùÔºåËÄåÊàëÂ∏åÊúõÁöÑÊòØÂàÜÊàê‚ÄúÊàë Ë¶Å Âê¨ Èõ™Êº´ È¢ëÈÅì‚Äù„ÄÇÂ¶ÇÊûúËÉΩÂä®ÊÄÅÂú∞‰ªéDoubleArrayTrieÁªìÊûÑÂä®ÊÄÅÂà†Èô§Êüê‰∏™Â≠óËØçÔºåÈÇ£ÊàëÂ∞±‰∏çÈúÄË¶ÅÈáçÂêØÊúçÂä°‰∫Ü
import junit.framework.TestCaseËøô‰∏™ÊòØ‰ªÄ‰πàÈîôËØØÔºåË¶ÅÊÄé‰πàËß£ÂÜ≥,"Description	Resource	Path	Location	Type
The import junit cannot be resolved	AdjustCorpus.java	/wenben/src/com/hankcs/test/corpus	line 28	Java Problem
"
Â¶Ç‰ΩïËÆ©Â§ö‰∏™tokenizerÂèòÈáèÊúâÂêÑËá™ÁöÑÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏CustomDictionary,"hiÔºåhankcsÔºåÊÑüË∞¢‰Ω†ÂØπ‰∏≠ÊñáNLPÈ¢ÜÂüüÂÅöÂá∫ÁöÑË¥°ÁåÆ„ÄÇ

Ê†πÊçÆÊàëËá™Â∑±ÁöÑÂú∫ÊôØÔºåËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºö
ÊØîÂ¶ÇÊúâ‰∏§‰∏™ÂàÜËØçÂèòÈáèÔºötokenizer1Âíåtokenizer2ÔºåÈúÄË¶ÅÂä®ÊÄÅÂ¢ûÂä†Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÂ¶Ç‰ΩïËÆ©‰∏§‰∏™ÂàÜËØçÂèòÈáèÊúâÂêÑËá™ÁöÑÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåËÄå‰∏ç‰∫íÁõ∏ÂΩ±ÂìçÂë¢Ôºü
‰æùÊçÆdemo‰∏≠Ë∞ÉÁî®CustomDictionary.add(""ÊîªÂüéÁãÆ"", ""nz 1024 n 1""); ÁÑ∂ÂêéÔºå‰æø‰ºöÂΩ±ÂìçÊâÄÊúâtokenizerÁöÑÂàÜËØçÁªìÊûú‰∫Ü„ÄÇ
ËØ∑ÈóÆÔºöÊòØÂê¶ÊúâÊñπÊ≥ïËÆ©Â§ö‰∏™ÂàÜËØçÂèòÈáèÔºåÊúâÂêÑËá™ÁöÑÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåËÄå‰∏ç‰∫íÁõ∏ÂΩ±ÂìçÂë¢Ôºü

Ë∞¢Ë∞¢ÔºÅ
"
‰∏∫‰ªÄ‰πà‰ª£Á†ÅÈÉΩÊòØ‰π±Á†Å,"

"
PythonË∞ÉÁî®CRFÂàÜËØç,"@hankcs  
ÁéØÂ¢ÉPython3Ôºå
ÂèÇËÄÉdemoÂêéÔºöhttps://github.com/hankcs/HanLP/tree/master/src/test/java/com/hankcs/demo 
CRF= JClass('com.hankcs.hanlp.seg.CRF.CRFSegment') 
#ÂºïÂÖ•ÊàêÂäüÔºå
#‰ΩÜÊòØ  
CRF.seg('""Â®ÅÂªâÁéãÂ≠êÂèëË°®ÊºîËØ¥ ÂëºÂêÅ‰øùÊä§ÈáéÁîüÂä®Áâ©\n""')
#Êä•ÈîôÔºö

Traceback (most recent call last):

  File ""<ipython-input-41-a8007c89d333>"", line 2, in <module>
    CRF.seg('""Â®ÅÂªâÁéãÂ≠êÂèëË°®ÊºîËØ¥ ÂëºÂêÅ‰øùÊä§ÈáéÁîüÂä®Áâ©\n""')

RuntimeError: No matching overloads found. at native\common\jp_method.cpp:121
 Ê±ÇÁ≠îÂ§çÔºåthanksÔºÅ"
root Âú∞ÂùÄÊîØÊåÅËÆæÁΩÆÊàê resources ‰∏≠ÁöÑÁõÆÂΩï,Áé∞Âú® root Âú∞ÂùÄÂè™ËÉΩËÆæÁΩÆÊàêÁ≥ªÁªüÂú∞ÂùÄÔºåÁé∞Âú®ÊàëÂ∏åÊúõÂ∞Ü data Áõ¥Êé•ÊâìÂåÖÂú® resource ‰∏≠ÔºåÊñπ‰æø‰ΩøÁî® docker ËøõË°åÈÉ®ÁΩ≤Ôºå‰∏çÁü•ÈÅìÁé∞Âú®Âè™ÊîØÊåÅ‰ªéÁ≥ªÁªüË∑ØÂæÑËØªÂèñÊòØÂê¶ÊúâÂà´ÁöÑËÄÉËôëÔºåÂ¶ÇÊûú‰∏çÊòØÔºåÊàëÊòØÂê¶ÂèØ‰ª•Ê∑ªÂä†Ëøô‰∏™ÂäüËÉΩÔºüÊØîÂ¶ÇÁî®resource:// ‰Ωú‰∏∫Ê†áËØÜ
DoubleArrayTrie Error ,"```
                DoubleArrayTrie<String> arrayTrie = new DoubleArrayTrie<String>();
		Map<String, String> v = new HashMap<>();
		v.put(""„Ää1,2,3,4„Äã"", ""model1"");
		v.put(""„Ää1,2,3"", ""model3"");
		v.put(""„Ää1,2"", ""model2"");
		v.put(""„Ää1,"", ""model5"");
                System.err.println(arrayTrie.exactMatchSearch(""„Ää1,""));

```
Error:
`
		p = b;
		int n = base[p];
		if (b == check[p] && n < 0) {
			result = -n - 1;
		}
`
ÈîôËØØË°®Á§∫ b == -1ÂÄº ÂèØÊòØ ÂèàËµãÂÄºÁªôp ÊâÄ‰ª•ÈÄ†ÊàêÊ∫¢Âá∫
"
Ê†∏ÂøÉËØçÂÖ∏Ëá™ÂÆö‰πâËØçÂÖ∏ÂêåÊó∂Âá∫Áé∞Êó∂ÁöÑËØçÊÄß‰ºòÂÖàÁ∫ßÈóÆÈ¢ò,"ÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÂÖ≥‰∫é‰ºòÂÖàÁ∫ßÁöÑÈóÆÈ¢òÔºåÊåâÁÖßÊñáÊ°£ÁöÑÊèèËø∞Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑ‰ºòÂÖàÁ∫ßÊòØÂÖ®Â±ÄÊúÄÈ´òÁöÑÔºü‰ΩÜÊòØÂú®Ê†∏ÂøÉËØçÂÖ∏‰∏≠Â≠òÂú®Êüê‰∏™ËØçÊó∂Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ÂÖ•ÂÖ∂‰ªñËØçÊÄßÂπ∂‰∏çËÉΩËØÜÂà´Âá∫Ëá™ÂÆö‰πâËØçÊÄßÔºåËØÜÂà´Âá∫ÁöÑËØçÊÄßÊòØÊ†∏ÂøÉËØçÂÖ∏‰∏≠ÂÆö‰πâÁöÑÊ¶ÇÁéáÊúÄÂ§ßÁöÑÔºàÂè™ÊµãËØï‰∫ÜÊ†áÂáÜÂàÜËØçÁöÑÊÉÖÂÜµÔºåÂêåÊó∂ÂºÄÂêØ‰∫ÜÈöêÈ©¨Â∞îÁßëÂ§´Âà§Êñ≠ËØçÊÄßÔºâÔºåÊ≠§Êó∂Â¶ÇÊûúÊääÊ†∏ÂøÉËØçÂÖ∏ÁöÑÈù¢Ëøô‰∏™ËØçÊù°Âà†ÊéâÂ∞±ÂèØ‰ª•ËØÜÂà´Âá∫Ëá™ÂÆö‰πâÁöÑËØçÊÄß„ÄÇÂ∫îËØ•‰∏çÊòØËΩ¨ÁßªÁü©ÈòµÂíåÈ¢ëÊï∞ÁöÑÈóÆÈ¢òÊµãËØïËøáÂêÑÁßçÁöÑËØçÊÄßÂíåÈ¢ëÊï∞Ôºå‰ºº‰πéÈÉΩÊòØÊ†∏ÂøÉËØçÂÖ∏‰ºòÂÖà„ÄÇ‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÁ®ãÂ∫èËÆæÂÆöÊú¨Ë∫´Â∞±ÊòØËøôÊ†∑Âë¢ÔºåÂ¶ÇÊûúÊòØÁöÑËØùÈô§‰∫ÜÂà†Èô§Ê†∏ÂøÉËØçÂÖ∏Áõ∏ÂÖ≥Êù°ÁõÆ‰∏çÁü•ÈÅìÊúâÊ≤°Êúâ‰ªÄ‰πàÂ•ΩÁöÑÂ§ÑÁêÜÊñπÊ≥ï„ÄÇÊàñËÄÖËøòÊòØÈöêÈ©¨ÁöÑ‰ΩúÁî®ËåÉÂõ¥Âè™ÈíàÂØπÊ†∏ÂøÉËØçÂÖ∏Ôºü
ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ"
Ë∞ÉÁî®crfÂàÜËØçÁöÑÊó∂ÂÄôÂá∫Áé∞ArrayIndexOutOfBoundsExceptionÈîôËØØ,"
ÈîôËØØÂ¶Ç‰∏ãÔºö
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:380)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:142)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at test.TegTest.main(TegTest.java:33)

ÊâæÂà∞ÊòØ‰∏ãÈù¢ËøôÂè•Âá∫Èîô
String str = ""‰ªÄ‰πàÊòØË∞∑Á≤æËçâ""; 
List<Term> seg = segment.seg(str);

Ë∞∑Á≤æËçâËøô‰∏™‰∏úË•øÊàëÈÄöËøáËá™ÂÆö‰πâËØçÂÖ∏ÂÆö‰πâ‰∏∫ÂÖ∂‰ªñÁöÑËØçËØ≠ÔºåÊ≠§Â§Ñ‰∏çÁü•ÈÅìÂá∫Áé∞‰∫Ü‰ªÄ‰πàÈîôËØØÔºåÂ∏åÊúõËÉΩÂ§üÂæóÂà∞Ëß£Á≠îÔºåË∞¢Ë∞¢"
ÂÖ≥‰∫éÂ¶Ç‰ΩïÂàÜÊûêÂè•Â≠ê‰∏îËÉΩÂ§üËá™ÂÆö‰πâÁöÑÂàÜÁ±ª,"Êó∂Èó¥ÊòØÂæàÂÆùË¥µÁöÑÔºåÂÄüÁî®Â§ßÂÆ∂ÂÆùË¥µÁöÑÊó∂Èó¥Â∏ÆÊàëËß£‰∏Ä‰∏ãÁñëÊÉëÔºåÊÑüÊøÄ‰∏çÂ∞Ω„ÄÇ
-----------ËøõÂÖ•Ê≠£È¢ò--------------
Â∞ÜË¶ÅÂàÜÊûêÂÜÖÂÆπÔºöB2B‰ºÅ‰∏öCMOÁöÑËÅåË¥£ÂåÖÊã¨‰∏§ÂùóÔºö‰∏Ä‰∏™ÊòØÂìÅÁâåÔºå‰∏Ä‰∏™ÊòØÈîÄÂîÆÁ∫øÁ¥¢„ÄÇËê•ÈîÄÂõ¢ÈòüÈáåËæπ‰∏ÄÂÆöÊòØÊúâ‰∏ÄÁæ§‰∫∫ÊòØÂÅöÂìÅÁâåÁöÑÔºåÂè¶‰∏ÄÊã®‰∫∫ÂÅöÈîÄÂîÆÁ∫øÁ¥¢„ÄÇÁÆÄÂçïËØ¥ÊòØ‰∏Ä‰∏™ÈÉ®Èó®ÊòØËä±Èí±ÁöÑÔºåÂè¶‰∏Ä‰∏™ÈÉ®Èó®ÊòØÂéªÊå£Èí±ÁöÑÔºåÂÅöÂìÅÁâåËôΩÁÑ∂‰∏çÁõ¥Êé•ÊãøÂçïÔºå‰ΩÜËÉΩÂ§üÁªôÈîÄÂîÆÂàõÈÄ†ÈùûÂ∏∏ÂùöÂÆûÁöÑÂü∫Á°Ä„ÄÇ
B2B‰ºÅ‰∏öÊúâ‰∏Ä‰∏™ÂæàÂ§ßÁöÑ‰ºòÂäøÔºåÂ∞±ÊòØÊúâÁùÄ‰∏∞ÂØåÂπ∂‰∏îÊòéÁ°ÆÁöÑË°å‰∏öÊ¥ûÂØüÔºåÊØîÂ¶ÇÊàë‰ª¨Ë¶ÅÂÅö‰∏Ä‰∏™ÁúüÊ≠£ËÆ©‰∏ñÁïåÂèòÂæóÊõ¥ÁæéÂ•ΩÁöÑÊú∫Âô®‰∫∫„ÄÅËá™Âä®ÂåñÔºåÂæóÂåÖÂê´Â§öÂ∞ëÊàë‰ª¨ÂØπËøô‰∏™Ë°å‰∏öÁöÑËßÅËß£Ôºü‰ΩÜÂÉèÂçñÁ¢≥ÈÖ∏È•ÆÊñô„ÄÅÂçñÂ∑ßÂÖãÂäõÁöÑËøô‰∫õ2C‰ºÅ‰∏öÔºåÂ∞±ÂæàÈöæÂéªË°®Áé∞Ëøô‰∫õ„ÄÇÊâÄ‰ª•‰Ω†ÁúãÈÇ£‰∫õÂ§ßÁöÑ2B‰ºÅ‰∏öÔºåÈÉΩÂú®‰∏çÈÅó‰ΩôÂäõÁöÑÂéªÊ†ëÁ´ãËá™Â∑±ÊÑèËßÅÈ¢ÜË¢ñÁöÑÂú∞‰Ωç„ÄÇ‰æãÂ¶ÇË•øÈó®Â≠ê„ÄÅIBM‰ª•ÂèäÂåÖÊã¨GEÁ≠âÂú®ÂÜÖ„ÄÇ

Â∏åÊúõÈÄöËøá‰∏äÈù¢ÊòØ‰∏ÄÁØáÊñáÁ´†ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂàÜÊûêÂá∫‰ª•‰∏ãÁªìÊûúÔºö
1ÔºâÁõ∏ÂÖ≥‰ºÅ‰∏öÔºöIBM,GE,Ë•øÈó®Â≠ê
2ÔºâÁõ∏ÂÖ≥Ë°å‰∏öÔºöÂà∂ÈÄ†‰∏ö
3ÔºâÂèó‰ºóÁ±ªÂà´Ôºö2B
4ÔºâÂèó‰ºóÁæ§‰ΩìÔºöÁÆ°ÁêÜ‰∫∫Âëò„ÄÅÈîÄÂîÆ‰∫∫Âëò

‰∏™‰∫∫ÊÄùË∑ØÔºö
ÊàëËá™Â∑±ÁöÑÊÄùË∑ØÊòØÔºöÈáåÈù¢Âá∫Áé∞‰∫Ü‚ÄùIBM,GE,Ë•øÈó®Â≠ê‚ÄúÁ≠âËØçÁÑ∂Âêé‚ÄùË•øÈó®Â≠ê‚ÄúÂèàÂèØ‰ª•ÂÖ≥ËÅî‰∏äÂà∂ÈÄ†‰∏öÔºåÊñáÁ´†‰∏≠Âá∫Áé∞B2BÂ≠óÊ†∑ÔºåÂèØ‰ª•ÂàÜÊûê‰∏∫2BÔºåÁÑ∂ÂêéÊúâCMO„ÄÅËê•ÈîÄÂõ¢ÈòüÂèØ‰ª•ÂØπÂ∫î‰∏äÁÆ°ÁêÜ‰∫∫ÂëòÂíåÈîÄÂîÆ‰∫∫Âëò„ÄÇ‰ΩÜÊòØÊàë‰∏çÂ§™ÊòéÁôΩÂ¶Ç‰ΩïÂ∞ÜËøô‰∫õÂÖ≥ËÅîÂà∞‰∏çÂêåÁöÑÊ†áÁ≠æ‰∏ã„ÄÇÊàëÁöÑÊÉ≥Ê≥ïÂèØËÉΩÊØîËæÉËÇ§ÊµÖÔºåËøòÊúõÊåáÁÇπ„ÄÇÂ¶ÇÊûúËÉΩÂ§üÊèê‰æõÊÄùË∑Ø‰ª£Á†ÅÊõ¥Â•Ω„ÄÇ
"
com.hankcs.hanlp.corpus.util.StringUtilsÊúâ‰∏™bug,"com.hankcs.hanlp.corpus.util.StringUtils.PATTERN Â∫îËØ•‰∏∫ ""&|[\uFE30-\uFFA0]|‚Äò|‚Äô|‚Äú|‚Äù""
ÂéüÊù•ÂÜôÈîô‰∫Ü"
solr6ÂÅúÊ≠¢ËØç,ËØ∑ÈóÆÂú®solr6.4ÈáåÈù¢ÔºåÊòØÂê¶ÊîØÊåÅ‰∏≠ÊñáÂÅúÊ≠¢ËØçÔºåÊàëÂØπÈªòËÆ§ÁöÑÂÅúÊ≠¢ËØ≠Â≠óÂÖ∏ÂÅö‰∫Ü‰øÆÊîπÂêéÔºåÂú®‰ΩøÁî®solrÂàÜÊûêÊó∂ÔºåÂÅúÊ≠¢ËØçÊó†Êïà„ÄÇ
hanlp.propertiesÂú®mavenÈ°πÁõÆË∑ØÂæÑÁöÑÈóÆÈ¢ò,"Âú®eclipse‰∏≠Âà©Áî®mavenÁöÑpom.xmlÂä†ÂÖ•‰∫Ühanlp
Êäähanlp.propertiesÊîæÂú®mavenÈ°πÁõÆ‰∏≠ÁöÑsrc‰∏ãÔºåÈÄöËøámavenÂëΩ‰ª§ÁºñËØëÈ°πÁõÆÔºåpropertiesÂπ∂Ê≤°ÊúâËá™Âä®Â§çÂà∂Âà∞classpath‰∏≠ÔºåÊØèÊ¨°ÈÉΩÂæóÊâãÂä®ÊääpropertiesÊîæÂà∞classpathÔºåÊúâÊ≤°Êúâ‰ªÄ‰πàËß£ÂÜ≥ÂäûÊ≥ïÈÇ£ÔºüÔºü"
CRFÂàÜËØçÊ®°Âûã Êï∞Â≠óÔºàmÔºâÂíå Ëã±ÊñáÂ≠óÁ¨¶ÔºàwÔºâ,ÁúãÂÆåÊÇ®ÁöÑCRFÊ∫êÁ†ÅÔºåÊúâ‰∏™ÁñëÊÉëÔºåÂ∞±ÊòØËÆ≠ÁªÉÁöÑÊó∂ÂÄôÔºåËØ∑ÈóÆÊÇ®ÊòØ‰∏çÊòØÂ§ÑÁêÜ‰∫Ü‰∏Ä‰∏ãÔºåÂ∞±ÊòØÊääËøûÁª≠ÁöÑÊï∞Â≠ó‰πüÊç¢Êàê‰∫ÜmËøõË°åËÆ≠ÁªÉÔºåËøûÁª≠ÁöÑËã±ÊñáÂ≠óÁ¨¶Êç¢ÊàêwËøõË°åËÆ≠ÁªÉ„ÄÇ
ËØ∑ÈóÆdata\model\dependency\WordNature.txt.binÊúâ‰ªãÁªç‰πàÔºü,"ËØ∑ÈóÆdata\model\dependency\WordNature.txt.binÊúâ‰ªãÁªç‰πàÔºü
ÊàëËØïÁî®crfÂ∑≤Â≠òÂè•Ê≥ïÂàÜÊûêÊó∂ÔºåËæìÂÖ•‚Äú‚ÄùÂÖöÂèÇÁöÑÂÆö‰πâÊòØ‰ªÄ‰πà‚Äú‚ÄùÔºåÊ≠§Â§Ñ‚ÄúÂÆö‰πâ‚ÄùÁöÑ‰æùÂ≠òÂÖ≥Á≥ª‰∏∫‚ÄúÂéüÂ§ÑÊâÄ‚ÄùÔºü
ÊïÖËÄåÂ∏åÊúõÊÇ®ËÉΩÁªôÂá∫‰∏Ä‰∏™WordNaturedescription„ÄÇË∞¢Ë∞¢ÔºÅ"
ÂÖ≥‰∫éÊòØÂê¶ÂèØ‰ª•ÁªÑÂª∫QQÁ§æÁæ§ÁöÑÂ∞èÂª∫ËÆÆ,hankcs ‰Ω†Â•ΩÔºåÈ¶ñÂÖàÊÑüË∞¢‰Ω†Âíå‰Ω†ÁöÑÂõ¢Èòü‰ª•ÂèäÂ•ΩËÄÅÊùøÁ†îÂèë‰∫ÜHanLPÂπ∂Â§ßÂÖ¨Êó†ÁßÅÁöÑÂºÄÊ∫ê„ÄÇÊàëÊòØ‰∏Ä‰∏™NLPÁöÑÂàùÂ≠¶ËÄÖÔºàÂîØ‰∏ÄÁöÑ‰ºòÂäøÊòØjavaÂºÄÂèë ÁªèÈ™åÊúâ6Âπ¥‰∫ÜÔºâÔºåÊàëÈÄöËøáÁôæÂ∫¶ÊâæÂà∞Áî±Âí±‰ª¨Ëøô‰πà‰∏Ä‰∏™Â∑•ÂÖ∑Ôºå‰ΩÜÊòØÊàëÊúâÂæàÂ§öÁöÑÁñëÊÉëÔºåÊÑüËßâÊ≤°Âú∞ÊñπÂèØ‰ª•Ê≤üÈÄöÔºåËÉΩÂê¶ÁªÑÁªá‰∏Ä‰∏™qqÁæ§ÔºåÂ§ßÂÆ∂ÊúâÈóÆÈ¢òÂèØ‰ª•ÈóÆ‰∏Ä‰∫õÂ∑≤Áªè‰ΩøÁî®ÁöÑÊØîËæÉÁÜüÁªÉÁöÑÈ´òÊâã‰ª¨ÔºåÁÑ∂ÂêéÂ§ßÂÆ∂ÈÉΩÂèØ‰ª•Áõ∏‰∫íËøõÊ≠•„ÄÇ‰πüÂèØ‰ª•Êõ¥Â•ΩÁöÑÂèëÊå•Áæ§‰ºóÁöÑÂäõÈáè„ÄÇ
"Ëá™ÂÆö‰πâÂ≠óÂÖ∏Êä•Èîôjava.lang.NumberFormatException: For input string: ""gi""","ÊàëÊää‰πãÂâçËá™ÂÆö‰πâËØçÊÄß‰∏∫giÁöÑÂ≠óÂÖ∏Êâ©ÂÖÖ‰∫Ü‰∏Ä‰∫õÂÜÖÂÆπËøõÂéªÔºå‰πãÂêéÊääÁºìÂ≠òÂà†ÊéâÈáçÊñ∞ÂêØÂä®Â∞±Êä•‰∫ÜËøô‰∏™ÈîôËØØ„ÄÇ„ÄÇ„ÄÇ„ÄÇ
‰∏âÊúà 20, 2017 4:13:45 ‰∏ãÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñD:/data/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.txt.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
‰∏âÊúà 20, 2017 4:13:54 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary loadDat
Ë≠¶Âëä: Â∞ùËØïËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂D:/data/dictionary/CoreNatureDictionary.ngram.txt.table.binÂèëÁîüÂºÇÂ∏∏[java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.ngram.txt.table.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)]Ôºå‰∏ãÈù¢Â∞ÜËΩΩÂÖ•Ê∫êÊñá‰ª∂Âπ∂Ëá™Âä®ÁºìÂ≠ò‚Ä¶‚Ä¶
‰∏âÊúà 20, 2017 4:13:58 ‰∏ãÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñD:/data/dictionary/custom/CustomDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: D:\data\dictionary\custom\CustomDictionary.txt.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
‰∏âÊúà 20, 2017 4:13:59 ‰∏ãÂçà com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
Ë≠¶Âëä: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
‰∏âÊúà 20, 2017 4:13:59 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏D:/data/dictionary/custom/tags.txtËØªÂèñÈîôËØØÔºÅjava.lang.NumberFormatException: For input string: ""gi""
‰∏âÊúà 20, 2017 4:13:59 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
Ë≠¶Âëä: Â§±Ë¥•ÔºöD:/data/dictionary/custom/tags.txt
ÂØºËá¥Âä†ËΩΩËøô‰∏™Â≠óÂÖ∏Â§±Ë¥•ÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅËøôÊòØ‰∏∫‰ªÄ‰πàÔºüÔºüÔºüÔºüÔºüÔºüÔºüÔºüÔºüÔºü"
ÂÖ≥‰∫éOrganizationDictionary Á±ª‰∏≠Â∑≤ÁªèÂ≠òÂú®ÁöÑËßíËâ≤ËßÑÂàôÁªÑÂêà,
ÂßìÊ∞èÊÄªÁªìÔºåÊÑüËßâÊ≤°ÊúâÂÖ®ÔºåÂ∫îËØ•ÊòØËÆ≠ÁªÉËØ≠ÊñôÂ∞ëÂºïËµ∑ÁöÑ,
ÂØπËØçÂÖ∏Âä†ËΩΩÁöÑ‰∏Ä‰∫õÂª∫ËÆÆ,
‰∏Ä‰∫õÂÆû‰ΩìËØÜÂà´‰∏çÂáÜÁ°ÆÁöÑÊÉÖÂÜµ,"Êú∫ÊûÑÂêçÂâçÂêéÁöÑ‰∏Ä‰∫õ‰ªãËØçÂ§ÑÁêÜÁöÑ‰∏çÂ•ΩÔºåÂ¶ÇÔºö
Âçó‰∫¨ÁöÑ‰∏úÂçóÂ§ßÂ≠¶‰ª•ÂâçÂè´Âçó‰∫¨Â∑•Â≠¶Èô¢
[Âçó‰∫¨/ns, ÁöÑ‰∏úÂçóÂ§ßÂ≠¶/nt, ‰ª•Ââç/f, Âè´/vi, Âçó‰∫¨Â∑•Â≠¶Èô¢/nt]

Âú®\data\dictionary\person\nr.txt‰∏≠Âä†‰∫ÜÈªÑÈπ§Ê•º A 1ËøòÊòØÊ≤°Áî®
ÊπñÂåóÊúÄÂá∫ÂêçÁöÑÁÉüÊòØÈªÑÈπ§Ê•º
[ÊπñÂåó/ns, ÊúÄ/d, Âá∫Âêç/a, ÁöÑ/ude1, ÁÉü/n, ÊòØ/vshi, ÈªÑÈπ§Ê•º/nr]

1998Âπ¥3Êúà1Âè∑ÊòØÊàëÁöÑÁîüÊó•
[1998/m, Âπ¥/qt, 3/m, Êúà1Âè∑/nt, ÊòØ/vshi, Êàë/rr, ÁöÑ/ude1, ÁîüÊó•/n]
"
ÁéãÂõΩÂº∫„ÄÅÈ´òÂ≥∞„ÄÅÊ±™Ê¥ã„ÄÅÂº†ÊúùÈò≥ÂÖâÁùÄÂ§¥„ÄÅÈü©ÂØí„ÄÅÂ∞èÂõõ ÔºåËøôÈáåÈù¢‚ÄúÂº†ÊúùÈò≥‚ÄùËøô‰∏™‰∫∫ÂêçÂàÜÈîô‰∫Ü,"ÊàëÂú®Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠Âä†ÂÖ•‚ÄúÂº†ÊúùÈò≥‚ÄùËøô‰∏™‰∫∫ÂêçÔºåËÆæÂÆö
Âº†ÊúùÈò≥ nr 10000
ÁÖßÊ†∑ÂàÜÈîô
debugÁúã‰∫Ü‰∏Ä‰∏ãÁ≤óÂàÜÁΩëÔºåÈáåÈù¢Ê≤°Êúâ""Èò≥""Â≠óÔºö
Á≤óÂàÜÁªìÊûú[ÁéãÂõΩÂº∫/nr, „ÄÅ/w, È´òÂ≥∞/n, „ÄÅ/w, Ê±™Ê¥ã/n, „ÄÅ/w, Âº†/q, Êúù/tg, Èò≥ÂÖâ/n, ÁùÄ/uzhe, Â§¥/n, „ÄÅ/w, Èü©ÂØí/nr, „ÄÅ/w, Â∞è/a, Âõõ/m]

ÊàëÂ∫îËØ•ÊÄé‰πàÂÅöÔºåÊâçËÉΩÊääËøô‰∏™‰∫∫ÂêçÂàÜÂØπÂë¢Ôºü"
‰øÆÊîπ‰∏Ä‰∏™ÊèêÁ§∫ÈóÆÈ¢ò,
Áî®‰∫Ü‰∏ãËΩΩÁöÑdataËØçÂÖ∏(Ëß£ÂéãÂêé1G)ÂêéÂàÜËØçÊïàÊûúÂèçËÄåÂèòÂ∑Æ‰∫ÜÔºüÔºüÔºü,"# Ê≤°Êúâ‰ΩøÁî®dataËØçÂÖ∏
< ÈóÆ‰∏ã‰Ω†‰ª¨ÊúâÂ•Ω‰∏ÄÁÇπÁöÑÂ•óÈ§êÂêóÔºåÊàëÁé∞Âú®Ëøô‰∏™Â•óÈ§êÊÑüËßâÊµÅÈáè‰∏çÊòØÂæàÁªèÁî®	ÈóÆ‰∏ã#‰Ω†‰ª¨#Êúâ#Â•Ω#‰∏ÄÁÇπ#ÁöÑ#Â•óÈ§ê#Âêó#Ôºå#Êàë#Áé∞Âú®#Ëøô‰∏™#Â•óÈ§ê#ÊÑüËßâ#ÊµÅÈáè#‰∏çÊòØ#Âæà#Áªè#Áî®
< ÈóÆ‰∏ã‰Ω†‰ª¨ÊúâÂ•Ω‰∏ÄÁÇπÁöÑÂ•óÈ§êÂêóÔºåÊàëÁé∞Âú®Ëøô‰∏™Â•óÈ§êÊÑüËßâÊµÅÈáè‰∏çÊòØÂæàÁªèÁî®	ÈóÆ‰∏ã#‰Ω†‰ª¨#Êúâ#Â•Ω#‰∏ÄÁÇπ#ÁöÑ#Â•óÈ§ê#Âêó#Ôºå#Êàë#Áé∞Âú®#Ëøô‰∏™#Â•óÈ§ê#ÊÑüËßâ#ÊµÅÈáè#‰∏çÊòØ#Âæà#Áªè#Áî®
< ÈóÆ‰∏ãÂäûÁêÜÂõΩÈôÖÊº´Ê∏∏ÊÄé‰πàÂäûÁêÜ	ÈóÆ‰∏ã#ÂäûÁêÜ#ÂõΩÈôÖÊº´Ê∏∏#ÊÄé‰πà#ÂäûÁêÜ
< ÈóÆ‰∏ãÂÆΩÂ∏¶‰ªÄ‰πàÊó∂ÂÄôÂèØ‰ª•Êù•‰∫∫‰øÆÔºåÊàë‰∏ãÂçàÊúâ‰∏™ËøúÁ®ã‰ºöËÆÆ	ÈóÆ‰∏ã#ÂÆΩÂ∏¶#‰ªÄ‰πà#Êó∂ÂÄô#ÂèØ‰ª•#Êù•‰∫∫#‰øÆ#Ôºå#Êàë#‰∏ãÂçà#Êúâ#‰∏™#ËøúÁ®ã#‰ºöËÆÆ

# ‰ΩøÁî®dataÊï∞ÊçÆËØçÂÖ∏
> ÈóÆ‰∏ã‰Ω†‰ª¨ÊúâÂ•Ω‰∏ÄÁÇπÁöÑÂ•óÈ§êÂêóÔºåÊàëÁé∞Âú®Ëøô‰∏™Â•óÈ§êÊÑüËßâÊµÅÈáè‰∏çÊòØÂæàÁªèÁî®	ÈóÆ#‰∏ã#‰Ω†‰ª¨#Êúâ#Â•Ω#‰∏ÄÁÇπ#ÁöÑ#Â•óÈ§ê#Âêó#Ôºå#Êàë#Áé∞Âú®#Ëøô‰∏™#Â•óÈ§ê#ÊÑüËßâ#ÊµÅÈáè#‰∏ç#ÊòØ#Âæà#Áªè#Áî®
> ÈóÆ‰∏ã‰Ω†‰ª¨ÊúâÂ•Ω‰∏ÄÁÇπÁöÑÂ•óÈ§êÂêóÔºåÊàëÁé∞Âú®Ëøô‰∏™Â•óÈ§êÊÑüËßâÊµÅÈáè‰∏çÊòØÂæàÁªèÁî®	ÈóÆ#‰∏ã#‰Ω†‰ª¨#Êúâ#Â•Ω#‰∏ÄÁÇπ#ÁöÑ#Â•óÈ§ê#Âêó#Ôºå#Êàë#Áé∞Âú®#Ëøô‰∏™#Â•óÈ§ê#ÊÑüËßâ#ÊµÅÈáè#‰∏ç#ÊòØ#Âæà#Áªè#Áî®
> ÈóÆ‰∏ãÂäûÁêÜÂõΩÈôÖÊº´Ê∏∏ÊÄé‰πàÂäûÁêÜ	ÈóÆ#‰∏ã#ÂäûÁêÜ#ÂõΩÈôÖ#Êº´Ê∏∏#ÊÄé‰πàÂäû#ÁêÜ
> ÈóÆ‰∏ãÂÆΩÂ∏¶‰ªÄ‰πàÊó∂ÂÄôÂèØ‰ª•Êù•‰∫∫‰øÆÔºåÊàë‰∏ãÂçàÊúâ‰∏™ËøúÁ®ã‰ºöËÆÆ	ÈóÆ#‰∏ã#ÂÆΩÂ∏¶#‰ªÄ‰πà#Êó∂ÂÄô#ÂèØ‰ª•#Êù•‰∫∫#‰øÆ#Ôºå#Êàë#‰∏ãÂçà#Êúâ#‰∏™#ËøúÁ®ã#‰ºöËÆÆ"
CRFËØÜÂà´Êñ∞ËØç‰ºº‰πé‰∏çËµ∑ÊïàÊûú,"Êàë‰ΩøÁî®""‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó""ËøôÂè•ËØùÂÅöÂÆûÈ™åÔºåÊ≤°ÊúâÂæóÂà∞ÁªìÊûú„ÄÇ

ÁÑ∂ÂêéÊàë‰ΩøÁî®ÂêåÊ†∑ÁöÑ‰∏ÄÁØáÊñ∞ÈóªÂÅöÂÆûÈ™åÔºåÂú®NLPIR‰∏äÈù¢ÂèØ‰ª•ÂæóÂà∞Êñ∞ËØç‚ÄúÂõΩÈôÖÊú∫Âú∫‚Äù Ôºå ‚ÄúÈ©¨ÁßªÊ∞ëÂ±Ä‚ÄùÁ≠âÊñ∞ËØçÔºå‰ΩÜÊòØÁî®hanlpÊó†Ê≥ïËé∑Âæó„ÄÇ

ÊñáÁ´†Â¶Ç‰∏ãÔºö

ÁéØÁêÉÁΩëÊä•ÈÅì ËÆ∞ËÄÖ ÁéãÊïèÊó•ÂâçÔºåÁΩëÂêç‰∏∫‰πîÂ¶πÁöÑÂè∞ÊπæÂ•≥Ê∏∏ÂÆ¢ÈÄöËøáËÑ∏Ë∞±ÂèëÊñáË°®Á§∫ÔºåËá™Â∑±Âú®ÂÖ•Â¢ÉÈ©¨Êù•Ë•ø‰∫öÊó∂ÔºåÂõ†Êä§ÁÖßÁ†¥ÊçüÔºåÂèóÂà∞È©¨Êù•Ë•ø‰∫öÊµ∑ÂÖ≥Èùû‰∫∫ÈÅìÂØπÂæÖÔºåÂ•π‰∏ç‰ΩÜË¢´Ê≤°Êî∂‰∫ÜÊâãÊú∫„ÄÅÊä§ÁÖßÁ≠âÁßÅ‰∫∫Áâ©ÂìÅÔºåËøòË¢´ÂÖ≥ÊäºÈïøËææ35‰∏™Â∞èÊó∂„ÄÇ
‚Äú03/09Êôö‰∏ä08Ôºö30Êàë‰∏ãÈ£ûÊú∫ÔºåÊàë‰æùÁÖßËßÑÂÆöÊéíÈòüÁ≠âÂÄôÁõñÂç∞Âá∫ÂÖ≥ÔºåËΩÆÂà∞ÊàëÊó∂ÔºåÁõñÂç∞‰∫∫Âëò‰ªñËØ¥ÊàëÊä§ÁÖßÊçüÂùèÔºå‰∏çËÉΩËøõÂéªÈ©¨Êù•Ë•ø‰∫öÔºå‰ªñË¶ÅÊàëÁ≠â‰∏Ä‰∏ã‚Äù
‰πîÂ¶πË°®Á§∫Ôºå‰ªñÂõûÊù•ÂêéËØ¢ÈóÆÂ•πÊä§ÁÖß‰∏∫‰Ωï‰ºöÊçüÂùèÔºåÂ•πËß£ÈáäËØ¥ÔºåÂú®Êó•Êú¨Ë¥≠Áâ©ÂÖçÁ®éÂçïË¢´Êó•Êú¨Êµ∑ÂÖ≥Êíï‰∏ãÊó∂ÈÄ†ÊàêÁöÑÁ†¥Êçü„ÄÇÊµ∑ÂÖ≥‰∫∫ÂëòÈöèÂç≥Â∞ÜÂÖ∂Â∏¶ÂÖ•ÂäûÂÖ¨ÂÆ§Ë°®Á§∫ÔºåÈ©¨Êù•Ë•ø‰∫ö‰∏çÊé•ÂèóÂπ∂Â∞ÜÊä§ÁÖßÊ≤°Êî∂„ÄÇ
‰πîÂ¶πÈöèÂêéË¢´Ë¶ÅÊ±ÇËøõÂÖ•Êõ¥ÈáåËæπÁöÑÂäûÂÖ¨ÂÆ§ÔºåÂΩìÂ•πËøõÂÖ•ÂêéÔºåÊâãÊú∫ÂèàË¢´Ê≤°Êî∂‰∫Ü„ÄÇËøôÊó∂Ôºå‰πîÂ¶πÂÜçÊ¨°Ë¢´Ë¶ÅÊ±ÇËøõÂÖ•Êõ¥ÂêéÈù¢ÁöÑÂ∞èÊàøÈó¥ÈáåÊîæÁΩÆË°åÊùéÔºåÂú®ÈÇ£ÈáåÂá∫Áé∞‰∫Ü‰∏Ä‰∏™ÂæàÂá∂ÁöÑÁî∑Â≠ê„ÄÇ
‚ÄúÊàëÊääÂ§ñÂ•óÊîæÂú®ÊâãÊèêË°åÊùé‰∏äÔºå‰ªñÂ∞±ÊääÊàëÁöÑÂ§ñÂ•óÂæàÂ§ßÂäõÁöÑ‰∏¢ÂêëÊàëÔºåË°åÊùéÁÆ±ÂæÄ‰∏ä‰∏¢ÔºåËøòÂè´ÊàëÊääÈí±ÊãøÂá∫Êù•ÔºåÊàë‰∏çÊÉ≥Áªô‰ªñÈí±ÔºåÂÅáË£ÖÂê¨‰∏çÊáÇÔºåÁÑ∂ÂêéÊâçÊúâ‰∏Ä‰∏™Â•≥ÁîüÊù•ÔºåÂ∞±Âè´ÊàëÊääÈí±ÊîæÂè£Ë¢ã‚Äù
‰πîÂ¶πÈöèÂêéÊâãË¢´ÈìêËµ∑Êù•Â∏¶Ëøõ‰∫ÜÁâ¢ÊàøÔºåÂØπÊñπ‰ªÖÂëäËØâÂ•πÁ¨¨‰∫åÂ§©Êó©‰∏äÂèØÂõûÂè∞Êπæ„ÄÇ‚ÄúÈáåÈù¢ÈªëÂòõÂòõÔºåÊúâÊó∂ÊâçÊúâÂºÄÁÅØÔºåÂéïÊâÄÂ•ΩËÑè‰πüÊ≤°ÊúâÈó®ÔºåÂè™ËÉΩÂùêÂú∞‰∏äÔºåÁ¥Ø‰∫ÜÁù°Âú∞‰∏äÔºåÂ•ΩÂ§ö‰∫∫‚Äù‰πîÂ¶πÁß∞Â•πËøòÈÅáÂà∞‰∫ÜÂè¶‰∏Ä‰∏™Âè∞Êπæ‰∫∫„ÄÇ
‰∏Ä‰∏™ÁªÜËäÇÔºå‰πîÂ¶πË°®Á§∫ÔºåÊúâÈ©¨Êù•Ë•ø‰∫öÂ∑•‰Ωú‰∫∫ÂëòÂêëÂÖ∂Ë°®Á§∫ÔºåÂè™Ë¶Å1000È©¨Â∏Å‰æøËÉΩÂÖàÂá∫ÂéªÔºå‰ΩÜÂ•πÂç¥ÂàÜÊòéÁúãÂà∞Â¢ô‰∏äÂØÜÂØÜÈ∫ªÈ∫ªÁöÑ‚Äú‰∏çË¶ÅÁªôÈí±ÔºåÈÉΩÊòØÈ™óÈí±‚ÄùÁöÑÂ≠ó„ÄÇ
Ë¥¥ÊñáÂèëÂ∏ÉÂêéÔºåÂú®È©¨Êù•Ë•ø‰∫öÔºå‰∫ã‰ª∂ËøÖÈÄüÁÉ≠Ëµ∑ÔºåÂÖ∂ÂêéÔºåÂè∞Â™í‰πüÁõ∏ÁªßË∑üËøõÔºåÂÅöÂá∫‰∫ÜÊä•ÈÅì„ÄÇÈ©¨Êù•Ë•ø‰∫öÁΩëÁ´ô‚ÄúËæ£ÊâãÁΩë‚Äù14Êó•‰∏ãÂçàÊä•ÈÅìÔºåÈ©¨È¶ñÁõ∏‰∏ú‰∫öÁâπ‰ΩøÂÖºÊ∞ëÈÉΩÈ≤ÅÂå∫ÂõΩ‰ºöËÆÆÂëòÂº†Â∫Ü‰ø°ËÆ§‰∏∫ÔºåËøô‰ª∂‰∫ãÂÖ≥‰πéÂõΩÂÆ∂ÂΩ¢Ë±°ÂíåÂÆòÂëòÁ¥†Ë¥®ÔºåÂèØËÉΩÊâìÂáªÈ©¨Êù•Ë•ø‰∫öÊóÖÊ∏∏‰∏ö„ÄÇ
Âº†Â∫Ü‰ø°Ë°®Á§∫ÔºåÈÅ≠Êâ£ÁïôÁöÑÂè∞ÊπæÊóÖÂÆ¢‰∏çÊòØÁΩ™ÁäØÔºå‰∏çËØ•Áî®Ëøô‰πà‰∏ç‰∫∫ÈÅìÁöÑÊñπÂºèÂØπÂæÖÂè∞ÊπæÊóÖÂÆ¢„ÄÇÂõ†Ê≠§ÂêëÂÜÖÊîøÈÉ®ÂèçÊò†„ÄÇËÄåÊçÆÂè∞‚ÄúËÅîÂêàÊñ∞ÈóªÁΩë‚ÄùÊä•ÈÅìÁß∞ÔºåÈ©¨ÂõΩÂâØÈ¶ñÁõ∏ÂÖºÂÜÖÊîøÈÉ®ÈïøÈòøÊú´ÊâéÂ∏åÂæóÁü•ÂêéÂ§ßÊÄíÔºåÂ∑≤‰∏ã‰ª§ÂΩªÊü•„ÄÇ
Âè∞Êπæ‚ÄúÂ§ñ‰∫§ÈÉ®‚ÄùÂæóÁü•Ê∂àÊÅØÂêéË°®Á§∫ÔºåÂ∑≤‰∫éÁ¨¨‰∏ÄÊó∂Èó¥‰∏ªÂä®ÊéåÊè°ËÆØÊÅØÔºåÂπ∂Áªè‚ÄúÈ©ªÂ§Ñ‚ÄùÊ¥ΩÁ≥ªÂêâÈöÜÂù°ÂõΩÈôÖÊú∫Âú∫ÁßªÊ∞ëÂ±ÄÊâßÊ≥ïÁªÑÔºåËΩ¨Ëø∞Ë¥¥ÊñáÊâÄËø∞ÊÉÖËäÇÔºå‰∫ÜËß£ÂΩìÊó•Áõ∏ÂÖ≥ÊÉÖÂΩ¢„ÄÇ
ËÄåÈ©¨Êù•Ë•ø‰∫öÂàôÂõûÂ§çÔºåËØ•ÂêçÂ•≥ÊÄß‰∫é3Êúà9Êó•Êê≠‰πò‰∫öÊ¥≤Ëà™Á©∫Áè≠Êú∫‰∫éÊôöÈó¥8Êó∂30ÂàÜÊäµËææÂêâÈöÜÂù°Á¨¨2ÂõΩÈôÖÊú∫Âú∫ÔºåÊãüÂÖ•Â¢ÉÊó∂ÔºåÂõ†ÊâÄÊåÅÊä§ÁÖßÊØÅÊçüÔºåË¢´Ë£ÅÂÆöÊãíÁªùÂÖ•Â¢ÉÂπ∂‰∫à‰ª•ÈÅ£ËøîÔºå‰∫éÊòØÂú®3Êúà11Êó•‰∏äÂçà9Êó∂20ÂàÜÊê≠‰πò‰∫öÊ¥≤Ëà™Á©∫Á¨¨D7372Âè∑Áè≠Êú∫ËøîÂõûÂè∞ÊπæÊ°ÉÂõ≠ÂõΩÈôÖÊú∫Âú∫„ÄÇ
ÈÇ£‰πàÔºåÈ©¨Êù•Ë•ø‰∫öÈÅ£ËøîÁöÑÊ†áÂáÜ‰Ωú‰∏öÊòØ‰ªÄ‰πàÂë¢ÔºåÂè∞‚ÄúÂ§ñ‰∫§ÈÉ®‚ÄùË°®Á§∫ÔºåÊ†πÊçÆ‰∫ÜËß£ÔºåÊóÖÂÆ¢ÁªèÊú∫Âú∫ÁßªÊ∞ëÂÆòË£ÅÂÆöÊãíÁªùÂÖ•Â¢ÉÂπ∂‰∫à‰ª•ÈÅ£ËøîÂêéÔºåÂÄòÂΩìÊó•Êó†Ê≥ïÁôªÊú∫ËøîÂõûÔºåÊóÖÂÆ¢Â∞ÜË¢´ÂÆâÁΩÆÂú®Êú∫Âú∫‰πã‚ÄúÁÖßÊä§ÂÆ§‚ÄùÔºåÂπ∂Â∞ΩÈÄüËøõË°åÈÅ£Ëøî„ÄÇ
ÊóÖÂÆ¢Âú®ÁïôÁΩÆÊúüÈó¥Áõ∏ÂÖ≥ÊÉÖÂΩ¢ÔºåÈ©¨ÁßªÊ∞ëÂ±Ä‰æã‰∏çÁü•‰ºöÂêÑÁõ∏ÂÖ≥È©ªÈ©¨‚Äú‰ΩøÈ¢ÜÈ¶Ü‚ÄùÂçï‰ΩçÔºåËã•È©¨ÁßªÊ∞ëÂ±ÄËÆ§‰∏∫ÊúâÂøÖË¶ÅÂ∞ÜÊóÖÂÆ¢ÁßªÈÄÅËá≥Êú∫Âú∫ÈôÑËøëÁöÑ‚Äú‰∏¥Êó∂ÂÆâÁΩÆÊâÄ‚ÄùÔºå‰∫à‰ª•ËæÉÈïøÊó∂Èó¥ÁöÑÁïôÁΩÆÊàñË∞ÉÊü•Êó∂ÔºåÂàôÂ∞ÜÂêåÊ≠•Áü•‰ºö„ÄÇ
‰∫ã‰ª∂ÂêåÊ†∑ÂæàÂø´‰æøÂú®Á§æ‰∫§ÁΩëÁªú‰∏äÂèëÈÖµÔºåÊà™Ê≠¢Ê≠§ÊñáÊó∂ÔºåËΩ¨ÂèëÊï∞Ëææ10280Ê¨°„ÄÇ‰∏çÂ∞ëÁΩëÂèãÂØπ‰πîÂ¶πÁöÑÈÅ≠ÈÅáË°®Á§∫ÂêåÊÉÖÔºå‰ΩÜ‰πüÊúâ‰∫õÁΩëÂèãË¥®Áñë‰∫ã‰ª∂ÁöÑÁúüÂÆûÊÄßÔºåÁîöËá≥Ë°®Á§∫Ôºå‰πîÂ¶πÂπ∂‰∏çÊòØÂè∞Êπæ‰∫∫ÔºåÊâÄÂÅöÂè™ÊòØ‰∏∫‰∫ÜÁÇí‰Ωú„ÄÇ
ÊØîÂ¶ÇÊúâÁΩëÂèãÂç≥ËÆ§‰∏∫ÔºåÊúâ‰∫∫Âõ†‰∏∫Ë¶ÅÁ∫¢ËÄåË¥•ÂùèÔºåÊçü‰∫∫‰∏çÂà©Â∑±ÁöÑË°å‰∏∫‰∏ç‰∏ÄÂÆöÈ´òÊòéÔºåÊõ¥‰ΩïÂÜµÊòØÊçüÂÆ≥‰∏Ä‰∏™ÂõΩÂÆ∂ÁöÑÂêçË™âÔºàÈ©¨Êù•Ë•ø‰∫öÔºâ„ÄÇ‰πüÊúâÁΩëÊ∞ëË°®Á§∫Ëá™Â∑±Â∞±ÊòØÈ©¨Êù•Ë•ø‰∫ö‰∫∫ÔºåË¶ÅÊ±ÇÂá∫Á§∫Êä§ÁÖß„ÄÇ
Â∞±Ê≠§Ôºå‰πîÂ¶πÂú®ËÑ∏Ë∞±Ë°®Á§∫Ôºå‚ÄúÂ§ßÂÆ∂‰∏ÄÁõ¥ËØ¥ÊàëÊÉ≥Á∫¢ÔºåÊàëÊÉ≥ÈóÆËøô‰∏™ÂæàÂÖâËç£ÂêóÔºü‰Ω†‰ª¨ÂèØ‰ª•‰Ωì‰ºöË¢´ÂÖ≥ÁöÑÂøÉÊÉÖÂêóÔºüÈáåÈù¢ÁéØÂ¢ÉÊúâÂ§öÁ≥üÂêóÔºü‚ÄùÈöèÂêéÊõ¥ÊòØË¥¥‰∏äÊä§ÁÖßËØÅÊòéËá™Ë∫´„ÄÇ
‰πîÂ¶πÁöÑÈÅ≠ÈÅáÂè¶‰∏ÄÊñπÈù¢ÂàôÊãõÊù•‰∫Ü‰∏çÂ∞ëÂ≤õÂÜÖÂ§ñÁöÑÂÖ±ÊÑüÔºåÂêçÂè´‚ÄúKuan wei pan‚ÄùÁöÑÁΩëÂèãË°®Á§∫Ëá™Â∑±‰πüÊúâËøáÁõ∏ÂêåÁöÑÁªèÈ™åÔºåÂπ∂Áß∞‚Äú‰Ω†Ê∞∏Ëøú‰∏çË¶ÅËÆ©ËøôÈáåÁöÑ‰∫∫Áü•ÈÅì‰Ω†ÁöÑÂè£Ë¢ãÊúâÂ§öÊ∑±‚Äù„ÄÇ
‰πüÊúâ‰∏çÂ∞ëÈ©¨Êù•Ë•ø‰∫öÁöÑÁΩëÂèã‰∏∫‰πîÂ¶πÁöÑÈÅ≠ÈÅáÊÑüÂà∞Êä±Ê≠âÔºåËøô‰ΩçÁΩëÂèãË°®Á§∫ÔºåËá™Â∑±‰Ωú‰∏∫‰∏Ä‰∏™È©¨Êù•Ë•ø‰ªñ‰∫∫ÔºåÈÉΩ‰∏∫ËøôÈáåÁöÑÈîÆÁõò‰æ†ÊÑüÂà∞ÂøÉÂØíÔºåËøôÊ†∑ÁöÑ‰∫ãÊÉÖÂèëÁîüÂú®Ë∞ÅÁöÑË∫´‰∏äÔºåÈÉΩ‰ºöÂ¥©Ê∫ÉÁöÑ„ÄÇ
ÂΩìÁÑ∂‰πüÊúâÁΩëÂèãÁúãÁöÑÊõ¥Â§ö‰∫õÔºåÊØîÂ¶ÇËøô‰ΩçÂú∞ÂùÄÊòæÁ§∫‰∏∫Âè∞ÂåóÁöÑÁΩëÂèãËøôÊ†∑ËØ¥ÔºåÂ¶ÇÊûúÊåÅÊúâÁöÑÊòØ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊä§ÁÖßÔºåË¢´Â§ÑÁêÜÁöÑËøáÁ®ãÂ∞±‰ºö‰∏ç‰∏ÄÊ†∑„ÄÇ"
python ËØçÂÖ±Áé∞ÁªüËÆ° Occurrence Â¶Ç‰ΩïÂÅúÁî® CustomDictionary,"Âíå DemoOccurrence ÁµêÊûú‰∏çÂêå
ÂêåÊ®£ÈÉΩÊòØÁî®
`Âú®ËÆ°ÁÆóÊú∫Èü≥ËßÜÈ¢ëÂíåÂõæÂΩ¢ÂõæÂÉèÊäÄÊúØÁ≠â‰∫åÁª¥‰ø°ÊÅØÁÆóÊ≥ïÂ§ÑÁêÜÊñπÈù¢ÁõÆÂâçÊØîËæÉÂÖàËøõÁöÑËßÜÈ¢ëÂ§ÑÁêÜÁÆóÊ≥ï`

Demo ÁöÑ uniGram ÁµêÊûú
[‰ø°ÊÅØ=‰ø°ÊÅØ=1, ÂÖàËøõ=ÂÖàËøõ=1, ÂõæÂΩ¢ÂõæÂÉè=ÂõæÂΩ¢ÂõæÂÉè=1, Â§ÑÁêÜ=Â§ÑÁêÜ=2, ÊäÄÊúØ=ÊäÄÊúØ=1, ÊñπÈù¢=ÊñπÈù¢=1, ÊØîËæÉ=ÊØîËæÉ=1, ÁõÆÂâç=ÁõÆÂâç=1, ÁÆóÊ≥ï=ÁÆóÊ≥ï=2, ËßÜÈ¢ë=ËßÜÈ¢ë=1, ËÆ°ÁÆóÊú∫=ËÆ°ÁÆóÊú∫=1, Èü≥ËßÜÈ¢ë=Èü≥ËßÜÈ¢ë=1]

ÊàëÁöÑÁµêÊûú
[ÂÖàËøõ=ÂÖàËøõ=1, ÂõæÂΩ¢ÂõæÂÉè=ÂõæÂΩ¢ÂõæÂÉè=1, Â§ÑÁêÜ=Â§ÑÁêÜ=2, ÊäÄÊúØ=ÊäÄÊúØ=1, ÊØîËæÉ=ÊØîËæÉ=1]

Âõ†ÁÇ∫ÊàëÁöÑËá™Ë®ÇË©ûÂÖ∏‰∏≠

[Âú®/p, ËÆ°ÁÆóÊú∫/product, Èü≥ËßÜÈ¢ë/product, Âíå/cc, ÂõæÂΩ¢ÂõæÂÉè/nz, ÊäÄÊúØ/keyword, Á≠â/udeng, ‰∫å/m, Áª¥/b, ‰ø°ÊÅØ/product, ÁÆóÊ≥ï/product, Â§ÑÁêÜ/keyword, ÊñπÈù¢/product, ÁõÆÂâç/product, ÊØîËæÉ/keyword, ÂÖàËøõ/a, ÁöÑ/ude1, ËßÜÈ¢ë/product, Â§ÑÁêÜ/keyword, ÁÆóÊ≥ï/product]

CRFÊúâÂÅúÁî®Ëá™Ë®ÇË©ûÂÖ∏ÁöÑÂäüËÉΩ
`CRFSegment().enableCustomDictionary(false);`

ÊàëË¶ÅÂ¶Ç‰ΩïÂú® ‰ΩøÁî® occurrence ÊôÇÂÅúÁî® CustomDictionary ?


-------------
Âè¶Â§ñÔºåËá™Ë®ÇÂ≠óË©ûÊàëÊòØÁî® LexiconUtility.setAttribute ÂéªÂä†Ë©ûÔºåÂõ†ÁÇ∫ÊàëÂä†ÁöÑË©ûÂê´Êúâ `space`
"
ÈöêÈ©¨Â∞îÂèØÂ§´ËØçÊÄßÊ†áÊ≥®Ê¶ÇÁéáËΩ¨ÁßªÁü©ÈòµÈóÆÈ¢ò,"‰Ω†Â•ΩÔºåÂçö‰∏ªÂ§ß‰∫∫ÔºåËøôÈáåÂú®ÁªüËÆ°Ê†áÁ≠æÂá∫Áé∞Ê¨°Êï∞Êó∂ÊòØ‰∏çÊòØ‰ºöÈáçÂ§çËÆ°ÁÆóÂë¢ÔºüËÄå‰∏î‰πüÊó†Ê≥ï‰øùËØÅ‰∫ÜÊ¶ÇÁéáËΩ¨ÁßªÁü©ÈòµÊØèË°åÂÖÉÁ¥†‰πãÂíåÁ≠â‰∫é1Âë¢ÔºüÊàëÊÑüËßâ total[j] += matrix[i][j];ËøôÂè•ÊúâÈáçÂ§çËÆ°ÁÆóÁöÑÂèØËÉΩ„ÄÇ
`// ÈúÄË¶ÅÁªüËÆ°‰∏Ä‰∏ãÊØè‰∏™Ê†áÁ≠æÂá∫Áé∞ÁöÑÊ¨°Êï∞
            total = new int[ordinaryMax];
            for (int j = 0; j < ordinaryMax; ++j)
            {
                total[j] = 0;
                for (int i = 0; i < ordinaryMax; ++i)
                {
                    total[j] += matrix[i][j];
                    total[j] += matrix[j][i];
                }
            }`"
ÈÄÇÈÖç redis Âπ≥Âè∞ÔºåËØªÂèñÂ≠óÁ¨¶‰∏≤ÔºåÂ≠óËäÇÊï∞ÁªÑÂíåÊï¥ÂûãËΩ¨Êç¢ÂºÇÂ∏∏ÔºåÂá∫Áé∞Ë¥üÊï∞ÊÉÖÂÜµ,"ByteUtil.bytesHighFirstToInt(byte[] bytes, int start)
È∫ªÁÉ¶Ë∑üË∏™‰∏Ä‰∏ã‰ªÄ‰πàÊÉÖÂÜµÔºåË∞¢Ë∞¢ÔºÅ"
ÂèëÁé∞Ê•º‰∏ªËøòÂú®Êõ¥Êñ∞ÔºåÈÇ£ÊàëÊèê‰∫§‰∏Ä‰∏™bug„ÄÇ Âà§Êñ≠Â≠óÁ¨¶Á±ªÂûã Ëøô‰∏™ÂáΩÊï∞ÊúâbugÔºåÊàë‰øÆÊ≠£ÂêéÁöÑÔºö," /**
     * Âà§Êñ≠Â≠óÁ¨¶Á±ªÂûã
     * @param str
     * @return
     */
    public static int charType(String str)
    {
        if (str != null && str.length() > 0)
        {
            if (""Èõ∂‚óã„Äá‰∏Ä‰∫å‰∏§‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅÂªøÁôæÂçÉ‰∏á‰∫øÂ£πË¥∞ÂèÅËÇÜ‰ºçÈôÜÊüíÊçåÁéñÊãæ‰Ω∞‰ªü"".contains(str)) return CT_NUM;
            byte[] b;
            try
            {
                b = str.getBytes(""GBK"");
            }
            catch (UnsupportedEncodingException e)
            {
                b = str.getBytes();
                e.printStackTrace();
            }
            byte b1 = b[0];
            byte b2 = b.length > 1 ? b[1] : 0;
            int ub1 = getUnsigned(b1);
            int ub2 = getUnsigned(b2);
            if (ub1 < 128)
            {
            	if(b1>='A' && b1<='Z'){
            		return CT_LETTER;
            	}
            	if(b1>='a' && b1<='z'){
            		return CT_LETTER;
            	}
                if (' ' == b1 || '\t' == b1) return CT_OTHER;
                if ('\n' == b1 || '\r' == b1) return CT_DELIMITER;
                if (""*\""!,.?()[]{}_-+=/\\;:|"".indexOf((char) b1) != -1)
                    return CT_DELIMITER;
                if (""0123456789"".indexOf((char)b1) != -1)
                    return CT_NUM;
                return CT_SINGLE;
            }
            else if (ub1 == 162)
                return CT_INDEX;
            else if (ub1 == 163 && ub2 > 175 && ub2 < 186)
                return CT_NUM;
            else if (ub1 == 163
                    && (ub2 >= 193 && ub2 <= 218 || ub2 >= 225
                    && ub2 <= 250))
                return CT_LETTER;
            else if (ub1 == 161 || ub1 == 163)
                return CT_DELIMITER;
            else if (ub1 >= 176 && ub1 <= 247)
                return CT_CHINESE;

        }
        return CT_OTHER;
    }"
python ÁÑ°Ê≥ï‰ΩøÁî® HanLP.Config.ShowTermNature,"
- python 2.7 ÊâÄÈ°ØÁ§∫ÁöÑ error Â¶Ç‰∏ã
AttributeError: type object 'com.hankcs.hanlp.HanLP' has no attribute 'Config'

ÂÖ∂‰ªñÂïèÈ°å
- CRFSegment Âú®.enableCustomDictionary(False) ÊâÄÊúâÁöÑPOStag ÊúÉËÆäÊàê `/null`
Âú® DemoCRFSegment.java ‰∏≠
Êää 30Ë°åÁöÑ ` HanLP.Config.ShowTermNature = false;    // ÂÖ≥Èó≠ËØçÊÄßÊòæÁ§∫` Ë®ªËß£ÂæåÂ∞±ÊúÉÁôºÁèæÈÄôÂÄãÂïèÈ°å

- CRFSegment Âú®.enablePartOfSpeechTagging(False) ÁÑ°Êïà
ÊàëË™çÁÇ∫ÔºåË¶ÅÂÅúÁî®POSTagÊôÇÔºåÂêÑÂÄãÂàÜË©ûÂô®Áî®Ëá™Â∑±ÁöÑË®≠ÂÆöÊúÉÊØîËºÉÁõ¥Ë¶∫‰∏Ä‰∫õÔºåÂú®ÂºÑÊàê web APIÊôÇÔºåÂ∞ëÂãïÁî®ÂÖ®ÂüüË®≠ÂÆöÊúÉÊØîËºÉÂ•Ω
"
ÁâàÊú¨1.3.2CRFDependencyModelPathÂØπÂ∫îË∑ØÂæÑ‰∏ãÂπ∂Ê≤°ÊúâÁõ∏Â∫îÊñá‰ª∂,Â∑≤‰∏ãËΩΩÂØπÁöÑ[data](https://pan.baidu.com/s/1pKUVNYF)ÂåÖÔºåÂπ∂Êú™ÂèëÁé∞data/model/dependency/CRFDependencyModelMini.txt ÔºåÊòØÊàë‰∏ãÁöÑ‰∏çÂØπÔºüËøòÊòØÂà´ÁöÑ‰ªÄ‰πàÂéüÂõ†ÔºåËÉΩÂê¶ÂëäÁü•‰∏Ä‰∏ã„ÄÇ
ÈÇ£‰∫õ‰∏çÂú®CoreNatureDictionary.txtÁöÑ‰∏≠ËØç‰ºöÊ†áÊ≥®‰ªÄ‰πàËØçÊÄß,"Âú®Êü•Áúã‰∫∫ÂêçËØçÂÖ∏nr.txtÊó∂„ÄÇÈááÁî®‰ª•‰∏ãÊñπÂºèÊü•ÊâæÂú®nr.txt ‰ΩÜ‰∏çÂú®CoreNatureDictionary.txt‰∏≠ÁöÑËØçÔºö
awk 'NR==FNR{a[$1]}NR>NFR{if(!($1 in a))print $0}' ../CoreNatureDictionary.txt nr.txt | wc -l
ÁªìÊûú1901‰∏™ËØçÔºåËøôÈÉ®ÂàÜËØçËØ•Â¶Ç‰ΩïÊ†áÊ≥®ËØçÊÄß„ÄÇ‰∫∫ÂêçËØÜÂà´‰∏≠ÔºåËØçÁöÑËßíÂ∫¶Êúâ‰∫∫ÂêçÂâçÁºÄÔºåÂßìÔºåÂêçÔºåÂêéÁºÄÔºå‰∏é‰∫∫ÂêçÊó†ÂÖ≥ËØçÁ≠â„ÄÇÂ¶ÇÊûúËøô1901‰∏™ËØç‰∏çÂú®CoreNatureDictionary.txt, ÈÇ£ÊÄé‰πà‰ºöÂàÜÂá∫Ëøô‰∫õËØçÊù•Âë¢Ôºü Â¶ÇÊûúÂàÜÂá∫Êù•‰∫ÜÔºåËØçÊÄßÂèàÊòØÂ¶Ç‰ΩïËÆæÁΩÆÁöÑÂë¢Ôºü"
‰Ω†ÁöÑËøô‰∏™ÂåÖÂÖÖÊª°‰∫Übug,"Ë¶Å‰∏çÊâìÂåÖ‰∏çË°åÔºåË¶Å‰∏çÊïàÊûú‰∏çÂ•ΩÔºåÂèçÊ≠£Ê≤°‰ªÄ‰πàÂ•ΩËØ¥ÁöÑ„ÄÇÈÉΩÁÉ¶Ê≠ª‰∫ÜÁî®Ëøô‰∏™ÂåÖ‰∫Ü„ÄÇÂàöÂàöÂºÄÂßã‰∫ÜË∑ëËøô‰∏™ÂºÇÂ∏∏Ôºö
`‰∏•Èáç: Ê≤°ÊúâÊâæÂà∞HanLP.propertiesÔºåÂèØËÉΩ‰ºöÂØºËá¥Êâæ‰∏çÂà∞data` Êú¨Êù•ÊòØÂ•ΩÂ•ΩÁöÑÔºåÁ™ÅÁÑ∂Â∞±ËøôÊ†∑‰∫Ü„ÄÇÊÄé‰πàÈÇ£‰πàÂ§öÈóÆÈ¢ò„ÄÇ„ÄÇ„ÄÇÔºüÔºü"
ES ÈõÜÊàê hanlpÊó∂Êä•Èîô„ÄÇ,"Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.seg.common.Vertex
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:68)
	at com.hankcs.lucene.HanLPTokenizer.incrementToken(HanLPTokenizer.java:76)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
"
CustomÂ≠óÂÖ∏ÁöÑÂ¢ûÂà†ÈóÆÈ¢ò,"1.  È¶ñÂÖàÂà†Èô§CustomDictionary.txt.binÊñá‰ª∂, Âπ∂Âú®CustomDictionary.txtÊñá‰ª∂Â¢ûÂä†""ÈòøÈáåÂ∑¥Â∑¥"". Âú®Ëøô‰πãÂâç‚ÄúÈòøÈáåÂ∑¥Â∑¥‚ÄùË¢´ÂàÜÊàê""ÈòøÈáå""Ôºå‚ÄúÂ∑¥Â∑¥‚Äù„ÄÇ
2.  ËøôÊó∂ÂàÜËØçËÉΩÊàêÂäüÂàÜÂá∫""ÈòøÈáåÂ∑¥Â∑¥""
3.  Âà†Èô§CustomDictionary.txt.binÊñá‰ª∂, Âπ∂Âú®CustomDictionary.txtÊñá‰ª∂Âà†Èô§""ÈòøÈáåÂ∑¥Â∑¥""
4.  ËøôÊó∂ÂàÜËØç‰æùÁÑ∂ËÉΩÂàÜËØç‰∏∫""ÈòøÈáåÂ∑¥Â∑¥""Ôºå  ËøôÊòØ‰ªÄ‰πàÈ¨ºÔºü ËØ∑ÊµãËØïÈ™åËØÅ"
Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ËΩΩ‰∏ç‰∫Ü,"1„ÄÅCustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; MyDic.txt;
2„ÄÅMyDic.txtÔºö‰ø°ÊÅØ‰∏éËÆ°ÁÆóÁßëÂ≠¶ v 1
3„ÄÅÁ°Æ‰øùÊòØuft-8
4„ÄÅÂ∑≤Âà†Èô§ÁºìÂ≠òÊñá‰ª∂
5„ÄÅLexiconUtility.getAttribute(""‰ø°ÊÅØ‰∏éËÆ°ÁÆóÁßëÂ≠¶"")
6„ÄÅHanLP.Config.enableDebug()Áúã‰∏çÂà∞ÊúâÂä†ËΩΩMyDic.txt
7„ÄÅCustomDictionary.add(""‰ø°ÊÅØ‰∏éËÆ°ÁÆóÁßëÂ≠¶"")ÂèØ‰ª•ÊàêÂäü
Ë∞¢Ë∞¢ÔºÅ"
hanlp-portable ÊÄéÊ†∑ÈÄöËøáproperties Êñá‰ª∂ÈÖçÁΩÆËá™ÂÆö‰πâËØçÂÖ∏Âë¢„ÄÇ,
ÂÖ≥‰∫éËá™ÂÆö‰πâÂ≠óÂÖ∏ËØÜÂà´ÈóÆÈ¢ò,"ÊàëÂÆö‰πâ‰∫ÜITÊäÄÊúØÁõ∏ÂÖ≥Â≠óÂÖ∏ÔºåËÆæÁΩÆËØçÊÄß‰∏∫Ôºögi(ËÆ°ÁÆóÊú∫Áõ∏ÂÖ≥ËØçÊ±á)
javascript gi 1
java gi 1
spring gi 1
scala gi 1
python gi 1
...
ÊàëÂú®ÂØπÂÜÖÂÆπJava javascript html5css mybatis hibernate springmvc python scalaËøõË°åÂàÜËØçÁöÑÊó∂ÂÄôËøòÊòØÂ∞ÜÈªòËÆ§ËØçÊÄßËØÜÂà´Êàê‰∫ÜÔºöjavajavascripthtml/nx, 5/m, cssmybatishibernatespringmvcpythonscala/nxÔºàÂ≠óÊØç‰∏ìËØçÔºâ
Ëá™ÂÆö‰πâÂ≠óÂÖ∏Âú∞ÂùÄÊ≤°ÈîôÔºå‰πüÊ∏Ö‰∫ÜÁºìÂ≠ò‰∫Ü
ÊúâÈÅáÂà∞ÂêåÊ†∑ÈóÆÈ¢òÁöÑÂêó"
Âú®ÈõÜÊàêelasticsearch 2.4.4 Êó∂ Êä•ÈîôÔºåCould not initialize class com.hankcs.hanlp.seg.common.Vertex,"Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.seg.common.Vertex
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.hankcs.lucene.SegmentWrapper.next(SegmentWrapper.java:76)
	at com.hankcs.lucene.HanLPTokenizer.incrementToken(HanLPTokenizer.java:67)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:378)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
"
ÂÖ≥‰∫éProjectÊâìÂåÖÁöÑÈóÆÈ¢ò,"ÊàëÂú®È°πÁõÆÈáåÁî®‰∫Ü‰Ω†ÁöÑjiaÂåÖÔºàmavenÊñπÂºèÂíåÁõ¥Êé•‰∏ãËΩΩjarÁöÑÊñπÂºèÔºâÊàëÂú®IDEÈáåËøêË°åÈÉΩÊ≤°ÈóÆÈ¢òÔºåÊàëÊòØÁî®SwingÊ°ÜÊû∂ÂÅö‰∫Ü‰∏™ÂõæÂΩ¢ÂåñÂ∑•ÂÖ∑ÔºåÁÑ∂ÂêéÊâìÂåÖÂÆå‰πãÂêé‰∏ÄÊó¶ËøêË°åÂ∞±Âá∫ÈóÆÈ¢òÔºåÈóÆÈ¢ò‰ø°ÊÅØÊòØÔºö

> Error: A JNI error has occurred, please check your installation and try again
> Exception in thread ""main"" java.lang.SecurityException: Invalid signature file d
> igest for Manifest main attributes
>         at sun.security.util.SignatureFileVerifier.processImpl(Unknown Source)
>         at sun.security.util.SignatureFileVerifier.process(Unknown Source)
>         at java.util.jar.JarVerifier.processEntry(Unknown Source)
>         at java.util.jar.JarVerifier.update(Unknown Source)
>         at java.util.jar.JarFile.initializeVerifier(Unknown Source)
>         at java.util.jar.JarFile.getInputStream(Unknown Source)
>         at sun.misc.URLClassPath$JarLoader$2.getInputStream(Unknown Source)
>         at sun.misc.Resource.cachedInputStream(Unknown Source)
>         at sun.misc.Resource.getByteBuffer(Unknown Source)
>         at java.net.URLClassLoader.defineClass(Unknown Source)
>         at java.net.URLClassLoader.access$100(Unknown Source)
>         at java.net.URLClassLoader$1.run(Unknown Source)
>         at java.net.URLClassLoader$1.run(Unknown Source)
>         at java.security.AccessController.doPrivileged(Native Method)
>         at java.net.URLClassLoader.findClass(Unknown Source)
>         at java.lang.ClassLoader.loadClass(Unknown Source)
>         at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
>         at java.lang.ClassLoader.loadClass(Unknown Source)
>         at sun.launcher.LauncherHelper.checkAndLoadMain(Unknown Source)

ÊàëÂú®ÁΩë‰∏äÊü•‰∫ÜÂæàÂ§öÁßçÊñπÊ≥ï ÈÉΩÊ≤°ÂäûÊ≥ïÔºåÂ∞±ÊòØÂæàÂ§ö‰∫∫ËØ¥‰∫ÜÂú®mavenÊñá‰ª∂ÈáåÂä†Ëøô‰∫õÔºö

```

> <artifact>*:*</artifact>
> <excludes>
>         <exclude>META-INF/*.SF</exclude>
>          <exclude>META-INF/*.DSA</exclude>
>           <exclude>META-INF/*.RSA</exclude>
> </excludes>
```

ÂèØÊòØÊó†ËÆ∫ÊÄé‰πàÊ†∑ÈÉΩ‰∏çË°å„ÄÇËØ∑ÈóÆËøôÊòØ‰ªÄ‰πàÈóÆÈ¢òÔºåÁ≠âÂæÖÂõûÂ§ç„ÄÇ„ÄÇ„ÄÇ"
ÂÖ≥‰∫éTermÁöÑoffsetÂ±ûÊÄßÈóÆÈ¢ò,ËØ∑ÈóÆÊÄé‰πàÂºÄÂêØÂàÜËØçÂô®ÁöÑoffsetÈÄâÈ°πÔºü
ÂÖ≥‰∫éËá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑË∑ØÂæÑÈóÆÈ¢ò,"ÁúãhankcsÁªôÂá∫ÁöÑËá™ÂÆö‰πâÂ≠óÂÖ∏ÁöÑÈÖçÁΩÆÊ†ºÂºèÊòØËøôÊ†∑ÁöÑÔºö
data/dictionary/custom/CustomDictionary.txt;CompanyName.txt;school.txt
‰ΩÜÂÆûÈôÖ‰∏äËøôÊ†∑ÈÖçÁΩÆÂç¥ËØª‰∏çÂà∞ÔºåÁ®ãÂ∫èËøêË°åÊó∂Áõ¥Êé•Êâæ‰∫ÜÊ†πË∑ØÂæÑ+CompanyName.txtÊñá‰ª∂Ôºå
ÊîπÊàêÔºö
data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CompanyName.txt;data/dictionary/custom/school.txt
Ëøô‰∏™Ê†∑Â≠êÁöÑÁªùÂØπË∑ØÂæÑÂ∞±ÂèØ‰ª•ËØªÂà∞‰∫ÜÔºå‰∏çÁü•ÊòØÂì™ÈáåÁöÑÈîôËØØÔºåËøòÊòØÊàëÁêÜËß£ÊúâÂÅèÂ∑ÆÔºåÊúõÊåáÊïô"
‰∏≠ÂõΩ‰∫∫ÂêçËØÜÂà´ÂëΩ‰∏≠ÁéáÊúâÁÇπ‰Ωé,"ÊàëÂú®ÁΩë‰∏äÊâæÂà∞sohu1.9GÁöÑËØ≠ÊñôÔºåÂÖ±110‰∏áÁØáÊñáÁ´†„ÄÇ ÁÑ∂ÂêéÂàÜËØçÔºåÁªüËÆ°ÂêÑËØçÊÄßÁöÑËØç„ÄÇ ÂèëÁé∞‰∫∫ÂêçÂ•ΩÂ§öÊòØÂàÜÈîôÁöÑ„ÄÇheadÈÉ®ÂàÜÁªìÊûúÂ¶Ç‰∏ãÔºö
ÈÉΩÊòØ	nr	50596	20170301
ËΩ¶ÂÜÖ	nr	39683	20170301
ÈÉΩÊúâ	nr	18472	20170301
Êù•Áúã	nr	12741	20170301
ÈÉΩ‰ºö	nr	11236	20170301
ÂÖ®ËΩ¶	nr	9696	20170301
ÂàòÁøî	nr	8981	20170301
ËÉ°Èî¶Ê∂õ	nr	8511	20170301
ÊâçÊòØ	nr	8322	20170301
ÈÉΩÂ∞Ü	nr	8162	20170301
ÈÉΩËÆØ	nr	8150	20170301
ËãèÂÆÅ	nr	8106	20170301
ÊâçËÉΩ	nr	7878	20170301
ÈÉΩËÉΩ	nr	7545	20170301
Âç†ÊØî	nr	7511	20170301
ÈÉΩË¶Å	nr	7469	20170301
Êñá‰∏≠ÊâÄ	nr	7412	20170301
ÂÖ®Á≥ª	nr	7187	20170301
ÊôØÊµ∑Èπè	nr	7184	20170301
Êàø‰ºÅ	nr	7176	20170301
ÊõæÂú®	nr	6558	20170301
È´òÂá∫	nr	6360	20170301
Â≠ôÊù®	nr	6077	20170301
ÂàòÊó∫	nr	5987	20170301
ÂÆâÂçó	nr	5821	20170301
ÂîØÂÜ†	nr	5678	20170301
‰ª§‰∫∫	nr	5602	20170301
ËµõÊâ¨	nr	5477	20170301
Êâç‰ºö	nr	5340	20170301
Â∞èÂæÆ	nr	5263	20170301
Êù®ÂπÇ	nr	5164	20170301
ÂàöÈúÄ	nr	5145	20170301
ÂÆ£‰º†	nr	4587	20170301"
NShortSegmentÂàÜËØç java.lang.OutOfMemoryError: GC overhead limit exceeded,"‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
`NShortSegment segment=new NShortSegment().enableAllNamedEntityRecognize(true).enablePlaceRecognize(true);
						List<Term> terms=segment.seg(text);`

ÊÄªÊòØÂàÜÂà∞ËøôÊÆµËØùÊó∂Âá∫Êù•ÂÜÖÂ≠òÈîôËØØÔºö
‚ÄúÁ¨¨‰∏ÄÈÉ®ÂàÜÈ©¨ÂÖãÊÄù‰∏ª‰πâÂì≤Â≠¶ÂéüÁêÜ„ÄÄ„ÄÄÂ§ç‰π†ÊÄªÊÄùË∑Ø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶1„ÄÄ„ÄÄÁ¨¨‰∏ÄÁ´†È©¨ÂÖãÊÄù‰∏ª‰πâÂì≤Â≠¶ÊòØÁßëÂ≠¶ÁöÑ‰∏ñÁïåËßÇÂíåÊñπÊ≥ïËÆ∫‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶3„ÄÄ„ÄÄÁ¨¨‰∫åÁ´†‰∏ñÁïåÁöÑÁâ©Ë¥®ÊÄßÂíå‰∫∫ÁöÑÂÆûË∑µÊ¥ªÂä®‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶15„ÄÄ„ÄÄÁ¨¨‰∏âÁ´†‰∏ñÁïåÁöÑËÅîÁ≥ª„ÄÅÂèëÂ±ïÂèäÂÖ∂ËßÑÂæã‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶30„ÄÄ„ÄÄÁ¨¨ÂõõÁ´†ËÆ§ËØÜÁöÑÊú¨Ë¥®ÂíåËøáÁ®ã‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶51„ÄÄ„ÄÄÁ¨¨‰∫îÁ´†‰∫∫Á±ªÁ§æ‰ºöÁöÑÊú¨Ë¥®ÂíåÂü∫Êú¨ÁªìÊûÑ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶76„ÄÄ„ÄÄÁ¨¨ÂÖ≠Á´†Á§æ‰ºöÂèëÂ±ïËßÑÂæãÂíåÂéÜÂè≤ÂàõÈÄ†ËÄÖ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶97„ÄÄ„ÄÄÁ¨¨‰∏ÉÁ´†Á§æ‰ºöÂèëÂ±ïÂíå‰∫∫ÁöÑÂèëÂ±ï‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶112„ÄÄ„ÄÄÁ¨¨‰∫åÈÉ®ÂàÜÈ©¨ÂÖãÊÄù‰∏ª‰πâÊîøÊ≤ªÁªèÊµéÂ≠¶ÂéüÁêÜ„ÄÄ„ÄÄÂ§ç‰π†ÊÄªÊÄùË∑Ø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶125„ÄÄ„ÄÄÁ¨¨‰∏ÄÁ´†ÂØºËÆ∫‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶128„ÄÄ„ÄÄÁ¨¨‰∫åÁ´†Á§æ‰ºöÁªèÊµéÂà∂Â∫¶‰∏éÁªèÊµéËøêË°åÁöÑ‰∏ÄËà¨ÂéüÁêÜ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶134„ÄÄ„ÄÄÁ¨¨‰∏âÁ´†ËµÑÊú¨‰∏ª‰πâÁîü‰∫ßÂÖ≥Á≥ªÁöÑÂÆûË¥®ÂèäÂÖ∂ÂèëÂ±ïÈò∂ÊÆµ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶152„ÄÄ„ÄÄÁ¨¨ÂõõÁ´†ËµÑÊú¨ÁöÑËøêË°å‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶183„ÄÄ„ÄÄÁ¨¨‰∫îÁ´†Á§æ‰ºö‰∏ª‰πâÁîü‰∫ßÂÖ≥Á≥ªÁöÑÂÆûË¥®‰∏éÁªèÊµéÂà∂Â∫¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶199„ÄÄ„ÄÄÁ¨¨ÂÖ≠Á´†Á§æ‰ºö‰∏ª‰πâÂ∏ÇÂú∫ÁªèÊµé‰ΩìÂà∂ÂíåÁªèÊµéËøêË°å‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶211„ÄÄ„ÄÄÁ¨¨‰∏ÉÁ´†ÁªèÊµéÂÖ®ÁêÉÂåñ‰∏éÂõΩÈôÖÁªèÊµéÂÖ≥Á≥ª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶224„ÄÄ„ÄÄÁ¨¨‰∏âÈÉ®ÂàÜÊØõÊ≥Ω‰∏úÊÄùÊÉ≥Ê¶ÇËÆ∫„ÄÄ„ÄÄÂ§ç‰π†ÊÄªÊÄùË∑Ø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶231„ÄÄ„ÄÄÁ¨¨‰∏ÄÁ´†ÊØõÊ≥Ω‰∏úÊÄùÊÉ≥ÊòØÈ©¨ÂÖãÊÄù‰∏ª‰πâ‰∏≠ÂõΩÂåñÁöÑÁêÜËÆ∫ÊàêÊûú‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶234„ÄÄ„ÄÄÁ¨¨‰∫åÁ´†Êñ∞Ê∞ë‰∏ª‰∏ª‰πâÈù©ÂëΩÁöÑÊÄªË∑ØÁ∫øÂíåÂü∫Êú¨Á∫≤È¢Ü‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶241„ÄÄ„ÄÄÁ¨¨‰∏âÁ´†Êñ∞Ê∞ë‰∏ª‰∏ª‰πâÈù©ÂëΩÁöÑÂü∫Êú¨ÈóÆÈ¢ò‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶259„ÄÄ„ÄÄÁ¨¨ÂõõÁ´†Á§æ‰ºö‰∏ª‰πâÊîπÈÄ†ÁöÑÁêÜËÆ∫ÂéüÂàô‰∏éÁªèÈ™åÊÄªÁªì‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶276„ÄÄ„ÄÄÁ¨¨‰∫îÁ´†Á§æ‰ºö‰∏ª‰πâËã•Âπ≤ÈáçÂ§ßÁêÜËÆ∫ÈóÆÈ¢òÁöÑÊé¢Á¥¢ÊàêÊûú‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶285„ÄÄ„ÄÄÁ¨¨ÂÖ≠Á´†Á§æ‰ºö‰∏ª‰πâÂª∫ËÆæÁöÑÊñπÈíàÊîøÁ≠ñ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶296„ÄÄ„ÄÄÁ¨¨‰∏ÉÁ´†ÊéåÊè°ÊØõÊ≥Ω‰∏úÊÄùÊÉ≥ÁöÑÊ¥ªÁöÑÁÅµÈ≠ÇÔºåÂùöÊåÅÂíåÂèëÂ±ïÊØõÊ≥Ω‰∏úÊÄùÊÉ≥‚Ä¶‚Ä¶‚Ä¶‚Ä¶310„ÄÄ„ÄÄÁ¨¨ÂõõÈÉ®ÂàÜÈÇìÂ∞èÂπ≥ÁêÜËÆ∫Âíå‚Äú‰∏â‰∏™‰ª£Ë°®‚ÄùÈáçË¶ÅÊÄùÊÉ≥Ê¶ÇËÆ∫„ÄÄ„ÄÄÂ§ç‰π†ÊÄªÊÄùË∑Ø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶323„ÄÄ„ÄÄÁ¨¨‰∏ÄÁ´†ÈÇìÂ∞èÂπ≥ÁêÜËÆ∫ÊòØÂΩì‰ª£‰∏≠ÂõΩÁöÑÈ©¨ÂÖãÊÄù‰∏ª‰πâ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶327„ÄÄ„ÄÄÁ¨¨‰∫åÁ´†‚Äú‰∏â‰∏™‰ª£Ë°®‚ÄùÈáçË¶ÅÊÄùÊÉ≥ÊòØÈ©¨ÂÖãÊÄù‰∏ª‰πâ‰∏≠ÂõΩÂåñÁöÑÊúÄÊñ∞ÁêÜËÆ∫ÊàêÊûú‚Ä¶332„ÄÄ„ÄÄÁ¨¨‰∏âÁ´†Ëß£ÊîæÊÄùÊÉ≥„ÄÅÂÆû‰∫ãÊ±ÇÊòØ„ÄÅ‰∏éÊó∂‰ø±Ëøõ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶341„ÄÄ„ÄÄÁ¨¨ÂõõÁ´†Á§æ‰ºö‰∏ª‰πâÁöÑÊú¨Ë¥®ÂíåÊ†πÊú¨‰ªªÂä°‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶348„ÄÄ„ÄÄÁ¨¨‰∫îÁ´†Á§æ‰ºö‰∏ª‰πâÂàùÁ∫ßÈò∂ÊÆµÂíåÂÖöÁöÑÂü∫Êú¨Ë∑ØÁ∫ø„ÄÅÂü∫Êú¨Á∫≤È¢Ü‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶355„ÄÄ„ÄÄÁ¨¨ÂÖ≠Á´†ÁßëÂ≠¶ÂèëÂ±ïËßÇÂíåÁ§æ‰ºö‰∏ª‰πâÂª∫ËÆæÁöÑÂèëÂ±ïÊàòÁï•‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶364„ÄÄ„ÄÄÁ¨¨‰∏ÉÁ´†‰∏≠ÂõΩÁâπËâ≤Á§æ‰ºö‰∏ª‰πâÁªèÊµé‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶389„ÄÄ„ÄÄÁ¨¨ÂÖ´Á´†‰∏≠ÂõΩÁâπËâ≤Á§æ‰ºö‰∏ª‰πâÊîøÊ≤ª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶402„ÄÄ„ÄÄÁ¨¨‰πùÁ´†‰∏≠ÂõΩÁâπËâ≤Á§æ‰ºö‰∏ª‰πâÊñáÂåñ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶415„ÄÄ„ÄÄÁ¨¨ÂçÅÁ´†‚Äú‰∏ÄÂõΩ‰∏§Âà∂‚ÄùÂíåÂÆûÁé∞Á•ñÂõΩÁöÑÂÆåÂÖ®Áªü‰∏Ä‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶424„ÄÄ„ÄÄÁ¨¨ÂçÅ‰∏ÄÁ´†Áª¥Êä§‰∏ñÁïåÂíåÂπ≥Ôºå‰øÉËøõÂÖ±ÂêåÂèëÂ±ï‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶430„ÄÄ„ÄÄÁ¨¨ÂçÅ‰∫åÁ´†‰∏≠ÂõΩÁâπËâ≤Á§æ‰ºö‰∏ª‰πâ‰∫ã‰∏öÁöÑ‰æùÈù†ÂäõÈáèÂíåÈ¢ÜÂØºÊ†∏ÂøÉ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶437„ÄÄ„ÄÄÁ¨¨‰∫îÈÉ®ÂàÜÂΩì‰ª£‰∏ñÁïåÁªèÊµé‰∏éÊîøÊ≤ª„ÄÄ„ÄÄÂ§ç‰π†ÊÄªÊÄùË∑Ø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶446„ÄÄ„ÄÄÁ¨¨‰∏ÄÁ´†ÂΩì‰ª£‰∏ñÁïåÁªèÊµéÁöÑÂèëÂ±ïÂèòÂåñ‰∏éÂü∫Êú¨Ë∂ãÂäø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶449„ÄÄ„ÄÄÁ¨¨‰∫åÁ´†ÂΩì‰ª£‰∏ñÁïåÊîøÊ≤ªÁöÑÂèëÂ±ïÂèòÂåñ‰∏éÂü∫Êú¨Ë∂ãÂäø‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶462„ÄÄ„ÄÄÁ¨¨‰∏âÁ´†ÂΩì‰ªäÊó∂‰ª£‰∏ªÈ¢ò‰∏éÂª∫Á´ãÂõΩÈôÖÊñ∞Áß©Â∫è‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶479„ÄÄ„ÄÄÁ¨¨ÂõõÁ´†ÊàòÂêéÂèëËææËµÑÊú¨‰∏ª‰πâÂõΩÂÆ∂ÁöÑÁªèÊµé‰∏éÊîøÊ≤ª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶487„ÄÄ„ÄÄÁ¨¨‰∫îÁ´†ÊàòÂêéÂèëÂ±ï‰∏≠ÂõΩÂÆ∂ÁöÑÊîøÊ≤ª‰∏éÁªèÊµé‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶502„ÄÄ„ÄÄÁ¨¨ÂÖ≠Á´†ÊàòÂêéÁ§æ‰ºö‰∏ª‰πâÂõΩÂÆ∂ÁöÑÁªèÊµé‰∏éÊîøÊ≤ª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶210„ÄÄ„ÄÄÁ¨¨‰∏ÉÁ´†Áã¨ËÅî‰ΩìÊàêÂëòÂõΩ‰∏éÂÜ∑ÊàòÂêé‰∏úÊ¨ßÂõΩÂÆ∂ÁöÑÁªèÊµé‰∏éÊîøÊ≤ª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶518„ÄÄ„ÄÄÁ¨¨ÂÖ´Á´†‰∏≠ÂõΩÂØπÂ§ñÂÖ≥Á≥ªÂèäÂú®‰∏ñÁïå‰∏äÁöÑÂú∞‰Ωç‰∏é‰ΩúÁî®‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶526Êõ¥Â§ö‰ø°ÊÅØËØ∑ËÆøÈóÆÔºöÊñ∞Êµ™ËÄÉÁ†îÈ¢ëÈÅìËÄÉÁ†îËÆ∫ÂùõËÄÉÁ†îÂçöÂÆ¢Âúà„ÄÄ„ÄÄÁâπÂà´ËØ¥ÊòéÔºöÁî±‰∫éÂêÑÊñπÈù¢ÊÉÖÂÜµÁöÑ‰∏çÊñ≠Ë∞ÉÊï¥‰∏éÂèòÂåñÔºåÊñ∞Êµ™ÁΩëÊâÄÊèê‰æõÁöÑÊâÄÊúâËÄÉËØï‰ø°ÊÅØ‰ªÖ‰æõÂèÇËÄÉÔºåÊï¨ËØ∑ËÄÉÁîü‰ª•ÊùÉÂ®ÅÈÉ®Èó®ÂÖ¨Â∏ÉÁöÑÊ≠£Âºè‰ø°ÊÅØ‰∏∫ÂáÜ„ÄÇ‚Äù

"
HanLP.propertiesÊñá‰ª∂ÈúÄË¶ÅËá™Â∑±ÂÜôÂêóÔºü,ÊòØËá™Â∑±ÂàõÂª∫Ê∑ªÂä†root ÈÖçÁΩÆ ËøòÊúâÂÖ∂‰ªñÁöÑÈÖçÁΩÆÂêóÔºü Êâæ‰∏çÂà∞ÂèÇËÄÉÊñá‰ª∂
elasticsearch-analysis-hanlpÊ∑ªÂä†Ëá™ÂÆö‰πâÂÅúÁî®ËØçÂΩ±Âìç‰∫ÜÊ≠£Â∏∏ËØçÊ±áÁöÑÂàÜËØçÊïàÊûú,"ÊàëÁúãËøô‰∏™Êèí‰ª∂ÔºåÁî®ÁöÑÂ•ΩÂÉèÊòØhanlpÁöÑÁª¥ÁâπÊØîÁÆóÊ≥ïËøõË°åÂàÜËØçÁöÑÔºü
Êàë‰øÆÊîπ‰∫ÜÂÅúÁî®ËØçÂ∫ìÔºåÂà†Èô§‰∫ÜÈªòËÆ§Êñá‰ª∂ÂÜÖÂÆπÔºåÊ∑ªÂä†‰∫ÜËá™Â∑±ÂÆö‰πâÁöÑÂá†‰∏™ËØçÔºå‰∏∫Âï•ÂéüÊù•ËÉΩÂàÜÁöÑËØçÔºà‰∏çÂåÖÂê´Ëá™ÂÆö‰πâÂÅúÁî®ËØçÔºâÂä†ÂÆåËøô‰∏™‰πãÂêéÂ∞±ÂàÜ‰∏ç‰∫Ü‰∫ÜÂë¢"
Â¶Ç‰ΩïËøáÊª§ÂÅúÁî®ËØç,ÁúãÂà∞‰∫ÜÂÅúÁî®ËØçËØçÂÖ∏Ôºå‰ΩÜÊòØ‰∏çÁü•ÈÅìÂ¶Ç‰ΩïËøáÊª§ÂÅúÁî®ËØçÂë¢Ôºü
ÂÅúËØç,"ËØ∑ÈóÆÔºåÊàëÂú®Ê®°ÊùøÈáåÊâæ‰∏çÂà∞ÂÅúËØçÁöÑ‰æãÂ≠ê
Â¶Ç‰Ωï‰ΩøÁî®ÂÅúËØçÂäüËÉΩÂë¢"
Âä®ÊÄÅÂä†ËΩΩËØçÂ∫ì,"Â§ßÁ•û‰Ω†Â•ΩÔºåËØ∑ÈóÆ‰∏Ä‰∏ãÔºåËøô‰∏™ÂèØ‰ª•ÊîØÊåÅ    Êèê‰æõËøúÁ®ãÂä†ËΩΩÈÖçÁΩÆÔºåËÆæÁΩÆÁõëÊéßÔºåËØçÂÖ∏Êõ¥Êñ∞Ëá™Âä®Âä†ËΩΩÔºåÊó†ÈúÄÈáçÂêØ„ÄÇ

Á±ª‰ººËøôÁßçÂÅöÊ≥ïÔºö https://github.com/medcl/elasticsearch-analysis-ik/pull/40   Ëøô‰∏™ÂèØ‰ª•Âú®ËøôËæπÂÆûÁé∞ÂêóÔºü

ËøôÊ†∑Â∞±‰∏çÈúÄË¶ÅÊØèÊ¨°Êõ¥Êñ∞ËØçÂÖ∏ÔºåÂèàË¶ÅÈáçÊñ∞ÂèëÂ∏ÉÂà∞Áîü‰∫ßÁéØÂ¢É‰∫Ü„ÄÇ "
Êüê‰∏™Âè•Â≠êÂàÜËØçÊúâËØØ,"ÊµãËØï‰∫ÜÂè•Â≠êÔºöÂØπÊïå‰∫∫ÂæàÊúâÁî®  Âíå ÂØπÊïåÊñπÂæàÊúâÁî®„ÄÇ
ÂâçËÄÖÂàÜËØçÈîôËØØÔºåÂêéËÄÖÊ≠£Á°Æ„ÄÇ
ÂâçËÄÖË¢´ÂàÜÊàê‰∫Ü  ‚ÄúÂØπÊïå  ‰∫∫  Âæà ÊúâÁî®‚ÄùÔºõÂêéËÄÖÂàÜÊàê‰∫Ü ‚ÄúÂØπ  ÊïåÊñπ Âæà  ÊúâÁî®‚Äù  

Âè¶Ôºö‚ÄúÊúâÁî®‚Äù‰∏∫Ëá™ÂÆö‰πâËØçÂÖ∏Ê∑ªÂä†ÁöÑÔºåÁî®ÁöÑCoreNatureDictionary.mini.txtÔºåËØ∑ÈóÆ‰∏ãÂì™ÈáåÊúâ ÈùûminiÁâàÊú¨ÁöÑËØçÂ∫ì‰∏ãËΩΩÔºü

ÊÑüË∞¢‰ΩúËÄÖ„ÄÇ"
"Êñ∞Â≠óË©û‰∏≠Âê´Êúâ""space""","ÊàëÂú®Ëá™ÂÆöÁæ©Ëæ≠ÂÖ∏‰∏≠ 
data/dictionary/custom/myDic.txt 
Êñ∞Â¢ûÂ≠óË©û
`Á¥ÖÁ±≥note 4x  nm`

‰ΩÜÂú®Âª∫Á´ã CustomDictionary.txt.bin ÊôÇ Á≥ªÁµ±Â†±ÈåØ
ÊòØÂõ†ÁÇ∫Ê†ºÂºèÂøÖÈ†àÁÇ∫
[Êñ∞Â≠óË©û][tab][POS tagging]
Êñ∞Â≠óË©û‰∏≠‰∏çÂæóÊúâ space?

Â¶ÇÊûúÊòØÈÄôÊ®£ÁöÑË©±
""Á¥ÖÁ±≥note 4x"", ""mackbook pro""
Ë©≤Â¶Ç‰ΩïÂä†ÂÖ•ÁÇ∫ Êñ∞Â≠óË©û?
"
demoÊºîÁ§∫ÁªìÊûúÂíåÂÆûÈôÖËøêË°åÁªìÊûú‰∏çÂêå,"demoÊüêÊÆµÁªìÊûú‰∏∫Ôºö
[Ê¨¢Ëøé/v, Êñ∞/a, ËÄÅ/a, Â∏àÁîü/n, ÂâçÊù•/vi, Â∞±È§ê/vi]
[Â∑•‰ø°Â§Ñ/nt, Â•≥Âπ≤‰∫ã/n, ÊØèÊúà/r, ÁªèËøá/p, ‰∏ãÂ±û/v, ÁßëÂÆ§/n, ÈÉΩ/d, Ë¶Å/v, ‰∫≤Âè£/d, ‰∫§‰ª£/v, 24/m, Âè£/n, ‰∫§Êç¢Êú∫/n, Á≠â/udeng, ÊäÄÊúØÊÄß/n, Âô®‰ª∂/n, ÁöÑ/ude1, ÂÆâË£Ö/v, Â∑•‰Ωú/vn]
[ÈöèÁùÄ/p, È°µÊ∏∏/nz, ÂÖ¥Ëµ∑/v, Âà∞/v, Áé∞Âú®/t, ÁöÑ/ude1, È°µÊ∏∏/nz, ÁπÅÁõõ/a, Ôºå/w, ‰æùËµñ‰∫é/v, Â≠òÊ°£/vi, ËøõË°å/vn, ÈÄªËæë/n, Âà§Êñ≠/v, ÁöÑ/ude1, ËÆæËÆ°/vn, ÂáèÂ∞ë/v, ‰∫Ü/ule, Ôºå/w, ‰ΩÜ/c, Ëøô/rzv, Âùó/q, ‰πü/d, ‰∏çËÉΩ/v, ÂÆåÂÖ®/ad, ÂøΩÁï•/v, Êéâ/v, „ÄÇ/w]
[‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊäÄÊúØÁ†îÁ©∂ÊâÄ/nt, ÁöÑ/ude1, ÂÆóÊàêÂ∫Ü/nr, ÊïôÊéà/nnt, Ê≠£Âú®/d, ÊïôÊéà/v, Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ/nz, ËØæÁ®ã/n]
ÂÆûÈôÖËøêË°åÁªìÊûú‰∏∫Ôºö
[Ê¨¢Ëøé/v, Êñ∞/a, ËÄÅÂ∏à/n, ÁîüÂâç/t, Êù•/v, Â∞±È§ê/v]
[Â∑•‰ø°Â§Ñ/n, Â•≥/b, Âπ≤‰∫ã/n, ÊØèÊúà/r, ÁªèËøá/p, ‰∏ãÂ±û/v, ÁßëÂÆ§/n, ÈÉΩË¶Å/nr, ‰∫≤Âè£/d, ‰∫§‰ª£/v, 24/m, Âè£/q, ‰∫§Êç¢Êú∫/n, Á≠â/u, ÊäÄÊúØÊÄß/n, Âô®‰ª∂/n, ÁöÑ/uj, ÂÆâË£Ö/v, Â∑•‰Ωú/vn]
[ÈöèÁùÄ/p, È°µ/q, Ê∏∏ÂÖ¥/n, Ëµ∑/v, Âà∞/v, Áé∞Âú®/t, ÁöÑ/uj, È°µÊ∏∏/nz, ÁπÅÁõõ/an, Ôºå/w, ‰æùËµñ‰∫é/v, Â≠òÊ°£/vn, ËøõË°å/v, ÈÄªËæë/n, Âà§Êñ≠/v, ÁöÑ/uj, ËÆæËÆ°/vn, ÂáèÂ∞ë/v, ‰∫Ü/ul, Ôºå/w, ‰ΩÜ/c, ËøôÂùó/r, ‰πü/d, ‰∏çËÉΩ/v, ÂÆåÂÖ®/ad, ÂøΩÁï•/v, Êéâ/v, „ÄÇ/w]
[‰∏≠ÂõΩÁßëÂ≠¶Èô¢/n, ËÆ°ÁÆó/v, ÊäÄÊúØ/n, Á†îÁ©∂ÊâÄ/n, ÁöÑ/uj, ÂÆóÊàêÂ∫Ü/nr, ÊïôÊéà/n, Ê≠£Âú®/d, ÊïôÊéà/n, Ëá™ÁÑ∂/d, ËØ≠Ë®Ä/n, Â§ÑÁêÜ/v, ËØæÁ®ã/n]"
Ëá™ÂÆö‰πâËØçÂÖ∏Ê∏ÖÁ©∫,ÊàëËøôÊúâ‰∏™ËØçÂ∫ìÊòØÂú®Êï∞ÊçÆÂ∫ì‰∏äÔºåÊúâ‰∫∫‰∏ÄÁõ¥Âú®Áª¥Êä§Â¢ûÂà†ËØçÔºåÊàëÁé∞Âú®ÊòØÈÄöËøácom.hankcs.hanlp.dictionary.CustomDictionary#add ÊñπÊ≥ïÂä®ÊÄÅÂä†ËΩΩÁöÑÔºåÁé∞Âú®Êúâ‰∏™ÈúÄÊ±ÇÊòØÂà∑Êñ∞ÈáçÊñ∞Âä†ËΩΩËøô‰∏™ËØçÂÖ∏ÔºåËØ∑ÈóÆÊúâ‰ªÄ‰πàÊñπÊ≥ïÂêóÔºüÊòØ‰∏çÊòØÂè™Ë¶ÅCustomDictionary.loadMainDictionary Ëøô‰∏™ÊñπÊ≥ïÂ∞±ÂèØ‰ª•ÈáçËΩΩËøô‰∏™Ëá™ÂÆö‰πâËØçÂÖ∏Ôºü
Êú∫ÊûÑÂêçËØÜÂà´ÈîôËØØ,"ÊÇ®Â•ΩÔºÅËØ∑Êïô‰∏Ä‰∏™HanLPÂàÜËØçÁöÑÈóÆÈ¢ò„ÄÇ""‰∏∫ÂπøÂ§ßËøêÁª¥ËÄÖÊâÄÂñúÁà±"",ÂØπ‰∫éËøôÂè•ËØùÁöÑ‚ÄúÊâÄ‚ÄùÔºåÂ∫îËØ•ÊòØË¢´Âä®ÁöÑÊÑèÊÄùÔºå""ËøêÁª¥ËÄÖÊâÄ""Ë¢´ËØÜÂà´Êàê‰∫ÜÊú∫ÊûÑÂêç„ÄÇËØ∑Êïô‰∏ãËøô‰∏™‰∏úË•øÊÄé‰πà‰ºòÂåñÂ•Ω?
Ë°•ÂÖÖÔºöËøôÈáåHanLPÁªèËøáÊ†∏ÂøÉËØçÂÖ∏ÂíåÁî®Êà∑ËØçÂÖ∏ÂàùÂàÜËØçÂêéÔºåÂàÜÊàê‰∫ÜËøêÁª¥/ËÄÖ/ÊâÄ‰∏â‰∏™ËØçÔºå‰ΩÜÊòØÂú®Êú∫ÊûÑÂêçËØÜÂà´ÁöÑËøáÁ®ã‰∏≠ÔºåÊú∫ÊûÑÂêçÊ†áÊ≥®ÊòØ‰∏çÊÄé‰πàÁÆ°ÂàùÂàÜËØçÊÄßÂíåÂâçÂêéËØ≠‰πâÂÖ≥Á≥ªÁöÑÔºåÂè™Ë¶ÅÁ¨¶ÂêàÊú∫ÊûÑÂêçÁöÑÊ®°ÂºèÂåπÈÖçÔºåÂ∞±‰ºö‰∏çÁÆ°‰∏â‰∏É‰∫åÂçÅ‰∏ÄÂΩí‰∏∫Êú∫ÊûÑÂêç„ÄÇÊú¨‰∫∫ÈÄöËøáÊ∫ê‰ª£Á†ÅÁêÜËß£ËøôÈáåÊòØ‰∏Ä‰∏™Áº∫Èô∑Ôºå‰∏çÁü•ÈÅìÊúâÊ≤°ÊúâÁêÜËß£ÈîôËØØ„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÁêÜËß£ÈîôËØØÔºåÊúâÊ≤°Êúâ‰ªÄ‰πàÂ•ΩÁöÑËß£ÂÜ≥ÂäûÊ≥ïÔºåÂõ†‰∏∫Êó†ËÆ∫ÊòØ‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÊú∫ÊûÑÂêçËØÜÂà´ÈÉΩÂ•ΩÔºåÈÉΩÊòØÁ±ª‰ººÁöÑÁº∫Èô∑ÔºåÂØπ‰∏Ä‰∫õÊñáÊú¨Â∞§ÂÖ∂ÊòØÁâπÂÆöÈ¢ÜÂüüÁöÑÊñáÊú¨ËØÜÂà´ÂáÜÁ°ÆÁéáÊòØÂæà‰ΩéÁöÑ„ÄÇÊúõËµêÊïôÔºåË∞¢Ë∞¢ÔºÅ"
ÂÖ≥‰∫édemo,ÂèØ‰∏çÂèØ‰ª•Êèê‰æõÊõ¥Â§öÂÖ≥‰∫étrainÂíåevaluateÁöÑdemoÔºüËøôÊ†∑Áî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôËøõË°åËÆ≠ÁªÉ„ÄÇwiki‰∏≠ÁªôÂá∫ÁöÑdemo‰∏çÂ§üÁ≥ªÁªü„ÄÇ
ÂÖ≥‰∫éÂêçÁß∞ËØÜÂà´,@Áé∞Âú®Ê†πÊçÆÂàÜËØç‰ΩøÁî®ÁöÑÊñπÂºèÔºåÊ†πÊçÆÁªÑËØçÁöÑÊñπÂºèÁªÑËØçÂêéËøõË°åÂØπÂêçÁß∞ÁöÑÂà§Âà´ÔºåËøôÊ†∑Â∞±Â≠òÂú®‰∏Ä‰∏™Â±ÄÈôêÊÄß„ÄÇÂ∞±ÊãøÊàëÁé∞Âú®Âú®‰∏Ä‰∏™ÊñáÊú¨‰∏≠ÊòØËØÜÂà´Êú™Âú®ËØçÂ∫ì‰∏≠Ê∑ªÂä†ÈÇ£ÁªÑ‚Äò‰ºÅ‰∏öÂêçÁß∞‚ÄòÁöÑËØçÂØπ‰ºÅ‰∏öÂêçÁß∞ËØÜÂà´Êó∂ÔºåÂæàÂÆπÂá∫Áé∞ËØÜÂà´Âá∫‰∏ÄÂçäÁîöËá≥‰∏çËÉΩËØÜÂà´ÔºåÂ¶ÇÊûúÊòØ‰∏ÄÁõ¥ÈÄöËøáÊ∑ªÂä†ËØçÊù•Ëß£ÂÜ≥Ëøô‰πü‰ºöÂØºËá¥Êñá‰ª∂Ë∂äÊù•Ë∂äÂ§ß„ÄÇÊÉ≥ËØ∑ÈóÆ‰∏Ä‰∏ãËøô‰∏™Êúâ‰ªÄ‰πàÂ•ΩÁöÑÂª∫ËÆÆÊàñËÄÖËß£ÂÜ≥ÊñπÊ≥ïÔºåÊù•Ë°•Ë∂≥Ëøô‰∏™Áº∫Èô∑„ÄÇ
ËØçÂÖ∏ÂàÜÂâ≤Á¨¶ÔºàÁ©∫Ê†ºÔºâÂØπ‰∫éËã±ÊñáÂÆû‰ΩìÁöÑÈóÆÈ¢ò,"‰æãÂ¶Çorganization.txtÁöÑÁ¨¨‰∏ÄÊù°Êï∞ÊçÆ
Valor Capital Group	nhy	1
Ëã±ÊñáÂÆû‰ΩìÂÜÖÂ≠òÂú®Á©∫Ê†ºÔºå‰ªéËÄåÂØºËá¥ËØçÂÖ∏ËØªÂèñÈîôËØØ
Âª∫ËÆÆ‰øÆÊîπÊñπÂºè:
Ôºë„ÄÅÔºàÂº∫ÁÉàÊé®ËçêÔºâËß£ÊûêËØçÂÖ∏ÊñπÂºèÔºöÂèñÂÄíÊï∞Á¨¨‰∏Ä‰∏™‰∏∫ËØçÈ¢ë„ÄÅÂÄíÊï∞Á¨¨‰∫å‰∏™‰∏∫ËØçÊÄß„ÄÅÂÖ∂ÂÆÉ‰∏∫ËØç
Ôºí„ÄÅÂ∞ÜËØçÁöÑÂàÜÈöîÁ¨¶ËÆæÁΩÆ‰∏∫Èõ∂ÂÆΩÂ≠óÁ¨¶Á≠âÂá†‰πé‰∏çÂèØËÉΩÂú®ÂÆûÈôÖÊñáÊú¨‰∏≠Âá∫Áé∞ÁöÑËØç"
Ëá™ÂÆö‰πâËØçÁöÑËØçÂΩ¢ÂàÜËØçÂêéÂá∫Áé∞ÈîôËØØ,"Âåó‰∫¨ÊãâÊâãÁΩëÁªúÊäÄÊúØÊúâÈôêÂÖ¨Âè∏ ntc 1  ËØçÂÖ∏Ê†áÊ≥®ntc

‰ª£Á†ÅÊâìÂç∞‰πüÊòØntc 1        System.out.println(LexiconUtility.getAttribute(""Âåó‰∫¨ÊãâÊâãÁΩëÁªúÊäÄÊúØÊúâÈôêÂÖ¨Âè∏""));

‰ΩÜÊòØÂú®‰∏Ä‰∏™ÁÆÄÂéÜÊñáÊú¨ÈáåÈù¢ÂÅö‰∫ÜÂàÜËØçÔºå ËôΩÁÑ∂ËØÜÂà´Âá∫‰∫ÜËøô‰∏™ÂÆû‰ΩìÔºå‰ΩÜÊòØËØçÊÄßÂèòÊàê‰∫Ünt„ÄÇ„ÄÇ
 
""Â∑•‰ΩúÊèèËø∞Ôºö\tÂ∑•‰ΩúËÅåË¥£\n"" +
                ""Ë¥üË¥£ÊãâÊâãÁΩëÂêéÂè∞Á≥ªÁªü‰∫ßÂìÅÔºàPCÁ´ØÂíåÊó†Á∫øÁ´ØÔºâ,ÂåÖÊã¨ÔºöÂæÖÂäû‰∫ãÈ°π„ÄÅ‰ªªÂä°ÁÆ°ÁêÜ„ÄÅ‰æõÂ∫îÂïÜÁÆ°ÁêÜ„ÄÅÂêàÂêåÁÆ°ÁêÜ„ÄÅÂõ¢Ë¥≠ÂçïÁÆ°ÁêÜ„ÄÅÂèòÊõ¥‰∏éÁâπÊâπ„ÄÅÂÆ¢Êúç‰∏≠ÂøÉ„ÄÅÊï∞ÊçÆ‰∏≠ÂøÉ\n"" +"
ES‰ΩøÁî®portalÈªòËÆ§ËØçÂÖ∏ÔºåÊ∑ªÂä†Ëá™ÂÆö‰πâÂÅúÁî®ËØç‰∏çËµ∑‰ΩúÁî®,Ê±ÇÈóÆÂ§ßÁ•ûÂ¶Ç‰ΩïËß£ÂÜ≥Âë¢
Â¶Ç‰ΩïÂú®ÊñáÊú¨‰∏≠ÊèêÂèñËØÜÂà´ÁöÑÂú∞ÂùÄÔºåÂÖ¨Âè∏ÂêçÁß∞ÔºåÈÇÆÁÆ±Á≠â„ÄÇÊèêÂèñÊñπÊ≥ïÊúâÂì™‰∫õÔºü,"![image](https://cloud.githubusercontent.com/assets/19337606/23128473/0a8952de-f7ba-11e6-8640-e72ff313db35.png)
"
add nGram distance function between strings,"fork you code,change jdk1.8 ,do some code optimize,want to join the project.
the function is in [getNGRAMDistance in EditDistance.java ](https://github.com/DusonWang/HanLP/blob/master/src/main/java/com/hankcs/hanlp/algoritm/EditDistance.java) "
Âä†ËΩΩÁî®Êà∑ËØçÂÖ∏Êó∂Âá∫Áé∞Êï∞ÁªÑÊ∫¢Âá∫ÈîôËØØÔºåÊàëÂä†ËΩΩ‰∫ÜËá™Â∑±ÁöÑË∑ØÂæÑ‰∏∫root=D:/LearnHanLP/data/ CustomDictionaryPath=dictionary/custom/CustomDictionary.txt; economic.txt; stork.txt; ,"‰∫åÊúà 13, 2017 5:38:41 ‰∏ãÂçà com.hankcs.hanlp.HanLP$Config$1 getProperty
Ë≠¶Âëä: root=D:/LearnHanLP/data/ Ëøô‰∏™ÁõÆÂΩï‰∏ãÊ≤°Êúâdata
‰∫åÊúà 13, 2017 5:38:41 ‰∏ãÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñD:/LearnHanLP/data/dictionary/custom/CustomDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: D:\LearnHanLP\data\dictionary\custom\CustomDictionary.txt.bin (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
‰∫åÊúà 13, 2017 5:38:41 ‰∏ãÂçà com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
Ë≠¶Âëä: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:288)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:55)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	at com.nlp.ner.Ner.main(Ner.java:29)"
ÂæàÂ§ö‰∫∫ÂêçnrË¢´Ê†áËÆ∞Áä∂ÊÄÅËØçzgÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢,"ÊØîÂ¶Ç
[ÊåâÁÖß/dg, ‰π†ËøëÂπ≥/zg, ÊåáÁ§∫/yg, Âíå/pbei, ÊùéÂÖãÂº∫/zg, Ë¶ÅÊ±Ç/yg, Ôºå/xx, Â§ñ‰∫§ÈÉ®/ntcb, Âíå/pbei, ÊàëÂõΩ/yg, È©ª/s, È©¨Êù•Ë•ø‰∫ö/nr2, ‰ΩøÈ¢ÜÈ¶Ü/yg, Âä†Âº∫/s, Âêå/dg

ÊàëÁúã‰∫Ü‰∏ãËÆ≠ÁªÉÂ≠óÂÖ∏ÈáåÈù¢Ôºå‰π†ËøëÂπ≥ÂíåÊùéÂÖãÂº∫ÈÉΩÊòØnr"
ËØ∑ÈóÆ‰ΩúËÄÖÂ¶Ç‰ΩïÁîüÊàêËá™Â∑±ÁöÑstopwords.txt.binÔºü,"Áî±‰∫éÂ∑•‰ΩúÈúÄË¶ÅÔºåÊàëË¶ÅÂæÄÂÅúÁî®ËØçË°®data\dictionary\stopwords.txt‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâÁöÑËØçÔºåÊ∑ªÂä†ÂêéÔºåÂà†Èô§‰∫ÜÂéüÂÖàÁöÑstopwords.txt.binÂêéÔºåÂêØÂä®È°πÁõÆÔºåÂèëÁé∞‰∏çËÉΩÂÉèÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÈÇ£Ê†∑Ëá™Âä®ÁîüÊàêÊñ∞ÁöÑstopwords.txt.bin„ÄÇ

ËÄå‰∏îÔºåÊäästopwords.txt.binÂà†Èô§ÊéâÂêéÔºåÂéüÂàô‰∏äÂÅúÁî®ËØçÂ∫îËØ•ÊòØËÉΩË¢´ÂàÜÂá∫Êù•ÁöÑÂØπÂêßÔºå‰ΩÜÂàÜËØçÂêéÔºåÂèëÁé∞ÂÅúÁî®ËØç‰ªçÁÑ∂ÊòØËµ∑‰ΩúÁî®ÁöÑÔºåÂç≥‰ªçÁÑ∂ÊòØÂàÜ‰∏çÂá∫Êù•ÁöÑ„ÄÇ

ÊàëÊÉ≥ÈóÆÔºö
1. Â¶Ç‰ΩïÁîüÊàêËá™Â∑±ÂÆö‰πâÁöÑÂÅúÁî®ËØçbinÊñá‰ª∂Ôºöstopwords.txt.bin
2. ‰∏∫‰ªÄ‰πàÂà†Èô§Êéâstopwords.txt.binÂêéÔºåÂÅúÁî®ËØç‰æùÁÑ∂Ëµ∑‰ΩúÁî®„ÄÇ

Ë∞¢Ë∞¢‰ΩúËÄÖËß£Á≠î„ÄÇ"
Integrate HanLP to KNIME Error,"    Âú®‰∏ãËØïÁùÄ‰∫é Knime ÁöÑ Java Snippet Node ‰ΩøÁî® HanLP, ‰ΩÜÂèëÁé∞Ë∑ëÂà∞
sentenceList = HanLP.extractSummary(document, 3);
Â∞±ÈÄ†Êàê Knime ""Èó™ÈÄÄ"", ÊòØÂê¶ÊàëÁ®ãÂºèÂì™Ë£°ÂÜôÈîô?
/************************************************************************************************************/
// Your custom imports:
import java.util.List;
import com.hankcs.hanlp.HanLP;

// Your custom variables:
String document;
List<String> sentenceList;

// Enter your code here:		
document = ""ÁÆóÊ≥ïÂèØÂ§ßËá¥ÂàÜ‰∏∫Âü∫Êú¨ÁÆóÊ≥ï„ÄÅÊï∞ÊçÆÁªìÊûÑÁöÑÁÆóÊ≥ï„ÄÅÊï∞ËÆ∫ÁÆóÊ≥ï„ÄÅËÆ°ÁÆóÂá†‰ΩïÁöÑÁÆóÊ≥ï„ÄÅÂõæÁöÑÁÆóÊ≥ï„ÄÅÂä®ÊÄÅËßÑÂàí‰ª•ÂèäÊï∞ÂÄºÂàÜÊûê„ÄÅÂä†ÂØÜÁÆóÊ≥ï„ÄÅÊéíÂ∫èÁÆóÊ≥ï„ÄÅÊ£ÄÁ¥¢ÁÆóÊ≥ï„ÄÅÈöèÊú∫ÂåñÁÆóÊ≥ï„ÄÅÂπ∂Ë°åÁÆóÊ≥ï„ÄÅÂéÑÁ±≥ÂèòÂΩ¢Ê®°Âûã„ÄÅÈöèÊú∫Ê£ÆÊûóÁÆóÊ≥ï„ÄÇ\n"" +
        ""ÁÆóÊ≥ïÂèØ‰ª•ÂÆΩÊ≥õÁöÑÂàÜ‰∏∫‰∏âÁ±ªÔºå\n"" +
        ""‰∏ÄÔºåÊúâÈôêÁöÑÁ°ÆÂÆöÊÄßÁÆóÊ≥ïÔºåËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑ‰∏ÄÊÆµÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇ‰ªñ‰ª¨ÂèØËÉΩË¶ÅËä±ÂæàÈïøÊó∂Èó¥Êù•ÊâßË°åÊåáÂÆöÁöÑ‰ªªÂä°Ôºå‰ΩÜ‰ªçÂ∞ÜÂú®‰∏ÄÂÆöÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇËøôÁ±ªÁÆóÊ≥ïÂæóÂá∫ÁöÑÁªìÊûúÂ∏∏ÂèñÂÜ≥‰∫éËæìÂÖ•ÂÄº„ÄÇ\n"" +
        ""‰∫åÔºåÊúâÈôêÁöÑÈùûÁ°ÆÂÆöÁÆóÊ≥ïÔºåËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫é‰∏Ä‰∏™ÔºàÊàñ‰∏Ä‰∫õÔºâÁªôÂÆöÁöÑÊï∞ÂÄºÔºåÁÆóÊ≥ïÁöÑÁªìÊûúÂπ∂‰∏çÊòØÂîØ‰∏ÄÁöÑÊàñÁ°ÆÂÆöÁöÑ„ÄÇ\n"" +
        ""‰∏âÔºåÊó†ÈôêÁöÑÁÆóÊ≥ïÔºåÊòØÈÇ£‰∫õÁî±‰∫éÊ≤°ÊúâÂÆö‰πâÁªàÊ≠¢ÂÆö‰πâÊù°‰ª∂ÔºåÊàñÂÆö‰πâÁöÑÊù°‰ª∂Êó†Ê≥ïÁî±ËæìÂÖ•ÁöÑÊï∞ÊçÆÊª°Ë∂≥ËÄå‰∏çÁªàÊ≠¢ËøêË°åÁöÑÁÆóÊ≥ï„ÄÇÈÄöÂ∏∏ÔºåÊó†ÈôêÁÆóÊ≥ïÁöÑ‰∫ßÁîüÊòØÁî±‰∫éÊú™ËÉΩÁ°ÆÂÆöÁöÑÂÆö‰πâÁªàÊ≠¢Êù°‰ª∂„ÄÇ"";
// below error ! ÈÄ†Êàê Knime ""Èó™ÈÄÄ""
sentenceList = HanLP.extractSummary(document, 3);
/************************************************************************************************************/

PS : I add ‚Äòhanlp-1.3.2.jar‚Äô to ‚ÄúAdditional Libraries‚Äù at Node, not ‚ÄúKnime\plugins‚Äù subdirectory.
 
Ë∞¢Ë∞¢ÊÇ®"
Mac‰∏äpycharmÂú®Á®ãÂ∫è‰∏≠Á¨¨‰∫åÊ¨°startjvmÊä•ÈîôUnable to start JVM at native/common/jp_env.cpp:78,"<img width=""812"" alt=""2017-02-09 6 42 30"" src=""https://cloud.githubusercontent.com/assets/6072224/22779906/922f6c94-eef7-11e6-80e1-6d353a245f78.png"">
ÊàëÂÅöÁöÑÊòØÁΩëÈ°µ‰∏äÁî®Êà∑ÊØè‰∏ÄÊ¨°Êèê‰∫§ËæìÂÖ•Êï∞ÊçÆÂêéÈÉΩËøõË°å‰∏ÄÊ¨°ÂàÜËØçÔºåÁ¨¨‰∏ÄÊ¨°startËøõË°åÂàÜËØçÊ≤°ÊúâÈóÆÈ¢ò „ÄÇ‰πãÂêéÂ∞±Êä•ÈîôÔºå‰ΩøÁî®isstartËøõË°åÂà§Êñ≠ÂèëÁé∞JVMÊ≤°Êúâshutdown  Ë≤å‰ººÊòØÂè™ÊúâpythonÁ®ãÂ∫èÁªìÊùüËøô‰∏™JVMÊâç‰ºöÂÆåÂÖ®ÂÖ≥Èó≠„ÄÇÂ∏åÊúõÂ§ßÂ§ßËß£Á≠îÔºÅÔºÅÊÄé‰πàÂäû"
‚ÄòÊó†Èî°ÈáëÈë´ÈõÜÂõ¢‚Äô Ëøô‰∏™ËØçÊÄé‰πàÊêûÈÉΩÂàÜ‰∏çÂØπ,"‰ΩøÁî®Á¥¢ÂºïÂàÜËØçÔºåÈ¢ÑÊúüÁªìÊûúÊòØÔºöÊó†Èî°ÔºåÊó†Èî°ÈáëÔºåÈî°ÈáëÔºåÈáëÈë´ÔºåÈõÜÂõ¢„ÄÇ
ÂæóÂà∞ÁöÑÁªìÊûúÊòØÔºöÊó†ÔºåÈî°ÈáëÔºåÈë´ÔºåÈõÜÂõ¢
ËØ•ÂíãÊï¥Ôºü"
ÂÖ≥‰∫éËØçÁöÑÈáçÂè†Êó†Ê≥ïË¢´ËØÜÂà´ÁöÑÈóÆÈ¢ò,"‰æãÂ¶ÇÔºöÁü≥/ng, Âè∞‰∏ú/ns, Êü±/ng, È£üÂìÅ/n, ÊúâÈôêÂÖ¨Âè∏
Áî±‰∫é‚ÄúÂè∞‰∏ú‚ÄúÂ∑≤ÁªèË¢´Ê†áËÆ∞ÊàênsÔºå‚ÄúÁü≥Âè∞‚Äú‰πüË¢´Ê†áËÆ∞ÊàênsÔºå‚Äú‰∏úÊü±‚Äú‰πüË¢´Ê†áËÆ∞Êàê‰∫Üns ÊúÄÂêéÊòæÁ§∫ÁöÑÁªìÊûúÔºåÂ∞±ÊòØ‰æãÂ≠ê‰∏≠ÊòæÁ§∫ÁöÑÈÇ£Ê†∑ÔºåÊÄé‰πàÂéª‰øÆÊîπÊâçËÉΩËÆ©ÂÆÉÂèòÊàê  Áü≥Âè∞/ns ‰∏úÊü±/ns È£üÂìÅ/n ÊúâÈôêÂÖ¨Âè∏Âë¢ÔºåÂú®‰∏çÂà†Èô§ ‚ÄúÂè∞‰∏ú‚ÄúË¢´Ê†áËÆ∞ÊàênsÁöÑÊÉÖÂÜµ‰∏ã"
Âä®ÊÄÅÊ∑ªÂä†ËØçÊó†Êïà,"ÊàëÂ∞ùËØïÁî®‰ΩøÁî®  CustomDictionary.add(""ÂéöÂ∫ï"") Âä®ÊÄÅÊ∑ªÂä†ËØçÔºå‰ª•‰æø‰Ωø‚ÄúÂéöÂ∫ïÊÉÖ‰æ£ÁôΩÈûã‚ÄùËÉΩÊ≠£Á°ÆÁöÑËøõË°åÂàÜËØçÔºå‰ΩÜÊòØÁªìÊûúÊòØ ‚ÄúÂéöÂ∫ï‚Äù Âπ∂Ê≤°ÊúâÁîüÊïà„ÄÇ

```java
CustomDictionary.add(""ÂéöÂ∫ï"");
CustomDictionary.add(""ÁôΩÈûã"");
System.out.println(HanLP.segment(""ÂéöÂ∫ï""));
System.out.println(HanLP.segment(""ÂéöÂ∫ïÊÉÖ‰æ£ÁôΩÈûã""));
```

ËæìÂá∫Ôºö
[ÂéöÂ∫ï/nz]
[Âéö/a, Â∫ïÊÉÖ/n, ‰æ£/n, ÁôΩÈûã/nz]

ÁÑ∂ÂêéÊàëÁúã‰Ω†readmeÈáåÈù¢ÊúâÂÜô‚ÄúÂú®Âü∫‰∫éÂ±ÇÂè†ÈöêÈ©¨Ê®°ÂûãÁöÑÊúÄÁü≠Ë∑ØÂàÜËØç‰∏≠ÔºåÂπ∂‰∏ç‰øùËØÅËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑËØç‰∏ÄÂÆöË¢´ÂàáÂàÜÂá∫Êù•„ÄÇÂ¶ÇÊûú‰Ω†ËÆ§‰∏∫Ëøô‰∏™ËØçÁªùÂØπÂ∫îËØ•ÂàáÂàÜÂá∫Êù•ÔºåÈÇ£‰πàËØ∑Â∞ÜËØçÈ¢ëËÆæÂ§ß‰∏Ä‰∫õ‚ÄùÔºåÊâÄ‰ª•ÊàëÂ∞ùËØï‰∫ÜÂ∞ÜËØçÈ¢ëË∞ÉÂ§ßÔºå‰ΩÜÊòØ‰∏çËµ∑‰ΩúÁî®ÔºåËøô‰∏™Â∫îËØ•ÊÄé‰πàÂ§ÑÁêÜÔºü
"
ËøêË°åÊó∂Âá∫Áé∞Êñá‰ª∂ËØªÂèñÈóÆÈ¢ò Ëøô‰∏™com.hankcs.hanlp.corpus.io.IOUtilÂåÖËøêË°åÂá∫Èîô,"![1](https://cloud.githubusercontent.com/assets/19542871/22288090/570e707e-e330-11e6-9def-0bfb92f50d1d.PNG)
![2](https://cloud.githubusercontent.com/assets/19542871/22288091/570f1614-e330-11e6-8a65-0373bd1c1554.PNG)
![3](https://cloud.githubusercontent.com/assets/19542871/22288092/5711489e-e330-11e6-989d-79e2419b39bf.PNG)

ÊàëÊåáÂÆöÁöÑrootÊòØÂú®CÁõòeclipseÁöÑworkplaceË∑ØÂæÑ‰∏ãÔºåÂπ∂‰∏îÊàëÁöÑDÁõòÂπ∂Ê≤°Êúâ Doc\ËØ≠ÊñôÂ∫ì Ëøô‰∏™Êñá‰ª∂Â§πÔºåÊâÄ‰ª•ÁâπÊù•ËØ∑Ê±ÇÂ§ßÁ•ûÔºåÂ∏ÆÊàëÁúã‰∏Ä‰∏ãÊòØ‰ªÄ‰πàÂéüÂõ†ÔºüÔºüÔºüÊãúÊâò‰∫Ü"
NLPTokenizer.segment Â§öÁ∫øÁ®ãÂºÇÂ∏∏,"java.lang.NullPointerException
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.getState(Trie.java:209)
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.parseText(Trie.java:140)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3749)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:37)

hanlp-1.3.1 Âú®Â§öÁ∫øÁ®ãÊÉÖÂÜµ‰∏ãÔºåÂá∫Áé∞Á©∫ÊåáÈíàÂºÇÂ∏∏„ÄÇ
NLPTokenizer.segmentÂ∫îËØ•‰∏çÊòØÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÂêßÔºåÊúâÊ≤°ÊúâÁ∫øÁ®ãÂÆâÂÖ®ÁöÑÊõø‰ª£ÊñπÊ≥ïÔºü"
CRFÂàÜËØç‰∏çËÉΩÁî®,"ÊàëÂΩìÁî®CRFÂàÜËØçÂô®ÁöÑÊó∂ÂÄôJava‰ºöÊäõÂºÇÂ∏∏Ôºö

> ```
> ËØªÂèñdata/model/segment/CRFSegmentModel.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.lang.NullPointerException
> Âú®ËØªÂèñËøáÁ®ã‰∏≠ÂèëÁîüÈîôËØØjava.lang.NullPointerException
> CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåËÄóÊó∂ 55 ms
> java.io.FileNotFoundException: data\model\segment\CRFSegmentModel.txt (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ)
> ```

ËØ∑ÈóÆËøôÊòØÊÄé‰πàÂõû‰∫ãÂë¢ÔºåÊÄé‰πàËÉΩËß£ÂÜ≥„ÄÇË∞¢Ë∞¢ÔºÅÔºÅ"
CRF model binÊñá‰ª∂ÂèØ‰ª•ÊÄé‰πàÁîüÊàêÂØπÂ∫îÁöÑtxtÊñá‰ª∂Âêó,"ÊÉ≥Âú®C++ÁéØÂ¢ÉÈáåÁî®hanlpÈáåÁöÑCRFÂàÜËØçÊ®°ÂûãÔºå‰ΩÜ‰∏çÁü•ÈÅìÊÄé‰πàËß£Êûê„ÄÇ
Â¶ÇÊûúËÉΩËΩ¨Êç¢‰∏∫ÂØπÂ∫îÁöÑtxtÊñá‰ª∂ÔºåÂ∞±ÂèØ‰ª•‰ΩøÁî®CRF++Êù•ËΩ¨Êç¢ËØªÂèñ‰∫Ü
Â¶ÇÊûú‰∏çËÉΩÁîüÊàê‰∏∫txtÊñá‰ª∂ÁöÑËØùÔºå @hankcs ÂèØ‰ª•ÂàÜ‰∫´‰∏Ä‰ªΩÂêóÔºåË∞¢Ë∞¢ÔºÅ"
ÂêçÂ≠óËØÜÂà´ÁöÑÈóÆÈ¢ò,"‰Ω†Â•ΩÔºå
ÊúÄËøëÂèëÁé∞hanlpÂØπÂêçÂ≠óÁöÑËØÜÂà´ÊúâÊó∂ÂÄô‰ºöÊúâ‰∫õÂÅèÂ∑ÆÔºåÊØîÂ¶Ç‚ÄúÂÖ®Â≠£Âêß‚ÄùË¢´ËØÜÂà´Êàê‰∫Ünr„ÄÇ
Áúãnr.txtÈáåÁöÑÂÜÖÂÆπÊ≤°ÊúâÁúãÊáÇÔºåËØ∑ÈóÆÊúâ‰ªÄ‰πàÂäûÊ≥ïÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÂêóÔºü
Â§öË∞¢ÔºÅ"
Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏‰∏çËµ∑‰ΩúÁî®ÔºåË∞¢Ë∞¢Ëß£Á≠îÔºÅ,"1„ÄÅCustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; define.txt
2„ÄÅdefine.txtÔºö‰ø°ÊÅØ‰∏éËÆ°ÁÆóÁßëÂ≠¶ v 1
3„ÄÅÁ°Æ‰øùÊòØuft-8
4„ÄÅÂ∑≤Âà†Èô§ÁºìÂ≠òÊñá‰ª∂
Ë∞¢Ë∞¢ÔºÅ"
HanLPÂèØ‰ª•ÊÉÖÊÑüÂàÜÊûêÂêóÔºü,"ÊÇ®Â•ΩÔºÅ
       HanLPÂèØ‰ª•ËøõË°åÊÉÖÊÑüÂàÜÊûêÂêóÔºàÊ≠£Èù¢„ÄÅË¥üÈù¢„ÄÅ‰∏≠ÊÄßÔºâÔºüËÉΩÂê¶Êèê‰æõ‰∏Ä‰∏™ÊÄùË∑Ø„ÄÇË∞¢Ë∞¢ÔºÅ"
‰∏ÄÈ¶ñÊó•ËØ≠Ê≠åÊõ≤ÂàÜËØçÈîôËØØ,"‰∏ÄÈ¶ñÊó•ËØ≠Ê≠åÊõ≤ÔºåÂàÜËØçÊàê‰∏ãÈù¢ÁöÑÁªìÊûúÔºåÂ¶Ç‰Ωï‰øÆÊîπÂ≠óÂÖ∏Ë°®ÔºåÊâçÂèØ‰ª•ÂàÜËØçÊ≠£Á°ÆÔºü
‰∏Ä/m, È¶ñÊó•/t, ËØ≠/ng, Ê≠åÊõ≤/n"
‰∏ÄÈ¶ñÊó•ËØ≠Ê≠åÊõ≤ÂàÜËØçÈîôËØØÔºå,"‰∏ÄÈ¶ñÊó•ËØ≠Ê≠åÊõ≤ÔºåÂàÜËØçÊàê‰∏ãÈù¢ÁöÑÁªìÊûúÔºåÂ¶Ç‰Ωï‰øÆÊîπÂ≠óÂÖ∏Ë°®ÔºåÊâçÂèØ‰ª•ÂàÜËØçÊ≠£Á°ÆÔºü
‰∏Ä/m, È¶ñÊó•/t, ËØ≠/ng, Ê≠åÊõ≤/n"
Âà©Áî®hanlpÊ†áÊ≥®ËØçÊÄß,Â¶Ç‰ΩïÂà©Áî®hanlpÊ†áÊ≥®ÔºàÂ∑≤ÁªèÂàÜÂ•ΩËØçÔºâÁöÑËØçÊÄßÔºü
Áî®Êà∑ÊÑèÂõæÂàÜÊûê,"ÊúâËøôÊ†∑ÁöÑ‰∏ÄÊÆµËØù""hello,hello,hello ÊàëË¶ÅÂéªÂæêÂÆ∂Ê±á"" ÈááÁî®‰ªÄ‰πàÊ†∑ÁöÑÊ®°ÂûãÂèØ‰ª•ÂàÜÊûêÂá∫""ÊàëË¶ÅÂéªÂæêÂÆ∂Ê±á""ÁöÑÊÑèÂõæ‰∫ÜÔºü hankcsËÉΩÂê¶Â∏ÆÂä©Áúã‰∏Ä‰∏ãÔºåÂ§öË∞¢"
ÂÖ≥‰∫éhanlp.properties‰∏≠ÂÆö‰πâÁöÑdataË∑ØÂæÑÈóÆÈ¢ò,"ÊúÄËøë‰∏ÄÁõ¥Âú®‰ΩøÁî®hanlpÂàÜËØçÂ∑•ÂÖ∑ÔºåÂèëÁé∞hanlp.propertiesÂú®data/dictionaryË∑ØÂæÑÁöÑÂÆö‰πâ‰∏äÂπ∂‰∏çÁÅµÊ¥ªÔºårootÂè™ËÉΩÂÆöÊ≠ªÁªùÂØπË∑ØÂæÑ„ÄÇÂú®Â∞ÜÁ®ãÂ∫èÊâìÂåÖÊîæÂà∞sparkÈõÜÁæ§‰∏äËøêË°åÔºå‰∏çÂèØË°å„ÄÇÂ∞ùËØïÊîπIOUtilsÁöÑ   
public static InputStream newInputStream(String path) throws IOException
    {
        if (IOAdapter == null) return new FileInputStream(path);
        return IOAdapter.open(path);
    }
ÈÄöËøáclassloaderÁöÑgetresourceasStreamÊñπÂºèËØªÂèñÔºåÂèëÁé∞‰πãÂêéËÉΩÂÆûÁé∞Â∞ÜdataÊîæÂÖ•Âà∞classpath‰∏ãÈÄöËøáÁõ∏ÂØπË∑ØÂæÑËØªÂèñÔºå‰ΩÜÊòØÁâµ‰∏ÄÂèëÂä®ÂÖ®Ë∫´ÔºåÂæàÂ§öÂú∞ÊñπÈÉΩÊúâÂºÇÂ∏∏ÔºåÂ∏åÊúõhancksÂ§ßÁ•ûÊúâÊó∂Èó¥ËÉΩËß£ÂÜ≥‰∏ãhanlp.propertiesÂè™ËÉΩÈÄöËøáÁªùÂØπË∑ØÂæÑËØªÂÖ•dataÁöÑÈóÆÈ¢ò"
HanlpÁöÑÊèêÂèñÊëòË¶ÅÁöÑÊñπÊ≥ï,"Âú®HanlpÁªôÂá∫ÁöÑ‰æãÂ≠ê‰∏≠ÔºåmainÊñπÊ≥ïÈáåÈù¢‰ΩøÁî®HanLP.extractPhrase(document,5)ÔºåËøô‰∏™ÊñπÊ≥ïÈÉΩÂæàÊ≠£Â∏∏Ôºå‰ΩÜÊòØÂú®ÂÖ∂‰ªñÁöÑÂú∞ÊñπÊñπÊ≥ïË∞ÉÁî®Ëøô‰∏™ÊñπÊ≥ïÁöÑÊó∂ÂÄôÂ∞±‰∏çÊàêÂäü"
hankcsÔºåÂ§öÊ®°ÂºèÂ≠óÁ¨¶‰∏≤ÂåπÈÖçÁî±‰∫éÊ®°Âºè‰∏çÊòØÁâπÂà´Â§öÔºåÊàëËßâÂæó‰∏çÈúÄË¶Å‰ΩøÁî®Ëá™Âä®Êú∫ÔºåÂùèÂ≠óÁ¨¶Ë∑≥ËΩ¨È¢ÑÂ§ÑÁêÜÂêéÔºå‰ΩøÁî®‰∏Ä‰∏™hashmapÂ∞±ÂèØ‰ª•‰∫Ü,
BM25ÊñáÊú¨Áõ∏‰ººÂ∫¶‰∏∫Ë¥ü,Âú®Áî®textrankÂÅöÂÖ≥ÈîÆÂè•ÊèêÂèñÊó∂ÂèëÁé∞ÔºåBM25ËÆ°ÁÆóÁöÑÁõ∏‰ººÂ∫¶ÁªìÊûúÈÉ®ÂàÜ‰∏∫Ë¥ü„ÄÇËøûÂè•Â≠ê‰∏éÊú¨Ë∫´ÁöÑÁõ∏‰ººÂ∫¶ÈÉΩÊòØË¥üÁöÑ„ÄÇÂ∫îËØ•ÊòØidfÂÖ¨ÂºèÁöÑÂàÜÊØçÂ§öÂáè‰∫Ü‰∏Ä‰∏™ÊñáÊ°£È¢ëÁéá
jdk 1.7Ê≠£Â∏∏ÂàÜËØçÔºåjdk1.8ÂàÜËØçÊï∞ÁªÑË∂äÁïå,"jarÁâàÊú¨1.3.2ÔºåÊàëÁúãÂÖ∂‰ªñ‰∫∫ÁöÑissue Â•ΩÂÉèÂæàÂ§ö‰∫∫‰πüÊúâËøô‰∏™ÈóÆÈ¢ò
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 149
	at com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)
	at com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)
	at com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:380)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:574)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:40)
	at test.main(test.java:8)
"
ÂÖ≥‰∫éËØçÁöÑÊ†áÊ≥®,Èô§‰∫ÜÂú®nt.txt‰∏≠Áõ¥Êé•Ê†áËÆ∞ÊàêC‰πãÂ§ñÔºåËøòÊúâÊ≤°ÊúâÂÖ∂‰ªñÁöÑÊ†áÊ≥®ÊñπÂºèÊòØËøô‰∏™ËØçÊÄßÂèòÊàêCÁöÑ ÊØîÂ¶Ç‰∏Ä‰∏™ËØç ÊàëÈúÄË¶ÅÊÄé‰πàÊ†áÊ≥®‰ªÄ‰πàÂ∞±ËÉΩÂú®‰ΩøÁî®ÁöÑÊó∂ÂÄô ËÆ©ÂÆÉÂèòÊàêËßíËâ≤ C
Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏,"ËØ∑ÈóÆÂ¶ÇÊûúÂú®hanlp.propertiesÊñá‰ª∂‰∏≠ÈÖçÁΩÆ‰∫ÜÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏

CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; ÊµÅË°åÊñ∞Ê≠åËØçÊ±á.txt; ÁîµÂΩ±Â§ßÂÖ®ËØçÂÖ∏.txt;„ÄÄÁé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; 
‰πãÂêé
 val content=""ÊàëÂñúÊ¨¢ÁúãÂ∞èÂ≤õÊÉäÈ≠ÇÔºåÈòøÁîòÊ≠£‰º†ÔºåËá¥ÂëΩIDÔºåÁ¶ÅÈó≠Â≤õ,Â∞èÂèÆÂΩì‰∏éÊµ∑Áõó‰ªôÂ≠êÔºåËÇñÁî≥ÂÖãÁöÑÊïëËµé.ÊàëÂñúÊ¨¢Âê¨ÈòøÊ≥¢ÁãÇÊÉ≥Êõ≤""
„ÄÄprintln(StandardTokenizer.segment(content))

ÊòØ‰ºòÂÖàÂéªÊ†πÊçÆÁî®Êà∑Ëá™ÂÆö‰πâÁöÑËØçÂÖ∏ËøõË°åÂàÜËØçÂêóÔºåÂ¶ÇÊûúÊàëÂ∏åÊúõÂÖàÊ†áÂáÜÂàÜËØçÔºå‰πãÂêéÂÜçÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØçÂ∫îËØ•ÊÄéÊ†∑Êìç‰ΩúÂë¢Ôºü
ÊúâÊ≤°ÊúâÂèØ‰ª•ÊòæÁ§∫ÊâãÂä®ÂÖ≥Èó≠Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØçÁöÑÂäüËÉΩÔºå‰πãÂêéË¶ÅÁî®ÁöÑÊó∂ÂÄôÂÜçÂêØÁî®ÁöÑÂáΩÊï∞Âë¢Ôºü"
ÂÖ≥‰∫éÂÖ≥ÈîÆËØçÊèêÂèñÈóÆÈ¢òÔºåÂçï‰∏™Â≠óÁöÑÂêçËØçÂ•ΩÂÉèË¢´ËøáÊª§Êéâ‰∫Ü,"‰Ω†Â•Ω !ÊàëÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢ò‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊÇ®ÊïÖÊÑè‰∏∫‰πã„ÄÇÂú®ÊèêÂèñÂÖ≥ÈîÆËØçÁöÑÊó∂ÂÄôÂçï‰∏™Â≠óÁöÑÂêçËØç Ë¢´ËøáÊª§Êéâ‰∫ÜÂêßÔºå‰æãÂ¶Ç‰∏ÄÁØáÂÖ≥‰∫éÊó∂ÁöÑÊñáÁ´† Á´üÁÑ∂Ê≤°ÊúâÊó∂Ëøô‰∏™ÂÖ≥ÈîÆËØçÔºå
ÊàëÁúã‰∫Ü‰∏ã‰Ω†ÁöÑÊ∫êÁ†Å ÁâàÊú¨ 1.3.1 com.hankcs.hanlp.summary.TextRankKeyword.class Á±ª‰∏≠ÁöÑÊñπÊ≥ïshouldInclude
/**
* ÊòØÂê¶Â∫îÂΩìÂ∞ÜËøô‰∏™termÁ∫≥ÂÖ•ËÆ°ÁÆóÔºåËØçÊÄßÂ±û‰∫éÂêçËØç„ÄÅÂä®ËØç„ÄÅÂâØËØç„ÄÅÂΩ¢ÂÆπËØç
*
* @param term
* @return ÊòØÂê¶Â∫îÂΩì
*/
public boolean shouldInclude(Term term){
...
default:
{
if (term.word.trim().length() > 1 && !CoreStopWordDictionary.contains(term.word))
{
return true;
}
} //ËøôÂè•ËØùÊòØ‰∏çÊòØÊúâÁÇπ‰∏çÂ¶•Ôºü Â∏åÊúõËÉΩÂèäÊó∂Êî∂Âà∞ÂõûÂ§çÔºÅÊàëÁöÑÈÇÆÁÆ± 515504936@qq.com
...
return false;
}"
ÁπÅÁÆÄ‰ΩìËΩ¨Êç¢ÔºöÂæ∑ÂúãÊº¢Â†°ËΩ¨Êç¢Êàê‰∫ÜÂæ∑ÂõΩÊ±âÂ†°ÂåÖ,Âæ∑ÂúãÊº¢Â†°ËΩ¨Êç¢Êàê‰∫ÜÂæ∑ÂõΩÊ±âÂ†°ÂåÖ„ÄÇ‰∏çÁü•ÈÅìhanlpÊ∫êÁ†ÅÊòØÂê¶ÂèØ‰ª•ÁºñËØë„ÄÇ
ÈááÁî®Ëá™ÂÆö‰πâÂ≠óÂÖ∏‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑËØçÊÄßÊ†áÊ≥®ÈóÆÈ¢ò,"Ëá™ÂÆö‰πâsong.txt ÊàëÂú®NatureÈáåÈù¢ÂÆö‰πâ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ±ªÂûã‰∏∫song‰∏é‰πãÁõ∏ÂØπÂ∫îÔºåÈááÁî®‰∏ãÈù¢ÁöÑÁª¥ÁâπÊØîÁÆóÊ≥ïËøõË°åÂàÜËØç„ÄÇ
 HanLP.newSegment()enablePartOfSpeechTagging(true)
ÂØπ‰∫é‚ÄúÂêéÊù•‚Äù
  Â¶Ç‰ΩïÂÆûÁé∞ ÂêéÊù•ÁöÑÊïÖ‰∫ã ‰ª•ÂèäÂàòËã•Ëã±ÁöÑÂêéÊù•  ‰∏≠ÁöÑ‚ÄúÂêéÊù•‚Äù‰∏§Áßç‰∏çÂêåÁöÑÂú∫ÊôØÁöÑÊ†áÊ≥®„ÄÇ ÂâçËÄÖÊ†áÊ≥®‰∏∫tÔºåÂêéËÄÖÊ†áÊ≥®‰∏∫song.

ÊòØÂê¶ÂèØ‰ª•Âú®ËΩ¨ÁßªÁü©Èòµ‰∏≠ÂÆûÁé∞Ôºü ÊàëËØï‰∫Ü‰∏Ä‰∏ãÔºåÊÑüËßâËøòÊòØÂæàÁπÅÁêêÁöÑÔºåËÄå‰∏î‰πüÊ≤°ÊúâÂæóÂà∞Ê≠£Á°ÆÁöÑÁªìÊûú„ÄÇËÉΩÂê¶ÂçèÂä©Áúã‰∏Ä‰∏ãÂë¢Ôºü
"
Ëá™ÂÆö‰πâËØçÂÖ∏‰∏çËµ∑‰ΩúÁî®,"1„ÄÅÂú®data\dictionary\customÁõÆÂΩï‰∏ãÊñ∞Âª∫Ëá™ÂÆö‰πâËØçÂÖ∏my.txtÔºåÂÜÖÂÆπÂ¶Ç‰∏ãÔºö
      È£é‰∏≠ÊúâÊúµÈõ®ÂÅöÁöÑ‰∫ë
      ‰∏ÄÂçÉ‰∏™‰º§ÂøÉÁöÑÁêÜÁî±
      ÂøòÊÉÖÊ∞¥
2„ÄÅÂú®hanlp.properties‰∏≠Â¢ûÂä†my.txt
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; my.txt ns; Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt;data/dictionary/person/nrf.txt nrf
3„ÄÅÊµãËØïÔºö
   String content = ""Âº†Â≠¶ÂèãÁöÑ‰∏ÄÂçÉ‰∏™‰º§ÂøÉÁöÑÁêÜÁî±ÊòØ‰∏ÄÈ¶ñÂ•ΩÂê¨ÁöÑÊ≠å"";
   Segment seg = HanLP.newSegment().enableCustomDictionary(true);
   List<Term> termList = seg.seg(content);
   System.out.println(termList);
   List<String> keywordList = HanLP.extractKeyword(content, 5);
   System.out.println(keywordList);

   ËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ãÔºö
[Âº†Â≠¶Âèã/nr, ÁöÑ/ude1, ‰∏ÄÂçÉ‰∏™/nz, ‰º§ÂøÉ/a, ÁöÑ/ude1, ÁêÜÁî±/n, ÊòØ/vshi, ‰∏Ä/m, È¶ñ/q, Â•ΩÂê¨/a, ÁöÑ/ude1, Ê≠å/n]
[Âº†Â≠¶Âèã, ÁêÜÁî±, ‰º§ÂøÉ, ‰∏ÄÂçÉ‰∏™, Â•ΩÂê¨]

4„ÄÅÁªìËÆ∫Ôºö
     Ëá™ÂÆö‰πâËØçÂÖ∏Ê≤°Ëµ∑Âà∞‰ΩúÁî®
     ÔºàÂ¶ÇÊûúÂú®ÂàÜËØç‰πãÂâçÊâßË°å‰∫ÜCustomDictionary.add(""‰∏ÄÂçÉ‰∏™‰º§ÂøÉÁöÑÁêÜÁî±"");Â∞±ËÉΩËµ∑‰ΩúÁî®Ôºâ


ËØ∑ÈóÆÊàëÁöÑÁî®Ê≥ïÊúâÈóÆÈ¢òÂêóÔºü
"
Ê†∏ÂøÉÂêå‰πâËØçËØçÂÖ∏‰∏≠‰∏Ä‰∏™ËØçÊúâÂ§ö‰∏™ÊÑèÊÄùÁöÑÊó∂ÂÄôÔºåÂè™ËøîÂõû‰∫Ü‰∏Ä‰∏™,"`CoreSynonymDictionary.get(""‰∏äÊµ∑"")` ‰∏äÊµ∑Â∫îËØ•ÊòØÂú®CoreSynonym.txtÂ§öÂ§ÑÂ≠òÂú®ÁöÑ„ÄÇ‰ΩÜÊòØÂè™‰ºöËøîÂõû‰∏Ä‰∏™"
solrÊãºÈü≥ÂàÜËØçÊèí‰ª∂ÈóÆÈ¢ò,"`public class HanlpPinYinTokenFilter extends TokenFilter{

	private static final Logger logger = LoggerFactory.getLogger(HanlpPinYinTokenFilter.class);
	
	private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
	
	public HanlpPinYinTokenFilter(TokenStream input) {
		super(input);
	}

	@Override
	public boolean incrementToken() throws IOException {
		
		if(termAtt.length() > 0) {
			String pinYin = HanLP.convertToPinyinString(termAtt.toString(), """", false);
			termAtt.setEmpty();
			termAtt.append(pinYin);
			return false;
		}
		return true;
}`
ÊÉ≥ÈÄöËøá‰Ω†ÁöÑÂàÜËØçÂô®ÂÅö‰∏Ä‰∏™ÊãºÈü≥ÂàÜËØçÔºå‰ΩÜÊòØÊØèÊ¨°Ëé∑ÂèñÂà∞‰ªçÊóßÊòØ‰∏≠ÊñáÁªìÊûúÔºå‰ΩÜÊòØÊâìÂç∞termAttÁöÑÁªìÊûúÁ°ÆÂÆûÊòØpinyinÁöÑÔºåËøô‰∏™Â∞±‰∏çÂ§™ÊòéÁôΩ‰∫ÜÔºåËÉΩÂ∏ÆÂøôÁúã‰∏ã‰πàÔºü Ë∞¢Ë∞¢‰∫Ü"
ÂÖ≥‰∫éÂä®ÊÄÅÊ∑ªÂä†Â≠óÂÖ∏ÁöÑÈóÆÈ¢ò,"hankcsÔºåÊÇ®Â•ΩÔºåÈùûÂ∏∏ÊÑüË∞¢ÊÇ®‰∏∫Â§ßÂÆ∂Êèê‰æõ‰∫ÜÂ¶ÇÊ≠§Êñπ‰æøÁöÑNLPÂ∑•ÂÖ∑„ÄÇÊàëÁé∞Âú®ÈÅáÂà∞ÁöÑÈóÆÈ¢òÊòØÂú®ÂàÜËØç‰πãÂâçÂèØ‰ª•Ëé∑ÂèñÂà∞ÂΩìÂâçÊñáÊ°£ÁöÑÂæàÂ§öÂÆû‰ΩìÁº©ÂÜôÂêçÁß∞„ÄÇÁÑ∂ÂêéÊÉ≥ÈÄöËøáCustomDictionary.insert()ÊñπÊ≥ïÂä®ÊÄÅÊ∑ªÂä†Â≠óÂÖ∏Â¢ûÂä†ËØÜÂà´ÁöÑÁ≤æÁ°ÆÂ∫¶„ÄÇ‰ΩÜÊòØÂæàÂ§öÊ∑ªÂä†ÁöÑÂ≠óÂÖ∏Ê≤°Ê≥ïÂáÜÁ°ÆËØÜÂà´Ôºå‰∏çÁü•ÈÅìÊòØÂõ†‰∏∫‰ªÄ‰πàÔºüÊòØ‰∏çÊòØÊàë‰ΩøÁî®ÁöÑÊñπÊ≥ïÊúâÈîôËØØÔºü

@Test
	public void dictionaryTest() {
		Segment segment = HanLP.newSegment().enableCustomDictionary(true)
				.enableOrganizationRecognize(true);
		String str = ""Áé∞Âú®ÊÄªÁªüÊï∞Á†ÅÊ∏Ø„ÄÅËá™Áî±Èó≤„ÄÅÂåóÊµ∑È´òÂ≤≠„ÄÅÊÄªÁªüÂ§ßÈÖíÂ∫óÂõõÂÆ∂Á≠æËÆ¢‰∫ÜÂçèËÆÆ"";
		List<Term> termList = segment.seg(str);
		CustomDictionary.insert(""ÊÄªÁªüÊï∞Á†ÅÊ∏Ø"", ""nt 1024"");
		CustomDictionary.insert(""Ëá™Áî±Èó≤"", ""nt 1024"");
		CustomDictionary.insert(""ÂåóÊµ∑È´òÂ≤≠"", ""nt 1024"");
		CustomDictionary.insert(""ÊÄªÁªüÂ§ßÈÖíÂ∫ó"", ""nt 1024"");
		System.out.println(termList);
	}
ÁªìÊûúÊòØËøôÊ†∑ÁöÑ
[Áé∞Âú®/t, ÊÄªÁªü/nnt, Êï∞Á†ÅÊ∏Ø/nz, „ÄÅ/w, Ëá™Áî±/a, Èó≤/v, „ÄÅ/w, ÂåóÊµ∑/ns, È´ò/a, Â≤≠/ng, „ÄÅ/w, ÊÄªÁªü/nnt, Â§ßÈÖíÂ∫óÂõõÂÆ∂/nt, Á≠æËÆ¢/v, ‰∫Ü/ule, ÂçèËÆÆ/n]
"
Êó•ÂøóÈóÆÈ¢ò,Âª∫ËÆÆÂ§ßÂ§ßÂ∞ÜÊó•ÂøóÊîπ‰∏∫slf4jÊó•ÂøóÁªÑ‰ª∂Ôºå‰ΩøÁî®Ëµ∑Êù•Êñπ‰æøÂæàÂ§ö
CoreStopWordDictionaryÈáåÈù¢ FILTER ËøáÊª§Âô®ÂÆûÈôÖË°®Áé∞‰∏éÊ≥®Èáä‰∏çÁ¨¶,"Ê≥®Èáä‰∏∫: ""Ê†∏ÂøÉÂÅúÁî®ËØçÂÖ∏ÁöÑÊ†∏ÂøÉËøáÊª§Âô®ÔºåËØçÊÄßÂ±û‰∫éÂêçËØç„ÄÅÂä®ËØç„ÄÅÂâØËØç„ÄÅÂΩ¢ÂÆπËØçÔºåÂπ∂‰∏î‰∏çÂú®ÂÅúÁî®ËØçË°®‰∏≠Êâç‰∏ç‰ºöË¢´ËøáÊª§"" ‰ΩÜÂÆûÈôÖ‰∏ä, Â∞±ÁÆóÊòØÊª°Ë∂≥Êù°‰ª∂ÁöÑÂêçËØç/Âä®ËØç Âè™Ë¶ÅÈïøÂ∫¶‰∏∫1, ‰πü‰ºöË¢´Âπ≤Êéâ. ÂØπÂ∫î ‰ª£Á†Å:
line 94: `if (term.word.length() > 1 && !CoreStopWordDictionary.contains(term.word))`"
Êï∞ÊçÆÂåÖ‰∏ãËΩΩÈóÆÈ¢òÔºödata-for-1.3.0.zip,"ÂïäÂì¶Ôºå‰Ω†ÊâÄËÆøÈóÆÁöÑÈ°µÈù¢‰∏çÂ≠òÂú®‰∫Ü„ÄÇ
ÂèØËÉΩÁöÑÂéüÂõ†Ôºö
1.Âú®Âú∞ÂùÄÊ†è‰∏≠ËæìÂÖ•‰∫ÜÈîôËØØÁöÑÂú∞ÂùÄ„ÄÇ
2.‰Ω†ÁÇπÂáªÁöÑÊüê‰∏™ÈìæÊé•Â∑≤ËøáÊúü„ÄÇ"
Ê£ÄÊü•ÊòØÂê¶Âª∫Á´ã‰∫ÜfailureË°® ,"ËØ∑ÈóÆ‰∏Ä‰∏ã Âú®TrieÊñá‰ª∂‰∏≠
![image](https://cloud.githubusercontent.com/assets/20895017/20953814/0fc56f82-bc71-11e6-86dd-58020b95298a.png)
Ëøô‰∏™Ë°®Á§∫Âú®‰ªÄ‰πàÊÉÖÂÜµÂª∫Á´ãÁöÑ  failureË°®ÊåáÁöÑÂèàÊòØÂèàÊòØ‰ªÄ‰πà"
Â¶Ç‰ΩïÂêàÂπ∂ÔºàËøûÊé•Ôºâ‰∏§‰∏™term Ôºü,"```java
        for(int i = 0; i < termList.size()-1; i++){
        	System.out.println(termList.get(i)+termList.get(i+1));
        }
```
ËøôÊ†∑‰∏çÂèØ‰ª•Âì¶ÔºåÂ∫îËØ•Â¶Ç‰ΩïËøûÊé•ÔºüThanks."
ÂàùÂßãÂåñÂàÜËØçÂô®Êó∂ÔºåÂ≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°®Âä†ËΩΩÂ§±Ë¥•ÔºàCharType.dat.yesÔºâ,"Â§ßÁ•ûÂ•ΩÔºåËøô‰∏™ÂºÇÂ∏∏‰∏çÊòØÊØè‰∏ÄÊ¨°ÈÉΩÂá∫Áé∞ÔºàÂÅ∂Â∞îÂá∫Áé∞Ôºâ

‰∏ãÈù¢ÊàëÂú®eclipse‰∏äÊµãËØïÊó∂ÁöÑËæìÂá∫ÁöÑ‰ø°ÊÅØ

**********************************************************************************

ÂçÅ‰∫åÊúà 06, 2016 1:59:13 ‰∏ãÂçà com.hankcs.hanlp.HanLP$Config <clinit>
‰∏•Èáç: Ê≤°ÊúâÊâæÂà∞HanLP.propertiesÔºåÂèØËÉΩ‰ºöÂØºËá¥Êâæ‰∏çÂà∞data
========Tips========
ËØ∑Â∞ÜHanLP.propertiesÊîæÂú®‰∏ãÂàóÁõÆÂΩïÔºö
D:\workspace\****\target\classes
WebÈ°πÁõÆÂàôËØ∑ÊîæÂà∞‰∏ãÂàóÁõÆÂΩïÔºö
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
Âπ∂‰∏îÁºñËæëroot=PARENT/path/to/your/data
Áé∞Âú®HanLPÂ∞ÜÂ∞ùËØï‰ªéD:\workspace\****ËØªÂèñdata‚Ä¶‚Ä¶
ÂçÅ‰∫åÊúà 06, 2016 1:59:13 ‰∏ãÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñdata/dictionary/CoreNatureDictionary.txt.binÊó∂ÂèëÁîüÂºÇÂ∏∏java.nio.channels.ClosedByInterruptException
ÂçÅ‰∫åÊúà 06, 2016 1:59:25 ‰∏ãÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñdata/dictionary/other/CharType.dat.yesÊó∂ÂèëÁîüÂºÇÂ∏∏java.nio.channels.ClosedByInterruptException
Â≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°®Âä†ËΩΩÂ§±Ë¥•Ôºödata/dictionary/other/CharType.dat.yes

****************************************************************************************

ÁÑ∂ÂêéÁ®ãÂ∫èÂà∞ËøôÈáåÂ∞±ÁªìÊùü‰∫Ü

ÂØπÂ∫îÁõÆÂΩï Êñá‰ª∂ÊòØÂ≠òÂú®ÁöÑ „Äêdata/dictionary/other/CharType.dat.yes„Äë"
ÂàÜËØçÂêéNatureÂ¶Ç‰ΩïÂæóÂà∞ÂÆåÊï¥ËØçÊÄß,"Âú®ÂàÜËØçÂêéÔºåÁªüËÆ°ÂêÑ‰∏™ËØçÊÄß‰∏ãÂá∫Áé∞ÁöÑËØçÔºå‰ª•w‰∏∫‰æãÔºåÂæóÂà∞Â¶Ç‰∏ãÁªìÊûúÔºö
w

,Ôºå,Ôºâ,Ôºà,„ÄÇ,„ÄÅ

ÊåâÁÖßhttp://www.hankcs.com/nlp/part-of-speech-tagging.htmlËØ¥ÊòéÔºåÂÉèÈ°øÂè∑„ÄÅÂ∫îËØ•‰∏∫Ôºö
wn
          È°øÂè∑ÔºåÂÖ®ËßíÔºö„ÄÅ
Â¶Ç‰ΩïÂæóÂà∞wn?

ÂàÜËØç‰ª£Á†ÅÔºö
HanLP.segment(msg)"
Viterbi ÁÆóÊ≥ï‰∏≠Ê±ÇËß£ÊúÄÂ§ßÊ¶ÇÁéáÈÉΩÂèòÊàêÂä†Ê≥ïÔºåÈùû‰πòÊ≥ï,com.hankcs.hanlp.algoritm ‰∏ãÁöÑViterbiÁÆóÊ≥ïÊ±ÇËß£ÊúÄÂ§ßÊ¶ÇÁéáÈÉΩÂèòÊàêÂä†Ê≥ïÔºåËøôÊòØ‰∏∫‰ΩïÔºü`V[0][y] = start_p[y] + emit_p[y][obs[0]];`
WordNetÊï∞ÁªÑË∂äÁïåÂºÇÂ∏∏,"Â§ßÁ•ûÂ•Ω Êàë‰ªégit‰∏äclone‰∫Ü‰ªäÂ§©ÁöÑÁâàÊú¨ÔºåÊò®Â§©ÈÇ£Â§öÁ∫øÁ®ãÂºÇÂ∏∏Ê≤°Âá∫Áé∞
‰ªäÂ§©ÊäõÂá∫‰∏Ä‰∏™Êõ¥ÈáçÂ§ßÁöÑException

java.lang.ArrayIndexOutOfBoundsException: 43
	at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:214)
	at com.hankcs.hanlp.seg.common.WordNet.insert(WordNet.java:166)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3777)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary$1.hit(OrganizationDictionary.java:3754)
	at com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie.parseText(AhoCorasickDoubleArrayTrie.java:101)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3753)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:99)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)


ÊâßË°å‰ª£Á†ÅÔºö

public static void main(String[] args) {
		
		String str=""ÂèóÁ∫¶ÊùüÔºåÈúÄË¶ÅÈÅµÂÆàÂøÉÁêÜÂ≠¶‰ºöÊâÄÂÆöÁöÑÈÅìÂæ∑ÂéüÂàôÔºåÊâÄÈúÄË¶ÅÊó∂È°ªËØ¥ÊòéËØ•ÂÆûÈ™å‰∏éÊâÄËÉΩÂæóÂà∞ÁöÑÁü•ËØÜÁöÑÂÖ≥Á≥ª"";
		
		Segment defaultSegment = StandardTokenizer.SEGMENT
				.enablePartOfSpeechTagging(true).enableOffset(true)
				.enableNameRecognize(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true);
		
		try {
			List<Term> list = defaultSegment.seg(str);
		} catch (Exception e) {
			// TODO: handle exception
			
			e.printStackTrace();
			LOGGER.error(""ÂàÜËØçÂºÇÂ∏∏"",e);
		}
		
		
	}
"
TrieÊäõÂá∫nullpointÂºÇÂ∏∏," Â§ßÁ•ûÂ•ΩÔºåÊàëË∞ÉÁî®hanlpÂàÜËØçÊó∂Ôºå ÊäõÂá∫Â¶Ç‰∏ãÂºÇÂ∏∏Ôºö

java.lang.NullPointerException
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.getState(Trie.java:212)
	at com.hankcs.hanlp.algoritm.ahocorasick.trie.Trie.parseText(Trie.java:140)
	at com.hankcs.hanlp.dictionary.nt.OrganizationDictionary.parsePattern(OrganizationDictionary.java:3749)
	at com.hankcs.hanlp.recognition.nt.OrganizationRecognition.Recognition(OrganizationRecognition.java:70)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:97)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:466)
	
„ÄêÊµãËØïÊó∂ÔºåÂ¶ÇÊûú‰∏çÂºÄÂêØÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂäüËÉΩÔºåÂàôÊ≠£Â∏∏ËøêË°å„Äë

ÊàëÊµãËØïÊó∂ÔºåÊâìÂç∞Âá∫Êù•ÊòØ‰∏ãÈù¢ÁöÑÂú∞Êñπ currentState‰∏∫null


/**
     * Ë∑≥ËΩ¨Âà∞‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅ
     *
     * @param currentState ÂΩìÂâçÁä∂ÊÄÅ
     * @param character    Êé•ÂèóÂ≠óÁ¨¶
     * @return Ë∑≥ËΩ¨ÁªìÊûú
     */
    private static State getState(State currentState, Character character)
    {
    	
        State newCurrentState = currentState.nextState(character);  // ÂÖàÊåâsuccessË∑≥ËΩ¨
        while (newCurrentState == null) // Ë∑≥ËΩ¨Â§±Ë¥•ÁöÑËØùÔºåÊåâfailureË∑≥ËΩ¨
        {
        	
            currentState = currentState.failure();

      **//ËøôÈáåÊàëÊâìÂç∞Âá∫ currentState‰∏∫nullpoint**
           
            newCurrentState = currentState.nextState(character);
        }
       
        return newCurrentState;
    }


ÊàëÁöÑË∞ÉÁî®ÊñπÂºèËøôËøôÊ†∑ÁöÑÔºö

public HanlpTokenizer(String text) {
		//tokenizer = new StringTokenizer(tokens);
		//System.out.println(""ÂΩìÂâçÊñáÊú¨:""+text);
		
		//System.out.println(""ÊûÑÈÄ†ÂáΩÊï∞"");
		tokens = new ArrayList<String>();
		
		
		Segment defaultSegment = StandardTokenizer.SEGMENT
				.enablePartOfSpeechTagging(true).enableOffset(true)
				.enableNameRecognize(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true);
		
		if(text!=null&&text.trim().length()>0)
		{
			//System.out.println(text);
			//List<Term> list=null;
			try {
				//System.out.println(""ËæìÂÖ•Ôºö""+text);
				List<Term> list = defaultSegment.seg(text);
				//System.out.println(list);							
				
				
			} catch (Exception e) {
				// TODO: handle exception
				e.printStackTrace();
				System.out.println(text);
				
				//System.out.println(e);
			}
		}		
		
	}



ÊúÄÂ§ñÂ±ÇÊòØÂú®Â§öÁ∫øÁ®ãÈáåÈù¢Ë∞ÉÁî®ÁöÑ Ë∞¢Ë∞¢"
ËØ∑ÊïôÊó•ÊúüÂÆû‰ΩìËØçÊèêÂèñÁöÑÈóÆÈ¢ò,"ÊàëÂú®‰∫åÂÖÉÁªßÁª≠ËØçÂÖ∏ÈáåÂÆö‰πâ‰∫ÜÊú™##Êï∞@Âπ¥„ÄÅÊú™##Êï∞@Êúà„ÄÅÊú™##Êï∞@Êó•Á≠â
‰πüÊ∏ÖÈô§‰∫ÜËØçÂÖ∏ÁöÑÁºìÂ≠ò
‰ΩÜÂÆûÈôÖÂàÜËØçÊó∂2007Âπ¥1Êúà11Êó•ËøòÊòØ‰ºöË¢´ÂàÜÊàê2007/m, Âπ¥/qt, 1/m, Êúà/n, 11/m, Êó•/b
Â¶Ç‰ΩïËÉΩÂêàÂπ∂Êàê‚Äú2007Âπ¥‚Äù„ÄÅ‚Äú1Êúà‚Äù„ÄÅ‚Äú11Êó•‚ÄùÁîöËá≥ÊòØ‚Äú2007Âπ¥1Êúà11Êó•‚ÄùÂë¢Ôºü
ËØ∑ÊåáÊïô„ÄÇ"
ÂÖ≥‰∫éÂàÜËØçÁöÑ‰∏Ä‰∏™ÈóÆÈ¢ò,Âú®ÂÅö‰∏Ä‰∏™Áà¨Ëô´ÁöÑÈ°πÁõÆÔºåÁî®Âà∞Hanlp ÂàÜËØçÔºåÈÅáÂà∞‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇÊØîÂ¶ÇËØ¥ÔºöÊàëÁöÑÂÖ≥ÈîÆËØçËÆæ‰∏∫‚ÄúÈïøÂÆâ‰∫∫Ê∞ëÊîøÂ∫ú‚ÄùÔºåHanlpÂàÜËØç‰ºöÂàÜÂá∫‚ÄúÈïøÂÆâ‚ÄùËøô‰∏™ËØçÊù•ÔºåÁÑ∂ÂêéÁà¨Ëô´ÂéªÊäìÁöÑÊó∂ÂÄôÔºå‰ºöÊää‰∏Ä‰∫õ‰∫∫Âêç‰πüÊäìËøõÊù•ÔºåÂ§öÂá∫ÂæàÂ§öÂûÉÂúæ‰ø°ÊÅØ„ÄÇËøôÊ†∑ËØ•ÊÄé‰πàÂ§ÑÁêÜÔºü
Á¥¢ÂºïÂàÜËØçÁ≤íÂ∫¶ËÉΩÈÖçÁΩÆÂêóÔºü,ÂÖ∑‰ΩìÈóÆÈ¢òÔºöÊØîÂ¶Ç ‚ÄúÂº†Â∏∏ÂÆÅÂú®Â∏∏ÂÆÅÊâìÁêÉ‚Äù  ÊÄé‰πàÊ†∑‰∏çÂØπÂº†Â∏∏ÂÆÅËøô‰∏™‰∫∫ÂêçÁªßÁª≠ÂàÜÂâ≤
ÂÖ≥‰∫éËßíËâ≤Ê†áÊ≥®ÁöÑÈóÆÈ¢ò,"#ÊàëÊ†áÊ≥®‰∏Ä‰∏™ËØç ÊØîÂ¶ÇÔºö‰∏πËä≠Á¢ß  nr  16  ‰πãÂêé‰ºöÂú®Á®ãÂ∫è‰∏≠ÊòæÁ§∫ ‰∏πËä≠Á¢ß F 8290 B 769 A 266 X 6
Áé∞Âú®ÊàëÊÉ≥ÈÄöËøáÂêåÊ†∑ÁöÑÊñπÂºè  ÊÄé‰πàÊää  ‰∏πËä≠Á¢ß  Ëøô‰∏™ËØçÊ†áÊ≥®ÊàêC Âë¢  Èô§‰∫ÜÂú®nt.txt Áõ¥Êé•Ê†áËÆ∞ÊàêC‰πãÂ§ñÔºåËøòÊúâÂÖ∂‰ªñÁöÑÊñπÂºèËÆ©Ëøô‰∏™ËØçÂèòÊàê CÂêóÔºü "
Âú®ÂØπÁÆÄ‰Ωì‰∏≠ÊñáË∞ÉÁî®HanLP.convertToSimplifiedChineseÂá∫Áé∞ÁöÑ‰∏Ä‰∫õbug,"Âõ†‰∏∫Âú®‰ΩøÁî®‰∏≠Êàë‰ª¨‰∏çÁü•ÈÅìÊñáÁ´†ÊòØÂê¶ÁÆÄ‰ΩìÊàñËÄÖÁπÅ‰ΩìÔºåÊâÄ‰ª•Êàë‰ª¨‰∏ÄÊ¶Ç‰ΩøÁî®ÁÆÄ‰ΩìËΩ¨ÂåñÔºåÂú®Â§ÑÁêÜÂ¶Ç‰∏ãËØçÊ±áÊó∂ÂÄôÈóÆÈ¢ò‰∏•Èáç

> 'Ê∞ë‰πê', 'Â•∂Ê≤π', 'ÊàòÂàóËà∞', 'Êàø‰ª∑', 'Ê†áËá¥', 'Ê≤ÉÂ∞ìÊ≤É', 'ÂØåË±™', 'Â§ß‰ºó', 'È©¨Ëá™Ëææ',
> 'ÂÖãÊãâ', 'ÂéÑÁìúÂ§ö', 'Â°îÂêâÂÖã', 'ÂÆâÁöÑÂàó', 'Â∞ºÊó•', 'Ê¥õÂìà', 'Êº¢Â†°', 'ËâæÊ£ÆË±™', 'Ë£ÅÂà§',
> 'Á¶èÊñØ', 'ËäùÂ£´', 'Ëµ∑Âè∏', 'ÊàøÂ±ã', 'ÂøåÂªâ', 'Âπ≥Ê≤ª',
> 'Á©∫‰∏≠ÂÆ¢ËΩ¶', 'Á¨ëÊòü', 'Ë∞êÊòü', 'Â•îÈ©∞', 'Èõ∂Èí±', 'ËÄÅÂπ¥Áó¥ÂëÜÁóá'

ÊúÄÂç±Èô©ÁöÑÊòØÊää‚Äú‰π†ËøëÂπ≥Ê≤ªÂÖö‚ÄùÊêûÊàê‰∫Ü‚Äú‰π†ËøëÂ•îÈ©∞ÂÖö‚ÄùÔºåÂú®Â§©ÊúùÔºåËøôÂèØÊòØÊùÄÂ§¥ÁöÑÁΩ™

```
In [5]: HanLP.convertToSimplifiedChinese('ÁøíËøëÂπ≥Ê≤ªÈª®')
Out[5]: '‰π†ËøëÂ•îÈ©∞ÂÖö'

```"
HanLP.convertToSimplifiedChinese('Â•îÈ©∞')„ÄÄ=> Ë≥ìÂ£´,Â•ΩÊòéÊòæÁöÑbug
‰ΩøÁî® python Ë∞ÉÁî® Hanlp.parseDependency Êó∂Êä•Èîô,"RTÔºö
[ÊåâÁÖßhttp://www.hankcs.com/nlp/python-calls-hanlp.html](http://www.hankcs.com/nlp/python-calls-hanlp.html) ‰æãÂ≠êËøõË°åÁöÑÊµãËØï
Êä•ÈîôÁöÑlog`java.lang.NullPointerExceptionPyRaisable: java.lang.NullPointerException`
Ê±ÇÂä©~"
nrËØçÂÖ∏Ê†ºÂºèÈóÆÈ¢ò,"‰Ω†Â•ΩÔºÅÊàëÁî®ÁöÑÊòØ1.3.1ÁöÑÁ®ãÂ∫èÂíå1.3.0ÁöÑdata„ÄÇÂú®ÂàÜËØçÊó∂‰ºöÊúâ‰∏Ä‰∫õËØçË¢´ÈîôËØØÁöÑÊ†áÊàê‰∫Ü‰∫∫ÂêçÔºåÊØîÂ¶ÇËÄÉËÄÉ‰Ω†ÔºåÈÉΩÂñúÊ¨¢„ÄÇÊåâÁÖßÊÇ®ÁöÑËØ¥ÊòéÔºåÊàëÊääËøô‰∫õËØçÂä†‰∏ä A 1Ê∑ªÂä†Âú®nr.txtÁöÑÂêéÈù¢Ôºå‰ΩÜÊòØËøêË°åÊó∂Â∞±Âá∫Áé∞ÈîôËØØ‰∫ÜÔºåÊèêÁ§∫ÊòØÔºöÂçÅ‰∏ÄÊúà 24, 2016 10:27:04 ‰∏äÂçà com.hankcs.hanlp.corpus.io.IOUtil readBytes
Ë≠¶Âëä: ËØªÂèñD:/data/data-for-1.3.0/data/dictionary/person/nr.txt.value.datÊó∂ÂèëÁîüÂºÇÂ∏∏java.io.FileNotFoundException: D:\data\data-for-1.3.0\data\dictionary\person\nr.txt.value.dat (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ)
ÂçÅ‰∏ÄÊúà 24, 2016 10:27:04 ‰∏äÂçà com.hankcs.hanlp.dictionary.nr.NRDictionary onLoadValue
‰∏•Èáç: ËØªÂèñD:/data/data-for-1.3.0/data/dictionary/person/nr.txtÂ§±Ë¥•[java.lang.NumberFormatException: For input string: ""1ÈòøÂìà""]
ËØ•ËØçÂÖ∏Ëøô‰∏ÄË°åÊ†ºÂºè‰∏çÂØπÔºöÊ¨ßÂØπ A 1ÈòøÂìà A 1ËÄÉËÄÉ‰Ω† A 1ÂæÆÁõò A 1Â∏ÉÁî®Ë∞¢ A 1ÂçóÈò≥ A 1ÈÉΩÂñúÊ¨¢ A 1
‰∫∫ÂêçËØçÂÖ∏Âä†ËΩΩÂ§±Ë¥•ÔºöD:/data/data-for-1.3.0/data/dictionary/person/nr.txt
ËØ∑ÈóÆËøôÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü"
hanlpÂÜôÁöÑpythonÂáΩÊï∞ÔºåÁî®flaskÂÅöÊúçÂä°Ë∞ÉÁî®Êó∂ÔºåÂá∫Áé∞ÈóÆÈ¢ò,
ÊàëÂèëÁé∞ÊúâÂæàÂ§öÂ∫îËØ•ÊòØÂêçËØçÁöÑËØçË¢´ÂΩí‰∏∫Nature.bÂå∫Âà´ËØçÂØºËá¥ÂéªÂÅúÁî®ËØçÁöÑÊó∂ÂÄôË¢´ÂéªÊéâ,"ÊØîÂ¶ÇÔºö‰øùÂÆâÔºåÊú∫ÁîµÂ∑•Á®ãÂ∏à
‰ΩøÁî®List<Term> list = HanLP.segment(text);ÂàÜËØçÂêé‰∏∫Ôºö[‰øùÂÆâ/b, Ôºå/w, Êú∫Áîµ/b, Â∑•Á®ãÂ∏à/nnt]
ÂéªÂÅúÁî®ËØçCoreStopWordDictionary.apply(list);ÂêéÁöÑÁªìÊûú  Â∑•Á®ãÂ∏à/nnt
ÊàëÂèëÁé∞CoreStopWordDictionary‰∏≠ÊúâÂØπ                
                case 'm':
                case 'b':
                case 'c':
                case 'e':
                case 'o':
                case 'p':
                case 'q':
                case 'u':
                case 'y':
                case 'z':
                case 'r':
                case 'w':
ËØçÊÄßÂºÄÂ§¥ÁöÑËØçÈÉΩËøáÊª§‰∫ÜÔºåÊàëÊÉ≥ÈóÆ‰øùÂÆâÂíåÊú∫Áîµ‰∏∫‰Ωï‰ºöË¢´‰∏∫ËÆ§‰∏∫ÊòØb Âå∫Âà´ËØçÔºü"
‰ΩøÁî®HMM-ViterbiÔºàÊ†áÂáÜÂàÜËØçÔºâÊ®°ÂºèÊó†Ê≥ïËé∑ÂèñËØçÊÄß,"ÁâàÊú¨Âè∑Ôºöhanlp-1.3.1
ÊñπÂºè‰∏ÄÔºö‰ΩøÁî®HanLPÈùôÊÄÅÂàÜËØçÂô®ÂèØ‰ª•Ëé∑ÂèñËØçÊÄß
String text = ""‰πêËßÜË∂ÖÁ∫ßÊâãÊú∫ËÉΩÂê¶ÊâøËΩΩË¥æÂ∏ÉÊñØÁöÑÁîüÊÄÅÊ¢¶"";
Segment segment = HanLP.newSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(text);
System.err.println(termList);

ËæìÂá∫ÁªìÊûúÔºö[‰πê/a, ËßÜ/vg, Ë∂ÖÁ∫ß/b, ÊâãÊú∫/n, ËÉΩÂê¶/v, ÊâøËΩΩ/v, Ë¥æ/nz, Â∏ÉÊñØ/nrf, ÁöÑ/ude1, ÁîüÊÄÅ/n, Ê¢¶/n]

ÊñπÂºè‰∫åÔºöÁõ¥Êé•new ‰∏Ä‰∏™HMMSegmentÁ±ªÊó†Ê≥ïËé∑ÂèñËØçÊÄß
String text = ""‰πêËßÜË∂ÖÁ∫ßÊâãÊú∫ËÉΩÂê¶ÊâøËΩΩË¥æÂ∏ÉÊñØÁöÑÁîüÊÄÅÊ¢¶"";
Segment segment =new HMMSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(text);
System.err.println(termList);

ËæìÂá∫ÁªìÊûúÔºö[‰πê/null, ËßÜ/null, Ë∂ÖÁ∫ß/null, ÊâãÊú∫/null, ËÉΩÂê¶/null, ÊâøËΩΩ/null, Ë¥æÂ∏ÉÊñØ/null, ÁöÑ/null, ÁîüÊÄÅ/null, Ê¢¶/null]

‰∏çÁü•ËøôÊòØ‰∏çÊòØbug
"
ËØ≠‰πâ‰æùÂ≠ò,ÊúâËÆ°Âàí ÂÅö ËØ≠‰πâ‰æùÂ≠òÂêóÔºü
"ÂæàÈ´òÁ∫ß  ÂàÜËØçÊàê‰∫Ü ÂæàÈ´ò  Âíå Á∫ß  ,   Ë¶ÅÊÄé‰πàÂÅö?","[java/nx, ÊòØ/v, ‰∏Ä/m, Áßç/q, ÂæàÈ´ò/d, Á∫ß/q, ÁöÑ/uj, ËØ≠Ë®Ä/n]
"
Ëá™ÂÆö‰πâËØçÂÖ∏indexÂàÜËØçÔºöÂ∏¶‚ÄúÊàë‚ÄùÂ≠óËØçÊ±áÊüê‰∫õÊÉÖÂÜµ‰∏ãÊó†Ê≥ïÂàÜÂá∫,"‰ΩøÁî®indexÊ®°ÂºèÂàÜËØç + Ëá™ÂÆö‰πâËØçÂÖ∏ËøõË°åÂàÜËØçÔºåËÆæÁΩÆÂ¶Ç‰∏ãÔºö
ÈÖçÁΩÆÊñá‰ª∂‰∏≠Ê∑ªÂä†Ôºömydic.txt n ‰Ωú‰∏∫Ëá™ÂÆö‰πâËØçÂÖ∏
ËØçÂÖ∏‰∏≠ÂåÖÂê´Ôºö

> ÈÖ∑ÊàëÈü≥‰πê
> ÈÖ∑Êàë
> ÈÖ∑ÁãóÈü≥‰πê
> ÈÖ∑Áãó

ÂàÜËØçÂô®ÈÖçÁΩÆÔºö
`seg = HanLP.newSegment().enableIndexMode(true).enableNameRecognize(true).enableCustomDictionary(true).enableTranslatedNameRecognize(true);`

ÂØπ‚ÄúÈÖ∑ÊàëÈü≥‰πê‚ÄùÂàÜËØçÔºåÁªìÊûúÈáåÊ≤°ÊúâÈÖ∑Êàë

> [ÈÖ∑ÊàëÈü≥‰πê/n, Èü≥‰πê/n]

ÂØπÈÖ∑ÁãóÈü≥‰πêÔºåÂàôÂèØ‰ª•ÂæóÂà∞È¢ÑÊúüÁªìÊûú

> [ÈÖ∑ÁãóÈü≥‰πê/n, ÈÖ∑Áãó/n, Èü≥‰πê/n]

ÂêéÁªèÊµãËØïÔºåÂèëÁé∞Â¶ÇÊûúËØçÊ±á‰∏≠Â∏¶Áî±ÊàëÂ≠óÔºåÂàôÊó†Ê≥ïÂàÜÂá∫Âè•Â≠ê‰∏≠ËæÉÁü≠ÁöÑÂ∏¶‚ÄúÊàë‚ÄùÂ≠óÁöÑËØçÊ±áÔºåÈô§ÈùûÊ≤°ÊúâÂÖ∂‰ªñÁöÑÂàÜËØçÂèØËÉΩÔºå‰æãÂ¶Ç‰∏ãÈù¢ÂàÜËØçÁªìÊûú

> Â§©Â§©ÈÖ∑ÊàëÔºö[Â§©Â§©/d, ÈÖ∑Êàë/n]

Á±ª‰ººÁöÑ‰æãÂ≠êÔºö
ËØçË°®Â¶Ç‰∏ãÔºö

> Êö¥È£éÈõÜÂõ¢ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏
> Êö¥È£é
> Âêà‰∏Ä‰ø°ÊÅØÊäÄÊúØÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏
> Âêà‰∏Ä
> ÊàëÊü•Êü•‰ø°ÊÅØÊäÄÊúØÔºà‰∏äÊµ∑ÔºâÊúâÈôêÂÖ¨Âè∏
> ÊàëÊü•Êü•

ÂàÜËØçÁªìÊûúÔºö

> 
> Êö¥È£éÈõÜÂõ¢ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏Ôºö[Êö¥È£éÈõÜÂõ¢ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏/n, Êö¥È£é/n, ÈõÜÂõ¢/nis, ËÇ°‰ªΩ/n, ÊúâÈôê/a, ÊúâÈôêÂÖ¨Âè∏/nis, ÂÖ¨Âè∏/nis] ÔºàÂåÖÂê´Êö¥È£éÔºâ
> Âêà‰∏Ä‰ø°ÊÅØÊäÄÊúØÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏Ôºö[Âêà‰∏Ä‰ø°ÊÅØÊäÄÊúØÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏/n, Âêà‰∏Ä/vi, ‰ø°ÊÅØ/n, ‰ø°ÊÅØÊäÄÊúØ/gi, ÊäÄÊúØ/n, Âåó‰∫¨/ns, ÊúâÈôê/a, ÊúâÈôêÂÖ¨Âè∏/nis, ÂÖ¨Âè∏/nis]ÔºàÂåÖÂê´Âêà‰∏ÄÔºâ
> ÊàëÊü•Êü•‰ø°ÊÅØÊäÄÊúØÔºà‰∏äÊµ∑ÔºâÊúâÈôêÂÖ¨Âè∏Ôºö[ÊàëÊü•Êü•‰ø°ÊÅØÊäÄÊúØÔºà‰∏äÊµ∑ÔºâÊúâÈôêÂÖ¨Âè∏/n, Êü•Êü•/v, ‰ø°ÊÅØ/n, ‰ø°ÊÅØÊäÄÊúØ/gi, ÊäÄÊúØ/n, ‰∏äÊµ∑/ns, ÊúâÈôê/a, ÊúâÈôêÂÖ¨Âè∏/nis, ÂÖ¨Âè∏/nis]Ôºà‰∏çÂåÖÂê´ÊàëÊü•Êü•Ôºâ
> ÊàëÊü•Êü•‰ø°ÊÅØÊäÄÊúØÔºà‰∏äÊµ∑ÔºâÊúâÈôêÂÖ¨Âè∏‰ªäÂ§©ÂÆ£Â∏ÉÔºö[ÊàëÊü•Êü•‰ø°ÊÅØÊäÄÊúØÔºà‰∏äÊµ∑ÔºâÊúâÈôêÂÖ¨Âè∏/n, Êü•Êü•/v, ‰ø°ÊÅØ/n, ‰ø°ÊÅØÊäÄÊúØ/gi, ÊäÄÊúØ/n, ‰∏äÊµ∑/ns, ÊúâÈôê/a, ÊúâÈôêÂÖ¨Âè∏/nis, ÂÖ¨Âè∏/nis, ‰ªäÂ§©/t, ÂÆ£Â∏É/v] Ôºà‰∏çÂåÖÂê´ÊàëÊü•Êü•Ôºâ
> ÊàëÊü•Êü•‰ø°ÊÅØÔºö[ÊàëÊü•Êü•/n, Êü•Êü•/v, ‰ø°ÊÅØ/n]ÔºàÊ≤°ÊúâÂÖ∂‰ªñÊõ¥ÈïøÁöÑÂàÜËØçÂèØËÉΩÔºåÊâÄ‰ª•ÂåÖÂê´ÊàëÊü•Êü•Ôºâ

Âõ†‰∏∫ÂàÜËØçÂô®ËÆæÁΩÆ‰∫ÜindexÊ®°ÂºèÔºåÊâÄ‰ª•Â∫îËØ•ÂåÖÂê´Â∞ΩÂèØËÉΩÂ§öÁöÑÂàÜËØçÂèØËÉΩÔºåÂõ†Ê≠§Ëøô‰∏™ÈóÆÈ¢òÊòØÂê¶ÊòØ‰∏Ä‰∏™ÊΩúÂú®ÈóÆÈ¢òÔºü
"
ËØ∑ÊïôËß£ÂÜ≥ÊñπÂêëÔºö‚ÄúÂêéÊù•‚ÄùËøô‰∏™ËØçÔºåÂ¶Ç‰ΩïÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÊ†áÊ≥®‰∏∫Ê≠£Á°ÆÁöÑËØçÊÄßÔºü,"ÈÅáÂà∞‰∫Ü‰∏Ä‰∏™case, ËØ∑Êïô‰∏Ä‰∏ãËß£ÂÜ≥ÁöÑÊñπÂêë„ÄÇ„ÄäÂêéÊù•„ÄãËøôÈ¶ñÊ≠åÔºåÂú®Âè•‰∏≠‰ºöË¢´Ê†áÊ≥®‰∏∫ t. Â¶ÇÊûúÂä†Ëá™ÂÆö‰πâËØçÂÖ∏ nzÔºåÂú®ÂÖ∂‰ªñÈùûÊåá‰ª£ËøôÈ¶ñÊ≠åÁöÑÂè•‰∏≠‰ºöË¢´Ê†áÊ≥®‰∏∫ nz, ÊÉ≥Ë¶ÅÂå∫ÂàÜËøô‰∏§ÁßçÂú∫ÊôØ‰∏ãÁöÑËØçÊÄßÔºåËØ•Â¶Ç‰ΩïËß£ÂÜ≥Ôºü"
ÈááÁî®ÈªòËÆ§Áª¥ÁâπÊØîÂàÜËØçÔºåÈªòËÆ§ÂàÜËØçÁªìÊûú‰∏∫‰ªÄ‰πà‰∏∫Á©∫ÁöÑÔºü,"key=ÁîµÂè∞ÊÉÖÊ≠å
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Áîµ, ÁîµÂè∞]
2:[Âè∞]
3:[ÊÉÖ, ÊÉÖÊ≠å]
4:[Ê≠å]
5:[ ]

Á≤óÂàÜÁªìÊûú[]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][ÁîµÂè∞ÊÉÖÊ≠å  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,ÁîµÂè∞ÊÉÖÊ≠å /A]
Âú∞ÂêçËßíËâ≤ËßÇÂØüÔºö[  S 1139590 A 23975 ][ÁîµÂè∞ÊÉÖÊ≠å  Z 21619956 ]
Âú∞ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,ÁîµÂè∞ÊÉÖÊ≠å /A]
[]
"
Á±ª‰ºº  xxx-xxx-xxx  Ë¢´ËØÜÂà´Êàê‰∏Ä‰∏™ËØç,Â∫îËØ•ÊÄé‰πàÂ§ÑÁêÜ‰∏ãÂë¢?
‰ΩøÁî®ÈªòËÆ§ÂàÜËØçÔºåÂÖ®ËßíÊ†áÁÇπÁ¨¶Âè∑ËØçÊÄßÊ†áÊ≥®Âùá‰∏∫‚Äú/xu‚Äù,"Âçö‰∏ªÊÇ®Â•ΩÔºåÊ∫êÁ†ÅÊòØÊúÄÊñ∞ÁâàÊú¨ÁöÑ„ÄÇÊåâÁÖßËØçÂÖ∏‰∏≠ÁöÑÊ†áÊ≥®‰∏≠ÊñáÊ†áÁÇπÁ¨¶Âè∑ËØçÊÄßÂ∫îËØ•‰∏∫‚Äúw‚ÄùÔºåNature‰∏≠Êõ¥ÊòØÂÆö‰πâ‰∫ÜÊõ¥ÁªÜÂàÜÁöÑÁ±ªÂûãÔºå‰ΩÜÊòØÂàÜËØçÂêéÁ°ÆÈÉΩÊòØ‚Äú/xu‚Äù„ÄÇ
======================ÂàÜËØç‰ª£Á†ÅÔºö
Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        Segment shortestSegment = new DijkstraSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        String[] testCase = new String[]{
                ""\""ÔºàF-35Ôºâ‚ÄúÈó™ÁîµÂ∑•„ÄÇÔºå„Äê„Äë‚Äò„ÄÅ‚Äù\""""
        };
        for (String sentence : testCase)
        {
            System.out.println(""N-ÊúÄÁü≠ÂàÜËØçÔºö"" + nShortSegment.seg(sentence) + ""\nÊúÄÁü≠Ë∑ØÂàÜËØçÔºö"" + shortestSegment.seg(sentence));
        }
==================ÂàÜËØçÁªìÊûúÔºö
[F-/nx, 35/m, ‚Äú/xu, Èó™Áîµ/n, Â∑•/n, ‚Äù/xu]
N-ÊúÄÁü≠ÂàÜËØçÔºö[""/w, Ôºà/xu, F-/nx, 35/m, Ôºâ/xu, ‚Äú/xu, Èó™/vx, ÁîµÂ∑•/nnt, „ÄÇ/xu, Ôºå/xu, „Äê/xu, „Äë/xu, ‚Äò/xu, „ÄÅ/xu, ‚Äù/xu, ""/w]
ÊúÄÁü≠Ë∑ØÂàÜËØçÔºö[""/w, Ôºà/xu, F-/nx, 35/m, Ôºâ/xu, ‚Äú/xu, Èó™/vx, ÁîµÂ∑•/nnt, „ÄÇ/xu, Ôºå/xu, „Äê/xu, „Äë/xu, ‚Äò/xu, „ÄÅ/xu, ‚Äù/xu, ""/w]"
ExtractSummaryÁöÑÈóÆÈ¢ò,"ÊÇ®Â•ΩÔºåÈùûÂ∏∏ÊÑüË∞¢ÊÇ®Êèê‰æõËøô‰πàÂ•ΩÁöÑÂºÄÊ∫êÈ°πÁõÆÔºå‰ΩÜÊòØÂú®ÁúãExtractSummaryÊó∂ÂèëÁé∞ÔºåÂØπÂçöÊñá‰∏≠Ëøô‰∏™‰æãÂ≠êÂÅösummaryÔºåÂæóÂà∞ÁöÑÁªìÊûúÊòØÔºö[Êó†ÈôêÁÆóÊ≥ïÁöÑ‰∫ßÁîüÊòØÁî±‰∫éÊú™ËÉΩÁ°ÆÂÆöÁöÑÂÆö‰πâÁªàÊ≠¢Êù°‰ª∂, ËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢, ÊúâÈôêÁöÑÈùûÁ°ÆÂÆöÁÆóÊ≥ï]ÔºåÂíåÊñá‰∏≠ÊâÄËØ¥‰∏ç‰∏ÄËá¥ÔºåËØ∑ÈóÆÈóÆÈ¢òÂá∫Âú®Âì™ÔºüË∞¢Ë∞¢"
Âçï‰∏™Ê±âÂ≠óÁöÑËØçÊÄß(nature)ÊòØÂ¶Ç‰ΩïÁªôÂÆöÁöÑÔºü,Âçï‰∏™Ê±âÂ≠óÂ¶ÇÊûúÊ≤°ÊúâÂá∫Áé∞Âú®Ê†∏ÂøÉËØçÂÖ∏‰∏≠ÔºåÂÆÉÁöÑËØçÊÄßÊòØÂ¶Ç‰ΩïÁªôÂÆöÁöÑÔºü
ÈáçÂêØ,‰∏çÂ∫îËØ•ÊòØzhong4qi3ÔºåÂ∫îËØ•ÊòØchong3qi3
ÂÖ≥‰∫éÊú∫ÊûÑËØÜÂà´ÁöÑÈóÆÈ¢ò,1„ÄÅ‰ºÅ‰∏öÂêçÁß∞ÂèØ‰ª•ËØÜÂà´‰ΩÜÊòØ‰ºÅ‰∏öÁöÑÁÆÄÁß∞ÊÄé‰πàÂéªËØÜÂà´Âë¢ÔºåÊúâÊ≤°ÊúâÁ±ª‰ººÁöÑÊñπÊ≥ïÂë¢
HanLP‰øÆÊîπ‰∏ªËØçÂÖ∏ÈúÄË¶ÅÊ≥®ÊÑè‰ªÄ‰πàÔºü,"ËØ∑ÈóÆ‰∏Ä‰∏ãÔºö
È°πÁõÆÈúÄË¶ÅÔºåÊÉ≥Áî®Ëá™Â∑±ÁöÑËØçÂ∫ìÂàÜËØç„ÄÇ
‰ΩÜÊòØÂÅöÂ∏¶ËØçÊÄßÂíåËØçÈ¢ëÁöÑËØçÂÖ∏ÔºåËøòÊúâÂØπÂ∫îÁöÑBiGramËØçÂÖ∏ÂèàÊ≤°ÊúâÂ∑•ÂÖ∑ÂÅöÔºåÊâÄ‰ª•ÊàëÊÉ≥ÊääHanLPËØçÂÖ∏‰∏≠ÂíåÊàëËá™Â∑±ËØçÂÖ∏‰∏≠ÈÉΩÊúâÁöÑËØç‰øùÁïôÔºõHanLPËØçÂÖ∏‰∏≠ÊúâÁöÑÔºåÊàëËá™Â∑±ËØçÂÖ∏‰∏≠Ê≤°ÊúâÁöÑËØçÂà†ÊéâÔºõHanLPËØçÂÖ∏‰∏≠Ê≤°ÊúâÔºåÊàëËá™Â∑±ËØçÂÖ∏‰∏≠ÊúâÁöÑËØçÊîæÂà∞customËØçÂÖ∏‰∏≠ÔºåËøôÊ†∑Èù†Ë∞±‰∏çÔºü

ÊàëËÉΩÊÉ≥Âà∞ÁöÑÈóÆÈ¢òÊúâ‰∏§‰∏™Ôºö
1. CoreDictionaryPathËøô‰∏™ÈÖçÁΩÆÂØπÂ∫îÁöÑËØçÂÖ∏‰∏≠ÁöÑÂÜÖÂÆπÔºåÈô§‰∫ÜÂ∏¶‚Äú##‚ÄùÁöÑÂ≠óÁ¨¶‰∏≤Â§ñÔºàËøô‰∏™Âà†‰∫Ü‰ºöÊä•java.lang.ArrayIndexOutOfBoundsExceptionÂºÇÂ∏∏ÔºâÔºåÂà´ÁöÑËØçÊù°ÊàëÂèØ‰ª•ÈöèÊÑèÂ¢ûÂà†ÂêóÔºü
2. Êîπ‰∫ÜCoreDictionaryPathÈÇ£‰∏™ËØçÂÖ∏ÔºåÂØπÂ∫îÁöÑBiGramDictionaryPathËØçÂÖ∏ÔºåÂ¶ÇÊûúËøòÊòØÁî®ÂéüÊù•ÁöÑÔºå‰ºö‰∏ç‰ºöÊúâ‰ªÄ‰πàÈóÆÈ¢òÔºü
3. ËøòÊúâÊ≤°ÊúâÂà´ÁöÑÈóÆÈ¢òÔºü

Ë∞¢Ë∞¢„ÄÇ
"
Âª∫ËÆÆÈááÁî®MapDBÈ°πÁõÆÊèê‰æõÁöÑMapÊù•ÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®„ÄÇ,"[MapDB](https://github.com/jankotek/mapdb)ÊòØ‰∏Ä‰∏™Âü∫‰∫émmapÊàñRAFÁöÑÈÄöÁî®ÂÆπÂô®Â∫ì„ÄÇ
ÁâπÁÇπÂ∞±ÊòØÈùûÂ∏∏ËäÇÁ∫¶JavaÊâòÁÆ°Â†Ü„ÄÇ
"
CRFÂàÜËØç‰∏éËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÁªìÂêàÂàÜËØç„ÄÇ,"HiÔºå‰Ω†Â•ΩÔºåËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºåÂÖ≥‰∫é‰ΩøÁî®CRFÁªüËÆ°ÂàÜËØçÂàáËØçÊó∂ÔºåÂá∫Áé∞badcase‰øÆÂ§çÔºåÁÑ∂ÂêéÂ∏åÊúõÂèØ‰ª•ÈÄöËøáËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊñπÂºèÂéªË∞ÉÊï¥ÂàáËØçÁªìÊûú„ÄÇËøô‰∏™ÂäüËÉΩÊúâÂÆûÁé∞ÂêóÔºüÊàñËÄÖCRFÂàáËØçÂá∫Áé∞‰∫ÜbadcaseÔºåÊàëËøôËæπÊúâ‰ªÄ‰πàÊñπÊ≥ïÂéªË∞ÉÊï¥Âë¢„ÄÇ
"
Ë™çÁÇ∫-ËÆ§‰∏∫ÔºàËÄåÈùûÔºöËÆ§ÁÇ∫Ôºâ,"to have a particular opinion about sth Ë™çÁÇ∫ÔºõË¶ñÁÇ∫ÔºõÁõ∏‰ø°‚Äî‚Äî ÁâõÊ¥•È´òÈò∂Ëã±Ê±âÂèåËß£ËØçÂÖ∏Á¨¨ÂÖ´ÁâàÔºàÁπÅ‰ΩìÁâàÔºâ

‰ΩúËÄÖÊòØÂê¶ËÄÉËôëÈ¶ôÊ∏Ø„ÄÅÂè∞ÊπæÂºÇ‰ΩìÂ≠óÂ§ÑÁêÜÔºåÂèÇËßÅopenccÈ°πÁõÆ
"
ËÉΩÂê¶Âä†ÂÖ•ÊãÜËØçÂíåÂêàËØçÁõ∏Â∫îÁöÑÊîØÊåÅ,"‚ÄúÁæéÂõΩÁ≠æËØÅ‚Äù->""ÁæéÂõΩ""Ôºå""Á≠æËØÅ""
‚Äú‰∏≠ÂõΩ‚ÄùÔºå""‰∫∫Ê∞ë"" ->""‰∏≠ÂõΩ‰∫∫Ê∞ë""
ÂèÇËÄÉ`https://github.com/ysc/word`ÁöÑ12„ÄÅrefine
"
ÈááÁî®ÈªòËÆ§ÂàÜËØçÁªìÊûú‰∏çÂáÜÁ°Æ,"ÈááÁî®ÈªòËÆ§ÂàÜËØç
ËæìÂÖ•ÂÜÖÂÆπ‰∏∫Ôºö ÈáëÈò≥ËßÇÈü≥Â±±ÊîØË°åÔºåÂàÜËØçÁªìÊûúÔºö
[ÈáëÈò≥/ns, ËßÇÈü≥Â±±/nz, ÊîØË°å/n]
‰ΩÜÊòØÈááÁî® NShortSegmentÂàÜËØçÔºåÁªìÊûú‰∏∫
[ÈáëÈò≥ËßÇÈü≥Â±±ÊîØË°å/nt]

ÈªòËÆ§ÁöÑÂàÜËØçÂÆû‰æã‰∏∫Ôºö
public static Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
NshortÂàÜËØçÂÆû‰æã‰∏∫Ôºö
private static Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);

Âè¶Â§ñ‰∏Ä‰∏™ÈóÆÈ¢òÊòØÔºö ‰∏∫‰ªÄ‰πàNshortÂàÜËØç‰∏çÊîØÊåÅËá™ÂÆö‰πâÂ≠óÂÖ∏Ôºü
"
surefireÊèí‰ª∂ÔºåÈÅøÂÖçmvn test ide console‰π±Á†Å,
‰ΩøÁî®StandardTokenizerÂàÜËØçÊó∂Âá∫Áé∞Êï∞ÁªÑË∂äÁïåÁöÑÈîôËØØÔºàÁî®CustomDictionaryÂ¢ûÂä†‰∫Ü‰∏Ä‰∫õËØçÔºâ,"Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 45
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:140)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:101)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:40)
"
Â¶Ç‰ΩïÂ∞ÜHanLP‰ª£Á†Å‰∏édataÁõÆÂΩï‰∏ãÁöÑËØçÂÖ∏Êñá‰ª∂ÊâìÊàê‰∏Ä‰∏™jar?,
ÁÆÄÁπÅ‰ΩìËΩ¨Êç¢ÊòØÂê¶Áî®Âà∞jpinyinÂåÖÔºü,
ÂºÄÂêØNormalizationÂêéÂàÜËØçÁªìÊûúÂá∫ÈóÆÈ¢ò,"ÁâàÊú¨1.3.1

‰ª£Á†ÅÔºö
`HanLP.Config.Normalization = true;
StandardTokenizer.SEGMENT.enableOrganizationRecognize(true);
System.out.println(StandardTokenizer.segment(""ÊàëÁöÑÁà±Â∞±ÊòØÁà±Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÇ""));`

ËøêË°åÁªìÊûúÊòØÔºö

> [Êàë/rr, ÁöÑ/ude1, Áà±/v, Â∞±ÊòØ/v, Áà±Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ./nt]
"
portable-1.2.11ÁâàÊú¨‚ÄúÂ∑•‰ø°Â§Ñ‚ÄùÂú®indexÊ®°ÂºèÂàÜËØçÂàÜÊàê‚ÄúÂ∑•‰ø°Â§Ñ/n Â∑•‰ø°Â§Ñ/nt‚Äù,"ËØ∑ÈóÆ‰∏Ä‰∏ãÔºå‰∏∫‰ªÄ‰πà‰ºöÂàÜÂá∫Êù•‰∏§‰∏™‰∏çÂêåËØçÊÄßÁöÑËØçÔºüËÄå‰∏îÊàëÁúãËØçÂÖ∏‰πüÊ≤°ÊâæÂà∞Â∑•‰ø°Â§ÑËØçÊÄß‰∏∫nÁöÑËØçÊù°„ÄÇ
"
Ê†áÂáÜÂàÜËØçÔºåÊúÄÁü≠Ë∑ØÂàÜËØçÔºåNÊúÄÁü≠Ë∑ØÂàÜËØçÁöÑÂå∫Âà´,"ÊàëÂéüÊù•ÁöÑÁêÜËß£ÊòØÔºö
Ê†áÂáÜÂàÜËØçÂ∞±ÊòØÁî®Âä®ÊÄÅËßÑÂàíÂéªÁÆóËØçÂõæÁöÑÊúÄÁü≠Ë∑ØÂæÑ
ÊúÄÁü≠Ë∑ØÂàÜËØçÂ∞±ÊòØÁî®DijkstraÁÆóÊ≥ïÁÆóËØçÂõæÁöÑÊúÄÁü≠Ë∑ØÂæÑ
NÊúÄÁü≠Ë∑ØÂ∞±ÊòØÁî®ÈÇ£‰∏™‰ªÄ‰πàNShortPathÁÆóÊ≥ïÁÆóËØçÂõæÁöÑÊúÄÁü≠Ë∑ØÂæÑ
Ëøô‰∫õÂàÜËØçÊñπÂºè‰ªÖ‰ªÖÊòØÁÆóÊ≥ï‰∏çÂêåÔºåÊúÄÁªàÁöÑÂàÜËØçÊïàÊûúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ

‰ΩÜÊñáÊ°£‰∏≠ËØ¥Ôºö

```
NÊúÄÁü≠Ë∑ØÂàÜËØçÂô®NShortSegmentÊØîÊúÄÁü≠Ë∑ØÂàÜËØçÂô®ÊÖ¢Ôºå‰ΩÜÊòØÊïàÊûúÁ®çÂæÆÂ•Ω‰∏Ä‰∫õÔºåÂØπÂëΩÂêçÂÆû‰ΩìËØÜÂà´ËÉΩÂäõÊõ¥Âº∫„ÄÇ
```

ÊâçÁü•ÈÅìËøô‰∫õÊñπÂºèÂèØËÉΩ‰∏ç‰ªÖ‰ªÖÊòØÊ±ÇÊúÄÁü≠Ë∑ØÂæÑÁöÑÁÆóÊ≥ïÊúâÂå∫Âà´ÔºåÊúÄÁªàÁöÑÂàÜËØçÊïàÊûúÈÉΩÊòØ‰ºö‰∏ç‰∏ÄÊ†∑ÁöÑ„ÄÇ

ÊàëÊÉ≥ËØ∑ÈóÆ‰∏Ä‰∏ãÔºö
1. Ëøô‰∏â‰∏™ÂàÜËØçÁÆóÊ≥ïÊúâ‰ªÄ‰πàÂå∫Âà´Ôºü
2. ‰∏∫‰ªÄ‰πàËÉΩÂá∫‰∏çÂêåÁöÑÂàÜËØçÊïàÊûúÔºü
"
Âä®ÊÄÅËá™ÂÆö‰πâËØçÂÖ∏,"saas‰∫ëÂπ≥Âè∞‰ΩøÁî®ÔºåÊØè‰∏™ÂÆ¢Êà∑ÈÉΩÊúâËá™Â∑±ÁöÑËá™ÂÆö‰πâËØçÂÖ∏ÔºåÈúÄË¶ÅÂú®ÊØèÊ¨°ÂàÜËØçËøáÁ®ã‰∏≠Â∞ÜÂÆ¢Êà∑ÁöÑËá™ÂÆö‰πâËØçÂÖ∏‰º†ÂÖ•ÂàÜËØçÊñπÊ≥ï„ÄÇÂÆûÁé∞ÈíàÂØπ‰∏çÂêåÂÆ¢Êà∑ÁöÑ‰∏çÂêåÂàÜËØçÊïàÊûú„ÄÇ
ÂàÜËØçÁöÑÈÖçÁΩÆ‰πüÊòØÂ¶ÇÊ≠§„ÄÇ
"
"ÂàÜËØç‰∏≠Âá∫Áé∞‰∫ÜÊºèÂ≠óÁé∞Ë±°, ÊòØÂê¶‰∏∫bug","Âú®‰∏çÈááÁî®Êó•ÊñáÂßìÂêçËØÜÂà´ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØπ‚ÄúÂåóÂ∑ùÊôØÂ≠êÂèÇÊºî‰∫ÜÊûóËØ£ÂΩ¨ÂØºÊºîÁöÑ„ÄäÈÄüÂ∫¶‰∏éÊøÄÊÉÖ3„Äã‚Äù ËøõË°åÂàÜËØçÔºå
ÁªìÊûú‰∏∫Ôºö
‚Äú[ÂåóÂ∑ù/nr, ÊôØ/ng, ÂèÇÊºî/v, ‰∫Ü/ul, ÊûóËØ£ÂΩ¨/nr, ÂØºÊºî/n, ÁöÑ/uj, „Ää/w, ÈÄüÂ∫¶/n, ‰∏é/p, ÊøÄÊÉÖ/n, 3/m, „Äã/w]‚Äù
ÂèëÁé∞‚ÄòÂ≠ê‚ÄôÂ≠ó‰∏¢Â§±„ÄÇ
"
ËØ∑ÈóÆËÉΩÂê¶Ê†πÊçÆÊú∫ÊûÑÂêç„ÄÅÂú∞ÂùÄÂêçËØÜÂà´ÊñπÊ≥ïËá™ÂÆö‰πâÂÖ∂ÂÆÉÂÆû‰ΩìËØÜÂà´ÊñπÊ≥ïÔºåÊúâÈÄöÁî®ÁöÑÊ®°Êùø‰πàÔºüË∞¢Ë∞¢ÔºÅ,
ËØ∑ÈóÆÂ¶ÇÊûúÂ∞ÜËá™Â∑±ÁöÑÊñáÊú¨ËÆ≠ÁªÉ‰∏∫CRFÂ≠óÂÖ∏Ôºü,
MDAGsetÁöÑ‰ΩúÁî®?,
HanLP.s2hkÊä•ÈîôÊú™ÂÆö‰πâ,"ÊÇ®Â•ΩÔºåÊñ∞Âª∫‰∫Ü‰∏Ä‰∏™ÊµãËØïÈ°πÁõÆÔºåÂºïÁî®1.2.11ÁâàÊú¨ÁöÑjar ‰∏Ä‰∫õtest‰∏ãÁöÑ‰ª£Á†ÅÊä•ÈîôÔºåÂ¶ÇÊ†áÈ¢òÊâÄÁ§∫ÔºåËØ∑Êú¨ËøôÊòØÊÄé‰πàÂõû‰∫ãÔºü
"
Âêå‰πâËØçÂÖ∏ÁöÑÊ†ºÂºè,"hankcs‰Ω†Â•ΩÔºåÂèØ‰ª•Ëß£Èáä‰∏Ä‰∏ãÂêå‰πâËØçÂÖ∏ÁöÑÊ†ºÂºèÂêóÔºü
"
ÊñáÊú¨‰∏≠ÊúâËã±ÊñáÊó∂ÔºåÂàÜËØçÁªìÊûúÊúâÈóÆÈ¢ò,"hello

world
‰∏äÈù¢ÊòØÂéüÂßãÊñáÊú¨ÔºàÂ≠òÂÇ®Âú®‰∏Ä‰∏™Êñá‰ª∂ÔºâÔºå‰∏≠Èó¥Êúâ‰∏™Êç¢Ë°å„ÄÇÂàÜËØçÁªìÊûú‰∏∫Ôºö
<hello

world>
Â∞ñÊã¨Âè∑Êã¨Ëµ∑Êù•ÁöÑÊòØ‰∏Ä‰∏™ËØçÔºåÊ≤°ÊúâÂàÜËØçhelloÂíåworld„ÄÇ
Áõ¥Êé•‰ΩøÁî®ÁöÑHanLP.segmentÊñπÊ≥ï
Â¶Ç‰ΩïËß£ÂÜ≥Ôºü
"
Áî®Êà∑ËØçÂ∫ìÁöÑÈóÆÈ¢ò,"ÁõÆÂâçËá™Â∏¶ÁöÑCustomDictionary.txt.binÊòØ17MÂ§öÔºåÊàëÂú®CustomDictionary.txt‰∏≠Ê∑ªÂä†Ëá™ÂÆö‰πâËØç‰πüÂèØ‰ª•Ëµ∑‰ΩúÁî®Ôºå‰ΩÜÊòØÊñ∞ÁöÑËá™Âä®ÁîüÊàêÁöÑCustomDictionary.txt.binÂè™Êúâ685KÔºåËÄå‰∏îÂàÜËØçÊïàÊûú‰∏ãÈôç‰∫ÜÔºåËØ∑ÈóÆ17MÁöÑCustomDictionary.txt.binÊòØÂ¶Ç‰ΩïÁîüÊàêÁöÑÔºüËøòÊòØÈúÄË¶ÅËøêË°å‰ªÄ‰πàÂáΩÊï∞Ôºü
"
ÂèØ‰ª•Êèê‰æõÁîüÊàêËØ≠ÊñôÂ∫ìÁöÑÊµãËØïÁî®‰æãÂêóÔºü,"ÊàëÊÉ≥ÁîüÊàêËá™Â∑±ÁâπÂÆöË°å‰∏öÈ¢ÜÂüüÁöÑËØçÂ∫ìÔºåÈúÄË¶ÅÁî®Âà∞hanlpÁöÑËØçÂ∫ìÁîüÊàêÂ∑•ÂÖ∑ÔºåÁîüÊàêÁ±ª‰ººCoreNatureDictionary.ngram.txtÂíåCoreNatureDictionary.txtÁöÑÊñá‰ª∂„ÄÇÂ∏åÊúõËÉΩÊèê‰æõ‰ª£Á†ÅNatureDictionaryMaker.java‰∏ã 139Ë°åÂàóÂá∫ÁöÑ‚ÄúD:\JavaProjects\CorpusToolBox\data\2014‚ÄùË∑ØÂæÑ‰∏ãÁöÑÊñá‰ª∂ÔºåÊàëÊñπ‰æøÊï¥ÁêÜËØ≠ÊñôÊ†ºÂºè„ÄÇ
"
CRFÂàÜËØçÂá∫Áé∞ÂÜÖÂ≠òÊ∫¢Âá∫,"Hankcs~ÊàëÂú®‰ΩøÁî®ÂêÑÁ±ªÂàÜËØçÂô®ÊØîËæÉÁªìÊûúÊó∂ÔºåÂ¶ÇÊûúÁõ¥Êé•‰ΩøÁî®CRFÂàÜËØçÂô®ÁªìÊûúÊ≠£Â∏∏ÔºåÂ¶ÇÊûúÂâçÈù¢‰ª£Á†ÅÂ∑≤ÁªèËøêË°åËøá‰∏Ä‰∏™ÂàÜËØçÂô®ÔºåÊé•ÁùÄËøêË°åCRFÁöÑÊó∂ÂÄô‰ºöÊä•ÂÜÖÂ≠òÊ∫¢Âá∫„ÄÇ
"
Âíåelasticsearch2.3.4ÈõÜÊàêÂêéÊä•Èîô,"Áî®1.2.5Âíåes2.3.4ÈõÜÊàêÂêéÔºåÊèíÂÖ•esÊï∞ÊçÆÂíåÊµãËØïesÂàÜËØçÊïàÊûúÊó∂Êä•Â¶Ç‰∏ãÈîôËØØÔºö
{
  ""error"" : {
    ""root_cause"" : [ {
      ""type"" : ""no_class_def_found_error"",
      ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
    } ],
    ""type"" : ""no_class_def_found_error"",
    ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
  },
  ""status"" : 500
}

Â†ÜÊ†à‰∏∫Ôºö

Caused by: NotSerializableExceptionWrapper[no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex]
        at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
        at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:62)
        at com.hylanda.hanlp.plugin.HanlpTokenizer.incrementToken(HanlpTokenizer.java:101)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
ËØ∑ÈóÆVertexËøô‰∏™Á±ªÂàùÂßãÂåñÁöÑÊó∂ÂÄôÊòØ‰∏çÊòØÊúâÊìç‰ΩúÊñá‰ª∂ÁöÑ‰ª£Á†ÅÔºüÊàëÊÄÄÁñëÊòØÊùÉÈôêÈóÆÈ¢ò
"
HanlpÁöÑ‰∏≠ÊñáÂàÜËØçÂíå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê‰ΩøÁî®ÁöÑËØçÂ∫ìÊòØÂê¶‰∏ÄËá¥Ôºü,"ËØ∑ÈóÆhancksÔºåHanlpÁöÑ‰∏≠ÊñáÂàÜËØçÂíå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê‰ΩøÁî®ÁöÑËØçÂ∫ìÊòØÂê¶‰∏ÄËá¥ÔºüÂØπÂêå‰∏ÄÊÆµËØùÔºåÂàÜËØçÂäüËÉΩÁöÑÊÆµËØçÊñπÊ≥ïÊòØÂê¶‰ºöË∑ü‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂäüËÉΩ‰∏ç‰∏ÄËá¥ÔºüÂ§öË∞¢Â§ßÁ•ûÔºÅ
"
ÊãºÈü≥ËΩ¨Êç¢‰∏≠Ëã±ÊñáÂ≠óÁ¨¶‰∏¢Â§±,"Âú®‰∏≠Êñá‰∏éÊãºÈü≥ÁöÑËΩ¨Êç¢ËøáÁ®ã‰∏≠ÔºåÂè™Ë¶ÅËæìÂÖ•Âè•Â≠ê‰∏≠Âê´ÊúâÈùû‰∏≠ÊñáÂ≠óÁ¨¶ÔºåÂ∞±ËæìÂá∫noneÔºåÂª∫ËÆÆËÉΩÂ§ü‰øùÁïôÂéüÊñá‰∏≠ÁöÑÈùû‰∏≠ÊñáÂ≠óÁ¨¶ÂíåÊ†áÁÇπÔºåÂ¶ÇÂΩìÂâçËæìÂÖ•‚ÄúÊà™Ëá≥2012Âπ¥Ôºå‚ÄùÂ∞±‰ºöËæìÂá∫ ‚Äú ji√© zh√¨  none  ni√°n none‚ÄùÔºåÂª∫ËÆÆËæìÂá∫‚Äú ji√© zh√¨  2012 ni√°nÔºå‚Äù
"
ÂÖ≥‰∫éÊ®°ÂûãÈóÆÈ¢ò,"@hankcs ÊÇ®Â•Ω„ÄÇ ËØ∑ÈóÆËæìÂÖ•Ê®°ÂûãÊòØÂê¶ÂèØ‰ª•Áº©ÂáèÂ∞∫ÂØ∏ÔºüÂú®AndroidËÆæÂ§á‰∏äË∑ëÁ¶ªÁ∫øÁâàÊú¨ÔºåÂèëÁé∞‰ΩøÁî®‰æùÂ≠òËØ≠Ê≥ïÊé•Âè£ÁöÑÊó∂ÂÄôÔºåËΩΩÂÖ•Ê®°ÂûãÊúâ800MÂ§ßÂ∞èÔºåÂØºËá¥ËÆæÂ§á‰∏çËÉΩÊ≠£Â∏∏ËøêË°å„ÄÇËØ∑ÈóÆÊ®°ÂûãÂä†ËΩΩËøáÁ®ãÊòØÁî®Âà∞Áõ∏Â∫îÊé•Âè£ÁöÑÊó∂ÂÄôÂÜçÂä†ËΩΩËøòÊòØÂàùÂßãÂåñÁöÑÊó∂ÂÄôÁªü‰∏ÄÂä†ËΩΩÔºü
"
Hanlp SegmentÁ±ªsegÂàÜËØçÊñπÊ≥ïÊä•ÂÜÖÂ≠òÊ∫¢Âá∫ÈîôËØØ,"Âú®‰ΩøÁî®HanlpÂØπËé∑ÂèñÁöÑÊñáÊú¨ËøõË°åÈ¢ÑÂ§ÑÁêÜÁöÑÊó∂ÂÄôÔºåÂΩìÂ§ÑÁêÜÁöÑÊòØÈáçÂ§çÁöÑÁõ∏ÂêåÂÜÖÂÆπÊñáÊú¨Êó∂Ê≠£Â∏∏ÔºåÂΩìÂ§ÑÁêÜÂ§ßÈáè‰∏çÂêåÊñáÊú¨Êó∂Êä•ÂÜÖÂ≠òÊ∫¢Âá∫ÁöÑÈîôËØØ„ÄÇ
`for (File txt : files) {
                    content = readtxt(txt.getAbsolutePath());
                    // ÂàÜËØç
                    /*System.out.println(content);*/
                    word_arr = StopWordsHandler.mPhraseDel(segmentNoTag.seg(content));
                /*  word_arr = new ArrayList<String>();
                    for (int i = 0; i < 10000; i++){
                        word_arr.add(""Â§ßÂÜíÈô©"");
                    }*/
                    //System.out.println(""content:""+content+""ÔºõsizeÔºö""+word_arr.size());
                    fileContent.add(StringUtils.strip(word_arr.toString().replaceAll("","", ""\t""),""[]"")+""\t"");
                    count++;
                }`
Â¶ÇÊûúÊääcontentÊç¢ÊàêÂõ∫ÂÆöÂ≠óÁ¨¶‰∏≤Â∞±‰∏ç‰ºöÊä•Èîô„ÄÇËØ∑ÈóÆÊòØ‰ªÄ‰πàÂéüÂõ†Âë¢Ôºü
"
ÊÄé‰πàÂ∞ÜHanLPÁöÑdataÂíåsource‰∏ÄËµ∑ÊâìÂåÖ,"Áé∞Âú®ÈúÄË¶Å‰øÆÊîπdata‰∏≠ÂÅúÁî®ËØçÁöÑ‰∏Ä‰∫õÊï∞ÊçÆÔºåÁÑ∂ÂêéÂ∞ÜsourceÂíådataÈáçÊñ∞ÊâìÂåÖÊàêjarÔºåÁÑ∂ÂêéÂÜçÂºÄÂèë‰∏≠‰ΩøÁî®ÔºåÂÖ∂‰∏≠dataÊâìÂåÖÊàêÁ±ª‰ººÂÆòÊñπsbt‰∏≠ÁöÑ‰∫åËøõÂà∂ÂΩ¢ÂºèÔºåËØ∑ÈóÆË¶ÅÊÄé‰πàÂÅöÂïäÔºü 
"
Ëá™ÂÆö‰πâËØçÂÖ∏ÂàÜËØçÈóÆÈ¢ò,"ÈÅáÂà∞Ëøô‰πà‰∏Ä‰∏™ÈóÆÈ¢ò. ËØ∑ÊåáÊïô
ËæìÂÖ• ""ÂæÆÊêúÈõÜÊàêÊâãÂÜå""     ÂàÜËØçÁªìÊûú   [ÂæÆ, ÊêúÈõÜ, Êàê, ÊâãÂÜå]
Â¶ÇÊûúÊää  ""ÂæÆÊêú"" Âä†ÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏‰∏≠.  ÂàÜËØçÁªìÊûúËøòÊòØËøôÊ†∑.
Âú® WordBasedGenerativeModelSegment ‰∏≠ÁöÑGenerateWordNetÊñπÊ≥ï ÁîüÊàêËØçÁΩë,Ê≥®Èáä‰∫ÜËá™ÂÆö‰πâÂàÜËØçÁöÑ‰ª£Á†Å.   Â¶ÇÊûúÂºÄÂêØÂêé. ÂèØ‰ª•ÂæóÂà∞ÂàÜËØçÁªìÊûú  [ÂæÆÊêú, ÈõÜÊàê, ÊâãÂÜå].   ËøôÊÆµ‰ª£Á†ÅÂºÄÂêØÊòØÂê¶ÊúâÂÖ∂‰ªñÂΩ±Âìç
![image](https://cloud.githubusercontent.com/assets/4652356/17685809/050db75a-639a-11e6-9e72-58464762d8bf.png)

Âú®ÁîüÊàêËØçÁΩëÂêéÂÜçÂºïÂÖ•Ëá™ÂÆö‰πâËØçÂÖ∏ÊòØ‰∏çÊòØÂ∞±Âè™ËÉΩÂØπÊú™ÊàêËØçÁöÑÂ≠óËøõË°åÂêàÂπ∂‰∫Ü
if (config.useCustomDictionary)
 {
        combineByCustomDictionary(vertexList);
}
"
‰ΩøÁî®AhoCorasickDoubleArrayTrieSegmentÂàÜËØçÔºåËá™ÂÆö‰πâËØçÂÖ∏Âä†ËΩΩÊàêÂäüÔºåËá™ÂÆö‰πâËØçÂÖ∏‰∏≠Êúâ‰∏ÄÊù°‰∏∫‚ÄúÈÖ∏ n 1000‚ÄùÂíå‚ÄúÁ¢± n 1000‚Äù ‚ÄúÈáëÂ±û n 1000‚ÄùÔºåÂ¶ÇÊûúÂæÖÂàÜËØçÁöÑÂè•Â≠ê‰∏∫‚ÄúÈÖ∏ÁöÑ‚ÄùÔºåÂàÜËØçÁªìÊûú‰∏∫‚ÄúÈÖ∏ÁöÑ nz‚Äù„ÄÇÂ¶ÇÊûúÊòØ‚ÄúÈáëÂ±ûÁöÑ‚ÄùÔºåÁªìÊûúÂ∞±Ê≠£Â∏∏Ôºå‰∏∫‚ÄúÈáëÂ±û n  ÁöÑnz‚ÄùÔºåÁªèÊµãËØïÁöÑÔºåËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÊúâÂçï‰∏™Â≠óÁöÑÊÉÖÂÜµÈÉΩÊúâËøô‰∏™bug,
Ëá™ÂÆö‰πâËØçÊÄßÈîôËØØ,"ÁâàÊú¨ÔºöHANLP-1.2.10 
ÁéØÂ¢É: JDK 1.8.0_45 64‰Ωç WIN10
Êñ∞Â¢ûÁöÑÂäüËÉΩËá™ÂÆö‰πâËØçÊÄßDEMOÁ¨¨‰∏Ä‰∏™ÂèØ‰ª•Ê≠£Â∏∏ËøêË°åÔºå‰ΩÜÊòØÂ∞ùËØïÂ∞ÜËØçÊÄßËµã‰∫àÂà∞Êüê‰∏™ËØçÁöÑÊó∂ÂÄôÊä•Èîô
ÊØîÂ¶ÇÔºö
Nature myNature = Nature.create(""DEFINED"");
 LexiconUtility.setAttribute(""Êüê‰∏™ËØç"", myNature );
‰ªéËØçÊÄßÊù•ÁúãÔºå‰∏çÁü•ÈÅìÂØπËøô‰∏™ËØçÊúâÊ≤°ÊúâÁâπÊÆäË¶ÅÊ±Ç

DEMOÁ¨¨‰∫åÈÉ®ÂàÜÊèíÂÖ•Âà∞Áî®Êà∑ËØçÂÖ∏‰πüÊä•Èîô
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
    at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:118)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:470)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
    at MainServer.main(MainServer.java:71)
"
ÂÖ®ÂõΩË°åÊîøÂå∫ÂàíÂú∞ÂùÄ,"ÊúÄËøëÂ∑•‰ΩúÈúÄË¶Å‰ªéÁªüËÆ°Â±ÄÁΩëÁ´ôÁà¨ÁöÑ‰∏Ä‰∏™Êï∞ÊçÆÔºåÁúãÊúâÈúÄË¶Å‰∏çÔºüÁôæÂ∫¶‰∫ëÈìæÊé•: https://pan.baidu.com/s/1kUADAYV ÂØÜÁ†Å: hau3
"
java Servlet ‰∏≠ out.println(word.HEAD);Êó†Ê≥ïËæìÂá∫,"   ÂØπ‰∫éCoNLLÊ†ºÂºèÔºåÂæàÂ•áÊÄ™ÔºåÂÖ∂‰ªñÂ≠óÊÆµÈÉΩÂèØ‰ª•ËæìÂá∫ÔºåÂîØÁã¨HEADÔºåÂä†‰∫ÜËøôË°å‰ª£Á†ÅÂ∞±ÈîôËØØ„ÄÇ
             CoNLLSentence sentence = HanLP.parseDependency(""""+u);
            //out.println(sentence);
            // ÂèØ‰ª•Êñπ‰æøÂú∞ÈÅçÂéÜÂÆÉ
           for (CoNLLWord word : sentence)
            {
                out.println(word.LEMMA);
                out.println(word.CPOSTAG);
               // out.println(word.HEAD);
                out.println(word.DEPREL);  
            }
"
java servletË∞ÉÁî®‰æùËµñÂàÜÊûêÔºåÈô§‰∫ÜÂä†ËΩΩÈÖçÁΩÆÊñá‰ª∂„ÄÇËøòÈúÄË¶ÅÊòæÂºèËæìÂá∫‰∏Ä‰∏ãÊâçÂèØ‰ª•,"Âçö‰∏ªÔºå‰Ω†Â•Ω„ÄÇÊÑüË∞¢‰Ω†ÁöÑ‰ªòÂá∫„ÄÇÊàëÂú®Ë∞ÉÁî®‰æùËµñÂàÜÊûêÁöÑÊó∂ÂÄô„ÄÇÂèëÁé∞
                        Properties properties = new Properties();
                        //Á¨¨‰∏ÄË°å‰ª£Á†Å
            properties.load(getServletContext().getResourceAsStream(""/WEB-INF/hanlp.properties""));
                        //Á¨¨‰∫åË°å‰ª£Á†Å
            out.println(""root=""+properties.getProperty(""root""));
                         //Á¨¨‰∏âË°å‰ª£Á†Å
                  Â¶ÇÊûúÂè™ÊúâÁ¨¨‰∏ÄË°åÂíåÁ¨¨‰∫åË°å‰ª£Á†ÅÔºåË∞ÉËØï‰∏çÊàêÂäü„ÄÇÂ¶ÇÊûúÊâßË°å‰∏ÄÈÅçÁ¨¨‰∏âË°å‰ª£Á†ÅÔºåË∞ÉËØïÂ∞±ÈÄöËøá„ÄÇ
                   Â¶ÇÊûúË∞ÉËØïÈÄöËøáÔºåÊ≥®ÈáäÁ¨¨‰∏âË°å‰ª£Á†ÅÔºå‰æùÁÑ∂ÂèØ‰ª•ÈÄöËøá„ÄÇ‰∏çÁü•ÈÅìËøô‰∏™ÊòØÊÄé‰πà‰∏™ÂéüÁêÜ„ÄÇ
"
Â¶Ç‰ΩïËé∑ÂèñportableÂàÜÊîØËé∑ÂèñÁöÑ‰ª£Á†Å?,"@hankcs  ‰Ω†Â•Ω:‰ªäÂ§©ÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºåÂ∞±ÊòØÈÄöËøágit clone ÂëΩ‰ª§Ëé∑ÂèñÊ∫ê‰ª£Á†ÅÁöÑÊó∂ÂÄôÔºåmasterÂíåportableÂàÜÊîØËé∑ÂèñÁöÑ‰ª£Á†ÅÊòØ‰∏ÄÊ†∑ÁöÑÔºåÈÉΩÊòØhttps://github.com/hankcs/HanLP.gitËøôÂêå‰∏Ä‰∏™Âú∞ÂùÄÔºåËØ∑ÈóÆÂ¶Ç‰ΩïËé∑ÂèñportableÂàÜÊîØÁöÑ‰ª£Á†ÅÔºü
"
‰∏≠ÊñáÊï∞Â≠óÂàÜËØçÊïàÊûú‰∏ç‰Ω≥,"ÊØîÂ¶ÇÂÉèÂ¶Ç‰∏ãÂàÜËØç

> ‰∏ÄÁôæ‰∫åÂçÅ‰∫î‰πò‰ª•ÂçÅ‰∫åÁ≠â‰∫éÂá†
> ÁªìÊûúÊòØÔºö
> 
> [‰∏ÄÁôæ‰∫åÂçÅ‰∫î/m, ‰πò‰ª•/matml, ÂçÅ/m, ‰∫åÁ≠â/b, ‰∫é/p, Âá†/d]

Â¶ÇÊûúÈááÂèñÂ≠óÁ¨¶ËΩ¨Êç¢Ôºö‰∏ÄÔºö1Ôºõ‰∫åÔºö2Ôºõ„ÄÇ„ÄÇ„ÄÇ‰πùÔºö9ÔºõÂàô‰∏çÂà©‰∫éËΩ¨Êç¢ÊàêÈòøÊãâ‰ºØÊï∞Â≠óÔºàÂõ†‰∏∫ÂÉè‰∏Ä‰∏áÈõ∂Âá†ÁöÑÊÉÖÂÜµ‰∏çÂ•ΩÂ§ÑÁêÜÔºâ„ÄÇ
ÊâÄ‰ª•Ë¶ÅÊòØÂàÜËØçËÉΩÊää‰∏≠ÊñáÂàáÂá∫Êù•ÔºåÂÉèÔºö

> [‰∏ÄÁôæ‰∫åÂçÅ‰∫î/m, ‰πò‰ª•/matml, ÂçÅ‰∫å/mÁ≠â‰∫é/mateq, Âá†/d]
> Â∞±‰ºöÊØîËæÉÁêÜÊÉ≥„ÄÇ

Ë≤å‰ºº**Á≠â‰∫é**Âú®Ëøô‰∏™Âè•Â≠ê‰∏≠Êó†Ê≥ïÂæàÂ•ΩË¢´Ëá™ÂÆö‰πâËØçÊÄßÊ†áÂáÜÂàáÂá∫Êù•, ËÄå‚Äú‰∫åÁ≠â‚ÄùÂ¶ÇÊûúÂú®‰∏ÄËµ∑ÂæàÂÆπÊòìË¢´Ê†áÂáÜ‰∏∫'b'ËØçÊÄßÔºå‰ΩÜÊòØÁ≠â‰∫éÁöÑËá™ÂÆö‰πâËØçÊÄßÁöÑËØçÈ¢ëÂ∑≤ÁªèÊòØËÆæÁΩÆÂæàÈ´ò‰∫ÜÔºåËÄå‰∏îÂ¶ÇÊûúÊää**Á≠â‰∫é**ÊéíÂà∞Á¨¨‰∏Ä‰∏™ÊàñËÄÖÂú®ÂÆÉÁöÑÂâçÈù¢Âä†‰∏Ä‰∏™‚ÄùÁöÑÁªìÊûú‚ÄúÂèàÂèØ‰ª•Ë¢´Ê†áÂáÜ‰∫Ü„ÄÇ

ËØ∑ÈóÆÔºåËøôÊ†∑ÁöÑÈóÆÈ¢òËØ•Â¶Ç‰ΩïËß£ÂÜ≥Âë¢Ôºü

Ë∞¢ÔºÅÔºÅ
R
"
‰ΩøÁî®ËØ≠Âè•‰æùËµñÈ°πÂàÜÊûêÊï∞ÁªÑË∂äÁïå,"ÂØπ‰∫éÊñáÁ´†ÈïøÂ∫¶ÊØîËæÉÁü≠ÁöÑÔºå‰ΩøÁî®Ëá™Â∏¶ÁöÑ‰æùÂ≠òÂÖ≥Á≥ªÂ§ÑÁêÜÊ≤°Êúâ‰ªª‰ΩïÈóÆÈ¢òÔºå‰ΩÜÊòØÂØπ‰∫éÈïøÊñáÁ´†Âç¥ÊèêÁ§∫Êï∞ÁªÑË∂äÁïåÈóÆÈ¢òÔºåËØ∑ÈóÆ‰∏Ä‰∏ãÂ∫îËØ•ÊÄéÊ†∑‰Ωø‰æùÂ≠òÂÖ≥Á≥ªÂ§ÑÁêÜÈïøÁöÑÊñáÁ´†ÔºüÁé∞Âú®ÊÉ≥Âà∞ÁöÑ‰∏Ä‰∏™ÊñπÊ≥ïÊòØÂ∞ÜÈïøÊñáÁ´†ÂàÜÂá†ÊÆµËøõË°åÂ§ÑÁêÜÔºåËØ∑ÈóÆ‰∏Ä‰∏ãÊÇ®ÊúâÊ≤°ÊúâÂ•ΩÁöÑÊñπÊ≥ïËß£ÂÜ≥ÂØπ‰∫éÈïøÊñáÊú¨ÁöÑ‰æùÂ≠òÂÖ≥Á≥ªÂ§ÑÁêÜÔºüË∞¢Ë∞¢~
"
Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏,"CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; ÊàëÁöÑËØçÂÖ∏.txt;
ÈÄöËøáËøôÁßçÊñπÂºèËøΩÂä†‰∫ÜËá™Â∑±ÂàõÂª∫ÁöÑËØçÂÖ∏Ôºå‰ΩÜÊòØ‰ΩøÁî®ÁöÑÊó∂ÂÄôÂπ∂Ê≤°ÊúâË∞ÉÁî®
"
Â¶ÇÊûúÂêØÁî®CustomDictionaryÔºå‰∫∫Áß∞‰ª£ËØçË¢´Ê†áÊ≥®‰∏∫zËÄå‰∏çÊòØrr‰∫Ü,"ËØ∑Áúã‰∏Ä‰∏ãËøô‰∏™ÈóÆÈ¢òËØ•Â¶Ç‰ΩïËß£ÂÜ≥„ÄÇ

Ë∞¢Ë∞¢ÔΩû
"
CRFSegmentÂàÜËØçÊää‰∫∫ÂêçÊ†áÊ≥®‰∏∫nzÔºåËÄå‰∏çÊòØnr,"ÊãâÂèñ‰∫ÜÊúÄÊñ∞‰ª£Á†ÅÔºåÂèëÁé∞ÈááÁî®CRFSegmentÂàÜËØçÔºåÂ∞±Êää‰∫∫ÂêçÊ†áÊ≥®Êàê‰∏∫nz‰∫Ü„ÄÇ

Â¶ÇÊûúÁî®Áº∫ÁúÅÁöÑÂàÜËØçÊñπÊ≥ïËÉΩÊ≠£Á°ÆÊ†áÊ≥®‰∫∫Âêç‰∏∫nr„ÄÇ

‰ΩÜÈóÆÈ¢òÊòØÁº∫ÁúÅÁöÑÂàÜËØçÊñπÊ≥ïËøòÊòØÊó†Ê≥ïÊääËá™ÂÆö‰πâÁöÑËØçÁªôÂàÜÂá∫Êù•„ÄÇ„ÄÇ„ÄÇ
"
update .gitignore to add .settings folder,"Since Eclipse projects create .setttings folder automatically, it'd be best to add this folder into the .gitignore file
"
Ê±ÇÂä©ÔºöViterbiSegment‰∏™Âà´Âè•Â≠êÊú∫ÊûÑËØÜÂà´ÂºÇÂ∏∏ÔºåÊä•NoSuchElementException,"```
val txt = ""ËÄåÂÖ∂‰ªñËÇ¢Ëß£Âá∫ÂéªÁöÑ‰∏É‰∏™Ë¥ùÂ∞îÂÖ¨Âè∏Â¶ÇË•øÂçóË¥ùÂ∞î„ÄÅÂ§™Âπ≥Ê¥ãË¥ùÂ∞î„ÄÅÂ§ßË•øÊ¥ãË¥ùÂ∞î„ÄÅÂçóÊñπË¥ùÂ∞îÁ≠âÂè™ËÉΩÁªèËê•Áü≠ÈÄîÁîµËØù‰∏öÂä°„ÄÇ""
val seg_viterbi = new ViterbiSegment().enablePartOfSpeechTagging(true).enableOffset(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true).enableNumberQuantifierRecognize(true)
println(seg_viterbi.seg(txt))
```

```
Exception in thread ""main"" java.util.NoSuchElementException
    at java.util.LinkedList.getFirst(LinkedList.java:244)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:127)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:90)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:401)
    at cn.com.cetc.lab.test.TestMulti$.main(TestMulti.scala:17)
    at cn.com.cetc.lab.test.TestMulti.main(TestMulti.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
```

Áúã‰∫Ü‰ª£Á†ÅÔºåÊ≤°Â§™ÁúãÊòéÁôΩÔºå‰∏çÁü•ÈÅìÂì™ÂÑøÁöÑÈóÆÈ¢ò„ÄÇ‰∏ÄËà¨Âè•Â≠êÊ≤°ÊúâÈóÆÈ¢òÔºå‰∏™Âà´Âè•Â≠ê‰ºöËøôÊ†∑„ÄÇ
"
Ê±ÇÂä©ÔºöÊó∂Èó¥Áõ∏ÂÖ≥ÂàÜËØçÈóÆÈ¢ò,"**ÂéüÂè•Ôºö‚ÄúÊàëÊÉ≥Âú®ÊòéÂ§©‰∏äÂçà9ÁÇπÂê¨‰∏Ä‰∏™ÂÑøÁ´•ÊïÖ‰∫ã‚Äù**
- Â¶ÇÊûúÁî®ÈªòËÆ§ÁöÑÂàÜËØçÊñπÊ≥ïÔºöHanLP.newSegment() Ëé∑ÂæóÁªìÊûú‰∏∫Ôºö
  
  > [Êàë/z, ÊÉ≥/s, Âú®/dg, Êòé/ad, Â§©‰∏ä/i, Âçà/l, 9/m, ÁÇπ/q, Âê¨/s, ‰∏Ä‰∏™/Rg, ÂÑøÁ´•ÊïÖ‰∫ã/ngsch]
- Â¶ÇÊûúÁî®CRFSegmentÁªìÊûúÂèò‰∏∫Ôºö
  
  > [Êàë/z, ÊÉ≥/s, Âú®/dg, ÊòéÂ§©/i, ‰∏äÂçà/tam, 9/nz, ÁÇπ/q, Âê¨/s, ‰∏Ä‰∏™/Rg, ÂÑøÁ´•ÊïÖ‰∫ã/ngsch]

**Â¶ÇÊûúÂéüÂè•Êîπ‰∏∫Ôºö‚ÄúÊàëÊÉ≥Âú®ÊòéÂ§©‰∏äÂçà‰πùÁÇπÂê¨‰∏Ä‰∏™ÂÑøÁ´•ÊïÖ‰∫ã‚Äù**
- Â¶ÇÊûúÁî®ÈªòËÆ§ÁöÑÂàÜËØçÊñπÊ≥ï:
  [Êàë/z, ÊÉ≥/s, Âú®/dg, Êòé/ad, Â§©‰∏ä/i, Âçà/l, ‰πù/m, ÁÇπ/q, Âê¨/s, ‰∏Ä‰∏™/Rg, ÂÑøÁ´•ÊïÖ‰∫ã/ngsch]
- Â¶ÇÊûúÁî®CRFSegmentÔºö
  
  > [Êàë/z, ÊÉ≥/s, Âú®/dg, ÊòéÂ§©/i, ‰∏äÂçà‰πù/nz, ÁÇπ/q, Âê¨/s, ‰∏Ä‰∏™/Rg, ÂÑøÁ´•ÊïÖ‰∫ã/ngsch]

Âè¶Â§ñÔºåÊàëÂú®ÂàÜËØç‰∏≠ÈááÁî®‰∫Ü‰∏™ÊÄßÂåñÁöÑËØçÊÄßÂ¶Ç‰∏ãÔºö

```
  CustomDictionary.add(""ÊïÖ‰∫ã"",    ""ngs 1024"");
  CustomDictionary.add(""ÂÑøÁ´•ÊïÖ‰∫ã"", ""ngsch 2048"");
  CustomDictionary.add(""Êò®Â§©"",    ""tzt 1024"");
  CustomDictionary.add(""‰ªäÂ§©"",    ""tjt 1024"");
  CustomDictionary.add(""ÊòéÂ§©"",    ""tmt 1024"");
  CustomDictionary.add(""ÂêéÂ§©"",    ""tht 1024"");
  CustomDictionary.add(""‰∏äÂçà"",    ""tam 1024"");
  CustomDictionary.add(""‰∏ãÂçà"",    ""tpm 1024"");
```

ËØ∑ÈóÆÔºå**Â¶Ç‰ΩïËÉΩÊ≠£Á°ÆÂàÜÂá∫Êó∂Èó¥Âë¢Ôºü**
ÊØîÂ¶ÇËÉΩÂàÜÂá∫Ôºö [ÊòéÂ§© ‰∏äÂçà 9 ÁÇπ] 

‰∏áÂàÜÊÑüË∞¢ÔΩû
"
data-for-1.2.10.zip ÈçµÁµêÂ§±Êïà,"https://github.com/hankcs/HanLP/releases
Êñ∞ÁâàÊï∞ÊçÆÂåÖÔºödata-for-1.2.10.zip

ÈÄ£ÁµêÂ•ΩÂÉèÂ∑≤Á∂ì‰∏çÂú®Á∂≤Áõ§
"
CRFSegmentÁöÑÂ≠óÁ¨¶Ê≠£ËßÑÂåñË°®Âä†ËΩΩÂ§±Ë¥•,"‰∏ãÈù¢ËøôË°å‰ª£Á†Å
`ObjectInputStream in = new ObjectInputStream(new FileInputStream(HanLP.Config.CharTablePath));
`
Áõ¥Êé•ÂØºËá¥‰∫Ü‰∏ãÈù¢ÈóÆÈ¢òÔºö

```
Aug 03, 2016 2:12:18 PM com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable <clinit>
SEVERE: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®Âä†ËΩΩÂ§±Ë¥•ÔºåÂéüÂõ†Â¶Ç‰∏ãÔºö
java.io.StreamCorruptedException: invalid stream header: 003D200A
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:806)
    at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:370)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:47)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at com.hankcs.demo.DemoCRFSegment.main(DemoCRFSegment.java:62)
```

ËØ∑Â∏ÆÂøôÊü•Áúã‰∏Ä‰∏ã„ÄÇ
"
HanLP.java ÂØπCustomDictionaryPath ÈáåÈù¢ÁöÑË∑ØÂæÑËÆæÁΩÆÈîôËØØ„ÄÇ,"HanLP/src/main/java/com/hankcs/hanlp/HanLP.java

ËØ∑Êåâ‰ª•‰∏ã‰øÆÊîπÔºåÂê¶ÂàôÊàêÂäüÊó†Ê≥ïÂä†ËΩΩ„ÄÇ

```
-                String[] pathArray = p.getProperty(""CustomDictionaryPath"", ""dictionary/custom/CustomDictionary.txt"").split("";"");
+                String[] pathArray = p.getProperty(""CustomDictionaryPath"", ""data/dictionary/custom/CustomDictionary.txt"").split("";"");
```
"
CRFmodelËÆ°ÁÆóÁªìÊûú‰∏écrf++ÁöÑcrf_testÁªìÊûú‰∏ç‰∏ÄËá¥„ÄÇ,"CRFmodelÂä†ËΩΩ‰∫Ücrf++ 0.58ÁâàÊú¨ËÆ≠ÁªÉÂá∫Êù•ÁöÑÊ®°ÂûãÂêéÔºåtagÂêéÁöÑÁªìÊûú‰∏éÈááÁî®ËØ•crf++Ëá™Â∏¶ÁöÑcrf_testËÆ°ÁÆóÂá∫Êù•ÁöÑÁªìÊûú‰∏ç‰∏ÄÊ†∑„ÄÇËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢ÔºåÁÆóÊ≥ïÊúâ‰ªÄ‰πà‰∏ç‰∏ÄÊ†∑ÁöÑÂú∞ÊñπÂêóÔºü‰ªéÁªìÊûúÊù•Áúãcrf_testÁöÑÁªìÊûúË¶ÅÂáÜÁ°ÆÂæàÂ§öÔºåCRFmodelËÆ°ÁÆóÂá∫Êù•ÁöÑÁªìÊûúÂæàÂ§öÊòØÈîôËØØÁöÑ„ÄÇ
`CRFModel model = CRFModel.loadTxt(""F:\\studio\\CRF++-0.58\\CRF++-0.58\\example\\model.txt"",
                new CRFModel(new DoubleArrayTrie<FeatureFunction>()));
        System.out.println(model);
        Table table = new Table();
        table.v = new String[][]{
            {""ÂæÆÈ≤∏"", ""ns"", null},
            {""ÂàõÊñ∞"", ""vn"", null},
            {""Ëê•ÈîÄ"",  ""n"", null},
            {""Êä¢Âç†"",  ""v"", null},
            {""ÁîµËßÜ"",  ""n"", null},
            {""ÂÖàÊú∫"",  ""n"", null}
        };
        model.tag(table);
        System.out.println(table);`

[crf_train.txt](https://github.com/hankcs/HanLP/files/392253/crf_train.txt)
[crf_test.txt](https://github.com/hankcs/HanLP/files/392252/crf_test.txt)
"
ÈåØË™§ÁπÅÁ∞°ËΩâÊèõÂ≠óË©û,"Ë´ãÊõ¥Êñ∞Áõ∏ÈóúÂ≠óË©ûË°®ÔºåË¨ùË¨ù

ÂñÆÂ≠óËΩâÊèõÔºö
Áî¢/‰∫ß (Ê≤íÊúâËΩâÊèõ)
Ê∫ù/Ê≤ü (ÈåØË™§ËΩâÊèõÁÇ∫Èí©)
ÁÇ∫/‰∏∫ (Ê≤íÊúâËΩâÊèõ)
ÂÉ±/Èõá (Ê≤íÊúâËΩâÊèõ)

‰ª•‰∏ãÂè™ËÉΩ‰ΩøÁî®Ë©ûË™ûË°®Ôºå‰∏çËÉΩÂñÆÂ≠óËΩâÊèõÔºö
Â§•‰º¥ -> ‰ºô‰º¥
Áî±Êñº -> Áî±‰∫é
‰ΩçÊñº -> ‰Ωç‰∫é
"
ËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºåÂ¶Ç‰Ωï‰ΩøÁî®Á¥¢ÂºïÂàÜËØçÊ®°ÂºèÂπ∂Âà†Èô§ÂÅúÁî®ËØçÔºü,
‰ΩøÁî®java servlet ÂÆûÁé∞ÊúâÁÇπÈóÆÈ¢ò„ÄÇ,"Âçö‰∏ªÔºå‰Ω†Â•ΩÔºåËæõËã¶‰∫Ü„ÄÇÊàëÁî®java servlet Ë∞ÉÁî®ËØ≠Ë®ÄÂåÖÔºåËøêË°å‰∏çÊàêÂäüÔºåËøôÊòØÊµãËØï‰ª£Á†Å„ÄÇË∞ÉËØïËøêË°åÔºåÊèêÁ§∫‚ÄúÁΩëÁ´ôÊó†Ê≥ïÊòæÁ§∫ËØ•È°µÈù¢/  HTTP 500 /ÊúÄÂèØËÉΩÁöÑÂéüÂõ†ÊòØ:
‚Ä¢ËØ•ÁΩëÁ´ôÊ≠£Âú®ËøõË°åÁª¥Êä§„ÄÇ
‚Ä¢ËØ•ÁΩëÁ´ôÊúâÁ®ãÂ∫èÈîôËØØ„ÄÇ
‚Äù
 ËØ∑ÊïôÂì™ÈáåÂá∫ÈóÆÈ¢ò‰∫Ü„ÄÇ

ÊàëÁî®ÁöÑÁºñËØëÂô®ÊòØeclipse-jee-neon-R-win32Ôºå   demoÂú®ÊéßÂà∂Âè∞Ê®°Âºè‰∏ãÊ≤°ÊúâÈóÆÈ¢ò„ÄÇËøôÊòØÈÉ®ÂàÜ‰ª£Á†Å

import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import com.hankcs.hanlp.HanLP;

public class HelloWorld extends HttpServlet implements javax.servlet.Servlet{

```
    public HelloWorld() {
        super();
    }       
    protected void doGet(HttpServletRequest request, HttpServletResponse response) 
        throws ServletException, IOException {
        //response.getWriter().write(""Hello"");
        request.setCharacterEncoding(""utf-8"");
        response.setContentType(""text/html;charset=utf-8"");
        PrintWriter out = response.getWriter();
        String u=request.getParameter(""userinput"");
        out.println(""<br/>ËæìÂÖ•:""+u);
        out.println(""È¶ñÊ¨°ÁºñËØëËøêË°åÊó∂"");
        out.println(HanLP.segment(""‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®HanLPÊ±âËØ≠Â§ÑÁêÜÂåÖÔºÅÊé•‰∏ãÊù•ËØ∑‰ªéÂÖ∂‰ªñDemo‰∏≠‰ΩìÈ™åHanLP‰∏∞ÂØåÁöÑÂäüËÉΩ~""));
    ....................................
```
"
CRFModelÁöÑÊûÑÈÄ†ÂáΩÊï∞Êúâ‰∏™ÈóÆÈ¢òÔºåÁõ¥Êé•‰ΩøÁî®‰ºöÂ§±Ë¥•ÔºÅ,"‰Ω†Â•ΩÔºåÊÉ≥ÈÄöËøá‰Ω†ÁöÑCRFModelÁöÑloadBinÂä†ËΩΩ‰Ω†Ëá™Â∏¶ÁöÑbinÊ®°ÂûãÊàñ‰ΩøÁî®loadTxtÂä†ËΩΩËá™Â∑±ËÆ≠ÁªÉÁöÑcrf++Ê®°Âûã„ÄÇÂä†ËΩΩÊ≤°ÊúâÊä•ÈîôÔºå‰ΩÜÊòØtagÂ∑•‰ΩúÂç¥ÊèêÁ§∫Á©∫ÊåáÈíà„ÄÇ

Áúã‰∫Ü‰Ω†CRFSegmentModelÁ±ªÁöÑÂä†ËΩΩ‰ª£Á†ÅÔºåËøôÈáå‰Ω†Áî®ÁöÑÊòØ`new BinTrie<FeatureFunction>()`ÂàùÂßãÂåñCRFModelÔºåËÄåÂú®CRFModelÁöÑÊûÑÈÄ†ÂáΩÊï∞‰∏≠‰ΩøÁî®ÁöÑÊòØ`new DoubleArrayTrie<FeatureFunction>()`„ÄÇËøô‰∏™ÊòØ‰∏çÊòØÁâàÊú¨ÂçáÁ∫ßÔºå‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇ

‰øÆÊîπÁõ∏ÂÖ≥ÁöÑDoubleArrayTrie‰∏∫BinTrieÂêéÔºåtagÂáΩÊï∞Â∑•‰ΩúÊ≠£Â∏∏„ÄÇ
"
CRFÊñ∞ËØçËØÜÂà´,"Segment segment = new CRFSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(""‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó"");
System.out.println(termList);
for (Term term : termList)
{
    if (term.nature == null)
    {
        System.out.println(""ËØÜÂà´Âà∞Êñ∞ËØçÔºö"" + term.word);
    }
}

Áî®masterÂàÜÊîØ‰∏äÁöÑ‰ª£Á†ÅËøêË°åËøôÊÆµ‰ª£Á†ÅÔºåËØÜÂà´‰∏çÂá∫Êñ∞ËØç„ÄÇËøêË°åÁªìÊûúÂ¶Ç‰∏ãÔºö
[‰Ω†/rr, ÁúãËøá/v, Á©ÜËµ´ÂÖ∞ÈÅì/nz, Âêó/y]
"
ËøêË°åËá™ÂÆö‰πâËØçÊÄßDemoÔºåNoSuchFieldException:$VALUES,"ÊÇ®Â•ΩÔºå‰ΩøÁî®MacBook Pro 10.11ÔºåEclipse Mars.1 ÔºåJDK1.7ÔºåËøêË°å1.2.10ÁâàÊú¨ÁöÑDemoCustomNature.javaÂêéÂ¶Ç‰∏ãÊä•ÈîôÔºö
‰∏ÉÊúà 24, 2016 6:55:03 ‰∏ãÂçà com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
Ë≠¶Âëä: Â∑≤ÊøÄÊ¥ªËá™ÂÆö‰πâËØçÊÄßÂäüËÉΩ,Áî±‰∫éÈááÁî®‰∫ÜÂèçÂ∞ÑÊäÄÊúØ,Áî®Êà∑ÈúÄÂØπÊú¨Âú∞ÁéØÂ¢ÉÁöÑÂÖºÂÆπÊÄßÂíåÁ®≥ÂÆöÊÄßË¥üË¥£!
Â¶ÇÊûúÁî®Êà∑‰ª£Á†ÅX.java‰∏≠Êúâswitch(nature)ËØ≠Âè•,ÈúÄË¶ÅË∞ÉÁî®CustomNatureUtility.registerSwitchClass(X.class)Ê≥®ÂÜåXËøô‰∏™Á±ª
Exception in thread ""main"" java.lang.IllegalArgumentException: Could not create enum
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:99)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
    at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
    at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
    at com.hankcs.demo.DemoCustomNature.main(DemoCustomNature.java:41)
Caused by: java.lang.IllegalArgumentException: Could not create the class
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:453)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:439)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:91)
    ... 4 more
Caused by: java.lang.NoSuchFieldException: $VALUES
    at java.lang.Class.getDeclaredField(Class.java:1961)
    at com.hankcs.hanlp.corpus.util.EnumBuster.findValuesField(EnumBuster.java:349)
    at com.hankcs.hanlp.corpus.util.EnumBuster.values(EnumBuster.java:429)
    at com.hankcs.hanlp.corpus.util.EnumBuster.access$0(EnumBuster.java:426)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:443)
"
"ÁπÅ‰ΩìÂ≠óÂÖ∏Â¢ûÂä†""Êì∑Âèñ=Êà™Âèñ""","ËßÅwikipediaÊù°ÁõÆ""Êï∞ÊçÆÂ∫ì""
Á¨¨‰∏ÄÂè•""Áî®Êà∑ÂèØ‰ª•ÂØπÊñá‰ª∂‰∏≠ÁöÑÊï∞ÊçÆËøêË°åÊñ∞Â¢û„ÄÅÊà™Âèñ„ÄÅÊõ¥Êñ∞„ÄÅÂà†Èô§Á≠âÊìç‰Ωú""ÁöÑÁπÅÁÆÄËΩ¨Êç¢.
"
Ê≠ß‰πâÂàÜËØçËß£ÂÜ≥,"ÊàëÁ¨¨‰∏ÄÊ¨°ËØïËøô‰∏™hanlpÔºåËßâÂæóÂàÜËØçÂæàÂ•ΩÔºåËØï‰∫Ü‰∏Ä‰∫õÂÆπÊòìÂºïËµ∑Ê≠ß‰πâÂè•Â≠êÔºåÂ§ßÈÉ®ÂàÜÊ≤°ÈóÆÈ¢ò„ÄÇ‰ª•‰∏ã4Âè•ÊòØCRFSegÂá∫ÈîôÁöÑÔºö
""Âú®Ëøô‰∫õ‰ºÅ‰∏ö‰∏≠ÂõΩÊúâ‰ºÅ‰∏öÊúâÂçÅ‰∏™"",
""Êñ∞Âª∫Âú∞ÈìÅ‰∏≠Â∞ÜÁ¶ÅÊ≠¢ÂïÜ‰∏öÊëäÁÇπ"",
""ÊñπÁ®ãÁöÑËß£Èô§‰∫ÜÈõ∂‰ª•Â§ñËøòÊúâÂÖ∂ÂÆÉÁöÑ‚Ä¶"",
""ËøôÁöÑÁ°ÆÂÆö‰∏ç‰∏ãÊù•"",

CRFSegÁöÑÁªìÊûúÔºö
[Âú®, Ëøô‰∫õ, ‰ºÅ‰∏ö, ‰∏≠ÂõΩ, Êúâ, ‰ºÅ‰∏ö, Êúâ, ÂçÅ‰∏™]
[Êñ∞Âª∫, Âú∞ÈìÅ, ‰∏≠Â∞Ü, Á¶ÅÊ≠¢, ÂïÜ‰∏ö, ÊëäÁÇπ]
[ÊñπÁ®ã, ÁöÑ, Ëß£Èô§, ‰∫Ü, Èõ∂‰ª•Â§ñ, ËøòÊúâ, ÂÖ∂ÂÆÉ, ÁöÑ, ‚Ä¶]
[Ëøô, ÁöÑ, Á°ÆÂÆö, ‰∏ç, ‰∏ãÊù•]

segment(Ê†áÂáÜÂàÜËØç)ÁöÑÁªìÊûúÔºö
[Âú®/p, Ëøô‰∫õ/rz, ‰ºÅ‰∏ö/n, ‰∏≠/f, ÂõΩÊúâ‰ºÅ‰∏ö/nz, Êúâ/vyou, ÂçÅ/m, ‰∏™/q]
[Êñ∞Âª∫/v, Âú∞ÈìÅ/n, ‰∏≠/f, Â∞Ü/d, Á¶ÅÊ≠¢/v, ÂïÜ‰∏ö/n, ÊëäÁÇπ/n]
[ÊñπÁ®ã/n, ÁöÑ/ude1, Ëß£Èô§/v, ‰∫Ü/ule, Èõ∂/m, ‰ª•Â§ñ/f, ËøòÊúâ/v, ÂÖ∂ÂÆÉ/rz, ÁöÑ/ude1, ‚Ä¶/w]
[Ëøô/rzv, ÁöÑ/ude1, Á°ÆÂÆö/v, ‰∏ç/d, ‰∏ãÊù•/vf]

Ê†áÂáÜÂàÜËØç‰∏≠Ââç‰∏§‰∏™ÂØπ‰∫ÜÔºåÂêé‰∏§‰∏™‰æùÁÑ∂ÊòØÈîôÁöÑ„ÄÇÊàëÁöÑÈóÆÈ¢òÊòØÔºö
1„ÄÅsegment‰ΩøÁî®ÁöÑ‰ªÄ‰πàÊñπÊ≥ïÔºü
2„ÄÅÂèØ‰ª•ÈÄöËøá‰ªÄ‰πàÂäûÊ≥ïËøõ‰∏ÄÊ≠•ÊîπËøõÂàÜËØçÁöÑÂáÜÁ°ÆÁéáÂêóÔºüÔºà‰Ω†ÁöÑÂàÜËØçÂáÜÁ°ÆÁéáÂ∑≤ÁªèÂæàÂ•Ω‰∫ÜÔºâ

Ë∞¢Ë∞¢ÔºÅ

Henry
"
ÈªòËÆ§ÁöÑÁπÅ‰ΩìËΩ¨Êç¢Â•ΩÂ•áÊÄ™,"ËàûÂè∞ÔºöËàûÈ¢±
Âõ†‰∏∫ÔºöÂõôÁà≤
ÈùíÊ¢ÖÁ´πÈ©¨ÔºöÈùíÊ•≥Á´πÈ¶¨
"
ÊåâÁ¨¨‰∫åÁßçÊñπÊ≥ïÈÖçÁΩÆÔºåË∞ÉÁî®‰æùËµñËß£ÊûêDemoÂèëÁîüÈîôËØØ„ÄÇ,"Âçö‰∏ªÔºå
‰Ω†Â•ΩÔºåÊÑüË∞¢‰Ω†ÁöÑ‰ªòÂá∫„ÄÇÊàëÂú®ÊµãËØï‰æùÂ≠òÂÖ≥Á≥ªdemoÁöÑÊó∂ÂÄôÂèëÁîü‰∫ÜËøô‰∏™ÈîôËØØÔºåËØ∑ÊïôÂì™ÈáåÊ≤°ÊúâËÆæÁΩÆÂØπÔºüÊàëÊòØÊåâÁ¨¨‰∫åÁßçÊñπÊ≥ïÈÖçÁΩÆÁöÑ„ÄÇÂç≥‰ΩøÁî®‰∫Ühanlp.properties
‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑ÊòØ
Eclipse IDE for Java Developers
Version: Mars.2 Release (4.5.2)
demo ÊòØ DemoDependencyParser

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.dependency.nnparser.Matrix.load(Matrix.java:1305)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:259)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:135)
    at com.hankcs.hanlp.dependency.nnparser.parser_dll.<clinit>(parser_dll.java:34)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.parse(NeuralNetworkDependencyParser.java:53)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.compute(NeuralNetworkDependencyParser.java:93)
    at com.hankcs.hanlp.HanLP.parseDependency(HanLP.java:407)
    at hanlp.DemoDependencyParser.main(DemoDependencyParser.java:26)
"
java.lang.ExceptionInInitializerError,"hankcsÂ•ΩÔºåÊàëÁî®PythonË∞ÉÁî®hanlpÂÅö‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÊó∂ÈÅáÂà∞Â¶Ç‰∏ãÈóÆÈ¢òÔºåËØ∑ÈóÆËØ•ÊÄé‰πàËß£ÂÜ≥ÔºüpsÔºö‰ΩøÁî®ÂàÜËØçÁ≠âÂäüËÉΩÊó∂Ê≠£Â∏∏ÔºåÂ§öË∞¢ÔºÅÔºÅÔºÅ
![sft cu_6w3pp c0 9aa11](https://cloud.githubusercontent.com/assets/19303343/16905158/5a4487a8-4cd4-11e6-97e0-8cf378bc35d9.png)
"
java.lang.ExceptionInInitializerError,"ËØ∑ÈóÆhankcsÔºåË∞ÉÁî®‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºåÂá∫Áé∞ÂàùÂßãÂåñÈîôËØØËØ•Â¶Ç‰ΩïËß£ÂÜ≥ÔºåÂ§öË∞¢Â§öË∞¢ÔºÅpsÔºöÂàÜËØçÁ≠âÂäüËÉΩÊ≠£Â∏∏
![vcp wuh b8 5n h aopw9p](https://cloud.githubusercontent.com/assets/19303343/16901354/92c08520-4c73-11e6-8f71-0e8d36710819.png)
"
NLPTokenizerÂàÜËØçÊó∂Áî®Âà∞ÁöÑÊ†∏ÂøÉËØçÂÖ∏ËΩ¨ÁßªÁü©Èòµ,"ÊàëÁé∞Âú®Áî®NLPTokenizerÂàÜËØçÊñπÊ≥ïËøõË°åÂàÜËØçÔºåÂú®Nature‰∏≠Ê∑ªÂä†ËØçÊÄßtextÔºåÊääÊ†∏ÂøÉËØçÂÖ∏‰∏≠ÂêàËÇ•Â∏ÇÁöÑËØçÊÄßÊ†áÊ≥®‰∏∫textÔºå‰ΩÜÊòØ‰ºöÂá∫Áé∞‰∏ãÈù¢ÁöÑÈîôËØØÔºå
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
    at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:118)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:37)
    at com.hankcs.test.seg.Test.main(Test.java:11)

ÊòæÁ§∫Â∫îËØ•ÊòØÊ†∏ÂøÉËØçÂÖ∏ËΩ¨ÁßªÁü©ÈòµÁöÑÈóÆÈ¢òÔºåÊ†∏ÂøÉÁü©Èòµ‰∏≠‰∏çÂåÖÂê´Êñ∞ÁöÑËØçÊÄßÔºåÈÇ£ËøôÊ†∑ÁöÑËØùÔºåÊ†∏ÂøÉËØçÂÖ∏ËΩ¨ÁßªÁü©ÈòµÈúÄË¶ÅÈáçÊñ∞ÁîüÊàêÂêóÔºåË¶ÅÊÄé‰πàÁîüÊàêÂïä
"
Áî®Êà∑Ëá™Â∑±Ê∑ªÂä†Êñ∞ÁöÑËØçÊÄßÁöÑÈóÆÈ¢ò,"Áé∞Âú®ÂèëÁé∞1.2.10ÁªôÂá∫ÁöÑÁî®Êà∑Ëá™ÂÆö‰πâÊ∑ªÂä†ËØçÊÄßÁöÑdemo‰∏çÈÄÇÁî®‰∫éNLPTokenizer.segment(text)ÂàÜËØçÔºåËøòÊ≤°ÊâæÂá∫ÂéüÂõ†ÔºåÂ∏åÊúõËÉΩÂ∏ÆÂøôÁúã‰∏Ä‰∏ãÔºåË∞¢Ë∞¢ÔºÅ
"
Class com.hankcs.hanlp.HanLP not found,"hankcsÂ§ßÁ•ûÔºåË∞ÉÁî®hanlpÊó∂Á¢∞Âà∞Â¶Ç‰∏ãÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÔºüÂè¶Â§ñÔºåÂØπÈ°πÁõÆÊñáÊ°£ÈáåÁöÑËøôÂè•ËØù‰πü‰∏çËÉΩÁêÜËß£‚ÄúÊúÄÂêéÂ∞ÜHanLP.propertiesÊîæÂÖ•classpathÂç≥ÂèØÔºåÂØπ‰∫é‰ªª‰ΩïÈ°πÁõÆÔºåÈÉΩÂèØ‰ª•ÊîæÂà∞srcÊàñresourcesÁõÆÂΩï‰∏ãÔºåÁºñËØëÊó∂IDE‰ºöËá™Âä®Â∞ÜÂÖ∂Â§çÂà∂Âà∞classpath‰∏≠„ÄÇ‚Äò‚Äô,Ê±ÇËß£Á≠îÔºüÂ§öË∞¢ÔºÅÔºÅÔºÅ

![78_d1 v13tj1r_6 0 lqu9w](https://cloud.githubusercontent.com/assets/19303343/16736512/d3455124-47bf-11e6-94b4-b6d686e2c43c.png)
"
‰ΩøÁî®mavenÁöÑjarÂåÖÔºåÁ®ãÂ∫èÂü∑Ë°åÊôÇÊâæ‰∏çÂà∞Ë©ûÂÖ∏Ë∑ØÂæë„ÄÇ,"ExceptionÂ¶Ç‰∏ãÔºö

```
Jul 07, 2016 5:00:15 PM com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable <clinit>
SEVERE: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®Âä†ËΩΩÂ§±Ë¥•ÔºåÂéüÂõ†Â¶Ç‰∏ãÔºö
java.io.FileNotFoundException: data/dictionary/other/CharTable.bin.yes (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.<init>(FileInputStream.java:138)
    at java.io.FileInputStream.<init>(FileInputStream.java:93)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:368)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:46)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at io.smackds.processors.NLP$$anonfun$receive$1.applyOrElse(NLP.scala:31)
    at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
    at io.smackds.processors.NLP.aroundReceive(NLP.scala:19)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
    at akka.actor.ActorCell.invoke(ActorCell.scala:495)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
    at akka.dispatch.Mailbox.run(Mailbox.scala:224)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
```

‰ΩÜÂ¶ÇÊûúÁõ¥Êé•‰ΩøÁî®githubÁöÑjarÔºåÂâáÊ≤íÊúâÈåØË™§„ÄÇ
ÊàëÁöÑhanlp.propertiesË£°ÊåáÂÆö‰∫Üroot=/Users/stephen/devel/hanlp-1.2.10Ôºå‰∏¶Âè¶Â§ñ‰∏ãËºâ‰∫ÜÂÆåÊï¥ÁöÑË©ûÂÖ∏Ê™î„ÄÇ
"
add traditional chinese synonym and trans dict,"Added some words to the dictionary for Traditional Chinese and synonym
"
Âú∞ÂùÄÈõÜ‰ΩìËØÜÂà´,"ÊÇ®Â•ΩÔºåÊàëÊòØÁ¨¨‰∏ÄÊ¨°‰ΩøÁî®hanlpÔºåÊàëÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÂØπ‰∫é‚ÄúÂåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Â≠¶Èô¢Ë∑Ø31Âè∑Âåó‰∫¨Ëà™Á©∫Ëà™Â§©Â§ßÂ≠¶‰ΩìËÇ≤È¶Ü‚ÄùËøô‰∏™Êï¥‰ΩìÂú∞ÂùÄÊúâ‰ªÄ‰πàÊñπÊ≥ïÂèØ‰ª•ÈõÜ‰Ωì‰∏ÄÊ¨°ÊÄßËØÜÂà´Âá∫Êù•ÂêóÔºüËÄå‰∏çÊòØÂåó‰∫¨Â∏Ç/nsÊµ∑Ê∑ÄÂå∫/nsÂ≠¶Èô¢Ë∑Ø/nz31/mÂè∑/qÂåó‰∫¨Ëà™Á©∫Ëà™Â§©Â§ßÂ≠¶/nt‰ΩìËÇ≤È¶Ü/nËøô‰∏≠ÊÄé‰πàÊää‰ªñ‰ª¨Âèò‰∏∫Âåó‰∫¨Â∏ÇÊµ∑Ê∑ÄÂå∫Â≠¶Èô¢Ë∑Ø31Âè∑Âåó‰∫¨Ëà™Á©∫Ëà™Â§©Â§ßÂ≠¶‰ΩìËÇ≤È¶Ü/ns„ÄÇÈ∫ªÁÉ¶ÊÇ®‰∫Ü
Ë∞¢Ë∞¢ÊÇ®„ÄÇ
"
HanLPËá™ÂÆö‰πâÂ≠óÂÖ∏‰∏çÁîüÊïà,"ÊÇ®Â•ΩÔºåÊàëÊåâÁÖßÂÆòÁΩëÁöÑÊñπÊ≥ïÈÖçÁΩÆ‰∫ÜpropertiesÔºåËÄå‰∏î‰πüËøõË°å‰∫ÜÂØπbinÁöÑÂà†Èô§Ôºå‰ΩÜÊòØÂ∞±ÊòØ‰∏çËÉΩËøõË°åÂØπÊàëÁöÑÂ≠óÂÖ∏.txtÈáåÁöÑÊ∑ªÂä†ÁöÑËØçËøõË°åËØÜÂà´ÔºåÂ≠óÂÖ∏ÈáåÂÆö‰πâ‰∫Ü‰∫∫ÊâçÊä•ÂëäÂéÖÔºåÈ∫ªÁÉ¶ÊÇ®Â∏ÆÊàëÁúã‰∏Ä‰∏ã„ÄÇ
![qq 20160705130443](https://cloud.githubusercontent.com/assets/20194150/16574453/a1e3515c-42b1-11e6-91f3-750f619e34d2.png)
![qq 20160705130632](https://cloud.githubusercontent.com/assets/20194150/16574452/a1d56236-42b1-11e6-8783-48e99c4c4cd9.png)
![qq 20160705130748](https://cloud.githubusercontent.com/assets/20194150/16574451/a1d3eeba-42b1-11e6-9cd1-5384baadecda.png)
"
CharTable.convert Â∞áÈÄóÂè•ËÆäÊàêÂè•Ëôü,"Â¶ÇÈ°åÔºåÊáâÂ¶Ç‰Ωï‰øÆÊ≠£Ôºü

ÊàëÁïôÊÑèÂà∞CharTable.java Êúâ‰∫õ‰øÆÊ≠£ÁöÑÂÇôË®ªÔºå‰∏çÁü•ËÉΩÂê¶‰ΩøÁî®

```
    CharTable.CONVERT['.'] = '.';
    CharTable.CONVERT['Ôºé'] = '.';
    CharTable.CONVERT['„ÄÇ'] = 'Ôºå';
    CharTable.CONVERT['ÔºÅ'] = 'Ôºå';
    CharTable.CONVERT['Ôºå'] = 'Ôºå';
    CharTable.CONVERT['‚Ä¶'] = 'Ôºå';
```

```
print txt
> ÊñπË™™ËëõÂèÉÈÅ∏Á´ãÊúÉÊôÇÊõæË®±Ë´æÂèçÂ∞çÊì¥Âª∫Â†ÜÂ°´ÂçÄÔºåÊäïÁ•®ÂâçÂçªÂèçÂè£ÔºåÂá∫Ë≥£Â∞áËªçÊæ≥Â±ÖÊ∞ëÔºåÊïÖÊúõÊîπËÆäËëõÊÉ≥Ê≥ïÔºå‰ΩÜÂê¶Ë™çÊõæÊçâÁùÄËëõÔºå‰πüÁÑ°Áî®Ê©´È°çÂúçÈÅéÂ•πÔºåÂèçÊòØÈô™ÂêåËëõÂà∞Â†¥ÁöÑÊ∞ëÂª∫ËÅØÂçÄË≠∞Âì°ÊùéÂÆ∂ËâØÁ™Å‰º∏ÊâãÊãâÊ©´È°çÔºå‰ª§ËëõÂíåÂ•π‰πüË∑åÂÄíÔºõËëõÂèàËø¥ÈÅøÂïèÈ°åËµ∞‰æÜËµ∞ÂéªÔºåÊñπË¶ãËëõË°åÈåØË∑Ø‰∏ÄÂ∫¶Âä©Â•πÈõ¢Èñã„ÄÇ


    return CharTable.convert(txt.replace(u'Ôºå',u'Ôºå'))

> ÊñπËØ¥ËëõÂèÇÈÄâÁ´ã‰ºöÊó∂ÊõæËÆ∏ËØ∫ÂèçÂØπÊâ©Âª∫Â†ÜÂ°´Âå∫„ÄÇÊäïÁ•®ÂâçÂç¥ÂèçÂè£„ÄÇÂá∫ÂçñÂ∞ÜÂÜõÊæ≥Â±ÖÊ∞ë„ÄÇÊïÖÊúõÊîπÂèòËëõÊÉ≥Ê≥ï„ÄÇ‰ΩÜÂê¶ËÆ§ÊõæÊçâÁùÄËëõ„ÄÇ‰πüÊó†Áî®Ê®™È¢ùÂõ¥ËøáÂ•π„ÄÇÂèçÊòØÈô™ÂêåËëõÂà∞Âú∫ÁöÑÊ∞ëÂª∫ËÅîÂå∫ËÆÆÂëòÊùéÂÆ∂ËâØÁ™Å‰º∏ÊâãÊãâÊ®™È¢ù„ÄÇ‰ª§ËëõÂíåÂ•π‰πüË∑åÂÄí„ÄÇËëõÂèàËø¥ÈÅøÈóÆÈ¢òËµ∞Êù•Ëµ∞Âéª„ÄÇÊñπËßÅËëõË°åÈîôË∑Ø‰∏ÄÂ∫¶Âä©Â•πÁ¶ªÂºÄ„ÄÇ
```
"
CoNLLWord ÁöÑCPOSTAG ÂèäPOSTAG ÂïèÈ°å,"Ê†πÊìö

```
https://github.com/hankcs/HanLP/blob/9456aebab678ef0ecfcd0a367d29a04e878c112a/src/main/java/com/hankcs/hanlp/corpus/dependency/CoNll/CoNLLWord.java

sb.append(ID).append('\t').append(LEMMA).append('\t').append(LEMMA).append('\t').append(CPOSTAG).append('\t')
                .append(POSTAG).append('\t')
```

DemoDependencyParser Á¨¨4 ÂíåÁ¨¨5 ÂÄãËº∏Âá∫ÂàÜÂà•ÊòØ CPOSTAG ÂèäPOSTAG„ÄÇ

ÊàëÂü∑Ë°åDemoDependencyParser ÊôÇÔºåÂçªÁôºÁèæÂæàÂ§ö‰∏çÊòéÁôΩÁöÑPOSTAG , e.g. **_nnt**_, **_nis**_

```
java -classpath ""."" com/hankcs/demo/DemoDependencyParser ""Ëá≥ÊñºÂâØ‰∏ªÂ∏≠ÁöÑÂΩ¢ÂäøÊõ¥‰∏∫Ê∑∑‰π±ÔºåÊçÆÁü•Âª∫Âà∂Ê¥æÂÜÖÈÉ®Â±ûÊÑèÁöÑÂÖ¨Ê∞ëÂäõÈáèÊ∏©ÊÇ¶ÊòåËá≥‰ªä‰ªçÊú™Êä•ÂêçÔºåÂºïÊù•Ê∞ëÂª∫ËÅîÂáåÊñáÊµ∑„ÄÅÂùëÂè£‰π°‰∫ãÂßîÂëò‰ºö‰∏ªÂ∏≠ÊàêÊ±âÂº∫„ÄÅÁã¨Á´ãÂë®Ë¥§ÊòéÁ≠âÂ§öÊñπÊúâÊÑèÊüìÊåáÔºåÈöèÊó∂‰ºöÊúâÊÑèÂ§ñÊàòÊûúÂá∫Áé∞„ÄÇ""

1       Ëá≥Êñº    Ëá≥Êñº    i       i       _       2       ÂÆö‰∏≠ÂÖ≥Á≥ª        _       _
2       ÂâØ‰∏ªÂ∏≠  ÂâØ‰∏ªÂ∏≠  nz      nnt     _       4       ÂÆö‰∏≠ÂÖ≥Á≥ª        _       _

...

27      ÂßîÂëò‰ºö  ÂßîÂëò‰ºö  nt      nis     _       28      ÂÆö‰∏≠ÂÖ≥Á≥ª        _       _

```

Â¶Ç‰ΩïÁêÜËß£POSTAG ÂíåCPOSTAG?
"
Ëá™Ë°åÊ∑ªÂä†customËØçÂÖ∏ÈóÆÈ¢ò,"ÁõÆÂâçÂ∫îÁî®ÊòØÂú®‰∏Ä‰∏™ÂÖ∑‰ΩìÈ¢ÜÂüüÔºåÂ∏åÊúõÂàÜËØçÁªìÊûú‰ª•Âá∫Áé∞Âú®ÊàëËá™Ë°åÊ∑ªÂä†ÁöÑËØçÂÖ∏‰∏≠ÁöÑÂêçËØç‰∏∫ÊúÄÈ´ò‰ºòÂÖàÁ∫ßÔºåËØçÂÖ∏Ê∑ªÂä†ÊñπÂºèÂèÇÁÖß‰∫ÜÊåáÂºï„ÄÅÂú®propertyÊñá‰ª∂‰∏≠Âä†‰∫ÜÈÖçÁΩÆÔºåÂπ∂Âà†Èô§‰∫ÜbinÊñá‰ª∂ËÆ©ÂÆÉÈáçÊñ∞ÁîüÊàê„ÄÇÁõÆÂâçÁúãÊïàÊûúÊúâÁöÑËØçÁîüÊïàÊúâÁöÑÊ≤°ÊúâÔºåËøòÊòØ‰ºöË¢´ÂàáÂºÄ„ÄÇ‰ΩøÁî®indextokenizerÔºå‰∏çÁü•ÈÅìÊòØÂê¶Âì™ÈáåÊ≤°ÂºÑÂØπÔºü‰æãÂ≠êÔºö‚ÄúÊ≠£‰∏≠Á•ûÁªè‚ÄùÔºåÂàá‰∏∫Ê≠£‰∏≠ÂíåÁ•ûÁªèÔºåË∞¢Ë∞¢ÔºÅ
"
ÁπÅÁ∞°Â≠óÂÖ∏ - Âü∫Êñº=Âü∫‰∫é,"Âä†ÂÖ•Âü∫Êñº=Âü∫‰∫é
"
ÊñπË®ÄÁöÑÂ≠óÂÖ∏ËôïÁêÜÊåëÊà∞,"‰Ω†Â•ΩÔºå

ÊàëÂú®Âª∫Á´ãÊñπË®Ä(È¶ôÊ∏Ø)Â≠óÂ∫´/ÂêåÁæ©Ë©û

ÊàëÁôºÁèæÊñπË®ÄÁöÑËôïÁêÜÊúâ‰∫õÊôÇÂÄô‰∏çÂÆπÊòìËôïÁêÜÔºå‰æãÂ¶ÇÂêåÁæ©Ë©û‰æøÂèØÁî®‰∏âÂÄã‰∏çÂêåÁöÑÂ≠óÂÖ∏ËôïÁêÜÔºö
1. ÁπÅÈ´îÂ≠óÂÖ∏
‰ªÄÈ∫º=ÁîöÈ∫º
1. ÂêåÁæ©Ë©û
   ‰∏çÂ•Ω=ÂîîÂ•Ω
2. ÊñπË®ÄCustomDictionary
   ÊúâÁÑ°Êî™ÈåØ 

‰æãÂ≠ê3 ÊòØÊØîËºÉÂ•ΩËôïÁêÜÁöÑÔºåÂõ†ÁÇ∫‰∏çÂΩ±ÈüøÂÖ∂‰ªñÂ≠óË©ûÁöÑÂàÜÊûê„ÄÇ

‰æãÂ≠ê1, 2 ÊúÉÂΩ±ÈüøÁèæÂ≠òÂ≠óÂ∫´ÁöÑÂàÜÊûêÁµêÊûú„ÄÇ

ÊàëÊÉ≥ÈÄôÂïèÈ°åÂíå‰∏ÄËà¨ domain ÂïèÈ°åÁöÑËôïÁêÜÊúâ‰∏çÂ∞ëÁõ∏‰ºº„ÄÇ

‰æãÂ¶ÇËÉΩÂê¶Âú®hanlp.properties Âä†ÂÖ•section/domain ÔºåÂèØ‰ª• Âú®ÈÅãÁÆóÊôÇÊèõdictionary/corpus.
"
ËØ∑ÈóÆHanLP.properties,"ËØ∑ÈóÆhanlp.propertiesÊñá‰ª∂,
Ê≤°ÁúãÂà∞ÂèëÂ∏ÉÈ°µ https://github.com/hankcs/HanLP/releases
ÊúâËøô‰∏™Êñá‰ª∂ÁöÑ‰æãÂ≠êÔºå 

ËØ∑ÈóÆÊòØÂè™Ë¶ÅËøôÊ†∑‰∏ÄË°åÂ∞±Â•Ω‰∫Ü‰πàÔºü
root=/myhanlpdata/
"
ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÂ§ÑÁêÜ,"ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÂ§ÑÁêÜÊúâÂá†‰∏™ÁñëÈóÆÔºåËØ∑Êïô‰∏Ä‰∏ã
1.ÁõÆÂâçËá™ÂÆö‰πâËØçÂÖ∏ÈÉΩÊòØÁî®‰∫éÂØπÁ≤óÂàÜËØçÁªìÊûúËøõË°åÂêàÂπ∂Â§ÑÁêÜÁöÑÔºå‰∏çÁü•ÈÅìËøô‰∏™ÊòØÂê¶ÁêÜËß£ÊúâËØØÔºü
2.ÂØπ‰∫éÁ≤óÂàÜËØç‰∏∫""AB/C""ÊÉÖÂÜµ‰∏ãÔºåÂ¶ÇÊûúÈúÄË¶ÅË∞ÉÊï¥ÂàÜËØç‰∏∫""A/BC""ÊòØÂê¶ÂøÖÈ°ª‰øÆÊîπÊ†∏ÂøÉËØçÂÖ∏CoreNatureDictionary.txtÔºü
3.Â¶ÇÊûúÊÉ≥Âä®ÊÄÅ‰øÆÊîπÊ†∏ÂøÉËØçÂÖ∏ÔºåÈô§‰∫ÜÊñá‰ª∂Á≥ªÁªü‰øÆÊîπÂÜçËß¶ÂèëÂä†ËΩΩËØçÂÖ∏ËøôÁßçÊñπÊ≥ïÂ§ñÔºåÊòØÂê¶ÂèØ‰ª•Áõ¥Êé•ÈÄöËøáÂÜÖÂ≠òÂèòÈáè‰øÆÊîπÂÆûÁé∞Ôºü
"
ÂÅáËÆæÊúâ‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑËØçÂ∫ìÔºåËæìÂÖ•‰∏Ä‰∏™ËØçÊàñËÄÖËØ≠Âè•ÂéªÂØªÊâæÂá∫ÊúÄÁõ∏ËøëÁöÑËØç,"Â§ßÁ•ûÔºå ‰Ω†Ëøô‰∏™È°πÁõÆÁõÆÂâçËÉΩÂ§üÂÆûÁé∞Ëøô‰∏™ÈúÄÊ±ÇÂêóÔºå ÊñáÂ≠ó‰∏äÊé•ËøëÂåπÈÖçÂ∞±ÂèØ‰ª•
"
ÊåâÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊñáÁ´†ÔºåÁî®Âì™‰∏™ÊñπÊ≥ï,"ÊåâÊ†áÁÇπÁ¨¶Âè∑ÂàÜÂâ≤ÊñáÁ´†ÔºåÁî®Âì™‰∏™ÊñπÊ≥ï
"
C#‰∏≠Ë∞ÉÁî®Hanlp.dllÊñá‰ª∂ÊâßË°åÁ®ãÂ∫èÂá∫Áé∞‰ª•‰∏ãÈîôËØØ,"![image](https://cloud.githubusercontent.com/assets/19245664/16475947/29d8d9b6-3eb5-11e6-8873-5a4770a76945.png)
"
Ëøô‰∏™ËÉΩ‰∏çËÉΩÂÆûÁé∞ÊêúÁ¥¢ËÅîÊÉ≥Êé®Ëçê,"Áé∞Âú®ÂèØ‰ª•ÂÆûÁé∞x5Â∑ßÂÖãÂäõ ÂàÜËØçÊé®ËçêÊàêÂ∑ßÂÖãÂäõ„ÄÇ
 ÈááÁî®Âêå‰πâËØçËØçÂ∫ìÊé®ËçêÁöÑËØùÔºåÊúâÊ≤°ÊúâÊñπÊ≥ïÂè™Ëé∑ÂèñÂà∞Âêå‰πâËØçËØçÂ∫ì‰∏≠ÁöÑËØçÔºå ÁõÆÂâçÁî®elasticsearchÁöÑikÂàÜËØçÂíåÂêå‰πâËØçfilterËøáÊª§Ôºå ÊàñÂá∫Áé∞ÂæàÂ§ö‰∏çÁõ∏ÂÖ≥ÁöÑËØçÔºåÊØîÂ¶ÇÊñπ‰æøÈù¢Ôºå‰ºöÂá∫Êù•Êñπ‰æøÔºåÊ≥°Èù¢...  ÊÄé‰πàËÆ©‰∏çÂú®ÂàÜËØçËØçÂ∫ì‰∏≠ÁöÑËØç‰∏çÂá∫Êù•„ÄÇ
ÂèØ‰ª•ËÆ§‰∏∫ÊúâÊ≤°ÊúâÂÆûÁé∞Ê†πÊçÆ‰∏Ä‰∏™ÊåáÂÆöÁöÑËØçÂ∫ìÔºåÊúâÂêå‰πâËØçÁöÑÊ¶ÇÂøµÔºåÂè™ÂàÜËØçÊàêËØçÂ∫ì‰∏≠ËØçÔºå‰∏çÂú®ËØçÂ∫ì‰∏≠ÁöÑ‰∏¢ÂºÉ
"
‰ΩøÁî® File.getCanonicalPath() to Â±ïÂºÄ hanlp.properties ÈáåÊåáÂÆöÁöÑ root ,"‰ΩøÁî® `File.getCanonicalPath()` to Â±ïÂºÄ`hanlp.properties` ÈáåÊåáÂÆöÁöÑ rootÔºåËøôÊ†∑ÂèØ‰ª•Âú® `hanlp.properties` Èáå‰ΩøÁî®Á±ª‰ºº `.` / `..` Á≠âÁõ∏ÂØπË∑ØÂæÑÁöÑË°®Ëææ
"
ËØçÁöÑÁªÜÂàÜÈóÆÈ¢ò,"ÂΩìÊàëËæìÂÖ•‚ÄúÊ¥óËΩ¶Ë°åÔºåÁêÜÂèëÂ∫óÔºåÊ∏∏Ê≥≥È¶Ü‚ÄùËøôÁ±ªËØçÊó∂ÔºåÂÆÉ‰ª¨Ë¢´ÂàáÂàÜÊàê""Ê¥ó/nz, ËΩ¶Ë°å/nis, Ôºå/w, ÁêÜÂèëÂ∫ó/nis, Ôºå/w, Ê∏∏Ê≥≥È¶Ü/n]""„ÄÇÂÖ∂ÂÆûÊ≠£Á°ÆÁöÑÂàáÂàÜÂ∫îËØ•Êó∂‚ÄúÊ¥óËΩ¶ÔºèË°å/, ÁêÜÂèëÔºèÂ∫ó/ , Ê∏∏Ê≥≥ÔºèÈ¶Ü/n]‚ÄùÔºåËøôÊ†∑ÁöÑËØùÂú®ÊêúÁ¥¢‚ÄúÊ¥óËΩ¶ÔºåÁêÜÂèëÔºåÊ∏∏Ê≥≥‚ÄùÊó∂‰πüËÉΩÁ≤æÁ°ÆÂåπÈÖçÂà∞
"
ËØçÂÖ∏Âä†ËΩΩÈîôËØØ,"‰∏ãËΩΩÁöÑ600mÁöÑdataËØçÂÖ∏ÔºåÂà†Èô§dic‰∏≠binÁ±ªÂûãÁöÑÊñá‰ª∂ÔºåÂú®Êú¨Âú∞ide‰∏≠ËÉΩÂä†ËΩΩÁîüÊàêbinÊñá‰ª∂Ôºå‰πüËÉΩÊ≠£Â∏∏ÂàÜËØçÔºå‰ΩÜÊòØÊâìÂåÖÊîæÂà∞ÊúçÂä°Âô®‰∏äÔºåÁîüÊàêbin ÁöÑÊó∂ÂÄôÊä•ÈîôÔºåÊÉ≥ËØ∑ÈóÆ‰∏ãÊòØ‰ªÄ‰πàÂéüÂõ†Âë¢Ôºå‰ø°ÊÅØÂ¶Ç‰∏ãÔºö
`at com.hankcs.hanlp.seg.Segment.quickAtomSegment(Segment.java:161)
        at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.GenerateWordNet(WordBasedGenerativeModelSegment.java:458)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:42)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)`

`Caused by: java.lang.ArrayIndexOutOfBoundsException: 32618
        at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToChar(ByteUtil.java:255)
        at com.hankcs.hanlp.corpus.io.ByteArray.nextChar(ByteArray.java:81)
        at com.hankcs.hanlp.dictionary.other.CharType.<clinit>(CharType.java:77)
        ... 15 more`
"
mac‰∏ã heap Size,"ÂæàÊúâÂèØËÉΩÊàëÊòØÂú®ÈóÆË†¢ÈóÆÈ¢òÔºö
ÊàëÂú®windowsËøêË°åÊó†‰ªª‰ΩïÈóÆÈ¢òÔºå1000Ê¨°Ë∑ë‰∏ãÊù•benchmarkÂú®122mbÂ∑¶Âè≥ÔºàËá™Â∑±ÁöÑÁ®ãÂ∫èÔºâ
‰ΩÜÂ∏ÆÂä©ÂÖ∂‰ªñ‰∫∫ÈÉ®ÁΩ≤Âú®Mac‰∏ãÂá∫Áé∞‰∫Üheap size ÈóÆÈ¢ò
‰ΩøÁî®IntellijÂíåEclipseÂùáÊó†Ê≥ïË∑ëËµ∑Demo1
-Xmx6g ‰º†ÂÖ•VMoptionËøòÊòØÊó†ÊûúÔºå
-Xmx4g -Xmx2g -Xmn2g‰πüÂ∑≤ÁªèËØïËøá

`
System.out.println(HanLP.segment(""‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®HanLPÊ±âËØ≠Â§ÑÁêÜÂåÖÔºÅ""));
`

`usr/lib/jvm/java-8-oracle/bin/java -d64 -Xmx6g -Didea.launcher.port=7534 -Didea.launcher.bin.path=/home/master/Prog/idea-IU-145.1617.8/bin -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/home/master/Documents/<Project Name>/target/classes:/home/master/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/master/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/master/Documents/<Project Name>/lib/hanlp-1.2.9.jar:/home/master/Prog/idea-IU-145.1617.8/lib/idea_rt.jar com.intellij.rt.execution.application.AppMain Main
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.dictionary.CoreDictionary.loadDat(CoreDictionary.java:141)
    at com.hankcs.hanlp.dictionary.CoreDictionary.load(CoreDictionary.java:63)
    at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:40)
    at com.hankcs.hanlp.seg.common.Vertex.<clinit>(Vertex.java:56)
    at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
    at Main.main(Main.java:30)
`
"
 ÂÖ≥‰∫éÊú∫ÊûÑÂàÜËØçÂ≠¶‰π†Êñ∞ËØçÈóÆÈ¢ò,"ËØª‰∫ÜÊÇ®ÂÜôÁöÑÊñáÊ°£ÔºåÂú®‰ΩøÁî®Â¶Ç‰Ωï‰ΩøÁî®ÂäüËÉΩÂÜôÁöÑÈùûÂ∏∏ËØ¶ÁªÜ„ÄÇËá™ÂÆö‰πâÁöÑÂÖ¨Âè∏ÂêçËØ≠ÊñôÊÄé‰πàÊ∑ªÂä†Âà∞Êú∫ÊûÑÂàÜËØçÊ®°Âûã‰∏≠„ÄÇ
"
package com.hankcs.hanlp;Â∞ÜÂàÜÂè∑Êîπ‰∏∫ÂÜíÂè∑,"ÂØºÂÖ•Âà∞IDEA‰∏≠ÈúÄË¶ÅÂ∞Üfor (String path : classPath.split("";""))Êîπ‰∏∫for (String path : classPath.split("":""))
"
Êñ∞ËØçÂèëÁé∞ÁöÑÈóÆÈ¢ò,"ËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰ªéÊÇ®ÁöÑÊñáÊ°£‰∏≠ÔºåÊàëÁü•ÈÅìÂèØ‰ª•ÈÄèËøáËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊñπÂºèÊù•Âä†ÂÖ•Êñ∞ËØç(TXTÊàñ‰ª£Á†Å‰∏≠Áõ¥Êé•Âä†ÂÖ•)ÔºåÁªèÂÆûÈôÖÊµãËØïÔºåËá™ÂÆö‰πâÁöÑÊñ∞ËØçÈÉΩÂèØ‰ª•Ë¢´Ëæ®ËØÜÁöÑÂæàÂ•Ω„ÄÇ‰ΩÜÊÉ≥ËØ∑Êïô‰∏Ä‰∏™Êõ¥Ê†πÊ∫êÁöÑÂ∫îÁî®ÈóÆÈ¢òÔºåÂÅáËÆæÂæÖÂ§ÑÁêÜÁöÑÊñáÊ°£Ê∫êÊ∫ê‰∏çÊñ≠ÁöÑËøõÊù•ÔºåÂàôÂèØ‰ª•ÈÄèËøá‰ªÄÈ∫ΩÊñπÂºèÊù•ÂèëÁé∞Êñ∞ËØçÂë¢Ôºü
ÊØîÊñπÊÇ®ÁöÑÊñáÊ°£ÊèêÂà∞ÔºåCRFÂèëÁé∞Êñ∞ËØçËÉΩÂäõËæÉÂº∫ÔºåÊàëÂÅö‰∫Ü‰∏Ä‰∏™ÂÆûÈ™åÔºåËæìÂÖ•Â∏¶Êúâ„ÄåÁêÜÈÉΩÊáÇÔºåÁÑ∂Âπ∂ÂçµÔºåÂüé‰ºöÁé©ÔºåÊó•‰∫ÜÁãó„ÄçÁ≠âÁΩëË∑ØËØ≠Ë®ÄÔºåÊàëÂèëÁé∞Ëøô‰∫õ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ìÈáå‰∏çÂ§ßÂèØËÉΩÂá∫Áé∞ÁöÑËØçÔºåÈÉΩ‰∏ç‰ºöË¢´ËØÜÂà´‰∏∫Êï¥ËØçÔºåËÄåÊòØË¢´ÊãÜÂàÜ‰∏∫Á≤íÂ∫¶Êõ¥Â∞èÁöÑËØçÔºåÂπ∂ËØÜÂà´Âá∫Êù•ÔºåÊç¢Ë®Ä‰πãÔºåÂÆÉ‰ª¨‰∏çË¢´ËØÜÂà´‰∏∫Êñ∞ËØç„ÄÇ
Âç≥‰ΩøÊòØÈÄèËøáCRFËøôÁßçËæ®ËØÜËÉΩÂäõÂº∫ÁöÑÁÆóÊ≥ïÔºåËøôÊ†∑ÁöÑÈóÆÈ¢òÊúâÊ≤°ÊúâËß£ÔºüÊòØ‰∏çÊòØ‰∏ÄÂÆöÂæóÈÄèËøáËØ≠ÊñôÂ∫ìÊâçÊúâÂäûÊ≥ïÂ§ÑÁêÜÔºåÊØîÊñπÂú®ËÆ≠ÁªÉCRFÊ®°ÂûãÁöÑÊó∂ÂÄôÔºåËØ≠ÊñôÂ∫ìÈáåÂ∞±ÂøÖÈ°ªÊúâËøô‰∫õÁΩëË∑ØËØçÔºü
"
 Êñ∞Ë©ûÁôºÁèæÁöÑÂïèÈ°å,"ËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰ªéÊÇ®ÁöÑÊñáÊ°£‰∏≠ÔºåÊàëÁü•ÈÅìÂèØ‰ª•ÈÄèËøáËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÊñπÂºèÊù•Âä†ÂÖ•Êñ∞ËØç(TXTÊàñ‰ª£Á†Å‰∏≠Áõ¥Êé•Âä†ÂÖ•)ÔºåÁªèÂÆûÈôÖÊµãËØïÔºåËá™ÂÆö‰πâÁöÑÊñ∞ËØçÈÉΩÂèØ‰ª•Ë¢´Ëæ®ËØÜÁöÑÂæàÂ•Ω„ÄÇ‰ΩÜÊÉ≥ËØ∑Êïô‰∏Ä‰∏™Êõ¥Ê†πÊ∫êÁöÑÂ∫îÁî®ÈóÆÈ¢òÔºåÂÅáËÆæÂæÖÂ§ÑÁêÜÁöÑÊñáÊ°£Ê∫êÊ∫ê‰∏çÊñ≠ÁöÑËøõÊù•ÔºåÂàôÂèØ‰ª•ÈÄèËøá‰ªÄÈ∫ΩÊñπÂºèÊù•ÂèëÁé∞Êñ∞ËØçÂë¢Ôºü
ÊØîÊñπÊÇ®ÁöÑÊñáÊ°£ÊèêÂà∞ÔºåCRFÂèëÁé∞Êñ∞ËØçËÉΩÂäõËæÉÂº∫ÔºåÊàëÂÅö‰∫Ü‰∏Ä‰∏™ÂÆûÈ™åÔºåËæìÂÖ•Â∏¶Êúâ„ÄåÁêÜÈÉΩÊáÇÔºåÁÑ∂Âπ∂ÂçµÔºåÂüé‰ºöÁé©ÔºåÊó•‰∫ÜÁãó„ÄçÁ≠âÁΩëË∑ØËØ≠Ë®ÄÔºåÊàëÂèëÁé∞Ëøô‰∫õ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ìÈáå‰∏çÂ§ßÂèØËÉΩÂá∫Áé∞ÁöÑËØçÔºåÈÉΩ‰∏ç‰ºöË¢´ËØÜÂà´‰∏∫Êï¥ËØçÔºåËÄåÊòØË¢´ÊãÜÂàÜ‰∏∫Á≤íÂ∫¶Êõ¥Â∞èÁöÑËØçÔºåÂπ∂ËØÜÂà´Âá∫Êù•ÔºåÊç¢Ë®Ä‰πãÔºåÂÆÉ‰ª¨‰∏çË¢´ËØÜÂà´‰∏∫Êñ∞ËØç„ÄÇ
Âç≥‰ΩøÊòØÈÄèËøáCRFËøôÁßçËæ®ËØÜËÉΩÂäõÂº∫ÁöÑÁÆóÊ≥ïÔºåËøôÊ†∑ÁöÑÈóÆÈ¢òÊúâÊ≤°ÊúâËß£ÔºüÊòØ‰∏çÊòØ‰∏ÄÂÆöÂæóÈÄèËøáËØ≠ÊñôÂ∫ìÊâçÊúâÂäûÊ≥ïÂ§ÑÁêÜÔºåÊØîÊñπÂú®ËÆ≠ÁªÉCRFÊ®°ÂûãÁöÑÊó∂ÂÄôÔºåËØ≠ÊñôÂ∫ìÈáåÂ∞±ÂøÖÈ°ªÊúâËøô‰∫õÁΩëË∑ØËØçÔºü
"
"hankcs, ‰∏∫Âï•Âú®CoreNatureDictionary.tr.txtÈáåÊ≤°ÊúâYgÊ†áÁ≠æÂïäÔºü","ÊòØÂõ†‰∏∫ÂÆÉÂá∫Áé∞ÁöÑÊ¨°Êï∞ËÄÅÂ∞ë‰∫ÜÂêóÔºåÂú®ÈÇ£‰∏™ÂåóÂ§ßÊ†áÊ≥®ÈõÜÈáåÊúâËøô‰∏™Ê†áÁ≠æÁöÑ
"
ËØ∑Êïô‰∏Ä‰∏ã,"Â∞èÁôΩËØ∑Êïô‰∏Ä‰∏™ÈóÆÈ¢òÔºåËØ∑ÈóÆÊÇ®Ëøô‰∏™dataÁõÆÂΩï‰∏≠ÁöÑdictionary‰∏ãÈù¢ÁöÑÊï∞ÊçÆÊòØÂì™ËæπÊù•ÁöÑÂïäÔºü
"
ÂºÄÂêØËØçÊÄßÈóÆÈ¢òÔºü,"Â§ßÁ•ûÂ•ΩÔºåÊàëÁî®ÈªòËÆ§ÁöÑÊ†áÂáÜÂàÜËØçÂô®ÔºåÊòæÂºèÂºÄÂêØËØçÊÄß‰∏é‰∏çËÆæÁΩÆÔºåÂàÜËØçÂêéÈÉΩËÉΩÂ§üËé∑ÂèñËØçÊÄßÔºü

List<Term> list=HanLP.newSegment().enablePartOfSpeechTagging(true).seg(document);

List<Term> list = HanLP.segment(document);

‰∏äÈù¢‰∏§Áßç ÂàÜËØçÁªìÊûúÈÉΩËÉΩÂ§üËé∑ÂèñËØçÊÄßÔºàË≤å‰ººÊ∫êÁ†ÅÈáåÈù¢ÈªòËÆ§ÈÖçÁΩÆÊòØ‰∏çÂºÄÂêØËØçÊÄßÁöÑÔºâ
"
Áî®Êà∑ËØçÂÖ∏ËØçÊÄßÈóÆÈ¢ò,"Áî®Êà∑ËØçÂÖ∏ÈááÁî®ÈªòËÆ§ËØçÊÄßÔºå‰πüÂ∞±ÊòØnzÔºåËÉΩÂê¶Âú®ÂàÜËØçÁöÑÊó∂ÂÄôÔºåËá™Âä®ËØÜÂà´Êñ∞Ê∑ªÂä†ËØçÁöÑËØçÊÄßÔºåËÄå‰∏çÊòØÂÖ®ÈÉ®ÈÉΩÊòØÂêçËØçÔºü
"
TraditionalChineseToker cannot use CustomerDictionary,"I am testing different tokenizer performance, I found that the TraditionalChineseTokenizer does not apply the CustomDicionary as the others do.

```
CustomDictionary.add(""ÈäÄ‰∏ªÁõ§"", ""nz 1024 n 1"");
  Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true);

        for (String sentence : testCase)
        {
            System.out.println( ""\n\n"" + ""Sentence"" + sentence);
            System.out.println(""HanLP"");
            List<Term> termList = segment.seg(sentence);
            System.out.println(termList);

            termList = TraditionalChineseTokenizer.segment(sentence);
            System.out.println(""TraditionalChineseTokenizer"");
            System.out.println(termList);


            termList = NLPTokenizer.segment(sentence);
            System.out.println(""NLPTokenizer"");
            System.out.println(termList);
        }
```

Results
Sentence (Only TraditionalChineseTokenizer does not apply the ""ÈäÄ‰∏ªÁõ§"" new word in CustomDictionary.)

```

ÈäÄ‰∏ªÁõ§,Â•ΩÂ§ö‰∫∫Á≠âÁ∑äÊ®ìÂ∏ÇÂ§ßË∑å!
HanLP
[ÈäÄ‰∏ªÁõ§/nz, ,/w, Â•ΩÂ§ö/mq, ‰∫∫/n, Á≠â/udeng, Á∑ä/n, Ê®ì/n, Â∏Ç/n, Â§ßË∑å/v, !/w]
TraditionalChineseTokenizer
[ÈäÄ/ng, ‰∏ªÁõ§/n, ,/w, Â•ΩÂ§ö/mq, ‰∫∫/n, Á≠â/udeng, Á∑ä/d, Ê®ìÂ∏Ç/n, Â§ßË∑å/v, !/w]
NLPTokenizer
[ÈäÄ‰∏ªÁõ§/nz, ,/w, Â•ΩÂ§ö‰∫∫/nt, Á≠â/udeng, Á∑ä/n, Ê®ì/n, Â∏Ç/n, Â§ßË∑å/v, !/w]
```
"
CRFSegment Âì™‰∏ãËºâ,"Âú®data.zip ‰∏≠Ê≤íÊúâCRFSegmentModel.txt, Âõ∫DemoCRFSegment.java ÊúâÂ¶Ç‰∏ãÈåØË™§:

```
java -classpath "".:../hanlp/src/main/java/""  com/hankcs/demo/DemoCRFSegment
Jun 14, 2016 1:02:29 AM com.hankcs.hanlp.model.CRFSegmentModel <clinit>
SEVERE: CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ hanlp/data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåËÄóÊó∂ 5 ms
```

ÊàëÂú® http://www.hankcs.com‰πüÊâæ‰∏çÂà∞Áõ∏ÈóúÊñáÁ´†„ÄÇË´ãÂïèÂú®Âì™ÂèØ‰∏ãËºâÊàñÊÄéÊ®£Âª∫Á´ãÔºü

Ë¨ùË¨ù.
"
ËøΩÂä†ÁπÅÁ∞°È´îË©ûÂÖ∏,"Âú®ËøΩÂä†Â≠óÂÖ∏ÁöÑ‰æãÂ≠ê‰∏≠Ôºö

```
8. Áî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏
...
ËøΩÂä†ËØçÂÖ∏

    CustomDictionary‰∏ªËØçÂÖ∏ÊñáÊú¨Ë∑ØÂæÑÊòØdata/dictionary/custom/CustomDictionary.txtÔºåÁî®Êà∑ÂèØ‰ª•Âú®Ê≠§Â¢ûÂä†Ëá™Â∑±ÁöÑËØçËØ≠Ôºà‰∏çÊé®ËçêÔºâÔºõ‰πüÂèØ‰ª•ÂçïÁã¨Êñ∞Âª∫‰∏Ä‰∏™ÊñáÊú¨Êñá‰ª∂ÔºåÈÄöËøáÈÖçÁΩÆÊñá‰ª∂CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; ÊàëÁöÑËØçÂÖ∏.txt;Êù•ËøΩÂä†ËØçÂÖ∏ÔºàÊé®ËçêÔºâ
```

ÊàëÊòéÁôΩÊé®ËçêÊñ∞Âä†‰∏ÄÂÄãË©ûÂÖ∏„ÄÇ

ÈÇ£ÊàëÊñ∞ÁöÑË©ûÂÖ∏ÊáâÂê¶ÊääÁπÅÈ´îÂíåÁ∞°È´îÂàÜÈñãÔºü (ÊúâÊ≤íÊúâÂØ¶ÈöõÁöÑÊïàÊûú/ÊïàËÉΩÂΩ±Èüø)„ÄÇ
‰æãÂ¶ÇÊàëÊñ∞Âä†‰∏ÄÂÄã`Á∂ìÊøüÂ≠óÂÖ∏.txt`ÔºåÊòØÂê¶Â¶Ç‰∏ãÊñ∞Â¢ûÊúÄ‰Ω≥Ôºü

`````` Á∂ìÊøüÂ≠óÂÖ∏.zh-TW.txt```
```Á∂ìÊøüÂ≠óÂÖ∏.zh-CN.txt```

``````
"
Â¶Ç‰ΩïÊ†πÊçÆÊãºÈü≥ÔºàÊàñËÄÖÊ±âÂ≠óÔºâËé∑ÂèñÊúÄÁõ∏ËøëÁöÑËØçÁªÑ,"ÊØîÂ¶ÇËØ¥Áî®Êà∑ËæìÂÖ•‰∫ÜÂæêÂÆ∂Ê±áÔºå‰ΩÜÊòØÁî®Êà∑ÂÆûÈôÖÊÉ≥ÂæóÂà∞ÁöÑÊ≠åÊâãËÆ∏‰Ω≥ÊÖßÔºü ÂèØ‰ª•ÊãºÈü≥ËΩ¨Êé•ÁßçÂ¢ûÂä†Áõ∏Â∫îÁöÑÊé•Âè£ÂêóÔºü
"
CRFÁöÑÁâπÂæÅÊ®°ÊùøÂíåÁâπÂæÅÂáΩÊï∞,"ÂØπCRF‰∏çÊòØÁâπÂà´ÁêÜËß£ÊÉ≥ËØ∑Êïô‰∏ã
1 ÁâπÂæÅÊ®°ÊùøÊòØËá™Â∑±ÊâãÂ∑•ÁºñÂÜôÁöÑÂêó? Â¶ÇÊûúÊúâÊúâÂæàÂ§öË°åÊôÆÈÄöÊñáÊú¨ÔºåÊòØ1Ë°åÊñáÊú¨Â∞±ÊòØ‰∏Ä‰∏™ÁâπÂæÅÊ®°ÊùøÂêóÔºü
2 ÂØπ‰∫éCRF++,Â¶ÇÊûúÊúâËØ≠ÊñôÂíåÁâπÂæÅÊ®°ÊùøÊòØ‰∏çÊòØÂ∞±ÂèØ‰ª•ÁîüÊàêÊ®°ÂûãÂíØ?
3  Á¨¨2ÁÇπÂ¶ÇÊûúÊòØÔºåÈÇ£ËØ≠ÊñôËØ•‰ªéÈÇ£ÈáåËé∑ÂèñÂë¢ÔºåÂÆÉÂíå(1)‰∏≠ÁöÑÊôÆÈÄöÊñáÊú¨Êúâ‰ªÄ‰πàÂÖ≥Á≥ª„ÄÇ
"
ÈóÆÈ¢òËØ∑Êïô,"http://www.hankcs.com/nlp/hanlp.html„ÄÄËøôÈáåËØ¥ÁöÑ‰ª•‰∏ãÂäüËÉΩÔºåÊúâÂÆû‰æãÂêóÔºüÊàñÊòØ‰ª£Á†ÅÂú®Âì™Ôºü
ËØ≠ÊñôÂ∫ìÂ∑•ÂÖ∑
ÂàÜËØçËØ≠ÊñôÈ¢ÑÂ§ÑÁêÜ
ËØçÈ¢ëËØçÊÄßËØçÂÖ∏Âà∂‰Ωú
BiGramÁªüËÆ°
ËØçÂÖ±Áé∞ÁªüËÆ°
CoNLLËØ≠ÊñôÈ¢ÑÂ§ÑÁêÜ
CoNLL UA/LA/DAËØÑÊµãÂ∑•ÂÖ∑
"
"Â¶Ç‰ΩïÂª∫Á´ã ÊÉÖÊÑüÂàÜÊûê, ËØÜÂà´ÊñáÊú¨Ëï¥Ê∂µ(RTE)Á≠âÂπ≥Âè∞","Âú®Ëã±ÊñáÊñáÊú¨ÊåñÊéòÔºåÂèØ‰ª•ÊâæÂà∞‰∏Ä‰∫õÊÉÖÊÑüÂàÜÊûê, ËØÜÂà´ÊñáÊú¨Ëï¥Ê∂µ ÁöÑÊáâÁî®ÂíåÂπ≥Âè∞„ÄÇ

Âú®‰∏≠ÊñáÊñáÊú¨ÊåñÊéòÔºåÊàëÊâæÂà∞ÁöÑÊòØÊù±ÊãºË•øÊπäÁöÑÊñπÊ°àÔºå‰æãÂ¶Ç python ÁöÑnltk +jieba (ÂàÜË©û)„ÄÇ

‰ΩúÁÇ∫Áµ±‰∏ÄÁöÑ‰∏≠ÊñáÊñáÊú¨ÊåñÊéòÁ≥ªÁµ±ÔºåHanLP ÊòØÈùûÂ∏∏ÂêàÈÅ©ÁöÑ„ÄÇ 

HanLP ÊúâÊ≤íÊúâÁõ∏ÈóúÁöÑÊñπÊ≥ï/ÊñπÊ°àÔºåËÉΩÂª∫ÊàêÂ¶Ç‰∏ãÁöÑÂπ≥Âè∞Ôºö

ÊÉÖÊÑüÂàÜÊûê„ÄÅ ËØÜÂà´ÊñáÊú¨Ëï¥Ê∂µ(RTE)Á≥ªÁµ±„ÄÅHadoop/Spark ÁµêÂêà„ÄÅÊú∫ÊûÑÂêçËØÜÂà´Ê®°ÂùóÁöÑÊ†∏ÂøÉÊ®°Âûã (http://www.hankcs.com/nlp/ner/place-name-recognition-model-of-the-stacked-hmm-viterbi-role-labeling.html)

ÊàëÂçÅÂàÜÊ®ÇÊÑèÂèÉËàáÁõ∏ÈóúÈñãÁôº„ÄÇ
"
Â¶Ç‰ΩïÊâìÂåÖhanlp portable,"ÊúÄËøëÊúâÊñ∞ÁöÑcommitÔºåÊÉ≥Ë¶ÅËá™Â∑±ÊâìÂåÖportableÁâàÊú¨
Ë´ãÂïèÊúâÊúâÈóúÁöÑÊâìÂåÖportableÁöÑÊïôÂ≠∏Âóé?
Ë¨ùË¨ù
"
Ëá™ÂÆö‰πâËØçÂÖ∏Âà†Èô§ËØçËØ≠Â§±Êïà,"‰Ω†Â•ΩÔºåÂú®‰ΩøÁî®Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑ‰æãÂ≠êÊó∂ÔºåÂèëÁé∞Â¶Ç‰∏ãÈóÆÈ¢òÔºö
Âä†ÂÖ•Ëá™ÂÆö‰πâËØçÁªÑËÉΩÂ§ü‰ΩøÂéüÂÖàË¢´ÂàÜÂâ≤ÂºÄÁöÑÂõ∫ÂÆöÊê≠ÈÖç‰øùÊåÅÊï¥‰Ωì‰∏çÂàáÂàÜÔºå‰ΩÜÊòØÂ¶ÇÊûú‰πãÂêéÂÜç‰ΩøÁî®removeÁ≠âÊñπÊ≥ïÁßªÈô§ËØ•ÁªÑÂêàÔºåÈÇ£‰πàÂ∞Ü‰∏çËµ∑‰ΩúÁî®„ÄÇ

```
    import com.hankcs.hanlp.HanLP;
    import com.hankcs.hanlp.dictionary.CustomDictionary;

    public class DemoCustomDictionary {
      public static void main(String[] args) {
        String customTerm = ""ÊîªÂüéÁãÆ"";
        String text = ""ÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´ÁãóÔºåËøéÂ®∂ÁôΩÂØåÁæéÔºåËµ∞‰∏ä‰∫∫ÁîüÂ∑ÖÂ≥∞"";
        System.out.println(""ÂéüÂßãÂàÜËØçÁªìÊûú"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
        // Âä®ÊÄÅÂ¢ûÂä†
        CustomDictionary.add(customTerm);
        System.out.println(""Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÁªÑÂàÜËØçÁªìÊûú"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
        // Âà†Èô§ËØçËØ≠
        CustomDictionary.remove(customTerm);
        System.out.println(""Âà†Èô§Ëá™ÂÆö‰πâËØçÁªÑÂàÜËØçÁªìÊûú"");
        System.out.println(""CustomDictionary.get(customTerm)="" + CustomDictionary.get(customTerm));
        System.out.println(HanLP.segment(text));
      }
    }
```

ÊúÄÂêéËæìÂá∫ÁªìÊûú

```
  ÂéüÂßãÂàÜËØçÁªìÊûú
  CustomDictionary.get(customTerm)=null
  [ÊîªÂüé/vi, ÁãÆ/ng, ÈÄÜË¢≠/nz, ÂçïË∫´/n, Áãó/n, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
  Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÁªÑÂàÜËØçÁªìÊûú
  CustomDictionary.get(customTerm)=nz 1 
  [ÊîªÂüéÁãÆ/nz, ÈÄÜË¢≠/nz, ÂçïË∫´/n, Áãó/n, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
  Âà†Èô§Ëá™ÂÆö‰πâËØçÁªÑÂàÜËØçÁªìÊûú
  CustomDictionary.get(customTerm)=null
  [ÊîªÂüéÁãÆ/nz, ÈÄÜË¢≠/nz, ÂçïË∫´/n, Áãó/n, Ôºå/w, ËøéÂ®∂/v, ÁôΩÂØåÁæé/nr, Ôºå/w, Ëµ∞‰∏ä/v, ‰∫∫Áîü/n, Â∑ÖÂ≥∞/n]
```

ÊÉ≥ÈóÆ‰∏ãÊòØ‰∏çÂÖÅËÆ∏ËøôÁßç‰∏¥Êó∂ÊÄßÊìç‰ΩúÂêóËøòÊòØÂè¶ÊúâÂÖ∂‰ªñÔºü

Âè¶Â§ñÔºåHanLPÊòØÂê¶ÊîØÊåÅÁî®Êà∑Ëá™ÂÆö‰πâÂàÜËØçÁªìÊûú(Â∑≤ÁªèËøá‰∫∫‰∏∫ÂàÜËØçÁöÑËØ≠Âè•‰πãÈó¥‰ΩøÁî®Á©∫Ê†ºÊàñËÄÖ###Á≠âÁâπÊÆäÁ¨¶Âè∑Èó¥Èöî)ÔºåÁÑ∂ÂêéÈÄöËøá‰∏ÄÁßçÁâπÂÆöÁöÑTokenizerÂàÜÂâ≤ÊàêÊï∞ÁªÑÂΩ¢ÂºèÁÑ∂Âêé‰º†ÂÖ•SegmentËøõË°åPOSÂàÜÊûê„ÄÇ
ÊØîÂ¶ÇËøôÂè•`ÊàëÂæàÊÉ≥Ë¶Å‰∏™ÂÆùÂÆù,‰ΩÜËßâÂæóÂ•ΩÈöæÊâçÊúâ,Êó©‰∫å‰∏™ÊúàÊÄÄ‰∏ä‰∫Ü,‰ΩÜÊµÅÂéª‰∫Ü`Ôºå
ÂÅáËÆæ""Ê≠£Á°Æ""ÁöÑÂàÜËØçÁªìÊûú‰∏∫
`Êàë Âæà ÊÉ≥ Ë¶Å ‰∏™ ÂÆùÂÆù , ‰ΩÜ ËßâÂæó Â•ΩÈöæ Êâç Êúâ , Êó© ‰∫å‰∏™Êúà ÊÄÄ‰∏ä ‰∫Ü , ‰ΩÜ ÊµÅÂéª ‰∫Ü`Ôºå‰∏çÁü•ÈÅìËØ•Â¶Ç‰ΩïÂú®HanLPÊìç‰ΩúÂæóÂà∞ÂàÜÊûêÁªìÊûúÁ≠â

ÊÑüË∞¢ÔºÅ

ËøêË°åÁéØÂ¢É
HanLP v1.2.9 
Ubuntu 14.04 x86_64
JDK 1.8.0_73
"
Â§ö‰∏™Ê†áÁÇπÁ¨¶Âè∑ÂàÜÊàê‰∏Ä‰∏™ËØç„ÄÇ,"‰∏ªË¶ÅÊúâÂá†ÊñπÈù¢ÂàÜÂéüÂõ†**:(**1)ÁªèÊµéËøõÂÖ•ËΩ¨ÂûãÊúü,Êñ∞Â∏∏ÊÄÅ‰∏ãÈúÄÊ±ÇÊú¨Ë∫´ÊúâÊâÄÊîæÁºì„ÄÇ
ÊØîÂ¶ÇËøôÂè•‰∏≠ :( ÂàÜÊàê‰∏Ä‰∏™ËØç‰∫Ü
ËøôÁßçÈóÆÈ¢òÂèØ‰ª•Ëß£ÂÜ≥ÂêóÔºü
"
Ëá™ÂÆö‰πâÂ≠óÂÖ∏ÂàÜËØçÈîôËØØ,"Ëá™ÂÆö‰∫ÜÂçïÁî∞Ëä≥Ôºå‰∏âÂõΩÊºî‰πâ„ÄÇ ‰∏îÁ≥ªÁªüÂÖÅËÆ∏Ëá™ÂÆö‰πâÂ≠óÂÖ∏„ÄÇ ‰ΩÜÊòØËøòÊòØÂàÜÊàê‰∫ÜÂçïÁî∞Ëä≥‰∏â/nz, ÂõΩ/n, Êºî‰πâ/nÔºåËÉΩÂê¶Â∏ÆÊü•‰∏Ä‰∏ãÔºü
"
GC overhead limit exceeded,"java.lang.OutOfMemoryError: GC overhead limit exceeded
    at com.hankcs.hanlp.corpus.io.ByteArray.nextString(ByteArray.java:113)
    at com.hankcs.hanlp.model.maxent.MaxEntModel.create(MaxEntModel.java:328)
    at com.hankcs.hanlp.dependency.MaxEntDependencyParser.<clinit>(MaxEntDependencyParser.java:45)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at com.jfinal.ext.kit.Reflect.forName(Reflect.java:692)
    at com.jfinal.ext.kit.Reflect.on(Reflect.java:85)
    at com.jfinal.ext.kit.ClassSearcher.extraction(ClassSearcher.java:49)
    at com.jfinal.ext.kit.ClassSearcher.search(ClassSearcher.java:144)
    at com.jfinal.ext.route.AutoBindRoutes.config(AutoBindRoutes.java:73)
    at com.jfinal.config.Routes.add(Routes.java:40)
    at config.JfinalConfig.configRoute(JfinalConfig.java:36)
    at com.jfinal.core.Config.configJFinal(Config.java:47)
    at com.jfinal.core.JFinal.init(JFinal.java:65)
    at com.jfinal.core.JFinalFilter.init(JFinalFilter.java:49)
    at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:137)
    at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:831)
    at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:300)
    at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1341)
    at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1334)
    at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744)
    at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:497)
    at org.eclipse.jetty.maven.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:281)
    at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)
    at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:60)
    at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:154)
    at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:132)
    at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:114)
    at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:60)

ËøôÊòØ‰ªÄ‰πàÂéüÂõ†ÂºïËµ∑ÁöÑÂë¢Ôºü
"
NShortSegmentË∑ëÈ£ûÈóÆÈ¢ò,"‰ΩøÁî®new NShortSegment().enableAllNamedEntityRecognize(true)‰Ωú‰∏∫textcrankÊäΩÂèñÂÖ≥ÈîÆËØçÁöÑÂàÜËØçÂô®ÔºåÊåÅÁª≠ÊèêÂèñÂÖ≥ÈîÆËØçÂêéÔºå‰ºöÂèëÁé∞Â§ßÈáèGC,ÁÑ∂ÂêéÁõ¥Êé•ÊåÇÊéâÔºåÂú®Â†ÜÊ†à‰∏≠Ôºå‰∏ÄÁõ¥Âú®running NShortPath.class 211Ë°å
"
Â¶Ç‰ΩïÂú®ÂàÜËØçÊó∂ÂéªÊéâÂÅúÁî®ËØçÔºü,"ÊàëÊÉ≥ÊääÂàÜËØçÁªìÊûú‰∏≠ÁöÑ‚ÄúxxÁöÑ‚ÄùËøôÁßçÁªìÊûúÂéªÊéâÔºåÂè™Áïô‰∏ãxxÔºåËØ•ÊÄé‰πàÂÅöÔºü
"
SpeedTokenizerÂàÜËØçÂô®ËØçÊÄßÊèêÂèñ‰∏çÂá∫Êù•,"ÊÇ®Â•ΩÔºåStandardTokenizerÊ†áÂáÜÂàÜËØçÂô®ÂèØ‰ª•ÊèêÂèñÂá∫ËØçÊÄßÔºå‰ΩÜÊòØSpeedTokenizerÊûÅÈÄüÂàÜËØçÂô®ÊèêÂèñ‰∏çÂá∫ËØçÊÄßÔºåÊòØ‰ªÄ‰πàÂéüÂõ†ÂØºËá¥ÁöÑ„ÄÇ
"
NShortSegmentÁî®Êà∑Â≠óÂÖ∏ÈóÆÈ¢ò,"Âú®‰ΩøÁî®NShortSegmentÂàÜËØçÁöÑÊó∂ÂÄôÔºåÊ∑ªÂä†‰∫ÜÁî®Êà∑Ëá™ÂÆö‰πâËØçÔºåCustomDictionary.add(""Ëá™Áî±‰æ†"", ""n 100000000"");Êó†Ê≥ïÂàÜÂá∫ËØ•ËØçÔºåÂÖ∂‰ªñÂàÜËØçÂô®ÂÄíÊòØÂèØ‰ª•ÔºåËØ∑ÈóÆ‰∏ãÊòØÁî®Êà∑ËØçÂÖ∏Âú®ËØ•ÂàÜËØçÂô®‰∏≠‰∏çËµ∑‰ΩúÁî®‰πàÔºü
"
ÂÖ≥‰∫éËØçÂÖ∏ÊâìÂåÖÈóÆÈ¢ò,"Êàë‰øÆÊîπ‰∫ÜhanlpÁÆóÊ≥ïÈáåÁöÑÂäüËÉΩÔºåÈáçÊñ∞ÊâìÂåÖÊàêjarÊñá‰ª∂ÔºåÂÖ∂‰ªñÁ®ãÂ∫èË∞ÉÁî®ÔºåËøêË°åÊàêÂäü„ÄÇ‰ΩÜÊòØÊääË∞ÉÁî®Á®ãÂ∫èÊâìÂåÖÊàê.jarÊñá‰ª∂‰∏•Èáç: Ê≤°ÊúâÊâæÂà∞HanLP.propertiesÔºåÂèØËÉΩ‰ºöÂØºËá¥Êâæ‰∏çÂà∞data
========Tips========
ËØ∑Â∞ÜHanLP.propertiesÊîæÂú®‰∏ãÂàóÁõÆÂΩïÔºö
WebÈ°πÁõÆÂàôËØ∑ÊîæÂà∞‰∏ãÂàóÁõÆÂΩïÔºö
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
Âπ∂‰∏îÁºñËæëroot=PARENT/path/to/your/data,ËØ∑ÈóÆËøô‰∏™ÈóÆÈ¢òÊÄé‰πàËß£ÂÜ≥ÔºåÊàëÂ∞ùËØïÊääÂØπÂ∫îÁöÑproperitesÁâàÊú¨‰øÆÊîπÊâìÂåÖË∞ÉÁî®ÁöÑÊó∂ÂÄôÊä•ÈîôÔºöÊ†∏ÂøÉËØçÂÖ∏Âä†ËΩΩÂ§±Ë¥•„ÄÇ
"
Â¶ÇÊûúcrfËÆ≠ÁªÉÁ≠âÈúÄË¶ÅÁî®Âà∞Â•Ω‰∏ÄÁÇπÁöÑÊúçÂä°Âô®,"Â¶ÇÊûúcrfËÆ≠ÁªÉÁ≠âÈúÄË¶ÅÁî®Âà∞Â•Ω‰∏ÄÁÇπÁöÑÊúçÂä°Âô®
ÂèØ‰ª•ÊâæÊàëÊêû‰∏Ä‰∏™Áã¨Á´ãlinuxÊúçÂä°Âô®ÁéØÂ¢ÉÁªô‰Ω†‰ΩøÁî®

jimichan@gmail.com
"
data/dictionary/person/familyname.txtÂπ∂‰∏çÂ≠òÂú®,"Âú®FamilyNameÁ±ª‰∏≠‰ºöÂéªËØªÂèñfamilyname.txtÔºåÁÑ∂ËÄåËøô‰∏™Êñá‰ª∂Âπ∂‰∏çÂ≠òÂú®
"
ÂàÜËØçÂíåÂè•Ê≥ï‰æùÂ≠òÁöÑ‰∏Ä‰∫õÁñëÈóÆ,"ÊØîÂ¶ÇÊ†∑Êú¨Êï∞ÊçÆÂ¶Ç‰∏ãÔºö
ÊùéÊòéÂíåÈ´òÊ¥Å‰∏ÄËµ∑Êù•Âà∞Â≤≠ÂçóÂå∫ÁöÑÁôæË¥ßÂïÜÂú∫ÈôÑËøëÔºåËÅîÁ≥ª‰∫ÜÊùéÂ∏àÂÇÖÔºåÊâãÊú∫Âè∑18622004501ÔºåÂÖ±Âêå‰πòÂùê‰∫Ü‰∏ÄËæÜÁâåÁÖß‰∏∫‰∫¨H56802ÁöÑÂ§ß‰ºóËΩøËΩ¶ÂéªÂåó‰∫¨Á´ô„ÄÇ
1.ÊâãÊú∫„ÄÅËΩ¶ÁâåËøôÁßçÂè∑Á†ÅÂàÜËØçÂíåËØçÊÄß‰∏çÂáÜÔºåËÉΩÂê¶Â∞ÜËøôÁ±ªÂè∑Á†ÅÁ±ªÂàÜËØçÁ∫†Ê≠£ÔºåËÉΩÂê¶Êâ©Â±ïÂá†‰∏™ËØçÊÄßÂ¶ÇÔºöÁîµËØùÂè∑Á†Å„ÄÅËΩ¶ÁâåÂè∑Á†Å„ÄÇ
ÊùéÊòéÂíå/nr, È´òÊ¥Å/a ‰∫∫ÂêçÂàíÂàÜ‰∏çÂØπ
2.Âè•Ê≥ïÂàÜÊûêÂèÇÁúã‰∫Ü‰∏Ä‰∏ãÂìàÂ∑•Â§ßhttp://www.ltp-cloud.com/demo/ÔºåËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÈÄöËøáÁ®ãÂ∫èÊÄé‰πàËé∑ÂèñÔºü
"
È´ò‰∫ÆÊêúÁ¥¢ÁªìÊûú‰ΩçÁΩÆÂÅèÂ∑Æ,"Ëøô‰∏™Â∫îËØ•Âíå https://github.com/hankcs/hanlp-solr-plugin/commit/7a797a6ac899074f0725398ac00878eb178e6be4 Ëøô‰∏™issues ÊúâÂÖ≥Á≥ªÂêßÔºü
"
‰øÆÊîπÊ†∏ÂøÉËØçÊÄßËØçÈ¢ëËØçÂÖ∏,"ÊØîÂ¶Ç‰Ω†Âú®data/dictionary/CoreNatureDictionary.txt‰∏≠ÂèëÁé∞‰∫Ü‰∏Ä‰∏™‰∏çÊòØËØçÁöÑËØçÔºåÊàñËÄÖËØçÊÄßÊ†áÊ≥®ÂæóÊòéÊòæ‰∏çÂØπÔºåÈÇ£‰πà‰Ω†ÂèØ‰ª•‰øÆÊîπÂÆÉÔºåÁÑ∂ÂêéÂà†Èô§ÁºìÂ≠òÊñá‰ª∂‰ΩøÂÖ∂ÁîüÊïà„ÄÇ

ËØ∑ÈóÆÔºåÊàëÁé∞Âú®‰øÆÊîπ‰∫ÜÊüêËØçÁöÑËØçÊÄßÔºåÊÄéÊ†∑ÊâçËÉΩ‰ΩøÂÖ∂ÁîüÊïàÔºüÂà†Èô§Âì™‰∫õÁºìÂ≠òÊñá‰ª∂ÔºüÔºüË∞¢Ë∞¢
"
DoubleArrayTrieÁöÑ‰∏Ä‰∏™resize bug,"Áî±‰∫éÂàùÂßãÂàÜÈÖçÂÜÖÂ≠ò‰Ωç65536*32‰∏∫1MÁ©∫Èó¥ÔºåÊâÄ‰ª•Á®ãÂ∫è‰∏≠Âü∫Êú¨Ê≤°ÊúâÂá∫ÂèëresizeÊñπÊ≥ï
‰ΩÜÊòØÂ¶ÇÊûúÂ∞ÜÂàùÂßãÂ§ßÂ∞èËÆæÁΩÆ‰Ωç10240 ÁÑ∂ÂêéÊâßË°åcoreËØçÂÖ∏buildÁöÑÊó∂ÂÄô
‰Ω†‰ºöÂèëÁé∞‰∏ãÈù¢Ëøô‰∏™‰ª£Á†ÅÁöÑ double l ‰ºöÊòØ‰∏™Ë∂ÖÁ∫ßÂ§ßÊï∞ 1‰∏áÂ§öÔºå‰ºöÂØºËá¥ÂÜÖÂ≠òÁàÜÁÇπ„ÄÇ
‰Ω†Ê£ÄÊü•‰∏Ä‰∏ãËøôËæπÁöÑÈÄªËæëÊòØÂê¶ÊúâÈóÆÈ¢ò

```
    begin = pos - siblings.get(0).code; // ÂΩìÂâç‰ΩçÁΩÆÁ¶ªÁ¨¨‰∏Ä‰∏™ÂÖÑÂºüËäÇÁÇπÁöÑË∑ùÁ¶ª
        if (allocSize <= (begin + siblings.get(siblings.size() - 1).code))
        {
            // progress can be zero // Èò≤Ê≠¢progress‰∫ßÁîüÈô§Èõ∂ÈîôËØØ
            double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0
                    * keySize / (progress + 1);
            resize((int) (allocSize * l));
        }
```
"
‰ΩøÁî®BitSet‰ª£Êõø boolean[] used Êï∞ÁªÑ,"BitSetÈááÁî®bit‰ΩçÔºåÈ´òÊïà‰∏îÈÅøÂÖçÁ©∫Èó¥Âç†Áî®
"
ËØ∑ÈóÆ‰∏Ä‰∏ãÊúâÊ≤°ÊúâÊää‰π¶ÂêçÂè∑ÔºåÂºïÂè∑Âú®ÂàÜËØçÁöÑÊó∂ÂÄôÂêàÂπ∂Êàê‰∏Ä‰∏™ËØçÁöÑÊÉÖÂÜµÂä†ËøõÊù•,"‰∏ç‰ºöÁé©javaÔºåÈÉΩ‰∏çÁü•ÈÅìÊÄé‰πàÊîπ
"
ÂàÜËØçÈîôËØØ,"ÊÇ®Â•ΩÔºåÊàëÂú®Ë∞ÉÁî®HanLP.segment(""ÊàëÈÅóÂøòÊàëÁöÑÂØÜÁ†Å‰∫Ü"")ËøõË°åÂàÜËØçÊó∂ÔºåÂèëÁé∞ÂàÜËØç‰∏çÂáÜÁ°ÆÔºåÁªìÊûú‰∏∫Ôºö[Êàë/r, ÈÅó/vg, ÂøòÊàë/b, ÁöÑ/uj, ÂØÜÁ†Å/n, ‰∫Ü/ul]ÔºåËÄåÂáÜÁ°ÆÁªìÊûúÂ∫îËØ•ÊòØÔºö[ÊàëÔºåÈÅóÂøòÔºåÊàëÔºåÁöÑÔºåÂØÜÁ†ÅÔºå‰∫Ü]Ôºå
ËøôÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü È∫ªÁÉ¶Â§ßÁ•ûÂ∏ÆÂøôÁúãÁúãÔºåË∞¢Ë∞¢„ÄÇ
"
v1.2.9 DemoCRFSegment error,"‰∏•Èáç: Â≠óÁ¨¶Ê≠£ËßÑÂåñË°®Âä†ËΩΩÂ§±Ë¥•ÔºåÂéüÂõ†Â¶Ç‰∏ãÔºö
java.io.FileNotFoundException: data\dictionary\other\CharTable.bin.yes (Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.<init>(FileInputStream.java:138)
    at java.io.FileInputStream.<init>(FileInputStream.java:93)
    at com.hankcs.hanlp.seg.CRF.CRFSegment$CharTable.<clinit>(CRFSegment.java:368)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:46)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:425)
    at com.madhouse.dsp.HanlpMain$$anonfun$1.apply(HanlpMain.scala:49)
    at com.madhouse.dsp.HanlpMain$$anonfun$1.apply(HanlpMain.scala:48)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    at com.madhouse.dsp.HanlpMain$.delayedEndpoint$com$madhouse$dsp$HanlpMain$1(HanlpMain.scala:48)
    at com.madhouse.dsp.HanlpMain$delayedInit$body.apply(HanlpMain.scala:18)
    at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
    at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
    at scala.App$$anonfun$main$1.apply(App.scala:76)
    at scala.App$$anonfun$main$1.apply(App.scala:76)
    at scala.collection.immutable.List.foreach(List.scala:381)
    at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
    at scala.App$class.main(App.scala:76)
    at com.madhouse.dsp.HanlpMain$.main(HanlpMain.scala:18)
    at com.madhouse.dsp.HanlpMain.main(HanlpMain.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
"
HanLP.parseDependency ÈóÆÈ¢ò,"HanLP.parseDependency(""Áæé‰∏ΩÂèàÂñÑËâØÁöÑ‰Ω†Ë¢´ÂçëÂæÆÁöÑÊàëÊ∑±Ê∑±ÁöÑÂñúÊ¨¢ÁùÄ"");
ÁªìÊûú
Áæé‰∏Ω --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> ‰Ω†
Âèà --(Áä∂‰∏≠ÁªìÊûÑ)--> ÂñÑËâØ
ÂñÑËâØ --(Âπ∂ÂàóÂÖ≥Á≥ª)--> Áæé‰∏Ω
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> Áæé‰∏Ω
‰Ω† --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÂñúÊ¨¢
Ë¢´ --(Áä∂‰∏≠ÁªìÊûÑ)--> ÂçëÂæÆ
ÂçëÂæÆ --(ÂÆö‰∏≠ÂÖ≥Á≥ª)--> Êàë
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ÂçëÂæÆ
Êàë --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÂñúÊ¨¢
Ê∑±Ê∑± --(Áä∂‰∏≠ÁªìÊûÑ)--> ÂñúÊ¨¢
ÁöÑ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> Ê∑±Ê∑±
ÂñúÊ¨¢ --(Ê†∏ÂøÉÂÖ≥Á≥ª)--> ##Ê†∏ÂøÉ##
ÁùÄ --(Âè≥ÈôÑÂä†ÂÖ≥Á≥ª)--> ÂñúÊ¨¢

ÂÖ∂‰∏≠ËøôÂùóÂ∫îËØ•‰∏çÊòØ‰∏ªË∞ìÂÖ≥Á≥ªÂêß
‰Ω† --(‰∏ªË∞ìÂÖ≥Á≥ª)--> ÂñúÊ¨¢
"
ËØ∑ÈóÆËØçÂÖ±Áé∞ÁªüËÆ°Â¶Ç‰Ωï‰ΩøÁî®,"ÁúãÂà∞ËØ≠ÊñôÂ∫ìÂ∑•ÂÖ∑Èáå‰ªãÁªçÊúâËØçÂÖ±Áé∞ÁªüËÆ°ÂäüËÉΩÔºå‰ΩÜÊòØÂ∏ÆÂä©ÊñáÊ°£ÈáåÊ≤°Êúâ‰ªãÁªçÔºåËØ∑ÈóÆÂ¶Ç‰Ωï‰ΩøÁî®
"
Â∞Ütest*Á±ªÂêç‰øÆÊîπ‰∏∫Test*,"ÈÅµ‰ªéJavaÁöÑÁºñÁ®ãËßÑËåÉÔºåÂ∞Ütest_Á±ªÂêç‰øÆÊîπ‰∏∫Test_
"
charÊï∞ÁªÑ‰ΩøÁî®Âª∫ËÆÆ,"Á®ãÂ∫è‰∏≠Â§öÂá∫‰ΩøÁî®
char[] chars = string.toCharArray()
ÁÑ∂ÂêéÈÄöËøá‰∏ãÊ†áËÆøÈóÆchar ÔºåÂ¶Ç chars[i] ËøôÁ±ª‰ª£Á†Å

ÂÖ∂ÂÆûJDK‰∏≠toCharArrayÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÊòØ
 char result[] = new char[value.length];
        System.arraycopy(value, 0, result, 0, value.length);
Áõ∏ÂΩì‰∫éÊØèÊ¨°toCharArrayÂ∞±‰ºöÂ§öÂá∫Â†Ü‰∏äcopyÁöÑÂä®‰Ωú

Âª∫ËÆÆÁõ¥Êé•‰ΩøÁî® 
string.charAt(i)‰ª£Êõø

ÊµãËØïÁªìËÆ∫ÔºöcharAtÊñπÊ≥ïÂú®ÊÄßËÉΩÁ®çÂæÆ‰ºòÂäøÁöÑÊÉÖÂÜµ‰∏ãÔºåÂáèÂ∞ëGCÊ∂àËÄó

=========ÊÄßËÉΩÊµãËØï‰ª£Á†Å=============
public static void main(String[] args) {
        String string = ""abcedfghijklml"";
        int len = string.length();
        char[] cc = string.toCharArray();

```
    for(int k=0;k<len;k++){
        char x = cc[k];
    }

    for(int k=0;k<len;k++){
        char x = string.charAt(k);
    }
    long t1 = System.currentTimeMillis();
    for(int i=0;i<500000;i++){
        char[] c = string.toCharArray();

        for(int k=0;k<len;k++){
            char x = c[k];
        }
    }
    long t2 = System.currentTimeMillis();       
    for(int i=0;i<500000;i++){
        for(int k=0;k<len;k++){
            char x = string.charAt(k);
        }
    }
    long t3 = System.currentTimeMillis();

    System.out.println(t2-t1);
    System.out.println(t3-t2);
}
```
"
ËÉΩÂê¶ÊääÁπÅÁÆÄËΩ¨Êç¢ÁöÑÂ≠óÂ∫ìÂàÜÂºÄÔºü,"Âú®ËøõË°åÁπÅÁÆÄËΩ¨Êç¢Êó∂‰ºöÈÅáÂà∞‰ª•‰∏ãÈóÆÈ¢òÔºö
1.‰∏Ä‰∏™ÁÆÄ‰ΩìÂ≠óÂØπÂ∫îÂ§ö‰∏™ÁπÅ‰ΩìÂ≠óÁöÑÊÉÖÂÜµÔºåÊØîÂ¶ÇÔºö
Âè∞    Ëá∫ Ê™Ø  È¢± Âè∞
Âá†    Âπæ Ê©ü Âá†
Âú®Â≠óÂ∫ìÈáåÈù¢ÂÜôÔºö
Ëá∫=Âè∞
Ê™Ø=Âè∞
È¢±=Âè∞
Áé∞Âú®Â¶ÇÊûúÂè™ÊòØËΩ¨Êç¢ÁÆÄ‰Ωì ‚ÄúÂè∞‚ÄùÔºåÂÆÉ‰ºöËΩ¨ÊàêÂì™‰∏Ä‰∏™ÔºüÊúÄ‰∏äÈù¢‰∏Ä‰∏™Ôºü

Â¶ÇÊûúÂ≠óÂ∫ìÈáåÈù¢Âè™ËÆ∞ÂΩïÁ¨¨‰∏ÄÊù° ‚ÄúËá∫=Âè∞‚Äù  ÈÇ£Â¶ÇÊûúÁπÅ‰ΩìÈáåÈù¢Êúâ ‚ÄúÊ™Ø‚Äù Â≠óÔºå Â∞±ËΩ¨‰∏ç‰∫ÜÁÆÄ‰Ωì‰∫Ü„ÄÇ

ËØ∑ÈóÆÂèØ‰∏çÂèØ‰ª•ÊääÁπÅÁÆÄËΩ¨Êç¢ÂíåÁÆÄÁπÅËΩ¨Êç¢ÁöÑÂ≠óÂ∫ìÂàÜÂºÄÔºü
"
ÂÖ≥‰∫éÊú∫ÊûÑËØÜÂà´,"K,L,M,P,S,WËøôÂá†‰∏™Ê†áÊ≥®ÂàÜÂà´‰ª£Ë°®‰ªÄ‰πàÔºüÊàëÂè™ÊâæÂà∞‰∫Ü‰∏ÄÈÉ®ÂàÜÁöÑÊ†áÊ≥®ÁöÑÊÑè‰πâ„ÄÇ
‰ΩÜÊòØËøôÂá†‰∏™ÁöÑÊÑè‰πâÂú®ÁΩëÈ°µ‰∏äÊâæ‰∏çÂà∞„ÄÇ
ÊàëÊúÄËøëÊâãÂ∑•Ê†áÊ≥®‰∫Ü‰∏ÄÊâπËØ≠Êñô„ÄÇ
Ê±áÊÄªÁöÑÁªìÊûúÂ¶Ç‰∏ã:
Ê®°Âºè  Ê≠£Á°Æ  ÈîôËØØ  ÊÄªÂÖ±  Ê≠£Á°ÆÁéá
CD  513 2512    3025    0.169586777
GD  2055    564 2619    0.78465063
FD  1695    694 2389    0.709501884
CCD 555 1790    2345    0.236673774
ÊúâÂÖ®ÈÉ®ÁöÑÊ†áÊ≥®ËØ≠ÊñôÔºå‰ΩÜÊòØÊ≤°Ë¥¥Âá∫Êù•„ÄÇ
ÊúâÊ≤°Êúâ‰ªÄ‰πàÊØîËæÉÂ•ΩÁöÑÊîπËøõÂª∫ËÆÆÔºü
ÊàëÁõÆÂâçÊÉ≥Âà∞ÁöÑÂè™Êúâ‰∏Ä‰∫õÁªüËÆ°Âä†ËßÑÂàôÁöÑÊñπÊ≥ï„ÄÇÊØîÂ¶ÇÔºöCDÁöÑÊ≠£Á°ÆÁéáËæÉ‰Ωé„ÄÇÂ∞±ÂÆö‰∏Ä‰∫õÁ±ªÂûãCÂíåDÊó†Ê≥ïÊàêËØç„ÄÇ
"
Question:ÂÖ≥‰∫éportable,"ÊÇ®Â•ΩÔºåÊàëÊÉ≥ËØ∑Êïô‰∏Ä‰∏ãÂ¶ÇÊûúÊàëÂú®mavenÈÖçÁΩÆ‰∏≠version‰∏∫portableÁöÑËØùÊòØÂê¶Êú¨Âú∞‰∏çÁî®ÂÜçÈÖçÁΩÆÂ≠óÂÖ∏‰∫ÜÔºàdictionaryÔºâÔºü
"
Unable to start JVM at src/native/common/jp_env.cpp:54,"hancks‰Ω†Â•ΩÔºåÊàëÂú®Áî®pythonË∞ÉÁî®hanlpÊó∂ÔºåÂ¶ÇÊûúÈáçÂ§çË∞ÉÁî®ÔºåÁ®ãÂ∫è‰ºöÊä•ÈîôÔºåÂπ∂‰∏îÊèêÁ§∫ÔºöUnable to start JVM at src/native/common/jp_env.cpp:54ÔºåÂ∞ùËØïËøáÊç¢ÁºñËØëÂô®ÔºåÊú™ËÉΩËß£ÂÜ≥ÔºåÊãúÊâòÁªôÁúãÁúãÔºåÂÖàË∞¢ËøáÔºÅ
‰ª£Á†ÅÂ¶Ç‰∏ãÔºö

![ail okn 15t 6 pkg petfn](https://cloud.githubusercontent.com/assets/19303343/15182773/c1d0a396-17c0-11e6-8d03-1d1e178586ec.png)
"
Â≠óÂÖ∏Êñ∞Â¢û‰∏ÄÂàóÂ±ûÊÄßÔºå‰ª£Á†Å‰øÆÊîπÈáèÊòØ‰∏çÊòØÂæàÂ§ß„ÄÇ,"Â≠óÂÖ∏Áé∞Âú®ÁöÑÊ†ºÂºèÊòØ‚ÄúËØç ËØçÊÄß ËØçÈ¢ë‚ÄùÔºåÊàëÊÉ≥Êîπ‰∏∫‚ÄúËØç ËØçÊÄß ËØçÈ¢ë ‰∏ì‰∏öËØç‚ÄùÔºå‰∏æ‰æãËØ¥ÊòéÔºö
‚ÄúÊü• v 1000‚Äù Êîπ‰∏∫ ‚ÄúÊü• v 1000 Êü•ËØ¢‚ÄùÔºå
"
ÁπÅ‰ΩìÂàÜËØç,"ÂΩìÂâçÂàÜËØçËØçÂÖ∏ÂíåÊ®°ÂûãÂ•ΩÂÉèÈÉΩÊòØÂü∫‰∫éÁÆÄ‰Ωì‰∏≠ÊñáÁöÑÔºåÊïàÊûú‰∏çÈîô„ÄÇ‰ΩÜÊòØÂØπ‰∫éÁπÅ‰Ωì‰∏≠Êñá,‰ºº‰πéÊïàÊûú‰∏ç‰Ω≥ÔºåÂç≥‰ΩøÂä†ÂÖ•‰∏Ä‰∫õËØçËØ≠Ôºå‰πüËøòÊòØ‰ºöË¢´ÂΩìÂÅöÁÆÄ‰Ωì‰∏≠ÊñáÂá∫Êù•„ÄÇ

```
    CustomDictionary.add(""Êç∑ÈÅãÁ´ô"");
    CustomDictionary.add(""Ëá∫Âåó"");

""Ëá∫ÂåóÂ§ßÁúæÊç∑ÈÅãËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏""
[Ëá∫/n, ÂåóÂ§ß/j, Áúæ/nz, Êç∑/j, ÈÅã/n, ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏/nis]
```

‰ΩÜÊòØÂ¶ÇÊûúÊääÊñáÊú¨ËΩ¨Âåñ‰∏∫ÁÆÄ‰ΩìÔºåÂàÜËØçÂ∞±ËõÆ‰∏çÈîôÁöÑ„ÄÇ

```
input = HanLP.convertToSimplifiedChinese(input);
[Âè∞Âåó/ns, Â§ß‰ºó/n, ËΩªËΩ®/n, ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏/nis]
```
1. ÊòØÂê¶ÊúâÊØîËæÉÂ•ΩÁöÑÁπÅ‰ΩìËØçÂÖ∏Êé®Ëçê
2. ËÉΩÂê¶ÊîØÊåÅÁπÅ‰Ωì‰∏≠ÊñáÂàÜËØçÔºàÂàÜËØçÊó∂ÂÄôÂèØ‰ª•ÊúâÁπÅ‰ΩìÊ®°ÂûãÂíåÁÆÄ‰ΩìÊ®°ÂûãÔºâ
3. ÊòØÂê¶ÂèØ‰ª•Ê†πÊçÆËΩ¨Âåñ‰∏∫ÁÆÄ‰ΩìÁöÑÁªìÊûúÊù•ÂàÜÂéüÊù•ÁöÑÂè•Â≠êÔºàÊ†πÊçÆÁÆÄ‰Ωì‰∏≠ÊñáÂàÜËØçÁöÑ‰ΩçÁΩÆÔºâ
"
"‰æãÂ≠ê‰∏≠""Âè™ÊúâËá™‰ø°ÁöÑÁ®ãÂ∫èÂëòÊâçËÉΩÊääÊè°Êú™Êù•""","‰æãÂ≠ê‰∏≠""Âè™ÊúâËá™‰ø°ÁöÑÁ®ãÂ∫èÂëòÊâçËÉΩÊääÊè°Êú™Êù•""ÔºåÁî®MainPartExtractorÊèêÂèñ‰∏ªË∞ìÂÆæÔºåÁªìÊûúÊòØÁ®ãÂ∫èÂëòÊâçËÉΩÊääÊè°Êú™Êù•ÔºåÊ≠£Á°ÆÁöÑÁªìÊûúÂ∫îËØ•ÊòØÔºöÁ®ãÂ∫èÂëòÊääÊè°Êú™Êù•
ÊòØ‰∏çÊòØ ""ÊâçËÉΩ"" Ëøô‰∏™ÂÅúÁî®ËØçÊ≤°ÊúâÂêØÁî®ÔºåÂ¶ÇÊûúÊòØÔºåÈúÄË¶ÅÊÄé‰πàÂêØÁî®„ÄÇÂ¶ÇÊûú‰∏çÊòØÔºåËØ∑ÈóÆÈóÆÈ¢òÂá∫Âú®Âì™Ôºü
"
Âº±ÈóÆÂ¶Ç‰ΩïÊèêÂèñÂõ∫ÂÆöÊ†ºÂºèÁöÑÊñáÊú¨Ôºü,"‰Ω†Â•Ωhankcs, Â¶ÇÊó•Êúü[2016/m, Âπ¥/qt, 4/m, Êúà/n, 29/m, Êó•/b], Â¶Ç‰ΩïËÆæÂÆö[m, qt, m, n ,m , b]ËßÑÂàôÊù•ÊèêÂèñËøôÊ†∑ÁöÑÊó•ÊúüÊ†ºÂºèÊñáÊú¨„ÄÇÂ∑•ÂÖ∑ÈáåÊúâÁé∞ÊàêÁöÑ‰πàÔºüÂõ†‰∏∫Áî®pythonÔºåÁúãjava‰∏çÂ§™ÁÜüÁªÉ„ÄÇ
bow
"
ÁõÆÂâçhanlpËÉΩÂØπÊé•ES‰πàÔºü,"<(Ôø£Ô∏∂Ôø£)>ÔºÅ
"
Ë∂ÖËµûÂ≠¶‰π†‰∏≠,"Ë∂ÖËµûÂ≠¶‰π†‰∏≠
"
Âè•Ê≥ïÂàÜÊûêÈóÆÈ¢ò,"‰Ω†Â•ΩÔºåËØ∑ÈóÆÁî®HanLpËøõË°åÂè•Ê≥ïÂàÜÊûêÔºåÊòØ‰∏çËøõË°åÂè•Ê≥ïÊàêÂàÜÊ†áËØÜÁöÑÂêóÔºü
"
ËÉΩÂê¶Êèê‰æõ1.ÂØπÁ≥ªÁªüÊû∂ÊûÑÂÅö‰∏Ä‰∏™ËØ¶ÁªÜÁöÑËÆ≤Ëß£ 2.‰ª£Á†ÅÊñá‰ª∂ÂíåÁÆóÊ≥ïÂéüÁêÜÂØπÂ∫îÂÖ≥Á≥ª,
CRFËØÜÂà´ÁöÑ‰∏Ä‰∏™bug,"‰ª£Á†ÅÔºö
        Segment segment = new CRFSegment();
        segment.enableCustomDictionary(false);// ÂºÄÂêØËá™ÂÆö‰πâËØçÂÖ∏
        segment.enablePartOfSpeechTagging(true);
        List<Term> termList = segment.seg(""Êõ¥Â§öÈááË¥≠"");
        System.out.println(termList);
        for (Term term : termList) {
            if (term.nature == null) {
                System.out.println(""ËØÜÂà´Âà∞Êñ∞ËØçÔºö"" + term.word);
            }
        }

ÁªìÊûúÔºö
[Êõ¥Â§ö/ad, ÈááË¥≠/null]
ËØÜÂà´Âà∞Êñ∞ËØçÔºöÈááË¥≠
‰ΩÜÊòØÈááË¥≠Ëøô‰∏™ËØçÂú®ËØçÂ∫ì‰∏≠ÊòØÂ≠òÂú®ÁöÑÔºåÂπ∂‰∏çÊòØÊñ∞ËØç„ÄÇ
"
python Ë∞ÉÁî®Âè•Ê≥ïÂàÜÊûêÔºå ÊÄé‰πàËé∑ÂæóÂØπÂ∫îÁöÑÁªìÊûúÔºü,"‰ΩøÁî®pythonË∞ÉÁî®HanLPÔºåÁî®‰∏ãÈù¢ÁöÑ‰ª£Á†ÅË∞ÉÁî®ÂæóÂà∞‰∫ÜstÔºå ËØ∑ÈóÆÊÄé‰πàËé∑ÂæóÈáåÈù¢ÁöÑÂÜÖÂÆπÂë¢Ôºü ‰∏∫‰ªÄ‰πàÁî®for word in stÁöÑÊñπÂºèËé∑Âèñ‰∏ç‰∫Ü„ÄÇ pythonÂ∫îËØ•Â¶Ç‰ΩïË∞ÉÁî®Ôºü Êúâ‰∫∫ÂèØ‰ª•ÂÜô‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰æãÂ≠êÂêóÔºåÂ§öË∞¢ÔºÅ ÔºàÁ±ª‰ººÂÆòÁΩëÁöÑ‰æãÂ≠êÔºöword.LEMMA, word.DEPREL, word.HEAD.LEMMAÔºâ

`st = HanLP.parseDependency(u""ÂæêÂÖàÁîüËøòÂÖ∑‰ΩìÂ∏ÆÂä©‰ªñÁ°ÆÂÆö‰∫ÜÊääÁîªÈõÑÈπ∞„ÄÅÊùæÈº†ÂíåÈ∫ªÈõÄ‰Ωú‰∏∫‰∏ªÊîªÁõÆÊ†á„ÄÇ"")
`
"
Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØçÊï∞ÈáèÊàñÊòØÊ°£Ê°àÂ§ßÂ∞èÊòØÂê¶ÊúâÈôêÂà∂Ôºü,"hankcs,ÊÇ®Â•ΩÔºÅ

   ÊÉ≥ËØ∑ÊïôÊÇ®Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑËØçÊï∞ÈáèÊàñÊòØÊ°£Ê°àÂ§ßÂ∞èÊòØÂê¶ÊúâÈôêÂà∂ÔºüË∂ÖËøáÁôæ‰∏á‰ª•‰∏äÁöÑËØçÂÖ∏ÊòØÂê¶ÂèØ‰ª•ËΩΩÂÖ•ÔºüÂÖàÂâç‰ª•solr5.2Êê≠ÈÖçËΩΩÂÖ•200‰∏á‰ª•‰∏äÁöÑËØçÂÖ∏ÔºåÁ≥ªÁªü‰ºöÂ¥©Ê∫ÉÔºåÊÉ≥ËØ∑ÊÇ®ÂçèÂä©ÔºåË∞¢Ë∞¢

Vincent
"
CRFÊñ∞ËØçËØÜÂà´,"Ëøô‰∏™ÊñπÊ≥ï‰∏ÄÊó¶ÂºÄÂêØËá™ÂÆö‰πâËØçÂÖ∏Â∞±Êó†Ê≥ïËØÜÂà´Êñ∞ËØç‰∫Ü‰πàÔºü
Segment segment = new CRFSegment();
segment.enableCustomDictionary(true);// ÂºÄÂêØËá™ÂÆö‰πâËØçÂÖ∏
ÊâÄ‰ª•enableCustomDictionary()ÂøÖÈ°ªËÆæÁΩÆ‰∏∫false?
ÊÑüËßâËøôÊ†∑ÊúâÁÇπ‰∏çÂ§™ÂêàÁêÜÂïä„ÄÇ
ÊàëËøòË¶ÅÂêêÊßΩÔºÅÔºÅÔºÅÔºÅ
ÊÑüËßâÂØπÊï∞Â≠óÂíåÂ≠óÊØçÁöÑËØÜÂà´ÊÑè‰πâ‰∏çÂ§ß„ÄÇÂ¶ÇÔºö
public static void main(String[] args) {
        Segment segment = new CRFSegment();
        segment.enableCustomDictionary(false);// ÂºÄÂêØËá™ÂÆö‰πâËØçÂÖ∏
        segment.enablePartOfSpeechTagging(true);
        List<Term> termList = segment.seg(""    ÈòøËé´Ë•øÊûóÊòØ""
                + ""TJGP-2015-ZP-5722,""
                + ""‰πêËßÜË∂ÖÁ∫ßÊâãÊú∫ËÉΩÂê¶ÊâøËΩΩË¥æÂ∏ÉÊñØÁöÑÁîüÊÄÅÊ¢¶ ""
                + ""ÂÅú‰∏öÊï¥È°øÂÅú‰∏ö"");
        System.out.println(termList);
        for (Term term : termList) {
            if (term.nature == null) {
                System.out.println(""ËØÜÂà´Âà∞Êñ∞ËØçÔºö"" + term.word);
            }
        }

```
}
```

ÁªìÊûúÔºö
ËØÜÂà´Âà∞Êñ∞ËØçÔºö  
ËØÜÂà´Âà∞Êñ∞ËØçÔºöÈòøËé´Ë•øÊûó
ËØÜÂà´Âà∞Êñ∞ËØçÔºöTJGP
ËØÜÂà´Âà∞Êñ∞ËØçÔºö-
ËØÜÂà´Âà∞Êñ∞ËØçÔºö2015
ËØÜÂà´Âà∞Êñ∞ËØçÔºö-
ËØÜÂà´Âà∞Êñ∞ËØçÔºöZP
ËØÜÂà´Âà∞Êñ∞ËØçÔºö-
ËØÜÂà´Âà∞Êñ∞ËØçÔºö5722
ËØÜÂà´Âà∞Êñ∞ËØçÔºö,
ËØÜÂà´Âà∞Êñ∞ËØçÔºöË¥æÂ∏ÉÊñØ
ËØÜÂà´Âà∞Êñ∞ËØçÔºö 
ËØÜÂà´Âà∞Êñ∞ËØçÔºöÂÅú‰∏ö

Ëøô‰∫õÊ±âÂ≠óÁöÑËØçÔºåÊàëËßâÂæóËøòË°å„ÄÇ‰ΩÜÊòØËøôÈáåÂêÑÁßçÈùûÊ±âÂ≠óÈÉΩ‰ºöË¢´ËØÜÂà´ÊàêÊñ∞ËØç„ÄÇÊàëËßâÂæóËøô‰∏™ÈóÆÈ¢òÈÄ†ÊàêËøô‰∏™ÊñπÊ≥ïÁöÑÂèØÁî®ÊÄß„ÄÇÂ§ßÂ§ßÈôç‰ΩéÂïä„ÄÇ

ÊàëÁõÆÂâçÊÉ≥ÂÅö‰∏Ä‰∏™ÁªôÂæàÂ§öÊñ∞ÁöÑÊñáÁ´†ÔºåËØÜÂà´Âá∫ÊñáÁ´†ÈáåÈù¢Êñ∞ÁöÑËØç„ÄÇ
Áé∞ÊúâÁöÑÊñπÊ≥ïÔºö
‰ΩøÁî®ËøáÁöÑÊñπÊ≥ï„ÄÇÂÖàÂàÜËØçÔºåÂ¶ÇÊûúÊúâ‰∏Ä‰∏™‰∏Ä‰∏™ÁöÑÂ≠óÊ≤°ÊúâÊàêËØç„ÄÇÂ¶ÇÔºöÊàëÁöÑÂêçÂ≠ó-ÊùéÂÅ•Âçö‰∏ç‰ºöÊàê‰ªª‰ΩïËØç„ÄÇÂ∞±Êää‰ªñ‰ª¨ÁªÑÂêàËµ∑Êù•„ÄÇÁî®HMMÔºåÊàñËÄÖCRFËØÜÂà´‰∏Ä‰∏ãÊòØ‰∏çÊòØÊñ∞ËØç„ÄÇ
‰ºòÁÇπÔºöËÉΩÂ§üÂâîÈô§‰∏äÈù¢[TJGP,2015]ËøôÊ†∑ÁöÑËØçÔºåÂè¨ÂõûÂ∫îËØ•ÊòØÊØîËæÉÈ´òÁöÑ„ÄÇ
Áº∫ÁÇπÔºöÂáÜÁ°ÆÁéá‰Ωé„ÄÇÂ§ßÈáèÁöÑ(ÂÆû‰ΩìÂêçÔºåÂ∞§ÂÖ∂ÊòØ‰∫∫ÂêçÔºå‰ºöÊ∑∑Âú®ÈáåÈù¢)‰ΩøÂæóËøòÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•„ÄÇ
ÂàõÊñ∞ÊñπÊ≥ïÔºö
Áî®‰∏äÈù¢ÁöÑÊñπÊ≥ïÂíåhanlpÁªìÂêàÔºö
ÊØîÂ¶Ç:ÊùéÂÅ•ÂçöÂú®hanlp‰∏≠‰ºöË¢´ËØÜÂà´‰∏∫‰∫∫ÂêçÔºåËøôÊ†∑ÁöÑËØçÂ∞±‰∏çË¶Å‰∫Ü„ÄÇ
Ê•º‰∏ªËßâÂæóÊàëËøô‰∏™ÂàõÊÑèÂíãÊ†∑ÔºüÂõ†‰∏∫ÊàëÂØπhanlp‰∏çÊòØÂ§™‰∫ÜËß£„ÄÇÊâÄ‰ª•Â∏åÊúõÊ•º‰∏ªÁúã‰∏ÄÁúãËøôÊ†∑ÂÅöÁöÑÂèØË°åÊÄß„ÄÇ
‰∏™‰∫∫ËßâÂæóhanlpÁöÑCRFÊñπÊ≥ïÂæàÂ•ΩÔºå‰ΩÜÊòØÂõ†‰∏∫[TJGP,2015]ËøôÊ†∑ÁöÑÂéüÂõ†‰ΩøÂæóÊó†Ê≥ïÁúüÊ≠£ÁöÑÊäïÂÖ•‰ΩøÁî®ÔºåÂÆûÂú®ÊòØÂçÅÂàÜÂèØÊÉú„ÄÇ
"
ÂØπÁÆÄ‰ΩìÂ≠óË∞ÉÁî®ËΩ¨‰∏∫ÁÆÄ‰ΩìÊñπÊ≥ï  ËæìÂá∫ÊúâÈóÆÈ¢ò ,"HanLP.convertToSimplifiedChinese(""Êõ¥Âá∂Êõ¥Áåõ"");   Êú¨Êù•ÊòØÊÉ≥Â∞ÜËøô‰∏™ÁÆÄ‰ΩìËØçËΩ¨Âåñ‰∏∫ÁÆÄ‰Ωì  ÁªìÊûúË¢´ËøîÂõû‰∏∫‚ÄúÊõ¥Âá∂Êõ¥Âãê‚Äù
"
"Âú®nr.txt‰∏≠Ê∑ªÂä†‰∫Ü""Êâç‰ºöÊúâ A 1""‰∏∫‰ªÄ‰πà‰ªñËøòË¢´ËØÜÂà´‰∏∫nr","ËØÜÂà´ÁöÑÊñáÁ´†Â¶Ç‰∏ãÔºö

„ÄÄ„ÄÄÂéüÊ†áÈ¢òÔºö‚ÄúÂéüÊù•‰∏•ËÇÉÁöÑËØ¥ÊïôËøòÂèØ‰ª•Ëøô‰πàÊé•Âú∞Ê∞î‚Äù
„ÄÄ„ÄÄÈöèÁùÄÁΩëÁªúÁöÑÊÄ•ÈÄüÂèëÂ±ïÔºå‰∫∫‰ª¨ÂèëÁé∞ÔºåÁΩëÁªúÊñáÂåñ‰∏ç‰ªÖÊòØÂΩì‰ª£ÊñáÂåñÁöÑÈáçË¶ÅÁªÑÊàêÔºåÊõ¥ÊòØÂΩì‰ª£ÊñáÂåñÁöÑ‰∏ÄÂ§ß‰∫ÆÁÇπÔºåÂÆÉËøòËÉΩÂæàÂ•ΩÂú∞Âê∏Êî∂„ÄÅËûçÂêà‰º†ÁªüÊñáÂåñÔºå‰ªéËÄåËΩ¨ÂåñÂá∫Áã¨ÁâπÁöÑ„ÄÅÂèóÂà∞ÁΩëÁªúÊó∂‰ª£ÂπøÂ§ßÁæ§‰ºóÂñúÁà±ÁöÑÊñ∞ÊñáÂåñÂΩ¢Âºè„ÄÅÊñáÂåñ‰∫ßÂìÅ„ÄÇ
„ÄÄ„ÄÄÈ°∫Â∫îÊó∂‰ª£ÊΩÆÊµÅÔºåÊâçËÉΩÊúâÊâÄ‰Ωú‰∏∫„ÄÇÂåó‰∫¨Â∏ÇÂ§öÂπ¥ÁßØÊûÅÊåáÂØº„ÄÅÈºìÂä±Â±ûÂú∞‰∏ªË¶ÅÁΩëÁ´ô‰º†Êâø‰ºòÁßÄ‰º†ÁªüÊñáÂåñÔºåÊâìÈÄ†Â§öÂΩ©ÁΩëÁªúÊñáÂåñÔºåÂπ∂ÂèëÊå•ÁªÑÁªá‰ΩúÁî®ÔºåÂçèË∞ÉÂ±ûÂú∞‰∏ªË¶ÅÁΩëÁ´ôËÅîÊâã‰∏æÂäû‰∏∞ÂØåÁöÑÁΩëÁªúÊñáÂåñÊ¥ªÂä®„ÄÇÂ∏ÇÁΩë‰ø°ÂäûÁõ∏ÂÖ≥Ë¥üË¥£‰∫∫Ë°®Á§∫ÔºåÂª∫ËÆæÁΩëÁªúÊñáÂåñÔºåÂøÖÈ°ªÂùöÊåÅÊ≠£Á°ÆÂØºÂêëÔºåÁßØÊûÅÂÆ£‰º†Ê≠£ËÉΩÈáèÔºå‰º†Êí≠ÂÖöÁöÑ‰∏ªÂº†„ÄÇ‰ΩÜÊòØÔºåÂÖöÁöÑÁêÜËÆ∫Ë∑ØÁ∫ø„ÄÅÊñπÈíàÊîøÁ≠ñÂæÄÂæÄÂ∫ÑÈáç‰∏•ËÇÉÔºåÂÆèËßÇÊÄß„ÄÅÁêÜËÆ∫ÊÄßÂº∫ÔºåÂÖ∂Â§ß‰ºóÂåñ„ÄÅÊôÆÂèäÊÄß‰º†Êí≠ÊòØ‰∏™ÈöæÁÇπ„ÄÇËøëÂπ¥Êù•ÔºåÂåó‰∫¨ÁΩë‰ø°Âäû‰∏éÂ±ûÂú∞‰∏ªË¶ÅÁΩëÁ´ôÂÖÖÂàÜÊ≤üÈÄö„ÄÅÂêà‰ΩúÔºåÂÖàÈÄöËøá‰∏ìÂÆ∂ËÆ≤Â∫ß„ÄÅÁºñËæëÂüπËÆ≠Á≠âÊñπÂºèÔºåÂ∏ÆÂä©ÁΩëÁ´ôÂ∞ÜÂÖöÁöÑÊîøÁ≠ñ‰∏ªÂº†‚ÄúÊèâÁ¢é‚Äù„ÄÅ‚ÄúÂêÉÈÄè‚ÄùÔºåÂÜçÈÄöËøáÂÖ±ÂêåÁ≠ñÂàíÔºåÁî®ÁΩëÁªúËØ≠Ë®Ä„ÄÅÁΩëÁªúË°®ËææÊñπÂºèËøõË°åÊ¢≥ÁêÜ„ÄÅÂëàÁé∞Ôºå‰º†Êí≠ÊïàÊûúÊòéÊòæ„ÄÇ
„ÄÄ„ÄÄËøôÊ†∑ÁöÑ‰æãÂ≠êÊØîÊØîÁöÜÊòØÔºå2013Âπ¥ÔºåÁî±Êñ∞Êµ™ÂæÆÂçöÂÄ°ËÆÆÁöÑ‚ÄúÂÖâÁõòË°åÂä®‚ÄùÊàê‰∏∫ÂΩìÂπ¥ÁöÑÂçÅÂ§ßÊñ∞ÈóªÁÉ≠ËØç„ÄÇ‰ªéËøôÊ¨°Ë°åÂä®ÁöÑÊìç‰Ωú‰∏≠ÂèØ‰ª•ÁúãÂá∫ÁßØÊûÅËøêÁî®Êñ∞ÁöÑÊñπÂºèÔºåÂºÄÂèëÂá∫ÂèóÂà∞ÁΩëÁªúÊó∂‰ª£Â§ß‰ºóÂñúÁà±ÁöÑÊñáÂåñ‰∫ßÂìÅÁöÑÈáçË¶ÅÊÑè‰πâ„ÄÇÂΩìÂàùÊñ∞Êµ™ÂæÆÂçöÊé•Âà∞ÁöÑÂÆ£‰º†‰ªªÂä°ÊòØ‚ÄúÂÄ°ÂØºËäÇÁ∫¶ÂûãÁ§æ‰ºö‚Äù„ÄÇËÄÉËôëÂà∞‚ÄúËäÇÁ∫¶‚Äù‰∫åÂ≠óÂ§™ÂÆΩÊ≥õÔºå‰∏ç‰æøÊìç‰ΩúÔºåÊñ∞Êµ™ÂæÆÂçöÁöÑÁºñËæë‰ª¨ÁªèËøáÂ§¥ËÑëÈ£éÊö¥ÔºåÊèêÂá∫‰∫Ü‚ÄúÂÖâÁõòË°åÂä®‚ÄùËøô‰∏™ÂΩ¢Ë±°„ÄÅÁîüÂä®ÂèàË¥¥Âú∞Ê∞îÁöÑÂè£Âè∑„ÄÇ‚ÄúÂÖâÁõòË°åÂä®‚Äù‰∏ÄÁªèÊé®Âá∫ËøÖÈÄüÂæóÂà∞ÂπøÊ≥õËÆ§ÂèØ„ÄÇÁΩëÊ∞ëÊàê‰∏∫‚ÄúËá™Êù•Ê∞¥‚ÄùÔºåËá™ÂèëÂëºÂêÅ‰∫≤‰∫∫ÊúãÂèãË∑µË°å‚ÄúÂÖâÁõòË°åÂä®‚ÄùÔºåÂΩ¢Êàê‰∫ÜËâØÂ•ΩÁöÑÁ§æ‰ºöÊïàÂ∫î„ÄÇ
„ÄÄ„ÄÄ2014Âπ¥Á§æ‰ºö‰∏ª‰πâÊ†∏ÂøÉ‰ª∑ÂÄºËßÇÁöÑÂÆ£‰º†Êä•ÈÅì‰∏≠ÔºåÂ±ûÂú∞ÁΩëÁ´ôÈÄöËøáÊ∑±ÂÖ•Â≠¶‰π†„ÄÅÁêÜËß£ÔºåÂ∞ÜÁ§æ‰ºö‰∏ª‰πâÊ†∏ÂøÉ‰ª∑ÂÄºËßÇÈÄöËøáÁΩëÁªúËØ≠Ë®ÄËøõË°åËß£ËØª„ÄÇÊêúÁãêÁΩëÁöÑ‰∏ìÈ¢ò‚Äú‰Ω†ËØªÂæóÊáÇÁöÑÁ§æ‰ºö‰∏ª‰πâÊ†∏ÂøÉ‰ª∑ÂÄºËßÇ‚ÄùÔºåÁî®‰øèÁöÆÂπΩÈªòÁöÑÂä®Êº´ÂΩ¢ÂºèÔºå‰ªéÁΩëÊ∞ëÁÜüÊÇâÁöÑÁîüÊ¥ªËßíÂ∫¶ÂàáÂÖ•ÔºåÂ±ïÁ§∫Á§æ‰ºö‰∏ª‰πâÊ†∏ÂøÉ‰ª∑ÂÄºËßÇÂú®ÂõΩÂÆ∂„ÄÅÁ§æ‰ºöÂíåÂÆ∂Â∫≠‰∏â‰∏™Â±ÇÈù¢ÁöÑ‰ª∑ÂÄºÁõÆÊ†áÂíåÂáÜÂàôÔºå‰ΩøÂÖ∂ÈÄÇÂêà‰∏çÂêåÁöÑ‰∫∫Áæ§ÂéªÊé•Êî∂„ÄÅÊ∂àÂåñÔºåÈÇ£‰∫õÂæÄÊó•Áúã‰∏äÂéªÂ∞±È´òÂ§ß‰∏äÁöÑÁêÜËÆ∫ÔºåÊãâËøë‰∫ÜÂíåÊôÆÈÄö‰∫∫ÁöÑË∑ùÁ¶ª„ÄÇ
„ÄÄ„ÄÄ2016Âπ¥ÂÖ®ÂõΩ‚Äú‰∏§‰ºö‚ÄùÊä•ÈÅì‰∏≠ÔºåÊñ∞Êµ™ÁΩëÂ∞ÜH5‰∏éÈü≥È¢ëÂµåÂÖ•ÊäÄÊúØÁªìÂêàÔºåÊé®Âá∫‚ÄúÂê¨Ôºå‰π†ËøëÂπ≥ËØ¥‚Äù„ÄÅ‚ÄúÂº∫Âì•Ëøô‰∏ÄÂπ¥ÈÉΩÊâπËØÑ‰∫ÜË∞Å‚Äù„ÄÇ‰∏≠ÂçéÁΩëÊèêÁÇºÊîøÂ∫úÂ∑•‰ΩúÊä•Âëä‰∏≠ÁöÑÊ∞ëÁîüÁ∫¢Âà©ÔºåÁî®‚ÄúH5+ÂæÆ‰ø°Á∫¢ÂåÖ‚ÄùÁöÑÂΩ¢ÂºèÊé®Âá∫‚Äú‰Ω†Êúâ‰∏Ä‰∏™Êù•Ëá™ÊùéÂÖãÂº∫ÁöÑÁ∫¢ÂåÖ‚Äù„ÄÇÊêúÁãêÊâìÈÄ†ÂÖ®ÊôØÁ≠ñÂàí‚ÄúË∑üÁùÄ‰ª£Ë°®‰∏ä‰∏§‰ºö‚ÄùÔºå‰º†ÈÄí‰ª£Ë°®ÂßîÂëòÂ£∞Èü≥„ÄÇÊñ∞Êµ™ÂæÆÂçö‰∏éVRÁªìÂêàÁöÑ‰∏§‰ºöÂæÆÁõ¥Êí≠ÔºåÂ∏¶È¢ÜÁΩëÊ∞ëËøëË∑ùÁ¶ªÁúã‰∏§‰ºö„ÄÇËøô‰∫õÁ≤æÂΩ©ÁöÑ‰∫ßÂìÅÂ§ßÂ§öÂú®Á§æ‰∫§Â™í‰ΩìÂΩ¢Êàê‰∫Ü‚ÄúÂà∑Â±è‚ÄùÁöÑÁé∞Ë±°„ÄÇ
„ÄÄ„ÄÄÁõ∏ÂÖ≥Ë¥üË¥£‰∫∫‰ªãÁªçÔºåÂú®‰∫íËÅîÁΩë‰∏ä‰º†Êí≠ÂÖöÁöÑÂ£∞Èü≥ÔºåÂ∫îËØ•ÂΩìÂ•ΩËΩ¨Êç¢Âô®„ÄÇË¶ÅÁ¥ßË∑üÁΩëÁªúÂèëÂ±ïÔºåÂãá‰∫éÂàõÊñ∞„ÄÅÊ≥®ÈáçÂÆûÊïàÔºåÂø´ÈÄüÂ≠¶‰π†Êñ∞Â™í‰ΩìÊâãÊÆµÔºåÈºìÂä±ÁΩëÁ´ôÊîæÊâãÂπ≤ÔºåËøêÁî®ÁΩëÁªúÊÄùÁª¥ÔºåÈááÁî®ÁΩëÁªúËØ≠Ë®ÄÔºåË°®ËææÂè£ËØ≠Âåñ„ÄÅÊé•Âú∞Ê∞îÔºåÂ∑ßÂ¶ôËûçÂêàÂõæÁâá„ÄÅÂõæË°®„ÄÅÊº´Áîª„ÄÅÂä®Êº´„ÄÅËßÜÈ¢ë„ÄÅH5„ÄÅÈü≥È¢ëÁ≠âÊñ∞Â™í‰ΩìÊâãÊÆµÔºåÁ°Æ‰øù‰º†Êí≠ÂÆûÊïàÔºåÊâìÈÄ†ÂÖÖÊª°Ê¥ªÂäõÁöÑÈº†Ê†áÊñáÂåñ„ÄÅÁßªÂä®ÊñáÂåñÔºåÊâçËÉΩÂú®ÁΩëÊ∞ëÁîüÊ¥ªÂ∑•‰Ωú‰∏≠Ëµ∑Âà∞ÂÆûÂÆûÂú®Âú®ÁöÑÊåáÂØº‰ΩúÁî®„ÄÇÊúâÁΩëÊ∞ëÁïôË®ÄË°®Á§∫ÔºåÂéü‰ª•‰∏∫‰∏•ËÇÉÁöÑËØ¥ÊïôÔºåËøòÂèØ‰ª•Ëøô‰πàÊé•Âú∞Ê∞î„ÄÇÊú¨ÁâàÊñá/Êú¨Êä•ËÆ∞ËÄÖ¬†ÊùéÊ≥Ω‰ºü
„ÄÄ„ÄÄÔºàÂåó‰∫¨ÈùíÂπ¥Êä•Ôºâ
Ë¥£‰ªªÁºñËæëÔºöËµµÂÆ∂Êòé SN146
‰∫∫ÊÄªÂú®ËøΩÊ±ÇÂúÜÊª°„ÄÇ‰∫ãÊÉÖË¶ÅÂÅöÂæóÂúÜÊª°Ôºå‰∫§ÂèãË¶Å‰∫§ÂæóÂúÜÊª°ÔºåÊñáÂ≠óË¶ÅÂÜôÂæóÂúÜÊª°„ÄÇÂè™ÊúâÂú®ÊÑüËßâÂà∞ÂúÜÊª°‰πãÊó∂ÔºåÊâç‰ºöÊúâÁúüÊ≠£ÁöÑÂñú‰πê„ÄÇ
ËøëÊó•ÔºåÁæéÂõΩÊÄªÁªüÂ••Â∑¥È©¨ÊäµËææÊ≤ôÁâπÂºÄÂêØ‰ªñÁöÑ‚ÄúÂëäÂà´‰πãÊóÖ‚ÄùÔºåËÄåÁªìÊûúÂç¥ÊòØÊó†ÊØîÂÜ∑Ê∏Ö„ÄÇ‰∏é‰ª•ÂæÄÊ≤ôÁâπÂõΩÁéã‰∫≤Ëµ¥Êú∫Âú∫‚ÄúËøéÊù•ÈÄÅÂæÄ‚ÄùÁõ∏ÊØîÔºåÂ••Â∑¥È©¨Ê≠§Ê¨°Ê≤ôÁâπ‰πãË°åÂèØË∞ì‚ÄúÂØíÈÖ∏Ëá≥ÊûÅ‚Äù„ÄÇ
ÁªôÈ¢ÜÂØºÂÜôËÆ≤ËØùÁ®øÁöÑÊú∫‰ºöÔºåÊùêÊñôÁãóÁöÑ‰∏ÄÁîüÂè™Êúâ‰∏§Ê¨°Êú∫‰ºöÔºåÁ¨¨‰∏ÄÊ≤°ÂÜôÂ•ΩÁöÑÊó∂ÂÄôÔºåÈ¢ÜÂØº‰ºöÂæàÂÆ¢Ê∞îÂú∞Êää‰Ω†Âè´Âà∞ÂäûÂÖ¨ÂÆ§Êù•ÔºåÂíåÈ¢úÊÇ¶Ëâ≤Âú∞ÂØπ‰Ω†ËØ¥Ôºö‚ÄúÂ∞èÂêåÂøóÔºåËøôÁØáÊñáÁ´†ËøòÊúâÂæàÂ§öËÆ∏Â§öÈúÄË¶ÅÊñüÈÖåÁöÑÂú∞ÊñπÔºå‰Ω†ÂÜçÂõûÂéªÊîπÊîπ„ÄÇ‚Äù
‰∏≠ÂõΩÂ§ßÈôÜÊàøÂú∞‰∫ßÂ∏ÇÂú∫ÂêØÂä®‰∫é‰∏ä‰∏ñÁ∫™90Âπ¥‰ª£‰∏≠ÂêéÊúü„ÄÇÂ¶ÇÊûú‰ª•ÊúÄÈ´òËΩ¨ËÆ©ÊúüÈôê70Âπ¥ËÆ°ÁÆóÔºåËæÉÊó©‰∏ÄÊâπÂïÜÂìÅÊàøÔºåÂÆûÈôÖÂâ©‰∏ã50Âπ¥Â∑¶Âè≥„ÄÇ
Copyright 
1996-2016 SINA Corporation, All Rights Reserved
Êñ∞Êµ™ÂÖ¨Âè∏ 
"
‰∏Ä‰∏™bugÂ¶ÇÊûúÊääÊú∫ÊûÑÂêçÂíåÊï∞Â≠óËØÜÂà´ÂêåÊó∂ÊâìÂºÄ,"ÂàáÂàÜ‰∏ãÈù¢ËøôÂè•ËØùÁöÑÊó∂ÂÄô‰ºöÂá∫exception
‰ª•ÊØèÂè∞Á∫¶200ÂÖÉÁöÑ‰ª∑Ê†ºÈÄÅÂà∞ËãπÊûúÂîÆÂêéÁª¥‰øÆ‰∏≠ÂøÉÊç¢Êñ∞Êú∫ÔºàËãπÊûúÁöÑ‰øù‰øÆÂü∫Êú¨ÊòØÂÖçË¥πÊç¢Êñ∞Êú∫Ôºâ
"
ËØ∑ÈóÆÊòØÂê¶Â∞ÜÊù•‰ºöÂä†ÂÖ•ÊÉÖÊÑüÂàÜÊûêÊé•Âè£,"1. Â∞±ÊòØÂØπÊñáÊú¨ËøõË°å‰∏Ä‰∏™ÊÉÖÊÑüÂÄºÁöÑÈáèÂåñÂàÜÊûêÔºåËæìÂá∫‰∏Ä‰∏™‰ª£Ë°®Ê≠£Ë¥üÈù¢ÊÉÖÊÑüÁöÑÊï∞ÂÄºÔºåÊØîÂ¶Ç0ÊòØÁªùÂØπË¥üÈù¢Ôºå1ÊòØÁªùÂØπÊ≠£Èù¢ÔºåËæìÂá∫ÁöÑÁªìÊûúÂ∞±Âú®0~1‰πãÈó¥ËøôÁßçÁ±ª‰ººÁöÑÂäüËÉΩ
"
ÂèëÁé∞Êñ∞ËØçÊ±á,"Â¶ÇÈ¢òÔºåÁªô‰∏ÄÂÆöÁöÑÂæÆÂçöËØ≠ÊñôÔºåËÉΩÂê¶ÊèêÂèñÂá∫Âì™‰∫õËØçÊ±áÊòØÊñ∞ÁîüËØçÊ±áÔºåÂ¶ÇÂèëÁé∞Êñ∞ÁöÑÁΩëÁªúÁî®ËØ≠„ÄÇËøô‰∏™ÂäüËÉΩËÉΩÂê¶ÂÆûÁé∞„ÄÇ
"
ÁÆÄÁπÅËΩ¨Êç¢ÂàÜÊ≠ßËØçËØÜÂà´Êüê‰∫õÊÉÖÂÜµ‰∏ãËΩ¨Êç¢ÈîôËØØ,"‰ªäÂ§©Âú®‰ΩøÁî®HanLPÂú®ÂÅöÁÆÄÁπÅËΩ¨Êç¢ÁöÑÊó∂ÂÄôÂèëÁé∞ÊúâÂ¶Ç‰∏ãÂàÜÊ≠ßËØçËØÜÂà´ÈîôËØØÔºö
""ÂÆèÂÆâÂú∞‰∫ß"" -> ""Â∑®ÈõÜÂÆâÂú∞Áî¢""
""È¶ôÊ∏ØËÅî‰∫§ÊâÄ‰∏ªÊùøIPO‰∏äÂ∏Ç"" -> ""È¶ôÊ∏ØËÅØ‰∫§ÊâÄ‰∏ªÊ©üÊùøIPO‰∏äÂ∏Ç""

ËøôÊòéÊòæÊòØÊúâÈóÆÈ¢òÁöÑÔºåÊàëÁî®chinese-utilsÁöÑËΩ¨Êç¢Â∞±ÊòØÊ≠£Á°ÆÁöÑÔºåÊúõ‰øÆÂ§ç
"
PythonË∞ÉÁî®hanlpÂ¶Ç‰ΩïÂ∞ÜHanLP.propertiesÊîæÂõæclasspath‰∏≠„ÄÇ,"ËøôÊòØÈ°πÁõÆÊñáÊ°£ËØ¥ÊòéÔºöÊúÄÂêéÂ∞ÜHanLP.propertiesÊîæÂÖ•classpathÂç≥ÂèØÔºåÂØπ‰∫é‰ªª‰ΩïÈ°πÁõÆÔºåÈÉΩÂèØ‰ª•ÊîæÂà∞srcÊàñresourcesÁõÆÂΩï‰∏ãÔºåÁºñËØëÊó∂IDE‰ºöËá™Âä®Â∞ÜÂÖ∂Â§çÂà∂Âà∞classpath‰∏≠„ÄÇ

ÊàëÁöÑIDEÊòØeclipse„ÄÇËØ∑ÈóÆpythonÈ°πÁõÆÔºåÂ∫îËØ•Â∞ÜHanLP.propertiesÊîæÂú®Âì™ÈáåÔºåÊÄé‰πàÊîæÂÖ•classpath‰∏≠~
"
Â¶Ç‰ΩïÂèñËá™ÂÆö‰πâËØçË°®‰∏≠ÊúÄËøë‰ººÁöÑÂçïËØçÔºü,"ÊàëÁé∞Âú®Ëá™ÂÆö‰πâÁöÑÂ≠óÂÖ∏‰∏≠ÂÆö‰πâ‰∫Ü‚ÄúÂ§ßÁéãÂè´ÊàëÊù•Â∑°Â±±‚ÄùÔºå ÊàëËæìÂÖ•‰∫ÜÁöÑÂ≠óÁ¨¶‰∏∫‚ÄúÂ§ßÁéãÊ¥æÊàëÊù•Â∑°Â±±‚ÄùÔºå ÊÄé‰πàÊ†∑ËÉΩËÆ©Á≥ªÁªüÁªôÂá∫Ëøô‰∏™ÊúÄÁõ∏‰ººÁöÑÂçïËØçÔºü ËÉΩÂê¶ÊåáÁÇπ‰∏Ä‰∏ãÂ§öË∞¢„ÄÇ
"
Â¶Ç‰ΩïËÆæÁΩÆÊõøÊç¢ÂåπÈÖç,"ÊàëÁé∞Âú®Âú®Ëá™ÂÆö‰πâÂ≠óÊÆµ‰∏≠ËÆæÁΩÆ‰∫Ü‚Äú‰∏≠ÂõΩÂ•ΩÂ£∞Èü≥Á¨¨‰∏ÄÂ≠£‚Äù‰∏∫Èü≥‰πê‰∏ìËæëÁ±ªÂûãÔºå ÊàëÊÉ≥ÂêéÈù¢Êó†ËÆ∫ÊòØ‚Äú‰∏≠ÂõΩÂ•ΩÂ£∞Èü≥Á¨¨‰∫åÂ≠£‚ÄùËøòÊòØ‚Äú‰∏≠ÂõΩÂ•ΩÂ£∞Èü≥Á¨¨‰∏âÂ≠£‚ÄùÁ≠âÁ≠âÈÉΩÂèØ‰ª•ËØÜÂà´‰∏∫Èü≥‰πê‰∏ìËæëÁ±ªÂûãÔºåËøô‰∏™ËØ•Â¶Ç‰ΩïÂéªÂÅöÔºü
"
Êúâ‰∏™ËØçËØ≠ÂàÜÁöÑÊúâËØØ,"2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreDictionary load
‰ø°ÊÅØ: Ê†∏ÂøÉËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:data/dictionary/CoreNatureDictionary.mini.txt
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
‰ø°ÊÅØ: data/dictionary/CoreNatureDictionary.mini.txtÂä†ËΩΩÊàêÂäüÔºå85585‰∏™ËØçÊù°ÔºåËÄóÊó∂265ms
Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[Ê±ü, Ê±üËãè]
2:[Ëãè]
3:[Âè•]
4:[ÂÆπ, ÂÆπ‰∫∫]
5:[‰∫∫]
6:[ ]

2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
‰ø°ÊÅØ: ÂºÄÂßãÂä†ËΩΩ‰∫åÂÖÉËØçÂÖ∏data/dictionary/CoreNatureDictionary.ngram.mini.txt.table
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary <clinit>
‰ø°ÊÅØ: data/dictionary/CoreNatureDictionary.ngram.mini.txt.tableÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂156ms
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
‰ø°ÊÅØ: Ëá™ÂÆö‰πâËØçÂÖ∏ÂºÄÂßãÂä†ËΩΩ:data/dictionary/custom/CustomDictionary.txt
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.CustomDictionary <clinit>
‰ø°ÊÅØ: Ëá™ÂÆö‰πâËØçÂÖ∏Âä†ËΩΩÊàêÂäü:83911‰∏™ËØçÊù°ÔºåËÄóÊó∂313ms
Á≤óÂàÜÁªìÊûú[Ê±üËãè/ns, Âè•/q, ÂÆπ‰∫∫/v]
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.common.CommonDictionary load
‰ø°ÊÅØ: Âä†ËΩΩÂÄºdata/dictionary/person/nr.txt.value.datÊàêÂäüÔºåËÄóÊó∂33ms
2016-4-12 16:18:00 com.hankcs.hanlp.dictionary.common.CommonDictionary load
‰ø°ÊÅØ: Âä†ËΩΩÈîÆdata/dictionary/person/nr.txt.trie.datÊàêÂäüÔºåËÄóÊó∂31ms
2016-4-12 16:18:01 com.hankcs.hanlp.dictionary.nr.PersonDictionary <clinit>
‰ø°ÊÅØ: data/dictionary/person/nr.txtÂä†ËΩΩÊàêÂäüÔºåËÄóÊó∂313ms
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][Ê±üËãè K 3 L 1 ][Âè• K 3 L 1 ][ÂÆπ‰∫∫ A 22202445 ][  A 22202445 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Ê±üËãè/K ,Âè•/K ,ÂÆπ‰∫∫/A , /A]
Ê±üËãè/ns
Âè•/q
ÂÆπ‰∫∫/v

ÁÖßÈÅìÁêÜÔºöÂ∫îËØ•ÂàÜËØçÊàê  Ê±üËãè„ÄÅÂè•ÂÆπ„ÄÅ‰∫∫  ÊàñËÄÖ  Ê±üËãè„ÄÅÂè•ÂÆπ‰∫∫
"
ÂàùÂßãÂåñËÉΩÂê¶ÊîØÊåÅ‰ªéClasspathËØªÂèñdata‰∏ãÁöÑËá™ÂÆö‰πâÂ≠óÂÖ∏ÂÖÉÊï∞ÊçÆÔºü,"Â¶ÇÈ¢òÔºåÁõÆÂâçÈúÄË¶ÅÂú®hanlp.propertiesËÆæÁΩÆrootÁöÑÂÄº‰∏∫ÁªùÂØπË∑ØÂæÑÔºåËÉΩÂê¶ËÆæÁΩÆ‰∏∫classpath‰∏ãÁöÑÁõ∏ÂØπË∑ØÂæÑÔºü
"
Áõ¥Êé•Ë∞ÉÁî®Âè•Ê≥ïÂàÜÊûêÂô®ÔºåÊòæÁ§∫Ê∫êÁ†ÅÈîôËØØ,"Áõ¥Êé•Ë∞ÉÁî®CRFÂè•Ê≥ïÂàÜÊûê
`System.out.println(CRFDependencyParser.compute(""ÊääÂ∏ÇÂú∫ÁªèÊµéÂ•âË°åÁöÑÁ≠â‰ª∑‰∫§Êç¢ÂéüÂàôÂºïÂÖ•ÂÖöÁöÑÁîüÊ¥ªÂíåÂõΩÂÆ∂Êú∫ÂÖ≥ÊîøÂä°Ê¥ªÂä®‰∏≠""));`
Âá∫Áé∞ÈóÆÈ¢òÔºö

```
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.dependency.CRFDependencyParser.parse(CRFDependencyParser.java:123)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:78)
    at bistu.idcc.features.ParseDependency.main(ParseDependency.java:19)

```

ËØ∑Êïô‰∏Ä‰∏ãÔºåÂ∫îËØ•Â¶Ç‰ΩïËß£ÂÜ≥
"
NLPTokenizer‰∏çÂêåËØ≠Âè•ÂàÜÊûêÁöÑÁªìÊûú‰∏∫‰ªÄ‰πà‰ºö‰∏çÂêåÔºü,"List<Term> termList = NLPTokenizer.segment(""Âº†‰∏âÊòØÊàëÂì•Âì•"");
 System.out.println(termList);

[Âº†/q, ‰∏â/m, ÊòØ/vshi, Êàë/rr, Âì•Âì•/n]

List<Term> termList = NLPTokenizer.segment(""Âº†‰∏âÊïôÊéàÊ≠£Âú®ÊïôÊéàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã"");
        System.out.println(termList);

[Âº†‰∏â/nr, ÊïôÊéà/nnt, Ê≠£Âú®/d, ÊïôÊéà/v, Ëá™ÁÑ∂/n, ËØ≠Ë®Ä/n, Â§ÑÁêÜ/vn, ËØæÁ®ã/n]
"
HanLPÂØπ‰∫éÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏ÁöÑÂÆòÊñπÊé®ËçêÊñπÊ≥ïÊòØÊÄéÊ†∑ÁöÑ?,"‰Ω†Â•Ω,ÊàëÂú®ÊµãËØïÁöÑÊó∂ÂÄô,ÂèëÁé∞ÈÄöËøá`CustomDictionary.add()`ÊñπÊ≥ïÊ∑ªÂä†ÁöÑÊñ∞ËØç‰∏ç‰ºöËá™Âä®ÊåÅ‰πÖÂåñ,Ê≤°ÊúâÂä®ÊÄÅ‰øÆÊîπ‰∫åËøõÂà∂ÁºìÂ≠òÊñá‰ª∂.
HanLPÂÆòÊñπÊé®ËçêÁöÑÊñπÊ≥ïÊòØËÆ∞ÂΩïÊ∑ªÂä†ÊàêÂäüÁöÑËá™ÂÆö‰πâËØçËØ≠,ÁÑ∂ÂêéÂú®‰∏ãÊ¨°È°πÁõÆÂêØÂä®‰πãÂâçËøΩÂä†Âà∞Ëá™ÂÆö‰πâËØçÂÖ∏Êñá‰ª∂Âêó?
"
Â¶Ç‰ΩïÂ¢ûÂä†Ê≠åÊõ≤Á±ªÁöÑÂàÜËØçÔºü,"ÊØîÂ¶ÇËØ¥ÊàëÊÉ≥ÊääÈïøÊ±ü‰πãÊ≠åÔºå ÊàñËÄÖÁà±ÁöÑÂ•âÁåÆ‰Ωú‰∏∫Êï¥‰∏™‰∏Ä‰∏™ËØçÂèØ‰ª•ÂêóÔºü
"
‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÈóÆÈ¢ò,"ËØïËøêË°å‰∫Ü‰∏Ä‰∏ã1.2.7ÁâàÊú¨ÁöÑdemoÔºåÂèëÁé∞ÊúâÈóÆÈ¢ò„ÄÇËØ∑ÁúãÁúãÊÄé‰πàËß£ÂÜ≥Ôºü
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.parse(NeuralNetworkDependencyParser.java:53)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.compute(NeuralNetworkDependencyParser.java:93)
    at com.hankcs.hanlp.HanLP.parseDependency(HanLP.java:407)
    at com.hankcs.demo.DemoDependencyParser.main(DemoDependencyParser.java:26)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Caused by: java.lang.NullPointerException at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.read_matrix(NeuralNetworkParser.java:294)at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.loadTxt(NeuralNetworkParser.java:166)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:136)
    at com.hankcs.hanlp.dependency.nnparser.parser_dll.<clinit>(parser_dll.java:34)
    ... 10 more
"
pythonÂºÄÂêØJVMË∞ÉÁî®HanLPÊó∂Êä•ÈîôUnable to start JVM at native\common\jp_env.cpp:60,"pythonÂºÄÂêØJVMË∞ÉÁî®HanLPÊó∂Êä•ÈîôUnable to start JVM at native\common\jp_env.cpp:60ËØ∑ÈóÆÂ¶Ç‰ΩïËß£ÂÜ≥
"
ÂÖ≥‰∫éHanLPËÆ°ÁÆóÊñáÁ´†Áõ∏‰ººÂ∫¶ÈóÆÈ¢ò,"‰πãÂâçÁî®AnsjÔºåÁé∞ÂàöÂèëÁé∞HanLPÔºåËøò‰∏çÁÜüÊÇâÔºå‰ΩÜËßâÂæóHanLPÈùûÂ∏∏‰∏çÈîô„ÄÇÂÜíÊòßËØ∑ÊïôÂçö‰∏ªÔºåÊàëÂ∏åÊúõÈÄöËøáHanLPÂÆûÁé∞ËÆ°ÁÆó‰∏§‰∏™ÊñáÁ´†ÁöÑÁõ∏‰ººÂ∫¶ÔºåÁõºÂ§ßÁ•ûÊèê‰æõ‰ΩøÁî®HanLPÂÆûÁé∞ÊñáÁ´†Áõ∏‰ººÂ∫¶ÁöÑÂ§ßÊ¶ÇÊÄùË∑ØÊàñÂäüËÉΩÔºåÊÑüÊøÄ‰∏çÂ∞Ω~ÊàëÁúãhttps://github.com/ysc/word/tree/master/src/main/java/org/apdplat/word/analysisÂ∑≤ÊúâËØ∏Â§öÁÆóÊ≥ïÂÆûÁé∞Ôºå‰ΩÜÊàë‰æùÁÑ∂Â∏åÊúõ‰ΩøÁî®HanLPÔºåÂèØÊÉúÊ≤°ÊúâÁ±ª‰ººÂäüËÉΩÔºåÁùÄÂÆûÈÅóÊÜæÔºå‰∫üÁõº‰ΩúËÄÖÊèê‰æõÂ∏ÆÂä©„ÄÇ
"
ÊèêÂèñÂÖ≥ÈîÆËØç,"ÊàëÊúâÂ§ßÈáèÂ¶Ç‰∏ãÊ†ºÂºèÁöÑÊñáÊú¨

```
ÂßìÂêçÔºöÂº†‰∏âÊÄßÂà´ÔºöÁî∑Âπ¥ÈæÑ18Ë∫´-È´ò180cmÂèë Âûã-È£ûÊú∫Â§¥ÊØç‰∫≤ÂßìÂêçÔºöÂº†Â¶àÂ¶à......
```

ËøôÈáåÁöÑ‚ÄúÂßìÂêç ÊÄßÂà´ Âπ¥ÈæÑ Ë∫´È´ò ÂèëÂûã‚Äù‰ªÖ‰ªÖ‰∏∫‰∫ÜËØ¥ÊòéÔºåÂÆûÈôÖ‰∏äÊúâÂ§ßÈáèÁöÑÁ±ª‰ººÂÖ≥ÈîÆËØç Ôºå‰∏îÊØè‰∏™ÊñáÊú¨‰∏≠ÂÖ≥ÈîÆËØçÂá∫Áé∞ÁöÑÊ¨°Êï∞ÊòØ‰∏çÂÆöÁöÑ(ÊúâÁöÑÂåÖÂê´ÂÖ∂‰∏≠10‰∏™ ÊúâÁöÑÂåÖÂê´ÂÖ∂‰∏≠‰∏Ä‰∏™) ‰ΩÜÊØè‰∏™ÂÖ≥ÈîÆËØçÊúÄÂ§öÂè™‰ºöÂá∫Áé∞‰∏ÄÊ¨°(ÂßìÂêçÂíåÊØç‰∫≤ÂßìÂêçËÆ§‰∏∫ÊòØ‰∏§‰∏™ÂÖ≥ÈîÆËØç)
Â∏åÊúõËÉΩÂ§üÂÆûÁé∞ÊèêÂèñÂà∞ÊâÄÊúâÂÖ≥ÈîÆËØç(ÁßªÈô§ÂÖ≥ÈîÆËØçÂÜÖÁöÑÁâπÊÆäÁ¨¶Âè∑Â¶ÇÁ©∫Ê†º„ÄÅ-Á≠â)ÂàóË°®ÔºåÂêåÊó∂ËÉΩÂ§üÂú®ÂÖ≥ÈîÆËØçÂ§ÑÊ∑ªÂä†ÂàÜÈöîÁ¨¶
Â¶Ç‰∏ã

```
ÂßìÂêçÔºöÂº†‰∏â ÊÄßÂà´ÔºöÁî∑ Âπ¥ÈæÑ18 Ë∫´È´ò180cm ÂèëÂûã-È£ûÊú∫Â§¥ ÊØç‰∫≤ÂßìÂêçÔºöÂº†Â¶àÂ¶à......
```

ÊòØÂê¶ÂèØ‰ª•‰ΩøÁî®ÊÇ®Êèê‰æõÁöÑÂ∫ìÂÆûÁé∞ÔºåÂèØÂê¶Áªô‰∫à‰∏Ä‰∫õ‰∏ì‰∏öÊÑèËßÅ
"
Âú®phraseËØÜÂà´‰∏≠ËÉΩÂê¶ÊåáÂÆöÊúÄÂ§ßÂà∞‰∫åÈò∂ÂÖ±Áé∞Ôºü,
Â¶Ç‰ΩïËé∑ÂæóÂàÜËØçËØçÈ¢ëÔºü,"‰ΩøÁî®segmentÁöÑÊó∂ÂÄôÔºåÂàÜËØçÁöÑËØçÈ¢ëÂ¶Ç‰ΩïËé∑ÂæóÂë¢Ôºü
"
ÊúâÊ≤°Êúâ‰∏Ä‰∏™ÂÆåÊï¥‰∏ÄÁÇπÁöÑËØçÊÄßËØ¥Êòé,
extractPhraseÂá∫Áé∞ÈîôËØØ RuntimeError: No matching overloads found. at src/native/common/jp_method.cpp:121,"Ë∞ÉÁî® HanLP.extractPhrase(ss) Âá∫Áé∞Ëøô‰∏™ÈîôËØØ
"
hanlp.propertiesÊñá‰ª∂,"Âº±Âº±ÁöÑÈóÆ‰∏ÄÂè• Ëøô‰∏™Êñá‰ª∂ÊòØ‰ªÄ‰πàÊñá‰ª∂Ôºå‰∏∫‰ªÄ‰πàÂú®github‰∏äÈù¢Ê≤°ÊúâÔºü
"
‰øÆÊ≠£‰∏Ä‰∏™ÊãºÈü≥,"Â∞Ü‚Äúnv3=n,u,3‚ÄùÊîπ‰∏∫‚Äúnv3=n,v,3‚Äù
"
Ê±âÂ≠óËΩ¨ÊãºÈü≥ÈóÆÈ¢ò,"ÂéüÊñáÔºöÂêâÊûóÁúÅÈïøÊò•ËçØÊàø,
ÊãºÈü≥ÔºàÊï∞Â≠óÈü≥Ë∞ÉÔºâ,ji2,lin2,sheng3,zhang3,chun1,yao4,fang2,
ÊãºÈü≥ÔºàÁ¨¶Âè∑Èü≥Ë∞ÉÔºâ,j√≠,l√≠n,shƒõng,zh«éng,ch≈´n,y√†o,f√°ng,
ÊãºÈü≥ÔºàÊó†Èü≥Ë∞ÉÔºâ,ji,lin,sheng,zhang,chun,yao,fang,
Â£∞Ë∞É,2,2,3,3,1,4,2,
Â£∞ÊØç,j,l,sh,zh,ch,y,f,
ÈüµÊØç,i,in,eng,ang,un,ao,ang,
ËæìÂÖ•Ê≥ïÂ§¥,j,l,sh,zh,ch,y,f,

ÂæàÊòéÊòæzhang3Ëøô‰∏™ÊãºÈü≥ÊòØÈîôÁöÑÔºåÊòØÂê¶ÂèØ‰ª•ËÄÉËôëÂä†‰∏äÂàÜËØçÊù•Ëé∑ÂèñÊãºÈü≥Âë¢Ôºü
"
‰∏âÂÖÉÁªÑÊèêÂèñÁöÑÈóÆÈ¢ò," @hankcs ÊÑüË∞¢‰Ω†ÂºÄÂèëÁöÑHanLPÂ∑•ÂÖ∑ÔºåÂÆûÂú®Â§™Ê£í‰∫ÜÔºÅ

ÊàëÂàöÂ≠¶NLPÔºåÊúâ‰∏™ÈóÆÈ¢òÊÉ≥Âêë‰Ω†ËØ∑Êïô„ÄÇ
ÊàëÊúâËøô‰πà‰∏™ÈúÄÊ±ÇÔºåËæìÂÖ•ÊòØ‰∏Ä‰∏™Âè•Â≠êÔºåËæìÂá∫ÊòØËÉΩË°®ËææÂè•Â≠êËØ≠‰πâÁöÑ‰∏âÂÖÉÁªÑÔºà‰∏ªË∞ìÂÆæÔºâ„ÄÇ‰æãÂ¶ÇÔºö

ÂéüÂè•ÔºöÊ†ºË®ÄÊòØÁÆÄÁªÉËÄåÂê´‰πâÊ∑±ÂàªÂπ∂ÂÖ∑ÊúâÊïôËÇ≤ÊÑè‰πâÁöÑË≠¶Âè•„ÄÇ
ÁªìÊûúÔºöÔºàÊ†ºË®ÄÔºåÂÆö‰πâÔºåÁÆÄÁªÉËÄåÂê´‰πâÊ∑±ÂàªÂπ∂ÂÖ∑ÊúâÊïôËÇ≤ÊÑè‰πâÁöÑË≠¶Âè•Ôºâ

ÂéüÂè•ÔºöÂÑøÂåñÂÖ∑ÊúâÂå∫Âà´ËØç‰πâ„ÄÅÂå∫ÂàÜËØçÊÄßÂíåË°®Á§∫ÊÑüÊÉÖËâ≤ÂΩ©ÁöÑ‰ΩúÁî®„ÄÇ
ÁªìÊûúÔºöÔºàÂÑøÂåñÔºå‰ΩúÁî®ÔºåÂå∫Âà´ËØç‰πâ„ÄÅÂå∫ÂàÜËØçÊÄßÂíåË°®Á§∫ÊÑüÊÉÖËâ≤ÂΩ©Ôºâ

ÂéüÂè•ÔºöÁÇπÂè∑ÂèàÂàÜÂè•Êú´ÁÇπÂè∑ÂíåÂè•ÂÜÖÁÇπÂè∑„ÄÇ
ÁªìÊûúÔºöÔºàÁÇπÂè∑ÔºåÂåÖÂê´ÔºåÂè•Êú´ÁÇπÂè∑ÂíåÂè•ÂÜÖÁÇπÂè∑Ôºâ

ÊàëÁúãËøá‰Ω†ÂÜôÁöÑÊèêÂèñÂè•Â≠ê‰∏ªË∞ìÂÆæÁöÑÈ°πÁõÆÔºåÊâÄ‰ª•ÊàëÊâìÁÆóÁî®Á±ª‰ººÁöÑÊñπÊ≥ïÊù•ÂÅöÔºå‰ΩÜÊòØËá™Â∑±ÂàöÂÖ•Èó®NLPÔºå‰∏çÁü•ÈÅìË°å‰∏çË°å„ÄÇ‰Ω†Êúâ‰ªÄ‰πàÊñπÊ≥ïÊàñËÄÖÂª∫ËÆÆ‰πàÔºü

Ë∞¢Ë∞¢ÔºÅ
"
‰∫∫ÂêçËØÜÂà´‰∏çÂáÜÁ°Æ,"ÊûóÂÜõÂíåÂ§ßÂ£Æ‰ªé‰æßÈó®ÔºåË¢´‰∏§‰∏™Ë≠¶ÂØüÂ∏¶Ëøõ‰∫ÜÂ∏ÇÂú∫ÊóÅËæπÁöÑ‰∏Ä‰∏™Â§ßÈô¢ÂÜÖÔºåÈöèÂç≥Áõ¥Êé•Ëøõ‰∫ÜÂäûÂÖ¨Ê•ºÈáå„ÄÇ

„ÄÄ„ÄÄ‚ÄúËøôÊ¥æÂá∫ÊâÄÊå∫Â§ßÂïäÔºü‚ÄùÊûóÂÜõÁúãÁùÄÁã≠ÈïøÁöÑËµ∞ÂªäÔºåË°®ÊÉÖÊúâ‰∫õÊÉäÊÑïÁöÑËØ¥‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂ§ßÂì•ÔºåËøôÊòØÂ∏ÇÂ±Ä‰∏ÉÂ§ÑÔºÅ‚ÄùÁî∑Ë≠¶ÂØüÊñúÁúºÂõû‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄÂ∏ÇÂÖ¨ÂÆâÂ±ÄÁ¨¨‰∏ÉÂàë‰æ¶Â§ßÈòüÔºå‰øóÁß∞‰∏ÉÂ§ÑÔºåÈÉ®Èó®ËÅåË¥£ÊòØ‰∏ªÊäìÁâπÂ§ßÈáçÁÇπÊ°à‰ª∂„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂíãÁªôÊàë‰ª¨Â∏¶ËøôÂÑøÊù•‰∫ÜÔºü‚ÄùÊûóÂÜõÂê¨Âêé‰∏ÄÊÑ£„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊù•ÔºåÂ∑¶Ëæπ‰∏Ä‰∏™ÔºåÂè≥Ëæπ‰∏Ä‰∏™ÔºåÈù†ÁùÄÊöñÊ∞îÁ´ôÂ•ΩÔºÅ‚ÄùÂ•≥Ë≠¶ÂÜ≤ÁùÄÊûóÂÜõÂíåÂ§ßÂ£ÆÔºå‰øèËÑ∏Èù¢Êó†Ë°®ÊÉÖÁöÑËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‰∫å‰∫∫Âê¨Âà∞ËøôËØùÔºå‰πüÊ≤°‰∫âËæ©ÔºåÈöèÂêéÂêÑËá™Èù†ÁùÄÊöñÊ∞îÁ´ô‰∫Ü‰∏ãÊù•„ÄÇËÄåÂ•≥Ë≠¶Ëµ∞ËøõÂäûÂÖ¨ÂÆ§Âèñ‰∫Ü‰∏§ÂπÖÊâãÈìêÔºåÈöèÂç≥Â∞Ü‰∫å‰∫∫ÂàÜÂà´ÈìêÂú®ÊöñÊ∞îÁÆ°Â≠ê‰∏äËØ¥ÈÅìÔºö‚ÄúÁ≠âÁùÄÂêßÔºå‰∏Ä‰ºöÊ¥æÂá∫ÊâÄËøáÊù•Âèñ‰Ω†‰ª¨‰ø©„ÄÇ‚Äù

„ÄÄ„ÄÄÊûóÂÜõÂè≥ÊâãË¢´Èìê‰∏äÊó∂,Ê≠£Â•Ω‰∏éÂ•≥Ë≠¶ËÑ∏ÂØπËÑ∏ÔºåÈöèÂç≥‰ªñÂèåÁúºÊú¨ËÉΩÁöÑÊâìÈáè‰∫ÜÂ•≥Ë≠¶‰∏Ä‰∏ã„ÄÇ

„ÄÄ„ÄÄÂ•πÁöÑÈïøÁõ∏Êúâ‰∫õÁâπÂà´ÔºåÈïøÂèëÊä´ËÇ©Ôºå‰∫îÂÆòÁ≤æËá¥Ôºå‰ΩÜÈºªÊ¢ÅÂæàÈ´òÔºåÁúºÁ™ùËæÉÊ∑±Ôºå‰∏ÄÂèåÁÅµÂä®ÁöÑÂ§ßÁúºÁùõÈùûÂ∏∏Â§∫ÁõÆ„ÄÇÁúãÁùÄÊúâÁÇπ‰∏çÂÉèÊ±â‰∫∫‰∫îÂÆòÔºåËÄåÊòØÊúâÁÇπÂÉèÂè≤ÂØÜÊñØÂ§´Â¶á‰∏≠ÁöÑÂÆâÂêâ‰∏ΩÂ®ú.Ëå±ËéâÔºÅ

„ÄÄ„ÄÄÂ•≥Ë≠¶Ë∫´ÊÆµÊå∫Áõ¥Ôºå‰∏™Â≠êËµ∑Á†Å‰∏ÄÁ±≥‰∏ÉÂ∑¶Âè≥Ôºå‰∏äÂçäË∫´Â•óÁùÄ‰∏Ä‰ª∂Á¥ßË∫´ÁöÑÂçäË¢ñË≠¶ÊúçË°¨Ë°´ÔºåÈ¢ÜÂè£Êâ£Â≠êÁ≥ªÁöÑ‰∏Ä‰∏ù‰∏çËãü„ÄÇÂ•π‰∏ãÂçäË∫´Á©øÁùÄÈªëËìùËâ≤ÂÆΩÊùæÁöÑÈïøË£§ÔºåËÑö‰∏äËπ¨ÁùÄ‰∏ÄÂèåÂπ≥Â∫ïÁöÑÈªëËâ≤Áì¢ÈûãÔºåÊï¥‰∏™‰∫∫ÁöÑÊ∞îË¥®Áªô‰∫∫‰∏ÄÁßçÂÖÖÊª°Ê¥ªÂäõÔºåËã±ÂßøÈ£íÁàΩÁöÑÊÑüËßâ„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂìéÔºåÈÉëË≠¶ÂÆòÔºåÊàë‰ø©ËøôÂ∞±ÊòØÂñùÂ§ö‰∫ÜÁûéÈóπËÖæÔºåÁäØ‰∏ç‰∏äÂú®‰Ω†ËøôÂÑøÂç†Âú∞Êñπ„ÄÇ‰Ω†ÁªôÊàë‰ø©ÊùæÂºÄÔºåÊàë‰ø©‰∏ÄÂùóÂéªÊ¥æÂá∫ÊâÄÂíåËß£‰∫ÜÂæó‰∫Ü„ÄÇ‚ÄùÂ§ßÂ£ÆÊ≠§ÂàªÂ∑≤ÁªèÊúâÁÇπË¢´ÊèçÁöÑÈÜíÈÖí‰∫ÜÔºå‰ªñÂ∑¶ÊâãÊçÇÁùÄËøòÂú®Ê∑åË°ÄÁöÑÂò¥ÂîáÔºåÈöèÂç≥Âê´Á≥ä‰∏çÊ∏ÖÁöÑÂñäÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÈó≠‰∏ä‰Ω†ÁöÑÂò¥ÔºåÂëÜÁùÄÔºÅ‚ÄùÂ•≥Ë≠¶ÂéåÊÅ∂ÁöÑÊâ´‰∫Ü‰ªñ‰∏ÄÁúºÔºåÈöèÂêéÂÜ≤Êî∂ÂèëÂÆ§ÂñäÈÅìÔºö‚ÄúÊùéÂèîÔºåÂ∏ÆÂøôÁúã‰∏Ä‰∏ãÔºå‰∏Ä‰ºöÊää‰ªñ‰ª¨‰∫§ÁªôÊ¥æÂá∫ÊâÄÂ∞±Ë°å„ÄÇ‚Äù

„ÄÄ„ÄÄ‚ÄúÂ•ΩÂèªÔºÅ‚Äù„ÄÄÊî∂ÂèëÂÆ§ÁöÑÂ§ßÁà∑Âõû‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄÈöèÂêéÂ•≥Ë≠¶Ë∏©ÁùÄÂπ≥Â∫ïÈûãÂ∞±‰∏ä‰∫ÜÊ•ºÔºåËÄåË∑ü‰ªñ‰∏ÄËµ∑ÁöÑÈÇ£‰∏™Áî∑Ë≠¶ÂØüËΩ¨Ë∫´ÂÜçÊ¨°Âéª‰∫ÜÂ∏ÇÂú∫ÔºåÁªßÁª≠ÂéªÁªôÂä†Áè≠ÁöÑÂêå‰∫ã‰π∞ÁõíÈ•≠„ÄÇ

„ÄÄ„ÄÄËµ∞ÂªäÂÜÖÔºåÂ∑•‰Ωú‰∫∫ÂëòÊù•ÂõûÁ©øÊ¢≠ÔºåËÄåÊûóÂÜõÂíåÂ§ßÂ£ÆÁõ∏‰∫íÂØπËßÜ‰∫Ü‰∏ÄÁúº„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊìç‰Ω†ÁéõÔºå‰Ω†Á≠âÂá∫ÂéªÁöÑÔºÅÊàëËÆ©‰Ω†Áü•ÈÅìÔºå‰Ω†ÊâìÊàëÁöÑÈÇ£‰∏ÄÈÖíÁì∂Â≠êÊúâÂ§öÊó†Áü•ÔºÅ‚ÄùÂ§ßÂ£ÆÁúãÁùÄÊûóÂÜõÂ∞èÂ£∞È™ÇÈÅì„ÄÇ

„ÄÄ„ÄÄÊûóÂÜõÂ∞ÜÂ§¥Êâ≠ËøáÂéªÔºåÊ†πÊú¨Ê≤°ÂõûËØù„ÄÇ

„ÄÄ„ÄÄ......

„ÄÄ„ÄÄÂõõÂçÅÂàÜÈíü‰ª•ÂêéÔºåÊ¥æÂá∫ÊâÄ‰∏Ä‰∏™Ê∞ëË≠¶ÔºåÂ∏¶ÁùÄ‰∏Ä‰∏™‰∫åÂçÅÂÖ≠‰∏ÉÁöÑÈùíÂπ¥ÔºåÂπ∂ËÇ©Ëµ∞Ëøõ‰∫ÜËµ∞Âªä„ÄÇÊ∞ëË≠¶ËøõÊù•‰ª•ÂêéÔºåÂ∞±Áõ¥Êé•Ëµ∞Ëøõ‰∫ÜÂäûÂÖ¨ÂÆ§ÔºåËÄåÈùíÂπ¥ËÖã‰∏ãÂ§πÁùÄÂåÖÔºåËÑñÂ≠ê‰∏äÊåÇÁùÄ‰ΩõÁâåÔºåÊâãÈáåÊêìÁùÄÁè†Â≠êÂÜ≤Â§ßÂ£ÆÈ™ÇÈÅìÔºö‚Äú‰∏ÄÂ§©ÂáÄ‰ªñÂ¶àÁªôÊàëÊÉπ‰∫ãÂÑøÔºÅ‚Äù

„ÄÄ„ÄÄ‚ÄúÊ∂õÔºå‰Ω†Áúã‰ªñÁªôÊàëÂπ≤ÁöÑÔºåÂò¥ÂîáÂ≠êÈÉΩÊï¥Ë±ÅË±Å‰∫Ü„ÄÇ‚ÄùÂ§ßÂ£ÆÊåáÁùÄËá™Â∑±ÁöÑÂò¥ÂîáÂ≠êËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‚Äú‰Ω†Èó≠Âò¥ÂêßÔºÅ‚ÄùÈùíÂπ¥Âõû‰∫Ü‰∏ÄÂè•ÔºåÈöèÂêéÊúùÁùÄÊ∞ëË≠¶Ëµ∞ËøõÁöÑÂäûÂÖ¨ÂÆ§Ëµ∞Âéª„ÄÇ

„ÄÄ„ÄÄ‰∫åÂçÅÂàÜÈíü‰ª•ÂêéÔºåÊ∞ëË≠¶ÂíåÈùíÂπ¥Ëµ∞‰∫ÜÂá∫Êù•„ÄÇ

„ÄÄ„ÄÄ‚ÄúÁéãÊ∂õÔºåË∞ÅÊòØ‰Ω†ÊúãÂèãÂïäÔºü‚ÄùÊ∞ëË≠¶ËôéÁùÄËÑ∏ÔºåËÉåÊâãÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂ∞±‰ªñÔºÅ‚ÄùÂè´ÁéãÊ∂õÁöÑÈùíÂπ¥Êåá‰∫ÜÊåáÂ§ßÂ£Æ„ÄÇ

„ÄÄ„ÄÄÊ∞ëË≠¶Êâ´‰∫Ü‰∏ÄÁúºÊûóÂÜõÂíåÂ§ßÂ£ÆÔºåÈöèÂç≥Áö±ÁúâÈóÆÈÅìÔºö‚ÄúÂ∞±ËøôÁÇπÁ†¥‰∫ãÂÑøÔºåËøòÁî®ÊàëË∞ÉËß£ÂïäÔºüÁî®È™å‰º§ÂêóÔºü‚Äù

„ÄÄ„ÄÄ‚ÄúÊàë‰∏çÁî®ÔºÅ‚ÄùÂ§ßÂ£ÆÊÄùËÄÉ‰∫Ü‰∏Ä‰∏ãÔºåÂπ≤ËÑÜÁöÑÂõûÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊàë‰πü‰∏çÁî®ÔºÅ‚ÄùÊûóÂÜõÊâ´‰∫Ü‰∏ÄÁúº‰∏â‰∫∫Ôºå‰πüÈù¢Êó†Ë°®ÊÉÖÁöÑÂõûÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÁúü‰∏çÁî®ÂïäÔºü‚ÄùÊ∞ëË≠¶ÂÜ≤ÁùÄÊûóÂÜõÂÜçÊ¨°ÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚Äú‰∏çÁî®„ÄÇ‚ÄùÊûóÂÜõÊØ´‰∏çÁäπË±´ÁöÑÊëá‰∫ÜÊëáÂ§¥„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊâìÂºÄÔºåËµ∞ÂêßÔºÅ‚ÄùÊ∞ëË≠¶ÈöèÊâãÊãøÁùÄÈí•Âåô‰∫§Áªô‰∫ÜÁéãÊ∂õ„ÄÇ

„ÄÄ„ÄÄÁéãÊ∂õÊé•ËøáÈí•ÂåôÔºåÂ∞ÜÂ§ßÂ£ÆÁöÑÈìêÂ≠êÊâìÂºÄÔºåÁÑ∂ÂêéÂèàÂ∞ÜÈí•ÂåôÊâîÁªô‰∫ÜÊûóÂÜõ„ÄÇ

„ÄÄ„ÄÄ‚ÄúË∞≠Âì•ÔºåÈ∫ªÁÉ¶‰∫ÜÔºåÊòéÂÑøËØ∑‰Ω†ÂêÉÈ•≠Âïä„ÄÇ‚ÄùÁéãÊ∂õÁ¨ëÁùÄÂÜ≤Ê∞ëË≠¶ËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúËΩªÁÇπÂòöÁëüÊØîÂï•ÈÉΩÂº∫ÔºåËµ∞ÂêßÔºåËµ∞Âêß„ÄÇ‚ÄùÊ∞ëË≠¶Ê∑°ÁÑ∂ÁöÑÊëÜ‰∫ÜÊëÜÊâã„ÄÇ

„ÄÄ„ÄÄ‚ÄúÈÇ£ÊàëËµ∞‰∫ÜÔºåË∞≠Âì•ÔºÅ‚Äù

„ÄÄ„ÄÄÁéãÊ∂õÂÜ≤Ê∞ëË≠¶Êâì‰∫Ü‰∏™ÊãõÂëºÔºåÈöèÂêéÂ∏¶ÁùÄÂ§ßÂ£ÆÊâ¨ÈïøËÄåÂéª„ÄÇÊûóÂÜõÊëò‰∏ãÊâãÈìê‰ª•ÂêéÔºåÁ´ñËµ∑Â§ßÊãáÊåáÂÜ≤Ê∞ëË≠¶ËØ¥ÈÅìÔºö‚ÄúËøôÊ°àÂ≠êÂäûÁöÑÁúüÂà©Á¥¢ÔºÅ‚Äù

„ÄÄ„ÄÄ‚Äú‰Ω†ËøòÊúâ‰∫ãÂÑøÂïäÔºü‚ÄùÊ∞ëË≠¶ÂõûÂ§¥ÔºåÈù¢Êó†Ë°®ÊÉÖÁöÑÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂëµÂëµÔºåÊ≤°‰∫ãÂÑø„ÄÇ‚ÄùÊûóÂÜõÊîæ‰∏ãÊâãÈìêÔºåÈöèÂç≥Â§¥‰πü‰∏çÂõûÁöÑËµ∞Âá∫‰∫Ü‰∏ÉÂ§Ñ„ÄÇ

„ÄÄ„ÄÄ.......

„ÄÄ„ÄÄ‰∏Ä‰∏™ÂçäÂ∞èÊó∂‰ª•ÂêéÔºåÊó∂Èó¥Êé•ËøëÊôö‰∏ä‰∏ÉÁÇπÂ§ö„ÄÇ

„ÄÄ„ÄÄÊûóÂÜõÂàöÂàöÊî∂ÊãæÂÆåËá™Â∑±ÁöÑÂ∞èÊëäÔºåÂπ∂Â∞ÜÁÉßÁÉ§Áî®ÂÖ∑ÊîæÂú®‰∫ÜÂº†Â∞è‰πêÁöÑ‰∏âËΩÆÂ≠ê‰∏ä„ÄÇ

„ÄÄ„ÄÄ‚Äú‰ªäÂ§©‰∏çÂá∫‰∫ÜÔºü‚Äù

„ÄÄ„ÄÄÂº†Â∞è‰πêÁ´ôÂú®‰∏ÄÊóÅÔºåÂº†Âò¥ÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúËøòÂá∫Âï•Âá∫ÔºåË¥ßÈÉΩËÆ©‰ªñË∏©‰∫Ü„ÄÇ‚ÄùÊûóÂÜõÊúâÁÇπÂøÉÁÉ¶ÁöÑÂõûÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúË°åÔºåÈÇ£ÊàëË∑ü‰Ω†Êää‰∏úË•øÈÄÅÂõûÂéª„ÄÇ‚ÄùÂº†Â∞è‰πêÁ©øÁùÄÂ∑•‰ΩúÊúçÔºåÊå∫‰ªó‰πâÁöÑÂõû‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄ‚Äú‰∏çÁî®‰∫ÜÔºå‰Ω†ÂçñË¥ßÂêßÔºåËΩ¶ÂÄüÊàëÁî®Áî®Â∞±Ë°å„ÄÇ‚ÄùÊûóÂÜõÈ™ë‰∏ä‰∏âËΩÆÂ≠êÔºåÂíßÂò¥‰∏ÄÁ¨ëËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúË¶Å‰∏ç‰ªäÂ§©‰Ω†Âà´ÈÄÅ‰∫ÜÔºå‰∏úË•øÁõ¥Êé•ÊâîÊàëËøôÂÑøÂæó‰∫Ü„ÄÇ‚ÄùÂº†Â∞è‰πêÊòéÊòæÊúâÁÇπÊãÖÂøÉÁöÑËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂëµÂëµ„ÄÇ‚ÄùÊûóÂÜõ‰∏ÄÁ¨ëÔºå‰πüÊ≤°Â§öËØ¥ÔºåÈ™ëËΩ¶Â∞±Ëµ∞‰∫Ü„ÄÇ

„ÄÄ„ÄÄÔºéÔºéÔºéÔºéÔºéÔºéÔºé

„ÄÄ„ÄÄÂ∏ÇÂú∫ÂêéÊñπÁöÑÂ∞èË∑Ø‰∏äÔºåÊûóÂÜõÂÅ•Á°ïÊúâÂäõÁöÑÂèåËÖøËπ¨ÁùÄ‰∫∫Âäõ‰∏âËΩÆËΩ¶ÔºåÈ°∫ÁùÄÁÅØÂÖâÊòèÊöóÁöÑË°óÈÅì‰∏ÄË∑ØÂâçË°å„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂíöÂíöÂíöÔºÅ‚Äù

„ÄÄ„ÄÄË∑ùÁ¶ªÂ≠òÊîæÁÉßÁÉ§Áî®ÂÖ∑ÁöÑËΩ¶Ê£öÔºåËøòÊúâ‰∏ÄÂçäË∑ØÁ®ãÊó∂ÔºåÂ∞èË∑ØÂØπÈù¢Á™ÅÁÑ∂Ê≥õËµ∑‰∏ÄÈòµÂÜúÁî®‰∏âËΩÆÂ≠êÁöÑÂ£∞Âìç„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂê±ÂòéÔºÅ‚Äù

„ÄÄ„ÄÄÊûóÂÜõË∏©‰∫Ü‰∏ÄËÑöÂàπËΩ¶ÔºåÂè≥ËÑöÁÇπÂú∞ÔºåÁúØÁùÄÁúºÁùõÂêëÂâçÊñπÊúõÂéª„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂ∞±ÈÇ£‰∏™ÂÇªÔº¢Ôºå‰∏Ä‰ºöÁªôÊàëÂæÄÊ≠ªÊÄº‰ªñÔºÅ‚ÄùÈ™ëÂú®‰∏âËΩÆÂ≠ê‰∏äÁöÑÂ§ßÂ£ÆÔºåÊª°Âò¥ÊºèÈ£éÁöÑÂ§ßÂêº‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄÂ§ßÂ£ÆÂñäÂÆåÔºåÂØπÊñπ‰∏âËΩÆËΩ¶Ë∑ùÁ¶ªÊûóÂÜõ‰∏çË∂ÖËøá‰∫åÂçÅÁ±≥ËøúÂêéÂÅúÊªûÔºåÂá†‰πéÂêåÊó∂Ôºå‰∏ÄÂâØÊûÅ‰∏∫ÈúáÊíºÁöÑÁîªÈù¢Âá∫Áé∞Âú®ÊûóÂÜõÁúº‰∏≠ÔºÅ

„ÄÄ„ÄÄÂÜúÁî®‰∏âËΩÆÂ≠êÊòØÊë©ÊâòÂºèÁöÑÔºåÈ©¨ÂäõÂæàÂ∞èÔºåÂÖ∑‰ΩìÂ§ßÂ∞è‰πüÂ∞±Ë∑üË∑ØËæπÊãâÈªëÊ¥ªÁöÑÈÇ£Áßç‚ÄúÊë©ÁöÑ‚ÄùÂ∑Æ‰∏çÂ§öÔºåËÄåËøôËΩ¶ÁöÑËΩ¶ÊñóËΩΩÈáçÈáèÔºå‰º∞ËÆ°‰πüÂ∞±ËÉΩÊãâÂá†Ë¢ãÁôæÊñ§ÈáçÁöÑÂ§ßÁ±≥„ÄÇ

„ÄÄ„ÄÄ‰ΩÜ‰ªäÂ§©Ëøô‰∏™ÂÜúÁî®‰∏âËΩÆÂ≠êÂç¥Á™ÅÁ†¥‰∫ÜÊûÅÈôêÔºåÂ∞±‰∏çË∂≥‰∏ÄÁ±≥ÂçäÈïøÁöÑËΩ¶ÊñóÔºåÁ´üÁÑ∂ÂÆõËã•Êò•ËøêÁÅ´ËΩ¶ËΩ¶Âé¢‰∏ÄËà¨ÔºåÊã•Êå§ÂæóÂæÄ‰∏ãË∑≥‰∫∫ÔºÅ

„ÄÄ„ÄÄ‰∏Ä‰∏™Ôºå‰∏§‰∏™Ôºå‰∏â‰∏™.......

„ÄÄ„ÄÄÊï∞ÁßíËøáÂêéÔºåËΩ¶Êñó‰πã‰∏äÁ´üÁÑ∂Ë∑≥‰∏ãÊù•‰∏É‰∏™ÊàêÂπ¥‰∫∫ÔºÅ‰∏É‰∏™ÂïäÔºÅÂ§©Áü•ÈÅì‰ªñ‰ª¨ÊòØÊÄé‰πàÊå§‰∏äÂéªÁöÑÔºåÊ≠§Âú∫ÊôØÂç≥‰ΩøË∑üÂç∞Â∫¶‰∏âÂì•PK‰∏Ä‰∏ãÔºåÈÇ£‰ªñÂ¶à‰πü‰∏çÂ∑ÆÂï•‰∫ÜÔºÅ

„ÄÄ„ÄÄ‚ÄúÊàëÊìçÔºÅ‚ÄùÊûóÂÜõÊï∞ÁùÄÂØπÊñπË∑≥‰∏ãÊù•ÁöÑ‰∫∫ÔºåËÑ∏Ëâ≤Ë¢´Èõ∑ÁöÑÊúâÁÇπÊÉäÊÑï„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂëºÂï¶Âï¶ÔºÅ‚Äù

„ÄÄ„ÄÄÂ§ßÂ£ÆË∑≥‰∏ãÊë©ÊâòËΩ¶ÔºåÂè≥Êâã‰ªéËΩ¶Êñó‰∏≠ÊäΩÂá∫‰∏ÄÊääÁâáÂàÄÔºåÈöèÂç≥Â∏¶ÁùÄ‰∏É‰∏™‰∫∫ÔºåÊâãÈáåÊãøÁùÄÈìÅÊ£çÂ≠êÔºåÈïêÊääÂ≠êÔºåÈï∞ÂàÄÔºåËøòÊúâË°óÂ§¥ÊñóÊÆ¥‰∏≠ÁôæÂπ¥ÈöæÂæó‰∏ÄËßÅÁöÑÁÇâÈí©Â≠êÁ≠âÂºÇÊ†∑Âá∂Âô®ÔºåËúÇÊã•ÁùÄÂÜ≤ÂêëÊûóÂÜõ„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂí£ÂΩìÂΩìÔºÅ‚Äù

„ÄÄ„ÄÄÊûóÂÜõ‰∏ãËΩ¶ÔºåÂú®Ëá™Áü•Êó†Ê≥ïË∫≤ÈÅøËøôÂú∫ÊñóÊÆ¥‰πãÊó∂ÔºåÁ´ãÈ©¨ÂõûÊâã‰ªé‰∏âËΩÆËΩ¶‰∏äÊäΩÂá∫‰∏ÄÊ†πÂçäÁ±≥ÈïøÁöÑÁ©∫ÂøÉÈí¢ÁÆ°ÔºåÈöèÂêéÁúâÂ§¥ÈÉΩÊ≤°Áö±‰∏Ä‰∏ãÔºåËøàÊ≠•Â∞±ÂÜ≤Âêë‰∫∫Áæ§„ÄÇ

„ÄÄ„ÄÄÂèåÊñπÁ¢∞ËßÅÔºåÂü∫Êú¨Ê≤°ÊúâÂ∫üËØùÔºåÁõ¥Êé•Â∞±ÂºÄÊÄº„ÄÇ

„ÄÄ„ÄÄÂØπÊñπ‰∏Ä‰∏™ËÄÅÂÜúÔºåÊä°ÁùÄÈï∞ÂàÄÁõ¥Êé•Âà®ÂêëÊûóÂÜõÔºåËÄåÊûóÂÜõ‰æßË∫´‰∏ÄÈó™ÔºåÂè≥ËáÇÊëÜÂä®ÂπÖÂ∫¶ÂæàÂ∞èÔºå‰ΩÜÂè≥ÊâãÊî•ÁùÄÁöÑÈí¢ÁÆ°Âç¥Èó™ÁîµËà¨ÁöÑÊäΩÂú®‰∫ÜËÄÅÂÜúÁöÑÊâãËÖï‰∏ä„ÄÇ

„ÄÄ„ÄÄÂΩìÁöÑ‰∏ÄÂ£∞ÔºåËÄÅÂÜúÊú¨ËÉΩ‰∏ÄÁº©ÊâãÔºåÊûóÂÜõÊâãÊåÅÈí¢ÁÆ°ÂØπÁùÄ‰ªñËÑëË¢ãÔºåÁú®ÁúºÈó¥Â∞±ÊäΩ‰∫Ü‰∏â‰∏ãÔºåÁõ¥Êé•Â∞ÜÂÖ∂ÊîæÂÄí„ÄÇ

„ÄÄ„ÄÄÂÖ∂‰Ωô‰ºó‰∫∫ÂÜ≤‰∏äÔºåÊûóÂÜõÂ∑¶ÊâãÊäìËøá‰∏Ä‰∫∫ÁöÑËÑñÈ¢ÜÂ≠êÔºåÂÆõËã•ÊãéÁùÄÈ∏°Â¥ΩÂ≠ê‰∏ÄËà¨ÔºåÁõ¥Êé•Â∞ÜÂÖ∂ÊëÜÂú®Ë∫´ÂâçÔºåÈöèÂç≥‰ªñË∫´‰ΩìÊôÉ‰∫Ü‰∏Ä‰∏ãÔºåÂè≥ÊâãÊî•ÁùÄÈí¢ÁÆ°ÔºåÂØπÁùÄÊóÅËæπÁöÑÂ§ßÂ£ÆÔºåÂèçÊâãÂ∞±ÊäΩ‰∫ÜËøáÂéªÔºÅ

„ÄÄ„ÄÄ‚ÄúÂò≠ÔºÅ‚Äù

„ÄÄ„ÄÄÈí¢ÁÆ°ÊäΩÂú®Â§ßÂ£ÆÂò¥‰∏äÔºå‰ªñÁñºÁöÑ‰∏ÄËπ¶ÂçäÁ±≥È´ò„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂôºÈáåÂï™Âï¶ÔºÅ‚Äù

„ÄÄ„ÄÄÂØπÊñπÁ†∏‰∏ãÊù•ÁöÑÊ≠¶Âô®ÔºåÊ†πÊú¨Êó†Â§ÑË∫≤ÈÅøÁöÑÂπ≤Âú®ÊûóÂÜõÂíåÂØπÊñπÈÇ£‰∫∫ÁöÑË∫´‰∏ä„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂæÄ‰ªñÊâã‰∏äÁ†çÔºÅÂ∞±ÁÖß‰∏Ä‰∏áÂùóÈí±Âπ≤‰ªñ‰∫ÜÔºÅ‚ÄùÂ§ßÂ£ÆÊçÇÁùÄÂò¥ÔºåË∑≥ËÑöÂêºÈÅì„ÄÇ

„ÄÄ„ÄÄÊûóÂÜõÈ¢ùÂ§¥ÔºåËÉ≥ËÜäÂºÄÂßãÂÜíË°ÄÔºå‰ªñÂ∑¶ËÉ≥ËÜä‰∏ÄÁî©ÔºåÂè≥ËÖø‰∏ÄÊâ´ÔºåÁõ¥Êé•Â∞ÜÊäìÁùÄÁöÑÊ±âÂ≠êÁªäÂÄí„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊìç‰Ω†ÁéõÔºåÊàëÊãøÊû™ËØ¥ËØùÔºåÊãøÂàÄÂêÉÈ•≠ÁöÑÊó∂ÂÄôÔºå‰Ω†‰ª¨ËøòËπ≤Âú∞Ê≤üÂûÑÈáåÂî±‰∏úÊñπÁ∫¢Âë¢ÔºÅ‚ÄùÊûóÂÜõÊ†πÊú¨Ê≤°ÁÆ°ÂÖ∂‰ªñ‰∫∫ÔºåÂèåÊâãÊî•ÁùÄÈí¢ÁÆ°ÔºåËÉ≥ËÜäÂçØË∂≥Âä≤ÁöÑÂæÄÊä°‰∫ÜÊï∞‰∏ã„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂò≠ÔºÅ‚Äù

„ÄÄ„ÄÄ‚ÄúÂò≠ÔºÅ‚Äù

„ÄÄ„ÄÄ‚ÄúÂò≠ÔºÅ‚Äù

„ÄÄ„ÄÄ‰∏âÂ£∞ËÑÜÂìçÔºåÂú®ÊûóÂÜõË∫´‰∏ãËøô‰∫∫ÁöÑËÑëË¢ã‰∏äÔºåËÑñÂ≠ê‰∏äÔºåÂêéËÉå‰∏äÊé•ËøûÂìçËµ∑ÔºÅ

„ÄÄ„ÄÄ‚ÄúÂëºÂï¶Âï¶ÔºÅ‚Äù

„ÄÄ„ÄÄÊûóÂÜõÂøÉÈªëÊâãÁã†ÁöÑÂπ≤ÂÆåËøô‰∏â‰∏ãÔºå‰∫∫Áæ§È°øÊó∂Êï£ÂºÄÔºå‰ºó‰∫∫ÁúãÁùÄ‰ªñÁ®çÂæÆÊúâÈÇ£‰πàÁÇπÁäØÊÄµÔºÅ

„ÄÄ„ÄÄ‚ÄúÂî∞Âî∞ÔºÅ‚Äù

„ÄÄ„ÄÄ‰∏éÊ≠§ÂêåÊó∂ÔºåË°óÂè£Â§ÑÊúâÂõõÂè∞Âá∫ÁßüËΩ¶ÂåÜÂøôËµ∂Êù•ÔºåËøô‰∫õËΩ¶ÊîØÁùÄËøúÂÖâÁÅØÔºåÂÅúÂú®Ë∑ØËæπ„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂí£ÂΩìÔºÅ‚Äù

„ÄÄ„ÄÄËΩ¶Èó®Êé®ÂºÄÔºåÂº†Â∞è‰πêÊâØËÑñÂ≠êÂñäÈÅìÔºö‚ÄúÂÜõÔºåË∞Å‰ªñÂ¶àË¶ÅÂπ≤‰Ω†ÂïäÔºü‚Äù

„ÄÄ„ÄÄÂ§ßÂ£ÆÂõ¢‰ºô‰∏ÄÁúãË°óÂè£ÂÅú‰∫ÜÂõõÂè∞Âá∫ÁßüËΩ¶ÔºåÂêåÊó∂ÂèåÁúºÂèàË¢´Â§ßÁÅØÊôÉÁöÑÁúã‰∏çÊ∏ÖÊ•öÂº†Â∞èÊñåÂ∏¶Êù•Â§öÂ∞ë‰∫∫ÔºåÊâÄ‰ª•Ôºå‰ªñ‰ª¨Á¨¨‰∏ÄÊó∂Èó¥ÊéâÂ§¥Â∞±Ë∑ëÔºåËøûËÉΩÊãâ‰∏É‰∏™‰∫∫ÁöÑÁ•ûÂ•áÂÜúÁî®‰∏âËΩÆÂ≠êÈÉΩÊâî‰∏ã‰∫Ü„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂí£ÂΩìÔºÅ‚Äù

„ÄÄ„ÄÄÊûóÂÜõËÑ∏‰∏çÁ∫¢Ê∞î‰∏çÂñòÁöÑÂ∞ÜÈí¢ÁÆ°ÊâîËøõËá™Â∑±ÁöÑ‰∏âËΩÆËΩ¶ÔºåÈöèÂêé‰º∏ÊâãÁÜüÁªÉÁöÑÊë∏‰∫Ü‰∏Ä‰∏ãÂêéËÉå„ÄÇÊâãÊåáÁ¢∞Ëß¶ÁöÆËÇ§Ôºå‰ªñÊÑüËßâÂá∫ÂêéËÉåÊ≤°ÊúâÂàÄ‰º§Ôºå‰ΩÜÂõûÂ§¥ÂÜçÁúãÂè≥ËáÇÁöÑÊó∂ÂÄôÔºå‰∏Ä‰∏™‰∏çË∂≥ÂçäÊåáÈïøÁöÑÂàÄÂè£ÔºåÊµÅÁùÄË°ÄÔºåËÄåÁöÆËÇâÂ∑≤ÁªèÁøªÂºÄ‰∫Ü„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊ≤°‰∫ãÂÑøÂêßÔºü‚ÄùÂº†Â∞è‰πêÂëºÂìßÂ∏¶ÂñòÁöÑË∑ëËøáÊù•ÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÊ≤°‰∫ãÂÑøÔºåËÉ≥ËÜä‰∏äÂàí‰∫Ü‰∏Ä‰∏ã„ÄÇ‚ÄùÊûóÂÜõÊãøËµ∑ËΩ¶‰∏äÁöÑÈ§êÂ∑æÁ∫∏Ôºå‰∏Ä‰∏ãÊäΩÂá∫‰∫ÜÂçäÁõíÁöÑÂéöÂ∫¶Â†µÂú®‰∫Ü‰º§Âè£‰∏äÔºåÈöèÂç≥Êâ≠Â§¥ÂÜ≤ÁùÄÂº†Â∞è‰πêÈóÆÈÅìÔºö‚Äú‰Ω†ÈÉΩÂ∏¶Ë∞ÅËøáÊù•ÁöÑÔºü‚Äù

„ÄÄ„ÄÄ‚ÄúÂ∏¶‰∏™Â±ÅÔºåÂõõÂè∞Âá∫ÁßüËΩ¶ÂÖ®ÊòØÁ©∫ÁöÑÔºåÁé∞Âú®ÁöÑ‰∫∫ÔºåËÉΩÂÄüÁªô‰Ω†Èí±ÔºåÂ∞±ÁÆóÂ•ΩÂì•‰ª¨‰∫ÜÔºåÂì™ÊúâËøòËÉΩÂ∏ÆÂøôÂπ≤‰ªóÁöÑÔºü‚ÄùÂº†Â∞è‰πêÈöèÂè£Âõû‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄ‚ÄúË∞¢‰∫ÜÔºå‰πê‰πêÔºÅ‚ÄùÊûóÂÜõÊÑ£‰∫Ü‰∏Ä‰∏ãÔºåÈöèÂç≥ËÆ§ÁúüÁöÑËØ¥ÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúË∞¢ÁöÑ‰∫ãÂÑøÂõûÂ§¥ÂÜçËØ¥ÔºåËµ∞ÂêßÔºå‰∏äÂåªÈô¢ÁúãÁúãÔºÅ‚ÄùÂº†Â∞è‰πêÊãâÁùÄÊûóÂÜõÔºåÁªßÁª≠ËØ¥ÈÅìÔºö‚Äú‰ªñ‰ª¨ËøôÂ∏Æ‰∫∫ÔºåÂÖ®ÊòØÂë®ËæπÂÜúÊùëÁöÑÔºåÁõ∏‰∫íÈÉΩËÆ§ËØÜÔºå‰∏Ä‰ºöËØ¥‰∏çÂÆöÂè´Êù•Â§öÂ∞ë‰∫∫ÔºÅ‚Äù

„ÄÄ„ÄÄ‚Äú‰ªñ‰ª¨Ë∑üË∞ÅÁé©ÁöÑÔºü‚ÄùÊûóÂÜõÊÄùËÄÉ‰∏Ä‰∏ãÔºåÁõ¥Êé•ÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚Äú‰Ω†Ë¶ÅÂπ≤Âï•ÂïäÔºü‚ÄùÂº†Â∞è‰πê‰∏ÄÊÑ£„ÄÇ

„ÄÄ„ÄÄ‚ÄúËøôÁÇπÁ†¥‰∫ãÂÑø‰∏çÊï¥ÊòéÁôΩ‰∫ÜÔºåÊàëÁúãÊòØÊ≤°ÂÆåÊ≤°‰∫Ü‰∫Ü„ÄÇ‚ÄùÊûóÂÜõ‰ΩéÂ§¥ÂõûÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂÜõÔºåÁäØÂæó‰∏äÂêóÔºü‚ÄùÂº†Â∞è‰πê‰∏ÄÂê¨ËøôËØùÔºåÈ°øÊó∂Ê≤âÈªòÂá†ÁßíÂêéÁö±ÁúâÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂπ≤ÈÉΩÂπ≤‰∫ÜÔºå‰Ω†ËØ¥ÂíãÊï¥Ôºü‰ªäÂ§©Ë¶ÅÊòØÊ≤°‰∏™ÁªìÊûúÔºåÈÇ£ÊòéÂ§©ÊàëËøòËÉΩ‰∏çËÉΩÂπ≤Ê¥ª‰∫ÜÔºü‚ÄùÊûóÂÜõÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÂõû‰∫Ü‰∏ÄÂè•ÔºåÈöèÂç≥ÂÜçÊ¨°ÈóÆÈÅìÔºö‚Äú‰ªñ‰ª¨ÊòØË∑üË∞ÅÁé©ÁöÑÔºü‚Äù

„ÄÄ„ÄÄ‚ÄúÁéãÊ∂õ„ÄÇ‚ÄùÂº†Â∞è‰πêÊÄùËÄÉ‰∫Ü‰∏Ä‰∏ãÔºåÈöèÂêéËøòÊòØÂ¶ÇÂÆûÁõ∏Âëä„ÄÇ

„ÄÄ„ÄÄ‚Äú‰∫åÂçÅÂ§öÂ≤ÅÔºåËÑñÂ≠ê‰∏äÊåÇÁùÄ‰ΩõÁâåÂÑøÔºåÊ≤°‰∫ãÂÑøÊâãÈáåËøòÊÑøÊÑèÊêìÁùÄÁè†Â≠êÔºåÊòØ‰ªñÂêóÔºü‚ÄùÊûóÂÜõËÑë‰∏≠Áû¨Èó¥ÊÉ≥Ëµ∑Âú®‰∏ÉÂ§ÑËµ∞ÂªäÁ¢∞ËßÅÁöÑÈÇ£‰∏™ÈùíÂπ¥„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂØπÔºÅ‚Äù

„ÄÄ„ÄÄ‚Äú‰ªñ‰∏çË°åÔºåÊÆµ‰ΩçÂ§™‰ΩéÔºå‰ªñ‰∏äÈù¢ËøòÊúâ‰∫∫ÂêóÔºü‚ÄùÊûóÂÜõÊëáÂ§¥ÂÜçÈóÆ„ÄÇ

„ÄÄ„ÄÄ‚ÄúÂ§ßÂì•Ôºå‰Ω†Â§™ÁãÇÁÇπ‰∫ÜÂêßÔºü‚ÄùÂº†Â∞è‰πêÊÑ£‰∫Ü‰∏Ä‰∏ãÔºåÈöèÂç≥ÊÉäÊÑïÁöÑÈóÆÈÅì„ÄÇ

„ÄÄ„ÄÄ‚ÄúËøô‰∫ãÂÑøË∑ü‰Ω†ËØ¥‰∏çÊòéÁôΩÔºå‰∏Ä‰∏™ÊÆµ‰ΩçÔºå‰∏Ä‰∏™Ë∞àÊ≥ïÔºÅ‚ÄùÊûóÂÜõÂπ≤ËÑÜÁöÑÂõûÈÅì„ÄÇ

„ÄÄ„ÄÄ‚Äú‚Ä¶‚Ä¶ÁéãÊ∂õÊòØË∑üÊª°Âåó‰ºêÁé©ÁöÑÔºÅ‚Äù

„ÄÄ„ÄÄ‚Äú‰ªñÂú®Âì™ÂÑøÔºü‚Äù

„ÄÄ„ÄÄ‚ÄúÊª°Âåó‰ºêÊòØÊï¥Âª∫Á≠ëÁöÑÔºåÊâãÈáåÊúâËΩ¶ÈòüÔºå‰∫∫Â•ΩÂÉèÂú®Ê±üÂåóÊúõÊ±üÂà´ËãëÁöÑ‰∏âÊúüÂ∑•Âú∞ÈáåÂë¢ÔºÅ‚ÄùÂº†Â∞è‰πêÂõû‰∫Ü‰∏ÄÂè•„ÄÇ

„ÄÄ„ÄÄ‚ÄúË∞¢‰∫ÜÔºå‰Ω†Â∏ÆÊàëÊää‰∏úË•øÈÄÅÂõûÂéªÔºåÂõûÊù•ËØ∑‰Ω†ÂêÉÈ•≠ÔºÅ‚ÄùÊûóÂÜõÂê¨ÂÆå‰ª•ÂêéÊãç‰∫ÜÊãçÂº†Â∞è‰πêÁöÑËÇ©ËÜÄÔºåÈöèÂç≥ËΩ¨Ë∫´Â∞±Ëµ∞„ÄÇ

„ÄÄ„ÄÄ‰∏§ÂàÜÈíü‰ª•ÂêéÔºåÊûóÂÜõÂçï‰∫∫ÂçïÈ™ëÔºåÊâì‰∫Ü‰∏ÄËæÜÂá∫ÁßüËΩ¶ÔºåÁõ¥Â•îÊ±üÂåóÊúõÊ±üÂà´Ëãë„ÄÇ

ËøôÊ†∑‰∏ÄÊÆµÊñáÊú¨ÔºåËØÜÂà´‰∫∫ÂêçÁªìÊûúÊòØÔºö[ÊûóÂÜõ, Âè≤ÂØÜÊñØ, ÂÆâÂêâ‰∏ΩÂ®ú, Ëå±Ëéâ, ÈÉëË≠¶ÂÆò, ÊùéÂèî, Êìç‰Ω†Áéõ, ÈÉΩÊï¥Ë±Å, ÁéãÊ∂õ, ÁëüÊØî, Âº†Â∞è‰πê, Á±≥È´ò, Èí±Âπ≤‰ªñ, ÈªëÊâã, Âº†Â∞èÊñå, ËøûËÉΩÊãâ, ÈÉΩÂ∏¶Ë∞Å]
ËØ∑ÈóÆËØÜÂà´Âá∫ÁöÑËøô‰∫õ‰∏çÂáÜÁ°ÆÁöÑÔºåÂ∫îËØ•ÊÄéÊ†∑ÊâçËÉΩÂ∞ΩÈáèÊéíÈô§Ôºü
"
‰Ω†Â•ΩÔºåÊàëÊúâ‰∏™ÂæàÊ£òÊâãÁöÑÈóÆÈ¢òÔºå‰∏Ä‰∏™50MÁöÑStringÔºåÁªôhanlpËøõË°åÂàÜËØçÔºåhanlpÁöÑÊâÄÊúâÂàÜËØçÊñπÊ≥ïÈÉΩËØïËøá‰∫ÜÔºåÊÄªÊòØÊä•ÂÜÖÂ≠òÊ∫¢Âá∫ÔºåJVMËÆæÁΩÆÁöÑÊòØ1GÔºåÊÄé‰πàÂäûÔºü,"Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.seg.CRF.CRFSegment.atomSegmentToTable(CRFSegment.java:251)
    at com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:48)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at Test.main(Test.java:70)
"
ÈüµÊØçËØÜÂà´ÈóÆÈ¢ò,"Áî®ÁöÑhanlp-1.2.9+data-for-1.2.8-standardÔºåÁî®JavaËøêË°åÂ¶Ç‰∏ã‰ª£Á†Å‰Ωú‰∏∫ÊµãËØïÔºö
`String text=""‰æùÂ±±‰ºöÁï•Âè•ÁéáÂ≠¶Â≠ê"";`
`List<Pinyin> listPinyin = HanLP.convertToPinyinList(text);`
`for (Pinyin pinyin : listPinyin){`
`System.out.printf(""%s,"", pinyin.getYunmu());`
`}`
ÁªìÊûúÂç¥ÊòØ‚Äúi,an,ui,ue,u,v,ue,eng,‚ÄùÔºåÊúÄÂêé‚ÄúÂ≠ê‚ÄùÁöÑÈüµÊØçÈîô‰∫ÜÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÂë¢Ôºü
"
"‰ΩøÁî®HanLPÊê≠ÈÖçSolr 4.10ËàáTomcat 7.0.68ÔºåÂú®‰ΩøÁî®""analysis""Áî®‰æÜÈ©óË≠â‰∏≠ÊñáÂàÜË©ûÁöÑÁµêÊûúÔºåÂçªÂá∫Áèæ‰∏ãÂàóÈåØË™§Ë®äÊÅØ""null:java.lang.RuntimeException: java.lang.AbstractMethodError""","ÊÇ®Â•ΩÔºåÊàë‰ΩøÁî®HanLPÊê≠ÈÖçSolr 4.10ËàáTomcat 7.0.68ÔºåÂú®solr admin ‰ªãÈù¢‰ΩøÁî®""analysis""Áî®‰æÜÈ©óË≠â‰∏≠ÊñáÂàÜË©ûÁöÑÁµêÊûúÔºåÂçªÂá∫Áèæ‰∏ãÂàóÈåØË™§Ë®äÊÅØ""null:java.lang.RuntimeException: java.lang.AbstractMethodError""ÔºåË´ãÂïèÊòØ‰ªÄÈ∫ºÂéüÂõ†Âë¢ÔºüÊàë‰ΩøÁî®ÁöÑÊèí‰ª∂ÁâàÊú¨ÁÇ∫hanlp-portable-1.2.9.jar,hanlp-solr-plugin-1.0.3.jarÔºåÈÇÑË´ãÊÇ®ÂçîÂä©ÊàëÊéíÈô§ÈÄôÂÄãÂïèÈ°åÔºåË¨ùË¨ùÔºÅ
"
Áª¥ÁâπÊØîÂàÜÈöîËØÜÂà´‰∫∫ÂêçÁöÑÈóÆÈ¢ò,"Êñ∞ÂçéÁ§æ‰∏ìÁîµÔºàËÆ∞ËÄÖÁÜäÁê≥Ôºâ„ÄäÈ¨ºÂêπÁÅØ„Äã‰ΩúËÄÖÂº†ÁâßÈáéËÆ§‰∏∫ÁîµÂΩ±„Ää‰πùÂ±ÇÂ¶ñÂ°î„ÄãÊ∂âÂ´å‰æµÁäØËëó‰ΩúÊùÉÔºåÂ∞Ü‰∏≠ÂõΩÁîµÂΩ±ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÂíåÂØºÊºîÈôÜÂ∑ùËµ∑ËØâËá≥Âåó‰∫¨Â∏ÇË•øÂüéÂå∫‰∫∫Ê∞ëÊ≥ïÈô¢„ÄÇËÆ∞ËÄÖ20Êó•‰ªéÊ≥ïÈô¢Ëé∑ÊÇâÔºåÂú®ÂèóÁêÜËØ•Ê°àÂêéÔºåÂ∫îË¢´Âëä‰πã‰∏ÄÈôÜÂ∑ùÁî≥ËØ∑ÔºåÊ≥ïÈô¢ËøΩÂä†Ê¢¶ÊÉ≥ËÄÖÁîµÂΩ±ÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏„ÄÅ‰πêËßÜÂΩ±‰∏öÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏‰∏∫Êú¨Ê°àÂÖ±ÂêåË¢´Âëä„ÄÇ‰ªäÂπ¥1Êúà7Êó•Ôºå„ÄäÈ¨ºÂêπÁÅØ„Äã‰ΩúËÄÖÂº†ÁâßÈáéËµ∑ËØâÁß∞„ÄÇÁîµÂΩ±„Ää‰πùÂ±ÇÂ¶ñÂ°î„ÄãÁ≥ªÁî±„ÄäÈ¨ºÂêπÁÅØ‰πãÁ≤æÁªùÂè§Âüé„ÄãÊîπÁºñÊãçÊëÑËÄåÊàêÔºå‰ΩÜ„Ää‰πùÂ±ÇÂ¶ñÂ°î„ÄãÁöÑÊïÖ‰∫ãÊÉÖËäÇ„ÄÅ‰∫∫Áâ©ËÆæÁΩÆ„ÄÅÊïÖ‰∫ãËÉåÊôØÂùá‰∏éÂéüËëóÁõ∏Â∑ÆÁîöËøúÔºåË∂ÖÂá∫‰∫ÜÊ≥ïÂæãÂÖÅËÆ∏ÁöÑÂøÖË¶ÅÁöÑÊîπÂä®ËåÉÂõ¥ÔºåÊûÑÊàêÂØπÂéüËëóÁöÑÊ≠™Êõ≤ÂíåÁØ°ÊîπÔºåÁªôÂéüÂëäÈÄ†Êàê‰∫ÜÁ≤æÁ•û‰º§ÂÆ≥Ôºå‰æµÁäØ‰∫ÜÂéüÂëäÁöÑ‰øùÊä§‰ΩúÂìÅÂÆåÊï¥ÊùÉ„ÄÇÊïÖÂ∞Ü‰∏≠ÂõΩÁîµÂΩ±ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÂèäÈôÜÂ∑ùËØâËá≥Ê≥ïÈô¢ÔºåËØ∑Ê±ÇÊ≥ïÈô¢Âà§‰ª§‰∫åË¢´ÂëäÁ´ãÂç≥ÂÅúÊ≠¢‰æµÊùÉË°å‰∏∫ÔºåÂêëÂº†ÁâßÈáéÂÖ¨ÂºÄËµîÁ§ºÈÅìÊ≠â„ÄÅÊ∂àÈô§ÂΩ±ÂìçÔºåÂπ∂ËµîÂÅøÂº†ÁâßÈáéÊçüÂ§±100‰∏áÂÖÉ‰∫∫Ê∞ëÂ∏Å„ÄÇÊú¨Ê°àÁ´ãÊ°àÂêéÔºåË¢´Âëä‰πã‰∏ÄÈôÜÂ∑ùÁî≥ËØ∑ËøΩÂä†Ê¢¶ÊÉ≥ËÄÖÁîµÂΩ±ÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏„ÄÅ‰πêËßÜÂΩ±‰∏öÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏‰∏∫ÂÖ±ÂêåË¢´Âëä„ÄÇÈôÜÂ∑ùËÆ§‰∏∫Ê¢¶ÊÉ≥ËÄÖÁîµÂΩ±ÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏‰∏é‰πêËßÜÂΩ±‰∏öÔºàÂåó‰∫¨ÔºâÊúâÈôêÂÖ¨Âè∏‰∏∫ÁîµÂΩ±„Ää‰πùÂ±ÇÂ¶ñÂ°î„ÄãÂá∫ÂìÅÊñπÂç≥ÂΩ±ÁâáËëó‰ΩúÊùÉ‰∫∫ÔºåÂ∫îÂΩì‰Ωú‰∏∫ÂÖ±ÂêåË¢´ÂëäÂèÇÂä†Êú¨Ê°àËØâËÆº„ÄÇÂº†ÁâßÈáé‰∫¶ÂêåÊÑèÂ∞ÜÊ¢¶ÊÉ≥ËÄÖÁîµÂΩ±ÂÖ¨Âè∏Âíå‰πêËßÜÂΩ±‰∏öËøΩÂä†‰∏∫ÂÖ±ÂêåË¢´Âëä„ÄÇË•øÂüéÊ≥ïÈô¢Âú®ÂÖÖÂàÜËÄÉËôëÂéüË¢´ÂëäÊÑèËßÅÁöÑÂü∫Á°Ä‰∏äÔºå‰∏∫‰∫Ü‰æø‰∫éÊü•Ê∏ÖÊ°à‰ª∂‰∫ãÂÆûÔºåÊåâÁÖßÁõ∏ÂÖ≥Ê≥ïÂæãËßÑÂÆöÔºåÂ∑≤ÂêåÊÑèËøΩÂä†‰∏äËø∞‰∫åÂÖ¨Âè∏‰∏∫Êú¨Ê°àË¢´Âëä„ÄÇ

‰∏äÈù¢ËøôÊÆµÊñáÊú¨ÔºåÂú® PersonRecognition.Recognition.roleTag ÂêéÔºåÁÜäÁê≥ ÁöÑËßíËâ≤Ê†áÊ≥®‰∏∫ [ÁÜä B 885 D 16 C 4 E 4 K 1 ][Áê≥ D 511 E 355 C 89 L 1 ]ÔºåËøô‰∏§‰∏™ËØçÊòØ‰ªéËØçÂÖ∏ÈáåÊü•Âá∫Êù•ÁöÑÔºå‰ΩÜÊòØÂú® PersonRecognition.Recognition.viterbiExCompute ÂêéÂ∞Ü ÁÜäÁê≥ ÁöÑËßíËâ≤Ê†áÊ≥®Êîπ‰∏∫‰∫Ü ÁÜä/D ,Áê≥/L „ÄÇ

Ëøô‰∏™ÊòØËΩ¨ÁßªÊ¶ÇÁéáÁü©ÈòµÊúâÈóÆÈ¢òÂêóÔºü
"
Áî®Hanlp‰∏≠ÁöÑNLPTokenizerÂàÜËØçÊó∂ÔºåÂØπÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑ45000Êï∞ÊçÆÊ≤°ÈóÆÈ¢òÔºåÂΩìÂØπ50000Êï∞ÊçÆÂàÜËØçÂêéÔºåÂ∞±‰∫ßÁîüÁ©∫ÊåáÈíàÈîôËØØÔºÅ,"![image](https://cloud.githubusercontent.com/assets/17736168/13973846/ccb7b3a4-f0e1-11e5-8e26-f64a5a6d66d3.png)
![image](https://cloud.githubusercontent.com/assets/17736168/13973847/d2c1852c-f0e1-11e5-873e-9cc0a987cd0f.png)
ÊúâÁü•ÈÅìËøôÊòØ‰ªÄ‰πàÂéüÂõ†ÁöÑ‰πàÔºü
"
HanLPÈáåÊúâËá™ÂÆö‰πâËØçÊÄßÁöÑÂäüËÉΩ‰πàÔºüÊØîÂ¶ÇAnsjSeg‰∏≠ÂèØ‰ª•ÈÄöËøá FilterModifWord.insertStopNatures()Êù•Ëá™ÂÆö‰πâËØçÊÄß,"ÊàëÊÉ≥ÁªôÁî®Êà∑ËØçÂÖ∏‰∏≠ÁöÑËØçÂÆö‰πâ‰∏∫‚ÄúuserDefine‚ÄùËøô‰∏™ËØçÊÄßÔºåÊØîÂ¶ÇAnsjSeg‰∏≠ÂèØ‰ª•ÈÄöËøá FilterModifWord.insertStopNatures(‚ÄúuserDefine‚Äù)Êù•Ëá™ÂÆö‰πâËØçÊÄßÔºåHanLP‰∏≠Èô§‰∫Ü‰øÆÊîπTerm.enum‰ª£Á†ÅÂ§ñÔºåÊúâÊ≤°ÊúâÁõ¥Êé•ÂèØÁî®ÁöÑÊñπÊ≥ïÔºü
"
1.2.9Âá∫ÈîôÔºöjava.lang.ArrayIndexOutOfBoundsException: 64,"‰ª£Á†Å
try{
    termList = HanLP.segment(nr.toUpperCase());
}
catch (Exception e2) {
    e2.printStackTrace();
    continue;
}

Âá∫Èîô‰ø°ÊÅØ
java.lang.ArrayIndexOutOfBoundsException: 64
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:140)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:101)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)
"
pythonË∞ÉÁî®‰∫∫ÂêçËØÜÂà´Êé•Âè£ÁöÑÈóÆÈ¢ò,"Êàë‰∏çÂ§™Ê∏ÖÊ•öÊòØ‰∏çÊòØËøôÊ†∑Ë∞ÉÁî®
Name_rec = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')
Name_rec.seg(u'ÂåóÂ∑ùÊôØÂ≠êÂèÇÊºî‰∫ÜÊûóËØ£ÂΩ¨ÁöÑÈÄüÂ∫¶‰∏éÊøÄÊÉÖ3')
ËøôÊ†∑‰ºöÂá∫ÈîôÔºå‰∏çÁü•ÈÅìÊÄé‰πàÁî®Ëøô‰∏™Êé•Âè£
"
ËØ∑ÈóÆÊòØÂê¶ÊîØÊåÅurlËØÜÂà´Ôºü,"RT
"
baidu ÁõòÂ¢ôÂ§ñË∂ÖÁ∫ßÊÖ¢ËÉΩ‰∏çËÉΩÊèê‰æõÂà´ÁöÑÈìæÊé•?,"baidu ÁõòÂ¢ôÂ§ñË∂ÖÁ∫ßÊÖ¢ËÉΩ‰∏çËÉΩÊèê‰æõÂà´ÁöÑ‰∏ãËΩΩÊñπÊ≥ï?
"
ÁªèÂ∏∏ Java heap space,"‰Ω†Â•ΩÔºåÁ®ãÂ∫èË∑ë‰∏Ä‰ºö‰πãÂêéÔºåÂ∞±ÁªèÂ∏∏Êä•Ëøô‰∏™

```
java.lang.OutOfMemoryError: Java heap space
        at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:73)
        at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:245)
        at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:472)
```

ÊòØ‰∏çÊòØËøòË¶ÅÈÖçÁΩÆ‰ªÄ‰πàÔºü
"
‰øÆÊîπ‰∫Ühanlp.propertiesÁ¨¨‰∏ÄË°åÁöÑÁõÆÂΩïÊåáÂêëDATAÁöÑÁà∂ÁõÆÂΩïÔºåËøòÊòØ‰∏çËÉΩÁî®Âè•Ê≥ïÂàÜÊûê,"jpype._jexception.LinkageErrorPyRaisable: java.lang.ExceptionInInitializerError
Âè•Ê≥ïÂàÜÊûêËøêË°åÁöÑÊó∂ÂÄôÊä•Ëøô‰∏™ÈîôËØØÔºåËØ∑ÈóÆÊÄé‰πàËß£ÂÜ≥ÂëÄ„ÄÇ
-Djava.class.path=C:\hanlp\hanlp-1.2.8.jar;C:\hanlp ËøôÊòØHanLP.jarÂíåHanLP.propertiesÁöÑ‰ΩçÁΩÆ„ÄÇ
root=C:/hanlp1  ËøôÊòØDATAÁöÑÁà∂ÁõÆÂΩï„ÄÇ

C:\Users***\workspacex86\python1\src ËøôÊòØÈ°πÁõÆÁöÑÁõÆÂΩï
ËØ∑ÈóÆË¶ÅÊÄé‰πàËß£ÂÜ≥Âë¢ÔºåÂ§öË∞¢‰∫Ü~
"
Âä†ËΩΩbinÊ®°ÂûãÔºåÊèêÁ§∫outofmemory,"‰ΩÜÂä†ËΩΩtxtÊ®°ÂûãÊ≤°ÊúâÈóÆÈ¢ò.
ÁéØÂ¢ÉÔºömacÔºåidea
Ôºçxms‰πüË∞ÉÁöÑÂæàÂ§ßÔºå‰∏çÊòØÔºçxmsÁöÑÈóÆÈ¢ò„ÄÇ
crfÔºãÔºãÁâàÊú¨0.53
"
ÊúÄÂ§ßÁÜµ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®ÁöÑÂÆûÁé∞,"hankcs,‰Ω†ËØ¥ÁöÑÂ≠óÁ¨¶‰∏≤ÁâπÂæÅÊòØ‰ªÄ‰πàÔºü
"
viterbiÊâÄÊúâÊ¶ÇÁéáÂèñÂØπÊï∞,"Ê±ÇËß£HMMÊ®°ÂûãÔºåÊâÄÊúâÊ¶ÇÁéáËØ∑ÊèêÂâçÂèñÂØπÊï∞Ôºü‰ªÄ‰πàÂéüÁêÜ
"
HanLPÂ¶Ç‰ΩïÂú®Elasticsearch‰∏ä‰ΩøÁî®Ôºü,"ÂèØÂê¶Êèê‰æõÁõ∏Â∫îÊèí‰ª∂Ôºü
"
‰∫∫ÂêçËØÜÂà´Â§±Ë¥•Ôºö‰π†ËøëÂπ≥ÔºçÔºçÔºçgitÁâàÊú¨,"public class TestErrorSeg {
    public static void main(String[] args) {
        HanLP.Config.enableDebug();
        String str = ""‰π†ËøëÂπ≥ËÆøÁæé‰∏∫ÂÖ®ÁêÉÁªèÊµéÊ≥®ÂÖ•Êñ∞Âä®Âäõ"";
        List<Term> terms = HanLP.newSegment().enableCustomDictionary(true).enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).seg(
                str);
        System.out.println(terms);
    }
}

‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 22202445 ][‰π†ËøëÂπ≥ null][ËÆøÁæé L 3 ][‰∏∫ L 1200 K 920 C 184 D 144 M 40 E 21 ][ÂÖ®ÁêÉ L 11 K 1 ][ÁªèÊµé L 19 K 2 ][Ê≥®ÂÖ• K 3 L 2 ][Êñ∞Âä®Âäõ A 22202445 ][  A 22202445 ]
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.algoritm.Viterbi.computeEnumSimply(Viterbi.java:254)
    at com.hankcs.hanlp.recognition.nr.PersonRecognition.viterbiExCompute(PersonRecognition.java:171)
    at com.hankcs.hanlp.recognition.nr.PersonRecognition.Recognition(PersonRecognition.java:50)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:76)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at org.nlp.segment.TestErrorSeg.main(TestErrorSeg.java:12)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
"
Êï∞ÁªÑÈÅçÂéÜË∂äÁïåÔºçv1.2.8ÁâàÊú¨,"ÈúÄË¶ÅÂàÜËØçÁöÑÂ≠óÁ¨¶‰∏≤Ôºö‚ÄúÂèåÂçÅ‰∫å‚ÄùÂä†Ê≤πÊúâÊäòÊâ£ËøòÊúâÁ∫¢ÂåÖ
ÂàÜËØçÊñπÂºèÔºö List<Term> terms = HanLP.newSegment().enableCustomDictionary(true).enableAllNamedEntityRecognize(true).enableNumberQuantifierRecognize(true).seg(
                content.trim());

ÈîôËØØÊèêÁ§∫ÔºöException in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 17
    at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
    at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:95)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
    at org.nlp.sentiment.ModelUtilsByRuleMap.calSentimentScore(ModelUtilsByRuleMap.java:177)
    at org.nlp.sentiment.ModelUtilsByRuleMap.calEmotionType(ModelUtilsByRuleMap.java:210)
"
hanlpÂàÜËØçÔºåËØçÊÄßÊ†áÊ≥®‰ΩøÁî®ÁöÑËØ≠ÊñôÂ∫ìÊù•Ê∫êÔºü,"‰πãÂâçÂè™ÊòØÂú®ÁÆÄÂçï‰ΩøÁî®hanlpÔºåÁé∞Âú®ÊÉ≥ÂØπËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊúâÊõ¥Ê∑±ÁöÑ‰∫ÜËß£„ÄÇÊçÆÊàëÁêÜËß£ÔºåÂÉèÂàÜËØçÔºåËØçÊÄßÊ†áÊ≥®Á≠âÈúÄË¶ÅÊúâ‰∏Ä‰ªΩÊèêÂâçÊ†áÊ≥®Â•ΩÁöÑËØ≠ÊñôÂ∫ìËøõË°åËÆ≠ÁªÉÔºå‰∏çÁü•ÈÅìhanlpÁöÑÊù•Ê∫êÊòØ‰ªéÂì™ÈáåÂèñÂæóÁöÑÔºü

Êàë‰ªéhankcsÁöÑ‰∏Ä‰∫õÂõûÁ≠îÈáåÁü•ÈÅìÂ∫îËØ•ÊòØÁî®‰∫∫Ê∞ëÊó•Êä•ÁöÑÊñáÊú¨ËøõË°åËÆ≠ÁªÉÔºå‰ΩÜÊòØ‰∫∫Ê∞ëÊó•Êä•ÁöÑÊñáÊú¨ÊòØÁîüËØ≠ÊñôÂ∫ìÔºå‰∏çÁü•ÈÅìhanlpÊòØÊÄé‰πàÂ§ÑÁêÜÁöÑÔºü
"
ËØçÂÖ∏ÈáåÊî∂ÂΩïÂæàÂ§öÂä®ËØçÔºãÂêçËØçÂêàÊàêÁöÑËØçÊòØ‰∏∫‰∫Ü‰ªÄ‰πàÔºü,"ÊØîÂ¶Ç‚ÄùÊâìÁØÆÁêÉ‚ÄúËøôÊ†∑ÁöÑËØçÂá∫Áé∞Âú®Ê†∏ÂøÉËØçÂÖ∏ÈáåÔºåÊêûÂæóÂàÜËØçÊÄªÊòØ‰∏çÁêÜÊÉ≥ÔºåÁâπÂà´ÊòØÂè•Ê≥ï‰æùÂ≠òÂàÜÊûêÂØºËá¥‰πüÂæàÊ∑∑‰π±„ÄÇÊ≠£Â∏∏ËØ≠‰πâÈáåÔºåÂÉèËøôÊ†∑ÁöÑËØç‰∏çÊòØÂ∫îËØ•ÂàÜÂºÄËÄÉËôëÔºåËøôÊ†∑‰∏ªË∞ìÂÆæÊâçËÉΩÊòéÁ°Æ„ÄÇ
"
Ê±ÇÊïôËøûÊé•javaÁöÑclass‰∏¢Â§±‰∫ÜÔºåËØ•Â¶Ç‰ΩïÊâæÂõûÔºü,"ËøêË°åÊó∂Á™ÅÁÑ∂Âá∫Áé∞jpype._jexception.ExceptionPyRaisable: java.lang.Exception: Class com.hankcs.hanlp.HanLP not foundÁöÑÈîôËØØÔºå‰πãÂâç‰∏çÂ∞èÂøÉÂç∏ËΩΩ‰∫Üpython2Êç¢Êàê3ÔºåÁé∞Âú®ÂèàÊç¢Êàê2Â∞±ÂèòÊàêËøôÊ†∑‰∫ÜÔºåË∑™Ê±ÇÈ´ò‰∫∫ÁöÑËß£ÂÜ≥ÂäûÊ≥ï
"
jpypeÊèêÁ§∫‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂá∫Èîô,"ËØ∑ÈóÆjpype._jexception.VirtualMachineErrorPyRaisable: java.lang.OutOfMemoryError: Java heap space
‰∏∫‰Ωï‰ºöË∂ÖÊó∂Âë¢Ôºü
"
‰∏â‰∏™‰∫∫ÊÄªÊòØË¢´ÈîôËØØÂå∫ÂàÜ‰∏∫‚Äù‰∏â‚Äú‚Äù‰∏™‰∫∫‚Äù,"Âè•Â≠êÈáåÂè™Ë¶ÅÊúâ‚Äú‰∏â‰∏™‰∫∫‚ÄùËøôÊ†∑ÁöÑËØçÔºåÊÄªÊòØ‰ºöÈîôËØØÂàíÂàÜ‰∏∫‚Äú‰∏â‚ÄùÔºå‚Äú‰∏™‰∫∫‚Äù„ÄÇ

ÊàëÁúãCoreNatureDictionaryÈáå‚Äú‰∏™‰∫∫‚ÄùÁöÑËØçÈ¢ëÂæàÈ´òÔºåËØïÁùÄÊääÂÆÉÁöÑËØçÈ¢ëÈôçÂà∞ÔºëÔºåÂèëÁé∞‰πüÊ≤°Êúâ‰ΩúÁî®„ÄÇÂè™ÊúâÊää‚Äú‰∏™‰∫∫‚ÄùËøô‰∏™ËØçÊù°Âà†Èô§ÊâçËÉΩÊ≠£Á°ÆÂàÜËØç„ÄÇÂèØÊòØ‚Äú‰∏™‰∫∫‚ÄùÁ°ÆÂÆûÊòØÂèØ‰ª•‰Ωú‰∏∫ËØçÂá∫Áé∞ÔºåÂÉèËøôÊ†∑ÁöÑÈóÆÈ¢òÊúâÂäûÊ≥ïËß£ÂÜ≥ÂêóÔºü

ÊàëËØï‰∫ÜÂìàÂ∑•Â§ßÁöÑËØ≠Ë®Ä‰∫ëdemoÔºåÂèëÁé∞ÊòØÂèØ‰ª•Âå∫ÂàÜËøô‰∏§ÁßçÊÉÖÂÜµÁöÑ„ÄÇ
"
È´òÊâãÔºåÈóÆ‰∏ãËØçÂÖ∏Ëá™ÂÆö‰πâÂπ∂ÊÄé‰πàÂà∂‰Ωú Ôºü,
‰∫∫ÂêçËØÜÂà´ËßíËâ≤Ê†áÊ≥®ÁöÑÈóÆÈ¢ò,"ËæìÂÖ•ÔºöÂå∫ÈïøÂ∫ÑÊú®ÂºüÊñ∞Âπ¥Ëá¥Ëæû
Á≤óÂàÜÁªìÊûú[Âå∫Èïø/nnt, Â∫Ñ/ag, Êú®/ng, Âºü/n, Êñ∞Âπ¥/t, Ëá¥Ëæû/vi]
‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 42634591 ][Âå∫Èïø G 1 K 1 ][Â∫Ñ B 263 E 53 D 40 C 13 L 9 K 1 ][Êú® B 106 C 85 D 39 E 15 L 13 K 2 ][Âºü D 60 E 11 K 10 C 2 ][Êñ∞Âπ¥ L 47 K 2 Z 1 ][Ëá¥Ëæû L 13 ][  A 42634591 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,Âå∫Èïø/K ,Â∫Ñ/B ,Êú®/C ,Âºü/D ,Êñ∞Âπ¥/L ,Ëá¥Ëæû/L , /A]
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÂ∫ÑÊú® BC
ËØÜÂà´Âá∫‰∫∫ÂêçÔºöÂ∫ÑÊú®Âºü BCD

ÈóÆÈ¢òÔºö
1.Êó¢ÁÑ∂ËØÜÂà´Âá∫Ê≠£Á°Æ‰∫∫ÂêçÂ∫ÑÊú®Âºü(BCD)Ôºå‰∏∫‰ΩïËøòË¶ÅËØÜÂà´Âá∫Â∫ÑÊú®(BC)Âë¢ÔºüÁõ¥Êé•ÈÄâÂ∫ÑÊú®Âºü(BCD)‰Ωú‰∏∫‰∫∫ÂêçËØÜÂà´ÁªìÊûú‰∏çÂèØ‰ª•ÂêóÔºü
2. ‰∫∫ÂêçËØÜÂà´‰∏≠ÔºåËßíËâ≤AË°®Á§∫ÂÖ∂‰ªñËßíËâ≤„ÄÇÂú®ËØ≠Êñô‰∏≠ÔºåËØçÊ±á‚ÄúÊñ∞Âπ¥‚Äù„ÄÅ""Ëá¥Ëæû""Á≠âÂ∫îËØ•ÈÉΩÊúâÂèØËÉΩÂ≠òÂú®AËßíËâ≤ÊÉÖÂÜµÂêßÔºåÊÄé‰πàÂú®ËßíËâ≤ËßÇÂØü‰∏≠ÈÉΩÊ≤°ÊúâAÂèäÂÖ∂Âá∫Áé∞È¢ëÊ¨°Âë¢Ôºü‰ΩúËÄÖÊòØÊÄé‰πàÁîüÊàêËØçÊ±áÁöÑËßíËâ≤&ËØçÈ¢ëËØçÂÖ∏ÁöÑÔºüÊÑüËßâÊúâÂÖ≥AÁöÑÊï∞ÊçÆÈÉΩÂà†Èô§‰∫ÜÂêßÔºü
"
Âú∞ÂêçËØÜÂà´Ê®°ÂûãÁöÑ‰∏ÄÁÇπÁêÜËß£ÂíåÂõ∞ÊÉë,"ËæìÂÖ•Ôºö‚ÄúËìùÁøîÁªôÂÆÅÂ§èÂõ∫ÂéüÂ∏ÇÂΩ≠Èò≥ÂéøÁ∫¢Ê≤≥ÈïáÈªëÁâõÊ≤üÊùëÊçêËµ†‰∫ÜÊåñÊéòÊú∫‚Äù
ËæìÂá∫Ôºö[ËìùÁøî/nr, Áªô/p, ÂÆÅÂ§è/ns, Âõ∫ÂéüÂ∏Ç/ns, ÂΩ≠Èò≥Âéø/ns, Á∫¢/a, Ê≤≥Èïá/ns, ÈªëÁâõÊ≤üÊùë/ns, ÊçêËµ†/v, ‰∫Ü/ule, ÊåñÊéòÊú∫/n]
ÂÖ∂‰∏≠Ôºå„ÄêÁ∫¢Ê≤≥Èïá„ÄëËØÜÂà´ÈîôËØØ„ÄÇ

Êé¢ÂÖ∂ÂéüÂõ†ÔºåÈ¶ñÂÖàÂú®Á≤óÂàÜÈò∂ÊÆµÔºåÁªìÊûúÂ¶Ç‰∏ãÔºö
[Ëìù/a, Áøî/vg, Áªô/p, ÂÆÅÂ§è/ns, Âõ∫ÂéüÂ∏Ç/ns, ÂΩ≠/nz, Èò≥/ag, Âéø/n, Á∫¢/a, Ê≤≥Èïá/ns, Èªë/a, Áâõ/n, Ê≤ü/n, Êùë/n, ÊçêËµ†/v, ‰∫Ü/ule, ÊåñÊéòÊú∫/n]
ÁÑ∂ÂêéÔºåÂú∞ÂêçËßíËâ≤ËßÇÂØüÈò∂ÊÆµÔºö
[  S 1139590 A 23975 ][Ëìù C 29 B 1 D 1 ][Áøî C 50 D 10 A 5 ][Áªô A 453 B 79 X 6 ][ÂÆÅÂ§è Z 41339414 ][Âõ∫ÂéüÂ∏Ç Z 41339414 ][ÂΩ≠ C 85 ][Èò≥ D 1255 C 81 B 1 ][Âéø H 6878 B 25 A 23 D 19 X 3 ][Á∫¢ C 1000 B 46 A 3 ][Ê≤≥Èïá Z 41339414 ][Èªë C 960 B 25 ][Áâõ D 24 C 8 B 7 ][Ê≤ü H 107 D 90 E 36 C 27 B 14 A 3 ][Êùë H 4467 D 68 B 28 A 8 C 3 ][ÊçêËµ† B 10 A 1 ][‰∫Ü A 4115 B 97 ][ÊåñÊéòÊú∫ B 1 ][  B 1322 ]
ÊúÄÂêéÔºåÂú∞ÂêçËßíËâ≤Ê†áÊ≥®Èò∂ÊÆµÔºö
[ /A ,Ëìù/C ,Áøî/D ,Áªô/B ,ÂÆÅÂ§è/Z ,Âõ∫ÂéüÂ∏Ç/Z ,ÂΩ≠/C ,Èò≥/D ,Âéø/H ,Á∫¢/B ,Ê≤≥Èïá/Z ,Èªë/C ,Áâõ/D ,Ê≤ü/E ,Êùë/H ,ÊçêËµ†/B ,‰∫Ü/A ,ÊåñÊéòÊú∫/B , /A]

Áî±‰∫éÁ≤óÂàÜÂêéÔºå""Á∫¢Ê≤≥Èïá""Ë¢´ÂàÜ‰∏∫‰∫Ü""Á∫¢/Ê≤≥Èïá""ÔºåËÄå‰∏çÊòØ""Á∫¢/Ê≤≥/Èïá""ÔºåÈÇ£‰πàÂú®Âú∞ÂêçËßíËâ≤ËßÇÂØüÈò∂ÊÆµÔºåËØç‚ÄúÊ≤≥Èïá‚ÄùÂ∞±Ê≤°Ê≥ïÁî®Âú∞ÂêçËßíËâ≤ÁÆÄË°®‰∏≠ÁöÑËßíËâ≤Ê†áËÆ∞Ôºå‚ÄúÊ≤≥‚ÄùÂ∫îËØ•ÊòØD(‰∏≠ÂõΩÂú∞ÂêçÁ¨¨‰∫å‰∏™Â≠ó)Ôºå""Èïá""Â∫îËØ•ÊòØH(‰∏≠ÂõΩÂú∞ÂêçÁöÑÂêéÁºÄ)Ôºå""Ê≤≥Èïá""Â∫îËØ•ÊòØDHÂêßÔºüÂç≥Âú∞ÂêçÁöÑÂ∞æÈÉ®‰∏éÂêéÁºÄÊàêËØçÁöÑÁé∞Ë±°ÔºåÊòØ‰∏çÊòØË¶ÅÂçïÁã¨Âú®ËÆæÂÆö‰∏Ä‰∏™Êñ∞ÁöÑËßíËâ≤Ê†áËÆ∞ÔºüÁî±‰∫éÂú∞ÂêçËØÜÂà´ÊòØÂú®Âè•Â≠êÁ≤óÂàÜËØçÁöÑÂü∫Á°Ä‰∏äÂÅöÁöÑÔºåÊâÄ‰ª•Âú∞ÂêçËØÜÂà´ÁöÑËæìÂÖ•Â≠òÂú®ÂçïËØçÁöÑÊÉÖÂÜµÊòØÂú®ÊâÄÈöæÂÖçÁöÑÔºåÊÉ≥ÂøÖ‰πü‰ºöÂá∫Áé∞ËØ∏Â¶Ç‚Äú‰∏≠ÂõΩÂú∞ÂêçÁ¨¨‰∏Ä‰∏™Â≠ó‰∏éÁ¨¨‰∫å‰∏™Â≠óÊàêËØç‚ÄùÁöÑÊÉÖÂÜµÔºåÁé∞ÊúâÁöÑÂú∞ÂêçËßíËâ≤ÁÆÄË°®‰πüÊó†Ê≥ïÊ∂µÁõñ„ÄÇ

Â¶ÇÊûúÊàëÁêÜËß£Èîô‰∫ÜÔºåÂç≥""Ê≤≥Èïá""‰∏çÊòØDHÔºåÈÇ£ÂèàÊòØ‰ªÄ‰πàÂë¢ÔºüÊàëÂèÇÁúã‰∫Ü @hankcs ÁöÑblogÂÖ≥‰∫éÂú∞ÂêçËØÜÂà´ÁöÑÊñáÁ´†ÔºåÂÖ∂‰∏≠Âú∞ÂêçËßíËâ≤ËßÇÂØüÈò∂ÊÆµÁöÑ‚ÄúÊ≤≥Èïá‚ÄùÁöÑËßíËâ≤ÊòØ[H 1000]Ôºå‰∏ç‰ªÖÂ¶ÇÊ≠§Ôºå""ÂÆÅÂ§è""„ÄÅ""Âõ∫ÂéüÂ∏Ç""ÁöÑËßíËâ≤‰πüÊòØ[H 1000]ÔºåÊàëÁåúÊµã‰ΩúËÄÖÂèØËÉΩÊòØÁ¨îËØØÔºåÂú®HanLP‰ª£Á†Å‰∏≠H‰ª£Ë°®‰∏≠ÂõΩÂú∞ÂêçÂêéÁºÄÔºåËÄåG‰ª£Ë°®Êï¥‰∏™Âú∞ÂùÄÔºåÊàëÁåúÊµã‰ΩúËÄÖÂú®blog‰∏äÁöÑÊÑèÊÄùÊòØ‚ÄúÊ≤≥Èïá‚Äù„ÄÅ‚ÄúÂÆÅÂ§è‚Äù„ÄÅ‚ÄúÂõ∫ÂéüÂ∏Ç‚ÄùÁöÑËßÇÂØüËßíËâ≤ÈÉΩÊòØÁúü‰∏™Âú∞ÂùÄÔºåÂç≥G„ÄÇÂ¶ÇÊûúÊòØËøôÊ†∑ÔºåÈÇ£‰πà‰∏∫‰ªÄ‰πàÂú®ÊàëË∑ëÁöÑÁ®ãÂ∫è‰∏≠Ôºå‚ÄúÊ≤≥Èïá‚Äù„ÄÅ‚ÄúÂÆÅÂ§è‚Äù„ÄÅ‚ÄúÂõ∫ÂéüÂ∏Ç‚ÄùÁöÑËßÇÂØüËßíËâ≤ÈÉΩÊòØZÂë¢Ôºü‰ΩúËÄÖÊòØ‰∏çÊòØÂú®Âú∞ÂêçËæûÂÖ∏ns.txt‰∏≠‰∏çÂä†ÂÖ•‚ÄúÊ≤≥Èïá‚Äù„ÄÅ‚ÄúÂÆÅÂ§è‚Äù„ÄÅ‚ÄúÂõ∫ÂéüÂ∏Ç‚Äù‰Ωú‰∏∫Âú∞ÂêçÁöÑËßíËâ≤ÔºåËÄåÊòØÂè™Èù†Âú®Ê†∏ÂøÉËØçÂÖ∏‰∏≠Â∞ÜËøô‰∫õÂ∏∏Áî®Âú∞ÂêçËØÜÂà´Âá∫Êù•ÔºåÊòØËøôÊ†∑ÁöÑÊÑèÂõæÂêóÔºü
"
ÁøªËØë‰∫∫Âêç„ÄÅÊó•Êú¨ÂêçËØÜÂà´‰∏∫Âï•‰∏çÈááÁî®HMMÔºü,"‰∏≠ÂõΩ‰∫∫ÂêçËØÜÂà´ÈááÁî®HMMÊ®°ÂûãÔºåËÄåÁøªËØë‰∫∫Âêç„ÄÅÊó•Êú¨ÂêçËØÜÂà´ÈááÁî®ËßÑÂàôÊù•Â§ÑÁêÜ„ÄÇ
‰ΩúËÄÖÂú®ÁøªËØë‰∫∫Âêç„ÄÅÊó•Êú¨ÂêçËØÜÂà´Êó∂ÊúâËÄÉËôëÈááÁî®HMMÂ§ÑÁêÜÂêóÔºüÊòØ‰∏çÊòØÊïàÊûú‰∏çÂ•ΩËΩ¨ËÄåÈááÁî®ËßÑÂàôÊù•ËØÜÂà´Âë¢Ôºü
"
PlaceDictionaryË∞ÉÁî®ÁöÑÁñëÈóÆ,"Âè•Â≠ê""ËìùÁøîÁªôÂÆÅÂ§èÂõ∫ÂéüÂ∏ÇÂΩ≠Èò≥ÂéøÁ∫¢Ê≤≥ÈïáÈªëÁâõÊ≤üÊùëÊçêËµ†‰∫ÜÊåñÊéòÊú∫""Á≤óÂàáÂàÜÂêéÂæóÂà∞ ""[Ëìù, Áøî, Áªô, ÂÆÅÂ§è, Âõ∫ÂéüÂ∏Ç, ÂΩ≠, Èò≥, Âéø, Á∫¢, Ê≤≥Èïá, Èªë, Áâõ, Ê≤ü, Êùë, ÊçêËµ†, ‰∫Ü, ÊåñÊéòÊú∫]„ÄÇÂú®Âú∞ÂêçËØÜÂà´Èò∂ÊÆµÔºåÂú®PlaceRecognition.javaÁ¨¨109Ë°åÊ∂âÂèä‰ªéÂú∞ÂêçËØçÂÖ∏ns.txt‰∏≠ËØªÂèñÂú∞ÂêçÁöÑNSÊûö‰∏æÁ±ªÂûãÁöÑ‰ª£Á†ÅÔºåÂ¶Ç‰∏ãÔºö
EnumItem<NS> NSEnumItem = PlaceDictionary.dictionary.get(vertex.word);
ÈóÆÈ¢òÊòØÔºåÊàëÂèëÁé∞ÂΩìÁ®ãÂ∫èÊâßË°åÂà∞vertex.wordÊòØ‚ÄúÂÆÅÂ§è‚ÄùÊó∂ÔºåËøîÂõûÁöÑNSEnumItem‰∏∫nullÔºåËÄåÊàëÊü•‰∫Üns.txtÂèëÁé∞Â≠òÂú®ËÆ∞ÂΩï""ÂÆÅÂ§è G 351""„ÄÇËØ∑ÈóÆÊòØËØ•ËøîÂõûnullÂêóÔºüÊàëÊÄé‰πàËßâÂæóÁêÜÂ∫îËØ•ËøîÂõû""G 351""Âë¢ÔºüÂì™ÈáåÂá∫Áé∞ÈóÆÈ¢ò‰∫ÜÔºü
"
Update TransformMatrixDictionary.java,"ÂàÜÊØçtotalFrequencyË°®Á§∫ÊâÄÊúâÁä∂ÊÄÅÁöÑÊÄªÈ¢ëÊï∞ÔºåÊàëËßâÂæóÂ∫îËØ•ÊòØstart_probability[from]ÔºåÂç≥Áä∂ÊÄÅfromÁöÑÈ¢ëÊï∞„ÄÇ
"
ËØªÂèñÁä∂ÊÄÅËΩ¨ÁßªÁü©ÈòµÊ®°ÂûãÊó∂ÁöÑ‰ª£Á†ÅËÆ°ÁÆóÊúâËØØÂêß,"package com.hankcs.hanlp.dictionaryÁöÑclassÊñá‰ª∂TransformMatrixDictionary.javaÔºö
Á¨¨139-140Ë°åÔºö
double frequency = matrix[from][to] + 1e-8;
transititon_probability[from][to] = -Math.log(frequency / totalFrequency);
ËøôÊÆµ‰ª£Á†ÅÂ∫îËØ•ÊòØËÆ°ÁÆó‰∏§‰∏™Áä∂ÊÄÅfrom,toÁöÑËΩ¨ÁßªÊ¶ÇÁéáp( to | from )=#(from,to)/#(from)
ÂàÜÊØçtotalFrequencyË°®Á§∫ÊâÄÊúâÁä∂ÊÄÅÁöÑÊÄªÈ¢ëÊï∞ÔºåÂÜôÈîô‰∫ÜÂêßÔºüÊàëËßâÂæóÂ∫îËØ•ÊòØstart_probability[from]ÔºåÂç≥Áä∂ÊÄÅfromÁöÑÈ¢ëÊï∞„ÄÇ
Á¨¨140Ë°å‰ª£Á†ÅÂ∫îËØ•ÊòØÔºö
transititon_probability[from][to] = -Math.log(frequency / start_probability[from]);
"
"‰∏∫‰ªÄ‰πàÂú®Êú∫ÊûÑÂêçËØÜÂà´‰ª£Á†Å‰∏≠ÂØπnisËØçÊÄßhard code‰∫ÜK, 1000ÂíåD, 1000","‰∏∫‰ªÄ‰πàÂú®Êú∫ÊûÑÂêçËØÜÂà´‰ª£Á†Å‰∏≠ÂØπnisËØçÊÄßhard code‰∫ÜK, 1000ÂíåD, 1000Ôºå
ÂØºËá¥ÂàÜËØçÊúâÈóÆÈ¢ò
 ""2007Âπ¥1Êúà9Êó•ÔºåÈòøÈáåÂ∑¥Â∑¥ÈõÜÂõ¢Âú®‰∏äÊµ∑ÂÆ£Â∏ÉÊóó‰∏ãÂÖ¨Âè∏ÈòøÈáåËΩØ‰ª∂Ê≠£ÂºèÊàêÁ´ã„ÄÇ"",
N-ÊúÄÁü≠ÂàÜËØçÔºö[2007Âπ¥/t, 1Êúà/t, 9Êó•/t, Ôºå/w, ÈòøÈáåÂ∑¥Â∑¥ÈõÜÂõ¢/nt, Âú®/p, ‰∏äÊµ∑/ns, ÂÆ£Â∏É/v, Êóó‰∏ã/d, ÂÖ¨Âè∏ÈòøÈáåËΩØ‰ª∂/nt, Ê≠£ÂºèÊàêÁ´ã/v, „ÄÇ/w]
ÊúÄÁü≠Ë∑ØÂàÜËØçÔºö[2007/m, Âπ¥/qt, 1/m, Êúà/n, 9/m, Êó•/b, Ôºå/w, ÈòøÈáåÂ∑¥Â∑¥ÈõÜÂõ¢/nt, Âú®/p, ‰∏äÊµ∑/ns, ÂÆ£Â∏É/v, Êóó‰∏ã/d, ÂÖ¨Âè∏ÈòøÈáåËΩØ‰ª∂/nt, Ê≠£ÂºèÊàêÁ´ã/v, „ÄÇ/w]
"
HanLPËØçÈ¢ë,"HanLPËØçÈ¢ëÊÄé‰πàÁªüËÆ°ÔºårankÊ≤°ÊúâÂä†ÂÖ•ËØçÈ¢ëÊùÉÈáç„ÄÇÂ¶Ç‰ΩïÂÆûÁé∞ Ôºü
"
ËøõË°åÂàÜËØçÊµãËØïÔºåÊÄé‰πàÂ±èËîΩËá™Â∏¶ÁöÑËØçÂÖ∏ÁöÑÂΩ±Âìç,"ÊàëÊÉ≥Ë¶ÅÁî®SIGHAN Bakeoff 2005ÊµãËØï‰∏çÂêåÁöÑÂàÜËØçÁÆóÊ≥ïÁöÑÊïàÊûú„ÄÇËøôÈáåË¶ÅÂ±èËîΩHanlpËá™Â∏¶ÁöÑËØçÂÖ∏ÁöÑÂΩ±ÂìçÔºåÊàëÁúãdataÈáåÊúâÂ•ΩÂ§öËØçÂÖ∏ÔºåË¶ÅÊÄé‰πàÂÅöÊâçË°å„ÄÇ

ÁõÆÂâçÊàëÁöÑÊÉ≥Ê≥ïÊòØÁî®Ëá™ÂÆö‰πâÁöÑËØçÂÖ∏ÊõøÊç¢ÊéâCoreNatureDictionary.txtÔºå‰ΩÜÊòØ‰∏çÁü•ÈÅìÂÖ∂‰ªñËØçÂÖ∏ÂØπÂàÜËØçÊúâ‰ªÄ‰πàÂΩ±ÂìçÔºü
"
Merge pull request #2 from hankcs/master,"Merge 02/01/2016
"
Áî®Áõ∏ÂêåÁöÑËØçÂ∫ìÔºåÊØîÂØπ‰∫Ü1.2.6Âíå2.8ÔºåÂèëÁé∞6ÊïàÊûúÂ•Ω,"ÊØîÂ¶Ç2.8ÂàÜÂá∫4‰∏™‰ΩïÊñáÔºå2.6ÂàÜÂá∫‰ΩïÊñáÂÖ®Ôºåj‰ΩïÊñáÊ°ÇÔºå‰ΩïÊñáÁé≤Ôºå‰ΩïÊñáËé≤
"
Merge pull request #1 from hankcs/master,"update
"
ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÈóÆÈ¢ò," ÊñáÊú¨ = ""‰∫§ÊòìË¥¶Âè∑Ôºö96600009934569490‰∏≠ÂõΩÈì∂Ë°åËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÊµéÂçó"";

ÂàÜËØçÁªìÊûú [‰∫§Êòì/vn, Ë¥¶Âè∑/n, Ôºö/w, 96600009934569490‰∏≠ÂõΩÈì∂Ë°åËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÊµéÂçó/nt]

BasicTokenizer‰∏≠SEGMENT = HanLP.newSegment().enableAllNamedEntityRecognize(true).enableCustomDictionary(true);

Ë≤å‰ººÂè™ÂºÄÂêØÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÊàñÊòØËá™ÂÆö‰πâËØçÂÖ∏‰∏≠ÁöÑ‰∏Ä‰∏™Â∞±ËÉΩÂ∞ÜÊï∞Â≠óÂíåÊú∫ÊûÑÂêçÂàÜÂºÄÔºåËã•‰∏§‰∏™ÂêåÊó∂ÂºÄÂêØÂàôÂΩìÂÅö‰∏Ä‰∏™Êï¥‰ΩìÔºåËøô‰∏™ÈóÆÈ¢òÊÄé‰πàËß£ÂÜ≥Âë¢
"
Âª∫ËÆÆÂ¢ûÂä†Â≠óÁ¨¶Ê†áÂáÜÂåñÊñπÊ≥ï,"ÊääËã±Êñá„ÄÅÊï∞Â≠óÁöÑÂÖ®ËßíÂ≠óÁ¨¶ÈÉΩËΩ¨Êç¢‰∏∫ÂçäËßíÊ†ºÂºè„ÄÇ
"
jvmÂÜÖÂ≠òÈóÆÈ¢ò,"Êúâ‰∏™ÈóÆÈ¢òÂæàÂõ∞ÊÉëÔºåÊòØ‰∏çÊòØjvmÊØîËæÉÂç†ÂÜÖÂ≠òÔºåÊàëÂú®Áî®pythonÊó∂ÔºåËøòÊòØÊØîËæÉÂπ≥Á®≥ÁöÑÔºå‰ΩÜÊòØÂú®Áî®jvmË∞ÉÁî®hanlpÊó∂ÔºåÂ∞±‰ºöÈ£ôÂçá‰∏äÂéª
"
ËØçÂÖ∏ËØªÂèñÈîôËØØ,"‰∏ÄÊúà 19, 2016 3:55:39 ‰∏ãÂçà com.hankcs.hanlp.dictionary.CustomDictionary load
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏D:/Workspace/NLP/data/dictionary/custom/word.txtËØªÂèñÈîôËØØÔºÅjava.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.L

ÈÖçÁΩÆÊñá‰ª∂Â¶Ç‰∏ã

root=D:/Workspace/NLP/
# Ê†∏ÂøÉËØçÂÖ∏Ë∑ØÂæÑ

CoreDictionaryPath=data/dictionary/CoreNatureDictionary.txt
# 2ÂÖÉËØ≠Ê≥ïËØçÂÖ∏Ë∑ØÂæÑ

BiGramDictionaryPath=data/dictionary/CoreNatureDictionary.ngram.txt
# ÂÅúÁî®ËØçËØçÂÖ∏Ë∑ØÂæÑ

CoreStopWordDictionaryPath=data/dictionary/stopwords.txt
# Âêå‰πâËØçËØçÂÖ∏Ë∑ØÂæÑ

CoreSynonymDictionaryDictionaryPath=data/dictionary/synonym/CoreSynonym.txt
# ‰∫∫ÂêçËØçÂÖ∏Ë∑ØÂæÑ

PersonDictionaryPath=data/dictionary/person/nr.txt
# ‰∫∫ÂêçËØçÂÖ∏ËΩ¨ÁßªÁü©ÈòµË∑ØÂæÑ

PersonDictionaryTrPath=data/dictionary/person/nr.tr.txt
# ÁπÅÁÆÄËØçÂÖ∏Ë∑ØÂæÑ

TraditionalChineseDictionaryPath=data/dictionary/tc/TraditionalChinese.txt
# Ëá™ÂÆö‰πâËØçÂÖ∏Ë∑ØÂæÑÔºåÁî®;ÈöîÂºÄÂ§ö‰∏™Ëá™ÂÆö‰πâËØçÂÖ∏ÔºåÁ©∫Ê†ºÂºÄÂ§¥Ë°®Á§∫Âú®Âêå‰∏Ä‰∏™ÁõÆÂΩïÔºå‰ΩøÁî®‚ÄúÊñá‰ª∂Âêç ËØçÊÄß‚ÄùÂΩ¢ÂºèÂàôË°®Á§∫Ëøô‰∏™ËØçÂÖ∏ÁöÑËØçÊÄßÈªòËÆ§ÊòØËØ•ËØçÊÄß„ÄÇ‰ºòÂÖàÁ∫ßÈÄíÂáè„ÄÇ
# Âè¶Â§ñdata/dictionary/custom/CustomDictionary.txtÊòØ‰∏™È´òË¥®ÈáèÁöÑËØçÂ∫ìÔºåËØ∑‰∏çË¶ÅÂà†Èô§

CustomDictionaryPath=data/dictionary/custom/word.txt n;data/dictionary/custom/CustomDictionary.txt; Áé∞‰ª£Ê±âËØ≠Ë°•ÂÖÖËØçÂ∫ì.txt; ÂÖ®ÂõΩÂú∞ÂêçÂ§ßÂÖ®.txt ns; ‰∫∫ÂêçËØçÂÖ∏.txt; Êú∫ÊûÑÂêçËØçÂÖ∏.txt; ‰∏äÊµ∑Âú∞Âêç.txt ns;data/dictionary/person/nrf.txt nrf;
# CRFÂàÜËØçÊ®°ÂûãË∑ØÂæÑ

CRFSegmentModelPath=data/model/segment/CRFSegmentModel.txt
# HMMÂàÜËØçÊ®°Âûã

HMMSegmentModelPath=data/model/segment/HMMSegmentModel.bin
# ÂàÜËØçÁªìÊûúÊòØÂê¶Â±ïÁ§∫ËØçÊÄß

ShowTermNature=true
"
Âú®linux‰∏ãpythonÊó∂JVMË∑ØÂæÑÈóÆÈ¢ò,"ÊàëÁî®‰∫ÜÁªùÂØπË∑ØÂæÑÔºåËøòÊòØÊâæ‰∏çÂà∞Ôºåjpype._jexception.RuntimeExceptionPyRaisable: java.lang.RuntimeException: Class com.hankcs.hanlp.HanLP not foundÔºå‰∏çÁü•ÈÅìÊÄé‰πàËß£ÂÜ≥
"
ËØçÊÄßÊ†áÂáÜÂíå‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÈóÆÈ¢ò,"ÊÇ®ÁöÑËØçÊÄßÊ†áÂáÜÊòØËá™Â∑±ÂÆö‰πâÁöÑËøòÊòØÂèÇËÄÉÊüêÊ†áÂáÜÔºüË∞¢Ë∞¢
"
ËØ¢ÈóÆÊúâÂÖ≥Âü∫‰∫éÂ≠óÊ†áÊ≥®ÁöÑ2Èò∂HMMÊ®°ÂûãÁöÑÂ≠¶‰π†ËµÑÊñô,"ÊàëÁúãÂà∞ÂåÖ`package com.hankcs.hanlp.seg.HMM;`‰∏≠2-orderHMMÁöÑÂÆûÁé∞Ôºå‰ΩÜÊòØ‰ΩúËÄÖÂ•ΩÂÉèÊ≤°Êúâ‰ªãÁªçËøáÔºåÊúâÊ≤°Êúâ‰∏Ä‰∫õ‰π¶Èù¢ÁöÑËµÑÊñôorËß£ÈáäÂèØ‰æõÂèÇËÄÉÔºü
"
ËøêÁî®ÁÆÄÁπÅËΩ¨Êç¢Âà∞ÂÆâÂçìÂ∫îÁî®ÂΩì‰∏≠Êó∂txtÂ≠óÂ∫ìÁöÑÈóÆÈ¢ò„ÄÇ,"ÊàëÁé∞Âú®ÊääÊï¥‰∏™Â∑•ÂÖ∑ÁöÑjarÂåÖÂØºÂÖ•Âà∞‰∫ÜÂÆâÂçìÈ°πÁõÆÂΩì‰∏≠Ôºå‰∏çËøátxtÁ≠âÂ≠óÂ∫ìÊñá‰ª∂ÈÉΩËøòÂú®ÁîµËÑëÈáåÔºåÂú®java‰ª£Á†ÅÈáåË∞ÉÁî®ÁÆÄÁπÅËΩ¨Êç¢Ê≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜÊòØËØ∑ÈóÆÊÄé‰πàÊäätxtÊîæÂÖ•ÂÆâÂçìÂ∫îÁî®‰∏≠Ôºü
Âõ†‰∏∫jarÂåÖÂΩì‰∏≠ÁöÑË∑ØÂæÑÊòØÊ≠ªÁöÑÔºåÊàëÊîπ‰∏ç‰∫ÜÔºå‰∏çÁÑ∂Â∞±ÂèØ‰ª•ÊäätxtÊîæÂÖ•ÊâãÊú∫ÔºåË∑ØÂæÑÊîπÊàêÁõ∏Â∫îÁöÑ‰ΩçÁΩÆ‰∫Ü„ÄÇ
"
ËØçÂÖ∏‰∏≠FrequencyÊï∞ÂÄºÁöÑÊÑè‰πâ,"ËØ∑ÈóÆËØçÂÖ∏‰∏≠FrequencyÊï∞ÂÄºÂÖ∑Êúâ‰ªÄ‰πàÊÑè‰πâÔºåÊòØÊÄé‰πàÊù•ÁöÑÔºå‰ª•ÂèäÂÖ∂Â§ßÂ∞èÂØπÂàÜËØçÁªìÊûúÂÖ∑Êúâ‰ªÄ‰πàÂΩ±ÂìçÔºü
"
ÂÖ≥‰∫éÊ∑ªÂä†ËØçÂÖ∏ÁöÑÈóÆÈ¢ò,"ÊàëÊñ∞Ê∑ªÂä†‰∏Ä‰∏™ËØçÂÖ∏ÔºåÂá∫Áé∞Ëøô‰∏™ÈîôËØØ
‰∏•Èáç: Ëá™ÂÆö‰πâËØçÂÖ∏D:/hanlp/data/dictionary/custom/word.txtËØªÂèñÈîôËØØÔºÅjava.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.INT'LÔºåËøôÊòØÊÄé‰πàÂõû‰∫ãÂë¢
"
Ê±ÇÈóÆ‰ΩøÁî®Ëá™ÂÆö‰πâËæûÂÖ∏ÂØπÂàùÂßãÂàÜËØçÁªìÊûú‰øÆÊîπÁöÑÊú∫Âà∂,"v1.2.8ÁâàÊú¨‰∏ãÁöÑÊ†áÂáÜÂàÜËØçÁªìÊûúÔºö
String text = ""ÊîªÂüéÁãÆÈÄÜË¢≠ÂçïË∫´Áãó"";
System.out.println(HanLP.segment(text));
ÂàÜËØçÁªìÊûúÊòØÔºöÊîªÂüé/ÁãÆ/ÈÄÜË¢≠/ÂçïË∫´/Áãó

‰ΩÜËã•Â¢ûÂä†‰∫ÜËá™ÂÆö‰πâËØçÔºåÂ¶Ç‰∏ãÔºö
CustomDictionary.add(""ÂüéÁãÆ"",""nz 100000"");   //ÂÅáËÆæ""ÂüéÁãÆ""ÊòØËØç
CustomDictionary.add(""ÂçïË∫´Áãó"");
ÂàÜËØçÁªìÊûúÊòØÔºöÊîªÂüé/ÁãÆ/ÈÄÜË¢≠/ÂçïË∫´Áãó
ÂçïË∫´ÁãóÁöÑÂàÜËØçÊòØÂØπÁöÑÔºåÂèØÊòØ‰∏∫‰ªÄ‰πàÊ≤°ÊúâÊää‚ÄúÂüéÁãÆ‚ÄùÂàÜÊàê‰∏Ä‰∏™ËØçÂë¢Ôºü
ÊàëÁü•ÈÅìÁé∞Âú®ÁöÑHanLPÊ†áÂáÜÂàÜËØçÊòØÂÖàÂØπÂè•Â≠êÁ≤óÂàÜËØçÔºåÂÜçÂà©Áî®Ëá™ÂÆö‰πâËæûÂÖ∏ËøõË°å‰øÆÊ≠£„ÄÇÈÇ£‰πàÁé∞Âú®Ëá™ÂÆö‰πâËæûÂÖ∏‰øÆÊ≠£ÁöÑÂü∫Êú¨ÊÄùË∑ØÊòØ‰ªÄ‰πàÔºü(Áõ∏ÂÖ≥ÈÉ®ÂàÜ‰ª£Á†ÅÊàëÊ≤°ÁúãÊòéÁôΩ)
Ëá™ÂÆö‰πâËØçÁöÑËØçÈ¢ëÂØπ‰øÆÊ≠£ÊúâÂΩ±ÂìçÂêóÔºü(‰∏∫‰ªÄ‰πàÊàëÁªô""ÂüéÁãÆ""ÁöÑËØçÈ¢ëËÆæ‰∏∫100000ËøòÊòØÊ≤°Ê≥ïÂàÜÂá∫Êù•Ôºü)
"
Áª¥ÁâπÊØîÁÆóÊ≥ïÂàÜËØçÁöÑÂπ≥ÊªëÈóÆÈ¢ò,"MathTools.java‰∏≠ÊúâÈùôÊÄÅÂáΩÊï∞`calculateWeight(Vertex from, Vertex to)`ËÆ°ÁÆó‰∏§‰∏™ÁªìÁÇπÁöÑËΩ¨ÁßªÊ¶ÇÁéáÔºö
line 40: `double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`
Êó¢ÁÑ∂Êúâ‰∫ÜÂπ≥ÊªëÂèÇÊï∞dSmoothingParaÔºå‰∏∫‰ΩïË¶ÅÁî®dTempË∞ÉÊï¥nTwoWordsFreq / frequencyÁöÑÁªìÊûúÂë¢Ôºü
"
CharTableËΩ¨Êç¢ÊúâËØØ,"‰∏ãËΩΩÁöÑv1.2.8ÁâàÊú¨ÔºåCharTable.java‰∏≠Âà©Áî®CONVERTÂØπÂ≠óÁ¨¶ËΩ¨Êç¢ÔºåÂÅ∂ÁÑ∂ÂèëÁé∞Ôºö
ÂÖ®ËßíÂíåÂçäËßíÁöÑÈÄóÂè∑„ÄÅÈóÆÂè∑„ÄÅÊÑüÂèπÂè∑ÈÉΩ‰ºöËΩ¨Êç¢ÊàêÂÖ®ËßíÁöÑÂè•Âè∑„ÄÇ
ÂÖ∂‰ªñÁöÑËøòÊ≤°ÊµãËØï„ÄÇ
"
ËØçÊÄßÈîôËØØÈóÆÈ¢ò,"ÊØîÂ¶Ç‚ÄúËãèÂÆÅÁîµÂô®ÈõÜÂõ¢Â¢ûÊåÅËãèÂÆÅ‰∫ëÂïÜ0.4%ËÇ°‰ªΩ‚ÄùÂæóÂà∞ÁöÑ‚ÄúÂ¢ûÊåÅ‚ÄùÊòØnzÔºåÂ∫îËØ•ÊòØÂä®ËØçÔºåÊàëÊÉ≥‰øÆÊîπËøô‰∏™ÔºåÂ∫îËØ•ÊÄé‰πàÂÅö
"
ËØçÊÄßnx‰∏énzÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü,"Âú®Nature.java‰∏≠ËØ¥nzÊòØÂÖ∂‰ªñ‰∏ìÂêçÔºånxÊòØÂ≠óÊØç‰∏ìÂêçÔºåËøôÊòØ‰ªÄ‰πàÊÑèÊÄùÔºüÂè¶Â§ñÔºåÂú®ÂåÖcom.hankcs.hanlp.utilityÁöÑPredifine.javaÈ¢ÑÂÆö‰πâ‰∫Ü
`/**
             * ‰∏ìÊúâÂêçËØç nx
             */
            TAG_PROPER = ""Êú™##‰∏ì"";
`
ËÄåÊ≤°ÊúâÈ¢ÑÂÆö‰πânz„ÄÇ
ÊàëÂú®‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ∫ì‰∏≠ÂèëÁé∞‚Äú‰∫∫Ê∞ëÁΩë‚ÄùÁöÑËØçÊÄßÊòØnzÔºå‰∏çÊòØnt„ÄÇ
ÈÇ£‰πàÂú®Á®ãÂ∫è‰∏≠Ôºånt„ÄÅnz„ÄÅnxÂà∞Â∫ïÊòØÂ¶Ç‰ΩïÂå∫Âà´ÁöÑÔºü
"
ÂΩìÂàÜËØçÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•Êó∂ÔºåÁõ¥Êé•ÂØºËá¥TomcatÊ≠ªÊéâ,"ÈîôËØØÈáçÁé∞ÊñπÊ≥ïÔºö
1.‰ΩøÁî®hanlp-1.2.8-release.zip‰∏≠Â∏¶ÁöÑhanlp.propertiesÔºå‰ªÖ‰øÆÊîπrootÂ±ûÊÄß
2.‰ΩøÁî®data-for-1.2.8-standard.zipÔºàÂ¶ÇÊûúÁî®fullÁâàdateÂåÖÂàô‰∏ç‰ºöÂá∫Áé∞Ê≠§ÈîôËØØÔºâ

Âª∫ËÆÆÔºö
1.Â∫îÁî®ÂÜÖÈÉ®ÂèëÁîüÈîôËØØÊó∂‰∏çÂ∫îÂØºËá¥TomcatÊ≠ªÊéâÔºåÂª∫ËÆÆÂ¢ûÂä†Áõ∏Â∫îÈîôËØØÈ¢ÑÈò≤Â§ÑÁêÜÊú∫Âà∂ÊàñÂèãÂ•ΩÁöÑÂºÇÂ∏∏Êú∫Âà∂Ôºå‰æãÂ¶ÇÂä†ËΩΩÂâçÂÖàÂà§Êñ≠Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
2.standardÂíåfullÂàÜÂà´Êèê‰æõhanlp.propertiesÂèÇËÄÉÊñá‰ª∂

ÂèÇËÄÉÊó•ÂøóÂ†ÜÊ†à‰ø°ÊÅØÂ¶Ç‰∏ãÔºàTomcatÁâàÊú¨8.0.18Ôºå‰∏çÁü•‰∏éTomcatÊúâÊó†ÂÖ≥Á≥ªÔºâÔºö
12-Jan-2016 01:05:47.794 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 13663 ms
12-Jan-2016 01:08:31.672 SEVERE [http-apr-8080-exec-2] com.hankcs.hanlp.model.CRFSegmentModel.<clinit> CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ C:/xxxx/demo_prj/WebContent/WEB-INF/hanlp_data/data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåËÄóÊó∂ 9 ms
12-Jan-2016 01:08:31.679 INFO [Thread-3] org.apache.coyote.AbstractProtocol.pause Pausing ProtocolHandler [""http-apr-8080""]
12-Jan-2016 01:08:31.684 INFO [Thread-3] org.apache.catalina.core.StandardService.stopInternal Stopping service Catalina
12-Jan-2016 01:08:33.857 WARNING [localhost-startStop-2] org.apache.catalina.loader.WebappClassLoaderBase.clearReferencesThreads The web application [demo_prj] is still processing a request that has yet to finish. This is very likely to create a memory leak. You can control the time allowed for requests to finish by using the unloadDelay attribute of the standard Context implementation. Stack trace of request processing thread:
 java.lang.Object.wait(Native Method)
 java.lang.Thread.join(Thread.java:1245)
 java.lang.Thread.join(Thread.java:1319)
 java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
 java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
 java.lang.Shutdown.runHooks(Shutdown.java:123)
 java.lang.Shutdown.sequence(Shutdown.java:167)
 java.lang.Shutdown.exit(Shutdown.java:212)
 java.lang.Runtime.exit(Runtime.java:109)
 java.lang.System.exit(System.java:968)
 com.hankcs.hanlp.model.CRFSegmentModel.<clinit>(CRFSegmentModel.java:43)
 com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:49)
 com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
 org.apache.jsp.hanlp.index_jsp._jspService(index_jsp.java:315)
 org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
 javax.servlet.http.HttpServlet.service(HttpServlet.java:725)
 org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:431)
 org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:396)
 org.apache.jasper.servlet.JspServlet.service(JspServlet.java:340)
 javax.servlet.http.HttpServlet.service(HttpServlet.java:725)
 org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291)
 org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
 org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
 org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239)
 org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
 org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219)
 org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106)
 org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
 org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142)
 org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79)
 org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:610)
 org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88)
 org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:516)
 org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1086)
 org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:659)
 org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:285)
 org.apache.tomcat.util.net.AprEndpoint$SocketProcessor.doRun(AprEndpoint.java:2431)
 org.apache.tomcat.util.net.AprEndpoint$SocketProcessor.run(AprEndpoint.java:2420)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
 java.lang.Thread.run(Thread.java:745)
12-Jan-2016 01:08:33.919 INFO [Thread-3] org.apache.coyote.AbstractProtocol.stop Stopping ProtocolHandler [""http-apr-8080""]
"
Ê†∏ÂøÉËØçÂÖ∏ÁîüÊàêÁöÑÁ§∫‰æã‰ª£Á†ÅÂá∫Áé∞ÈîôËØØ,"HanLPÁâàÊú¨v1.2.8ÔºåÁ§∫‰æã‰ª£Á†ÅÊòØÈ°πÁõÆwikiÈ°µÈù¢Áî®‰∫éËÆ≠ÁªÉHMM-NGramÊ®°ÂûãÁöÑ‰ª£Á†ÅÔºö
`
        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();
        CorpusLoader.walk(""path/to/your/corpus"", new CorpusLoader.Handler()
        {
            @Override
            public void handle(Document document)
            {
                dictionaryMaker.compute(document.getComplexSentenceList());
            }
        });
        dictionaryMaker.saveTxtTo(""data/test/CoreNatureDictionary"");
`
ÈîôËØØÂéüÂõ†ÊòØdictionaryMaker.compute()ËæìÂÖ•ÁöÑÈìæË°®Â≠òÂú®CompoundWordÔºåËÄåPrecompile.java‰∏≠95Ë°åÔºö
`
    public static Word compile(IWord word)
    {
        return compile((Word)word);
    }`
Âº∫Ë°åÂ∞ÜwordËΩ¨‰∏∫WordÊâÄËá¥„ÄÇ

ÊâÄ‰ª•ÊàëÊÉ≥ÈóÆÔºåHanLPÁöÑAPI dictionaryMaker.compute()‰ª•ÂêéÊâìÁÆóÊé•ÂèóCompoundWordÂêóÔºüÁé∞Âú®wikiÈ°µÈù¢ÁöÑË∞ÉÁî®‰ºöÂá∫ÈóÆÈ¢ò„ÄÇ
"
ÂÖ≥‰∫éÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÈ´òÊÄßËÉΩ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÂô®ÁöÑÈóÆÈ¢ò,"ÊÇ®Â•ΩÔºåËØ∑ÈóÆNNParserModel.txt.binÊòØÂ¶Ç‰ΩïËÆ≠ÁªÉÂá∫Êù•ÁöÑÔºåËÆ≠ÁªÉÈÉ®ÂàÜÁöÑ‰ª£Á†ÅÂÖ∑‰ΩìÂú®Âì™
"
‰∏≠ÊñáÊï∞Â≠óÂàáÊñ≠,"hanlp.ViterbiSegment(u‚ÄòÂçÅ‰∫å„ÄÅÂÆåÊàêÊÄªÁªèÁêÜ‰∫§ÂäûÁöÑÂÖ∂‰ªñÂ∑•‰Ωú„ÄÇ‚Äô)

ÂçÅ m
‰∫å m
„ÄÅ w
ÂÆåÊàê v
ÊÄªÁªèÁêÜ nnt
‰∫§Âäû vn
ÁöÑ ude1
ÂÖ∂‰ªñ rzv
Â∑•‰Ωú vn
„ÄÇ w
"
ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏‰øÆÊîπÂêéÔºåÁºñËØëÁöÑÊñá‰ª∂ÂèòÂ§ß‰∫Ü,"‰∏∫‰ªÄ‰πàÊàëËá™Â∑±‰øÆÊîπËá™ÂÆö‰πâËØçÂÖ∏ÂêéÔºåÈáçÊñ∞ÁºñËØëÔºåÊñá‰ª∂ÊØî‰ª•ÂâçÂ§ßÂæàÂ§öÂë¢ÔºåÈöæÈÅìÁºñËØëÊó∂ÊúâÁâπÊÆäÊñπÊ≥ï
"
ÂÖ≥‰∫éÂè•Ê≥ïËß£ÊûêÊ®°ÂûãÁöÑÈóÆÈ¢ò,"‰Ω†Â•ΩÔºÅËØ∑ÈóÆÂè•Ê≥ïËß£ÊûêÊ®°ÂûãËÉΩÂê¶ÂºÄÊîæÔºåÂ≠¶ÁîüÂú®ÂÅöÂè•Ê≥ïËß£ÊûêË∞ÉËØïÊ®°ÂûãÊó∂ÔºåÁº∫Â∞ëÊ®°ÂûãÁ®ãÂ∫èÊó†Ê≥ïË∑ëÔºåÈùûÂ∏∏ÊÑüË∞¢ÔºÅ
"
ËÉΩÂê¶Êèê‰æõ‰∏≠ÂõΩÂ∞ëÊï∞Ê∞ëÊóèÁöÑÂßìÂêçÁöÑÊãæÂèñÔºåÁ≤æÂ∫¶Â¶Ç‰Ωï„ÄÇ,"ÊÇ®Â•ΩÔºåÁõÆÂâç‰ΩøÁî®ÊÇ®ÁöÑÈ°πÁõÆÊµãËØï‰∏≠ÂõΩÂ∞ëÊï∞Ê∞ëÊóèÂßìÂêçÊãæÂèñÊÉÖÂÜµÔºåÁ≤æÂáÜÂ∫¶‰∏çÊòØÂ§™Â•Ω„ÄÇ
Â¶ÇÁª¥Êóè„ÄÅËíôÊóè„ÄÅËóèÊóè‰∫∫ÁöÑÂßìÂêçÔºåÂ∏åÊúõËÉΩÂ§ü‰∏∫‰∏≠ÂõΩÁöÑ‰∫∫ÂêçÁß∞ÊèêÂèñ‰ºòÂÖàÂÅö‰∏ÄÂÆöÁöÑ‰ºòÂåñÔºåË∞¢Ë∞¢„ÄÇ
"
ÊÄé‰πàÂÖ≥ÊéâextractKeyword‰∏≠ÁöÑËæìÂá∫ÂëÄÔºü,"ÊÉ≥ÂáèÂ∞ëÁ≥ªÁªüÁöÑÂºÄÈîÄ„ÄÇ„ÄÇ„ÄÇ
"
.jarÈúÄË¶ÅÊîæÂú®Âì™ÈáåÂëÄÔºü,
‰æùÂ≠òÂàÜÊûêÊó∂ËØçÊÄß‰∏∫‚Äúx‚ÄùÊó∂Êä•Èîô„ÄÇ,"NeuralNetworkParser.java 461Ë°å Âú®ËØçÊÄß‰∏∫""x""Êó∂postags_alphabet.idOf(data.postags.get(i))ËøîÂõûnull
Ë≤å‰ººÊòØÂ≠óÂÖ∏Ê†ëÈáåÊ≤°ÊúâÂ≠ò""x""Ëøô‰∏™ËØçÊÄßÊ†áËÆ∞,Ê®°ÂûãÈáå‰∏çÂ≠òÂú®‚Äúx‚ÄùËøô‰∏™ËØçÊÄßÔºü
"
ÂÖ≥‰∫éÂè•Ê≥ïÂàÜÊûêÁöÑÈóÆÈ¢ò,"ÊàëÁé∞Âú®ÂèØ‰ª•Âà©Áî®hanlpËøõË°åÂè•Ê≥ïÂàÜÊûêÔºåÊàëÁúãÂà∞‰Ω†Âè¶Â§ñ‰∏Ä‰∏™È°πÁõÆhttps://github.com/hankcs/MainPartExtractorÔºåÊòØÂÖ≥‰∫éÊèêÂèñÂè•Â≠ê‰∏ªÂπ≤ÁöÑÔºåÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ãÔºåÊàëÂæóÂà∞Âè•Ê≥ïÂàÜÊûêÁªìÊûú‰ª•ÂêéÔºåÊàëÊÄé‰πàÊù•ÊèêÂèñÂè•Â≠ê‰∏ªÂπ≤Âë¢
"
Â•≥Â£´„ÄÅÂÖàÁîü„ÄÅËÄÅÂ∏àÔºàÂßì+ËÅå‰ΩçÔºâ‰πãÁ±ªÁöÑ‰∫∫ÂêçÊó†Ê≥ïËØÜÂà´,"‰æãÂ¶ÇÔºö
ËµµÁßëÈïø 
ÊùéÂ•≥Â£´ 
Êú±Â•≥Â£´ 
Âº†ËÄÅÂ∏à 
Âº†Â•≥Â£´ 
‰æØÂ•≥Â£´
Áõ∏‰ººÁöÑÂ∫îËØ•ËøòÔºöÁéãÂåªÁîü„ÄÅÂë®ÂæãÂ∏àÁ≠â 
ËøôÁÇπÊòØÂê¶ÂèØ‰ª•ÊîπËøõ‰∏ã
"
Â¶Ç‰ΩïÂú®python‰∏≠‰ΩøÁî®CRFÂàÜËØçÔºü,"Â∞±ÊòØÂú®JClassÂØºÂÖ•ËøôÈáåÔºåÊàë‰∏çÊ∏ÖÊ•öÂØºÂÖ•Âì™‰∏™ÂèØ‰ª•‰ΩøÁî®CRFÂàÜËØç
"
ÊÄé‰πàÂà†Èô§ÁºìÂ≠òÔºü,"```
       Êú¨‰∫∫‰ª•ËÆ≠ÁªÉÊñ∞ÁöÑËØçÂÖ∏ÔºåÊÄé‰πàÂà†Èô§È°πÁõÆ‰∏≠ÁöÑÁºìÂ≠ò‰ΩøÂÖ∂ÁîüÊïàÔºüË∞¢Ë∞¢
```
"
‰∫∫ÂêçËØÜÂà´ÈîôËØØ,"ÂèçÈ¶à‰∏ÄÈÉ®ÂàÜ‰∫∫ÂêçËØÜÂà´ÁöÑÈóÆÈ¢ò:
ÊàëÁà±Êù®‰∫ëÂÆµ
Èæô‰∏éÂá§ÁöÑ‰º†ËØ¥
Èôå‰∏äËä±ÂºÄÁºìÁºìÂΩí
È±ºÂíåÊ∞¥ÁöÑÊïÖ‰∫ã
ÈôïË•øÊ¶ïÊÄ°ÂÖ¨Âè∏
ÈôàÈõØÊô¥Ë¶ÅÂä™Âäõ
"
python Ë∞ÉÁî®Êó∂ÂÄôÂá∫Áé∞ÁöÑ‰π±Á†ÅÈóÆÈ¢ò,"Êú¨‰∫∫ÁéØÂ¢ÉÊòØwin7 64bitÔºåpython2.7.*ÁâàÊú¨ÔºåÊåâÁÖßhttp://www.hankcs.com/nlp/python-calls-hanlp.htmlÁî®ÁöÑÊó∂ÂÄôÔºåÂú®ËæìÂá∫print(HanLP.segment('‰Ω†Â•ΩÔºåÊ¨¢ËøéÂú®Python‰∏≠Ë∞ÉÁî®HanLPÁöÑAPI'))Êó∂Âá∫Áé∞‰π±Á†ÅÔºåÈóÆ‰∏ãÊÄé‰πàÂèØ‰ª•Ëß£ÂÜ≥ÔºåÁé∞Âú®Â∞±ÊòØÂç°Âú®<class 'jpype._jclass.java.util.ArrayList'>Ôºà(HanLP.segment('‰Ω†Â•ΩÔºåÊ¨¢ËøéÂú®Python‰∏≠Ë∞ÉÁî®HanLPÁöÑAPI'))ÁöÑÊï∞ÊçÆÁ±ªÂûãÊòØ<class 'jpype._jclass.java.util.ArrayList'>ÔºâÊï∞ÊçÆÁ±ªÂûãÂæÄpythonËΩ¨ÂåñÊó∂‰∏çÁü•ÈÅìÊÄé‰πàÂÅöÔºåÂØπjava‰∏ÄÁÇπÈÉΩ‰∏ç‰∫ÜËß£„ÄÇ
"
‰Ω†Â•ΩÔºåÊúÄËøëÈÅáÂà∞‰∏Ä‰∏™ÂÖ≥‰∫éhanlp.propertiesÁöÑÈóÆÈ¢ò,"Êàë‰ΩøÁî®ÁöÑIDEÊòØEclipseÔºåÈÖçÁΩÆÂ•ΩjarÂåÖÂíåhanlp.propertiesÊñá‰ª∂ÂêéÔºåËÉΩÂ§üÊ≠£Â∏∏‰ΩøÁî®Ôºå‰ΩÜÂú®‰∏ãÊ¨°ÊâìÂºÄÁöÑÊó∂ÂÄôÔºåÂÅ∂Â∞î‰ºöÂá∫Áé∞hanlp.propertiesÊñá‰ª∂Êó†ÁºòÊó†ÊïÖÁöÑÊ∂àÂ§±ÁöÑÈóÆÈ¢òÔºåËøô‰∏™ÊòØÊÄé‰πà‰∏ÄÂõû‰∫ãÂïäÔºü
"
ÂÖ≥‰∫éËá™ÂÆö‰πâËØçÂÖ∏ÁöÑÈóÆÈ¢ò,"ÊÇ®Â•Ω ÊàëÈÖçÁΩÆÂ•Ω‰∫ÜpropertyÔºå‰ΩøÁî®CustomDictionary.add(""ÊîªÂüéÁãÆ"");ÂíåCustomDictionary.insert ÊñπÊ≥ïÊèíÂÖ•ÔºåËøô‰∏§ÁßçÊñπÊ≥ïÊúâ‰ªÄ‰πàÂå∫Âà´ÂêóÔºüË≤å‰ººÈÉΩÊòØÁºìÂ≠òÁöÑÂΩ¢ÂºèÔºåÂΩìÊúâinsertËØ≠Âè•Êó∂ËÉΩÂ§üÊ≠£Á°ÆÂàÜËØçÔºåÂéªÊéâËøôÂè•ÈáçÊñ∞ËøêË°åÂ∞±‰∏çË°å‰∫Ü
"
ÂÖ≥‰∫éDemoSuggesterÁöÑÊúÄÁõ∏‰ººÈóÆÈ¢ò,"ÊàëÈááÁî®ÁöÑÊòØ3.3.1ÁöÑÁâàÊú¨,DemoSuggesterÁ±ª‰∏≠Êü•ÊâæÁõ∏ÂÖ≥Â∫¶ÁöÑÂè•Â≠ê,
ÊØîÂ¶ÇËØ¥:[‰ªäÂ§©ÊàëÂêÉÈ•≠‰∫Ü,‰ªäÂ§©ÊàëÊ≤°ÂêÉÈ•≠]
Êü•ÊâæÂÖ≥ÈîÆËØçÊòØÂêÉÈ•≠,2‰∏™Âè•Â≠êÈÉΩÂá∫Êù•‰∫Ü,ËÉΩÂÅöÂê¶ÂÆöËØçËøáÊª§Âêó?
Âú®ÂÖ∂Ê¨°ÊàëÊü•ÊâæÊ≤°ÂêÉÈ•≠,ÂêÉÈ•≠‰πü‰∏çÂõ†ËØ•Âá∫Êù•.
"
ÊÇ®Â•ΩÔºåÊàëÂú®‰ΩøÁî®ÊúÄÊñ∞ÁâàÊú¨1.2.7ÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÁöÑÊó∂ÂÄôÈÅáÂà∞‰∏Ä‰∏™bug,"‰ΩøÁî®ÁöÑÊòØCRFÁöÑ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûêÔºåÂá∫Áé∞‰∫ÜÁ©∫ÊåáÂêëÂºÇÂ∏∏Ôºö
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.exactMatchSearch(DoubleArrayTrie.java:674)
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.exactMatchSearch(DoubleArrayTrie.java:660)
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.get(DoubleArrayTrie.java:949)
    at com.hankcs.hanlp.model.bigram.BigramDependencyModel.get(BigramDependencyModel.java:108)
    at com.hankcs.hanlp.model.bigram.BigramDependencyModel.get(BigramDependencyModel.java:121)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.parse(CRFDependencyParser.java:153)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:78)
    at com.sxl.extractor.ParsingBaseNP.main(ParsingBaseNP.java:34)

ËØ∑ÈóÆÔºåÂ¶Ç‰ΩïËß£ÂÜ≥Ôºü
"
ËÇ°‰ªΩÔºåÁπÅ‰ΩìËΩ¨ÁÆÄ‰Ωì,"ËÇ°‰ªΩÔºåÁπÅ‰ΩìËΩ¨ÁÆÄ‰ΩìÔºåÂèòÊàê‰∫ÜËÇ°ÂàÜ„ÄÇ

ËÇ°‰ªΩÂÖ∂ÂÆûÊòØ‰∏Ä‰∏™ÁÆÄ‰ΩìÔºåË∞ÉÁî®‰∫ÜHanLP.convertToSimplifiedChinese(""ËÇ°‰ªΩ"")
"
ËÉΩÂê¶ÊåáÂÆö‰ΩøÁî®‰∏Ä‰∏™Áî®Êà∑ËØçÂÖ∏,"ËØ∑ÈóÆÂØπ‰∫é‰∏Ä‰∏™ÂàÜËØçÔºåËÉΩ‰∏çËÉΩÊåáÂÆö‰ΩøÁî®‰∏Ä‰∏™ËØçÂÖ∏ËøõË°åÂàÜËØçÔºåËÄå‰∏îÂè™Áî®Ëøô‰∏Ä‰∏™Áî®Êà∑ËØçÂÖ∏Ôºå‰∏ç‰ΩøÁî®Ê†∏ÂøÉËØçÂÖ∏Á≠âÂÖ∂ÂÆÉËØçÂÖ∏
"
ËØ∑ÈóÆËÉΩÂê¶‰ΩøÁî®Êà∑ËØçÂÖ∏ÁöÑ‰ºòÂÖàÁ∫ßÈ´ò‰∫éÊ†∏ÂøÉËØçÂÖ∏,"ÊØîÂ¶ÇÊ†∏ÂøÉËØçÂÖ∏‰∏≠Êúâ""ÂùêÁÅ´ËΩ¶ v 187""ÔºåËÄåÊàëÊÉ≥ÊääÂÆÉÂàÜ‰∏∫""Âùê   ÁÅ´ËΩ¶""ÔºåÊàëÂú®Áî®Êà∑ËØçÂÖ∏‰∏≠Âä†‰∫Ü""Âùê v 1024"",""ÁÅ´ËΩ¶ n 1024""Ôºå‰ΩÜÊòØÂπ∂Ê≤°ÊúâÁî®„ÄÇ
"
CustomDirectionaryPath‰∏≠Â≠òÂú®ÁöÑbug,"hiÔºåhankcs:
     ÊàëÁé∞Âú®‰ΩøÁî®ÁöÑÊòØ1.2.7ÁâàÊú¨ÁöÑHanLPÔºåÂΩìÊàëÂú®Â¢ûÂä†‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑËØçÂÖ∏ÁöÑÊó∂ÂÄôÂèëÁé∞‰∏Ä‰∏™bug„ÄÇÊàëÁõ¥Êé•Âú®Ë°åÂ∞æÊ∑ªÂä†"" ËøΩÂä†ÂêçËØç.txt;""ÔºåÁªìÊûúÂú®Âä†ËΩΩÁöÑÊó∂ÂÄôÂÆö‰ΩçÂà∞‰∫Üdata/dictionary/person/ËøΩÂä†ÂêçËØç.txt„ÄÇÁÑ∂ÂêéÂèëÁé∞Âú®/hanlp/HanLP.java‰∏≠ÁöÑ201Ë°åÂà∞209Ë°åÁöÑÂÆûÁé∞‰∏≠ÔºåÂÄº""data/dictionary/person/nrf.txt nrf""Â∞ÜË°åÈ¶ñËé∑ÂæóÁöÑË∑ØÂæÑÊõ¥Êîπ‰∫ÜÔºàdata/dictionary/custom/ÔºâÔºåËøôÊ†∑Â∞±Â∏¶Êù•‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºåËôΩÁÑ∂ËøôÊ†∑ÂèØ‰ª•Âú®custom‰∏≠Ë∞ÉÁî®ÂÖ∂ÂÆÉË∑ØÂæÑ‰∏ãÁöÑËØçÂÖ∏ÔºàÈùûdata/dictionary/custom/‰∏ãÔºâÔºå‰ΩÜÊòØ‰ºöÁõ¥Êé•Êõ¥Êîπ‰πãÂêéÁöÑËØçÂÖ∏ÁöÑÈªòËÆ§Ë∑ØÂæÑ„ÄÇ
    ÊàëËßâÂæóÂèØ‰ª•Âº∫Âà∂Âè™‰ΩøÁî®‚Äúdata/dictionary/custom/‚ÄùÂç≥Á¨¨‰∏Ä‰∏™ÂÄºÁöÑÁõÆÂΩïË∑ØÂæÑÔºåÊàñËÄÖÂú®ÊñáÊ°£‰∏≠Ê≥®ÊòécustomËØçÂÖ∏Ê∑ªÂä†Ê†ºÂºèÔºà‰øÆÊîπÈªòËÆ§Ë∑ØÂæÑÊó∂"";""‰πãÂêé‰∏çËÉΩÊúâÁ©∫Ê†ºÔºå‰∏î‰πãÂêéÁöÑËØçÂÖ∏ÈªòËÆ§Ë∑ØÂæÑ‰∏∫Ê≠§Ë∑ØÂæÑ„ÄÇÊ∑ªÂä†ÈªòËÆ§Ë∑ØÂæÑ‰∏ãÁöÑËØçÂÖ∏Êó∂Ôºå"";""‰πãÂêéË¶ÅÊúâÁ©∫Ê†ºÔºâ„ÄÇ
    Áî±‰∫éÁ¨¨‰∫åÁßçÊñπÂºè‰∏çÁî®Êîπ‰ª£Á†ÅÔºåÊâÄ‰ª•ÂÖàËÆ®ËÆ∫‰∏ãÔºåÂÜçÁúãÁúãË¶Å‰∏çË¶ÅÊù•‰∏™Pull request„ÄÇ
   ÁÑ∂Âêé‰∏∫‰ªÄ‰πà‰∏çÂú®È°πÁõÆ‰∏≠Êîæ‰∏ähanlp.propertiesÂë¢ÔºåÊàëÊâæ‰∫ÜÂ•Ω‰πÖÊ≤°ÊâæÂà∞:)
"
Êó†Ê≥ïÂú®‰ª£Á†Å‰∏≠ÈÖçÁΩÆÊï∞ÊçÆÁõÆÂΩïÔºåËÉΩÂê¶Ê∑ªÂä†setPropertyÁöÑÊñπÂºèÂú®‰ª£Á†Å‰∏≠Ëá™Áî±‰ΩøÁî®,"‰Ωú‰∏∫LibÔºåÁõ¥Êé•Ë∞ÉÁî®jarÔºåËøõË°åÂàÜËØçÊó∂ÔºåÊó†Ê≥ïÂÉè‰Ω†ËØ¥ÁöÑÈÇ£Ê†∑Áî®ÈÖçÁΩÆÊñá‰ª∂ÈÖçÁΩÆdataÁöÑÁõÆÂΩïÔºåÂØºËá¥Á±ª‰ººÂ¶Ç‰∏ãÈóÆÈ¢òÂèëÁîüÔºö

ERROR HanLP: HMMÂàÜËØçÊ®°Âûã[ data/model/segment/HMMSegmentModel.bin ]‰∏çÂ≠òÂú®

ÁúãÂà∞HanLPÈáåÊúâ‰∏ÄÊÆµ
public static final class Config ÂÜôÊ≠ª‰∫ÜËØçÂÖ∏ÂíåÊ®°ÂûãÊï∞ÊçÆÁöÑÂú∞ÂùÄÔºåÂ∞ΩÁÆ°Êúâ‰∏Ä‰∏™rootÈÄöËøáÈÖçÁΩÆÊñá‰ª∂ÂÆûÁé∞‰∫ÜËÆæÁΩÆÔºå‰ΩÜÊòØËøò‰∏çÊñπ‰æøÔºåËÉΩÂê¶Âä†‰∏Ä‰∏™Á±ª‰ººsetPropertyÁöÑÊñπÂºèÔºåÂú®‰ª£Á†Å‰∏≠Ëá™Áî±‰ΩøÁî®Ôºü

Ë∞¢Ë∞¢„ÄÇ

PS: ÊàëÁî®ÁöÑscala
"
ËØ≠‰πâË∑ùÁ¶ªÊ®°ÂùóËÆ°ÁÆóÊñπÊ≥ï,"ËØ∑ÈóÆ‰∏Ä‰∏ãËØ≠‰πâË∑ùÁ¶ªËÆ°ÁÆóÊñπÊ≥ïÔºåÂÖ∑‰ΩìÂéüÁêÜÊòØ‰ªÄ‰πàÂë¢ÔºüÊúâÂÖ∑‰ΩìÂèÇËÄÉÊñáÁåÆ‰πàÔºüO(‚à©_‚à©)OË∞¢Ë∞¢
"
‰øÆÊîπËØçÊÄß,"Âú®‰∏ÄËà¨ÁöÑÂú∞ÂùÄÈáå ‚ÄúÂè∑‚Äù Ëøô‰∏™ËØçÊòØÂæàÁâπÊÆä ÊØîÂ¶Ç 150Âè∑Ê∏ØÊ±áÂπøÂú∫ Â∞±Êàê‰∫Ü 150Âè∑Ê∏Ø/ns ‰∫ÜÔºåÊÄé‰πàÊää""Âè∑""ÊîπÊàê‰∏™ÂêçËØçÔºåÂ∏åÊúõÁöÑÁªìÊûúÊòØ 150/m,Âè∑/nÔºåÊ∏Ø/nËøôÊ†∑ÁöÑÂ∞±Â•Ω„ÄÇ
"
Âª∫ËÆÆÊñá‰ª∂ËØªÂÜôÁöÑÁªü‰∏Ä,"ÂæàÂ§öÂú∞ÊñπÁöÑÊñá‰ª∂ËØªÂÜôÈÉΩÊ≤°ÊúâÁªü‰∏Ä‰ΩøÁî®IOUtilÁ±ª‰∏≠ÁöÑÊñπÊ≥ïÔºå
Âª∫ËÆÆÁªü‰∏Ä‰∏Ä‰∏ãÔºå
ËøôÊ†∑ÊîæÂà∞hadoopÈõÜÁæ§‰∏äËøêË°åÂ∞±Âè™ÈúÄË¶ÅÁªü‰∏Ä‰øÆÊîπIOUtilÂ∞±ÂèØ‰ª•‰∫ÜÔºÅ
"
for a unexpected crash if the custom dictionary is empty.,
ËØ∑ÊïôÔºåÂ¶Ç‰ΩïÂÅöËá™Â∑±ÁöÑËØçÂÖ∏Ôºü,"‰Ω†Â•ΩÔºÅ
ÊàëÊâìÁÆó‰ΩøÁî® HanLP ÂÅöËá™Â∑±ÁöÑËØçÂÖ∏ÔºåÊàë‰ªîÁªÜËÆ§ÁúüÁöÑÈòÖËØª‰∫ÜÂÆòÊñπÊñáÊ°£ÔºåË∞ÅÁü•ÊñáÊ°£Â∑ßÂ¶ôÁöÑÈÅøÂºÄ‰∫ÜËøô‰∫õÂÜÖÂÆπÔºåÊâÄ‰ª•Âú®ËøôÈáåËØ∑Êïô‰∏ãÔºåÊÄé‰πà‰ΩøÁî® HanLP ÂÅöËá™Â∑±ÁöÑËØçÂÖ∏Âë¢Ôºü

  È°∫È¢Ç
ÊïôÁ•∫
"
SetPropertiesFilePath,"## Ê∑ªÂä†ÊåáÂÆöPropertiesFileÊñá‰ª∂Ë∑ØÂæÑÁöÑÊñπÊ≥ï„ÄÇ

Áî±‰∫éIKVMËΩ¨Êç¢ÁöÑDLLÊñá‰ª∂ÊöÇÊó†Ê≥ïÊîØÊåÅjavaÁöÑgetContextClassLoader().getResourceAsStreamÊñπÊ≥ï„ÄÇÂá∫Ê≠§‰∏ãÁ≠ñÔºåÁõ¥Êé•ÊåáÂÆöpropertiesÊñá‰ª∂‰ΩçÁΩÆ„ÄÇÊ∑ªÂä†Ê≠§ÂäüËÉΩÂêéÔºåIKVMËΩ¨Êç¢ÂêéÁöÑDLLÊñá‰ª∂ÂèØË¢´.netÁ®ãÂ∫èÁõ¥Êé•‰ΩøÁî®„ÄÇÊàëËØïÈ™å‰∫ÜÂ§öÁßçÂàÜËØçÊñπÊ≥ïÂíåÊñáÊ°£ÊëòË¶ÅÊñπÊ≥ïÔºåÂùáÂèØÊâßË°å„ÄÇ
"
ÂÖ≥ÈîÆËØçÊèêÂèñ‰ΩøÁî®ÂÖ∂‰ªñTokenizer,"Áé∞Âú® TextRankKeyword ÁöÑ‰ª£Á†ÅÈáåÊòØ‰ΩøÁî® StandardTokenizerÊù•ËøõË°åÂàÜËØçÁöÑ

https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/summary/TextRankKeyword.java#L45

ÊàëÊÉ≥ÈóÆ‰∏Ä‰∏ã

**1. ËÉΩ‰∏çËÉΩ‰ΩøÁî®ÂÖ∂‰ªñTokenizer ÊØîÂ¶Ç  IndexTokenizer ËøòÊòØÂõ†‰∏∫Êüê‰∫õÂéüÂõ†Âè™ËÉΩÁî®StandardTokenizer**, nlpÊàë‰ª•ÂâçÊ≤°ÊúâÊé•Ëß¶Ëøá...

ÊØîÂ¶Ç:

``` java
public List<String> getKeyword(String content)
    {
        List<Term> termList = IndexTokenizer.segment(content);
        List<String> wordList = new ArrayList<String>();
```

**2. ËøîÂõûÂÖ®ÈÉ®ÁöÑÂÖ≥ÈîÆËØçÂíåÂØπÂ∫îÁöÑrank, Áé∞Âú®ÁöÑÂÆûÁé∞ÊòØ‰ªéÂÆåÊï¥ÁöÑÁªìÊûúÁöÑMapÈáåÂèñ‰∏ÄÈÉ®ÂàÜ(ÂΩìsizeÂ§ßÊàñËÄÖÁªìÊûúÂ∞ëÊó∂ÊòØÂÖ®ÈÉ®), ËÄå‰∏îÂè™Âèñ‰∫ÜkeyÂπ∂Ê≤°ÊúâÁªôrank**

https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/summary/TextRankKeyword.java#L105

ÊØîÂ¶Ç:

``` java
Map<String, Float> sortedScore = new HashMap<String, Float>();
// ... sort score
return sortedScore;
```

**3. Âä†ÂÖ•Êñ∞ÁöÑTermÂíåÂ∑•ÂÖ∑Á±ªÔºåÂú®ËøîÂõûÂàÜËØçÁªìÊûúÁöÑÂêåÊó∂ËøîÂõûÈÉ®ÂàÜ(ÂÖ®ÈÉ®)termÁöÑrank. Âõ†‰∏∫Ë∞ÉÁî®TextRankÊó∂‰πüË¶ÅÂàÜ‰∏ÄÊ¨°ËØç„ÄÇÊàñËÄÖÂÖÅËÆ∏‰º†ÂÖ•Â∑≤ÁªèÂàÜÂ•ΩÁöÑËØç, Á¨¨‰∏Ä‰∏™ÂèÇÊï∞ÂèØ‰ª•ÊòØ List<Term> , Á¨¨‰∫å‰∏™ÊòØÊñáÊú¨**

ÊØîÂ¶Ç

``` java
// ËøîÂõûÂÖ®ÈÉ®ÂàÜËØçÁªìÊûúÂíåÂØπÂ∫îÁöÑrank
public Map<String,Float> getTermAndRank(String content);
// ËøîÂõûÈÉ®ÂàÜÂàÜËØçÁªìÊûúÂíåÂØπÂ∫îÁöÑrank
public Map<String,Float> getTermAndRank(String content, Integer size);
// ‰ΩøÁî®Â∑≤ÁªèÂàÜÂ•ΩÁöÑËØçÊù•ËÆ°ÁÆórank
public Map<String,Float> getRank(List<Term> termList, String content);
```

Ë∞¢Ë∞¢~ 
"
ÂÖ≥‰∫éÊ≠ß‰πâËØçÂÖ∏,"hankcsÔºå‰Ω†Â•ΩÔºå

```
  ÊàëÊÉ≥ÈóÆ‰∏ãÁé∞Âú®ÁöÑhanlp‰∏≠ÂàÜËØçÊòØÂê¶ÊîØÊåÅÊ≠ß‰πâËØçÂÖ∏„ÄÇ‰æãÂ¶Ç ‚ÄúËøúÂæÅÊ±ÇÂá∫Ë¥ß‚ÄùÔºåËã•ÊåâÊ≠£Â∏∏ÂàÜËØçÂàÜÊàê [Ëøú/a, ÂæÅÊ±Ç/v, Âá∫Ë¥ß/vi] Ôºå‰ΩÜÊòØÊàëÊÉ≥ÂàÜÊàê ËøúÂæÅÔºåÊ±ÇÔºå Âá∫Ë¥ß„ÄÇ ÈóÆÊòØÂê¶ÊúâÁ±ª‰ººÂäüËÉΩÊèê„ÄÇ

Ë∞¢Ë∞¢„ÄÇBTWÔºåÁúãËøá‰Ω†ÁΩëÁ´ôÁöÑaboutÔºåÊñ∞ÁîüÂ¥áÊãúÔºÅ
```

Thanks
pan
"
PortableÂêåÊ≠•ÂçáÁ∫ßÂà∞v1.2.7,
‰ΩøÁî®HMMÂàÜËØçÂêéÔºåÁªìÊûúÁöÑËØçÊÄßÊ†áÊ≥®ÈÉΩ‰∏∫nullÊòØ‰∏∫‰ªÄ‰πà,"‰ΩøÁî®ÁöÑÊòØHanLP-1.2.6ÔºåÂú®ËøêË°åDemoHMMSegment.javaÊó∂ÔºåÂàÜËØçÁöÑËØçÊÄßÁªìÊûúÈÉΩ‰∏∫null
"
Added the missing pick_sentence function,"Added the missing pick_sentence function
"
Add new methods in TextRankSentence class,"Add new methods in TextRankSentence class to support the newly added text summary API (which is based on user specified summary length). 

The return type of the method will be String, which is human-readable text based on the original text sequence.
"
Add a new text summary API,"Add a new text summary API, this API can output summarized text based on the maximum summary length (character length) specified by the user.
"
ÂæÆË∞ÉÊú∫ÊûÑÂêçËØÜÂà´Ê®°Âûã,"Êñ∞ÂºÄÂ§¥ÁöÑ‰∏Ä‰∫õÊú∫ÊûÑÁöÑÂêçÁß∞Ôºå‚ÄúÊñ∞‚ÄùËØØË¢´ËØÜÂà´‰∏∫‚ÄúA‚ÄùÔºåÊàê‰∏∫Êú∫ÊûÑÁöÑÂâçÁºÄËØç
"
ÂæÆË∞ÉÊú∫ÊûÑÂêçËØÜÂà´Ê®°Âûã,"Êñ∞ÂºÄÂ§¥ÁöÑ‰∏Ä‰∫õÊú∫ÊûÑÁöÑÂêçÁß∞Ôºå‚ÄúÊñ∞‚ÄùËØØË¢´ËØÜÂà´‰∏∫‚ÄúA‚ÄùÔºåÊàê‰∏∫Êú∫ÊûÑÁöÑÂâçÁºÄËØç
"
ËØ∑ÊïôÊ∑ªÂä†Áî®Êà∑Â≠óÂÖ∏ÁöÑÈóÆÈ¢ò,"È¶ñÊ¨°‰ΩøÁî®NLPÔºåËØ∑Ê±ÇÂ∏ÆÂøô„ÄÇ
ËøôÈáåÊúâ‰∏Ä‰∏™Áî®Êà∑Â≠óÂÖ∏ÁöÑÈóÆÈ¢òÔºåÂèëÁé∞ËØÜÂà´‚ÄúÂº†ÂÖãÊô∫‰∏éÊΩçÂùäÂú∞ÈìÅÂª∫ËÆæÂ∑•Á®ãÂÖ¨Âè∏‚ÄùÈáåÁöÑÊú∫ÊûÑÂÆû‰ΩìÂá∫Áé∞‰ª•‰∏ãÁªìÊûúÔºö
[Âº†ÂÖãÊô∫/nr, ‰∏éÊΩçÂùäÂú∞ÈìÅÂª∫ËÆæÂ∑•Á®ãÂÖ¨Âè∏/nt]
ÊÉ≥Âà©Áî®Áî®Êà∑ËØçÂÖ∏Â∞Ü‚ÄúÊΩçÂùäÂú∞ÈìÅÂª∫ËÆæÂ∑•Á®ãÂÖ¨Âè∏‚ÄùÂä†Âà∞Áî®Êà∑Â≠óÂÖ∏ÈáåÔºåË°•ÂÖÖ‰∫Ü/data/dictionary/custom/Êú∫ÊûÑÂêçËØçÂÖ∏.txt,Ê∑ªÂä†‰∫ÜÂ¶Ç‰∏ã‰∏ÄË°åÔºö
ÊΩçÂùäÂú∞ÈìÅÂª∫ËÆæÂ∑•Á®ãÂÖ¨Âè∏ nt 1
Âú®‰ª£Á†ÅÈáåËÆæÁΩÆ‰∫ÜÔºàenableCustomDictionary(true)), ÂèëÁé∞‰ªçÁÑ∂Ê≤°ÊúâËØÜÂà´ÊàêÂäü„ÄÇ
ËØ∑Êïô‰∏ãËøô‰∏™ÂÅöÊ≥ïÂØπ‰∏çÂØπÔºüË∞¢Ë∞¢ÔºÅ
"
ÔºªReopenÔºΩHightLighterÂÆö‰Ωç‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢ò,"luceneÊèí‰ª∂Â∑≤ÁªèÊõ¥Êñ∞Âà∞Êñ∞ÁâàÊú¨ÔºåËøêË°åHighLighterTest.javaÊ≤°ÊúâÈóÆÈ¢ò„ÄÇ
‰ΩÜÊòØÔºå‰øÆÊîπHighLighterTest.javaÁöÑÊ£ÄÁ¥¢ÂÜÖÂÆπ‰πãÂêéÔºåËøòÊòØ‰ºöÂá∫Áé∞ÂÆö‰ΩçÈóÆÈ¢òÔºåÈ∫ªÁÉ¶Â∏ÆÊàëÁúã‰∏Ä‰∏ãÔºåÂ§öË∞¢„ÄÇ
‰∏ãÈù¢ÊòØ‰øÆÊîπÁ§∫‰æã‰ª£Á†Å‰πãÂêéÁöÑÂÜÖÂÆπÔºåÂè™ÊòØÂú®ÂÜÖÂÆπ‰∏≠Â§öÂä†‰∫ÜÂá†‰∏™ ""\n""   Ôºö
  IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
            iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
            iwriter = new IndexWriter(directory, iwConfig);
            {
                // Âä†ÂÖ•‰∏Ä‰∏™ÊñáÊ°£
                Document doc = new Document();
                doc.add(new TextField(fieldName, ""ÊàëÁôΩÂ§©ÊòØ‰∏ÄÂêçËØ≠Ë®Ä\n\n\n\nÂ≠¶‰π†ËÄÖÔºåÊôö‰∏äÊòØ‰∏ÄÂêçÂàùÁ∫ßÁ†ÅÂÜú„ÄÇÁ©∫ÁöÑÊó∂ÂÄôÂñúÊ¨¢ÁúãÁÆóÊ≥ïÂíåÂ∫îÁî®Êï∞Â≠¶‰π¶Ôºå‰πüÂñúÊ¨¢ÊÇ¨ÁñëÊé®ÁêÜÂ∞èËØ¥ÔºåACGÊñπÈù¢ÂñúÊ¨¢ÂûãÊúà„ÄÅËΩ®Ëøπ„ÄÇÂñúÊ¨¢Êúâ\n\n\n\n\nÊÄùÊÉ≥Ê∑±Â∫¶ÁöÑ‰∫ãÁâ©ÔºåËÆ®ÂéåÊÄ•Ë∫Å„ÄÅÊãúÈáë‰∏éÂÆâÈÄ∏ÁöÑ‰∫∫„ÄÇÁõÆÂâçÂú®È≠îÈÉΩÊüêÂ•≥Ê†°Â≠¶‰π†ÔºåËøôÊòØÊàëÁöÑ‰∏™‰∫∫ÂçöÂÆ¢„ÄÇÈóªÈÅìÊúâÂÖàÂêéÔºåÊúØ‰∏öÊúâ‰∏ìÊîªÔºåËØ∑Â§öÂ§öÂÖ≥ÁÖß„ÄÇ‰Ω†ÂñúÊ¨¢ÂÜô‰ª£Á†ÅÂêóÔºü"", Field.Store.YES));
                doc.add(new TextField(""title"", ""ÂÖ≥‰∫éhankcs"", Field.Store.YES));
                iwriter.addDocument(doc);
            }
            {
                // ÂÜçÂä†ÂÖ•‰∏Ä‰∏™
                Document doc = new Document();
                doc.add(new TextField(fieldName, ""\n\n   \n\n\n\n\n\n\n\n\nÁ®ãÂ∫èÂëòÂñúÊ¨¢ÈªëÂ§ú"", Field.Store.YES));
                doc.add(new TextField(""title"", ""ÂÖ≥‰∫éÁ®ãÂ∫èÂëò"", Field.Store.YES));
                iwriter.addDocument(doc);
            }
            iwriter.close();

ËøêË°åÁªìÊûúÂ¶Ç‰∏ãÔºö

test
Query = text:ÂñúÊ¨¢
ÂëΩ‰∏≠Ôºö2
ÂÖ≥‰∫éÁ®ãÂ∫èÂëò , 0.2972674

Á®ãÂ∫èÂëò<font color='red'>ÂñúÊ¨¢</font>ÈªëÂ§ú
ÂÖ≥‰∫éhankcs , 0.124633156
ÊàëÁôΩÂ§©ÊòØ‰∏ÄÂêçËØ≠Ë®Ä

Â≠¶‰π†ËÄÖÔºåÊôö‰∏äÊòØ‰∏ÄÂêç<font color='red'>ÂàùÁ∫ß</font>Á†ÅÂÜú„ÄÇÁ©∫ÁöÑÊó∂ÂÄôÂñúÊ¨¢ÁúãÁÆó<font color='red'>Ê≥ïÂíå</font>Â∫îÁî®Êï∞Â≠¶‰π¶Ôºå‰πüÂñúÊ¨¢ÊÇ¨ÁñëÊé®<font color='red'>ÁêÜÂ∞è</font>ËØ¥ÔºåACGÊñπ<font color='red'>Èù¢Âñú</font>Ê¨¢ÂûãÊúà„ÄÅËΩ®Ëøπ„ÄÇÂñúÊ¨¢<font color='red'>Êúâ
</font>

ÊÄùÊÉ≥Ê∑±Â∫¶ÁöÑ‰∫ãÁâ©ÔºåËÆ®ÂéåÊÄ•Ë∫Å„ÄÅÊãúÈáë‰∏éÂÆâÈÄ∏ÁöÑ‰∫∫„ÄÇÁõÆÂâçÂú®È≠îÈÉΩÊüêÂ•≥Ê†°Â≠¶‰π†ÔºåËøôÊòØÊàëÁöÑ‰∏™‰∫∫ÂçöÂÆ¢„ÄÇÈóªÈÅìÊúâÂÖàÂêéÔºåÊúØ‰∏öÊúâ‰∏ìÊîªÔºåËØ∑Â§öÂ§öÂÖ≥ÁÖß„ÄÇ‰Ω†ÂñúÊ¨¢ÂÜô‰ª£Á†ÅÂêóÔºü
"
Lucene5.3Ë∞ÉÁî®HanLPÂÖ≥ÈîÆËØçHightLightÂÆö‰ΩçÈîôËØØ,"Âú®Lucene5.3‰∏≠Ë∞ÉÁî®HanLPÂØπÊåáÂÆöÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂ËøõË°åÊ£ÄÁ¥¢ÔºàÊñá‰ª∂Ëß£ÊûêÁî®ÁöÑTikaÔºâÔºåÂÖ≥ÈîÆÂ≠óÈ´ò‰∫Æ‰ΩçÁΩÆÂÆö‰Ωç‰∏çÂØπÔºåË∞ÉÁî®IKAnalyzer5xÂÆö‰ΩçÂ∞±Ê≤°ÈóÆÈ¢òÔºåËØ∑ÈóÆÊòØ‰ªÄ‰πàÊÉÖÂÜµÔºüÂ§öË∞¢„ÄÇ
‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
package com.std.test;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.StringReader;
import java.nio.file.Paths;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.TextField;
import org.apache.lucene.document.Field.Store;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleFragmenter;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.SAXException;

import com.hankcs.lucene5.HanLPAnalyzer;
import com.std.core.ik5x.IKAnalyzer5x;

public class TikaFileSearch {
    /**
     \* Â§öÊ†ºÂºèÊñá‰ª∂ÂÜÖÂÆπËß£Êûê
     \* 
     \* @param file
     \* @return
     \* @throws IOException
     \* @throws SAXException
     \* @throws TikaException
     */
    public static String parseFile(File file) throws IOException, SAXException,
            TikaException {
        FileInputStream fs = new FileInputStream(file);
        Parser parse = new AutoDetectParser();// Ëá™Âä®Ëé∑Âèñ‰∏Ä‰∏™ÂêàÈÄÇÁöÑËß£ÊûêÂô®Á±ªÂûã
        // Â¶ÇÊûúÊñá‰ª∂ÂæàÂ§ßÔºåÈÇ£‰πàËøô‰∏™ÂÄºÂèØ‰ª•ÈÄÇÂΩìË∞ÉÂ§ß
        BodyContentHandler handler = new BodyContentHandler(10000);
        Metadata metadata = new Metadata();
        ParseContext parseContext = new ParseContext();
        parse.parse(fs, handler, metadata, parseContext);
        System.out.println(""ÁõÆÊ†áÊñá‰ª∂ÂêçÁß∞Ôºö"" + file.getAbsolutePath());
        System.out.println(""ÁõÆÊ†áÊñá‰ª∂ÂÜÖÂÆπÔºö"" + handler.toString());

```
    return handler.toString();
}

/**
 * ‰∏∫ÁõÆÊ†áÊñáÊ°£ÂàõÂª∫Á¥¢Âºï
 * 
 * @throws Exception
 */
public static void indexDoc() throws Exception {
    File[] files = new File(""/Projects/IS/filesPath/"").listFiles();
    for (File file : files) {
        String content = parseFile(file);

        Analyzer analyzer = new HanLPAnalyzer();
        // ÊûÑÈÄ†Â≠óÊÆµ
        TextField contentField = new TextField(""content"", content,
                Store.YES);
        TextField nameField = new TextField(""name"", file.getName(),
                Store.YES);
        TextField pathField = new TextField(""path"", file.getAbsolutePath(),
                Store.YES);

        // Ê∑ªÂä†Â≠óÊÆµ
        Document doc = new Document();
        doc.add(contentField);
        doc.add(nameField);
        doc.add(pathField);
        IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
        iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
        try {
            Directory fsDirectory = FSDirectory.open(Paths.get(
                    ""/Projects/IS/filesIndexPath/"", new String[0]));
            IndexWriter indexWriter = new IndexWriter(fsDirectory, iwConfig);
            indexWriter.addDocument(doc);

            indexWriter.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

/**
 * ÊêúÁ¥¢Êñá‰ª∂
 * 
 * @param field
 * @param keyword
 */
public static void search(String field, String keyword) {
    Analyzer analyzer = new HanLPAnalyzer();
    try {
        Directory fsDirectory = FSDirectory.open(Paths.get(
                ""/Projects/IS/filesIndexPath/"", new String[0]));
        DirectoryReader ireader = DirectoryReader.open(fsDirectory);

        IndexSearcher isearcher = new IndexSearcher(ireader);

        QueryParser qp = new QueryParser(field, analyzer); // ‰ΩøÁî®QueryParserÊü•ËØ¢ÂàÜÊûêÂô®ÊûÑÈÄ†QueryÂØπË±°
        qp.setDefaultOperator(QueryParser.AND_OPERATOR);
        Query query = qp.parse(keyword);
        TopDocs topDocs = isearcher.search(query, 5); // ÊêúÁ¥¢Áõ∏‰ººÂ∫¶ÊúÄÈ´òÁöÑ5Êù°ËÆ∞ÂΩï
        System.out.println(""ÂëΩ‰∏≠:"" + topDocs.totalHits);

        SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(
                ""<span style='color:green'>"", ""</span>"");
        Highlighter highlighter = new Highlighter(simpleHTMLFormatter,
                new QueryScorer(query));
        // È´ò‰∫ÆhtmlFormatterÂØπË±°
        // ËÆæÁΩÆÈ´ò‰∫ÆÈôÑËøëÁöÑÂ≠óÊï∞
        highlighter.setTextFragmenter(new SimpleFragmenter(200));

        ScoreDoc[] scoreDocs = topDocs.scoreDocs;
        for (int i = 0; i < topDocs.totalHits; i++) {
            Document targetDoc = isearcher.doc(scoreDocs[i].doc);
            String value = targetDoc.get(""content"");
            TokenStream tokenStream = analyzer.tokenStream(value,
                    new StringReader(value));
            String freg = highlighter.getBestFragment(tokenStream, value);

            System.out.println(freg);

        }

    } catch (Exception e) {

    }
}

public static void main(String[] args) throws Exception {

    indexDoc();
    search(""content"", ""Ê∞ëÊóè"");
}
```

}

ËøêË°åÊà™ÂõæÂ¶Ç‰∏ãÔºö
![079da201-023c-4232-a856-80fb063a7476](https://cloud.githubusercontent.com/assets/14268210/10640239/78728792-7846-11e5-8b93-443ca970dda1.png)

Ë∞ÉÁî®IKÁöÑËøêË°åÁªìÊûúÂ¶Ç‰∏ãÔºö
![56e95ae2-12c8-4b04-a58c-10d00ed28921](https://cloud.githubusercontent.com/assets/14268210/10640257/8818074e-7846-11e5-85f9-208098472519.png)
"
Â¶Ç‰ΩïÊâçËÉΩÂàÜÂá∫Â∏¶Á©∫Ê†ºÁöÑËã±Êñá/Êï∞Â≠óËØç,"ÊØîÂ¶Ç iPad Pro
ÊØîÂ¶Ç PM 2.5
Ë∞¢Ë∞¢ÔºÅ
"
segment.enableAllNamedEntityRecognize(true)ÔºåÊúâ‰∫õÂΩ±ÂìçÂÖ∂‰ªñÂàÜËØçÁöÑÁªìÊûú,"‚Äú‰ªäÂ§©Êàë‰ª¨Âçï‰Ωç‚ÄúÔºåË¢´ÂΩì‰Ωú‰∏Ä‰∏™ËØç‰∫Ü„ÄÇ
Segment segment = HanLP.newSegment();
Segment segment = HanLP.newSegment();
segment = segment.enableMultithreading(true);
segment = segment.enableAllNamedEntityRecognize(true);
[‰ªäÂ§©Êàë‰ª¨Âçï‰Ωç/nt, ËØÑÈÄâ/vn, Âá∫/vf, ‰∫ÜÂÖ®Âçï‰Ωç/nt, ÊúÄ/d, ÊºÇ‰∫Æ/a, ÁöÑ/ude1, Â•≥Âêå‰∫ã/nz, Ôºå/w, ‰ΩÜ/c, Â•π/rr, Â•ΩÂÉè/v, ÂØπ/p, ‚Äú/w, Â±Ä/n, Ëä±/n, ‚Äù/w, Ëøô‰∏™/rz, Â§¥Ë°î/n, Âπ∂‰∏ç/d, Êª°ÊÑè/v]

segment.enableAllNamedEntityRecognize(false)ÔºåÁªìÊûúÂ•Ω‰∫õ„ÄÇ‰ΩÜÊòØÔºåÂèàÂ∏åÊúõËÉΩËØÜÂà´ÂÆû‰Ωì„ÄÇ
[‰ªäÂ§©/t, Êàë‰ª¨/rr, Âçï‰Ωç/n, ËØÑÈÄâ/vn, Âá∫/vf, ‰∫Ü/ule, ÂÖ®/a, Âçï‰Ωç/n, ÊúÄ/d, ÊºÇ‰∫Æ/a, ÁöÑ/ude1, Â•≥Âêå‰∫ã/nz, Ôºå/w, ‰ΩÜ/c, Â•π/rr, Â•ΩÂÉè/v, ÂØπ/p, ‚Äú/w, Â±Ä/n, Ëä±/n, ‚Äù/w, Ëøô‰∏™/rz, Â§¥Ë°î/n, Âπ∂‰∏ç/d, Êª°ÊÑè/v]
"
"Êä•ÈîôÔºöenableNumberQuantifierRecognize Âíå enableAllNamedEntityRecognize ÂêåÊó∂Áî®, ÁâàÊú¨1.2.4","Segment segment = HanLP.newSegment();
segment = segment.enableAllNamedEntityRecognize(true);
segment = segment.enableNumberQuantifierRecognize(true);
System.out.println(segment.seg(""ÊõæÂπªÊÉ≥ËøáÔºåËã•Âπ≤Âπ¥ÂêéÁöÑÊàëÂ∞±ÊòØËøô‰∏™Ê†∑Â≠êÁöÑÂêó""));

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 21
    at com.hankcs.hanlp.seg.common.WordNet.add(WordNet.java:101)
    at com.hankcs.hanlp.seg.common.WordNet.addAll(WordNet.java:201)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:95)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:420)
    at hanlpdemo.HanlpDemo.main(HanlpDemo.java:41)
"
DATÂÜÖÂ≠òÂàÜÈÖçÁÆóÊ≥ï,"src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java

```
resize(65536 * 32); // 32‰∏™ÂèåÂ≠óËäÇ
```

```
            if (allocSize <= (begin + siblings.get(siblings.size() - 1).code))
            {
                // progress can be zero // Èò≤Ê≠¢progress‰∫ßÁîüÈô§Èõ∂ÈîôËØØ
                double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0
                        * keySize / (progress + 1);
                resize((int) (allocSize * l));
            }
```

Ê≠§ÊÆµ‰ª£Á†ÅÂ•ΩÂÉèÂ∞èËßÑÊ®°Â≠óÂÖ∏‰∏ç‰ºöËøêË°åÂà∞Ôºå‰æãÂ¶ÇÂú®‰∫∫ÂêçËØÜÂà´Êó∂Ôºå21‰∏™Â≠óÂÖ∏È°πÁöÑÂ∞èÂ≠óÂÖ∏Ôºå‰πüÈúÄË¶ÅÂºÄ‰∏™65535ÂÜÖÂ≠ò„ÄÇ
"
ÊØîËæÉÁÆÄÂçïÁöÑÈóÆÈ¢òÔºö.txt.binÊñá‰ª∂Â¶Ç‰ΩïÊèêÂèñÔºü,"Âõ†‰∏∫Á†îÁ©∂ÈúÄË¶ÅÔºåÊÉ≥Â≠¶‰π†HanLP‰∏≠ÁöÑÂè•Ê≥ïÂàÜÊûêÈÉ®ÂàÜÔºåÊåâÁÖß‰ªãÁªç‰∏ãËΩΩ‰∫ÜdataÊñá‰ª∂Â§πÔºåÂèëÁé∞ÈáåÈù¢ÁöÑÊñá‰ª∂ÂÖ®ÈÉ®‰∏∫txt.binÔºåËÄådemo‰∏≠ÊèêÁ§∫
‚Äú‰∏•Èáç: CRFÂàÜËØçÊ®°ÂûãÂä†ËΩΩ .../data/model/segment/CRFSegmentModel.txt Â§±Ë¥•ÔºåËÄóÊó∂ 12 ms‚Äù
"
CRFÊ®°ÂûãËÆ≠ÁªÉÊ±ÇÊïô,"‰Ω†Â•ΩÔºåÂ§ßÁ•ûÔºåËØ∑Êïô‰∏Ä‰∏ãÔºöÂ¶ÇÊûúcrfÊ®°ÂûãËÉΩÂ§üËØÜÂà´""Ëä±ÂçÉÈ™®""Ëøô‰∏™Êñ∞ËØçÔºåÊòØ‰∏çÊòØÈúÄË¶ÅÂú®ËÆ≠ÁªÉËØ≠Êñô‰∏≠ËçØÂåÖÂê´Â§ö‰∏™  Ëä±ÂçÉÈ™® Ëøô‰∏™ËØçÊâçËÉΩËÆ≠ÁªÉÂá∫Ëøô‰∏™Ê®°ÂûãÔºüÂè¶ÊÇ®ËÉΩ‰∏çËÉΩÊèê‰æõ‰∏Ä‰∏ãCRFÊ®°ÂûãÁöÑËÆ≠ÁªÉËØ≠ÊñôÊï∞ÊçÆÔºåÈÇÆÁÆ±‰∏∫1527zhaobin@163.comÔºåÂ§öË∞¢ÔºÅ
"
"TermÊ∑ªÂä†equals, hashCodeÊñπÊ≥ïÂèäÂÆûÁé∞comparableÊé•Âè£","Âè™ÊòØ‰∏Ä‰∏™Âª∫ËÆÆÔºåÊàëÁé∞Âú®ÊòØÁªßÊâø‰∫Ü‰∏Ä‰∏™TermÁ±ªÊù•ÂÅöÂêÑÁßçÈõÜÂêàÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú‰ª•ÂèäÊéíÂ∫è
‰∏çËøáÂ¶ÇÊûúTermÂéüÁîüÂ∞±ÊúâÁöÑËØùÂ∞±Êõ¥Â•Ω‰∫Ü
"
ArrayIndexOutOfBoundsException,"Exception in thread ""main"" java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:58)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 65535
        at com.hankcs.hanlp.collection.trie.bintrie.BinTrie.getChild(BinTrie.java:322)
        at com.hankcs.hanlp.collection.trie.bintrie.BaseNode.transition(BaseNode.java:56)
        at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:239)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:55)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:436)
        at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
        at com.hankcs.hanlp.HanLP.segment(HanLP.java:371)
"
CustomDictionary.txtÁ¨¨‰∏ÄË°å‰∏çËÉΩÂä†ÂÖ•ËØçÂÖ∏,"‰∏çÁü•ÈÅìÊòØ‰∏∫‰ªÄ‰πà‚Ä¶‚Ä¶
Ëß£ÂÜ≥ÊñπÊ≥ïÔºöÂâç‰∏§Ë°åÈáçÂ§ç‰∏Ä‰∏ã
"
ÂÅúÁî®ËØçËØçÂÖ∏,"Âª∫ËÆÆÂä†‰∏Ä‰∏™„ÄåÊ≤°Êúâ„Äç
"
‰∏≠Êñá‰∫∫ÂêçÂÆû‰ΩìÁ≤æÂ∫¶ÁöÑÈóÆÈ¢ò,"ÊàëÂú®‰ΩøÁî®hanlp 1.2.4 ÂØπÂ¶Ç‰∏ãÂè•Â≠êÂàÜËØçÁöÑÊó∂ÂÄôÔºö
‚ÄúÂπ∂ÊúâÊúõÂú®ÈÇ£‰∏é1993Âπ¥Â∞±ÁªìËØÜÁöÑÂèã‰∫∫ÈáçËÅö„ÄÇ‚Äù

ÂæóÂà∞ÁöÑÁªìÊûúÊòØÂ¶Ç‰∏ãËøôÊ†∑ÁöÑÔºö
Âπ∂/cc  ÊúâÊúõ/v    Âú®/p   ÈÇ£‰∏é/nr   1993/m  Âπ¥/qt  Â∞±/d   ÁªìËØÜ/v    ÁöÑ/ude1    Âèã‰∫∫/n    ÈáçËÅö/vi   „ÄÇ/w

ÂÖ∂‰∏≠ÁöÑ‚ÄúÈÇ£‰∏éÔºènr‚ÄùÂ∫îËØ•‰∏çÊòØ‰∫∫Âêç„ÄÇÊàëÁúãÊ∫êÁ†ÅÂ•ΩÂÉèÊòØÂõ†‰∏∫ÂÆÉËøô‰∏™ËØçÁªÑÁ¨¶ÂêàÊûÑÊàê‰∫∫ÂêçÁöÑËßÑÂàô ÊâÄ‰ª•Êää‰ªñÁªÑÂêàÊàê‰∫Ü‰∫∫ÂêçÔºüÁÑ∂ÂêéÂÜçËøõË°åhmmÁöÑËØÜÂà´ÔºüËøòÂ∏åÊúõ‰ΩúËÄÖËØ¥Êòé‰∏Ä‰∏ãÂ¶Ç‰ΩïÊîπËøõËøô‰∏ÄÂùó„ÄÇ
"
ÂÖ≥‰∫éÁâàÊùÉÈóÆÈ¢ò,"hankcsÔºå‰Ω†Â•Ω„ÄÇ
‰πãÂâçÊàë‰∏ÄÁõ¥‰ΩøÁî®python+jpypeÁöÑÊñπÂºèÔºå‰ΩøÁî®‰Ω†ÁöÑHanLP„ÄÇÊàëÁöÑÂ∑•‰Ωú‰∏≠ÊâÄÊúâÁöÑ‰ª£Á†ÅÂÖ®ÈÉ®ÊòØÂü∫‰∫épythonÂºÄÂèëÔºåÂπ∂‰∏îËÄÉËôë‰ª•ÂêéË¶ÅÂú®hadoopÂπ≥Âè∞‰∏ä‰ΩøÁî®„ÄÇÊâÄ‰ª•ÊàëÁî®pythonÂÆûÁé∞‰∫Ü‰∏Ä‰∏™HanLPÁöÑÂ≠êÈõÜ„ÄÇÁ≠âÊàêÁÜüÂêé‰πüÂ∏åÊúõÊîæÂà∞github‰∏ä„ÄÇpythonÁâàÊú¨Áõ¥Êé•‰ΩøÁî®‰∫ÜdataÁõÆÂΩï‰∏ãÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºåÂ∏åÊúõÂèØ‰ª•ÂæóÂà∞‰Ω†ÁöÑÊéàÊùÉ„ÄÇÊ†πÊçÆ‰Ω†ÁöÑË¶ÅÊ±ÇÔºåÊàë‰ºöÂú®È°πÁõÆÈ¶ñÈ°µÊ≥®ÊòéÂ≠óÂÖ∏Êï∞ÊçÆÁöÑÊù•Ê∫êÔºå‰ª•ÂèäpythonÁâàÊú¨‰∏éHanLPÁöÑÂÖ≥Á≥ª„ÄÇ
"
Ëøô‰∫õÂ§©ÂàöÂàöÊé•Ëß¶ÂàÜËØçÔºåËØ∑Êïô‰∏Ä‰∏ãÂä†ËΩΩCRFÊ®°ÂûãÁöÑÈóÆÈ¢ò,"ÊàëÊåâÁÖßÂçö‰∏ªÁöÑÊñπÊ≥ïÂú®‰∏ãËΩΩ‰∫Üdata-for-1.2.4Ëøô‰∏™Êï∞ÊçÆÂåÖÔºåÊï∞ÊçÆÂåÖÈáåÊúâdata-for-1.2.4\data\model\segment\CRFSegmentModel.txt.bin Ëøô‰∏™Ê®°ÂûãÊñá‰ª∂ÔºåËØ∑ÈóÆÊàëË¶ÅÊÄé‰πàÂä†ËΩΩÂà∞Á®ãÂ∫èÈáåÂë¢ÔºåÂä†ËΩΩËøáÁ®ã‰∏≠ÊÄªÊòØÊä•ÈîôÔºåË∞¢Ë∞¢Âçö‰∏ª
"
Lucene5.3Ë∞ÉÁî®HanLPÂàõÂª∫Á¥¢ÂºïÊä•Èîô,"Âú®Lucene5.3Ë∞ÉÁî®HanLPÂàõÂª∫Á¥¢ÂºïÁöÑÊó∂ÂÄôÔºåÊä•ÈîôÂ¶Ç‰∏ãÔºö
Exception in thread ""main"" java.lang.AbstractMethodError: org.apache.lucene.analysis.Analyzer.createComponents(Ljava/lang/String;)Lorg/apache/lucene/analysis/Analyzer$TokenStreamComponents;
    at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:179)
    at org.apache.lucene.document.Field.tokenStream(Field.java:562)
    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:607)
    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:344)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:300)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1475)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1254)
    at com.ohoyee.test.search.SearchTest.createIndex(SearchTest.java:51)
    at com.ohoyee.test.search.SearchTest.main(SearchTest.java:104)

Á¥¢ÂºïÂàõÂª∫‰ª£Á†ÅÂ¶Ç‰∏ãÔºö 
static void createIndex() throws Exception {
        Directory dir = FSDirectory.open(Paths.get(indexPath, new String[0]));
        Analyzer analyzer = new HanLPAnalyzer();
        IndexWriterConfig iwc = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(dir, iwc);
        File[] files = new File(targetPath).listFiles();
        for (File file : files) {
            Document doc = new Document();
            String content = getContent(file);
            String name = file.getName();
            String path = file.getAbsolutePath();
            doc.add(new TextField(""content"", content, Store.YES));
            doc.add(new TextField(""name"", name, Store.YES));
            doc.add(new TextField(""path"", path, Store.YES));
            System.out.println(name + ""==="" + content + ""==="" + path);
            writer.addDocument(doc);
            writer.commit();
        }

```
}
```
"
ÈÖçÁΩÆÊñá‰ª∂Â§ßÂ∞èÂÜôÈóÆÈ¢ò,"linux‰∏ãÂ§ßÂ∞èÂÜôÊïèÊÑüÔºåÊåâÁÖßÊñáÊ°£ÈÖçÁΩÆhanlp.properties‰ºöÊúâÈóÆÈ¢òÔºåÈúÄË¶ÅÊäähanlp.propertiesÁöÑÂÖ®ÈÉ®Â≠óÊØçÂ∞èÂÜô„ÄÇ
Âè¶linux‰∏ãÂá∫Áé∞‚ÄúÂ≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°®Âä†ËΩΩÂ§±Ë¥•‚ÄùÔºåËß£ÂÜ≥Â¶Ç‰∏ã„ÄÇHanLP.Config.CharTypePath=""/var/server/java/data/dictionary/other/CharType.dat.yes"";
"
Â≠óÁ¨¶Á±ªÂûãÂØπÂ∫îË°®Âä†ËΩΩÂ§±Ë¥•,"Âú®windows‰∏ãÊ≠£Â∏∏ÔºåÊîæÂà∞centos‰∏ãÂéªË∑ëÔºåÂá∫Áé∞Ëøô‰∏™ÊèêÁ§∫„ÄÇ
"
ÊúâÊ≤°ÊúâÂäûÊ≥ïÂÖ≥Èó≠ÂàÜËØçÂêéÁöÑÂêéÁºÄËØçÊÄßÊ†áÊ≥®,"ÊàëÊÉ≥ÂæóÂà∞ÁöÑÁªìÊûúÊòØÂàÜËØç‰øùÁïôÂéüÊù•ÁöÑÂΩ¢ÂºèÔºåÂè™ÊòØÂàÜËØç‰πãÂêéÊúâÁ©∫Ê†ºÂàÜÂºÄÔºå‰∏ç‰ºöÂá∫Áé∞ÊñúÊù†ÂíåÂêéÈù¢ÁöÑËØçÊÄßÊ†áÊ≥®„ÄÇ
ËØ∑ÈóÆÊÄéÊ†∑ÂäûÔºü
"
ËØ∑ÈóÆËøô‰∏™È°πÁõÆÂíåÂìàÂ∑•Â§ßltpÊàñËÄÖÂ§çÊó¶ÁöÑÈÇ£‰∏™Áõ∏ÊØîÔºå‰∏ªË¶ÅÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü,"Â¶ÇÈ¢òÔºåÊä±Ê≠â‰∏Ä‰∏ãÂ≠êËØª‰∏çÂÆåÊâÄÊúâ‰ª£Á†ÅÂ∞±Áõ¥Êé•ËØ∑Êïô‰∏Ä‰∏ã‰∫ÜÔºöÔºâ
"
CRFÁöÑÂàÜËØçÁªìÊûúÂ¶Ç‰ΩïÂéªÈô§ÂÅúÁî®ËØçÔºü,"‰Ω†Â•ΩÔºåÂØπCRFÂàÜËØç‰πãÂêéÂæóÂà∞ÁöÑList<Term>‰ΩøÁî®coreStopWordDictionary.apply‰πãÂêéÂÖ®ÊòØÁ©∫ÁöÑÔºåËØ∑ÈóÆÊòØ‰ªÄ‰πàÊÉÖÂÜµÔºü
ÁâàÊú¨1.2.4
"
ÂÖ≥‰∫éÊèêÈ´òÊú∫ÊûÑËØÜÂà´Á≤æÂ∫¶ÈóÆÈ¢ò,"Êâæ‰∫Ü‰∏ÄÊÆµË¥¢ÁªèÂ™í‰ΩìÁöÑÊä•ÈÅìÔºåÂ∞ùËØïÂØπÂÖ¨Âè∏ÂêçÁß∞ËøõË°åËØÜÂà´„ÄÇÂÜÖÂÆπÊòØ‚Äú2015Âπ¥7Êúà10Êó•,ÂÖ¨Âè∏ÂÖ®ËµÑÂ≠êÂÖ¨Âè∏Ê∑±Âú≥Â∏Ç‰∏≠Ëà™‰πùÊñπËµÑ‰∫ßÁÆ°ÁêÜÊúâÈôêÂÖ¨Âè∏(‰ª•‰∏ãÁÆÄÁß∞‚Äú‰∏≠Ëà™‰πùÊñπ‚Äù)‰∏éÊ∑±Âú≥Â∏Ç‰∏≠Ëà™ÂçéÂüéÁΩÆ‰∏öÂèëÂ±ïÊúâÈôêÂÖ¨Âè∏(‰ª•‰∏ãÁÆÄÁß∞‚Äú‰∏≠Ëà™ÂçéÂüéÁΩÆ‰∏ö‚Äù)Á≠æËÆ¢‰∫Ü„ÄäÁßüËµÅÊé®ÂπøÊúçÂä°ÂçèËÆÆ„ÄãÂíå„ÄäÁÆ°ÁêÜ„ÄÅÁßüËµÅ„ÄÅÊé®Âπø‰∏éÂí®ËØ¢ÂçèËÆÆ„Äã,Áî±‰∏≠Ëà™‰πùÊñπÂèóÊâò‰∏∫‰∏≠Ëà™ÂçéÂüéÁΩÆ‰∏öÊåÅÊúâÁöÑÊ∑±Âú≥‰πùÊñπË¥≠Áâ©‰∏≠ÂøÉ(‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊ∑±Âú≥‰πùÊñπ‚Äù)(G/M„ÄÅHÂú∞Âùó)Êèê‰æõÂâçÊúüÁßüËµÅÊé®ÂπøÊúçÂä°‰ª•ÂèäÂïÜ‰∏öËøêËê•„ÄÅÁª¥‰øÆ‰øùÂÖª„ÄÅË¥¢Âä°ÁÆ°ÁêÜÁ≠âÂïÜ‰∏öÂí®ËØ¢ÊúçÂä°„ÄÇ‰∏≠Ëà™ÂçéÂüéÁΩÆ‰∏öÂ∞ÜÊåâÁÖßÂâçËø∞ÂçèËÆÆÁ∫¶ÂÆöÂàÜÂà´Âêë‰∏≠Ëà™‰πùÊñπÊîØ‰ªòÁßüËµÅÊé®ÂπøÊúçÂä°Ë¥πÂíåÂí®ËØ¢ÊúçÂä°Ë¥π,È¢ÑËÆ°Áõ∏ÂÖ≥Ë¥πÁî®ÊÄªÊî∂ÂÖ•Á∫¶‰∏∫451‰∏áÂÖÉ„ÄÇ‚Äù
## ÊàëÁî®ÁöÑÊòØ‰∏äÂçäÂπ¥1.1.4‰ª•ÂâçÁöÑÁâàÊú¨ÔºåËØÜÂà´Âá∫Êù•ÁöÑÁªìÊûúÂ¶Ç‰∏ãÔºö
## [2015/m, Âπ¥/q, 7/m, Êúà/n, 10/m, Êó•/ag, ,/w, ÂÖ¨Âè∏/nit, ÂÖ®ËµÑ/nr, Â≠êÂÖ¨Âè∏/nit, Ê∑±Âú≥Â∏Ç/nr2, ‰∏≠Ëà™/gg, ‰πù/ag, ÊñπËµÑ‰∫ßÁÆ°ÁêÜÊúâÈôêÂÖ¨Âè∏/nsf, (/w, ‰ª•‰∏ã/tg, ÁÆÄÁß∞/s, ‚Äú/xx, ‰∏≠Ëà™/gg, ‰πù/ag, Êñπ/mq, ‚Äù/xx, )/w, ‰∏é/pbei, Ê∑±Âú≥Â∏Ç‰∏≠Ëà™ÂçéÂüéÁΩÆ‰∏öÂèëÂ±ïÊúâÈôêÂÖ¨Âè∏/nsf, (/w, ‰ª•‰∏ã/tg, ÁÆÄÁß∞/s, ‚Äú/xx, ‰∏≠Ëà™ÂçéÂüé/nr2, ÁΩÆ‰∏ö/n, ‚Äù/xx, )/w, Á≠æËÆ¢/s, ‰∫Ü/u, „Ää/xx, ÁßüËµÅ/v, Êé®Âπø/v, ÊúçÂä°/v, ÂçèËÆÆ/n, „Äã/xx, Âíå/pbei, „Ää/xx, ÁÆ°ÁêÜ/v, „ÄÅ/xx, ÁßüËµÅ/v, „ÄÅ/xx, Êé®Âπø/v, ‰∏é/pbei, Âí®ËØ¢/v, ÂçèËÆÆ/n, „Äã/xx, ,/w, Áî±/dg, ‰∏≠Ëà™/nr, ‰πù/ag, Êñπ/mq, Âèó/s, Êâò/s, ‰∏∫/dg, ‰∏≠Ëà™ÂçéÂüé/nr2, ÁΩÆ‰∏ö/n, ÊåÅÊúâ/s, ÁöÑ/ule, Ê∑±Âú≥/nr2, ‰πùÊñπË¥≠Áâ©‰∏≠ÂøÉ/nsf, (/w, ‰ª•‰∏ã/tg, ÁÆÄÁß∞/s, ‚Äú/xx, Ê∑±Âú≥/nr2, ‰πù/ag, Êñπ/mq, ‚Äù/xx, )/w, (/w, G/M/nx, „ÄÅ/xx, H/nx, Âú∞Âùó/n, )/w, Êèê‰æõ/s, ÂâçÊúü/tg, ÁßüËµÅ/v, Êé®Âπø/v, ÊúçÂä°/v, ‰ª•Âèä/pbei, ÂïÜ‰∏ö/n, ËøêËê•/v, „ÄÅ/xx, Áª¥‰øÆ/v, ‰øùÂÖª/v, „ÄÅ/xx, Ë¥¢Âä°/n, ÁÆ°ÁêÜ/v, Á≠â/ude3, ÂïÜ‰∏ö/n, Âí®ËØ¢ÊúçÂä°/s, „ÄÇ/xx, ‰∏≠Ëà™ÂçéÂüé/nr2, ÁΩÆ‰∏ö/n, Â∞Ü/qv, ÊåâÁÖß/dg, ÂâçËø∞/ag, ÂçèËÆÆ/n, Á∫¶ÂÆö/s, ÂàÜÂà´/qv, Âêë/dg, ‰∏≠Ëà™/nr, ‰πù/ag, Êñπ/mq, ÊîØ‰ªò/s, ÁßüËµÅ/v, Êé®Âπø/v, ÊúçÂä°Ë¥π/n, Âíå/pbei, Âí®ËØ¢ÊúçÂä°/s, Ë¥π/n, ,/w, È¢ÑËÆ°/s, Áõ∏ÂÖ≥/v, Ë¥πÁî®/n, ÊÄªÊî∂ÂÖ•/n, Á∫¶/qv, ‰∏∫/dg, 451/m, ‰∏áÂÖÉ/mq, „ÄÇ/xx]

Áúã‰∏äÂéªÂè™Ê≠£Á°ÆËØÜÂà´Âà∞‰∫Ü1‰∏™ÂÆåÂÖ®ÂáÜÁ°ÆÁöÑÂÖ®Áß∞ÔºåÂè¶Â§ñ1‰∏™ÂÖ®Áß∞Ôºå3‰∏™ÁÆÄÁß∞‰ºº‰πéÈÉΩÊ≤°ÊúâËØÜÂà´Á≤æÁ°Æ„ÄÇËØ∑ÁúãÁúãÊòØÂê¶ÊúâÂäûÊ≥ïÊèêÈ´òËøô‰∏™ÔºüË∞¢Ë∞¢ÔºÅ
"
ÊàëÊòØËßâÂæóËá™ÂÆö‰πâËØçÂÖ∏‰ºòÂÖàÁ∫ßÂ∫îËØ•Â§ß‰∫éÊ†∏ÂøÉËØçÂÖ∏.,"ÊàëÊòØËßâÂæóËá™ÂÆö‰πâËØçÂÖ∏‰ºòÂÖàÁ∫ßÂ∫îËØ•Â§ß‰∫éÊ†∏ÂøÉËØçÂÖ∏, ÊàñËÄÖËØ¥Êúâ‰∏Ä‰∏™ÈÄâÈ°πÂèØ‰ª•ËÆæÁΩÆ‰ºòÂÖàÁ∫ß. ÂÉè‰∏Ä‰∫õË°å‰∏öÁöÑËØçÊ±á, ÈÄöËøá‰∫∫Ê∞ëÊó•Êä•ËøôÁßçÈÄö‰øóËØ≠ÊñôÂ∫ìÊòØËØÜÂà´‰∏çÂá∫Êù•ÁöÑ. ÊØîÂ¶ÇËØ¥: ÂÆùÈ©¨X5, ÂÖ´Ëç£ÂÖ´ËÄª, ‰∏â‰∏•‰∏âÂÆû. Ëøô‰∫õ‰∏ìÁî®ËØçËØ≠Âú®ÂàÜËØçÊó∂ÂÄôÊòØÂ∫îÂΩìË¢´ÊèêÂèñÂá∫Êù•ÁöÑ. ÂΩìÁÑ∂, ‰Ω†Ë¶ÅËØ¥ÂèØ‰ª•‰øÆÊîπÊ†∏ÂøÉËØçÂÖ∏, ‰ΩÜÊàëËÆ§‰∏∫‰øÆÊîπÊ†∏ÂøÉËØçÂÖ∏ÂèàÊòØËøáÈáçÁöÑÂÅöÊ≥ï, ËØçÂÖ∏ÈÉΩ‰∏çÂ•ΩÁÆ°ÁêÜ. ËøòÊòØÂ∏åÊúõÊúâËá™ÂÆö‰πâËØçÂÖ∏‰ºòÂÖàÁ∫ßÁöÑËÆæÁΩÆ.
"
‰∫∫ÂêçÂíåÁµÑÁπîÊ©üÊßãË≠òÂà•Á∂ìÂ∏∏ÂåÖÂê´Ê®ôÈªûÁöÑÂïèÈ°å,"‰æãÂ¶ÇÊú∫ÊûÑËØÜÂà´Âá∫Ôºö‚ÄúÁæéÂúãÔºå‰∏≠ËèØÈöä‚Äù
Ê†áÁÇπÈóÆÈ¢òÈúÄË¶Å‰∏Ä‰∏™Ëß£ÂÜ≥ÊñπÊ°àÔºåCommonAhoCorasickSegmentUtil‰ºöÊääÊ†áÁÇπ‰Ωú‰∏∫Êú™Áü•ËØ≠Á¥†ÂíåÂâçÂêéÁöÑËØ≠Á¥†ÂêàÂπ∂ÔºåÊòæÁÑ∂‰∏çÂ§™ÂêàÁêÜ„ÄÇÊ†áÁÇπÂàÜËØçÁöÑ‰ºòÂÖàÁ∫ßÂ∫îËØ•ÊèêÈ´ò„ÄÇ
"
ÊèêÂèñÂÖ≥ÈîÆËØç„ÄÅÁü≠ËØ≠,"‰Ω†Â•ΩÔºåHanLPÊèêÂèñÊñ∞ÈóªÁöÑÂÖ≥ÈîÆËØçÊàñÁü≠ËØ≠ÁöÑÊó∂ÂÄôÊïàÊûúÊÄª‰∏çÂ∞Ω‰∫∫ÊÑèÔºåÈÄöÂ∏∏ÊèêÂèñÂá∫Êù•ÁöÑËØç‰∏çÊòØÁâπÂà´ËÉΩË°®ËææÂá∫Êñ∞ÈóªÂÖ≥ÈîÆÂÖ≥Ê≥®ÁÇπÔºåÊúâÂï•Â•ΩÁöÑËß£ÂÜ≥ÊñπÊ°àÂêóÔºü
"
ËØ∑ÈóÆÂ¶Ç‰ΩïÁî®ÊÇ®ÁöÑÂ∑•ÂÖ∑ÂåÖÂÆûÁé∞Êñ∞ËØçÂèëÁé∞ÂäüËÉΩÂë¢Ôºü,"ÊÇ®Â•ΩÔºåÁúã‰∫ÜÊÇ®ÁöÑÂ∑•ÂÖ∑ÂåÖÁöÑ‰ª£Á†ÅÔºåÊÑüËßâÂÜôÁöÑÈùûÂ∏∏Â•ΩÔºåÊàëÈùûÂ∏∏‰Ω©Êúç„ÄÇÂèØËÉΩÊú¨‰∫∫Ê∞¥Âπ≥Â∑Æ‰∏Ä‰∫õÔºå‰∏çÁü•ÈÅìÊÇ®ÁöÑÂ∑•ÂÖ∑ÊúâÊ≤°ÊúâÂÆûÁé∞Êñ∞ËØçÂèëÁé∞ÂäüËÉΩÔºåËã•ÂÆûÁé∞‰∫ÜÊÄé‰πàÁî®ÔºåÊàñÂ¶Ç‰ΩïÂà©Áî®Áé∞ÊúâÂäüËÉΩÂÆûÁé∞Êñ∞ËØçÂèëÁé∞ÂäüËÉΩ„ÄÇ
"
CRFSegment.cs‰∏≠tagÊñπÊ≥ïÊúâ‰∏ÄË°å‰ª£Á†ÅÊ≤°ÁúãÊáÇ,"Âçö‰∏ªÔºåÂú®tagÊñπÊ≥ï‰∏≠ÔºåÂà©Áî®Áª¥ÁâπÊØîÊñπÊ≥ïËÆ°ÁÆóÂ≠óÁ¨¶‰∏≤ÁöÑÊ†áÊ≥®Ë∑ØÂæÑÊó∂ÔºåÊúâ‰∏ÄÊÆµ‰ª£Á†ÅÊòØif (matrix[pre][now] <= 0) continueÔºõ ‰∏∫‰ªÄ‰πàtagÁöÑËΩ¨ÁßªÊ¶ÇÁéá<0ÔºåÂ∞±Áï•Ëøá‰∫ÜÂë¢Ôºü
"
fix:ÁπÅ‰ΩìÂàÜËØçbug,"ÁâàÊú¨:208e338600be8ac219b9112a1163aaeef2df2b8a

```
TraditionalChineseTokenizer.segment(""ËÆ§ÂèØÁ®ãÂ∫¶"");
```

ÁπÅ‰ΩìÂàÜËØçÂØºËá¥Á®ãÂ∫èÂºÇÂ∏∏Âá∫Èîô„ÄÇ
"
fix:ÁπÅ‰ΩìÂàÜËØçbug,"ÁâàÊú¨:208e338600be8ac219b9112a1163aaeef2df2b8a

```
 TraditionalChineseTokenizer.segment(""ËÆ§ÂèØÁ®ãÂ∫¶"");
```

ÁπÅ‰ΩìÂàÜËØçÔºåËæìÂÖ•ÂÖ®‰∏∫ÁÆÄ‰ΩìÊó∂ÂàÜÂØºËá¥Á®ãÂ∫èÂºÇÂ∏∏Âá∫Èîô„ÄÇ
ÂéüÂõ†ÊòØÁÆÄ‰ΩìËæìÂÖ•Âú®ÁπÅÁÆÄËΩ¨Êç¢Êó∂ÁîüÊàêÁöÑtermÂàóË°®Êï∞ÈáèÂíåÂêéÈù¢ÁÆÄ‰ΩìÂàÜËØçÊó∂ÁîüÊàêÁöÑtermÊï∞Èáè‰∏ç‰∏ÄËá¥„ÄÇ
"
ÊúâÂÜ≤Âä®ÊÉ≥ÊîπÊàêC#,
ËÉΩÂê¶Áªô‰∏Ä‰∏™CRFÊ®°Âûã‰∏ãËΩΩÁöÑÈìæÊé•Ôºü,
Âè•Ê≥ïÂàÜÊûêÁöÑÈÄªËæëÂª∫ËÆÆ,"ÊØîÂ¶ÇÈíàÂØπ‰∏ìÊúâÂêçËØç/Êú∫ÊûÑÁöÑÊ†áËÆ∞ÈÄªËæë
ÊàëÁªèÂ∏∏Âú®ÂÖ®ËÅöÊ±áÂêÉÈ•≠
[Êàë/rr, ÁªèÂ∏∏/d, Âú®/p, ÂÖ®ËÅö/nr, Ê±á/v, ÂêÉÈ•≠/vi, Ôºå/w]

 ""ÊàëÁªèÂ∏∏Âú®Â§ßÈìúÈîÖÂêÉÈ•≠Ôºå"",
[Êàë/rr, ÁªèÂ∏∏/d, Âú®/p, Â§ß/a, Èìú/n, ÈîÖ/n, ÂêÉÈ•≠/vi, Ôºå/w]

""ÊàëÁªèÂ∏∏Âú®Â§ßÈìúÈîÖÊµ∑È≤úÊ•ºÂêÉÈ•≠Ôºå""
[Êàë/rr, ÁªèÂ∏∏/d, Âú®/p, Â§ß/a, Èìú/n, ÈîÖ/n, Êµ∑È≤ú/nf, Ê•º/n, ÂêÉÈ•≠/vi, Ôºå/w]

Âá∫Áé∞ÁöÑ‰ΩçÁΩÆÊúÄËøëÁöÑ ‰ªãËØç„Äê„ÄëÂä®ËØç‰πãÈó¥‰∏îËá≥Â∞ëÊúâ‰∏Ä‰∏™ÂêçËØçÂá∫Áé∞ÔºåËÉΩÂê¶Â∞ÜËøô‰∫õËØçÂΩ¢Êàê‰∏ìÊúâÂêçËØç
"
‰∫∫ÂêçÁß∞ÊèêÂèñ‰∏çÂáÜÁ°Æ,"1.ÂèëÁé∞NatureÊ∫êÁ†Å‰∏≠Êúâ 
/**
     \* ËíôÂè§ÂßìÂêç
     */
nr2, ‰ΩÜÊòØÊµãËØïÂπ∂Ê≤°ÊúâÊ≠£Á°ÆÊ†áËÆ∞„ÄÇÊØîÂ¶Ç
‰∫∫ÂêçÁß∞ËØÜÂà´ÁöÑ‰æãÂ≠ê‰∏≠Ôºö
Á≠æÁ∫¶‰ª™ÂºèÂâçÔºåÁß¶ÂÖâËç£„ÄÅÈòøÂ∏ÉÊùúÂãí¬∑ÂìàÂ∞ºÂæ∑„ÄÅÈòøÂçúÊùúÁÉ≠Ë•øÊèê„ÄÅ‰ªáÂíå„ÄÅÁ©ÜÊãâÂ∏ùÂäõ„ÄÅÊñØÁê¥Ê†ºÊó•‰πê„ÄÅ Â®ú‰ªÅÈ´òÂ®É„ÄÅ‰πåÂÖ∞ÊâòÈõÖÂíåÂÆ´Êú¨Ê≠£Â§™ÈÉéÁ≠â‰∏ÄÂêå‰ºöËßÅ‰∫Ü‰ºÅ‰∏öÂÆ∂ÈòøÂçúÊùúÂãí¬∑ÂìàÁ±≥Âæ∑„ÄÇ
‰ΩøÁî®Ëøô‰∏™Âè•Â≠ê‰∏Ä‰∫õ‰∫∫ÂêçÁß∞ÊèêÂèñÈîôËØØÔºåÊàñÊ†áÊ≥®ÈîôËØØ„ÄÇ
‰∏çÂ§™ÁêÜËß£ÁÆóÊ≥ïÔºåÊòØÂê¶‰∏ÄÂÆöÈúÄË¶ÅÂä†ÂÖ•Â≠óÂÖ∏Ôºü

2.ÂÜçÊØîÂ¶ÇÔºö
                ""ËØ∑Êî∂ÂõûÊùéÁèÖ300ÂÖÉÈí±"",
                ""ËØ∑Êî∂ÂõûÊùéÁªÖ300ÂÖÉÈí±"",
                ""ËØ∑Êî∂ÂõûÊùéË•ø300ÂÖÉÈí±"",
                ""ËØ∑Êî∂ÂõûÊùéÁ°í300ÂÖÉÈí±"",

Ê†áËÆ∞‰∫∫ÂêçÔºöÊùéÁèÖ ÊùéË•ø Ê≠£Á°ÆÔºõÊùéÁªÖ ÊùéÁ°í ÈîôËØØ„ÄÇ

Ëøô‰∏™‰∫∫ÂêçÁß∞Ê†áËÆ∞Ôºå‰∏çÁî®Ê®°ÂûãÊñá‰ª∂ÂêóÔºü
"
ËÉΩÂê¶Êèê‰æõSOLR5.XÁöÑÊîØÊåÅ,"ÊÇ®ÂèØ‰ª•ÂèÇËÄÉIKÂàÜËØçÂô®ÁöÑSOLRÂàÜËØçÊ∫êÁ†ÅÔºàËøô‰∏™ÁâàÊú¨ÊòØÊîØÊåÅsolr5ÁöÑ‰øÆËÆ¢ÁâàÔºâ
https://github.com/EugenePig/ik-analyzer-solr5
"
‰øÆÊîπÁπÅ‰ΩìÂàÜËØçbug,"ÁπÅ‰ΩìÂàÜËØçÊµÅÁ®ã‰øÆÊîπ‰∏∫ÂÖàÁπÅÁÆÄËΩ¨Êç¢ÔºåÂÜçÊ†πÊçÆÂàÜËØçÁªìÊûúËøîÂõûÂéüÂ≠óÁ¨¶‰∏≤ÔºåËÄå‰∏çÊòØÁõ¥Êé•‰ΩøÁî®ÁÆÄÁπÅËΩ¨Êç¢„ÄÇ
"
‰øÆÊ≠£‰∏â‰∏™ÁÆÄÁπÅËΩ¨Êç¢ÁöÑÊÉØÁî®Ê≥ï„ÄÇ,"""ÂúãÁ´ãÂòâÁæ©Â§ßÂ≠∏ÁîüÂëΩÁßëÂ≠∏Èô¢È£üÂìÅÁßëÂ≠∏Á≥ª""
Ë¢´ÈîôËØØÂú∞ÂàÜËØç‰∏∫Ôºö
""ÂúãÁ´ã ÂòâÁæ© Â§ßÊñà ÁîüÂëΩ ÁßëÊñàÈô¢ È£üÂìÅ ÁßëÊñàÁπ´""
"
dfsfsdf,"fdfdf
"
Suggester ËÉΩÂê¶Â¢ûÂä†removeAllSentencesÊñπÊ≥ïÔºå‰æø‰∫é‰∏ÄÊ¨°Âä†ËΩΩÂ§öÊ¨°‰ΩøÁî®,"Â¶ÇÔºö
    public void removeAllSentences() {
        for (IScorer scorer : scorerList)
        {
            scorer.removeAllSentences();
        }
    }
"
ÁπÅ‰Ωì‰∏≠ÊñáËØÜÂà´‰∫∫Âêç,"""„ÄåÂúãÈöõË∂≥ÁêÉÁ∏ΩÊúÉ„ÄçÔºàFIFAÔºâ‰∏ªÂ∏≠Â∏ÉÊãâÁâπÔºàSepp BlatterÔºâ5Êúà29Êó•ÈÄ£‰ªªÊàêÂäü.""

Â∞Ü‚Äú‰ªªÊàêÂäü‚ÄùËØÜÂà´‰∏∫‰∫∫ÂêçÔºåÊÑüËßâÁ≤óÊö¥ÁöÑÂ∞Ü‚Äú‰ªªÊàêÂäü‚ÄùÂä†Âà∞nr.txtÈáå‰ºº‰πé‰∏çÂ¶•Ôºå‰πüËÆ∏Êüê‰∫õËØ≠Â¢ÉÈáåÁúüÊúâ‰∫∫Âè´‚Äú‰ªªÊàêÂäü‚ÄùÔºå‰∏çÁü•ÈÅìÊúâÊ≤°ÊúâÊõ¥Â•ΩÁöÑÂ§ÑÁêÜÂäûÊ≥ï„ÄÇ
"
PortableÂêåÊ≠•ÂçáÁ∫ßÂà∞v1.2.4,
PortableÂêåÊ≠•ÂçáÁ∫ßÂà∞v1.2.4,
Êó∂Èó¥Ê†áÂáÜÂåñÁöÑÈóÆÈ¢ò,"‰∏çÁü•ÈÅìÂ§ßÁ•ûÊúâÊ≤°ÊúâÊó∂Èó¥Âä†‰∏Ä‰∏™Êó∂Èó¥Ê†áÂáÜÂåñÁöÑÊ®°ÂùóÔºüÂè¶Â§ñÔºåÊó∂Èó¥ÁöÑÂà§Êñ≠Â•ΩÂÉè‰πüÊúâ‰∫õÈóÆÈ¢òÔºå‰æãÂ¶Ç  ‰∏ãÂçà/t, 3/m, Êó∂Áî±/nr, Âåó‰∫¨/ns, Âá∫Âèë/v, ËøôÂè•ËØùÂ∞±Ê≤°ÊúâÊää‰∏ãÂçà3Êó∂Êï¥‰∏™Âà§Êñ≠‰∏∫Êó∂Èó¥ÔºåÂ∏åÊúõÂ§ßÁ•ûËÉΩÊîπËøõ‰∏ãÔºåË∞¢Ë∞¢„ÄÇ
"
Êú∫ÊûÑÂêçËØÜÂà´ÂØºËá¥ÁöÑÂàáËØçÈîôËØØ,"ÊåâÁÖßË¶ÅÊ±ÇÔºåÂüéÈïáÂ±ÖÊ∞ëÂåª‰øùÂíåÊñ∞ÂÜúÂêàÊîøÁ≠ñËåÉÂõ¥ÂÜÖ‰ΩèÈô¢Ë¥πÁî®ÊîØ‰ªòÊØî‰æãÂàÜÂà´ËææÂà∞70%‰ª•‰∏äÂíå75%Â∑¶Âè≥„ÄÇÈÄÇÂΩìÊèêÈ´òÂüéÈïáÂ±ÖÊ∞ëÂåª‰øùÂíåÊñ∞ÂÜúÂêàÈó®ËØäÁªüÁ≠πÂæÖÈÅáÊ∞¥Âπ≥„ÄÇ ÂõΩÂä°Èô¢ÂåªÊîπÂäûÊîøÁ≠ñÁªÑË¥üË¥£‰∫∫ÂÇÖÂç´Âú®Êé•ÂèóËÆ∞ËÄÖÈááËÆøÊó∂Ë°®Á§∫Ôºå‰ªéÊ∑±ÂåñÂåªÊîπ‰ª•Êù•ÔºåÂü∫Êú¨ÂåªÁñó‰øùÈöúÂà∂Â∫¶Âª∫ËÆæÂä†Âø´Êé®ËøõÔºåÂåª‰øù‰øùÈöúÊ∞¥Âπ≥‰πüÂú®‰∏çÊñ≠ÊèêÈ´òÔºåÂêÑÁ∫ßÊîøÂ∫úÂØπÂüéÈïáÂ±ÖÊ∞ëÂåª‰øùÂíåÊñ∞ÂÜúÂêàÁöÑË°•Âä©Ê†áÂáÜ‰ªé2008Âπ¥ÁöÑ‰∫∫Âùá80ÂÖÉÊèêÈ´òÂà∞‰∫Ü2013Âπ¥ÁöÑ280ÂÖÉ„ÄÇÂà∞2015Âπ¥ÔºåÂüéÈïáÂ±ÖÊ∞ëÂåª‰øùÂíåÊñ∞ÂÜúÂêàÊîøÂ∫úË°•Âä©Ê†áÂáÜÂ∞ÜÊèêÈ´òÂà∞ÊØè‰∫∫ÊØèÂπ¥360ÂÖÉ‰ª•‰∏ä„ÄÇ ‰∏çËøáÔºå‰πüÊúâ‰∏ìÂÆ∂ÊãÖÂøÉÔºåÈöèÁùÄ‰øùÈöúÊ∞¥Âπ≥ÊèêÂçáÔºå‰∏çÊñ≠Â¢ûÂä†ÁöÑÂåª‰øùÂü∫ÈáëÊîØÂá∫Ê≠£ÂØπÂåª‰øùÂü∫Èáë‚ÄúÊî∂ÊîØÂπ≥Ë°°„ÄÅÁï•ÊúâÁªì‰Ωô‚ÄùÁöÑËøêË°åÂéüÂàôÂΩ¢ÊàêÂéãÂäõÔºåÁîöËá≥Âú®‰∏Ä‰∫õÂú∞Âå∫ÈÄ†ÊàêË∂ÖÊîØÈ£éÈô©„ÄÇ ËÆ∞ËÄÖ‰∫ÜËß£Âà∞ÔºåÂõΩÂä°Èô¢Ê≠§ÂâçÂèëÊñáË¶ÅÊ±ÇÔºåÂú®‰ªäÂπ¥6ÊúàÂ∫ïÂâçÂêÑÁúÅË¶ÅÂÖ®Èù¢ÂêØÂä®Âüé‰π°Â±ÖÊ∞ëÂ§ßÁóÖ‰øùÈô©ËØïÁÇπÂ∑•‰Ωú„ÄÇÂ§ßÁóÖÂåª‰øùÁöÑËµÑÈáëÊù•Ê∫ê‰∫éÁé∞ÊúâÂåª‰øùÂü∫ÈáëÁöÑÁªì‰ΩôÔºåÂç≥‰ªéÂüéÈïáÂ±ÖÊ∞ëÂåª‰øùÂü∫Èáë„ÄÅÊñ∞ÂÜúÂêàÂü∫Èáë‰∏≠ÂàíÂá∫ÔºåÈááÂèñÂêëÂïÜ‰∏ö‰øùÈô©Êú∫ÊûÑË¥≠‰π∞‰øùÈô©ÁöÑÊñπÂºèÔºå‰∏çÂÜçÈ¢ùÂ§ñÂ¢ûÂä†Áæ§‰ºó‰∏™‰∫∫Áº¥Ë¥πË¥üÊãÖ„ÄÇ Ë¥¢ÁªèËØÑËÆ∫Âëò‰Ωô‰∏∞ÊÖßËÆ§‰∏∫ÔºåÂ¶ÇÊûúÊèêÈ´òÂ§ßÁóÖ‰øùÈô©‰øùÈöúÊ∞¥Âπ≥ÔºåÊâ©Â§ßÊä•ÈîÄÁßçÁ±ªÔºåÊèêÈ´òÊä•ÈîÄÊØî‰æãÔºåËÄåÂè™Âú®Âåª‰øùÂü∫ÈáëÁªì‰ΩôÁöÑÂ≠òÈáè‰∏äË¶ÅËµÑÈáëÊù•Ê∫êÔºåÂæàÂø´Â∞ÜÈÄ†Êàê‰∏§‰∏™ÊñπÈù¢ÁöÑÈóÆÈ¢òÔºö‰∏ÄÊòØÂèØËÉΩÂΩ±ÂìçÂà∞Âåª‰øùÁöÑËµÑÈáëÊîØ‰ªòÔºå‰∫åÊòØÁªì‰ΩôËµÑÈáëËøúËøú‰∏çÂ§ü„ÄÇ ÂØπÊ≠§Ôºå„Ää‰ªªÂä°„ÄãË¶ÅÊ±ÇÔºåÊé®ËøõÂüé‰π°Â±ÖÊ∞ëÂü∫Êú¨Âåª‰øùÂà∂Â∫¶Êï¥ÂêàÂíåÂÆåÂñÑÁ≠πËµÑÊú∫Âà∂„ÄÇÂÆåÂñÑÊîøÂ∫ú„ÄÅÂçï‰ΩçÂíå‰∏™‰∫∫ÂêàÁêÜÂàÜÊãÖÁöÑÂü∫Êú¨Âåª‰øùÁ≠πËµÑÊú∫Âà∂„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂Âª∫Á´ãÁ®≥ÂÆöÂèØÊåÅÁª≠„ÄÅÂä®ÊÄÅË∞ÉÊï¥ÁöÑÁ≠πËµÑÊú∫Âà∂ÔºåÂú®ÈÄêÊ≠•ÊèêÈ´òÊï¥‰ΩìÁ≠πËµÑÊ†áÂáÜÁöÑÂêåÊó∂ÔºåÊåâÁÖßÁßØÊûÅÁ®≥Â¶•„ÄÅÈÄêÊ≠•Âà∞‰ΩçÁöÑÂéüÂàôÔºåÈÄêÊ≠•ÊèêÈ´ò‰∏™‰∫∫Áº¥Ë¥πÂç†Êï¥‰ΩìÁ≠πËµÑÁöÑÊØîÈáç„ÄÇ ËÄåÊ†πÊçÆÊ≠§ÂâçÁõ∏ÂÖ≥Ë¶ÅÊ±ÇÔºåÂüéÈïáÂ±ÖÊ∞ëÂåªÁñó‰øùÈô©‰∏™‰∫∫Áº¥Ë¥πÂ∫îÈöèÊÄªÁ≠πËµÑÊ∞¥Âπ≥‰ΩúÁõ∏Â∫îË∞ÉÊï¥Ôºå‰∏™‰∫∫Áº¥Ë¥πÂ∫îÂç†‰∫∫ÂùáÊÄªÁ≠πËµÑ20%Â∑¶Âè≥„ÄÇÂØπÊ≠§Ôºå‰∏≠Â§ÆË¥¢ÁªèÂ§ßÂ≠¶ÊïôÊéàË§öÁ¶èÁÅµÂú®Êé•ÂèóËÆ∞ËÄÖÈááËÆøÊó∂Ë°®Á§∫ÔºåÁõÆÂâçÂêÑÂú∞Ê≠£Á†îÁ©∂Âª∫Á´ãÂüéÈïáÂ±ÖÊ∞ëÂåªÁñó‰øùÈô©Ë¥¢ÊîøË°•Âä©Âíå‰∏™‰∫∫Áº¥Ë¥πÁßëÂ≠¶ÂêàÁêÜ„ÄÅÂçèÂêåÂ¢ûÈïøÁöÑÊú∫Âà∂ÔºåÂº∫Âåñ‰∏™‰∫∫Áº¥Ë¥π‰πâÂä°„ÄÇ ÊÉ†Ê∞ëÂõûË¥≠Â§ßÂûãÂåªÁñóËÆæÂ§áÈôç‰ΩéÊ£ÄÊü•Ë¥π „Ää‰ªªÂä°„ÄãÊåáÂá∫ÔºåÈôç‰ΩéËçØÂìÅÂíåÈ´òÂÄºÂåªÁî®ËÄóÊùê‰ª∑Ê†ºÔºåÈôç‰ΩéÂ§ßÂûãÂåªÁî®ËÆæÂ§áÊ£ÄÊü•„ÄÅÊ≤ªÁñó‰ª∑Ê†ºÔºåÂ∑≤Ë¥∑Ê¨æÊàñÈõÜËµÑË¥≠‰π∞ÁöÑÂ§ßÂûãËÆæÂ§áÂéüÂàô‰∏äÁî±ÊîøÂ∫úÂõûË¥≠ÔºåÂõûË¥≠ÊúâÂõ∞ÈöæÁöÑÈôêÊúüÈôç‰Ωé‰ª∑Ê†º„ÄÇ‰ª∑Ê†ºË∞ÉÊï¥ÊîøÁ≠ñË¶Å‰∏éÂåª‰øùÊîØ‰ªòÊîøÁ≠ñÁõ∏Ë°îÊé•„ÄÇ ‚ÄúËÄÅÁôæÂßìÊä±ÊÄ®ÁúãÁóÖË¥µÁöÑÈóÆÈ¢òËÉΩÂ§üÁúüÊ≠£ÁºìËß£‰∫Ü„ÄÇ‚Äù‰∏Ä‰Ωç‰∏çÊÑøÈÄèÈú≤ÂßìÂêçÁöÑ‰∏öÂÜÖ‰∫∫Â£´ÊåáÂá∫ÔºåÂ§öÂπ¥Êù•ÊÇ£ËÄÖÊä±ÊÄ®ÁúãÁóÖË¥µÔºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËØäÁñóË¥πÁî®‰∏äÂåÖÊã¨Â§ßÂûãËÆæÂ§áÁöÑÊ£ÄÊü•Ë¥πÁî®Á≠âÊñπÈù¢„ÄÇÊîøÂ∫ú‰πüËÆ©ÂåªÈô¢Èôç‰ª∑ÔºåÂΩìÂ§ßÂûãËÆæÂ§áÁöÑÊàêÊú¨„ÄÅÁª¥‰øÆË¥πÁî®ÈÉΩÊëÜÂú®ÈÇ£ÈáåÔºåÊàêÊú¨ÂøÖÁÑ∂ÂàÜÊëäÂà∞ÊÇ£ËÄÖÁöÑÊ£ÄÊü•Ë¥πÁî®‰∏≠„ÄÇ ËÆ∞ËÄÖ5Êúà28Êó•Ëá¥ÁîµÂç´ËÆ°ÂßîÊñ∞ÈóªÂÆ£‰º†Âè∏ÂâØÂè∏ÈïøÂÆãÊ†ëÁ´ã‰∫ÜËß£ÂÖ∑‰ΩìÁöÑÂõûË¥≠ÊîøÁ≠ñÔºå‰ΩÜÊà™Ëá≥ËÆ∞ËÄÖÂèëÁ®øÂâçÔºåÁîµËØù‰∏ÄÁõ¥Êó†Ê≥ïÊé•ÈÄö„ÄÇ ËÆ∞ËÄÖ‰ªéÈ£üËçØÊÄªÂ±Ä‰∏ªÂäûÁöÑÂåªÁñóÂô®Ê¢∞Ë¥®Èáè‰∏áÈáåË°åÊ¥ªÂä®‰∏≠‰∫ÜËß£Âà∞ÔºåÂõΩÂÜÖÂ§ßÁ∫¶Ëøë70%ÁöÑÂ§ßÂûãÂåªÁñóËÆæÂ§á‰ªéÈîÄÂîÆÂà∞ÂîÆÂêéÊúçÂä°ÂùáË¢´GE„ÄÅË•øÈó®Â≠ê„ÄÅÈ£ûÂà©Êµ¶Á≠âË∑®ÂõΩÂÖ¨Âè∏ÂûÑÊñ≠„ÄÇËøô‰ΩìÁé∞Âú®ÊäÄÊúØÂíåËÄóÊùê‰∏äÁöÑÂûÑÊñ≠Áõ¥Êé•ÂØºËá¥Â∏ÇÂú∫Á´û‰∫â‰∏çÂÖÖÂàÜÔºå‰ª∑Ê†ºËôöÈ´ò„ÄÇ‰∏∫‰∫ÜËøΩÊ±ÇÊõ¥È´òÁöÑÂà©Ê∂¶ÔºåÁîü‰∫ßÂéÇÂÆ∂‰ªéÊèê‰æõÁª¥‰øÆÁöÑÈÖç‰ª∂‰∏≠ËµöÂèñÈ´òÈ¢ùÂ∑Æ‰ª∑Êàê‰∏∫‰∏ªË¶ÅÁõàÂà©ÊâãÊÆµÔºåÂîÆÂêéÊúçÂä°Êèê‰æõÂïÜÂÆûÈôÖ‰∏äÂèòÊàê‰∫ÜÂåªÁñóËÆæÂ§áÈÖç‰ª∂ÈîÄÂîÆÂïÜ„ÄÇ ÊçÆÂØπÂ±±‰∏úËèèÊ≥Ω‰∏ÄÂÆ∂ÂåªÈô¢ÁöÑË∞ÉÊü•ÔºåÁõÆÂâçÂåªÁñóËÆæÂ§áÂéÇÂÆ∂ÂîÆÂêé‰øùË¥®Êúü‰∏ÄËà¨ÈÉΩÊòØ‰∏ÄÂπ¥Ôºå‰πãÂêéÁöÑÁª¥‰øÆÊúâÂÖ®‰øù„ÄÅÊäÄÊúØ‰øùÂíåÂçïÊ¨°Áª¥‰øÆ3ÁßçÊÉÖÂÜµ„ÄÇÂéÇÂÆ∂ÂØπÂá†ÁßçËÆæÂ§áÁöÑÁª¥‰øÆÊä•‰ª∑Â¶Ç‰∏ãÔºöÁæéÂõΩ‰∫ß64ÊéíVCTÔºå‰∏ÄÂπ¥ÂÖ®‰øù38‰∏áÂÖÉÔºå‰∏ÄÂπ¥ÊäÄÊúØ‰øù15‰∏áÂÖÉÔºåÂçïÊ¨°‰∫∫Â∑•Ë¥π3.1‰∏áÂÖÉÔºõÁæéÂõΩ‰∫ßÂçïÊéíCT‰∏ÄÂπ¥ÂÖ®‰øù30‰∏áÂÖÉÔºå‰∏ÄÂπ¥ÊäÄÊúØ‰øù13.6‰∏áÂÖÉÔºåÂçïÊ¨°‰∫∫Â∑•Ë¥π1.1‰∏áÂÖÉ‚Ä¶‚Ä¶Áî±‰∫éÁõÆÂâçÂõΩÂÆ∂Ê≤°Êúâ‰∏Ä‰∏™Â§ßËá¥Áªü‰∏ÄÁöÑÂÆö‰ª∑Ê†áÂáÜÔºåÂü∫Êú¨‰∏äÁî±ÂåªÈô¢ÂíåÂéÇÂÆ∂Ë∞àÂà§ÔºåÊúÄÂêéËøòÊòØÂéÇÂÆ∂ËØ¥‰∫ÜÁÆó„ÄÇ ‚Äú‰∏çÂ∞ëËøõÂè£ÁöÑÂ§ßÂûãÂåªÁñóËÆæÂ§áÁª¥‰øÆË¥πÁî®Âç†‰∫ßÂìÅÊú¨Ë∫´Ë¥πÁî®ÁöÑ‰∏âÂàÜ‰πã‰∏ÄÊàñ‰∫åÂàÜ‰πã‰∏ÄÔºåÁîöËá≥ÊúâÁöÑÂ∑≤ÁªèË∂ÖËøá‰∫Ü‰∫ßÂìÅ‰ª∑Ê†º„ÄÇÂõ†Ê≠§ÔºåÂç´ÁîüÂ±ÄËÆ©ÂåªÈô¢Èôç‰ΩéÊ£ÄÊü•Ë¥πÁî®‰ª∑Ê†ºÔºåÊ†πÊú¨Â∞±‰∏çÂèØËÉΩÔºåËÄåÁî®ÂõûË¥≠Ëøô‰∏ÄÊâãÊÆµËß£ÂÜ≥‰∫ÜËØäÁñóË¥πÁî®È´òÁöÑÈóÆÈ¢ò„ÄÇ‚Äù‰∏äËø∞‰∫∫Â£´ÊåáÂá∫„ÄÇ ÈºìÂä±ÂÆûÁé∞Ë∑®ÁúÅËÅîÂêàÊãõÊ†áÈôç‰ΩéËçØ‰ª∑ ‚ÄúÂ∞ΩÁÆ°Ë∑®ÁúÅÊãõÊ†áÂú®ÊîøÂ∫úÈááË¥≠‰∏≠Âπ∂‰∏çÊòØ‰∏Ä‰∏™Êñ∞È≤ú‰∫ãÔºå‰ΩÜËøôÊòØËçØÂìÅË°å‰∏öÁ¨¨‰∏ÄÊ¨°ÊèêÂá∫„ÄÇ‚Äù‰∏≠ÂõΩÂåªËçØ‰ºÅ‰∏öÁÆ°ÁêÜÂçè‰ºöÂâØ‰ºöÈïøÁâõÊ≠£‰πæ28Êó•Ë°®Á§∫ÔºåË∑®ÁúÅËÅîÂêàÊãõÊ†áÂèØËÉΩÊé®ËøõÊãõÊ†áÁéØËäÇÁöÑÂÖ¨ÂºÄ„ÄÅÈÄèÊòé„ÄÇ „Ää‰ªªÂä°„ÄãÂú®‚ÄúÂä†Âø´Êé®Ë°åÂÖ¨Á´ãÂåªÈô¢ÊîπÈù©‚Äù‰∏≠ÊåáÂá∫ÔºåÈºìÂä±Ë∑®ÁúÅËÅîÂêàÊãõÊ†áÈááË¥≠‰øùËØÅËçØÂìÅË¥®ÈáèÂÆâÂÖ®ÔºåÂàáÂÆûÈôç‰ΩéËçØÂìÅ‰ª∑Ê†ºÔºåÊúâÊù°‰ª∂ÁöÑÂú∞Âå∫Ë¶ÅÂª∫Á´ã‰∏éÂü∫Â±ÇÂü∫Êú¨ËçØÁâ©ÈááË¥≠ËÅîÂä®ÁöÑÊú∫Âà∂„ÄÇÈÄêÊ≠•ËßÑËåÉÈõÜ‰∏≠ÈááË¥≠ËçØÂìÅÁöÑÂâÇÂûã„ÄÅËßÑÊ†ºÂíåÂåÖË£Ö„ÄÇÊé®ËøõÈ´òÂÄºÂåªÁî®ËÄóÊùêÂÖ¨ÂºÄÈÄèÊòé„ÄÅÂÖ¨Âπ≥Á´û‰∫âÁΩë‰∏äÈò≥ÂÖâÈááË¥≠„ÄÇËçØÂìÅÂíåÈ´òÂÄºÂåªÁî®ËÄóÊùêÈááË¥≠Êï∞ÊçÆÂÆûË°åÈÉ®Èó®ÂíåÂå∫ÂüüÂÖ±‰∫´„ÄÇ ‰∏≠ÊäïÈ°æÈóÆÂåªËçØË°å‰∏öÁ†îÁ©∂ÂëòËÆ∏Áé≤Â¶ÆÊåáÂá∫ÔºåÂéøÁ∫ßÂåªÈô¢ËÅîÂêàÊãõÊ†áËÉΩÂ§üÈôç‰ΩéËçØÂìÅÈÖçÈÄÅË¥πÁî®ÔºåËøôÈÉ®ÂàÜÂ∑Æ‰ª∑ÂàÜÊëäÂà∞ËçØÂìÅ‰∏äÊúâÂà©‰∫éÈôç‰ΩéËçØÂìÅ‰ª∑Ê†ºÔºåËÄå‰∏îÊúâÂà©‰∫éÂπ≥Ë°°ÊãõÊ†áËçØÂìÅÁöÑË¥®ÈáèÂíå‰ª∑Ê†ºÔºåÊîπÂèò‰ª•ÂæÄÂ§áÂèó‰∏öÁïåËØüÁóÖ‚ÄúÂîØ‰Ωé‰ª∑ÊòØÂèñ‚ÄùÁöÑÊãõÊ†áËßÑÂàô„ÄÇ ‰∏çÂ∞ëÂåªËçØ‰ºÅ‰∏öÁöÑËë£‰∫ãÈïøÈÉΩÂêëËÆ∞ËÄÖË°®Á§∫ÔºåÁé∞Ë°å‰ª•ÁúÅ‰∏∫Âçï‰ΩçÁöÑËçØÂìÅÈõÜ‰∏≠ÊãõÊ†áÈááË¥≠Âà∂Â∫¶Â≠òÂú®‰∏çÂ∞ëÂºäÁ´ØÔºåÂä≥Ê∞ë‰º§Ë¥¢ÔºåÂ∫îËØ•ËøõË°åÊîπÈù©„ÄÇ ËÆ∏Áé≤Â¶ÆÊåáÂá∫ÔºåËçØÂìÅÊãõÊ†áÈááË¥≠ËßÑÂàôÂ≠òÂú®ÊºèÊ¥ûÔºåÁº∫‰πèÁî±ÊîøÂ∫úÈÉ®Èó®Êèê‰æõÂøÖË¶ÅÁöÑ„ÄÅËæÉÂÖ®Èù¢ÁöÑËçØÂìÅ‰ø°ÊÅØ„ÄÇ‰∫åÊòØËçØÂìÅÈõÜ‰∏≠ÊãõÊ†áÈááË¥≠ÁõëÁù£ÁÆ°ÁêÜÈöæ‰ª•Âà∞‰ΩçÔºåÊãõÊ†áÂêéÊ≤°ÊúâËßÑËåÉÁöÑ‰ø°ÊÅØÂèçÈ¶àÁ®ãÂ∫èÔºåÁõëÁÆ°ÈÉ®Èó®Êó†Ê≥ïÂØπÊãõÊ†á‰∫∫ÂêàÂêåÂ±•Ë°åÊÉÖÂÜµËøõË°åÊúâÊïàÁõëÁÆ°Ôºå‰ΩøÈõÜ‰∏≠ÊãõÊ†áÈááË¥≠Ê¥ªÂä®‚ÄúÂÖàÁÉ≠ÂêéÂáâÔºåÊµÅ‰∫éÂΩ¢Âºè‚Äù„ÄÇ ‚ÄúÊãõÊäïÊ†áÊú¨Êù•ÊòØÂõΩÈôÖ‰∏äÈÄöË°åÁöÑ‰∏ÄÁßçÊØîËæÉÂ•ΩÁöÑÂ∏ÇÂú∫ÂåñÈááË¥≠ÊñπÂºèÔºå‰ΩÜÁõÆÂâçÊàëÂõΩÊîøÂ∫úÂåÖÂäûÁöÑËçØÂìÅÊãõÊ†áÊîøÁ≠ñ‰∏•ÈáçÂºÇÂåñÔºåÊàê‰∏∫ËçØÂìÅËøõÂÖ•Â∏ÇÂú∫ÊâÄÊâßË°åÁöÑ‰∫åÊ¨°Ë°åÊîøÁÆ°Âà∂„ÄÇ‚ÄùËëµËä±ËçØ‰∏öÈõÜÂõ¢Ëë£‰∫ãÈïøÂÖ≥ÂΩ¶ÊñåÊåáÂá∫ÔºåÁé∞Ë°åËçØÂìÅÊãõÊäïÊ†áÁöÑÂÆûË¥®ÊòØÊääËçØÂìÅËøõÂÖ•ÂåªÁñóÊú∫ÊûÑ‰ΩøÁî®ÁöÑÊ≠£Â∏∏Â∏ÇÂú∫Ë°å‰∏∫ÂèòÊàê‰∫ÜË°åÊîøÂÆ°ÊâπÔºåÂπ∂‰∏îÂêåÁßç‰∫ßÂìÅ‰∏çÂêåÂå∫ÂüüÂπ¥Âπ¥ÂÆ°ÊâπÔºåÊãõÊ†á‰∏ªÁÆ°ÈÉ®Èó®Êàê‰∫ÜËçØÂìÅÈ¢ÜÂüüÁöÑÊúÄÂ§ßÂÆ°ÊâπÊùÉÊú∫ÊûÑ„ÄÇ ‰∏çËøáÔºåËÆ∏Áé≤Â¶Æ‰πüÊåáÂá∫ÔºåËçØÂìÅË∑®ÁúÅËÅîÂêàÊãõÊ†áÈááË¥≠ÊòØÂê¶ÂèØË°å‰∏éÂêÑÁúÅÁöÑÂå∫‰ΩçÊúâÂÖ≥ÔºåÂõ†‰∏∫ËçØ‰ºÅË¶ÅÊâøÊãÖËøêËæìÊàêÊú¨ÔºåÂ¶ÇÊûúÂà∞‰∏§ÁúÅË∑ùÁ¶ª‰∏ç‰∏ÄÊ†∑ÔºåÊâøÊãÖÁöÑËøêË¥πÊàêÊú¨ÊòéÊòæ‰∏çÂêåÔºåÂ¶ÇÊûúÊåâÁÖßÁªü‰∏ÄÊãõÊ†áÂπ∂‰∏çÂêàÁêÜ„ÄÇ‚ÄúÊØîÂ¶ÇÂåó‰∫¨„ÄÅÂ§©Ê¥•Ëøô‰∏§‰∏™Áõ¥ËæñÂ∏ÇÔºåËæñÂå∫ËæÉÂ∞èÔºåËçØ‰ºÅÁªü‰∏ÄÈÖçÈÄÅÊâøÊãÖÁöÑË¥πÁî®Â∑Æ‰∏çÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºåË∑®ÁúÅËÅîÂêàÊãõÊ†áÈááË¥≠ÊòØÂèØË°åÁöÑ„ÄÇ‚Äù

ÂêØÁî®Êú∫ÊûÑÂêçËØÜÂà´ÂêéÔºåÂàáËØçÁªìÊûú‰ºöÊää  ""ËëµËä±  ÂΩìÂÅöÊú∫ÊûÑÂêç
"
Â¶Ç‰ΩïÊ∑ªÂä†Êñ∞ÁöÑÂÅúÁî®ËØçÔºàËøòÊòØ‰∏çË°åÔºâ,"ÊàëÂú®dictionaryÁõÆÂΩï‰∏ãÁöÑÈªòËÆ§ÂÅúÁî®ËØçÊñá‰ª∂‰∏≠Ê∑ªÂä†Êñ∞ÁöÑÂÅúÁî®ËØçÔºåÈáçÊñ∞ËøêË°å‰πãÂêéÂèëÁé∞Êñ∞ÁöÑÂÅúÁî®ËØçÊ≤°ÊúâÂéªÊéâ„ÄÇ
Â¶ÇÊûúÂÖàÂà†Èô§ÁºìÂ≠òÊñá‰ª∂ÂÜçÊ∑ªÂä†ËØçËØ≠‰πü‰∏çË°åÔºå‰ΩÜÊåâÁÖßËøôÁßçÊñπÊ≥ïÊ∑ªÂä†Ëá™ÂÆö‰πâËØçËØ≠ÊòØÊúâÊïàÁöÑÔºå‰ΩÜÊ∑ªÂä†ÂÅúÁî®ËØç‰∏çË°å„ÄÇ
"
crf problem,"ÂΩìÈááÁî®data-for1.2.2‰∏≠ÁöÑcrfËÆ≠ÁªÉÊï∞ÊçÆËøõË°åÂàÜËØçÊó∂,Âá∫Áé∞ÈîôËØØ,ÂÖ∑‰ΩìÊòØ
‰ª£Á†Å‰∏∫:
CRFModel crfModel = CRFModel.loadTxt(""E:\scalaworkspace\Eyas\data\model\segment\CRFSegmentModel.txt.bin"");
        System.out.println(""locad finish"");
ÈîôËØØ‰∏∫
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""U 0 0 : (   ??z?_K????]S?ÂÜµ??{???k??n??""
"
Ëá™ÂÆö‰πâËØçÂÖ∏‰ºº‰πéÊ≤°Áî®,"System.out.println(CustomDictionary.add(""È´òÂ§ß‰∏ä"",""a 1024""));
termList = HanLP.segment(""Â§ñËßÇÁªùÂØπÈ´òÂ§ß‰∏äÔºå‰∏ç‰ø°ÁöÑÊòØÊ≤°ËßÅËøá."");
System.out.println(termList);

ËæìÂá∫ÁªìÊûú

[Â§ñËßÇ/n, ÁªùÂØπ/d, È´òÂ§ß/a, ‰∏ä/f, Ôºå/w, ‰∏ç‰ø°/v, ÁöÑ/ude1, ÊòØ/vshi, Ê≤°/d, ËßÅËøá/v, ./w]
"
ÊåâÁÖß‰Ω†ÊâãÂÜå‰∏äÊèê‰æõÊñπÊ≥ïÊ∑ªÂä†‰∫∫ÂêçËØØÂà§‰∏çÊàêÂäü,"ÊåâÁÖß‰Ω†Êèê‰æõÁöÑÊñπÊ≥ïÔºå‰øÆÊîπ‰∫Üdata /dictionary / person / nr.txtÔºåÂä†ÂÖ•‰∏Ä‰∏™Êñ∞ËØçÊù°Ôºå‰ΩÜÊòØÁ≥ªÁªü‰æùÁÑ∂ËØØÂà§„ÄÇ
ÊØîÂ¶ÇÔºö‚Äú‰∏á‰ΩôÂÖÉ‚Äù Êú¨‰∏çËØ•ÊòØ‰∫∫ÂêçÔºåÊ∑ªÂä†ËØ•ËØçÊù°Ôºå‰ΩÜÊòØÊúÄÁªàÁªìÊûú‰æùÁÑ∂ËØØÂà§ÔºÅÊ±ÇËß£ÂÜ≥ÊñπÊ≥ï„ÄÇ
"
Â¶Ç‰ΩïÊ∑ªÂä†Êñ∞ÁöÑÂÅúÁî®ËØçÔºàËøòÊòØ‰∏çË°åÔºâ,"ÊàëÂú®dictionaryÁõÆÂΩï‰∏ãÁöÑÈªòËÆ§ÂÅúÁî®ËØçÊñá‰ª∂‰∏≠Ê∑ªÂä†Êñ∞ÁöÑÂÅúÁî®ËØçÔºåÈáçÊñ∞ËøêË°å‰πãÂêéÂèëÁé∞Êñ∞ÁöÑÂÅúÁî®ËØçÊ≤°ÊúâÂéªÊéâ
"
ÂØπ‰∏çËµ∑ÔºåÊúÄËøëÊúâÁÇπÂøôÔºåÂõûÂ§çÂèØËÉΩ‰ºöÊúâÁÇπÊÖ¢,"Â§ßÂÆ∂Â•ΩÔºåÊÑüË∞¢ÊèêÂá∫ÁöÑÂÆùË¥µÊÑèËßÅ„ÄÇ
ÊàëÁ≤óÁï•‰∏ÄÁúãÈÉΩÊòØÂ∏∏ËßÑÈóÆÈ¢òÔºå‰πüÂ∞±‰∏ÄÂë®ÈõÜ‰∏≠Â§ÑÁêÜ‰∏ÄÊ¨°‰∫Ü„ÄÇ
‰∏çÂ•ΩÊÑèÊÄùÊúÄËøëÊúâÁÇπÂøôÔºåÂèØËÉΩÂæó‰∏ÉÊúà‰ªΩÊâçËÉΩÊÅ¢Â§çÁßíÂõûÁöÑÊïàÁéá„ÄÇ
"
ÂÖ≥ÈîÆËØçÊèêÂèñÁÆóÊ≥ï‰ºöÂá∫Áé∞ÈáçÂ§çÂÖ≥ÈîÆËØç,"List<String> textrank = HanLP.extractKeyword(article, 5);

ËæìÂá∫ÔºöÂ∏ÇÂú∫ ËÆæÂ§á idc Á©øÊà¥ fitbit fitbit
ËæìÂÖ•ÔºöÊñ∞Êµ™ÁßëÊäÄËÆØ Âåó‰∫¨Êó∂Èó¥6Êúà7Êó•Êó©Èó¥Ê∂àÊÅØÔºåIDCËøëÊúüÂÖ¨Â∏É‰∫Ü‚ÄúÂÖ®ÁêÉÂ∏ÇÂú∫Â≠£Â∫¶ÂèØÁ©øÊà¥ËÆæÂ§áË∑üË∏™Êä•Âëä‚ÄùÔºåÁ¨¨‰∏ÄÂ≠£Â∫¶FitbitÊòØÂÖ®ÁêÉÊéíÂêçÁ¨¨‰∏ÄÁöÑÂèØÁ©øÊà¥ËÆæÂ§áÂéÇÂïÜ„ÄÇÁÑ∂ËÄåÔºåÈöèÁùÄËãπÊûúApple WatchÁöÑÂÖ®Èù¢ÂºÄÂîÆÔºåFitbitÁöÑ‰ºòÂäøÂæàÂèØËÉΩÂ∞Ü‰∏çÂ§çÂ≠òÂú®„ÄÇ

„ÄÄ„ÄÄIDCÂÖ¨Â∏ÉÁöÑÊï∞ÊçÆÊòæÁ§∫Ôºå2015Âπ¥Á¨¨‰∏ÄÂ≠£Â∫¶ÔºåFitbit„ÄÅÂ∞èÁ±≥ÂíåGarminÊòØÂÖ®ÁêÉÂâç‰∏âÂ§ßÂèØÁ©øÊà¥ËÆæÂ§áÂéÇÂïÜ„ÄÇ‰∏çËøáÔºåËãπÊûú‰ªäÂπ¥4ÊúàÂºÄÂßãÈîÄÂîÆApple Watch„ÄÇÂõ†Ê≠§ÂΩìÁ¨¨‰∫åÂ≠£Â∫¶Êï∞ÊçÆÂèëÂ∏ÉÊó∂ÔºåËãπÊûúÂ∞ÜË∑ªË∫´Ëøô‰∏ÄÊéíË°åÁöÑÂâçÂàó„ÄÇ

„ÄÄ„ÄÄIDCÂèØÁ©øÊà¥ËÆæÂ§áÁ†îÁ©∂ÁªèÁêÜÈõ∑Ëíô¬∑ÊãâÈ©¨ÊñØ(Ramon Llamas)Ë°®Á§∫Ôºö‚ÄúApple WatchÂæàÂèØËÉΩÂ∞ÜÊàê‰∏∫ÂÖ∂‰ªñÂèØÁ©øÊà¥ËÆæÂ§áÁöÑÂØπÊØîÂØπË±°„ÄÇËøôÂ∞ÜËø´‰ΩøÁ´û‰∫âÂØπÊâãÈááÂèñÊé™ÊñΩÔºå‰ª•‰øùÊåÅÂú®Â∏ÇÂú∫ÁöÑÈ¢ÜÂÖà‰ºòÂäø„ÄÇ‚Äù

„ÄÄ„ÄÄÊï¥‰ΩìÊù•ÁúãÔºåÁ¨¨‰∏ÄÂ≠£Â∫¶ÂèØÁ©øÊà¥ËÆæÂ§áÂ∏ÇÂú∫ÂêåÊØîÂ¢ûÈïø200%ÔºåÂÖ®ÁêÉÂá∫Ë¥ßÈáè‰∏∫1140‰∏á‰∏™„ÄÇËøôË°®ÊòéÔºåËøô‰∏ÄÂ∏ÇÂú∫ÈùûÂ∏∏Âº∫Âä≤„ÄÇ

„ÄÄ„ÄÄÊãâÈ©¨ÊñØË°®Á§∫Ôºö‚ÄúÁ¨¨‰∏ÄÂ≠£Â∫¶ÔºåÂç≥‚ÄòÂêéÂÅáÊó•Â≠£‚ÄôÊó∂ÊÆµÈîÄÂîÆÊªëÂù°‰∏çÊòéÊòæÔºåËøôË°®ÊòéÂèØÁ©øÊà¥ËÆæÂ§áÂ∏ÇÂú∫ÈùûÂ∏∏Âº∫Âä≤„ÄÇÁªàÁ´ØÁî®Êà∑ÁöÑÂÖ¥Ë∂£‰∏çÊñ≠ÊèêÂçáÔºåËÄåÁõ∏ÂÖ≥ÂéÇÂïÜÂèØ‰ª•Êèê‰æõÂ§öÊ†∑ÂåñÁöÑËÆæÂ§áÂíå‰ΩìÈ™å„ÄÇÊ≠§Â§ñÔºåÊñ∞ÂÖ¥Â∏ÇÂú∫ÁöÑÈúÄÊ±ÇÊ≠£Âú®‰∏äÂçáÔºåËÄåÂéÇÂïÜÊ∏¥ÊúõÊääÊè°Ëøô‰∫õÊñ∞Êú∫‰ºö„ÄÇ‚Äù

„ÄÄ„ÄÄÂØºËá¥Apple WatchÊó†Ê≥ï‰∏ªÂÆ∞Â∏ÇÂú∫ÁöÑ‰∏ÄÂ§ßÈöúÁ¢çÂú®‰∫éÔºåËøôÊ¨æ‰∫ßÂìÅÁöÑÂÖ•Èó®‰ª∑Ê†ºËæÉÈ´ò„ÄÇIDCÊåáÂá∫Ôºå‰ª∑Ê†º‰∏ãÈôçÊòØÊé®Âä®ÂèØÁ©øÊà¥ËÆæÂ§áÈîÄÂîÆÁÅ´ÁàÜÁöÑÂéüÂõ†‰πã‰∏Ä„ÄÇApple WatchÁöÑËµ∑Ê≠•‰ª∑Ê†º‰∏∫349ÁæéÂÖÉÔºåÊòØÂÖ∂‰ªñÂèØÁ©øÊà¥ËÆæÂ§áÂπ≥Âùá‰ª∑Ê†ºÁöÑ3ÂÄç„ÄÇIDC‰º∞ËÆ°ÔºåÊúâ40%ÁöÑÂèØÁ©øÊà¥ËÆæÂ§á‰ª∑Ê†º‰Ωé‰∫é100ÁæéÂÖÉ„ÄÇ

„ÄÄ„ÄÄIDCÂÖ®ÁêÉÁßªÂä®ËÆæÂ§áË∑üË∏™Êä•ÂëäÈ´òÁ∫ßÂàÜÊûêÂ∏àÊù∞Áâπ‰ªÄ¬∑‰πåÂ∏ÉÊãâÂ∞º(Jitesh Ubrani)Ë°®Á§∫Ôºö‚Äú‰∏é‰ªª‰ΩïÊñ∞ÁîüÂ∏ÇÂú∫‰∏ÄÊ†∑Ôºå‰ª∑Ê†º‰∏ãÈôçÈùûÂ∏∏ÁåõÁÉà„ÄÇÂú®Âπ≥Âùá‰ª∑Ê†º‰∏ãÈôçÁöÑÊÉÖÂÜµ‰∏ãÔºåËãπÊûúÊê∫‰∏ÄÊ¨æÈ´ò‰ª∑‰∫ßÂìÅÂÖ•Â∏ÇÂ∞ÜÊ£ÄÈ™åÁî®Êà∑ÊòØÂê¶ÊÑøÊÑè‰∏∫Êüê‰∏ÄÂìÅÁâåÔºåÊàñÊòØÂèóÂ∏ÇÂú∫ÂÖ≥Ê≥®ÁöÑ‰∫ßÂìÅËÄåÊîØ‰ªòÊõ¥È´òÁöÑ‰ª∑Ê†º„ÄÇ‚Äù

„ÄÄ„ÄÄIDCÁöÑÊï∞ÊçÆÊòæÁ§∫ÔºåÁ¨¨‰∏ÄÂ≠£Â∫¶ÔºåFitbitÁöÑËÆæÂ§áÂá∫Ë¥ßÈáè‰∏∫390‰∏á‰∏™ÔºåÂ∏ÇÂú∫‰ªΩÈ¢ù‰∏∫34%ÔºåËøôÊòØÁî±‰∫éCharge„ÄÅCharge HRÂíåSurgeÁ≠âÊñ∞‰∫ßÂìÅÁöÑÈúÄÊ±ÇÂº∫Âä≤„ÄÇÊ≠§Â§ñÔºåÁî®Êà∑‰πüÂú®ÁªßÁª≠ËøΩÊçßFitbitÁöÑFlex„ÄÅOneÂíåZipÁ≠â‰∫ßÂìÅ„ÄÇIDCËÆ§‰∏∫ÔºåÂêåÊó∂‰∏ìÊ≥®‰∫é‰ºëÈó≤ÂíåÈ´òÁ´ØÂ∏ÇÂú∫ÊòØFitbitÂèñÂæóÊàêÂäüÁöÑÈáçË¶ÅÂéüÂõ†„ÄÇ

„ÄÄ„ÄÄFitbitÁöÑÂ∏ÇÂú∫‰ªΩÈ¢ùÊØîÊéíÂêçÁ¨¨‰∫åÁöÑÂ∞èÁ±≥È´ò10%„ÄÇÂ∞èÁ±≥ÊéíÂêçÁ¨¨‰∫å‰∏ªË¶ÅÊòØ‰æùÈù†Â∞èÁ±≥ÊâãÁéØÂú®‰∏≠ÂõΩÂõΩÂÜÖÂ∏ÇÂú∫ÁöÑÈîÄÂîÆ„ÄÇIDCËÆ§‰∏∫ÔºåÂ∞èÁ±≥Â∞ÜÂæàÂø´Ëøõ‰∏ÄÊ≠•ÂºÄÊãìÂõΩÈôÖÂ∏ÇÂú∫Ôºå‰ªéËÄåÊàê‰∏∫FitbitÁöÑÊúâÂäõÁ´û‰∫âÂØπÊâã„ÄÇ

„ÄÄ„ÄÄ‰∏éFitbitÁ±ª‰ººÔºåGarmin‰πüÊèê‰æõ‰∫ÜÂ§öÊ†∑ÂåñÁöÑÂèØÁ©øÊà¥ËÆæÂ§á‰∫ßÂìÅ„ÄÇ‰∏çËøáÔºåGarminÁöÑÂ∏ÇÂú∫‰ªΩÈ¢ù‰ªÖÁï•È´ò‰∫é6%„ÄÇ

„ÄÄ„ÄÄ‰∏âÊòüÊéíÂêçÁ¨¨Âõõ„ÄÇIDCÂàÜÊûêÂ∏àÊåáÂá∫Ôºå‰∏âÊòüÁöÑË°®Áé∞‰∏ç‰Ω≥‰∏ªË¶ÅÊòØÁî±‰∫éÔºåÂÖ∂GearËÆæÂ§áÂè™ËÉΩËøûÊé•Êüê‰∫õÈ´òÁ´ØÁöÑ‰∏âÊòüÊô∫ËÉΩÊâãÊú∫„ÄÇ

„ÄÄ„ÄÄJawbone„ÄÅÁ¥¢Â∞ºÂíåPebbleÊ≠£Âú®‰∫âÂ§∫Â∏ÇÂú∫Á¨¨‰∫îÁöÑ‰ΩçÁΩÆÔºåËÄåÁ¨¨‰∏ÄÂ≠£Â∫¶JawboneÂèñÂæó‰∫ÜÈ¢ÜÂÖà„ÄÇÊãâÈ©¨ÊñØÂíå‰πåÂ∞îÂ∑¥Â∞ºË°®Á§∫ÔºåJawboneÁöÑUP MOVEÂíåUP24Á¨¨‰∏ÄÂ≠£Â∫¶Â∏¶Êù•‰∫ÜÂ∏ÆÂä©„ÄÇËÄåÈöèÁùÄÁ¨¨‰∫åÂ≠£Â∫¶JawboneÂÜçÊé®Âá∫‰∏§Ê¨æÊñ∞‰∫ßÂìÅÔºåËøôÊ†∑ÁöÑ‰ºòÂäøËøòÂ∞ÜÂæóÂà∞Âä†Âº∫„ÄÇ(Áª¥Èáë)
"
Ê≠£ËßÑÂåñÂÖ®ËßíÊ†áÁÇπÁ¨¶Âè∑‰ºöËΩ¨ÊàêÂçäËßíÈÄóÂè∑,"```
    System.out.println(CharTable.convert('ÔºÅ'));
```

ÁªìÊûúÊòØ','ÔºåËÄå‰∏çÊòØ'!'
"
ËÉΩÂê¶Êèê‰æõ‰∏ªËØçÂÖ∏Âä®ÊÄÅÂà†Èô§ÊñπÊ≥ïÔºü,"Áúã‰∫Ü‰∏ãÂéüÁ†ÅÔºåÂèëÁé∞BinTrieÊúâÊèê‰æõremoveÊñπÊ≥ïÔºåÊâÄ‰ª•Ëá™ÂÆö‰πâËØçÂÖ∏ÂèØ‰ª•Âä®ÊÄÅÂà†Èô§„ÄÇ
‰∏çËøáDoubleArrayTrieÂ•ΩÂÉèÂπ∂Ê≤°ÊúâÊèê‰æõÂà†Èô§ÊñπÊ≥ïÔºåËØ∑ÈóÆÊòØ‰∏∫‰ªÄ‰πàÔºüÊòØÂê¶ÊòØÂõ†‰∏∫Êï∞ÊçÆÁªìÊûÑÁöÑÈóÆÈ¢òÔºü

Âª∫ËÆÆËÉΩÊèê‰æõ‰∏ªËØçÂÖ∏Âä®ÊÄÅÂà†Èô§ÊñπÊ≥ïÔºåÂõ†‰∏∫Ëã•ÊòØÊäähanLPÊãøÊù•ÂÅöÁîü‰∫ßÁéØÂ¢ÉsolrÁöÑÂàÜËØçÔºåÊúçÂä°‰∏çËÉΩÂÅúÔºå
Â¶ÇÊûúË¶Å‰øÆÊîπËØçÂ∫ìÁöÑËØùÔºåÂª∫ËÆÆËÉΩÊúâÂä®ÊÄÅÂà†Èô§ÊâÄÊúâËØçÂ∫ìÁöÑÊñπÊ≥ï„ÄÇ

Ë∞¢Ë∞¢ÔºÅ
"
"""ÈΩê‰∫ëÂ±±È´òÁ∫ØÂ±±Ëå∂Ê≤π""ÂàÜËØçbug","ÂÅ∂ÁÑ∂ÈÅáÂà∞Ëøô‰∏™ÂπøÂëäËØçÔºåÁõÆÂâçÁöÑnlpÂàÜËØçÁªìÊûúÊòØ‚ÄúÈΩê‰∫ëÂ±±È´ò/ns, Á∫Ø/a, Â±±Ëå∂Ê≤π/nf‚Äù
Ê≠£Á°ÆÁöÑÂ∫îËØ•ÊòØ‚ÄúÈΩê‰∫ëÂ±± È´òÁ∫Ø Â±±Ëå∂Ê≤π‚Äù
"
"ÂàÜËØçÂô®ÂØπ""‰∏âÂπ¥""ÁöÑÂàÜËØçÁªìÊûú‰∏çÊòØÂæàÁêÜÊÉ≥","ÊµãËØï‰ª£Á†Å:
List<Term> termList = StandardTokenizer.segment(""‰∏âÂπ¥"");
System.out.println(termList);
termList = StandardTokenizer.segment(""3Âπ¥"");
System.out.println(termList);
termList = StandardTokenizer.segment(""‰∏â Âπ¥"");
System.out.println(termList);
termList = StandardTokenizer.segment(""‰∏âÂÖÉ"");
System.out.println(termList);
termList = StandardTokenizer.segment(""3ÂÖÉ"");
System.out.println(termList);
======================ÂàÜËØçÁªìÊûú======================================
[‰∏â/m, Âπ¥/n]
[3Âπ¥/m]
[‰∏â/m,  /w, Âπ¥/q]
[‰∏âÂÖÉ/nz]
[3ÂÖÉ/mq]

È¶ñÂÖàÊü•ÁúãËØçÂ∫ì,ÊâæÂà∞ÂÖ≥‰∫éÂπ¥ÂíåÂÖÉÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØ:
Âπ¥ D 219 B 61 C 42 L 20 K 12 E 8  
Âπ¥ B 105 D 57 A 11 X 1 
Âπ¥ q   2421    n   95  m   1  
Âπ¥ qt 14340    

ÂÖÉ P 47 D 2 A 1  
ÂÖÉ D 957 C 672 E 60 B 22 K 3 L 1  
ÂÖÉ q   1536    tg  5   n   1

ËØçÂ∫ì‰∏≠ÂØπÂπ¥ÁöÑÁªüËÆ°,ÊòéÊòæÈáèËØçÁöÑËØçÈ¢ëÊØîËæÉÈ´ò,‰ΩÜÊòØ‰∏∫‰ªÄ‰πàÂàÜÂá∫Êù•ÊòØÂêçËØçÂë¢.ÊàëÁöÑ‰ª£Á†ÅÂ∑≤ÁªèÊòØÊúÄÊñ∞ÁöÑ‰ª£Á†Å‰∫Ü.
"
ÂàÜËØçbug,"ÂØπ‰∏ÄÁâáÊñáÊú¨ËøõË°åÂàÜËØçÔºåÂèëÁé∞‰ºöÊäõIllegalArgumentExceptionÂºÇÂ∏∏ÔºåÊØèÊ¨°ÂøÖÁé∞„ÄÇ‰∏çÁü•ÈÅìÊòØ‰ªÄ‰πàÂéüÂõ†ÔºåËÉΩËß£Èáä‰∏Ä‰∏ãÂêóÔºüÊòØÊàë‰ΩøÁî®ÁöÑÊñπÊ≥ï‰∏çÂØπÂêóÔºüÊàëË∑ë‰∫ÜÂá†ÂçÉÁØáÊñáÁ´†ÈÉΩÊ≤°ÈóÆÈ¢òÔºåÂè™ÊúâËøôÁØáÊñáÁ´†ÊúâÈóÆÈ¢ò„ÄÇ

ÊµãËØïÊñáÊú¨Ôºö
„ÄÄ„ÄÄÊò®Êó•ÔºåÈôàÁáïÂíåÂØºÁõ≤Áä¨Âà∞ËææÈõçÂíåÂÆ´Á´ôÂáÜÂ§áÊç¢‰πò2Âè∑Á∫ø„ÄÇÊ†πÊçÆÂΩìÂ§©Ëµ∑ÂÆûÊñΩÁöÑÊñ∞ËßÑÔºåÂØºÁõ≤Áä¨Ë¢´ÂÖÅËÆ∏‰πòÂùêÂú∞ÈìÅÂíåÁÅ´ËΩ¶„ÄÇÊñ∞‰∫¨Êä•ËÆ∞ËÄÖ ÁéãË¥µÂΩ¨ ÊëÑ
„ÄÄ„ÄÄ 
„ÄÄ„ÄÄÂú∞ÈìÅÂÜÖ‰∏Ä‰ΩçÂ∞èÂ•≥Â≠©ËßÅÂà∞ÂØºÁõ≤Áä¨Êúâ‰∫õÂÆ≥ÊÄïÔºåÈôàÁáïË°®Á§∫ÂØºÁõ≤Áä¨ÂæàÂÆâÂÖ®Ôºå‚ÄúÂ∞±ÊòØË∏©Âà∞ÂÆÉÁöÑËÑöÔºåÂÆÉ‰πü‰∏ç‰ºöÂí¨‰Ω†„ÄÇ‚ÄùÊñ∞‰∫¨Êä•ËÆ∞ËÄÖ ÁéãË¥µÂΩ¨ ÊëÑ
„ÄÄ„ÄÄ 
„ÄÄ„ÄÄÊò®Êó•ÔºåÂú∞ÈìÅË•øÁõ¥Èó®Á´ôÔºåÊµÅÂä®Â∑°Êü•ÁöÑÂåó‰∫¨Â∏ÇËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÂ§ßÈòüÊâßÊ≥ï‰∫∫ÂëòÔºåËÉ∏Ââç‰Ω©Êà¥ÊâßÊ≥ïËÆ∞ÂΩï‰ª™„ÄÇ
„ÄÄ„ÄÄÂÆû‰π†Áîü ÂΩ≠Â≠êÊ¥ã ÊëÑ
„ÄÄ„ÄÄËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÈòüÊâßÊ≥ïÈ¶ñÊó•Êú™ÂºÄÁΩöÂçï
„ÄÄ„ÄÄ„ÄäÂåó‰∫¨Â∏ÇËΩ®ÈÅì‰∫§ÈÄöËøêËê•ÂÆâÂÖ®Êù°‰æã„ÄãÂÆûÊñΩÔºåÂàùÊúüÂØπ‰πûËÆ®ÂçñËâ∫‰ª•ÂäùÂØº‰∏∫‰∏ªÔºõË¢´Êãí11Ê¨°ÂØºÁõ≤Áä¨Áªà‰πòÂú∞ÈìÅ
„ÄÄ„ÄÄÊñ∞‰∫¨Êä•ËÆØ ÂÆûÊñΩÈ¶ñÊó•ÔºåÂåó‰∫¨È¶ñÊîØËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÈòü‰πü‰∫éÂΩìÂ§©ÂºÄÂßãËøõÂú∞ÈìÅÊâßÊ≥ïÔºåÂΩìÂ§©Âπ∂Êú™ÂºÄÂá∫ÁΩöÂçï„ÄÇ‰∫§ÈÄöÊâßÊ≥ïÈÉ®Èó®Áõ∏ÂÖ≥Ë¥üË¥£‰∫∫Ë°®Á§∫ÔºåÊ≥ïËßÑÂÆûÊñΩÂâçÊúü‰ª•ÂäùÂØº‰∏∫‰∏ª„ÄÇ
„ÄÄ„ÄÄÊúÄÁªàÁõÆÊ†áÔºöÁ´ôÂùá‰∏§ÂêçÊâßÊ≥ï‰∫∫Âëò
„ÄÄ„ÄÄÊ†πÊçÆ„ÄäÊù°‰æã„ÄãËßÑÂÆöÔºåÂú®Âú∞ÈìÅÈáå‰πûËÆ®„ÄÅÂçñËâ∫Â∞ÜË¢´Â§Ñ‰ª•50ÂÖÉ‰ª•‰∏ä1000ÂÖÉ‰ª•‰∏ãÁΩöÊ¨æ„ÄÇÂú®ËΩ¶Á´ô„ÄÅËΩ¶Âé¢ÂÜÖÊ¥æÂèëÂπøÂëäÁ≠âÁâ©ÂìÅÁöÑÔºåÈù¢‰∏¥ÊúÄÈ´ò1‰∏áÂÖÉÁöÑÁΩöÊ¨æ„ÄÇ‰Ωú‰∏∫‰æùÁÖßÊ≥ïËßÑÊâßÊ≥ïÁöÑÂåó‰∫¨Â∏ÇËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÂ§ßÈòüÊò®Â§©È¶ñÊ¨°ËøõÂú∞ÈìÅÂ∑•‰ΩúÔºåÂΩìÂ§©‰∏äÂçàÔºåËÆ∞ËÄÖÂú®Ë•øÁõ¥Èó®Á≠âÂú∞ÈìÅËΩ¶Á´ôÁúãÂà∞ÔºåÂú®Èó∏Êú∫Â§ÑÂíåÊç¢‰πòÈÄöÈÅìÂÜÖÔºåË∫´ÁùÄ‰∫§ÈÄöÊâßÊ≥ïÂà∂ÊúçÁöÑÊâßÊ≥ï‰∫∫Âëò‰∏§‰∫∫‰∏ÄÁªÑÊµÅÂä®Â∑°Êü•„ÄÇ
„ÄÄ„ÄÄÊâßÊ≥ïÈòüÁõ∏ÂÖ≥Ë¥üË¥£‰∫∫ÂëäËØâËÆ∞ËÄÖÔºåÁõÆÂâçÊµÅÂä®Â∑°Êü•ÂØπ‰∫éÂèëÁé∞ËøùËßÑÁöÑ‰∫∫ÂëòÔºåÂèØËÉΩÈ¶ñÂÖàËøõË°åÂäùÂØºËØ¢ÈóÆÔºå‰∏ÄÂºÄÂßã‰∏ç‰ºöËøõË°åÂ§ÑÁΩö„ÄÇËã•Âá∫Áé∞‰∏•ÈáçÊÉÖËäÇÔºåÂ¶Ç‰∏çÂÅúÊ≠¢ËøùÊ≥ïËøùËßÑË°å‰∏∫ÁöÑÔºå‰ºöÈááÂèñÂº∫Âà∂Êé™ÊñΩ‰ª•ÂèäÂ§ÑÁΩöÔºå‚ÄúÊâßÊ≥ï‰∫∫ÂëòÈÉΩË¶ÅËøõË°åÁé∞Âú∫ÂèñËØÅ‚Äù„ÄÇ
„ÄÄ„ÄÄÊçÆÁªüËÆ°ÔºåËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÂ§ßÈòüÊ≠£Âºè‰∏äÂ≤óÁöÑÁ¨¨‰∏ÄÂ§©ÔºåÂ§ßÈòüÂÖ®Âëò‰∏äÂ≤óÔºåÂú®ÂÖºÈ°æÂú∞ÈìÅÂÖ®Á∫øÁΩëÁöÑÊÉÖÂÜµ‰∏ãÔºåÈáçÁÇπÂØπ1Âè∑Á∫ø„ÄÅ2Âè∑Á∫ø„ÄÅ4Âè∑Á∫ø„ÄÅ5Âè∑Á∫ø„ÄÅ10Âè∑Á∫øÁöÑ70‰∏™ÈáçÁÇπËΩ¶Á´ôËøõË°åÁõëÁÆ°ÔºåÂ∑°ËßÜËΩ¶Á´ô97Â∫ßÔºåÂäùÈòªÂú®Âú∞ÈìÅÁ´ôÂÜÖÁé©ËΩÆÊªëË°å‰∏∫3Ëµ∑ÔºõÂäùÈòªÊëÜÊëäÂ∞èÂïÜË¥©4Ëµ∑ÔºõÂäùÈòªÊï£ÂèëÂ∞èÂπøÂëä1Ëµ∑ÔºõÂäùÈòªÁøªË∂äÈó∏Êú∫2Ëµ∑„ÄÇ
„ÄÄ„ÄÄ‰∫§ÈÄöÊâßÊ≥ïÊÄªÈòüÂâØÊÄªÈòüÈïøÊ¢ÅÂª∫‰ºü‰ªãÁªçÔºåËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ïÂ§ßÈòüÁé∞Ê≠£ÂºèÁºñÂà∂ÂÖ±Êúâ88‰∫∫ÔºåÊåâÁÖßÂú∞ÈìÅËøêËê•ÂÖ¨Âè∏ÁÆ°ÁêÜËåÉÂõ¥ÂàÜ‰∏∫5‰∏™‰∏≠ÈòüÔºåÂíå‰∏Ä‰∏™‰∏ìÈó®Ë¥üË¥£Â∫îÂØπÈáçÁÇπËΩ¶Á´ôÈáçÁÇπÊó∂ÊÆµÊàñÂ§ßÂÆ¢ÊµÅÂÜ≤ÂáªÊó∂ÂÆâÂÖ®‰øùÈöúÁöÑÊú∫Âä®‰∏≠Èòü„ÄÇ
„ÄÄ„ÄÄ‚ÄúÊàë‰ª¨ÁöÑÊúÄÁªàÁõÆÊ†áÊòØ318‰∏™Á´ôÔºåÂπ≥ÂùáÊØèÁ´ô‰∏§ÂêçÊâßÊ≥ï‰∫∫Âëò‚ÄùÔºåÊ¢ÅÂª∫‰ºüËØ¥ÔºåÊâßÊ≥ïÂ§ßÈòüÂêéÁª≠Ëøò‰ºöÊãõËÅò‰∫∫ÂëòËæÖÂä©ÊâßÊ≥ïÔºåÂêåÊó∂ÂÄüÂä©Âú∞ÈìÅËøêËê•Êñπ‰∏∫ÊâßÊ≥ïÊèê‰æõ‰øùÈöú„ÄÇ
„ÄÄ„ÄÄÊªëÊùøËΩ¶Á≠âÊàñÂ∞ÜÁ¶ÅÂÖ•Âú∞ÈìÅ
„ÄÄ„ÄÄÊ¢ÅÂª∫‰ºü‰ªãÁªçÔºåÊâßÊ≥ïÈòüÂëòËÅåË¥£ÂåÖÊã¨ÂØπÂç±ÂÆ≥ËΩ®ÈÅì‰∫§ÈÄöËÆæÂ§áËÆæÊñΩÂÆâÂÖ®ÁöÑË°å‰∏∫„ÄÅÂç±ÂÆ≥ËΩ®ÈÅì‰∫§ÈÄöËøêËê•ÂÆâÂÖ®ÁöÑË°å‰∏∫ÂèäËΩ®ÈÅì‰∫§ÈÄöÁõ∏ÂÖ≥Âçï‰Ωç‰∏çËêΩÂÆûÂÆâÂÖ®Áîü‰∫ß‰∏ª‰ΩìË¥£‰ªªÁöÑË°å‰∏∫ÂÆûÊñΩË°åÊîøÂ§ÑÁΩöÔºåÂÖ∂‰∏≠Êúâ41È°πË°åÊîøÂ§ÑÁΩöÔºå5È°πË°åÊîøÂº∫Âà∂„ÄÇ
„ÄÄ„ÄÄ‚ÄúÊ≥ïËßÑÂÆûÊñΩÂâçÊúüÔºåÊàë‰ª¨ËøòÊòØ‰ª•ÂäùÈòª‰∏∫‰∏ª„ÄÇ‚ÄùÊ¢ÅÂª∫‰ºüËØ¥ÔºåÊù°‰æãÂØπÂú®ËΩ¶Á´ô„ÄÅËΩ¶Âé¢ÂÜÖ‰πûËÆ®ÂçñËâ∫„ÄÅÊ¥æÂèëÂπøÂëä‰ª•ÂèäÂÖ∂‰ªñÂç±ÂÆ≥Âú∞ÈìÅÂÆâÂÖ®ÁöÑË°å‰∏∫ÈÉΩÊúâ‰∫ÜËßÑÂÆöÔºå‰ΩÜÂÆûÊñΩ‰∏äÊúâÂÖ∑‰ΩìÈóÆÈ¢òÔºåËøòÈúÄË¶ÅÂà∂ÂÆöÁªÜÂàôÊù•Ëøõ‰∏ÄÊ≠•ÊòéÁ°ÆÔºåÁõÆÂâçÁªÜÂàôÊ≠£Âú®Á†îÁ©∂‰∏≠ÔºåÂ∞Ü‰∫é1‰∏™ÊúàÂà∞2‰∏™ÊúàÂÜÖÂá∫Âè∞„ÄÇ
„ÄÄ„ÄÄÂåó‰∫¨Âú∞ÈìÅËøêËê•ÂÖ¨Âè∏Áõ∏ÂÖ≥Ë¥üË¥£‰∫∫Ë°®Á§∫Ôºå„ÄäÊù°‰æã„Äã‰∏≠ÊèêÂà∞ÔºåÂú®ËΩ¶Á´ôÂÜÖÁ¶ÅÊ≠¢‰ªé‰∫ãÊªëÊùø„ÄÅËΩÆÊªë„ÄÅËá™Ë°åËΩ¶Á≠âËøêÂä®„ÄÇ‰∏ã‰∏ÄÊ≠•ÔºåÊªëÊùø„ÄÅËΩÆÊªëÁ≠âËøêÂä®Âô®ÂÖ∑ÂèØËÉΩÁ¶ÅÊ≠¢Â∏¶ÂÖ•Âú∞ÈìÅ„ÄÇÁõÆÂâçÔºåÊúâÂÖ≥Á¶ÅÂ∏¶Áâ©ÂìÅÁöÑÁõÆÂΩïÁõ∏ÂÖ≥ÈÉ®Èó®ËøòÂú®Ëøõ‰∏ÄÊ≠•ÂÆåÂñÑÔºå‰πãÂâçÂÖÅËÆ∏Â∏¶ÁöÑÂ§ßÂûãÂ∑•ÂÖ∑Á≠âÁ≠âÔºåÊú™Êù•ÊàñÂàóÂÖ•ËøùÁ¶ÅÂêçÂçï‰∏≠„ÄÇ
„ÄÄ„ÄÄ‚ñ† Êé¢ËÆø
„ÄÄ„ÄÄ‰πòÂÆ¢ÔºöÊâìÂáª‰πûËÆ®‰∏çËÉΩ‰ªÖÈù†ÁΩöÊ¨æ
„ÄÄ„ÄÄÊò®Êó•‰∏≠Âçà1Êó∂ËÆ∏ÔºåËÆ∞ËÄÖÂú®Âú∞ÈìÅ2Âè∑Á∫øÂÆ£Ê≠¶Èó®Á´ôÁúãÂà∞‰∏Ä‰ΩçÂ¶áÂ•≥ÊãøÁùÄ‰∏Ä‰∏™Èü≥ÁÆ±Ôºå‰∏ÄËæπÊîæÈü≥‰πê‰∏ÄËæπÂêë‰πòÂÆ¢‰πûËÆ®Ôºå‰∏ÄËäÇËΩ¶Âé¢ÂÜÖÔºåÂè™Êúâ‰∏Ä‰ΩçÂπ¥ËΩªÁöÑÂ•≥‰πòÂÆ¢Áªô‰∫ÜÂ•π1ÂÖÉÈí±„ÄÇÊ†πÊçÆ„ÄäÂåó‰∫¨Â∏ÇËΩ®ÈÅì‰∫§ÈÄöËøêËê•ÂÆâÂÖ®Êù°‰æã„ÄãÔºåÂØπÂú∞ÈìÅ‰πûËÆ®ÂçñËâ∫Á≠âÂç±ÂÆ≥Âú∞ÈìÅËøêËê•ÂÆâÂÖ®ÁöÑË°å‰∏∫ÔºåËΩ®ÈÅì‰∫§ÈÄöÊâßÊ≥ï‰∫∫ÂëòÂèØÂÆûÊñΩË°åÊîøÂ§ÑÁΩö„ÄÇ
„ÄÄ„ÄÄËÆ∞ËÄÖÈöèÂêéÊã®Êâì‰∫ÜÂú∞ÈìÅËΩ¶Âé¢ÂÜÖË¥¥ÁùÄÁöÑÂåó‰∫¨Âú∞ÈìÅÁõëÁù£ÁîµËØù96165„ÄÇÂæóÁü•ÊòØ‰∏æÊä•‰πûËÆ®ÂçñËâ∫Ë°å‰∏∫ÂêéÔºåÊé•Á∫øÂëòËØ¢ÈóÆ‰∫ÜÁ∫øË∑Ø„ÄÅËΩ¶Á´ô„ÄÅÂàóËΩ¶ËøêË°åÊñπÂêë„ÄÅÊâÄÂú®ËΩ¶Âé¢‰ª•Âèä‰πûËÆ®‰∫∫ÂëòË°åËøõÁöÑÊñπÂêëÁ≠â‰ø°ÊÅØÔºåÂπ∂Ë°®Á§∫‰ºöÈ©¨‰∏äËÅîÁ≥ªËΩ¶Á´ôÂ∑•‰Ωú‰∫∫Âëò„ÄÇ
„ÄÄ„ÄÄ‚Äú‰πûËÆ®ÂçñËâ∫Ë°å‰∏∫ËÆ©‰∫∫ÊÑüÂà∞ÂéåÁÉ¶‚ÄùÔºåÂêåËΩ¶‰πòÂÆ¢Ë∞¢Â•≥Â£´Ë°®Á§∫ÔºåËá™Â∑±ÊõæÊúâËøáÂ§öÊ¨°Âú®Âú∞ÈìÅ‰∏äÁù°ÁùÄÂêéË¢´‰πûËÆ®ËÄÖÂêµÈÜíÁöÑÁªèÂéÜÔºå‰ΩÜÂ•πÂêåÊó∂ËÆ§‰∏∫ÂØπËøô‰∫õ‰∫∫ÂëòËøòÊòØÂ∫î‰ª•ÂäùÂØº‰∏∫‰∏ª„ÄÇÂêåËΩ¶Â§ßÂ§öÊï∞‰πòÂÆ¢‰πüËÆ§‰∏∫ÔºåÂú∞ÈìÅ‰∏≠ÁöÑ‰πûËÆ®Ë°å‰∏∫‰∏ÄËà¨ÈÉΩÊúâÁªÑÁªáÔºå‰∏çÂ§ÑÁΩö‰πûËÆ®ÂçñËâ∫ÁªÑÁªáËÄÖÔºå‰ΩÜÈù†ÁΩöÊ¨æÈöæ‰ª•ÊùúÁªùËøôÊ†∑ÁöÑË°å‰∏∫„ÄÇ
„ÄÄ„ÄÄÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∞‰πòÂÆ¢ÂØπ‰∫éÊï£ÂèëÂ∞èÂπøÂëä‚ÄúÊ∑±ÊÅ∂ÁóõÁªù‚ÄùÔºåÁß∞ËØ•Ë°å‰∏∫‰ΩøÂú∞ÈìÅËΩ¶Âé¢ÂÜÖÁéØÂ¢ÉËÑè‰π±„ÄÇ‰∫¨Ê∏ØÂú∞ÈìÅÂÆâÂÖ®ÈÉ®Èó®Ë¥üË¥£‰∫∫‰ªãÁªçÔºå‰ªñ‰ª¨Â∑•‰Ωú‰∫∫ÂëòÂØπÂú∞ÈìÅÂÜÖÊï£ÂèëÂ∞èÂπøÂëäÁöÑË°å‰∏∫Âè™ËÉΩÂäùÈòªÔºåËÄå‰∏çËÉΩÊ≤°Êî∂‰ªñ‰ª¨ÁöÑÂ∞èÂπøÂëä„ÄÇ5Êúà1Êó•‰ª•ÂêéÔºåÊúâ‰∫ÜÊù°‰æãÁöÑÊîØÊåÅÔºå‰ºöÂú®ËøôÊñπÈù¢Âä†Âº∫ÁÆ°ÁêÜ„ÄÇ
„ÄÄ„ÄÄÂØºÁõ≤Áä¨ÁèçÂ¶ÆÁªà‰∫é‚ÄúËé∑ÂáÜ‚Äù‰πòÂú∞ÈìÅ
„ÄÄ„ÄÄÊ≠§ÂâçË¢´Êãí11Ê¨°Ôºõ„ÄäÊù°‰æã„ÄãÊòéÁ°ÆËßÜÂäõÊÆãÈöúËÄÖÂèØÊê∫Â∏¶ÂØºÁõ≤Áä¨ËøõÁ´ô‰πòËΩ¶
„ÄÄ„ÄÄÂú®ÁªèÂéÜ‰∫Ü11Ê¨°Ë¢´ÊãíÁªù‰ª•ÂêéÔºåÂØºÁõ≤Áä¨ÁèçÂ¶ÆÊò®Â§©Áªà‰∫éÊÑüÂèó‰∫Ü‰∏ÄÊääÂåó‰∫¨Âú∞ÈìÅ„ÄÇÊ≠§Ê¨°„ÄäÊù°‰æã„ÄãÊòéÁ°ÆÔºåËßÜÂäõÊÆãÈöúËÄÖÂèØÊê∫Â∏¶ÂØºÁõ≤Áä¨ËøõÁ´ô‰πòËΩ¶„ÄÇ
„ÄÄ„ÄÄÊò®Â§©‰∏äÂçàÔºåÂú∞ÈìÅ5Âè∑Á∫øÂ§©ÈÄöËãëÁ´ôÔºåËßÜÈöú‰∫∫Â£´ÈôàÁáïÁâµÁùÄÂØºÁõ≤Áä¨ÁèçÂ¶ÆÔºåÊù•Âà∞Âú∞ÈìÅÂ§©ÈÄöËãëÁ´ôÔºåËΩ¶Á´ôÂ∑•‰Ωú‰∫∫ÂëòÊ£ÄÊü•‰∫ÜËßÜÂäõÊÆãÈöúËØÅ‰ª∂„ÄÅÂØºÁõ≤Áä¨ËØÅ‰ª∂ÂêéÔºåÂêëÈôàÁáïÂèëÊîæ‰∫ÜÁ¶èÂà©Á•®„ÄÇ
„ÄÄ„ÄÄÂú®Â∑•‰Ωú‰∫∫ÂëòÂºïÂØº‰∏ãÔºåÈôàÁáïÂíåÂØºÁõ≤Áä¨È°∫Âà©ËøõÁ´ô„ÄÇÁ´ôÂú®Â∏¶Êó†ÈöúÁ¢çÊ†áÂøóÁöÑÂÄôËΩ¶Èó®ÂâçÔºåÈôàÁáïÂèçÂ§çÊåá‰∏Ä‰∏ãÊ†áÂøóÔºåÂèàÊãç‰∏Ä‰∏ãÁèçÂ¶ÆÁöÑÂ§¥ÔºåÂëäËØâÂÆÉË¶ÅËÆ∞‰ΩèËøô‰∏™‰ΩçÁΩÆ„ÄÇ
„ÄÄ„ÄÄÂòÄÂòÄÂòÄÔºåÂú∞ÈìÅÈó®ÊâìÂºÄÔºåÁèçÂ¶ÆÂ∞èÂøÉÁøºÁøºÂú∞Ëµ∞ËøõËΩ¶Âé¢ÔºåÂÆÉÂºïÂØºÈôàÁáïÊâæÂà∞ËΩ¶Âé¢Êé•Âè£Â§ÑÁöÑÁ©∫‰ΩçÁΩÆÂêéÂÅú‰ΩèÔºåÁ≠âÈôàÁáïÁ´ôÂÆöÂêéÔºåÁèçÂ¶ÆÈùôÈùôÁöÑË∂¥Âú®ËÑöËæπ„ÄÇËΩ¶Âé¢ÈáåÂÖ∂‰ªñ‰πòÂÆ¢ÂØπÂØºÁõ≤Áä¨‰πòËΩ¶ÈùûÂ∏∏ÊÑüÂÖ¥Ë∂£Ôºå‰ΩÜÂπ∂Ê≤°Êúâ‰∫∫ÂºïÈÄóÔºåÁèçÂ¶Æ‰πü‰∏ùÊØ´Ê≤°ÊúâË∫ÅÂä®Ë°å‰∏∫„ÄÇ5Âè∑Á∫øÊç¢‰πò2Âè∑Á∫ø‰πüÊúâÂ∑•‰Ωú‰∫∫ÂëòÂºïÂØºÔºåËøôÊ¨°ÔºåÈôàÁáïÊâæÂà∞‰∏Ä‰∏™Â∫ß‰ΩçÂùê‰∏ãÔºåÁèçÂ¶Æ‰æùÊóßÂçßÂú®Â•πËÑöËæπ„ÄÇ
„ÄÄ„ÄÄÈôàÁáïËØ¥Ôºå‰πòÂùêÂÖ¨ÂÖ±‰∫§ÈÄöÂ∑•ÂÖ∑ÁöÑËÉΩÂäõÔºåÊòØÂØºÁõ≤Áä¨Âú®ËÆ≠ÁªÉÊó∂ÁöÑÂøÖ‰øÆËØæ„ÄÇÁèçÂ¶ÆÊò®Â§©ÁöÑË°®Áé∞ÂæàÂ•ΩÔºåÂπ∂Ê≤°ÊúâÂØπ‰∫∫Áæ§ÊúâÊÅêÊÉßÊÑü„ÄÇÂ•πÂ∏åÊúõÔºåÈÄöËøáËøô‰∏ÄÊ¨°ÁöÑÂÆûË∑µÔºåËÆ©Êõ¥Â§ö‰∫∫‰∫ÜËß£ËßÜÂäõÊÆãÈöú‰∫∫Â£´ÂØπÂÖ¨ÂÖ±Âá∫Ë°åÁöÑÈúÄÊ±ÇÔºå‰∫ÜËß£ÂØºÁõ≤Áä¨„ÄÇÁõÆÂâçÂÖ®ÂõΩ‚ÄúÊåÅËØÅ‚ÄùÁöÑÂØºÁõ≤Áä¨‰πü‰∏çËøá‰∏ÉÂÖ´ÂçÅÂè™ÔºåÂåó‰∫¨Â∏ÇÁõÆÂâçÂÖ±Êúâ9Âè™ÔºåÂ∏ÇÂå∫Êúâ7Âè™ÔºåÈÉäÂå∫Êúâ2Âè™„ÄÇ

‰ª£Á†ÅÂ¶Ç‰∏ãÔºö
        StringBuffer article = new StringBuffer();
        String line;
        try(BufferedReader out = new BufferedReader(new FileReader(""C:/Users/huan.wang/Desktop/corpus.txt""))) {
            while((line = out.readLine()) != null) {
                article.append(line);
            }
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        List<Term> words = HanLP.segment(article.toString());
        for(Term w : words) {
            System.out.println(w);
        }

ÂºÇÂ∏∏Ê†à‰ø°ÊÅØÔºö
Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1
    at java.util.ArrayList.<init>(Unknown Source)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.convert(WordBasedGenerativeModelSegment.java:241)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:114)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:384)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:371)
"
ÂõûÂ§ç‚ÄúCRFÂàÜËØçÁöÑÁ∫ØJavaÂÆûÁé∞‚Äù,"URL:http://www.hankcs.com/nlp/segment/crf-segmentation-of-the-pure-java-implementation.html

ÂèØËÉΩÊòØÊàëÁé∞Âú®ÂØπHanLPÁöÑ‰∫ÜËß£ËøòÂ§™ÁâáÈù¢„ÄÇËØ∑ËÆ©ÊàëÂÖàËØ¥Êòé‰∏ã‰∏∫‰ªÄ‰πà‰ºöËßâÂæóÊúâ‰∫õ‰∏çÂ¶•ÂΩì„ÄÇ
Âú®ÁúãËøôÁØáÊñáÁ´†ÂâçÔºåÂÖàÁúã‰∫Ü„ÄäHanLPÂºÄÊ∫ê„ÄãÔºå‰∏≠Èó¥ÊúâÊèêÂà∞
‚ÄúHanLPÂá†‰πéÊâÄÊúâÁöÑÂäüËÉΩÈÉΩÂèØ‰ª•ÈÄöËøáÂ∑•ÂÖ∑Á±ªHanLPÂø´Êç∑Ë∞ÉÁî®ÔºåÂΩì‰Ω†ÊÉ≥‰∏çËµ∑Êù•Ë∞ÉÁî®ÊñπÊ≥ïÊó∂ÔºåÂè™ÈúÄÈîÆÂÖ•HanLP.ÔºåIDEÂ∫îÂΩì‰ºöÁªôÂá∫ÊèêÁ§∫ÔºåÂπ∂Â±ïÁ§∫HanLPÂÆåÂñÑÁöÑÊñáÊ°£„ÄÇ
Êé®ËçêÁî®Êà∑ÂßãÁªàÈÄöËøáÂ∑•ÂÖ∑Á±ªHanLPË∞ÉÁî®ÔºåËøô‰πàÂÅöÁöÑÂ•ΩÂ§ÑÊòØÔºåÂ∞ÜÊù•HanLPÂçáÁ∫ßÂêéÔºåÁî®Êà∑Êó†ÈúÄ‰øÆÊîπË∞ÉÁî®‰ª£Á†Å„ÄÇ‚Äù
‰∫éÊòØÔºåÂú®Â∞ùËØïÊµãËØïÊú¨ÊñáÁöÑ‚Äú‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó‚ÄùÊó∂Ôºå‰πüËØïÁùÄÁî®HanLPË∞ÉÁî®Ôºå‰ΩÜÁªìÊûú‰∏é‰Ω†Â±ïÁ§∫ÁªìÊûú‰∏çÂêåÔºåÊü•ÁúãÊ∫êÁ†ÅÂèëÁé∞HanLPÈªòËÆ§‰ΩøÁî®ÁöÑÊòØViterbiSegmentÂØπË±°Ôºå‰ΩÜÂèàÊ≤°ÊúâÂèØ‰ª•ËÆæÁΩÆÊàëÈúÄË¶ÅÁöÑsegmentÁöÑÂú∞ÊñπÔºåÂõ†Ê≠§ËßâÂæóÁé∞Êúâ‰ª£Á†ÅÂπ∂Ê≤°ÊúâÂÉè‰Ω†ËØ¥ÁöÑÈÇ£Ê†∑ÔºåËÉΩÂ§üÂá†‰πéÊâÄÊúâÁöÑÂäüËÉΩÈÉΩËÉΩË¢´HanLPË∞ÉÁî®„ÄÇ

ÁúãËøá‰Ω†ÁöÑÂõûÂ§çÔºåÁü•ÈÅì‰∫ÜnewSegment()ÁöÑ‰ΩúÁî®Ôºå‰πüÂàùÊ≠•‰∫ÜËß£‰Ω†Ë¶ÅËøô‰πàÂÅöÁöÑÂéüÂõ†:ÈÅøÂÖç‰∫ßÁîüÁ∫øÁ®ã‰∏çÂÆâÂÖ®ÁöÑÈóÆÈ¢òÔºåÂõ†Ê≠§‰Ω†‰ΩøÁî®‰∫Ü‰∏Ä‰∏™ÈùôÊÄÅÊñπÊ≥ïËøîÂõû‰∏Ä‰∏™Êñ∞ÂØπË±°ÁöÑÊñπÂºèÂÆûÁé∞„ÄÇÂè¶Â§ñ‰Ω†ÊèêÂà∞ÁöÑ‚ÄúÈÇ£‰πàÂ∞±ÂæóËÄÉËôëÁî®Êà∑ÁöÑ‰ª£Á†ÅÁöÑÂâØ‰ΩúÁî®Ôºå‰ªñ‰ºö‰∏ç‰ºöÊäätextÁöÑcharArrayÊîπÂä®‰∫Ü‚ÄùÊàë‰∏çÊòØÂ§™ÊòéÁôΩ„ÄÇ

‰∏∫‰∫ÜËææÂà∞Ëá≥Â∞ëÊàëËÆ§‰∏∫ÁöÑ‚ÄúHanLPÂá†‰πéÊâÄÊúâÁöÑÂäüËÉΩÈÉΩÂèØ‰ª•ÈÄöËøáÂ∑•ÂÖ∑Á±ªHanLPÂø´Êç∑Ë∞ÉÁî®‚ÄùÔºåÊàëÂ§çÂà∂‰∫Ü‰Ω†Êèê‰æõÁöÑHanLPÊ∫êÁ†ÅÔºåÂπ∂ÂÅö‰∫Ü‰∏Ä‰∫õÊîπÂä®Ôºö
    Ê∑ªÂä†‰∫ÜÂ¶Ç‰∏ã‰ª£Á†ÅÔºö  
    public static final byte CRF_SEGMENT = 0;
    public static final byte HMM_SEGMENT = 1;
    public static final byte AHO_CORASICK_DOUBLE_ARRAY_TRIE_SEGMENT = 2;
    public static final byte DOUBLE_ARRAY_TRIE_SEGMENT = 3;
    public static final byte DIJKSTRA_SEGMENT = 4;
    public static final byte N_SHORT_SEGMENT = 5;
    public static final byte VITERBI_SEGMENT = 6;
    public static ThreadLocal<Segment> segmentLocal = new ThreadLocal<Segment>(){
        protected Segment initialValue() {
            return new ViterbiSegment();
        }
    };
    public static Segment setSegment(Segment segment){
        segmentLocal.set(segment);
        return segment;
    }
    public static Segment setSegment(byte segment){
        Segment segmentObj;
        switch(segment){
            case 0:
                segmentObj = new CRFSegment();
                break;
            case 1:
                segmentObj = new HMMSegment();
                break;
            case 2:
                segmentObj = new AhoCorasickDoubleArrayTrieSegment();
                break;
            case 3:
                segmentObj = new DoubleArrayTrieSegment();
                break;
            case 4:
                segmentObj = new DijkstraSegment();
                break;
            case 5:
                segmentObj = new NShortSegment();
                break;
            case 6:
                segmentObj = new ViterbiSegment();
                break;
            default:
                segmentObj = new ViterbiSegment();
        }
        return setSegment(segmentObj);
    }

ÊîπÊñπÊ≥ïnewSegment()‰∏∫Ôºö
    public static Segment newSegment() {
        return segmentLocal.get();// ‰ª•ViterbiÂàÜËØçÂô®ÂÅö‰∏∫ÈªòËÆ§ÂàÜËØçÂ∑•ÂÖ∑ÔºåÂêåÊó∂ÊîØÊåÅÁî®Êà∑ÈÄâÊã©ÂÖ∂‰ªñÂàÜËØçÂô®
    }

Èô§Ê≠§‰πãÂ§ñÔºåÂÖ∂‰ªñÁ±ªÈÉΩ‰∏çÈúÄË¶ÅÊîπÂä®„ÄÇÊàëÊÉ≥ÔºåThreadLocalÁöÑËøêË°åÊú∫Âà∂ËÉΩ‰øùËØÅÂèòÈáèÂú®Âçï‰∏™Á∫øÁ®ãÂÜÖÈÉ®ÂÖ±‰∫´‰ΩÜÁ∫øÁ®ãÈó¥‰∏çÂÖ±‰∫´ÔºåÈô§‰∫ÜÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®Â§ñÔºå‰∏ç‰ºö‰∫ßÁîüÁ∫øÁ®ã‰∏çÂÆâÂÖ®ÁöÑÈóÆÈ¢òÔºå‰∫¶‰∏ç‰ºöÊúâËµÑÊ∫ê‰∫âÁî®ÁöÑÈóÆÈ¢òÔºåËøòËÉΩÂêëÁî®Êà∑Êèê‰æõËá™Â∑±ÂÆö‰πâÈúÄË¶ÅÁöÑsegmentÁöÑÂäüËÉΩ„ÄÇ

Â¶Ç‰∏ãÊòØ‰∏§ÁßçÂÆûÁé∞‚Äú‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó‚ÄùÁöÑÊñπÂºèÔºå(ÂΩìÁÑ∂ÔºåÁúãËµ∑Êù•Â∑Æ‰∏çÂ§ö)Ôºö
        CRFSegment segment = new CRFSegment();
        segment.enablePartOfSpeechTagging(true);
        System.out.println(""previous version:\t""+segment.seg(""‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó""));

```
    NewHanLP.Config.enableDebug(true);
    NewHanLP.setSegment(NewHanLP.CRF_SEGMENT).enablePartOfSpeechTagging(true);
    System.out.println(""new version:\t\t""+NewHanLP.segment(""‰Ω†ÁúãËøáÁ©ÜËµ´ÂÖ∞ÈÅìÂêó""));
```

ËæìÂá∫Ôºö
previous version:       [‰Ω†/rr, ÁúãËøá/v, Á©ÜËµ´ÂÖ∞ÈÅì/null, Âêó/y]
new version:        [‰Ω†/rr, ÁúãËøá/v, Á©ÜËµ´ÂÖ∞ÈÅì/null, Âêó/y]

Ê¨¢ËøéÊãçÁ†ñ :)
"
ViterbiSegmentÂØπÂ∫îÁü≠ËØçËØ≠Ëæ®ËØÜÈóÆÈ¢ò,"ËØ∑ÈóÆViterbiSegmentÂØπÂ∫îÁü≠ËØçËØ≠Ëæ®ËØÜ‰∏ç‰Ω≥,ÊòØ‰∏çÊòØË∑üËÆ≠ÁªÉËØ≠ÊñôÊòØÈïøÊñáÁ´†ÊúâÂÖ≥Ôºü

‰æãÂ¶Ç ÂèØ‰πê„ÄÅÂπ¥Ë¥ß  Ëøô‰∫õËØç ‰ºöÂàÜÊàê [ÂèØ ,‰πê]  ,[Âπ¥ , Ë¥ß]
Âú®CoreNatureDictionary.txt Â∑≤ÁªèÊúâÂá∫Áé∞   ÂèØ‰πê„ÄÅÂπ¥Ë¥ß 
‰ΩÜCoreNatureDictionary.ngram.txt ÈáåÊ≤°Êúâ 
ÂèØ‰πê@Êú´##Êú´
Âπ¥Ë¥ß@Êú´##Êú´

Ëøô‰∫õÊúâÊØîËæÉ‰ºòÈõÖÁöÑÊñπÂºèËß£ÂÜ≥ÂêóÔºü ËøòÊòØÂøÖÈ°ªÊ∑ªÂä†ËØ≠ÊñôËá≥CoreNatureDictionary.ngram.txt ?
"
CRFÂàÜËØçÊ®°ÂûãÁöÑËÆ≠ÁªÉ Âíå CRFÂàÜËØçËßÜÊ†áÁÇπ‰∏∫Êñ∞ËØçÈóÆÈ¢ò,"CRFÂàÜËØçÊ®°Âûã‰ΩøÁî®ÁöÑÊòØ‰ªÄ‰πàËØ≠ÊñôÔºü‰ª•Âèä-f, -cÂèÇÊï∞ÁöÑÈÄâÂèñ„ÄÇÂè™ËÉΩÈÄöËøá‰∏çÊñ≠ÁöÑÊµãËØïÔºåÈÄâÂèñÁªèËøáÊµãËØïÈÄâÂèñÊïàÊûúÂ•ΩÁöÑÔºü

Âè¶ÔºåÁªè‰ΩøÁî®1.2.2ÁâàÁöÑHanLPÂíådataÔºåÊµãËØïCRFÂàÜËØçÔºåÊµãËØïÊï∞ÊçÆÂ¶Ç‰∏ãÔºö

```
""„ÄäÂ§úÊôöÁöÑÈ™∞Â≠ê„ÄãÈÄöËøáÊèèËø∞ÊµÖËçâÁöÑËàûÂ•≥Âú®ÊöóÂ§ú‰∏≠ÊâîÈ™∞Â≠êÁöÑÊÉÖÊôØ,ÂØÑÊâò‰∫Ü‰ΩúËÄÖÂØπÂ∫∂Ê∞ëÁîüÊ¥ªÂå∫ÁöÑÊÉÖÊÑü"",
""Ëøô‰∏™ÂÉèÊòØÁúüÁöÑ[ÂßîÂ±à]ÂâçÈù¢ÈÇ£‰∏™ÊâìÊâÆÂ§™Ê±üÊà∑‰∫ÜÔºå‰∏ÄÁÇπ‰∏ç‰∏äÂìÅ...@hankcs"", 
```

ÂÖ≥Èó≠ËØçÊÄßÊ†áÊ≥®ÁöÑÂàÜËØçÊïàÊûúÂ¶Ç‰∏ãÔºö

```
[„Ää, Â§úÊôö, ÁöÑ, È™∞Â≠ê, „Äã, ÈÄöËøá, ÊèèËø∞, ÊµÖËçâ, ÁöÑ, ËàûÂ•≥, Âú®, ÊöóÂ§ú, ‰∏≠, Êâî, È™∞Â≠ê, ÁöÑ, ÊÉÖÊôØ, ,, ÂØÑÊâò, ‰∫Ü, ‰ΩúËÄÖ, ÂØπ, Â∫∂Ê∞ë, ÁîüÊ¥ªÂå∫, ÁöÑ, ÊÉÖÊÑü]
[Ëøô‰∏™, ÂÉè, ÊòØ, ÁúüÁöÑ, [, ÂßîÂ±à, ], ÂâçÈù¢, ÈÇ£‰∏™, ÊâìÊâÆ, Â§™, Ê±üÊà∑, ‰∫Ü, Ôºå, ‰∏ÄÁÇπ, ‰∏ç, ‰∏äÂìÅ, ., ., ., @, hankcs]
```

ÂÜçÊü•ÈòÖËØçÂÖ∏CoreNatureDictionary.txtÔºå‰∏çÂ≠òÂú®ÂÖ∂‰∏≠ÁöÑÊ†áÊ≥®‰∏∫nullËØçÊÄßÁöÑËØçËßÜ‰∏∫Êñ∞ËØçÔºå‰ª•‰∏ä‰∏§‰æãÂè•ËØÜÂà´Âá∫ÁöÑÊñ∞ËØçÂàÜÂà´‰∏∫Ôºö

```
ËØÜÂà´Âà∞Êñ∞ËØçÔºö,(wc: 1)
ËØÜÂà´Âà∞Êñ∞ËØçÔºö@(wc: 1), hankcs(wc: 1), [(wc: 1), ](wc: 1), .(wc: 3)
```

Êï¥‰ΩìÊïàÊûú‰∏çÈîôÔºåËØ∏Â¶Ç""hankcs""ËØçËØÜÂà´Âá∫‰∫ÜÔºå‰ΩÜÂÉè"","", ""@"", ""["", ""]"", "".""Á≠âÊ†áÁÇπÁ¨¶Âè∑‰πüË¢´ËßÜ‰∏∫Êñ∞ËØç„ÄÇËÉΩÂ§üÂú®CRFÂàÜËØç‰πãÂêéÂä†ÂÖ•ÂÅúÁî®ËØç‰πãÁ±ªÁöÑÂÅöÊ≥ïÔºåËøáÊª§ÊéâÊ≠§Á±ªÁöÑËØçÂë¢Ôºü

Ë∞¢Ë∞¢ÔºÅ
"
ÂÖ≥‰∫éÂàÜËØçÂô®ÂØπÁ©∫Ê†ºÁöÑÂ§ÑÁêÜ!,"ÂÖà‰∏æ‰æã:
‰ª£Á†Å:
StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
List<Term> termList = StandardTokenizer.segment(""4Êúà30Âè∑ 9ÁÇπÈíü"");
System.out.println(termList);
termList = StandardTokenizer.segment(""4Êúà30Âè∑9ÁÇπÈíü"");
System.out.println(termList);
termList = StandardTokenizer.segment(""4Êúà30Êó•9ÁÇπÈíü"");
System.out.println(termList);
===========================ÁªìÊûú====================================
[4Êúà/mq, 30/m, Âè∑null/nz, 9ÁÇπ/m, Èíü/n]
[4Êúà/mq, 30Âè∑9ÁÇπ/m, Èíü/n]
[4Êúà/mq, 30Êó•/mq, 9ÁÇπ/m, Èíü/n]

ÂÖ∂ÂÆû ÊàëÁêÜÊÉ≥‰∏≠ÁöÑÊïàÊûúÂ∫îËØ•ÊòØ:
[4Êúà/mq, 30Âè∑/mq, 9ÁÇπÈíü/mq]
[4Êúà/mq, 30Âè∑/mq, 9ÁÇπÈíü/mq]
[4Êúà/mq, 30Êó•/mq, 9ÁÇπÈíü/mq]

ËØ∑ÊïôÂçö‰∏ª,Ëøô‰∏™ÈóÆÈ¢ò  ËØ•Â¶Ç‰ΩïÂéªËß£ÂÜ≥Âë¢?
"
CRFÊ®°Âûã Âú∞ÂùÄ,"ÊÇ®Â•ΩÔºåÊàëÊòØnlpÊñ∞ÊâãÔºåÊÉ≥‰ΩøÁî®CRFÂàÜËØçÔºå‰ΩÜÁº∫Â∞ëÊ®°ÂûãÊñá‰ª∂Ôºå‰πãÂâçÊèê‰æõÁöÑÂú∞ÂùÄ‰πüÂ∑≤Â§±ÊïàÔºåËÉΩÂ§üÈáçÊñ∞ÂàÜ‰∫´‰∏ãÔºåË∞¢Ë∞¢ÔºÅ
"
"Âª∫ËÆÆÂú®ÂàÜËØçÊó∂,Â∞ÜÂ≠óÂÖ∏‰∏≠ÁöÑÂøΩÁï•Â§ßÂ∞èÂÜôÂäüËÉΩÊ∑ªÂä†Âà∞ÂàÜËØç‰∏≠!","Âú®ÂÆûÈôÖÁöÑÊêúÁ¥¢ÂΩì‰∏≠,Áî®Êà∑ÊòØ‰∏çËÄÉËôëËæìÂÖ•ÁöÑÂ≠óÊØçÊòØÂ§ßÂÜôËøòÊòØÂ∞èÂÜôÁöÑÈóÆÈ¢òÁöÑ.ÊâÄ‰ª•Âçö‰∏ªÂèØ‰ª•ËÄÉËôëÂú®ÂàÜËØçÁöÑÊó∂ÂÄô‰πü‰∏çËÄÉËôëËæìÂÖ•ÊñáÊú¨ÁöÑÂ§ßÂ∞èÂÜôÈóÆÈ¢ò,Áõ¥Êé•ËÉΩÂ§üÂåπÈÖçËØçÂ∫ì‰∏≠ÁöÑËØç.‰∏æ‰æãÂ¶Ç‰∏ã:
List<Term> termList = NLPTokenizer.segment(""Áà±Âê¨4g"");
System.out.println(termList);
==============ÂàÜËØçÁªìÊûú====================
[Áà±/v, Âê¨/v, 4g/nz]

List<Term> termList = NLPTokenizer.segment(""Áà±Âê¨4G"");
System.out.println(termList);
==============ÂàÜËØçÁªìÊûú====================
[Áà±Âê¨4G/nz]

ÂÖ∂ÂÆûÊàëÊÉ≥Ë¶ÅÁöÑÁªìÊûúÊòØ: Êó†ËÆ∫ËæìÂÖ•ÁöÑÊòØ Áà±Âê¨4g ËøòÊòØ Áà±Âê¨4G ÈÉΩËÉΩÂàÜÂá∫Êù•[Áà±Âê¨4g/nz]
"
PortableÂêåÊ≠•ÂçáÁ∫ßÂà∞v1.2.2,"v1.2.2ÁâàÊú¨Âü∫Êú¨Á®≥ÂÆö‰∫ÜÔºå‰∫éÊòØÂ∞ÜPortableÁâà‰πüÂçáÁ∫ßÂà∞v1.2.2„ÄÇ
"
CRFSegment Â¶Ç‰ΩïÂøΩÁï•Ê†áÁÇπÔºü,"segment.seg(sentence); Â¶Ç‰ΩïÈÖçÁΩÆÊâçËÉΩÂøΩÁï•Âè•Â≠ê‰∏≠ÁöÑÊ†áÁÇπÔºüÂèëÁé∞Ëøô‰∏™ÊñπÊ≥ï‰ºöÊääÊ†áÁÇπÂíåËØçÁöÑÁªÑÂêà‰Ωú‰∏∫Êñ∞ËØç
"
Portable,
ÂÖ≥‰∫éjdk7‰∏≠ ‰ΩøÁî®TextRankKeywordÊèêÂèñÂÖ≥ÈîÆËØçÊä•Comparison method violates its general contract!ÂºÇÂ∏∏,"ÊµãËØï‰ª£Á†Å:
String src = ""data/test.txt"";
        Scanner scanner = new Scanner(Paths.get(src),""gbk"");
        StringBuilder sb = new StringBuilder();
        while(scanner.hasNextLine()){
            sb.append(scanner.nextLine().trim());
        }
//      System.out.println(sb.toString());
        scanner.close();
        System.out.println(TextRankKeyword.getKeywordList(sb.toString(), 20));
# 

ÈîôËØØ‰ª£Á†Å:
java.lang.IllegalArgumentException: Comparison method violates its general contract!
    at java.util.TimSort.mergeLo(Unknown Source)
    at java.util.TimSort.mergeAt(Unknown Source)
    at java.util.TimSort.mergeCollapse(Unknown Source)
    at java.util.TimSort.sort(Unknown Source)
    at java.util.TimSort.sort(Unknown Source)
    at java.util.Arrays.sort(Unknown Source)
    at java.util.Collections.sort(Unknown Source)
    at com.hankcs.hanlp.summary.TextRankKeyword.getKeyword(TextRankKeyword.java:115)
    at com.hankcs.hanlp.summary.TextRankKeyword.getKeywordList(TextRankKeyword.java:47)

ÁªèËøáÁΩë‰∏äÊêúÁ¥¢:
http://www.tuicool.com/articles/MZreyuv
http://blog.csdn.net/ghsau/article/details/42012365

ÂèëÁé∞ÊòØjdk7 ‰∏≠ CollectionsÁöÑÊéíÂ∫èÁÆóÊ≥ïÂ∑≤ÁªèÂèëÁîüÂèòÂåñ,ÈúÄË¶ÅÂ§ÑÁêÜ‰∏§‰∏™ÊØîËæÉÂØπË±°Áõ∏Á≠âÁöÑÊÉÖÂÜµ.
Áî±‰∫éTextRankKeyword‰∏≠ÁöÑÊØîËæÉÂØπË±°ÊòØFloatÂØπË±°,ÊâÄ‰ª•ÊàëÊü•‰∫Ü‰∏ãFloatÁöÑcompareÊñπÊ≥ï(FloatÊòØÂÆûÁé∞Comparable<Float>Êé•Âè£ÁöÑ).‰ª£Á†Å:
public static int compare(float f1, float f2) {
        if (f1 < f2)
            return -1;           // Neither val is NaN, thisVal is smaller
        if (f1 > f2)
            return 1;            // Neither val is NaN, thisVal is larger

```
    // Cannot use floatToRawIntBits because of possibility of NaNs.
    int thisBits    = Float.floatToIntBits(f1);
    int anotherBits = Float.floatToIntBits(f2);

    return (thisBits == anotherBits ?  0 : // Values are equal
            (thisBits < anotherBits ? -1 : // (-0.0, 0.0) or (!NaN, NaN)
             1));                          // (0.0, -0.0) or (NaN, !NaN)
}
```

ÊâÄ‰ª•ÊàëÂ∞ÜÂçö‰∏ªÁöÑ‰ª£Á†Å:
Collections.sort(entryList, new Comparator<Map.Entry<String, Float>>()
        {
            @Override
            public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2)
            {
                return (o1.getValue() - o2.getValue() > 0 ? -1 : 1);
            }
        });

Êîπ‰∏∫‰∫Ü:
 Collections.sort(entryList, new Comparator<Map.Entry<String, Float>>()
        {
            @Override
            public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2)
            {
                return Float.compare(o1.getValue(),o1.getValue());
            }
        });

ËøôÊ†∑Â∞±‰∏çÊä•Èîô‰∫Ü.

ËØ∑Âçö‰∏ªÂèÇËÄÉÂìà. Âª∫ËÆÆÊúÄÂ•Ω‰ª£Á†Å‰∏≠ÁöÑÊâÄÊúâFloatÂèÇÊï∞ÁöÑÊØîËæÉÂÆûÁé∞ÈÉΩÈááÁî®Ëøô‰∏≠ÊñπÂºè.
"
"ÂêåÊó∂ÂºÄÂêØÊ†áÂáÜÂàÜËØçÂíåÁ¥¢ÂºïÂàÜËØçÁöÑÊï∞ÈáèËØçËØÜÂà´,ÁÑ∂ÂêéÁ¥¢ÂºïÊï∞ÈáèËØçÊó∂ÂèëÁîüÊï∞ÁªÑË∂äÁïåÂºÇÂ∏∏.","ÊµãËØï‰ª£Á†Å:
public class TestHanLP {
    @Test
    public void test1(){
        StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
        IndexTokenizer.SEGMENT.enableNumberQuantifierRecognize(true);
        List<Term> termList = StandardTokenizer.segment(""Ê≠§Â∏êÂè∑ÊúâÊ¨†Ë¥π‰∏öÂä°ÊòØ‰ªÄ‰πà"");
        termList = IndexTokenizer.segment(""Ê≠§Â∏êÂè∑ÊúâÊ¨†Ë¥π‰∏öÂä°ÊòØ‰ªÄ‰πà"");
        termList = StandardTokenizer.segment(""15307971214ËØùË¥πËøòÊúâÂ§öÂ∞ë"");
        termList = IndexTokenizer.segment(""15307971214ËØùË¥πËøòÊúâÂ§öÂ∞ë"");
        System.out.println(termList);
    }
}  
Âú®ÂØπ""Ê≠§Â∏êÂè∑ÊúâÊ¨†Ë¥π‰∏öÂä°ÊòØ‰ªÄ‰πà""ÂàÜËØçÊó∂ÂæàÊ≠£Â∏∏.
ÂØπ""""15307971214ËØùË¥πËøòÊúâÂ§öÂ∞ë""ÂàÜËØçÊó∂ÂèëÁîüÊï∞ÁªÑË∂äÁïåÂºÇÂ∏∏ÈîôËØØ.
# ÂºÇÂ∏∏ÈîôËØØ:

java.lang.ArrayIndexOutOfBoundsException: 19
    at com.hankcs.hanlp.seg.common.WordNet.get(WordNet.java:214)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.decorateResultForIndexMode(WordBasedGenerativeModelSegment.java:489)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:105)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:291)
    at com.hankcs.hanlp.tokenizer.IndexTokenizer.segment(IndexTokenizer.java:33)
    at com.xin.file.FileTest.test4(FileTest.java:93)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
"
Âä†ÂÖ•‰∫ÜÂØπÊï∞ÈáèËØçÁöÑËØÜÂà´!,"Âçö‰∏ª! ÊàëÂä†ÂÖ•‰∫ÜÂØπÊï∞ÈáèËØçÁöÑËØÜÂà´! ‰∏ªÈ¢ò‰ª£Á†ÅÂ¶Ç‰∏ã:
package com.hankcs.hanlp.recognition.mq;

import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.corpus.tag.Nature;
import com.hankcs.hanlp.dictionary.CoreDictionary;
import com.hankcs.hanlp.seg.common.Vertex;
import com.hankcs.hanlp.seg.common.WordNet;
import com.hankcs.hanlp.utility.Predefine;

import java.util.List;
import java.util.ListIterator;

import static com.hankcs.hanlp.dictionary.nr.NRConstant.WORD_ID;

/**
- Êï∞ÈáèËØçËØÜÂà´
- @author hankcs
  _/
  public class TranslatedQuantifierRecognition
  {
  /_*
  - ÊâßË°åËØÜÂà´
  - @param segResult Á≤óÂàÜÁªìÊûú
  - @param wordNetOptimum Á≤óÂàÜÁªìÊûúÂØπÂ∫îÁöÑËØçÂõæ
  - @param wordNetAll ÂÖ®ËØçÂõæ
    */
    public static void Recognition(List<Vertex> segResult, WordNet wordNetOptimum, WordNet wordNetAll)
    {
    StringBuilder sbQuantifier = new StringBuilder();
    int appendTimes = 0;
    ListIterator<Vertex> listIterator = segResult.listIterator();
    listIterator.next();
    int line = 1;
    int activeLine = 1;
    while (listIterator.hasNext())
    {
        Vertex vertex = listIterator.next();
        if (appendTimes > 0)
        {
            if (vertex.guessNature() == Nature.q ||vertex.guessNature() == Nature.qt
                    ||vertex.guessNature() == Nature.qv 
                    || vertex.guessNature() == Nature.qt
                    ||vertex.guessNature() == Nature.nx)
            {
                sbQuantifier.append(vertex.realWord);
                ++appendTimes;
            }
            else
            {
                // ËØÜÂà´ÁªìÊùü
                if (appendTimes > 1)
                {
                    if (HanLP.Config.DEBUG)
                    {
                        System.out.println(""Êï∞ÈáèËØçËØÜÂà´Âá∫Ôºö"" + sbQuantifier.toString());
                    }
                    wordNetOptimum.insert(activeLine, new Vertex(Predefine.TAG_QUANTIFIER, sbQuantifier.toString(), new CoreDictionary.Attribute(Nature.mq), WORD_ID), wordNetAll);
                }
                sbQuantifier.setLength(0);
                appendTimes = 0;
            }
        }
        else
        {
            // Êï∞Â≠ómËß¶ÂèëËØÜÂà´
            if (vertex.guessNature() == Nature.m)
            {
                sbQuantifier.append(vertex.realWord);
                ++appendTimes;
                activeLine = line;
            }
        }
    
    ```
    line += vertex.realWord.length();
    ```
    
    }
    }
    }
"
ÂÖ≥‰∫éÂàÜËØçÁöÑÈóÆÈ¢ò?,"Âçö‰∏ª! ‰Ω†Â•Ω!
     ÂàÜËØç""ÊâãÊú∫Â•ó"",ÁªìÊûúÔºö[ÊâãÊú∫/n, Â•ó/q]
‰ΩÜÊòØÂú®ËØçÂ∫ìCoreNatureDictionary.txt‰∏≠ÊâæÂà∞ÊúâÊâãÊú∫Â•óÁöÑÁõ∏ÂÖ≥ËØçÔºåÂ¶Ç‰∏ãÔºö
    Line 11870: ‰ΩøÊâãÊú∫ n 2
    Line 61022: ÊâãÊú∫ n 6656
    Line 61023: ÊâãÊú∫ÂÖö nz 2
    Line 61024: ÊâãÊú∫Âç° nz 17
    Line 61025: ÊâãÊú∫Â•ó nz 2
    Line 61026: ÊâãÊú∫Êä• nz 37
    Line 137480: ÈÉ®ÊâãÊú∫ n 53
‰∏∫‰ªÄ‰πàÂàÜËØç‰∏çÊòØÂàÜÊàê„ÄÄ[ÊâãÊú∫Â•ó/nz] Âë¢„ÄÄ
"
ÂØπ‰∫é‰∏≠ÊñáÊï∞ÈáèËØçÊîπÂ¶Ç‰ΩïÂàáÂàÜ?,"ËøôÊòØÊàëÁöÑÂàáÂàÜ‰æãÂ≠ê:
Êú™ÂàÜËØç:ÂçÅ‰πùÂÖÉÂ•óÈ§êÂåÖÊã¨‰ªÄ‰πà
Ê†áÂáÜÂàÜËØç:[ÂçÅ/m, ‰πù/b, ÂÖÉ/q, Â•óÈ§ê/n, ÂåÖÊã¨/v, ‰ªÄ‰πà/ry]
Êô∫ËÉΩÂàÜËØç:[ÂçÅ/m, ‰πù/b, ÂÖÉ/q, Â•óÈ§ê/n, ÂåÖÊã¨/v, ‰ªÄ‰πà/ry]
Á¥¢ÂºïÂàÜËØç:[ÂçÅ/m, ‰πù/b, ÂÖÉ/q, Â•óÈ§ê/n, ÂåÖÊã¨/v, ‰ªÄ‰πà/ry]

ÁêÜÊÉ≥ÊïàÊûúÔºö
[ÂçÅ‰πùÂÖÉ/mq, Â•óÈ§ê/n, ÂåÖÊã¨/v, ‰ªÄ‰πà/ry]
"
BaseNodeÁºñËØëÈîôËØØ,"ÂèëÁé∞BaseNodeÁöÑwalkToLoadÊñπÊ≥ïÊúâÁºñËØëÈîôËØØ„ÄÇ
ÈîôËØØ‰ø°ÊÅØÔºö

> The method walkToLoad(ByteArray, BaseNode.ValueArray) in the type BaseNode is not applicable for the arguments (ByteArray, BaseNode<V>.ValueArray)
"
ËøêË°åTestSegment.java ÊµãËØïÁ±ª‰∏≠ÁöÑ testShortest„ÄÅtestNTÊñπÊ≥ï Âá∫Áé∞‰ª•‰∏ãÈîôËØØ,"ÁâàÊú¨‰∏∫ÂΩìÂâçÊúÄÊñ∞1.1.5
java.lang.ExceptionInInitializerError
    at com.hankcs.hanlp.recognition.ns.PlaceRecognition.roleTag(PlaceRecognition.java:106)
    at com.hankcs.hanlp.recognition.ns.PlaceRecognition.Recognition(PlaceRecognition.java:36)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:75)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:242)
    at com.hankcs.test.seg.TestSegment.testShortest(TestSegment.java:71)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at junit.framework.TestCase.runTest(TestCase.java:168)
    at junit.framework.TestCase.runBare(TestCase.java:134)
    at junit.framework.TestResult$1.protect(TestResult.java:110)
    at junit.framework.TestResult.runProtected(TestResult.java:128)
    at junit.framework.TestResult.run(TestResult.java:113)
    at junit.framework.TestCase.run(TestCase.java:124)
    at junit.framework.TestSuite.runTest(TestSuite.java:232)
    at junit.framework.TestSuite.run(TestSuite.java:227)
    at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:131)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
    at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.get(DoubleArrayTrie.java:1210)
    at com.hankcs.hanlp.dictionary.CoreDictionary.get(CoreDictionary.java:183)
    at com.hankcs.hanlp.dictionary.ns.PlaceDictionary.<clinit>(PlaceDictionary.java:60)
    ... 23 more
"
Â§ßËµûÔºå‰∏çËøáÊëòË¶ÅÊïàÊûúÊúâÁÇπÈóÆÈ¢ò,"Êèê‰æõÁöÑ‰æãÂ≠êÁªìÊûúÊòØÔºö
[Êó†ÈôêÁÆóÊ≥ïÁöÑ‰∫ßÁîüÊòØÁî±‰∫éÊú™ËÉΩÁ°ÆÂÆöÁöÑÂÆö‰πâÁªàÊ≠¢Êù°‰ª∂, ËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑÊó∂Èó¥ÂÜÖÁªàÊ≠¢, ËøôÁ±ªÁÆóÊ≥ïÂú®ÊúâÈôêÁöÑ‰∏ÄÊÆµÊó∂Èó¥ÂÜÖÁªàÊ≠¢]

ÁúãÊù•ËøòÊòØÊúâÂæàÂ§ö‰ºòÂåñÁ©∫Èó¥ÁöÑ„ÄÇ

‰∏çËøáËøô‰πàÂÖ®Èù¢ÁöÑNLP packageÂ∑≤ÁªèÂæàËµû‰∫ÜÔºåÂâ©‰∏ãÁöÑ‰ºòÂåñËøáÁ®ãÂÖ∂ÂÆûÁõ∏ÂØπÂÆπÊòì‰∏Ä‰∫õÂï¶~
"
ËØ∑ÈóÆËØ•Â¶Ç‰ΩïÂàáÂàÜ*/,"ËæìÂÖ•  È±º300g*2/ÁªÑ

‰ΩøÁî®  HanLP.newSegment() 
ÂàáÂàÜÁªìÊûú [È±º/n, 300g*2//nx, ÁªÑ/n]

‰ΩøÁî® AhoCorasickSegment().enablePartOfSpeechTagging(true)
ÂàáÂàÜÁªìÊûú [È±º/nz, 300g*2//nx, ÁªÑ/nz]

‰ΩøÁî® NShortSegment()
ÂàáÂàÜÁªìÊûú [È±º/n, 300g*2//nx, ÁªÑ/n]

ÈÉΩÊòØ  [È±º, 300g*2/, ÁªÑ]

ËØ∑ÈóÆËØ•Â¶Ç‰ΩïÈÖçÁΩÆÊâçËÉΩÂàáÂá∫
[È±º, 300g, \* ,2,  / , ÁªÑ]  or  [È±º, 300 , g, *,2 , / , ÁªÑ] ?

/////////////////////
Ë°•‰∏ä È±º300ÂÖã*2/ÁªÑÁöÑÊÉÖÂΩ¢

 [È±º/n, 300/m, ÂÖã/q, *2//nx, ÁªÑ/n]
‰ªçÊúâ *2/ Âàá‰∏∫‰∏Ä‰∏™ËØçÁöÑÊÉÖÂÜµÂèëÁîü
"
Êâ©Â±ïËØçÂ∫ìÂä†ÂÖ•Ëã±ÊñáÔºåËæìÂÖ•Êâ©Â±ïËã±ÊñáËøûÊé•Âè¶‰∏ÄËã±ÊñáÔºåÂàÜËØç‰ºöÊä•Èîô„ÄÇ,"Êâ©Â±ïËØçÂ∫ìÂä†ÂÖ•Ëã±ÊñáÔºåËæìÂÖ•Êâ©Â±ïËã±ÊñáËøûÊé•Âè¶‰∏ÄËã±ÊñáÔºåÂàÜËØç‰ºöÊä•Èîô„ÄÇ
ÂéüÊú¨‰ª•‰∏∫ÊòØËá™ÂÆöËØçÊÄßÁöÑÈóÆÈ¢òÔºå‰ΩÜÂ∞ÜËØçÊÄßÊîπ‰∏∫nÔºå‰ªç‰ºöÊä•Èîô

ËØçÂ∫ìÂÜÖÂÆπ‰∏∫
BENQ    n   1024
BENTLEY n   1024

ËæìÂÖ•""BENQphone"";
‰ΩøÁî®Ê†áÂáÜÂàÜËØç   HanLP.segment(text)

ÂºÄÂêØdebugÂ¶Ç‰∏ãÔºö

Á≤óÂàÜËØçÁΩëÔºö
0:[ ]
1:[BENQ]
2:[ENQphone]
3:[]
4:[]
5:[]
6:[]
7:[]
8:[]
9:[]
10:[ ]

‰ºöÊä•Âá∫ËøôÊ†∑ÁöÑÈîôËØØ 

Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1
    at java.util.ArrayList.<init>(ArrayList.java:142)
    at com.hankcs.hanlp.seg.HiddenMarkovModelSegment.convert(HiddenMarkovModelSegment.java:238)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:50)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:144)
    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:39)
    at com.hankcs.hanlp.HanLP.segment(HanLP.java:354)

ÂéüÂõ†ÊòØ  com.hankcs.hanlp.seg.Viterbi.ViterbiSegment  ‰∏≠ 47Ë°å
 List<Vertex> vertexList = viterbi(wordNetAll);

ËøîÂõûÁªìÊûú
vertexList =[ ]
vertexList.size() = 1

‰ΩÜËæìÂÖ• ""BENQBENTLEYphone""  

ÂàôËæìÂá∫Ê≤°Êä•ÈîôÔºå‰ΩÜÁªìÊûú‰∏çÊòØÊÉ≥Ë¶ÅÁöÑ

‰∫∫ÂêçËßíËâ≤ËßÇÂØüÔºö[  A 42634591 ][BENQ A 42634591 ][B L 3 ][ENTLEYphone A 42634591 ][  A 42634591 ]
‰∫∫ÂêçËßíËâ≤Ê†áÊ≥®Ôºö[ /A ,BENQ/A ,B/L ,ENTLEYphone/A , /A]
[BENQ/n, B/nx, ENTLEYphone/nx]

ËØ∑ÈóÆËØ•Â¶Ç‰Ωï‰øÆÊîπÊâçËÉΩÊ∑ªÂä†Ëã±ÊñáËØçÂ∫ìÂë¢Ôºü
"
while ÊñπÊ≥ïÂèØËÉΩÂ∞ëÂÜôbreak,"javaÊñ∞ÊâãÁ¨¨‰∏ÄÊ¨°‰ΩøÁî®github,Â¶ÇÊúâÁº∫Â§±ËØ∑ËßÅË∞Ö

‰ΩøÁî® findbugsÊâæÂà∞ÁöÑÔºåÊä•‰ª•‰∏ãÈîôËØØ
Value of Node.label from previous case is overwritten here due to switch statement fall through [Scariest(1), High confidence]

Âú® com.hankcs.hanlp.dependency.common.NodeÁöÑÁ¨¨182Ë°å

```
        case wh:
            label = ""x"";
        case begin:
            label = ""root"";
            break;
```

case wh Ê≤°Êúâbreak ,‰∏çÁ°ÆÂÆöËøôÊ†∑ÊòØÂê¶‰ºöÈÄ†ÊàêÈóÆÈ¢ò„ÄÇ
"
Bump tensorflow-gpu from 1.4.0 to 2.12.0,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.12.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.12.0</h2>
<h1>Release 2.12.0</h1>
<h2>TensorFlow</h2>
<h3>Breaking Changes</h3>
<ul>
<li>
<p>Build, Compilation and Packaging</p>
<ul>
<li>Removed redundant packages <code>tensorflow-gpu</code> and <code>tf-nightly-gpu</code>. These packages were removed and replaced with packages that direct users to switch to <code>tensorflow</code> or <code>tf-nightly</code> respectively. Since TensorFlow 2.1, the only difference between these two sets of packages was their names, so there is no loss of functionality or GPU support. See <a href=""https://pypi.org/project/tensorflow-gpu"">https://pypi.org/project/tensorflow-gpu</a> for more details.</li>
</ul>
</li>
<li>
<p><code>tf.function</code>:</p>
<ul>
<li><code>tf.function</code> now uses the Python inspect library directly for parsing the signature of the Python function it is decorated on. This change may break code where the function signature is malformed, but was ignored previously, such as:
<ul>
<li>Using <code>functools.wraps</code> on a function with different signature</li>
<li>Using <code>functools.partial</code> with an invalid <code>tf.function</code> input</li>
</ul>
</li>
<li><code>tf.function</code> now enforces input parameter names to be valid Python identifiers. Incompatible names are automatically sanitized similarly to existing SavedModel signature behavior.</li>
<li>Parameterless <code>tf.function</code>s are assumed to have an empty <code>input_signature</code> instead of an undefined one even if the <code>input_signature</code> is unspecified.</li>
<li><code>tf.types.experimental.TraceType</code> now requires an additional <code>placeholder_value</code> method to be defined.</li>
<li><code>tf.function</code> now traces with placeholder values generated by TraceType instead of the value itself.</li>
</ul>
</li>
<li>
<p>Experimental APIs <code>tf.config.experimental.enable_mlir_graph_optimization</code> and <code>tf.config.experimental.disable_mlir_graph_optimization</code> were removed.</p>
</li>
</ul>
<h3>Major Features and Improvements</h3>
<ul>
<li>
<p>Support for Python 3.11 has been added.</p>
</li>
<li>
<p>Support for Python 3.7 has been removed. We are not releasing any more patches for Python 3.7.</p>
</li>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Add 16-bit float type support for built-in op <code>fill</code>.</li>
<li>Transpose now supports 6D tensors.</li>
<li>Float LSTM now supports diagonal recurrent tensors: <a href=""https://arxiv.org/abs/1903.08023"">https://arxiv.org/abs/1903.08023</a></li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Coordination service now works with <code>dtensor.initialize_accelerator_system</code>, and enabled by default.</li>
<li>Add <code>tf.experimental.dtensor.is_dtensor</code> to check if a tensor is a DTensor instance.</li>
</ul>
</li>
<li>
<p><code>tf.data</code>:</p>
<ul>
<li>Added support for alternative checkpointing protocol which makes it possible to checkpoint the state of the input pipeline without having to store the contents of internal buffers. The new functionality can be enabled through the <code>experimental_symbolic_checkpoint</code> option of <code>tf.data.Options()</code>.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.random()</code> operation, which controls whether the sequence of generated random numbers should be re-randomized every epoch or not (the default behavior). If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>random()</code> operation will produce a different (deterministic) sequence of numbers every epoch.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.sample_from_datasets()</code> operation, which controls whether the sequence of generated random numbers used for sampling should be re-randomized every epoch or not. If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>sample_from_datasets()</code> operation will use a different (deterministic) sequence of numbers every epoch.</li>
</ul>
</li>
<li>
<p><code>tf.test</code>:</p>
<ul>
<li>Added <code>tf.test.experimental.sync_devices</code>, which is useful for accurately measuring performance in benchmarks.</li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.12.0</h1>
<h3>Breaking Changes</h3>
<ul>
<li>
<p>Build, Compilation and Packaging</p>
<ul>
<li>Removed redundant packages <code>tensorflow-gpu</code> and <code>tf-nightly-gpu</code>. These packages were removed and replaced with packages that direct users to switch to <code>tensorflow</code> or <code>tf-nightly</code> respectively. Since TensorFlow 2.1, the only difference between these two sets of packages was their names, so there is no loss of functionality or GPU support. See <a href=""https://pypi.org/project/tensorflow-gpu"">https://pypi.org/project/tensorflow-gpu</a> for more details.</li>
</ul>
</li>
<li>
<p><code>tf.function</code>:</p>
<ul>
<li><code>tf.function</code> now uses the Python inspect library directly for parsing the signature of the Python function it is decorated on. This change may break code where the function signature is malformed, but was ignored previously, such as:
<ul>
<li>Using <code>functools.wraps</code> on a function with different signature</li>
<li>Using <code>functools.partial</code> with an invalid <code>tf.function</code> input</li>
</ul>
</li>
<li><code>tf.function</code> now enforces input parameter names to be valid Python identifiers. Incompatible names are automatically sanitized similarly to existing SavedModel signature behavior.</li>
<li>Parameterless <code>tf.function</code>s are assumed to have an empty <code>input_signature</code> instead of an undefined one even if the <code>input_signature</code> is unspecified.</li>
<li><code>tf.types.experimental.TraceType</code> now requires an additional <code>placeholder_value</code> method to be defined.</li>
<li><code>tf.function</code> now traces with placeholder values generated by TraceType instead of the value itself.</li>
</ul>
</li>
<li>
<p>Experimental APIs <code>tf.config.experimental.enable_mlir_graph_optimization</code> and <code>tf.config.experimental.disable_mlir_graph_optimization</code> were removed.</p>
</li>
</ul>
<h3>Major Features and Improvements</h3>
<ul>
<li>
<p>Support for Python 3.11 has been added.</p>
</li>
<li>
<p>Support for Python 3.7 has been removed. We are not releasing any more patches for Python 3.7.</p>
</li>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Add 16-bit float type support for built-in op <code>fill</code>.</li>
<li>Transpose now supports 6D tensors.</li>
<li>Float LSTM now supports diagonal recurrent tensors: <a href=""https://arxiv.org/abs/1903.08023"">https://arxiv.org/abs/1903.08023</a></li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Coordination service now works with <code>dtensor.initialize_accelerator_system</code>, and enabled by default.</li>
<li>Add <code>tf.experimental.dtensor.is_dtensor</code> to check if a tensor is a DTensor instance.</li>
</ul>
</li>
<li>
<p><code>tf.data</code>:</p>
<ul>
<li>Added support for alternative checkpointing protocol which makes it possible to checkpoint the state of the input pipeline without having to store the contents of internal buffers. The new functionality can be enabled through the <code>experimental_symbolic_checkpoint</code> option of <code>tf.data.Options()</code>.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.random()</code> operation, which controls whether the sequence of generated random numbers should be re-randomized every epoch or not (the default behavior). If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>random()</code> operation will produce a different (deterministic) sequence of numbers every epoch.</li>
<li>Added a new <code>rerandomize_each_iteration</code> argument for the <code>tf.data.Dataset.sample_from_datasets()</code> operation, which controls whether the sequence of generated random numbers used for sampling should be re-randomized every epoch or not. If <code>seed</code> is set and <code>rerandomize_each_iteration=True</code>, the <code>sample_from_datasets()</code> operation will use a different (deterministic) sequence of numbers every epoch.</li>
</ul>
</li>
<li>
<p><code>tf.test</code>:</p>
<ul>
<li>Added <code>tf.test.experimental.sync_devices</code>, which is useful for accurately measuring performance in benchmarks.</li>
</ul>
</li>
<li>
<p><code>tf.experimental.dtensor</code>:</p>
<ul>
<li>Added experimental support to ReduceScatter fuse on GPU (NCCL).</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0db597d0d758aba578783b5bf46c889700a45085""><code>0db597d</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60051"">#60051</a> from tensorflow/venkat2469-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/1a12f5939238da4c4372a095e589f2801baecf41""><code>1a12f59</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/aa4d558019c48a7ab5ff403b56e7d80fda4d13ce""><code>aa4d558</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60050"">#60050</a> from tensorflow/venkat-patch-6</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bd1ab8a9ef0df2fbaefd6cfa994687c0aa1dca55""><code>bd1ab8a</code></a> Update the security section in RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4905be05fd63d9943dec222ee83dcc6f522b671b""><code>4905be0</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60049"">#60049</a> from tensorflow/venkat-patch-5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9f96caad340d0f4bdb10a1addd61fe8a40fa6fdb""><code>9f96caa</code></a> Update setup.py on TF release branch with released version of Estimator and k...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e719b6b83b60fb03ff06c5397686888834fc5296""><code>e719b6b</code></a> Update Relese.md (<a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60033"">#60033</a>)</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/64a9d548b520659a9a226250dfd204e0038069bc""><code>64a9d54</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/60017"">#60017</a> from tensorflow/joefernandez-patch-2.12-release-notes</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7a4ebfd7f814a1b45a9d57c880fcbf8abb111fff""><code>7a4ebfd</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e0e10a9f2e3e09656f14b7b1339d94c7b43f84e6""><code>e0e10a9</code></a> Merge pull request <a href=""https://redirect.github.com/tensorflow/tensorflow/issues/59988"">#59988</a> from tensorflow-jenkins/version-numbers-2.12.0-8756</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.12.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.12.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Model_onehot.h5  was lost.Could you upload a new one? THkS,
Bump pillow from 6.0.0 to 9.3.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.3.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Initialize libtiff buffer when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Limit SAMPLESPERPIXEL to avoid runtime DOS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>Inline fname2char to fix memory leak <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6329"">#6329</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Fix memory leaks related to text features <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6330"">#6330</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Use double quotes for version check on old CPython on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6695"">#6695</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>GHA: replace deprecated set-output command with GITHUB_OUTPUT file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6697"">#6697</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Remove backup implementation of Round for Windows platforms <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6693"">#6693</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Upload fribidi.dll to GitHub Actions <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6532"">#6532</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Fixed set_variation_by_name offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6445"">#6445</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Windows build improvements <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6562"">#6562</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Fix malloc in _imagingft.c:font_setvaraxes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6690"">#6690</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Only use ASCII characters in C source file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6691"">#6691</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Release Python GIL when converting images using matrix operations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6418"">#6418</a> [<a href=""https://github.com/hmaarrfk""><code>@‚Äãhmaarrfk</code></a>]</li>
<li>Added ExifTags enums <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6630"">#6630</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Do not modify previous frame when calculating delta in PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6683"">#6683</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added support for reading BMP images with RLE4 compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6674"">#6674</a> [<a href=""https://github.com/npjg""><code>@‚Äãnpjg</code></a>]</li>
<li>Decode JPEG compressed BLP1 data in original mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6678"">#6678</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>pylint warnings <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6659"">#6659</a> [<a href=""https://github.com/marksmayo""><code>@‚Äãmarksmayo</code></a>]</li>
<li>Added GPS TIFF tag info <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6661"">#6661</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added conversion between RGB/RGBA/RGBX and LAB <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6647"">#6647</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Do not attempt normalization if mode is already normal <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6644"">#6644</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed seeking to an L frame in a GIF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6576"">#6576</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Consider all frames when selecting mode for PNG save_all <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6610"">#6610</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Don't reassign crc on ChunkStream close <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6627"">#6627</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Raise a warning if NumPy failed to raise an error during conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6594"">#6594</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Only read a maximum of 100 bytes at a time in IMT header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6623"">#6623</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Show all frames in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6611"">#6611</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Allow FLI palette chunk to not be first <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6626"">#6626</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>If first GIF frame has transparency for RGB_ALWAYS loading strategy, use RGBA mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6592"">#6592</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Round box position to integer when pasting embedded color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6517"">#6517</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Removed EXIF prefix when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6582"">#6582</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Pad IM palette to 768 bytes when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6579"">#6579</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added DDS BC6H reading <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6449"">#6449</a> [<a href=""https://github.com/ShadelessFox""><code>@‚ÄãShadelessFox</code></a>]</li>
<li>Added support for opening WhiteIsZero 16-bit integer TIFF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6642"">#6642</a> [<a href=""https://github.com/JayWiz""><code>@‚ÄãJayWiz</code></a>]</li>
<li>Raise an error when allocating translucent color to RGB palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6654"">#6654</a> [<a href=""https://github.com/jsbueno""><code>@‚Äãjsbueno</code></a>]</li>
<li>Moved mode check outside of loops <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6650"">#6650</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added reading of TIFF child images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6569"">#6569</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved ImageOps palette handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6596"">#6596</a> [<a href=""https://github.com/PososikTeam""><code>@‚ÄãPososikTeam</code></a>]</li>
<li>Defer parsing of palette into colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6567"">#6567</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Apply transparency to P images in ImageTk.PhotoImage <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6559"">#6559</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use rounding in ImageOps contain() and pad() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6522"">#6522</a> [<a href=""https://github.com/bibinhashley""><code>@‚Äãbibinhashley</code></a>]</li>
<li>Fixed GIF remapping to palette with duplicate entries <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6548"">#6548</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Allow remap_palette() to return an image with less than 256 palette entries <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6543"">#6543</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Corrected BMP and TGA palette size when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6500"">#6500</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.3.0 (2022-10-29)</h2>
<ul>
<li>
<p>Limit SAMPLESPERPIXEL to avoid runtime DOS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a>
[wiredfool]</p>
</li>
<li>
<p>Initialize libtiff buffer when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a>
[radarhere]</p>
</li>
<li>
<p>Inline fname2char to fix memory leak <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6329"">#6329</a>
[nulano]</p>
</li>
<li>
<p>Fix memory leaks related to text features <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6330"">#6330</a>
[nulano]</p>
</li>
<li>
<p>Use double quotes for version check on old CPython on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6695"">#6695</a>
[hugovk]</p>
</li>
<li>
<p>Remove backup implementation of Round for Windows platforms <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6693"">#6693</a>
[cgohlke]</p>
</li>
<li>
<p>Fixed set_variation_by_name offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6445"">#6445</a>
[radarhere]</p>
</li>
<li>
<p>Fix malloc in _imagingft.c:font_setvaraxes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6690"">#6690</a>
[cgohlke]</p>
</li>
<li>
<p>Release Python GIL when converting images using matrix operations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6418"">#6418</a>
[hmaarrfk]</p>
</li>
<li>
<p>Added ExifTags enums <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6630"">#6630</a>
[radarhere]</p>
</li>
<li>
<p>Do not modify previous frame when calculating delta in PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6683"">#6683</a>
[radarhere]</p>
</li>
<li>
<p>Added support for reading BMP images with RLE4 compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6674"">#6674</a>
[npjg, radarhere]</p>
</li>
<li>
<p>Decode JPEG compressed BLP1 data in original mode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6678"">#6678</a>
[radarhere]</p>
</li>
<li>
<p>Added GPS TIFF tag info <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6661"">#6661</a>
[radarhere]</p>
</li>
<li>
<p>Added conversion between RGB/RGBA/RGBX and LAB <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6647"">#6647</a>
[radarhere]</p>
</li>
<li>
<p>Do not attempt normalization if mode is already normal <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6644"">#6644</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d594f4cb8dc47fb0c69ae58d9fff86faae4515bd""><code>d594f4c</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/909dc64ed5f676169aa3d9b0c26f132a06321b83""><code>909dc64</code></a> 9.3.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1a51ce7b955c65c8f2c6bc7772735b197b8a6aa3""><code>1a51ce7</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6699"">#6699</a> from hugovk/security-libtiff_buffer</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/2444cddab2f83f28687c7c20871574acbb6dbcf3""><code>2444cdd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6700"">#6700</a> from hugovk/security-samples_per_pixel-sec</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/744f455830871d61a8de0a5e629d4c2e33817cbb""><code>744f455</code></a> Added release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/0846bfae48513f2f51ca8547ed3b8954fa501fda""><code>0846bfa</code></a> Add to release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/799a6a01052cea3f417a571d7c64cd14acc18c64""><code>799a6a0</code></a> Fix linting</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/00b25fd3ac3648bc28eff5d4c4d816e605e3f05f""><code>00b25fd</code></a> Hide UserWarning in logs</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/05b175ef88c22f5c416bc9b8d5b897dea1abbf2c""><code>05b175e</code></a> Tighter test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/13f2c5ae14901c89c38f898496102afd9daeaf6d""><code>13f2c5a</code></a> Prevent DOS with large SAMPLESPERPIXEL in Tiff IFD</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.3.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.9.3,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.9.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.9.3</h2>
<h1>Release 2.9.3</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes an overflow in <code>tf.keras.losses.poisson</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41887"">CVE-2022-41887</a>)</li>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
<li>Fixes a heap OOB in <code>FractionalAvgPool</code> and <code>FractionalMaxPool</code>(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41900"">CVE-2022-41900</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> in <code>SparseMatrixNNZ</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41901"">CVE-2022-41901</a>)</li>
<li>Fixes an OOB write in grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41902"">CVE-2022-41902</a>)</li>
<li>Fixes a overflow in <code>ResizeNearestNeighborGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41907"">CVE-2022-41907</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>PyFunc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41908"">CVE-2022-41908</a>)</li>
<li>Fixes a segfault in <code>CompositeTensorVariantToComponents</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41909"">CVE-2022-41909</a>)</li>
<li>Fixes a invalid char to bool conversion in printing a tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41911"">CVE-2022-41911</a>)</li>
<li>Fixes a heap overflow in <code>QuantizeAndDequantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41910"">CVE-2022-41910</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> via missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>TensorListScatter</code> and <code>TensorListScatterV2</code> in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
</ul>
<h2>TensorFlow 2.9.2</h2>
<h1>Release 2.9.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a <code>CHECK</code> failure in tf.reshape caused by overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35934"">CVE-2022-35934</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes an OOB read in <code>Gather_nd</code> op in TF Lite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35937"">CVE-2022-35937</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>TensorListReserve</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35960"">CVE-2022-35960</a>)</li>
<li>Fixes an OOB write in <code>Scatter_nd</code> op in TF Lite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35939"">CVE-2022-35939</a>)</li>
<li>Fixes an integer overflow in <code>RaggedRangeOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35940"">CVE-2022-35940</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>AvgPoolOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35941"">CVE-2022-35941</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>UnbatchGradOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35952"">CVE-2022-35952</a>)</li>
<li>Fixes a segfault TFLite converter on per-channel quantized transposed convolutions (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-36027"">CVE-2022-36027</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>AvgPool3DGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35959"">CVE-2022-35959</a>)</li>
<li>Fixes a <code>CHECK</code> failures in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35963"">CVE-2022-35963</a>)</li>
<li>Fixes a segfault in <code>BlockLSTMGradV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35964"">CVE-2022-35964</a>)</li>
<li>Fixes a segfault in <code>LowerBound</code> and <code>UpperBound</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35965"">CVE-2022-35965</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.9.3</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes an overflow in <code>tf.keras.losses.poisson</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41887"">CVE-2022-41887</a>)</li>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
<li>Fixes a heap OOB in <code>FractionalAvgPool</code> and <code>FractionalMaxPool</code>(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41900"">CVE-2022-41900</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> in <code>SparseMatrixNNZ</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41901"">CVE-2022-41901</a>)</li>
<li>Fixes an OOB write in grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41902"">CVE-2022-41902</a>)</li>
<li>Fixes a overflow in <code>ResizeNearestNeighborGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41907"">CVE-2022-41907</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>PyFunc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41908"">CVE-2022-41908</a>)</li>
<li>Fixes a segfault in <code>CompositeTensorVariantToComponents</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41909"">CVE-2022-41909</a>)</li>
<li>Fixes a invalid char to bool conversion in printing a tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41911"">CVE-2022-41911</a>)</li>
<li>Fixes a heap overflow in <code>QuantizeAndDequantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41910"">CVE-2022-41910</a>)</li>
<li>Fixes a <code>CHECK</code> failure in <code>SobolSample</code> via missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>TensorListScatter</code> and <code>TensorListScatterV2</code> in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-35935"">CVE-2022-35935</a>)</li>
</ul>
<h1>Release 2.8.4</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap OOB failure in <code>ThreadUnsafeUnigramCandidateSampler</code> caused by missing validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41880"">CVE-2022-41880</a>)</li>
<li>Fixes a segfault in <code>ndarray_tensor_bridge</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41884"">CVE-2022-41884</a>)</li>
<li>Fixes an overflow in <code>FusedResizeAndPadConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41885"">CVE-2022-41885</a>)</li>
<li>Fixes a overflow in <code>ImageProjectiveTransformV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41886"">CVE-2022-41886</a>)</li>
<li>Fixes an FPE in <code>tf.image.generate_bounding_box_proposals</code> on GPU (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41888"">CVE-2022-41888</a>)</li>
<li>Fixes a segfault in <code>pywrap_tfe_src</code> caused by invalid attributes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41889"">CVE-2022-41889</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>BCast</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41890"">CVE-2022-41890</a>)</li>
<li>Fixes a segfault in <code>TensorListConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41891"">CVE-2022-41891</a>)</li>
<li>Fixes a <code>CHECK_EQ</code> fail in <code>TensorListResize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41893"">CVE-2022-41893</a>)</li>
<li>Fixes an overflow in <code>CONV_3D_TRANSPOSE</code> on TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41894"">CVE-2022-41894</a>)</li>
<li>Fixes a heap OOB in <code>MirrorPadGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41895"">CVE-2022-41895</a>)</li>
<li>Fixes a crash in <code>Mfcc</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41896"">CVE-2022-41896</a>)</li>
<li>Fixes a heap OOB in <code>FractionalMaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41897"">CVE-2022-41897</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41898"">CVE-2022-41898</a>)</li>
<li>Fixes a <code>CHECK</code> fail in <code>SdcaOptimizer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41899"">CVE-2022-41899</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a5ed5f39b675a1c6f315e0caf3ad4b38478fa571""><code>a5ed5f3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58584"">#58584</a> from tensorflow/vinila21-patch-2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/258f9a1251346d93e129c53f82d21732df6067f5""><code>258f9a1</code></a> Update py_func.cc</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/cd27cfb438b78a019ff8a215a9d6c58d10c062c3""><code>cd27cfb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58580"">#58580</a> from tensorflow-jenkins/version-numbers-2.9.3-24474</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3e75385ee6c9ef8f06d6848244e1421c603dd4a1""><code>3e75385</code></a> Update version numbers to 2.9.3</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bc72c39774b0a0cb38ed03e5ee09fa78103ed749""><code>bc72c39</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58482"">#58482</a> from tensorflow-jenkins/relnotes-2.9.3-25695</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3506c90f5ac0f471a6b1d60d4055b14ca3da170b""><code>3506c90</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8dcb48e384cd3914458f3c494f1da878ae8dc6d5""><code>8dcb48e</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4f34ec84994e63cf47c1d13748a404edd3d5a0d3""><code>4f34ec8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58576"">#58576</a> from pak-laura/c2.99f03a9d3bafe902c1e6beb105b2f2417...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6fc67e408f239384d26acabc34d287911af92dc8""><code>6fc67e4</code></a> Replace CHECK with returning an InternalError on failing to create python tuple</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5dbe90ad21068007cbc31a56e8ed514ec27e0b26""><code>5dbe90a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/58570"">#58570</a> from tensorflow/r2.9-7b174a0f2e4</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.9.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.9.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump numpy from 1.16.4 to 1.22.0,"Bumps [numpy](https://github.com/numpy/numpy) from 1.16.4 to 1.22.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>
<blockquote>
<h2>v1.22.0</h2>
<h1>NumPy 1.22.0 Release Notes</h1>
<p>NumPy 1.22.0 is a big release featuring the work of 153 contributors
spread over 609 pull requests. There have been many improvements,
highlights are:</p>
<ul>
<li>Annotations of the main namespace are essentially complete. Upstream
is a moving target, so there will likely be further improvements,
but the major work is done. This is probably the most user visible
enhancement in this release.</li>
<li>A preliminary version of the proposed Array-API is provided. This is
a step in creating a standard collection of functions that can be
used across application such as CuPy and JAX.</li>
<li>NumPy now has a DLPack backend. DLPack provides a common interchange
format for array (tensor) data.</li>
<li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The
new methods provide a complete set of the methods commonly found in
the literature.</li>
<li>A new configurable allocator for use by downstream projects.</li>
</ul>
<p>These are in addition to the ongoing work to provide SIMD support for
commonly used functions, improvements to F2PY, and better documentation.</p>
<p>The Python versions supported in this release are 3.8-3.10, Python 3.7
has been dropped. Note that 32 bit wheels are only provided for Python
3.8 and 3.9 on Windows, all other wheels are 64 bits on account of
Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.
All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix
the occasional problems encountered by folks using truly huge arrays.</p>
<h2>Expired deprecations</h2>
<h3>Deprecated numeric style dtype strings have been removed</h3>
<p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,
and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>
<h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>
<p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that
users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both
deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with
the appropriate value for the <code>usemask</code> parameter.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>
<li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>
<li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>
<li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>
<li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>
<li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>
<li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>
<li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.16.4...v1.22.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.16.4&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.7.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.7.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.7.2</h2>
<h1>Release 2.7.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h2>TensorFlow 2.7.1</h2>
<h1>Release 2.7.1</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.7.2</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/dd7b8a3c1714d0052ce4b4a2fd8dcef927439a24""><code>dd7b8a3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56034"">#56034</a> from tensorflow-jenkins/relnotes-2.7.2-15779</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/1e7d6ea26dec19c8be5a67bdb4fa574a69f3da86""><code>1e7d6ea</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/50851350cbafeb82d8edc91cc9974b20db257bab""><code>5085135</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56069"">#56069</a> from tensorflow/mm-cp-52488e5072f6fe44411d70c6af09e...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/adafb45c7812dac1e84d4f23524106ba45d441c2""><code>adafb45</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56060"">#56060</a> from yongtang:curl-7.83.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/01cb1b8bb061c40a7b7b0f632235439ac7ba981e""><code>01cb1b8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56038"">#56038</a> from tensorflow-jenkins/version-numbers-2.7.2-4733</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8c90c2fa07e4376f032a425d863ef11ce357e3c5""><code>8c90c2f</code></a> Update version numbers to 2.7.2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/43f3cdc95f4dc6ea9f6979cdb82005b79103f591""><code>43f3cdc</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/98b0a48e852364f17f5f3b4b6525d8c3efd0e73d""><code>98b0a48</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/dfa5cf382323f0d3ffb4d96477d9d2fbd7d48abb""><code>dfa5cf3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56028"">#56028</a> from tensorflow/disable-tests-on-r2.7</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/501a65c3469bfeafb508c97db0654ad694460167""><code>501a65c</code></a> Disable timing out tests</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.7.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.7.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.6.4,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.6.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.6.4</h2>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h2>TensorFlow 2.6.3</h2>
<h1>Release 2.6.3</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.6.4</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29216"">CVE-2022-29216</a>)</li>
<li>Fixes a missing validation which causes <code>TensorSummaryV2</code> to crash (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29193"">CVE-2022-29193</a>)</li>
<li>Fixes a missing validation which crashes <code>QuantizeAndDequantizeV4Grad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29192"">CVE-2022-29192</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>DeleteSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29194"">CVE-2022-29194</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>GetSessionTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29191"">CVE-2022-29191</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>StagePeek</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29195"">CVE-2022-29195</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>UnsortedSegmentJoin</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29197"">CVE-2022-29197</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LoadAndRemapMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29199"">CVE-2022-29199</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>SparseTensorToCSRSparseMatrix</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29198"">CVE-2022-29198</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>LSTMBlockCell</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29200"">CVE-2022-29200</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29196"">CVE-2022-29196</a>)</li>
<li>Fixes a <code>CHECK</code> failure in depthwise ops via overflows (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes issues arising from undefined behavior stemming from users supplying invalid resource handles (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29207"">CVE-2022-29207</a>)</li>
<li>Fixes a segfault due to missing support for quantized types (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29205"">CVE-2022-29205</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>SparseTensorDenseAdd</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29206"">CVE-2022-29206</a>)</li>
<li>Fixes a missing validation which results in undefined behavior in <code>QuantizedConv2D</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29201"">CVE-2022-29201</a>)</li>
<li>Fixes an integer overflow in <code>SpaceToBatchND</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29203"">CVE-2022-29203</a>)</li>
<li>Fixes a segfault and OOB write due to incomplete validation in <code>EditDistance</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29208"">CVE-2022-29208</a>)</li>
<li>Fixes a missing validation which causes denial of service via <code>Conv3DBackpropFilterV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29204"">CVE-2022-29204</a>)</li>
<li>Fixes a denial of service in <code>tf.ragged.constant</code> due to lack of validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29202"">CVE-2022-29202</a>)</li>
<li>Fixes a segfault when <code>tf.histogram_fixed_width</code> is called with NaN values (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29211"">CVE-2022-29211</a>)</li>
<li>Fixes a core dump when loading TFLite models with quantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29212"">CVE-2022-29212</a>)</li>
<li>Fixes crashes stemming from incomplete validation in signal ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29213"">CVE-2022-29213</a>)</li>
<li>Fixes a type confusion leading to <code>CHECK</code>-failure based denial of service (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-29209"">CVE-2022-29209</a>)</li>
<li>Updates <code>curl</code> to <code>7.83.1</code> to handle (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-22576"">CVE-2022-22576</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27774"">CVE-2022-27774</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27775"">CVE-2022-27775</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27776"">CVE-2022-27776</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27778"">CVE-2022-27778</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27779"">CVE-2022-27779</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27780"">CVE-2022-27780</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27781"">CVE-2022-27781</a>, (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-27782"">CVE-2022-27782</a> and (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=VE-2022-30115"">CVE-2022-30115</a></li>
<li>Updates <code>zlib</code> to <code>1.2.12</code> after <code>1.2.11</code> was pulled due to <a href=""https://www.openwall.com/lists/oss-security/2022/03/28/1"">security issue</a></li>
</ul>
<h1>Release 2.8.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li>
<p><code>tf.lite</code>:</p>
<ul>
<li>Added TFLite builtin op support for the following TF ops:
<ul>
<li><code>tf.raw_ops.Bucketize</code> op on CPU.</li>
<li><code>tf.where</code> op for data types
<code>tf.int32</code>/<code>tf.uint32</code>/<code>tf.int8</code>/<code>tf.uint8</code>/<code>tf.int64</code>.</li>
<li><code>tf.random.normal</code> op for output data type <code>tf.float32</code> on CPU.</li>
<li><code>tf.random.uniform</code> op for output data type <code>tf.float32</code> on CPU.</li>
<li><code>tf.random.categorical</code> op for output data type <code>tf.int64</code> on CPU.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>tensorflow.experimental.tensorrt</code>:</p>
<ul>
<li><code>conversion_params</code> is now deprecated inside <code>TrtGraphConverterV2</code> in
favor of direct arguments: <code>max_workspace_size_bytes</code>, <code>precision_mode</code>,
<code>minimum_segment_size</code>, <code>maximum_cached_engines</code>, <code>use_calibration</code> and</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/33ed2b11cb8e879d86c371700e6573db1814a69e""><code>33ed2b1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56102"">#56102</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e1ec480caf1279bdca5f3fbc243d39c35bb03fce""><code>e1ec480</code></a> Fix build due to importlib-metadata/setuptools</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/63f211ca6b60dfa3bc8451c2cd0a2630e3598b9a""><code>63f211c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56033"">#56033</a> from tensorflow-jenkins/relnotes-2.6.4-6677</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/22b8fe48ce23ae1db23b2daa7b6f1e502d253d86""><code>22b8fe4</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/ec30684dc0922060eedd6887881e7e5bbd5da009""><code>ec30684</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56070"">#56070</a> from tensorflow/mm-cp-adafb45c781-on-r2.6</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/38774ed7562851ee3aae328ad0c238b0492ac641""><code>38774ed</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56060"">#56060</a> from yongtang:curl-7.83.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9ef160463d1b1fef4aa1e55b5cab30422fcc95cc""><code>9ef1604</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/56036"">#56036</a> from tensorflow-jenkins/version-numbers-2.6.4-9925</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a6526a3acba675a8425dfad323a634f537c4cbb9""><code>a6526a3</code></a> Update version numbers to 2.6.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/cb1a481ed144908b73c486db72b8315c89b1c0e5""><code>cb1a481</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4da550f0e6c1a02f045daca9d824a68b211f56ac""><code>4da550f</code></a> Insert release notes place-fill</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.6.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.6.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 9.0.1,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>In show_file, use os.remove to remove temporary images. CVE-2022-24303 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6010"">#6010</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>, <a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Restrict builtins within lambdas for ImageMath.eval. CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6009"">#6009</a> [radarhere]</li>
</ul>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@‚Äãkmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@‚ÄãDWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@‚Äãaclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@‚Äãm-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@‚Äãcmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@‚Äãilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@‚Äãinfmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.1 (2022-02-03)</h2>
<ul>
<li>
<p>In show_file, use os.remove to remove temporary images. CVE-2022-24303 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6010"">#6010</a>
[radarhere, hugovk]</p>
</li>
<li>
<p>Restrict builtins within lambdas for ImageMath.eval. CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/6009"">#6009</a>
[radarhere]</p>
</li>
</ul>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/6deac9e3a23caffbfdd75c00d3f0a1cd36cdbd5d""><code>6deac9e</code></a> 9.0.1 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/c04d812b902356b8c20ee2ab881e1d96f7d66b4b""><code>c04d812</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/4fabec36197735438c80c174d018498be606c46c""><code>4fabec3</code></a> Added release notes for 9.0.1</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/02affaa491df37117a7562e6ba6ac52c4c871195""><code>02affaa</code></a> Added delay after opening image with xdg-open</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ca0b58521881b95e47ea49d960d13d1c3dac823d""><code>ca0b585</code></a> Updated formatting</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/427221ef5f19157001bf8b1ad7cfe0b905ca8c26""><code>427221e</code></a> In show_file, use os.remove to remove temporary images</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/c930be0758ac02cf15a2b8d5409d50d443550581""><code>c930be0</code></a> Restrict builtins within lambdas for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/75b69dd239a4647032f67a80d9b444228af2b736""><code>75b69dd</code></a> Dont need to pin for GHA</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cd938a7744cb46e2ea525a0c3dd79aa08f98c150""><code>cd938a7</code></a> Autolink CWE numbers with sphinx-issues</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/2e9c461ca417083c43145a991bf9e1ec93237d89""><code>2e9c461</code></a> Add CVE IDs</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.3,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.3</h2>
<h1>Release 2.5.3</h1>
<p><strong>Note</strong>: This is the last release in the 2.5 series.</p>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
<li>Fixes a division by zero in <code>FractionalMaxPool</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21735"">CVE-2022-21735</a>)</li>
<li>Fixes a number of <code>CHECK</code>-fails when building invalid/overflowing tensor shapes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23569"">CVE-2022-23569</a>)</li>
<li>Fixes an undefined behavior in <code>SparseTensorSliceDataset</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21736"">CVE-2022-21736</a>)</li>
<li>Fixes an assertion failure based denial of service via faulty bin count operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21737"">CVE-2022-21737</a>)</li>
<li>Fixes a reference binding to null pointer in <code>QuantizedMaxPool</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21739"">CVE-2022-21739</a>)</li>
<li>Fixes an integer overflow leading to crash in <code>SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21738"">CVE-2022-21738</a>)</li>
<li>Fixes a heap overflow in <code>SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21740"">CVE-2022-21740</a>)</li>
<li>Fixes an FPE in <code>BiasAndClamp</code> in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23557"">CVE-2022-23557</a>)</li>
<li>Fixes an FPE in depthwise convolutions in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21741"">CVE-2022-21741</a>)</li>
<li>Fixes an integer overflow in TFLite array creation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23558"">CVE-2022-23558</a>)</li>
<li>Fixes an integer overflow in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23559"">CVE-2022-23559</a>)</li>
<li>Fixes a dangerous OOB write in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23561"">CVE-2022-23561</a>)</li>
<li>Fixes a vulnerability leading to read and write outside of bounds in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23560"">CVE-2022-23560</a>)</li>
<li>Fixes a set of vulnerabilities caused by using insecure temporary files (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23563"">CVE-2022-23563</a>)</li>
<li>Fixes an integer overflow in Range resulting in undefined behavior and OOM (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23562"">CVE-2022-23562</a>)</li>
<li>Fixes a vulnerability where missing validation causes <code>tf.sparse.split</code> to crash when <code>axis</code> is a tuple (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
<li>Fixes a <code>CHECK</code>-fail when decoding resource handles from proto (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23564"">CVE-2022-23564</a>)</li>
<li>Fixes a <code>CHECK</code>-fail with repeated <code>AttrDef</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23565"">CVE-2022-23565</a>)</li>
<li>Fixes a heap OOB write in Grappler (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23566"">CVE-2022-23566</a>)</li>
<li>Fixes a <code>CHECK</code>-fail when decoding invalid tensors from proto (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23571"">CVE-2022-23571</a>)</li>
<li>Fixes an unitialized variable access in <code>AssignOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23573"">CVE-2022-23573</a>)</li>
<li>Fixes an integer overflow in <code>OpLevelCostEstimator::CalculateTensorSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23575"">CVE-2022-23575</a>)</li>
<li>Fixes an integer overflow in <code>OpLevelCostEstimator::CalculateOutputSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23576"">CVE-2022-23576</a>)</li>
<li>Fixes a null dereference in <code>GetInitOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23577"">CVE-2022-23577</a>)</li>
<li>Fixes a memory leak when a graph node is invalid (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23578"">CVE-2022-23578</a>)</li>
<li>Fixes an abort caused by allocating a vector that is too large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23580"">CVE-2022-23580</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures during Grappler's <code>IsSimplifiableReshape</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23581"">CVE-2022-23581</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures during Grappler's <code>SafeToRemoveIdentity</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23579"">CVE-2022-23579</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures in <code>TensorByteSize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23582"">CVE-2022-23582</a>)</li>
<li>Fixes multiple <code>CHECK</code>-failures in binary ops due to type confusion (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23583"">CVE-2022-23583</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.3</h1>
<p>This releases introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a floating point division by 0 when executing convolution operators
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21725"">CVE-2022-21725</a>)</li>
<li>Fixes a heap OOB read in shape inference for <code>ReverseSequence</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21728"">CVE-2022-21728</a>)</li>
<li>Fixes a heap OOB access in <code>Dequantize</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21726"">CVE-2022-21726</a>)</li>
<li>Fixes an integer overflow in shape inference for <code>Dequantize</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21727"">CVE-2022-21727</a>)</li>
<li>Fixes a heap OOB access in <code>FractionalAvgPoolGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21730"">CVE-2022-21730</a>)</li>
<li>Fixes an overflow and divide by zero in <code>UnravelIndex</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21729"">CVE-2022-21729</a>)</li>
<li>Fixes a type confusion in shape inference for <code>ConcatV2</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21731"">CVE-2022-21731</a>)</li>
<li>Fixes an OOM in <code>ThreadPoolHandle</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21732"">CVE-2022-21732</a>)</li>
<li>Fixes an OOM due to integer overflow in <code>StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21733"">CVE-2022-21733</a>)</li>
<li>Fixes more issues caused by incomplete validation in boosted trees code
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes an integer overflows in most sparse component-wise ops
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23567"">CVE-2022-23567</a>)</li>
<li>Fixes an integer overflows in <code>AddManySparseToTensorsMap</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23568"">CVE-2022-23568</a>)</li>
<li>Fixes a number of <code>CHECK</code>-failures in <code>MapStage</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21734"">CVE-2022-21734</a>)</li>
<li>Fixes a division by zero in <code>FractionalMaxPool</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21735"">CVE-2022-21735</a>)</li>
<li>Fixes a number of <code>CHECK</code>-fails when building invalid/overflowing tensor
shapes
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23569"">CVE-2022-23569</a>)</li>
<li>Fixes an undefined behavior in <code>SparseTensorSliceDataset</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21736"">CVE-2022-21736</a>)</li>
<li>Fixes an assertion failure based denial of service via faulty bin count
operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21737"">CVE-2022-21737</a>)</li>
<li>Fixes a reference binding to null pointer in <code>QuantizedMaxPool</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21739"">CVE-2022-21739</a>)</li>
<li>Fixes an integer overflow leading to crash in <code>SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21738"">CVE-2022-21738</a>)</li>
<li>Fixes a heap overflow in <code>SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21740"">CVE-2022-21740</a>)</li>
<li>Fixes an FPE in <code>BiasAndClamp</code> in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23557"">CVE-2022-23557</a>)</li>
<li>Fixes an FPE in depthwise convolutions in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-21741"">CVE-2022-21741</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/959e9b2a0c06df945f9fb66bd367af8832ca0d28""><code>959e9b2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54213"">#54213</a> from tensorflow/fix-sanity-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d05fcbc589a2d45c0d83a3ef384c7b982dc75d4a""><code>d05fcbc</code></a> Fix sanity build</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f2526a06250a1c70c882dff17b1f007642f5c7b8""><code>f2526a0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54205"">#54205</a> from tensorflow/disable-flaky-tests-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a5f94dff540d6351251eda7e380cb1c217b667df""><code>a5f94df</code></a> Disable flaky test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7babe52a746c9b42342b80622fe9d605fe681fac""><code>7babe52</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54201"">#54201</a> from tensorflow/cherrypick-510ae18200d0a4fad797c0bf...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e5d378c4557c84d0326af0bc61a6ec0d8df5856""><code>0e5d378</code></a> Set Env Variable to override Setuptools new behavior</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/fdd419543d8428bf924b765d837722b5a16cb1a8""><code>fdd4195</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54176"">#54176</a> from tensorflow-jenkins/relnotes-2.5.3-6805</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4083165848a741912bd37c6d75466e5508b5376b""><code>4083165</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/a2bb7f160b8b7b3f955aecb7da3bdb12d79363ba""><code>a2bb7f1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/54185"">#54185</a> from tensorflow/cherrypick-d437dec4d549fc30f9b85c75...</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5777ea37552e85c8a1f25c25f8f533cb1299d682""><code>5777ea3</code></a> Update third_party/icu/workspace.bzl</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
libtiff,"when i am trying to install libtiff .it is showing this error
 Building wheel for libtiff (setup.py) ... error"
Bump pillow from 6.0.0 to 9.0.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@‚Äãkmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@‚ÄãDWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@‚Äãaclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@‚Äãm-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@‚Äãcmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@‚Äãilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@‚Äãinfmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix pushes_fd method signature <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5833"">#5833</a> [<a href=""https://github.com/hoodmane""><code>@‚Äãhoodmane</code></a>]</li>
<li>Add support for pickling TrueType fonts <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5826"">#5826</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Only prefer command line tools SDK on macOS over default MacOSX SDK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5828"">#5828</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix compilation on 64-bit Termux <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5793"">#5793</a> [<a href=""https://github.com/landfillbaby""><code>@‚Äãlandfillbaby</code></a>]</li>
<li>Replace 'setup.py sdist' with '-m build --sdist' <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5785"">#5785</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Use declarative package configuration <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5784"">#5784</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Use title for display in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5788"">#5788</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix for PyQt6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5775"">#5775</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
<li>
<p>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a>
[m-shinder, radarhere]</p>
</li>
<li>
<p>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a>
[cmbruns, radarhere]</p>
</li>
<li>
<p>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/82541b6dec8452cb612067fcebba1c5a1a2bfdc8""><code>82541b6</code></a> 9.0.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cae5ac495badd7c7ecfad8223a08f55f5d2eaacb""><code>cae5ac4</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5924"">#5924</a> from radarhere/cves</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ed4cf7813777ad8478cac46f448bc45416a2a99e""><code>ed4cf78</code></a> CVEs TBD</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d7f60d1d5a746eb01d4cb3c7fb05b6593f46b0f5""><code>d7f60d1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> from radarhere/imagemath_eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8531b01d6cdf0b70f256f93092caa2a5d91afc11""><code>8531b01</code></a> Restrict builtins for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1efb1d9fabd1dfdbf7982035eca0dae7306abef1""><code>1efb1d9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5922"">#5922</a> from radarhere/releasenotes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f6c78713a491764dfac576f6c42127755f2c62b3""><code>f6c7871</code></a> Added release notes for <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> and <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a></li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/032d2dc3658f94718109068ac70799313e440754""><code>032d2dc</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/baae9ec4b67c68e3adaf1208cf54e8de5e38a6fd""><code>baae9ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> from radarhere/jpeg_eoi</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1059eb537639925c96d3245dcd73c106d4266c83""><code>1059eb5</code></a> If appended EOI did not work, do not keep trying</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.2</h2>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory during shape inference in <code>Cudnn*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by <code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to <code>nullptr</code> in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">CVE-2021-41207</a>)</li>
<li>Fixes FPE issues in convolutions with zero size filters (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
<li>Fixes a segfault produced while copying constant resource tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41204"">CVE-2021-41204</a>)</li>
<li>Fixes a vulnerability caused by unitialized access in <code>EinsumHelper::ParseEquation</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41201"">CVE-2021-41201</a>)</li>
<li>Fixes several vulnerabilities and segfaults caused by missing validation during checkpoint loading (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41203"">CVE-2021-41203</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.range</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41202"">CVE-2021-41202</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.image.resize</code> when size is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41199"">CVE-2021-41199</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.tile</code> when tiling tensor is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41198"">CVE-2021-41198</a>)</li>
<li>Fixes a vulnerability produced due to incomplete validation in <code>tf.summary.create_file_writer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41200"">CVE-2021-41200</a>)</li>
<li>Fixes multiple crashes due to overflow and <code>CHECK</code>-fail in ops with large tensor shapes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes a crash in <code>max_pool3d</code> when size argument is 0 or negative (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41196"">CVE-2021-41196</a>)</li>
<li>Fixes a crash in <code>tf.math.segment_*</code> operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41195"">CVE-2021-41195</a>)</li>
<li>Updates <code>curl</code> to <code>7.78.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22922"">CVE-2021-22922</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22923"">CVE-2021-22923</a>,  <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22924"">CVE-2021-22924</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22925"">CVE-2021-22925</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22926"">CVE-2021-22926</a>.</li>
</ul>
<h2>TensorFlow 2.5.1</h2>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory
during shape inference in <code>Cudnn*</code> ops
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by
<code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix
multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to
<code>nullptr</code> in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code>
ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> ([CVE-2021-41207]
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207</a>))</li>
<li>Fixes FPE issues in convolutions with zero size filters
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple
TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/957590ea15cc03ee2e00fc61934647d54836676f""><code>957590e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52873"">#52873</a> from tensorflow-jenkins/relnotes-2.5.2-20787</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2e1d16d7aac34983e4ff0d55f434e4d07fea7bce""><code>2e1d16d</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2fa6dd95659985a9ee9429d146c29c27f12e342c""><code>2fa6dd9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52877"">#52877</a> from tensorflow-jenkins/version-numbers-2.5.2-192</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/480748994b151d28818cdfc659f4332bce8a97b2""><code>4807489</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52881"">#52881</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d398bdfd5d2190a3274141416c54f3e7207e96f3""><code>d398bdf</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/857ad5ef1eb23bbeb378b1f3c3cbe6f38286c2d0""><code>857ad5e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52878"">#52878</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6c2a215be0afd10008046bd072daaf81228a19a1""><code>6c2a215</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f5c57d495753bbc166abc928430ea808aa8aa6b3""><code>f5c57d4</code></a> Update version numbers to 2.5.2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e51f9495418eb373074a652b5bf4bba1c41aa132""><code>e51f949</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2620d2cd5c4e03df6d02ae18fda2a9fdc2466738""><code>2620d2c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52863"">#52863</a> from tensorflow/fix-build-3-on-r2.5</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump opencv-python from 4.1.0.25 to 4.2.0.32,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.2.0.32.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.2.0.32</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.2.0.</p>
<p>Changes:</p>
<ul>
<li>macOS environment updated from xcode8.3 to xcode 9.4</li>
<li>macOS uses now Qt 5 instead of Qt 4</li>
<li>Nasm version updated to Docker containers</li>
<li>multibuild updated</li>
</ul>
<p>Fixes:</p>
<ul>
<li>don't use deprecated brew tap-pin, instead refer to the full package name when installing <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/267"">#267</a></li>
<li>replace get_config_var() with get_config_vars() in setup.py <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/274"">#274</a></li>
<li>add workaround for DLL errors in Windows Server <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/264"">#264</a></li>
</ul>
<h2>4.1.2.30</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.2.</p>
<p>Changes:</p>
<ul>
<li>Python 3.8 builds added to the build matrix</li>
<li>Support for Python 3.4 builds dropped (Python 3.4 is in EOL)</li>
<li>multibuild updated</li>
<li>minor build logic changes</li>
<li>Docker images rebuilt</li>
</ul>
<p>Notes:</p>
<p>Please note that Python 2.7 enters into EOL phase in January 2020. <code>opencv-python</code> Python 2.7 wheels won't be provided after that.</p>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.2.0.32)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump opencv-python from 4.1.0.25 to 4.1.1.26,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.1.1.26.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<ul>
<li>FFmpeg has been compiled with https support on Linux builds <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/229"">#229</a></li>
<li>CI build logic related changes <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/197"">#197</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/227"">#227</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/228"">#228</a></li>
<li>Custom libjepg-turbo removed because it's provided by OpenCV <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/231"">#231</a></li>
<li>64-bit Qt builds are now smaller <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/236"">#236</a></li>
<li>Custom builds should be now rather easy to do locally <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/235"">#235</a>:
<ol>
<li>Clone this repository</li>
<li>Optional: set up ENABLE_CONTRIB and ENABLE_HEADLESS environment variables to 1 if needed</li>
<li>Optional: add additional Cmake arguments to CMAKE_ARGS environment variable</li>
<li>Run <code>python setup.py bdist_wheel</code></li>
</ol>
</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.1.1.26)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.3.2,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.3.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.3.2</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html</a></p>
<h2>Security</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
</ul>
<h2>Python 3.10 wheels</h2>
<ul>
<li>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</li>
</ul>
<h2>Fixed regressions</h2>
<ul>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a> [<a href=""https://github.com/t-vi""><code>@‚Äãt-vi</code></a>]</li>
</ul>
<h2>8.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Use snprintf instead of sprintf <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a> [<a href=""https://github.com/kmilos""><code>@‚Äãkmilos</code></a>]</li>
<li>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a> [<a href=""https://github.com/newpanjing""><code>@‚Äãnewpanjing</code></a>]</li>
<li>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a> [<a href=""https://github.com/gofr""><code>@‚Äãgofr</code></a>]</li>
<li>Do not use background or transparency index for new color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5564"">#5564</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Simplified code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5315"">#5315</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.3.2 (2021-09-02)</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
<li>
<p>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</p>
</li>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1 (2021-07-06)</h2>
<ul>
<li>
<p>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a>
[radarhere]</p>
</li>
<li>
<p>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a>
[radarhere]</p>
</li>
<li>
<p>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a>
[t-vi, radarhere]</p>
</li>
</ul>
<h2>8.3.0 (2021-07-01)</h2>
<ul>
<li>
<p>Use snprintf instead of sprintf. CVE-2021-34552 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a>
[radarhere]</p>
</li>
<li>
<p>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a>
[kmilos]</p>
</li>
<li>
<p>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a>
[baletu, radarhere, newpanjing, hugovk]</p>
</li>
<li>
<p>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a>
[gofr, radarhere]</p>
</li>
<li>
<p>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8013f130a5077b238a4346b73e149432b180a8ea""><code>8013f13</code></a> 8.3.2 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/23c7ca82f09df6ba1047d2d96714eb825f0d7948""><code>23c7ca8</code></a> Update CHANGES.rst</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8450366be331762ae327036e3c6658c517b05638""><code>8450366</code></a> Update release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/a0afe89990f5ba40a019afc2f22e1b656f8cfd03""><code>a0afe89</code></a> Update test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/9e08eb8f78fdfd2f476e1b20b7cf38683754866b""><code>9e08eb8</code></a> Raise ValueError if color specifier is too long</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bd5cf7db87c6abf7c3510a50170851af5538249f""><code>bd5cf7d</code></a> FLI tests for Oss-fuzz crash.</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/94a0cf1b14f09626c7403af83fa9fef0dfc9bb47""><code>94a0cf1</code></a> Fix 6-byte OOB read in FliDecode</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cece64f4be10ab28b12a83a3555af579dad343a5""><code>cece64f</code></a> Add 8.3.2 (2021-09-02) [CI skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e42238637651f191c2fc6e3f4024348c126e0ccc""><code>e422386</code></a> Add release notes for Pillow 8.3.2</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/08dcbb873217874eee0830fc5aaa1f231c5af4fa""><code>08dcbb8</code></a> Pillow 8.3.2 supports Python 3.10 [ci skip]</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.3.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.3.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 2.5.1,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.1</h2>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642"">CVE-2021-37642</a>)</li>
<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641"">CVE-2021-37641</a>)</li>
<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644"">CVE-2021-37644</a>)</li>
<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643"">CVE-2021-37643</a>)</li>
<li>Fixes an integer overflow due to conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645"">CVE-2021-37645</a>)</li>
<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646"">CVE-2021-37646</a>)</li>
<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647"">CVE-2021-37647</a>)</li>
<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648"">CVE-2021-37648</a>)</li>
<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649"">CVE-2021-37649</a>)</li>
<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650"">CVE-2021-37650</a>)</li>
<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651"">CVE-2021-37651</a>)</li>
<li>Fixes a use after free in boosted trees creation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652"">CVE-2021-37652</a>)</li>
<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653"">CVE-2021-37653</a>)</li>
<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654"">CVE-2021-37654</a>)</li>
<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655"">CVE-2021-37655</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656"">CVE-2021-37656</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657"">CVE-2021-37657</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658"">CVE-2021-37658</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659"">CVE-2021-37659</a>)</li>
<li>Fixes a division by 0 in inplace operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660"">CVE-2021-37660</a>)</li>
<li>Fixes a crash caused by integer conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661"">CVE-2021-37661</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662"">CVE-2021-37662</a>)</li>
<li>Fixes a heap OOB in boosted trees (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664"">CVE-2021-37664</a>)</li>
<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663"">CVE-2021-37663</a>)</li>
<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665"">CVE-2021-37665</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666"">CVE-2021-37666</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667"">CVE-2021-37667</a>)</li>
<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668"">CVE-2021-37668</a>)</li>
<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669"">CVE-2021-37669</a>)</li>
<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670"">CVE-2021-37670</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671"">CVE-2021-37671</a>)</li>
<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672"">CVE-2021-37672</a>)</li>
<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673"">CVE-2021-37673</a>)</li>
<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674"">CVE-2021-37674</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676"">CVE-2021-37676</a>)</li>
<li>Fixes a division by 0 in most convolution operators (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675"">CVE-2021-37675</a>)</li>
<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677"">CVE-2021-37677</a>)</li>
<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678"">CVE-2021-37678</a>)</li>
<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679"">CVE-2021-37679</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations
restoring tensors
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642"">CVE-2021-37642</a>)</li>
<li>Fixes a heap OOB in <code>RaggedGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641"">CVE-2021-37641</a>)</li>
<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644"">CVE-2021-37644</a>)</li>
<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643"">CVE-2021-37643</a>)</li>
<li>Fixes an integer overflow due to conversion to unsigned
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645"">CVE-2021-37645</a>)</li>
<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646"">CVE-2021-37646</a>)</li>
<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647"">CVE-2021-37647</a>)</li>
<li>Fixes an incorrect validation of <code>SaveV2</code> inputs
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648"">CVE-2021-37648</a>)</li>
<li>Fixes a null pointer dereference in <code>UncompressElement</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649"">CVE-2021-37649</a>)</li>
<li>Fixes a segfault and a heap buffer overflow in
<code>{Experimental,}DatasetToTFRecord</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650"">CVE-2021-37650</a>)</li>
<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651"">CVE-2021-37651</a>)</li>
<li>Fixes a use after free in boosted trees creation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652"">CVE-2021-37652</a>)</li>
<li>Fixes a division by 0 in <code>ResourceGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653"">CVE-2021-37653</a>)</li>
<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654"">CVE-2021-37654</a>)</li>
<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655"">CVE-2021-37655</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to nullptr in
<code>RaggedTensorToSparse</code></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1""><code>8222c1c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381"">#51381</a> from tensorflow/mm-fix-r2.5-build</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d""><code>d584260</code></a> Disable broken/flaky test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7""><code>f6c6ce3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367"">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76""><code>3ca7812</code></a> Update version numbers to 2.5.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6""><code>4fdf683</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361"">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a""><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d""><code>bee1dc4</code></a> Update release notes for the new patch release</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84""><code>47beb4c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597"">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d""><code>6f39597</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383"">#49383</a> from ashahab/abin-load-segfault-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3""><code>0539b34</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979"">#48979</a> from liufengdb/r2.5-cherrypick</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.2.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.2.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.2.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Security fixes for 8.2.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Move getxmp() to JpegImageFile <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5376"">#5376</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a> [<a href=""https://github.com/UrielMaD""><code>@‚ÄãUrielMaD</code></a>]</li>
<li>Compile LibTIFF with CMake on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5359"">#5359</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a> [<a href=""https://github.com/latosha-maltba""><code>@‚Äãlatosha-maltba</code></a>]</li>
<li>Tiff crash fixes in TiffDecode.c <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5372"">#5372</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>Remove redundant check (addition to <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>) <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5366"">#5366</a> [<a href=""https://github.com/kkopachev""><code>@‚Äãkkopachev</code></a>]</li>
<li>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a> [<a href=""https://github.com/raygard""><code>@‚Äãraygard</code></a>]</li>
<li>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a> [<a href=""https://github.com/elejke""><code>@‚Äãelejke</code></a>]</li>
<li>Only import numpy when necessary <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5323"">#5323</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Add support for reading TIFFs with PlanarConfiguration=2  <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>More OSS-Fuzz support <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5328"">#5328</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Use quantization method attributes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5353"">#5353</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Removed build_distance_tables return value <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5363"">#5363</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Improved efficiency when creating GIF disposal images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5326"">#5326</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Replaced tiff_deflate with tiff_adobe_deflate compression when saving TIFF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5343"">#5343</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Save ICC profile from TIFF encoderinfo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5321"">#5321</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Moved RGB fix inside ImageQt class <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5268"">#5268</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix -Wformat error in TiffDecode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5305"">#5305</a> [<a href=""https://github.com/lukegb""><code>@‚Äãlukegb</code></a>]</li>
<li>Allow alpha_composite destination to be negative <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5313"">#5313</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Ensure file is closed if it is opened by ImageQt.ImageQt <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5260"">#5260</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added ImageDraw rounded_rectangle method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5208"">#5208</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added IPythonViewer <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5289"">#5289</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Only draw each rectangle outline pixel once <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5183"">#5183</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use mmap instead of built-in Win32 mapper <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5224"">#5224</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Handle PCX images with an odd stride <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5214"">#5214</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Only read different sizes for &quot;Large Thumbnail&quot; MPO frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5168"">#5168</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>Updated harfbuzz to 2.8.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5334"">#5334</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<h2>Deprecations</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.2.0 (2021-04-01)</h2>
<ul>
<li>
<p>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a>
[UrielMaD, radarhere]</p>
</li>
<li>
<p>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a>
[latosha-maltba, radarhere]</p>
</li>
<li>
<p>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a>
[zewt, radarhere]</p>
</li>
<li>
<p>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a>
[raygard]</p>
</li>
<li>
<p>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a>
[radarhere]</p>
</li>
<li>
<p>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a>
[radarhere]</p>
</li>
<li>
<p>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a>
[radarhere]</p>
</li>
<li>
<p>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a>
[elejke, radarhere]</p>
</li>
<li>
<p>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a>
[radarhere]</p>
</li>
<li>
<p>Add support for reading TIFFs with PlanarConfiguration=2 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>
[kkopachev, wiredfool, nulano]</p>
</li>
<li>
<p>Deprecated categories <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5351"">#5351</a>
[radarhere]</p>
</li>
<li>
<p>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a>
[nulano]</p>
</li>
<li>
<p>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a>
[nulano]</p>
</li>
<li>
<p>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a>
[radarhere]</p>
</li>
<li>
<p>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a>
[radarhere]</p>
</li>
<li>
<p>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a>
[radarhere, kkopachev]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e0e353c0ef7516979a9aedce3792596649ce4433""><code>e0e353c</code></a> 8.2.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ee635befc6497f1c6c4fdb58c232e62d922ec8b7""><code>ee635be</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> from hugovk/security-and-release-notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/694c84f88f4299319bac49b20bd9baae82ca41b8""><code>694c84f</code></a> Fix typo [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8febdad8dd51ad5c75a1db78492973588c7cbf6b""><code>8febdad</code></a> Review, typos and lint</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/fea419665b75f11910e44cfe6f89622fda63e78b""><code>fea4196</code></a> Reorder, roughly alphabetic</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/496245aa4365d0827390bd0b6fbd11287453b3a1""><code>496245a</code></a> Fix BLP DOS -- CVE-2021-28678</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/22e9bee4ef225c0edbb9323f94c26cee0c623497""><code>22e9bee</code></a> Fix DOS in PSDImagePlugin -- CVE-2021-28675</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ba65f0b08ee8b93195c3f3277820771f5b62aa52""><code>ba65f0b</code></a> Fix Memory DOS in ImageFont</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bb6c11fb889e6c11b0ee122b828132ee763b5856""><code>bb6c11f</code></a> Fix FLI DOS -- CVE-2021-28676</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/5a5e6db0abf4e7a638fb1b3408c4e495a096cb92""><code>5a5e6db</code></a> Fix EPS DOS on _open -- CVE-2021-28677</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.2.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump pillow from 6.0.0 to 8.1.1,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.1.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.1.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.1.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.1.1.html</a></p>
<h2>8.1.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.1.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.1.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Fix TIFF OOB Write error <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5175"">#5175</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix for Buffer Read Overrun in PCX Decoding <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5174"">#5174</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix for SGI Decode buffer overrun <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5173"">#5173</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix OOB Read when saving GIF of xsize=1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5149"">#5149</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>Add support for PySide6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5161"">#5161</a> [<a href=""https://github.com/hugovk""><code>@‚Äãhugovk</code></a>]</li>
<li>Moved QApplication into one test <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5167"">#5167</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use disposal settings from previous frame in APNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5126"">#5126</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Revert &quot;skip wheels on 3.10-dev due to wheel#354&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5163"">#5163</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Better _binary module use <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5156"">#5156</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Added exception explaining that <em>repr_png</em> saves to PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5139"">#5139</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Use previous disposal method in GIF load_end <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5125"">#5125</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Do not catch a ValueError only to raise another <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5090"">#5090</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Allow putpalette to accept 1024 integers to include alpha values <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5089"">#5089</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix OOB Read when writing TIFF with custom Metadata <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5148"">#5148</a> [<a href=""https://github.com/wiredfool""><code>@‚Äãwiredfool</code></a>]</li>
<li>Removed unused variable <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5140"">#5140</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix dereferencing of potential null pointers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5111"">#5111</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Fixed warnings assigning to &quot;unsigned char *&quot; from &quot;char *&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5127"">#5127</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Add append_images support for ICO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4568"">#4568</a> [<a href=""https://github.com/ziplantil""><code>@‚Äãziplantil</code></a>]</li>
<li>Fixed comparison warnings <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5122"">#5122</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Block TIFFTAG_SUBIFD <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5120"">#5120</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Fix dereferencing potential null pointer <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5108"">#5108</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Replaced PyErr_NoMemory with ImagingError_MemoryError <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5113"">#5113</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Remove duplicate code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5109"">#5109</a> [<a href=""https://github.com/cgohlke""><code>@‚Äãcgohlke</code></a>]</li>
<li>Moved warning to end of execution <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4965"">#4965</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Removed unused fromstring and tostring C methods <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5026"">#5026</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>init() if one of the formats is unrecognised <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5037"">#5037</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>Updated libtiff to 4.2.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5153"">#5153</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated openjpeg to 2.4.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5151"">#5151</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated harfbuzz to 2.7.4 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5138"">#5138</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated harfbuzz to 2.7.3 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5128"">#5128</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated libraqm to 0.7.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5070"">#5070</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated libimagequant to 2.13.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5065"">#5065</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Update FriBiDi to 1.0.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5064"">#5064</a> [<a href=""https://github.com/nulano""><code>@‚Äãnulano</code></a>]</li>
<li>Updated libraqm to 0.7.1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5063"">#5063</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
<li>Updated libjpeg-turbo to 2.0.6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5044"">#5044</a> [<a href=""https://github.com/radarhere""><code>@‚Äãradarhere</code></a>]</li>
</ul>
<h2>Deprecations</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.1.1 (2021-03-01)</h2>
<ul>
<li>
<p>Use more specific regex chars to prevent ReDoS. CVE-2021-25292
[hugovk]</p>
</li>
<li>
<p>Fix OOB Read in TiffDecode.c, and check the tile validity before reading. CVE-2021-25291
[wiredfool]</p>
</li>
<li>
<p>Fix negative size read in TiffDecode.c. CVE-2021-25290
[wiredfool]</p>
</li>
<li>
<p>Fix OOB read in SgiRleDecode.c. CVE-2021-25293
[wiredfool]</p>
</li>
<li>
<p>Incorrect error code checking in TiffDecode.c. CVE-2021-25289
[wiredfool]</p>
</li>
<li>
<p>PyModule_AddObject fix for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5194"">#5194</a>
[radarhere]</p>
</li>
</ul>
<h2>8.1.0 (2021-01-02)</h2>
<ul>
<li>
<p>Fix TIFF OOB Write error. CVE-2020-35654 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5175"">#5175</a>
[wiredfool]</p>
</li>
<li>
<p>Fix for Read Overflow in PCX Decoding. CVE-2020-35653 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5174"">#5174</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Fix for SGI Decode buffer overrun. CVE-2020-35655 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5173"">#5173</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Fix OOB Read when saving GIF of xsize=1 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5149"">#5149</a>
[wiredfool]</p>
</li>
<li>
<p>Makefile updates <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5159"">#5159</a>
[wiredfool, radarhere]</p>
</li>
<li>
<p>Add support for PySide6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5161"">#5161</a>
[hugovk]</p>
</li>
<li>
<p>Use disposal settings from previous frame in APNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5126"">#5126</a>
[radarhere]</p>
</li>
<li>
<p>Added exception explaining that <em>repr_png</em> saves to PNG <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5139"">#5139</a>
[radarhere]</p>
</li>
<li>
<p>Use previous disposal method in GIF load_end <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5125"">#5125</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/741d8744a54bedbc49f16922c61a06fcb3681f53""><code>741d874</code></a> 8.1.1 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/179cd1c8f94aabc47e9e522e01683ea9aadbd3a5""><code>179cd1c</code></a> Added 8.1.1 release notes to index</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/7d296653da045e18b379c991797f933e054a7476""><code>7d29665</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d25036fca7c8658b698492088361453bb20073e2""><code>d25036f</code></a> Credits</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/973a4c333ab6d603e82f6eb2aa6f39d1cfcecccb""><code>973a4c3</code></a> Release notes for 8.1.1</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/521dab94c7ab72b037bd9a83e9663401e0fd2cee""><code>521dab9</code></a> Use more specific regex chars to prevent ReDoS</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8b8076bdcb3815be0ef0d279651d8d1342b8ea61""><code>8b8076b</code></a> Fix for CVE-2021-25291</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e25be1e33dc526bfd1094bc778a54d8e29bf66c9""><code>e25be1e</code></a> Fix negative size read in TiffDecode.c</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f891baa604636cd2506a9360d170bc2cf4963cc5""><code>f891baa</code></a> Fix OOB read in SgiRleDecode.c</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cbfdde7b1f2295059a20a539ee9960f0bec7b299""><code>cbfdde7</code></a> Incorrect error code checking in TiffDecode.c</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.1.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.1.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
resource exhausted at gtx1070,"dear friendÔºÅ
when I use your code ÔºåI have a problem about ‚Äú resource exhausted‚Äù
could you tell me what type about your GPU and my types is GTX1070Ôºà8GÔºâ
and could you give me  some advice at not change my GPU's type.

thanks! hope get your help!"
Bump tensorflow-gpu from 1.4.0 to 2.3.1,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.3.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.3.1</h2>
<h1>Release 2.3.1</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes several vulnerabilities in <code>RaggedCountSparseOutput</code> and <code>SparseCountSparseOutput</code> operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196"">CVE-2020-15196</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197"">CVE-2020-15197</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198"">CVE-2020-15198</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199"">CVE-2020-15199</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200"">CVE-2020-15200</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201"">CVE-2020-15201</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Fixes several vulnerabilities in TFLite implementation of segment sum (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212"">CVE-2020-15212</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213"">CVE-2020-15213</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214"">CVE-2020-15214</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes deprecated usage of <code>collections</code> API</li>
<li>Removes <code>scipy</code> dependency from <code>setup.py</code> since TensorFlow does not need it to install the pip package</li>
</ul>
<h2>TensorFlow 2.3.0</h2>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:
<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot"">snapshot</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/service"">tf.data service</a>.</li>
</ul>
</li>
</ul>
<p>In addition checkout the detailed <a href=""https://www.tensorflow.org/guide/data_performance_analysis"">guide</a> for analyzing input pipeline performance with TF Profiler.</p>
<ul>
<li>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy""><code>tf.distribute.TPUStrategy</code></a> is now a stable API and no longer considered experimental for TensorFlow. (earlier <code>tf.distribute.experimental.TPUStrategy</code>).</p>
</li>
<li>
<p><a href=""https://www.tensorflow.org/guide/profiler"">TF Profiler</a> introduces two new tools: a memory profiler to visualize your model‚Äôs memory usage over time and a <a href=""https://www.tensorflow.org/guide/profiler#events"">python tracer</a> which allows you to trace python function calls in your model. Usability improvements include better diagnostic messages and <a href=""https://tensorflow.org/guide/profiler#collect_performance_data"">profile options</a> to customize the host and device trace verbosity level.</p>
</li>
<li>
<p>Introduces experimental support for Keras Preprocessing Layers API (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing?version=nightly""><code>tf.keras.layers.experimental.preprocessing.*</code></a>) to handle data preprocessing operations, with support for composite tensor inputs. Please see below for additional details on these layers.</p>
</li>
<li>
<p>TFLite now properly supports dynamic shapes during conversion and inference. We‚Äôve also added opt-in support on Android and iOS for <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack"">XNNPACK</a>, a highly optimized set of CPU kernels, as well as opt-in support for <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md#running-quantized-models-experimental"">executing quantized models on the GPU</a>.</p>
</li>
<li>
<p>Libtensorflow packages are available in GCS starting this release. We have also started to <a href=""https://github.com/tensorflow/tensorflow#official-builds"">release a nightly version of these packages</a>.</p>
</li>
<li>
<p>The experimental Python API <a href=""https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info""><code>tf.debugging.experimental.enable_dump_debug_info()</code></a> now allows you to instrument a TensorFlow program and dump debugging information to a directory on the file system. The directory can be read and visualized by a new interactive dashboard in TensorBoard 2.3 called <a href=""https://www.tensorflow.org/tensorboard/debugger_v2"">Debugger V2</a>, which reveals the details of the TensorFlow program including graph structures, history of op executions at the Python (eager) and intra-graph levels, the runtime dtype, shape, and numerical composistion of tensors, as well as their code locations.</p>
</li>
</ul>
<h2>Breaking Changes</h2>
<ul>
<li>Increases the <strong>minimum bazel version</strong> required to build TF to <strong>3.1.0</strong>.</li>
<li><code>tf.data</code>
<ul>
<li>Makes the following (breaking) changes to the <code>tf.data</code>.</li>
<li>C++ API: - <code>IteratorBase::RestoreInternal</code>, <code>IteratorBase::SaveInternal</code>, and <code>DatasetBase::CheckExternalState</code> become pure-virtual and subclasses are now expected to provide an implementation.</li>
<li>The deprecated <code>DatasetBase::IsStateful</code> method is removed in favor of <code>DatasetBase::CheckExternalState</code>.</li>
<li>Deprecated overrides of <code>DatasetBase::MakeIterator</code> and <code>MakeIteratorFromInputElement</code> are removed.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.3.1</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes several vulnerabilities in <code>RaggedCountSparseOutput</code> and
<code>SparseCountSparseOutput</code> operations
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15196"">CVE-2020-15196</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15197"">CVE-2020-15197</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15198"">CVE-2020-15198</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15199"">CVE-2020-15199</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15200"">CVE-2020-15200</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15201"">CVE-2020-15201</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Fixes several vulnerabilities in TFLite implementation of segment sum
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15212"">CVE-2020-15212</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15213"">CVE-2020-15213</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15214"">CVE-2020-15214</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes deprecated usage of <code>collections</code> API</li>
<li>Removes <code>scipy</code> dependency from <code>setup.py</code> since TensorFlow does not need it
to install the pip package</li>
</ul>
<h1>Release 2.2.1</h1>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/fcc4b966f1265f466e82617020af93670141b009""><code>fcc4b96</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43446"">#43446</a> from tensorflow-jenkins/version-numbers-2.3.1-16251</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/4cf223069a94c78b208e6c829d5f938a0fae7d07""><code>4cf2230</code></a> Update version numbers to 2.3.1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/eee82247288e52e9b8a5c2badeb65f871b4da4c4""><code>eee8224</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43441"">#43441</a> from tensorflow-jenkins/relnotes-2.3.1-24672</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0d41b1dfc97500e1177cb718a0b14b04914df661""><code>0d41b1d</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d99bd631ea9b67ffc39c22b35fbf7deca77ad1f7""><code>d99bd63</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d71d3ce2520587b752e5d27b2d4a4ba8720e4bd5""><code>d71d3ce</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43414"">#43414</a> from tensorflow/mihaimaruseac-patch-1-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9c91596d4d24bc07b6d36ae48581a2e7b2584edf""><code>9c91596</code></a> Fix missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f9f12f61867159120ce6eb08fdbd225d454232b5""><code>f9f12f6</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43391"">#43391</a> from tensorflow/mihaimaruseac-patch-4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/3ed271b0b05b4f1dfd5660944c54b5fe8cc3d8dc""><code>3ed271b</code></a> Solve leftover from merge conflict</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/9cf3773b717dfd46b37be2ba8cad4f038a8ff6f7""><code>9cf3773</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43358"">#43358</a> from tensorflow/mm-patch-r2.3</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.3.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.3.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 1.15.4,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 1.15.4</h2>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses both NumPy and TensorFlow headers.</li>
</ul>
<h2>TensorFlow 1.15.3</h2>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Updates <code>sqlite3</code> to <code>3.31.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880"">CVE-2019-19880</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244"">CVE-2019-19244</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a></li>
<li>Updates <code>curl</code> to <code>7.69.1</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601"">CVE-2019-15601</a></li>
<li>Updates <code>libjpeg-turbo</code> to <code>2.0.4</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664"">CVE-2018-19664</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330"">CVE-2018-20330</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960"">CVE-2019-13960</a></li>
<li>Updates Apache Spark to <code>2.4.5</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099"">CVE-2019-10099</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190"">CVE-2018-17190</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770"">CVE-2018-11770</a></li>
</ul>
<h2>TensorFlow 1.15.2</h2>
<h1>Release 1.15.2</h1>
<p>Note that this release no longer has a single pip package for GPU and CPU. Please see <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36347"">#36347</a> for history and details</p>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes a security vulnerability where converting a Python string to a <code>tf.float16</code> value produces a segmentation fault (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215"">CVE-2020-5215</a>)</li>
<li>Updates <code>curl</code> to <code>7.66.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482"">CVE-2019-5482</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481"">CVE-2019-5481</a></li>
<li>Updates <code>sqlite3</code> to <code>3.30.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646"">CVE-2019-19646</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168"">CVE-2019-16168</a></li>
</ul>
<h2>TensorFlow 1.15.0</h2>
<h1>Release 1.15.0</h1>
<p>This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.</p>
<h2>Major Features and Improvements</h2>
<ul>
<li>As <a href=""https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0"">announced</a>, <code>tensorflow</code> pip package will by default include GPU support (same as <code>tensorflow-gpu</code> now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. <code>tensorflow-gpu</code> will still be available, and CPU-only packages can be downloaded at <code>tensorflow-cpu</code> for users who are concerned about package size.</li>
<li>TensorFlow 1.15 contains a complete implementation of the 2.0 API in its <code>compat.v2</code> module. It contains a copy of the 1.15 main module (without <code>contrib</code>) in the <code>compat.v1</code> module. TensorFlow 1.15 is able to emulate 2.0 behavior using the <code>enable_v2_behavior()</code> function.
This enables writing forward compatible code: by explicitly importing either <code>tensorflow.compat.v1</code> or <code>tensorflow.compat.v2</code>, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.</li>
<li><code>EagerTensor</code> now supports numpy buffer interface for tensors.</li>
<li>Add toggles <code>tf.enable_control_flow_v2()</code> and <code>tf.disable_control_flow_v2()</code> for enabling/disabling v2 control flow.</li>
<li>Enable v2 control flow as part of <code>tf.enable_v2_behavior()</code> and <code>TF2_BEHAVIOR=1</code>.</li>
<li>AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside <code>tf.function</code>-decorated functions. AutoGraph is also applied in functions used with <code>tf.data</code>, <code>tf.distribute</code> and <code>tf.keras</code> APIS.</li>
<li>Adds <code>enable_tensor_equality()</code>, which switches the behavior such that:
<ul>
<li>Tensors are no longer hashable.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>,
and
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses
both NumPy and TensorFlow headers.</li>
</ul>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/df8c55ce12b5cfc6f29b01889f7773911a75e6ef""><code>df8c55c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43442"">#43442</a> from tensorflow-jenkins/version-numbers-1.15.4-31571</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e8cbcb0b1756de4afda8677add8a55355720ab7""><code>0e8cbcb</code></a> Update version numbers to 1.15.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5b65bf202a00f558784e61b7dba5063195cce0f5""><code>5b65bf2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43437"">#43437</a> from tensorflow-jenkins/relnotes-1.15.4-10691</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/814e8d83f5966af55168bc1141dc8ba68561556f""><code>814e8d8</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/757085e3e62197ab5ad6a10c667aae08a8929556""><code>757085e</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e99e53dda53644e49f4b8b4ec16ef92f6399fc3b""><code>e99e53d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43410"">#43410</a> from tensorflow/mm-fix-1.15</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bad36df000e97cfe0a271e08778a81db4ce8834a""><code>bad36df</code></a> Add missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f3f1835aed4ab1874c0891c487cd6d0340fed67b""><code>f3f1835</code></a> No <code>disable_tfrt</code> present on this branch</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7ef5c62a21f2c03483c21566dd6c048218dced26""><code>7ef5c62</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43406"">#43406</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/abbf34a5885400f81620df23d9da70f30630e699""><code>abbf34a</code></a> Remove import that is not needed</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Pre-trained model link,"The pre-trained model link seems broken:

https://drive.google.com/file/d/1U21Bwchcm96-Nz1SVEo0IvSoPyqqA7gm/view?usp=sharing

Any chance you could upload it again?

Thank you for sharing your work!"
Library error libtiff,"I'm getting when trying to run unet.py
cannot import name 'tif_lzw' from 'libtiff' ."
could use RGB image of the test data?,
Bump tensorflow-gpu from 1.4.0 to 1.15.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.2.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.2
> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
>   * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
>   * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
>   * Expand support for TensorFlow operators in TensorRT conversion (e.g.
>     `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
>   * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
>      significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> 
> # Release 2.1.0
> 
> TensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.
> 
> ## Major Features and Improvements
> * The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install ""Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019"", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).
>   * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.
>   * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.
> * The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.
> * `tf.keras`
>   * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).
>   * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).
>   * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.
>   * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).
>   * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.
>   * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.
>   * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.
>   * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).
> * `tf.data`
>   * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.
>   * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.
>   * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`
> * `tf.debugging`
>   * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.
> * `tf.distribute`
>   * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.
>   * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.
> * `TensorRT`
>   * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.
> * Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to ""true"" or ""1"", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\*D and MaxPool\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.
> 
> ## Breaking Changes
> * Deletes `Operation.traceback_with_start_lines` for which we know of no usages.
> * Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.
> * Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> * The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.
> * `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.
> * `tf.config.experimental_list_devices` has been removed, please use
> `tf.config.list_logical_devices`.
> 
> ## Bug Fixes and Other Changes
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`5d80e1e`](https://github.com/tensorflow/tensorflow/commit/5d80e1e8e6ee999be7db39461e0e79c90403a2e4) Merge pull request [#36215](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36215) from tensorflow-jenkins/version-numbers-1.15.2-8214
- [`71e9d8f`](https://github.com/tensorflow/tensorflow/commit/71e9d8f8eddfe283943d62554d4c676bdaf79372) Update version numbers to 1.15.2
- [`e50120e`](https://github.com/tensorflow/tensorflow/commit/e50120ee34e1e29252f4cbc8ac4cd328e9a9840c) Merge pull request [#36214](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36214) from tensorflow-jenkins/relnotes-1.15.2-2203
- [`1a7e9fb`](https://github.com/tensorflow/tensorflow/commit/1a7e9fbf670ef9d03b2f8fdf1ae2276b2d100fab) Releasing 1.15.2 instead of 1.15.1
- [`85f7aab`](https://github.com/tensorflow/tensorflow/commit/85f7aab93b65ed1fcc589f54d40793b1afb65bf4) Insert release notes place-fill
- [`e75a6d6`](https://github.com/tensorflow/tensorflow/commit/e75a6d6e6e20df83f19e72e04c7984587d768bd3) Merge pull request [#36190](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36190) from tensorflow/mm-r1.15-fix-v2-build
- [`a6d8973`](https://github.com/tensorflow/tensorflow/commit/a6d897351e483dfd0418e5cad2900ad9ef24188c) Use `config=v1` as this is `r1.15` branch.
- [`fdb8589`](https://github.com/tensorflow/tensorflow/commit/fdb85890df5df1e6b3867c842aabb44f561b446d) Merge pull request [#35912](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/35912) from tensorflow-jenkins/relnotes-1.15.1-31298
- [`a6051e8`](https://github.com/tensorflow/tensorflow/commit/a6051e8094c5e7d26ec9573a740246c92e4057a2) Add CVE number for main patch
- [`360b2e3`](https://github.com/tensorflow/tensorflow/commit/360b2e318af2db59152e35be31c8aab1fb164088) Merge pull request [#34532](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/34532) from ROCmSoftwarePlatform/r1.15-rccl-upstream-patch
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.2)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.2)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
Bump tensorflow-gpu from 1.4.0 to 1.15.0,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.0.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
>   * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
>   * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
>   * Expand support for TensorFlow operators in TensorRT conversion (e.g.
>     `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
>   * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
>      significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. 
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * EagerTensor now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
>   * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.
>   * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.
> * `tf.lite`
>   * Add `GATHER` support to NN API delegate.
>   * tflite object detection script has a debug mode.
>   * Add delegate support for `QUANTIZE`.
>   * Added evaluation script for COCO minival.
>   * Add delegate support for `QUANTIZED_16BIT_LSTM`.
>   * Converts hardswish subgraphs into atomic ops.
> * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`590d6ee`](https://github.com/tensorflow/tensorflow/commit/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b) Merge pull request [#31861](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/31861) from tensorflow-jenkins/relnotes-1.15.0rc0-16184
- [`b27ac43`](https://github.com/tensorflow/tensorflow/commit/b27ac431aa37cfeb9d5c35cc50081cdb6763a40e) Update RELEASE.md
- [`07bf663`](https://github.com/tensorflow/tensorflow/commit/07bf6634f602757ef0b2106a92c519d09e80157e) Merge pull request [#33213](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33213) from Intel-tensorflow/mkl-dnn-0.20.6
- [`46f50ff`](https://github.com/tensorflow/tensorflow/commit/46f50ff8a0f099269ac29573bc6ac09d1bc6cab7) Merge pull request [#33262](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33262) from tensorflow/ggadde-1-15-cp2
- [`49c154e`](https://github.com/tensorflow/tensorflow/commit/49c154e17e9fdfe008f8b0b929d1a729e5939c51) Merge pull request [#33263](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33263) from tensorflow/ggadde-1-15-final-version
- [`a16adeb`](https://github.com/tensorflow/tensorflow/commit/a16adeb793b587a08958a72cbbf0d338e063a042) Update TensorFlow version to 1.15.0 in preparation for final relase.
- [`8d71a87`](https://github.com/tensorflow/tensorflow/commit/8d71a87b0e3de6d07588f9139660a77271d12498) Add saving of loaded/trained compatibility models in test and fix a compatibi...
- [`8c48aff`](https://github.com/tensorflow/tensorflow/commit/8c48affdf8ec0e5a9c5252f88e63aa5b97daf239) [Intel Mkl] Upgrading MKL-DNN to 0.20.6 to fix SGEMM regression
- [`38ea9bb`](https://github.com/tensorflow/tensorflow/commit/38ea9bbfea423eb968fcc70bc454471277c9537c) Merge pull request [#33120](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33120) from tensorflow/perf
- [`a8ef0f5`](https://github.com/tensorflow/tensorflow/commit/a8ef0f5d3bff3fe6f46b821832a4e9073dd7c01d) Automated rollback of commit db7e43192d405973c6c50f6e60e831a198bb4a49
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.0)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.0)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>"
which semantic segmentation software did you use for this project ?,
how to run this code on kaggle kernel??,when I was trying to run the code on kaggle kernel it shows import error for libtiff . can you help with this? i tried replacing libtiff with PIL but again some error came.
MemoryError,"I got some trouble, that when I read all train_x images, I got a problem named  MemoryError. The image size is 7200*6800*4 , I'm so confused, could you help me?
**the content of train.py  is:**
```
#!usr/bin/env python
#-*- coding:utf-8 _*-
#@author:mqray
#@file: train.py
#@time: 2019/6/28 12:35

import glob,os
from libtiff import TIFF
from funcs import *
from keras.preprocessing.image import ImageDataGenerator

model = model.UNet(16)

train_src_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_train\src\*.tif')
train_label_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_train\label\*.tif')
val_src_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_val\src\*.tif')
val_label_filelist = glob.glob(r'E:\2019rscup_segamentation\data\main_val\label\*.tif')
test_src_filelist =  glob.glob(r'E:\2019rscup_segamentation\data\main_test\*.tif')
print(train_src_filelist)
#ËÆ≠ÁªÉÈõÜ
train_x = []
for train_src in train_src_filelist:
    tif = TIFF.open(train_src)
    img = tif.read_image()
    crop_lists = crops(img)
    train_x = train_x + crop_lists
    # print(train_x.dtype)
# print(len(train_src_tmp))

trainx = np.asarray(train_x)


train_y = []
for train_label in train_label_filelist[0]:
    tif = TIFF.open(train_label)
    img = tif.read_image()

    crop_lists = crops(img)
    train_y = train_y + crop_lists
trainy = np.asarray(train_y)


#È™åËØÅÈõÜ
val_x = []
for val_src in val_src_filelist:
    tif = TIFF.open(val_src)
    img= tif.read_image()

    crop_lists = crops(img)
    val_x = val_x + crop_lists
valx = np.asarray(val_x)

val_y =[]
for val_label in val_label_filelist:
    tif = TIFF.open(val_label)
    img = tif.read_image()

    crop_lists = crops(img)
    val_y = val_y + crop_lists
valy = np.asarray(val_y)

color_dict = {0:(0,200,0),
              1:(150,250,0),
              2:(150,200,150),
              3:(200,0,200),
              4:(150,0,250),
              5:(150,150,250),
              6:(250,200,0),
              7:(200.200,0),
              8:(200,0,0),
              9:(250,0,150),
              10:(200,150,150),
              11:(250,150,150),
              12:(0,0,200),
              13:(0,150,200),
              14:(0,200,250),
              15:(0,0,0)}

'''
Â∞ÜÊ†áÁ≠æÂÄºone-hotÂåñ
'''
trainy_hot = []
for i in range(trainy.shape[0]):
    hot_img = rgb_to_onehot(train_label_filelist[i], color_dict)
    trainy_hot.append(hot_img)
trainy_hot = np.asarray(trainy_hot)

val_hot = []
for i in range(valy.shape[0]):
    hot_img = rgb_to_onehot(val_label_filelist[i], color_dict)
    val_hot.append(hot_img)
val_hot = np.asarray(val_hot)

trainy  = trainy / np.max(trainy)
valy  = valy / np.max(valy)

# data augmentation

datagen_args = dict(rotation_range=45.,
                         width_shift_range=0.1,
                         height_shift_range=0.1,
                         shear_range=0.2,
                         zoom_range=0.2,
                         horizontal_flip=True,
                         vertical_flip=True,
                         fill_mode='reflect')
x_datagen = ImageDataGenerator(**datagen_args)
y_datagen = ImageDataGenerator(**datagen_args)
seed = 1
batch_size = 16
x_datagen.fit(train_x, augment=True, seed = seed)
y_datagen.fit(trainy, augment=True, seed = seed)
x_generator = x_datagen.flow(train_x, batch_size = 16, seed=seed)
y_generator = y_datagen.flow(trainy, batch_size = 16, seed=seed)
train_generator = zip(x_generator, y_generator)
X_datagen_val = ImageDataGenerator()
Y_datagen_val = ImageDataGenerator()
X_datagen_val.fit(valx, augment=True, seed=seed)
Y_datagen_val.fit(valy, augment=True, seed=seed)
X_test_augmented = X_datagen_val.flow(valx, batch_size=batch_size, seed=seed)
Y_test_augmented = Y_datagen_val.flow(valy, batch_size=batch_size, seed=seed)
test_generator = zip(X_test_augmented, Y_test_augmented)
history = model.fit_generator(train_generator, validation_data=test_generator, validation_steps=batch_size/2, epochs = 10, steps_per_epoch=len(x_generator))
model.save(""model_augment.h5"")



# history = model.fit(train_src_tmp,trainy_hot,epochs=1,validation_data=(val_src_x,val_hot),batch_size=1,verbose=1)
# model.save('model_onehot.h5')

print(history.history.keys())
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train','val'],'upper left')
plt.savefig('acc_plot.jpg')
plt.show()
plt.close()

print(history.history.keys())
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','val'],'upper left')
plt.savefig('loss_plot.jpg')
plt.show()
plt.close()
```

**and the model file is :**
```
#!usr/bin/env python
#-*- coding:utf-8 _*-
#@author:mqray
#@file: uunet.py
#@time: 2019/6/24 10:48

import PIL
from PIL import Image
import matplotlib.pyplot as plt
from libtiff import TIFF
from libtiff import TIFFfile, TIFFimage
from scipy.misc import imresize
import numpy as np
import glob
import cv2
import os
import math
import skimage.io as io
import skimage.transform as trans
from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K


# %matplotlib inline

def UNet(num_class,shape=(512, 512, 4)):
    # Left side of the U-Net
    inputs = Input(shape)
    #    in_shape = inputs.shape
    #    print(in_shape)
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv1)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv2)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv3)
    conv3 = BatchNormalization()(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv4)
    conv4 = BatchNormalization()(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    # Bottom of the U-Net
    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='random_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv5)
    conv5 = BatchNormalization()(conv5)
    drop5 = Dropout(0.5)(conv5)

    # Upsampling Starts, right side of the U-Net
    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(drop5))
    merge6 = concatenate([drop4, up6], axis=3)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge6)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv6)
    conv6 = BatchNormalization()(conv6)

    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv6))
    merge7 = concatenate([conv3, up7], axis=3)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge7)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv7)
    conv7 = BatchNormalization()(conv7)

    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv7))
    merge8 = concatenate([conv2, up8], axis=3)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge8)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv8)
    conv8 = BatchNormalization()(conv8)

    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='random_normal')(
        UpSampling2D(size=(2, 2))(conv8))
    merge9 = concatenate([conv1, up9], axis=3)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(merge9)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv9)
    conv9 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='random_normal')(conv9)
    conv9 = BatchNormalization()(conv9)

    # Output layer of the U-Net with a softmax activation
    conv10 = Conv2D(num_class, 1, activation='softmax')(conv9)

    model = Model(input=inputs, output=conv10)

    model.compile(optimizer=Adam(lr=0.000001), loss='categorical_crossentropy', metrics=['accuracy'])

    model.summary()

    # filelist_modelweights = sorted(glob.glob('*.h5'), key=numericalSort)

    # if 'model_nocropping.h5' in filelist_modelweights:
    #   model.load_weights('model_nocropping.h5')
    return model

```

"
 ImportError: cannot import name imresize,Hey. I believe you've got a typo in scipy version in requirements.txt. According to this issue on scipy github https://github.com/scipy/scipy/issues/6212 there are no 'imresize' function in scipy.misc above version 1.3. Try changing it to 1.2.
NameError: name 'iou' is not defined in unet.py,"Hi, It seems like variable/function 'iou' is missing in unet.py

My stack trace: 
File ""/macierz/home/s174520/eye-in-the-sky/unet.py"", line 84, in UNet
    model.compile(optimizer = Adam(lr = 0.000001), loss = 'categorical_crossentropy', metrics = ['accuracy', iou])
NameError: name 'iou' is not defined

Edit:
I checked commit history. I think you have added iou function to test_unet.py file istead of unet.py and you're not passing it to UNet model."
requirements.txt file has incorrect structure,"Hi, could you please fix structure of the requirements.txt file that anybody can use pip install -r requirements.txt command? Also, you didn't mention any info about the versions of libraries you used. "
Issue with pretrained weight file model_onehot.h5,"I am trying to use pretrained weight file model_onehot.h5. I changed the file name at all relevant places in test_unet.py. However, when we load the weight file, it gives following error:

ValueError: Dimension 0 in both shapes must be equal, but are 1 and 9. Shapes are [1,1,16,3] and [9,16,1,1]. for 'Assign_82' (op: 'Assign') with input shapes: [1,1,16,3], [9,16,1,1].

Seems like when loading model.load_weights(weights_file), there is some inconsistency for dimensions."
Please mention the validation set in README.md,"Hi, I think everyone who presented in this competition just played it dirty by not mentioning the validation set. I'd like to request that you mention the images taken in the validation set so that reader will not be misled by the results if one tries to reproduce them. Btw nice code  we'll also make our code public soon :)"
Update test_unet.py,
How to get the RGB images?,"Very great work!
I want to ask, is your RGB images rendered by yourself? Is there color information?
If I want to get a dataset which includes RGB images and point clouds of each single object, is there such a data set?
Looking forward to your reply!
Thank you!"
"What is ""disf"" in ""PerspectiveGridGenerator.lua""","Hi, I'm trying to replicate your projection work with PyTorch. But I'm confused about the definition of ""focal length"" and ""disf"" in your code. That is:
___________________________________________________
focal_length = math.sqrt(3)/2 ? 
dmin = 1/(focal_length + math.sqrt(3))
dmax = 1/(focal_length)
for k=1,depth do
    disf = dmin + (k-1)/(depth-1) * (dmax-dmin)
baseGrid[k][i][j][1] = 1/disf
___________________________________________________
Please forgive my offense, I have followed your advice and read the appropriate books. I still don't understand how focal length is defined. And in the paper your point that ""the minimum and maximum disparity in the camera frame are denoted as dmin and dmax"", So who is a disparty, disf or 1/disf? Your help means a lot to me."
About focal_length and the translate matrix,"Thank you for what you have done!  I'm confused about the translate matrix .here are the specific question:
(1)Is the original focal length=1(i.e. if elevation=0deg) ?
(2)Why the translate matrix set to be as follows at first?
![1](https://user-images.githubusercontent.com/41320257/68128617-01dadd80-ff53-11e9-9272-0f3cca857986.png)

could you please explain it?
I'll appreciate for you reply!"
TensorFlow implement,"I am sorry to bother you . Recently  i want to reproduct the PTN with tensorflow implementation . I want to use my own data , but having problems in making tfrecords format data. Could you please show me the codes to make  tfrecords format data with the following features(float representations) : image , mask, vox . Looking forward to your reply , thanks."
"Hi, I can't download data ,model. Web pages cannot download data sets and modelsÔºüÔºüThanks",
"HelloÔºå I can't download your data, do you have other method for us to  download your data?",
 tensorflow implementation is broken,"Hi, @xcyan,

The [link ](https://github.com/tensorflow/models/tree/master/ptn) of your tensorflow implementation is broken, could you fix it?

THX"
Out of memory,"Hi,
I was just evaluating the pretrained model and encountered the following issue regarding the memory space of the gpu,
THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
/home/sbasavaraju/torch/install/bin/luajit: ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu:66
stack traceback:
	[C]: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function <...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:245>
	[C]: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	/home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	/home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'
	scripts/eval_quant_test.lua:63: in main chunk
	[C]: in function 'dofile'
	...raju/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

I have Ubuntu 14,04 with graphics model NVIDIA GeForce GTX 470 1 GB , is the graphics not sufficient to run the program ?

Thanks "
Data corrupted,"Hi Doc. @xcyan, I encountered the following problem during running ""eval_models.sh"". 
`torch/install/share/lua/5.1/torch/File.lua:351: read error: read 39866875 blocks instead of 46656000 at torch/pkg/torch/lib/TH/THDiskFile.c:356`
I thought it is because of the data corruption. Could you please share the checksum of the pretrained models for the data validation? Thank you. "
Only half of the training data is used,"In https://github.com/xcyan/nips16_PTN/blob/master/scripts/train_rotatorRNN_base.lua#L274  in the expression math.min(data:size() * opt.nview / 2 , opt.ntrain), I don't understand where the division by two is coming. data:size() is the number of trainings scenes you have, 4744 in this case, and you have 24 views per scene, so multiplication makes sense, but why divide it by two. Maybe I missed something, but it seems that only half of the views/data is used?"
Results of trained models don't match paper,"Hello,
after training the encoder(CNN-Vol) and the perspective transformer (PTN-Proj) I test the final model by changing the lines:
`base_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(0,1)_ks24_vs32/net-epoch-100.t7')
encoder = base_loader.encoder
base_voxel_dec = base_loader.voxel_dec

unsup_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(1,0)_ks24_vs32/net-epoch-100.t7')
unsup_voxel_dec = unsup_loader.voxel_dec

sup_loader = torch.load(opt.checkpoint_dir .. 'ptn_comb.t7')
sup_voxel_dec = sup_loader.voxel_dec`

The results on the testset are:
cat [chair]:	CNN-VOL IOU = 0.459553	PTN-COMB IOU = 0.162989	PTN-PROJ IOU = 0.472389
	
which are 4 to 5 points lower than reported in the paper. What could be the reasons for it? Also I noticed the pretrained ptn-comb model has some problems (0.16) when evaluated with my encoder (instead of the pretrained encoder). What is the reason for this?
"
Can't test the trained encoder,"Hi,
I trained the single class encoder with ./demo_pretrain_singleclass.sh. Now I wanted to evaluate the trained models. So in eval_quant_test.lua I just changed the name of the loaded file (cnn_vol.t7) to the last trained model:
base_loader = torch.load(opt.checkpoint_dir .. 'arch_rotatorRNN_singleclass_nv24_adam2_bs8_nz512_wd0.001_lbg10_ks16/net-epoch-20.t7')
encoder = base_loader.encoder
base_voxel_dec = base_loader.voxel_dec

When I run the testcript eval_models.sh I get following error:
/home/meeso/torch/install/bin/luajit: scripts/eval_quant_test.lua:90: attempt to index global 'base_voxel_dec' (a nil value)
stack traceback:
	scripts/eval_quant_test.lua:90: in main chunk
	[C]: in function 'dofile'
	...eeso/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

Any idea how I fix this?"
Results don't match paper,"Hi,
I downloaded the pretrained models and after running run ./eval_models.sh and got these numbers
CNN-VOL IOU = 0.500177	PTN-COMB IOU = 0.509016	PTN-PROJ IOU = 0.503761	
they are not far off, but still don't correspond to the paper numbers, any idea why I am getting different results?"
typo?,"There might be a typo in `scripts/eval_quant_test.lua`:
`require 'stn'     ------> require 'ptn'`"
latest TF 2.13,
Does it support causal mask?,"The effect is the same as the argument ""use_casual_mask"" in keras official layer class Attention.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention
Thanks for your useful module and expect your reply.
"
Add support for the bahdanau score,
Number of parameters in Attention layer,"Thank you for your contribution of attention python package.

When I am using it as a novice, I have two questions. 
If you have time available, can you give me a  #hand?

In the next example code you provided, 

1) Can you explain to me how to calculate the number of parameters in Attention layer (8192)?
    I can calculate the number of LSTM and Dense layers (16896, 33) 
    but despite many attempts, I can't figure it out how to calculate 8192 in the case of Attention layer.

2) This attention in the example belongs to Luong's version or Bahdanau's version?

----------------- Example code you provided --------------------

num_samples, time_steps, input_dim, output_dim = 100, 10, 1, 1
data_x = np.random.uniform(size=(num_samples, time_steps, input_dim))
data_y = np.random.uniform(size=(num_samples, output_dim))

model_input = Input(shape=(time_steps, input_dim))
x = LSTM(64, return_sequences=True)(model_input)
x = Attention(32)(x)
x = Dense(1)(x)
model = Model(model_input, x)

* Model Structure
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
 Layer (type) ______________________ Output Shape ______ Param   
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
 input_10 (InputLayer)  ___________  [(None, 10, 1)]  ____ 0                                                                 
 lstm_146 (LSTM) ________________  (None, 10, 64)  ____ 16896                                                   
 attention_146 (Attention) _______ (None, 32)  ________ 8192                                                        
 dense_283 (Dense) ______________ (None, 1)  _________ 33    
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Total params: 25,121
Trainable params: 25,121
Non-trainable params: 0
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
TypeError: __call__() takes 2 positional arguments but 3 were given,"when i use the codeÔºågot TypeError: __call__() takes 2 positional arguments but 3 were given

whta is the problemÔºü how to fix it Ôºü

thinks"
Update lib + misc,
Please update version,"Running `pip install attention` installs version 3 (Sep 2020) but there have been updates since.  Currently I cannot specify the # of units without receiving an error:

> TypeError: __init__() takes 1 positional argument but 2 were given

The traceback shows an init method that does not handle the `units` parameter (old code)."
"TypeError: Expected `trainable` argument to be a boolean, but got: 64","I'm getting this error suddenly,
```
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-26-f6e02ef33fa0> in <module>()
----> 1 regressor = best_lstm_model(N_FEATURES, BATCH_SIZE, TIME_STEPS)

2 frames

<ipython-input-24-fcdb2d61d17a> in best_lstm_model(n_features, batch_size, look_back)
     34 
     35     d = Add(name='add')([b1, b2])
---> 36     a = Attention(64)(d)
     37 
     38     y = Dense(1)(a)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    528     self._self_setattr_tracking = False  # pylint: disable=protected-access
    529     try:
--> 530       result = method(self, *args, **kwargs)
    531     finally:
    532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)
    339              trainable.dtype is tf.bool)):
    340       raise TypeError(
--> 341           'Expected `trainable` argument to be a boolean, '
    342           f'but got: {trainable}')
    343     self._trainable = trainable

TypeError: Expected `trainable` argument to be a boolean, but got: 64
```

Here's my model:
```
from tensorflow import keras as keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import CSVLogger

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Input, Flatten, Add, Concatenate, Dot, Multiply, Bidirectional, GaussianNoise
from tensorflow.keras.layers import Maximum, Average, Activation

from attention import Attention
import tensorflow
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# attention: https://github.com/philipperemy/keras-attention-mechanism

def best_lstm_model(n_features, batch_size, look_back):
    """"""
    Returns a keras LSTM model. Our architecture will be kept 
    in this method.
    """"""
    x_i1 = Input((look_back, 1), name='ip1')
    g = GaussianNoise(0.05, name='g')(x_i1)

    b1 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l1'), name='b1')(x_i1)
    b1 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l3'), name='b3')(b1)

    b2 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l2'), name='b2')(g)
    b2 = Bidirectional(LSTM(units = 64, return_sequences = True, name='l4'), name='b4')(g)
    

    d = Add(name='add')([b1, b2])
    a = Attention(64)(d)

    y = Dense(1)(a)

    model = Model(x_i1, y)

    model.compile(optimizer = 'adam', loss = 'mean_squared_error')

    return model
```"
Attention not working for MLP,"I need to add attention to my following model. It works perfectly for LSTM model but I get the below error : 
```
def get_ANN_attention_model(num_hidden_layers, num_neurons_per_layer, dropout_rate, activation_func, train_X):
    with tf.device('/gpu:0'):
        model_input = tf.keras.Input(shape=(train_X.shape[1]))  # input layer.
        for i in range(num_hidden_layers):
            x = layers.Dense(num_neurons_per_layer,activation=activation_func,bias_regularizer=L1L2(l1=0.0, l2=0.0001),activity_regularizer=L1L2(1e-5,1e-4))(model_input)
            x = layers.Dropout(dropout_rate)(x)
            x = Attention(num_hidden_layers)(x)
        outputs = layers.Dense(1, activation='linear')(x)
        model = tf.keras.Model(inputs=model_input, outputs=outputs)
        model.summary()
    return model
```

**ERROR**
    hidden_size = int(hidden_states.shape[2])
  File ""C:\Users\bhask\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\tensor_shape.py"", line 896, in __getitem__
    return self._dims[key].value
IndexError: list index out of range"
Output with multiple time steps,"Hi, 

Can this be used for predicting output with multiple time-steps?
If no, how can the code be changed to accommodate this? Thanks. "
what do the h_t mean in the Attention model?,"Hi there!
Thanks so much for implementing this and all of the other work that you do!
I wanna know the meaning of h_tÔºåi.e h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states) . Well, in Luong's paper the h_t was used as the input the hidden state. But how to explain it in a scene which is not seq2seq?
 "
Attention Mechanism not working,"Hi,
I have added an attention layer (following the example) to my simple LSTM network shown below. 

`timestep = timesteps`
`features = 11`
`model = Sequential()`
`model.add(LSTM(64, input_shape=(timestep,features), return_sequences=True))`
`model.add(Dropout(0.2))`
`model.add(LSTM(32, return_sequences=True))`
`model.add(LSTM(16, return_sequences=True))`
`model.add(Attention(32))`
`model.add(Dense(32))`
`model.add(Dense(16))`
`model.add(Dense(1))`
`print(model.summary())`
The code worked fine up till last week and I got a summary of model having attention layer details like this:
![image](https://user-images.githubusercontent.com/61421364/119895079-5ebe3000-bf0b-11eb-9f06-9298f9ea9f68.png)


However, now running the same code gives me a weird error. 
`ValueError: tf.function-decorated function tried to create variables on non-first call.`

What I noticed is that the model summary has changed too:
![image](https://user-images.githubusercontent.com/61421364/119895129-6bdb1f00-bf0b-11eb-9ca9-3fd20a70869d.png)


I am tight on time due an upcoming deadline. Any assistance would be highly appreciated. 
P.S. This was a fully working model that has stopped working all of a sudden for no apparent reason."
Add guidance to README to use Functional API for saving models that use this layer,"Hi there!

Thanks so much for implementing this and all of the other work that you do!

I ran in to an issue with loading a model uses this the Attention layer in a sequential model. However, the Attention layer is defined using the Function API and Keras does not like it when you try to load a mixed model. 

Specifically, my error was 

```
m = keras.models.load_model('saved_mixed_model_path',
            custom_objects = { 'Attention': Attention}
           )

=> ValueError: A merge layer should be called on a list of inputs.

```

To solve this, I had to convert my model to one that uses the functional API and retrain.

Part of my confusion stems from the examples where both the Sequential and Functional APIs are used. In [this example](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/example-attention.py) you successfully save and load a model using only Functional API. But in [this lstm example](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/find_max.py) the Sequential API is used and no loading/saving is done.

Could a caveat be added to the README.md saying that if you plan to load/save these models, only the Functional API should be used when building the model that uses the Attention layer?

Cheers"
Interpreting attention weights for more than one input features.,"How can we get attention weights for each input feature when our input consists of multiple inputs?
I am getting only one array of attention weights and I am not sure how to interpret it for multiple inputs.

shape of attention weights (attached as fig) is:
(300, 6) 
where 6 is the sequence_length/lookback steps/time steps. 

![attention_weight](https://user-images.githubusercontent.com/52854229/110059164-0877a000-7da7-11eb-84d1-4687600b6889.png)
"
Loading model problems,"When I'm trying to load a saved model, I get the following error. ! ""A `Dot` layer should be called on a list of 2 inputs""."
Using attention with multivariate timeseries data,"Hey, I' am trying to use attention with timeseries data that has more than 1 feature this leads to an incompatible shapes error. What changes do I make to get it to work?"
get_config,"Hi,
Perhaps do you have another implementation with the get_config function for saving the model in keras? I had been trying but I always get this error: 
raise ValueError('A `Dot` layer should be called '

ValueError: A `Dot` layer should be called on a list of 2 inputs.

Thanks!
"
attention when using more than one feature,"Hi Philip
Your example of attention has 1 feature (2000, 20,1),  my dataset has 60 features (200, 1000,60), in that case I have to do something different to what you do in your example?  

Thank you!"
weird attention weights when adding sequence of numbers.,"I am trying to slightly modify your example of [adding numbers](https://github.com/philipperemy/keras-attention-mechanism/blob/master/examples/example-attention.py) such that the target is the sum of all the numbers in the sequence before delimiter. Below is the modified code

```python

def add_numbers_before_delimiter(n: int, seq_length: int, delimiter: float = 0.0,
                                         index_1: int = None) -> (np.array, np.array):
    """"""
    Task: Add all the numbers that come before the delimiter.
    x = [1, 2, 3, 0, 4, 5, 6, 7, 8, 9]. Result is y =  6.
    @param n: number of samples in (x, y).
    @param seq_length: length of the sequence of x.
    @param delimiter: value of the delimiter. Default is 0.0
    @param index_1: index of the number that comes after the first 0.
    @return: returns two numpy.array x and y of shape (n, seq_length, 1) and (n, 1).
    """"""
    x = np.random.uniform(0, 1, (n, seq_length))
    y = np.zeros(shape=(n, 1))
    for i in range(len(x)):
        if index_1 is None:
            a = np.random.choice(range(1, len(x[i])), size=1, replace=False)
        else:
            a = index_1
        y[i] =  np.sum(x[i, 0:a])
        x[i, a] = delimiter

    x = np.expand_dims(x, axis=-1)
    return x, y


def main():
    numpy.random.seed(7)

    # data. definition of the problem.
    seq_length = 20
    x_train, y_train = add_numbers_before_delimiter(20_000, seq_length)
    x_val, y_val = add_numbers_before_delimiter(4_000, seq_length)

    # just arbitrary values. it's for visual purposes. easy to see than random values.
    test_index_1 = 4
    x_test, _ = add_numbers_before_delimiter(10, seq_length, 0, test_index_1)
    # x_test_mask is just a mask that, if applied to x_test, would still contain the information to solve the problem.
    # we expect the attention map to look like this mask.
    x_test_mask = np.zeros_like(x_test[..., 0])
    x_test_mask[:, test_index_1:test_index_1 + 1] = 1

    model = Sequential([
        LSTM(100, input_shape=(seq_length, 1), return_sequences=True),
        SelfAttention(name='attention_weight'),
        Dropout(0.2),
        Dense(1, activation='linear')
    ])

    model.compile(loss='mse', optimizer='adam')
    print(model.summary())

    output_dir = 'task_add_two_numbers'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    max_epoch = int(sys.argv[1]) if len(sys.argv) > 1 else 200

    class VisualiseAttentionMap(Callback):

        def on_epoch_end(self, epoch, logs=None):
            attention_map = get_activations(model, x_test, layer_names='attention_weight')['attention_weight']

            # top is attention map.
            # bottom is ground truth.
            plt.imshow(np.concatenate([attention_map, x_test_mask]), cmap='hot')

            iteration_no = str(epoch).zfill(3)
            plt.axis('off')
            plt.title(f'Iteration {iteration_no} / {max_epoch}')
            plt.savefig(f'{output_dir}/epoch_{iteration_no}.png')
            plt.close()
            plt.clf()

    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=max_epoch,
              batch_size=64, callbacks=[VisualiseAttentionMap()])


if __name__ == '__main__':
    main()

```

I was expecting the model to focus on all values in `x_test` sequence before index `4`. However as you can see in gif, the model focuses on just one point. Can you please elaborate where I am mistaking? 

Thank in advance.

![add_numbers](https://user-images.githubusercontent.com/25817388/95296837-6bc2dc00-08b4-11eb-96bb-0f85023f82ac.gif)
"
add sequential examples + keras layer,"Update function to ""real"" Keras Layer."
2D attention,"
@philipperemy 

Do you know how I can apply the attention module to a 2D shaped input , I would like to apply to apply attention after the LSTM layer-

```
Layer (type)                    Output Shape         Param #     Connected to                     
features (InputLayer)           (None, 16, 1816)     0                                            
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 2048)         31662080    features[0][0]                   
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         2098176     lstm_1[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 1024)         0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 120)          123000      leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
feature_weights (InputLayer)    (None, 120)          0                                            
__________________________________________________________________________________________________
multiply_1 (Multiply)           (None, 120)          0           dense_3[0][0]                    
                                                                 feature_weights[0][0]            

Total params: 33,883,256
Trainable params: 33,883,256
Non-trainable params: 0
__________________________________________________________________________________________________
```
Would really appreciate your suggestion on how to modify attention_3D block to make it work for a 2D input as well. thanks."
Use this repository for CNN ,"Dear Sir,

Would it be possible to use this repo for CNN network also?

Thanks and regards."
"pip install and numpy, keras packages are forced to be uninstalled","Hi,

As I install the keras-attention-mechanism to my conda3 by pip, the essential packages of numpy and keras are unexpectedly being uninstalled. Do you know why? 

Bests,
Peiwan"
"Hiddent state parameter, what really should be passed? ","Hi, thanks for the implementation!
I have been trying to implement this code
`model = Sequential() 
model.add(Embedding(300000, 100, input_length=250))
model.add(LSTM(units=250, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))
model.add(attention_3d_block( )) 
model.add(Flatten())
 model.add(Dense(200, activation='relu'))
model.add(Dense(3, activation='softmax'))`


**Error**  `TypeError: attention_3d_block() missing 1 required positional argument: 'hidden_states'
`
I tried to explore the given documentation but I couldn't understand what really should be passed there. 
"
Restricting attention weights to domain,"In my application, the attention weights are centering on locations which are indicative of a subset of the classes. Therefore, while the algorithm performs well on this subset, it sometimes misclassifies on the other classes because the attention weights cause the obvious differences to be considered ""residual"". 

Is there a documented way of restricting the attention weights to a certain value or index domain to enforce constraints on its focus?  This question makes me think of NLP problems where frameworks commonly pair ML methodologies with a set of predetermined rules (usually defined with spacy).

Any thoughts? Thanks in advance."
Visualizing attention weights with input arrays,"When predicting on test data with the trained model, how can I visualize the attention weights? I'd like to study where the model designates as ""important areas"". 

For reference, my input data is usually of shape (100, 900, 4) with  3 output classification options.

Thanks!"
attention implementation help for OCR,"Hi,

how can i add the attention model fot keras image_ocr implementation 
"
CI + examples,
ask a problem about your code,"in your code ,you want to pay more attention on the 10th step. your Experimental results also prove it.
But, your code seems not foucs on the 10th step. please read following code.
` score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states) 
    #            score_first_part           dot        last_hidden_state     => attention_weights
    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)
    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)
    score = dot([score_first_part, h_t], [2, 1], name='attention_score') `

the way you calculate ‚Äòscore‚Äô is  score_first_part dot h_t. 
the way you get h_t  :  h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state') .  in my view 'lambda x: x[:, -1, :]' means you choose the last step in the time sequence , in other word, you pay more attention on the 20th step.(in your  code you define TIME_STEPS = 20). 
so, if my understanding is right, you should change you code to be   h_t = Lambda(lambda x: x[:, 9, :], output_shape=(hidden_size,), name='last_hidden_state') . 
of course, my understanding perhaps wrong. i am lookingforward your reply . 
thank you."
where is dense attention implementation Ôºü,
"what is the meaning of the second parameter in dot([], [1, 1], name='context_vector')","Hi, Thanks for your awesome work. 
I have a confusion about the code: context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')
What is the meaning of the second parameter?"
Update to the attention mechanism,Related to thread: https://github.com/philipperemy/keras-attention-mechanism/issues/14
Update README.md,mini (nano) grammar fix :)
TypeError: 'module' object is not callable,"> output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
At this line error happens:

output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
TypeError: 'module' object is not callable

Not sure what is wrong. Could you help to resolve?"
visualizing soft attention,How can we visualize the soft attention similar to the Bengio et al. paper?
What is the logic behind the attention layer?,"Ik would like to understand intuitively or theoretically, how the attention layer reflects the attention of the model for a prediction?
Because it is easy for the model to give equal weight for each input feature in the attention layer, and that defeats the purpose of the attention layer."
papers using dense attention mechanism,"Hello,

Is the dense attention mechanism based on a particulier paper?
Or are there papers using this mechanism?

"
How to do Stacked LSTM with attention using this framework ?,"hello, 

I have run your code successful. 

I have also include stacked  LSTM in your code : 

```
def model_attention_applied_before_lstm():
    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))
    attention_mul = attention_3d_block(inputs)
    lstm_units = 32
    attention_mul = LSTM(lstm_units, return_sequences=True)(attention_mul)
    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)
    output = Dense(1, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    return model

```

But maybe this is not the correct way to apply staked LSTM with attention right ?

My ultimate goal is to include attention into this code (classification of multivariate time series ) : 
```

class LSTMNet:
    @staticmethod
    def build(timeSteps,variables,classes):
        inputNet = Input(shape=(timeSteps,variables))
       lstm=Bidirectional(GRU(100,recurrent_dropout=0.4,dropout=0.4,return_sequences=True),merge_mode='concat')(inputNet) 
       lstm=Bidirectional(GRU(50,recurrent_dropout=0.4,dropout=0.4,return_sequences=True),merge_mode='concat')(lstm) 
        lstm=Bidirectional(GRU(20,recurrent_dropout=0.4,dropout=0.4,return_sequences=False),merge_mode='concat')(lstm) 
        # a softmax classifier
        classificationLayer=Dense(classes,activation='softmax')(lstm)
        model=Model(inputNet,classificationLayer)
        return model
```


Thanks in advance for any possible info "
why add a Dense(64) layer after the attention layer,what's the point of adding another `attention_mul = Dense(units=64)(attention_mul)` ?
"get_activations use  multi-input data, does not work.","Here is the error message
```
    layer_name='attention_vec')[0], axis=2).squeeze()
  File ""/Users/yu/proj/cancel_blame/code/src/lib/attention/attention_utils.py"", line 16, in get_activations
    layer_outputs = [func([inputs, 1.])[0] for func in funcs]
  File ""/Users/yu/proj/cancel_blame/code/src/lib/attention/attention_utils.py"", line 16, in <listcomp>
    layer_outputs = [func([inputs, 1.])[0] for func in funcs]
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2666, in __call__
    return self._call(inputs)
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2619, in _call
    dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))
AttributeError: 'list' object has no attribute 'dtype'
```"
"Update dependeicies to latest Tensorflow, with minior bug fixes.","The current code base is not working anymore even with the default `pip install -r requirements.txt`.

So I upgrade it to using the latest tensorflow (v1.12.0). Other dependencies are all updated as well."
Updated code to keras 2.2,
One to One keras model with Attention in Keras ,"Hello,

I have a keras model that has sequence of inputs and sequence of outputs where each input has an associated output(Label). lets say (part of speech tagging (POS tagging)

Seq_in[0][0:3]
array([[15],[28], [23]])


Seq_out[0][0:3]
array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
	   [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],
dtype=float32)


I want to build attention on top of the lstm layer. I am following this work "" Attention-Based Bidirectional Long Short-Term Memory Networks for
Relation Classification  "" Zhou et al, 2016



X_train, X_val, Y_train, Y_val = train_test_split(Seq_in,Seq_out, test_size=0.20)

TIME_STEPS = 500
INPUT_DIM = 1
lstm_units = 256



    
inputs = Input(shape=(TIME_STEPS, INPUT_DIM))

activations = Bidirectional(LSTM(lstm_units, return_sequences=True))(inputs) # First laer bidirictional 
activations = Dropout(0.2)(activations)
activations = Bidirectional(LSTM(lstm_units, return_sequences=True))(activations) # Second layer bidirectional 
activations = Dropout(0.2)(activations)
attention = Dense(1,activation='tanh')(activations) # This is equation (9) in the paper. Squashing each output state vector to a scaler. 
attention = Flatten()(attention)
attention = Activation('softmax')(attention) # This is equation (10) in the paper.
attention = RepeatVector(512)(attention) # Repeating the softmax vector to have the same dimintion as the output state  vector (512)
attention = Permute([2,1])(attention) # permute 

sent_representation = multiply([activations,attention]) # multiply the attention vector with the output state vector element-wise. 
sent_representation = Lambda(lambda xin: K.sum(xin, axis=-1))(sent_representation) # summation of all output state vectors 
sent_representation = RepeatVector(TIME_STEPS)(sent_representation) # Repeat vector to be the same diminsion as the time steps
sent_representation = concatenate([activations,sent_representation]) # concatenate the sentence representation to the output states 



output = Dense(15, activation='softmax')(sent_representation)#(out_attention_mul) # Find the softmax for the current label
model = Model(inputs=inputs, outputs=output)  
    

 
sgd = optimizers.SGD(lr=.1,momentum=0.9,decay=1e-3,nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(X_train,Y_train,epochs=2, validation_data=(X_val, Y_val),verbose=1)




__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 500, 1)       0                                            
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 500, 512)     528384      input_1[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500, 512)     0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 500, 512)     1574912     dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 500, 512)     0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 500, 1)       513         dropout_2[0][0]                  
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 500)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 500)          0           flatten_1[0][0]                  
__________________________________________________________________________________________________
repeat_vector_1 (RepeatVector)  (None, 512, 500)     0           activation_1[0][0]               
__________________________________________________________________________________________________
permute_1 (Permute)             (None, 500, 512)     0           repeat_vector_1[0][0]            
__________________________________________________________________________________________________
multiply_1 (Multiply)           (None, 500, 512)     0           dropout_2[0][0]                  
                                                                 permute_1[0][0]                  
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 500)          0           multiply_1[0][0]                 
__________________________________________________________________________________________________
repeat_vector_2 (RepeatVector)  (None, 500, 500)     0           lambda_1[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 500, 1012)    0           dropout_2[0][0]                  
                                                                 repeat_vector_2[0][0]            
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 500, 15)      15195       concatenate_1[0][0]              
==================================================================================================
Total params: 2,119,004
Trainable params: 2,119,004
Non-trainable params: 0
______________________________________________

I think this code performs what the paper does, except that the concatenate step merges the attention weights to all the output state vectors and do not change them for each time step so for each output label. 
So I think, for each time step output, I have to do something so the attention weights differ. Am I right? 
Any help is appreciated

Thanks in advance"
why Permute before attention dense layer in attention_3d_block?,"```
    a = Permute((2, 1))(inputs)
    a = Dense(TIME_STEPS, activation='softmax')(a)
```
this line ,why do you permute times_tep and input_dim
what if I don't permute , and followed by a dense layer with input_dim ? since dense layer is with the shape of  ""time_Step *time_step"" ,what is the difference when I change it to ""input_dim * input_dim""
`Dense(input_Dim activation='softmax')(a)`"
Is this attention is applicable for use with the encoder/decoder mechanism?,
You code is outdated!,"Your code doesn't fit to new versions of keras
To fix it change those strings in ""attention_dense.py"":

1. **""from keras.layers import Input, Dense, merge""** on **""from keras.layers import Input, Dense,multiply""**;
2. **""attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')""** on **""attention_mul = multiply([inputs, attention_probs],name='attention_mul')"" ;
and in ""attention_lstm.py""**:
in""attention_lstm.py"":

1. 1. **import multiply too**;
2. 2. **""output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')""** change on **""output_attention_mul = multiply([inputs, a_probs], name='attention_mul')""**"
How to implement Multi-Hop Attention using Keras?,MultiHopAttention was proposed by Fackbook.
Attention Visualization,"In the final visualization of the attention weights it says this is showing the attention over input dimensions but the x axis goes to the length of the time steps. So it is showing how important the time step is and not each feature. Shouldn't it be the other way? Where each x is a feature? 

When I apply this to my own dataset it just says the most recent time steps are the most important."
How to visualise as 2dimensional heatmap?,"lets say we are predicting with timestep of 24, and get 24 result as output. how can we visualise as heatmap like in https://github.com/datalogue/keras-attention"
some confusions.,"![attention_luong](https://user-images.githubusercontent.com/11025093/39625088-bbd45082-4fbd-11e8-85c1-5895f1824bed.png)
Hello , Thanks for an easy code to read. But i have some confusions.

1) your attention functions takes the hidden state of input i.e lstm outputs from encoders and then does all the processes then. but according to what I have read , it must form some kind of function with the hidden state of the target , like in the given picture . Why haven't you did that ? otherwise you are just making an lstm function manually.

2) Why have you used permute layers before softmax layer ?
3) why have you averaged the outputs of softmax layer ?"
IndexError: list index out of range,"Dear sir: when I run python attention_dense.py ,the following errors show:

----- activations -----
Traceback (most recent call last):
  File ""attention_dense.py"", line 39, in <module>
    attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()
IndexError: list index out of range

would you please help me ?thank you very much!"
bucketing problem,"My sequences have varying lengths and I‚Äôm using bucketing to solve the issue. Therefore I define the LSTM input shape as (None, None, features), i.e. there are no explicit timesteps. I wonder if the code can fit my input? Thanks."
Ëøô‰∏™ÊòØCNNÁâàÊú¨ÁöÑattentionÂêóÔºü,Ëøô‰∏™ÊòØkerasÂÆûÁé∞ÁöÑCNNÂä†‰∏äattentionÁöÑ‰ª£Á†ÅÂêóÔºü
Questions on implementation details,"Update on 2019/2/14, nearly one year later:

The implementation in this repo is definitely bugged. Please refer to my implementation [in a reply below](https://github.com/philipperemy/keras-attention-mechanism/issues/14#issuecomment-371132446) for correction. My version has been working in our product since this thread and it outperforms both vanilla LSTM without attention and the incorrect version in this repo by a significant margin. I am not the only one raising the question [1].

Both this repo and my version of attention are intended for sequence-to-one networks (although it can be easily tweaked for seq2seq by replacing `h_t` with current state of the decoder step). If you are looking for a ready-to-use attention for sequence-to-sequence networks, check this out: https://github.com/farizrahman4u/seq2seq.

[1]: https://github.com/keras-team/keras/issues/4962#issuecomment-311885962

============Original answer==============

I am currently working on a text generation task and learnt attention from [TensorFlow tutorials](https://www.tensorflow.org/tutorials/seq2seq#intermediate). The implementation details seems quite different from your code.

This is how TensorFlow tutorial describes the process:

![image](https://user-images.githubusercontent.com/2417391/36968826-add96394-209e-11e8-801c-822a710157fa.png)

![image](https://user-images.githubusercontent.com/2417391/36968819-a598f03c-209e-11e8-99d9-c2a411e44303.png)

If I am understanding it correctly, all learnable parameters in the attention mechanism are stored in ![W], which has a shape of `(rnn_size, rnn_size)` (`rnn_size` is the size of hidden state). So first you need to use ![W] to calculate the score of each hidden state based on the value of the hidden state ![h_t] and ![h_s], but I am not seeing ![h_t] anywhere in your code. Instead, you applied a dense layer on all ![h_s]. And that means ![pre_act] __(Edit: h_t should be h_s in this equation)__  becomes the ![score] in the paper. This seems wrong.

In the next step you element-wise multiplies the attention weights with hidden states as equation (2). Then somehow missed the equation (3).

I noticed the tutorial is about Seq2Seq (Encoder-Decoder) model and your code is an RNN. Maybe that is why your code is different. Do you have any source on how attention is applied to a non Seq2Seq network?

Here is your code:

```python
def attention_3d_block(inputs):
    # inputs.shape = (batch_size, time_steps, input_dim)
    input_dim = int(inputs.shape[2])
    a = Permute((2, 1))(inputs)
    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.
    a = Dense(TIME_STEPS, activation='softmax')(a)
    if SINGLE_ATTENTION_VECTOR:
        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)
        a = RepeatVector(input_dim)(a)
    a_probs = Permute((2, 1), name='attention_vec')(a)
    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
    return output_attention_mul


def model_attention_applied_after_lstm():
    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))
    lstm_units = 32
    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)
    attention_mul = attention_3d_block(lstm_out)
    attention_mul = Flatten()(attention_mul)
    output = Dense(1, activation='sigmoid')(attention_mul)
    model = Model(input=[inputs], output=output)
    return model
```

[W]: https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cmathit%7B%5Cmathbf%7BW%7D%7D
[h_t]: https://latex.codecogs.com/gif.latex?%5Cinline%20h_t
[h_s]: https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cbar%7Bh%7D_s
[pre_act]: https://latex.codecogs.com/gif.latex?%5Cinline%20h_t%5Ctimes%20%5Cmathbf%7B%5Cmathit%7BW_%7Bt%2Ct%27%7D%7D%7D%20&plus;%20b_%7Bt%27%7D
[score]: https://latex.codecogs.com/gif.latex?%5Cinline%20score%28%5Cmathbf%7B%5Cmathit%7Bh_t%7D%7D%2C%5Cmathbf%7B%5Cmathit%7B%5Cbar%7Bh%7D_s%7D%7D%29"
Many to many sequence generation,"Can you give an example of how to use this for many to many sequence generation with different input and output lengths (greater than 1)? For example, if we have input of 10 timesteps say [1,2,3,4,5,6,7,8,9,10] and we want to generate output [1,10]. "
 SINGLE_ATTENTION_VECTOR = false,"Do you have some reference paper, about  **SINGLE_ATTENTION_VECTOR = false** ?

As far as I know, most of papers will set SINGLE_ATTENTION_VECTOR = true.


"
2D LSTM attention,Can we use the same code for 2D LSTM attention ?
Change before and after cases in the IF condition.,
possible bug in attention_lstm.py,"lines 56-59 should be 

```
if APPLY_ATTENTION_BEFORE_LSTM:
  m = model_attention_applied_before_lstm()
else:
  m = model_attention_applied_after_lstm()
```"
fig ,"Hi, I am wondering the figures in your markdown.
What app you used to create these beautiful hand-written figures. 
Thx"
use attention_3d_block in many to many mapping,"Hi, I'm beginner of Keras and tring to use attention_3d_block in translation module.
I have input of 5 sentences, each sentences has padding to 6 words, each word is presented in 620 dim(as embedding dim). 
And the output is 5 sentences, sentences padding to 9 words, and word is presented in 1-of-k in 30 dim(as vocabulary size)
How to use attention_3d_block in this scenario as the LSTM is many to many?

![s b3v8 0fr ex 3 he0wk](https://user-images.githubusercontent.com/21202514/27814158-4571e454-60ad-11e7-8028-bd8f38d593f0.png)
"
attention_lstm.py and Tensorflow,"In the attention_3d_block, I have some questions/bug (I think). I am running on Tensorflow. 
(1) inputs doesn't have a shape method. So it crashes. I assume you meant to call the shape function on  the numpy array on inputs_1.
(2) Is there a reason for calling Permute?
(3) What is the Reshape layer supposed to do? After the call to Permute, isn't the output of the previous permute layer already in shape (Batch Size, input_dim, TIME_STEPS)?
(4) The next call to Dense expects ndim =2, not 3. So the code crashes for me. I assume you meant the previous Reshape layer to map the 3d input to 2d?
(5) I would just like to point out that APPLY_ATTENTION_BEFORE_LSTM is False iff you call model_attention_applied_before_lstm. "
attention_lstm.py does not work for Theano backend,"When I run the script attention_lstm.py, there is a problem in the line 17. Just like the following problem:
""input_dim=int(inputs.shape[2])""
""TypeError: int() argument must be a string or a number, not 'TensorVariable'"""
Small Improvement,Please have a look at this issue for more information: https://github.com/philipperemy/keras-attention-mechanism/issues/3#issuecomment-309312748
Is this Reshape step redundant?,"See this line of code:  https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py#L19

Isnt this redundant?  Because the Permute layer right before it will reshape the Tensor.  

Let me know if I'm missing something.  I am trying to understand attention and thus far your writeup is helping"
edit get_activations,edit output list comprehension
get_activations not producing list,"Thanks for uploading this to github! Great for learning more about attention models. When I run attention_dense.py, however, I get this error (after the model finishing training):

> ---------------------------------------------------------------------------
> IndexError                                Traceback (most recent call last)
> <ipython-input-9-5fdd7f84f2d8> in <module>()
>      37     # Attention vector corresponds to the second matrix.
>      38     # The first one is the Inputs output.
> ---> 39     attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()
>      40     print('attention =', attention_vector)
>      41 
> 
> IndexError: list index out of range

Any idea why the get_activations function isn't working properly?"
Can't enable NEGEX ('--negex') option,"How can I use custom options? I rewrote function extract_concepts from SubprocessBackend class and add some custom options. I added line ""command.append('--negex')"", but there is no difference in result between this approach and the first one. How can I enable negation detection display?"
"Hi, is there anyone use MetaMapLite, I always got an error with no data file","<img width=""744"" alt=""image"" src=""https://user-images.githubusercontent.com/95683271/222970722-bfe7c3cf-8f69-47d0-87a2-c1bcbaf922d5.png"">
<img width=""344"" alt=""image"" src=""https://user-images.githubusercontent.com/95683271/222970739-6f7bdaa2-80fd-46a2-907f-80fd0b73684b.png"">
"
Value extraction from MetaMap,"Hi Anthony,
Thanks for the pymetamap wrapper
I want to extract values of the keys that are being extracted
for example:
input_sent = [""Patient:   John, parker            MRN: 456            FIN: 123            Age: 29""]
from this input it is able to extract ""Patient"" and ""Age"" word but I want to extract the name and age number and such information in key value pair
"
MetaMap ERROR even though servers are running,"Hi, thanks for making this very cool library. I'm having trouble getting the example to work. I get the MetaMap error that I see other people are getting, but I *do* have the servers running, as you can see in this screenshot.

![image](https://user-images.githubusercontent.com/9650729/150874874-9798d64b-e430-46d4-ba92-ff9b27147214.png)

Any ideas what's going on?

P.S. Sifei Han says ""hi"". ;-)"
IndexError: list index out of range,"Hi Antony,

Thanks for your great software. But when I use this tool to process MIMIC dataset, I got the following problem,

File ""/usr/local/python38/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/SubprocessBackend.py"", line 243, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/usr/local/python38/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/Concept.py"", line 75, in load
    if fields[1] == 'MMI':
IndexError: list index out of range

Thanks for your help!
"
Resource error: insufficient memory,"I have the problem of insufficient memory. The sever memory is 256 GB, so ideally, the size is enough. I added `-Xmx16g -Xms2g` to the metamap server scripts to start the services. But I still encounter the error:
```
0it [00:00, ?it/s]! Resource error: insufficient memory
0it [02:27, ?it/s]
Traceback (most recent call last):
  File ""get_concepts.py"", line 133, in <module>
    process_json(task_path, task)
  File ""get_concepts.py"", line 110, in process_json
    results = extract_entities(documents)
  File ""get_concepts.py"", line 87, in extract_entities
    concepts = mm.extract_concepts(
  File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/SubprocessBackend.py"", line 243, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/Concept.py"", line 75, in load
    if fields[1] == 'MMI':
IndexError: list index out of range
```
Here is my function:
```python
def extract_entities(docs):
    results = []
    mm = MetaMap.get_instance('/data/tools/public_mm/bin/metamap')
    for doc in docs:
        concepts = mm.extract_concepts(
            [doc], word_sense_disambiguation=True, unique_acronym_variants=True,
            ignore_stop_phrases=True, no_derivational_variants=True,
        )
        results.append(concepts)

    # below implementation will cause insuficient memory error, even with 16G XMx
    # results = [[]] * len(docs)
    # concepts = mm.extract_concepts(docs, ids=list(range(len(docs))), word_sense_disambiguation=True)
    # for concept in concepts:
    #     results[int(concept.index)].append(concept)

    return results
```"
Why I get different scores use the same code?,"I use the same code you report in the README section, like that:
`meatapath = ""/HardDisk/lsc_part/app/MetaMap/public_mm_lite/""
mm = MetaMapLite.get_instance(meatapath)
sents =  ['Heart Attack', 'John had a huge heart attack']

concepts, error = mm.extract_concepts(sents,[1,2])
for concept in concepts:
	print(concept)
	print(concept.trigger, concept.score)`
but I get the result like:
![image](https://user-images.githubusercontent.com/26566742/96360570-e8757600-1150-11eb-93fc-1e4249795277.png)
the score and tree_codes different from you result :
Concept(index='1', mm='MM', score='14.64', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""Heart attack""-tx-1-""Heart Attack""]', location='TX', pos_info='1:12', tree_codes='C14.280.647.500;C14.907.585.500')
Concept(index='2', mm='MM', score='13.22', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""Heart attack""-tx-1-""heart attack""]', location='TX', pos_info='17:12', tree_codes='C14.280.647.500;C14.907.585.500')"
Support negated concepts,"MetaMap allows detection of negated concepts via the `--negex` option.

**Command**
```
echo ""Fever, but no headache"" | metamap --negex
```

**Output**

```
Phrase: Fever,
Meta Mapping (1000):
  1000   FEVER (Fever) [Sign or Symptom]

Phrase: but

Phrase: no headache.
Meta Mapping (1000):
  1000 N HEADACHE (Headache) [Sign or Symptom]
```

Can you add this option to the `extract_concepts` function?"
Different order return different number of concept,"Hi Anthony,

Thanks for your great work! That's really helpful since I can do all the implementation in python without switching.
The issue for me is every time I shuffle the sentence list, the number of concepts is different. And if I run metamap again on  the non-recognizable words, some turn to be recognized, which makes me confused.

So in my implementation, I have to run metamap constantly on the currently non-recognized sentences until there's no concept return. Actually, I'm still not confident about this approach and kind of worried about what's going on.

For example, there are 1000 sentences in my list. I use extract_concept and there are 379 results. And I run extract_concept again on the remaining 621 results, it returns 89 results this time. I keep running on the remaining until it returns no concept.

I am not sure it's problem with metamap or pymetamap. Let me know if you have any thoughts! Thanks in advance!

Best,
Yiheng"
error while running extract_concepts,"I tried to run the sample input. I used MetaMaplite. I got the following error when using extract_concepts().

```
File ""/usr/local/lib/python3.6/dist-packages/pymetamap-0.2-py3.6.egg/pymetamap/SubprocessBackendLite.py"", line 102, in extract_concepts
    with open(output_file_name) as fd:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpey8rfuom.mmi'.
```

Can you please help? Thanks.

"
position returned by pymetamap,"Hi Anthony, 

Firstly, thank you for the wonderful implementation of metamap. 

However, I was running into some issues while extracting the keywords using pymetamap.

For example, in the sentence itself ""John had a huge heart-attack"", could you please direct me to how to extract the exact position of the keyword identified by pymetamap. It shows position = 17:12, but in several cases, I see the exact character position is off by 1-2 characters. 

Could you provide some insight into this?"
MetaMap Error,"Hey Anthony, thanks a lot for this making this really useful wrapper. I've installed MetaMap and cloned your repository, however when I run
`mm= MetaMap.get_instance('/Users/usr/Downloads/metamapfolder/public_mm/bin/metamap18')`

`mm.extract_concepts('some sentence')`

I get the following error:
`MetaMap ERROR: Calling socket_client_open for TAGGER Server on host localhost and port 1795:
error(system_error,system_error(SPIO_E_NET_CONNREFUSED))`

I know this might not be related to pymetamap but to MetaMap itself, but maybe you know what the issue could be? 

Thanks a lot and all the best."
added no_nums arg,"This commit adds the `no_nums=[]` kwarg to extract_concepts and follows the pattern for including `--no_nums` for the CLI based on this document:
https://metamap.nlm.nih.gov/Docs/FAQ/NoNums.pdf"
converting MetaMap returned output in bytes to str for common handlin‚Ä¶,‚Ä¶g of python 2 and python 3. Resolves issue #9 raised on 11th Dec 2019.
Error Running Example ,"TypeError Traceback (most recent call last)
in 
1 sents = ['Heart Attack']
----> 2 concepts,error = mm.extract_concepts(sents,[1])
3 concepts

~/pyte/pymetamap/SubprocessBackend.py in extract_concepts(self, sentences, ids, composite_phrase, filename, file_format, allow_acronym_variants, word_sense_disambiguation, allow_large_n, strict_model, relaxed_model, allow_overmatches, allow_concept_gaps, term_processing, no_derivational_variants, derivational_variants, ignore_word_order, unique_acronym_variants, prefer_multiple_concepts, ignore_stop_phrases, compute_all_mappings, mm_data_version, exclude_sources, restrict_to_sources, restrict_to_sts, exclude_sts)
152 if ids is not None:
153 for identifier, sentence in zip(ids, sentences):
--> 154 input_text += '{0!r}|{1!r}\n'.format(identifier, sentence).encode('utf8')
155 else:
156 for sentence in sentences:

TypeError: can only concatenate str (not ""bytes"") to str

I tried running the example and this is what I get"
1. In MetaMapLite public_mm_lite folder needs to be passed. Hence changed input parameter name to metamap_home to make it clear.  2. user access check added as done for MetaMap,
MetaMap ERROR,">>> concepts,error = mm.extract_concepts(sents,[1,2])
Berkeley DB databases (USAbase 2018AB strict model) are open.
Static variants will come from table varsan in /home/ztt/NYTexperiments/pymetamap-master/public_mm/DB/DB.USAbase.2018AB.strict.
Derivational Variants: Adj/noun ONLY.
Variant generation mode: static.

### MetaMap ERROR: Calling socket_client_open for TAGGER Server on host localhost and port 1795:
error(system_error,system_error(SPIO_E_NET_CONNREFUSED))
--------------------------------------------------------------------------------------
HelloÔºåhow can I solve the problem above?"
Output one result with highest score,"Hi,

I wonder is there a way to output only one result with the highest score for each detected entity? For example,

```
sents = ['metastatic disease']
concepts,error = mm.extract_concepts(sents)

concepts
```

```
[ConceptMMI(index='00000000', mm='MMI', score='14.64', preferred_name='Neoplasm Metastasis', cui='C0027627', semtypes='[neop]', trigger='[""metastatic disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes='C04.697.650;C23.550.727.650'),

 ConceptMMI(index='00000000', mm='MMI', score='5.18', preferred_name='Metastatic Neoplasm', cui='C2939420', semtypes='[neop]', trigger='[""Metastatic Disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes=''),
 
ConceptMMI(index='00000000', mm='MMI', score='5.18', preferred_name='Secondary Neoplasm', cui='C2939419', semtypes='[neop]', trigger='[""Metastatic disease""-tx-1-""metastatic disease""-noun-0]', location='TX', pos_info='1/18', tree_codes='')]
```
 only output the first one - 'Neoplasm Metastasis'.

Thanks,
Wei"
Avoiding creating temporary input and output files when input is list of sentences,"Resolving https://github.com/AnthonyMRios/pymetamap/issues/37
Have tested on 2016v2 and used it in [NegBio](https://github.com/ncbi-nlp/NegBio/blob/master/negbio/pipeline/dner_mm.py#L48)
`concepts, error = mm.extract_concepts(sents, ids)`

Though haven't tested on Windows, but it should resolve the issue: https://github.com/AnthonyMRios/pymetamap/issues/10
(As mentioned in https://github.com/AnthonyMRios/pymetamap/issues/9#issuecomment-281675040 using NamedTemporaryFile was the bottleneck.)
Note: echo -e option doesn't works on Windows cmd, but will work if one runs on something like gitbash.

Have followed the approach mentioned in Taymon's answer and the corresponding comments:
https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes
"
Check added for MetaMap file existence and executable access for the user,"As discussed in https://github.com/AnthonyMRios/pymetamap/issues/9#issuecomment-487552373
made the changes.

Followed Jay's answer in https://stackoverflow.com/questions/377017/test-if-executable-exists-in-python"
Avoiding NamedTemporaryFile when input is list of sentences,"MetaMap2016 Usage Notes mentions

> There are two ways to use MetaMap interactively, reading input text from the keyboard and seeing output on the screen:
> 1. metamap [ options ]
> then type your input text, e.g., lung cancer, at the \|:"" prompt.
> 2. echo lung cancer | metamap [ options ]
>
> For processing an input file:
> metamap [ options ] InputFile OutputFile

In pymetamap, we are using the option:
> metamap [ options ] InputFile OutputFile

This is being done by creating temporary input and output file when list of sentences are passed.

Isn't it better if we choose the option:
> echo lung cancer | metamap [ options ]

This will reduce the I/O operations time.

I was making an attempt to choose the above option for the input: list of sentences.
```
~/kaushik/$ echo -e  ""1|'Heart Attack'\n2|'John had a huge heart attack'"" | /home/kaushik/lib/MetaMap/2016v2/public_mm/bin/metamap16 -N -Q 4 -y --sldiID --silent
/home/kaushik/lib/MetaMap/2016v2/public_mm/bin/SKRrun.16 /home/kaushik/lib/MetaMap/2016v2/public_mm/bin/metamap16.BINARY.Linux --lexicon db -Z 2016AA -N -Q 4 -y --sldiID --silent
1|MMI|14.64|Myocardial Infarction|C0027051|[dsyn]|[""-- Heart Attack""-tx-1-""Heart Attack""-noun-0]|TX|1/12|C14.280.647.500;C14.907.585.500
2|MMI|13.22|Myocardial Infarction|C0027051|[dsyn]|[""-- Heart Attack""-tx-1-""heart attack""-noun-0]|TX|17/12|C14.280.647.500;C14.907.585.500
```

Even with the option --silent, I found that MetaMap prints the command as initial part of the output.
If we can identify the lines which corresponds to MMI output format, then this problem can be solved.
"
Adding python wrapper for MetaMapLite,Issue: https://github.com/AnthonyMRios/pymetamap/issues/14
Access metamap if client and server are on different machine?,"If the MetaMap server (mmserver)  is running on different machine other than the one the Java api client is running, we can specify the hostname of the MetaMap server when instantiating the api. 

```
MetaMapApi api = new MetaMapApiImpl(""resource.example.org"");
```
Is such option available in pymetamap?"
Accessing elements within extract_concepts,"Hello Anthony,

Your python wrapper package is great. Quick question: Is there a way to access specific concept attributes within the list that is outputted by the extract_concepts function?

Thanks for all your help.

Haley Howell "
"Added options: allow overmatches, concept gaps, term processing requested in issue #30",
"Added options: restrict_to_sts, exclude_sts to restrict/exclude semantic types",
Assertion error if Metamap binary path not absolute,
Term processing / restrict to semantic types,"Hi - thank you for such an awesome tool! Comparing the tool with the metamap UI, there seem to be a few parameters missing such as:

Being able to only return certain semantic types
Option to select term processing / overmatches / concept gaps

Would it be possible to add these to 'SubprocessBackend.py'?"
what is the utility of backend='subprocess',"Hi Dear Anthony,

Thanks for your great software, I have another two quick questions that, 

1. what is the utility of 

> backend='subprocess'?

Because when I using pymetamap to extract concepts, the screen will show a lot of intermediate processes, is this 'backend='subprocess'' used to make the process silent?

2. Is it possible to get concepts by multi-threading?

Because I am working on a pretty large dataset, thus if I extract their concepts one by one, it will take pretty long time, thus I am thinking is there any chance to use the python multi-threading feature, like this:

> from multiprocessing import Pool
> p = Pool(5)
> def f(x):       <--------------in the function I will extract concepts
...     return x*x 
...
> p.map(f, [1,2,3])

Thanks a million in advance!
"
TypeError: can only join an iterable,"Hi Anthony,

Thanks for your great program! It is super convenient and easy to use!

I have successfully installed everything and was trying to run that Example Usage. However, I got a TypeError like this:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""pymetamap/SubprocessBackend.py"", line 123, in extract_concepts
>     command.append(str(','.join(exclude_sources)))

I try to print the exclude_resources and command, it looks like:
> (False, ['/project/public_mm/bin/metamap16', '-N', '-Q', '4', '-e'])

So basically the exclude_resources is False, it can't be joined by the string method.

Would you please take a look at this error, was I making any mistake?

Thanks a million!"
Filter sources,Add filter sources option.
Interpreting SCORE,"Is there a way to interpret the scores that I get from `pymetamap`. I see that I get different scores for matches from metamap run on the command line and from the pymetamap. I assume this is because of different default settings in each case.
I am trying to see if I can set a certain threshold and filter bad matches. I dont know if this approach is correct.

Example:

`text: John had a huge heart attack`


metamap command line:
`score=901`
output:
   `901   -- Heart Attack (Myocardial Infarction) [Disease or Syndrome]`

pymetamap:
`Score=13.22`
output:
`ConceptMMI(index='2', mm='MMI', score='13.22', preferred_name='Myocardial Infarction', cui='C0027051', semtypes='[dsyn]', trigger='[""-- Heart Attack""-tx-1-""heart attack""-noun-0]', location='TX', pos_info='17/12', tree_codes='C14.280.647.500;C14.907.585.500')`

Thanks,
Deepak"
change source,"Hi Anthony,

Firstly thank you for making this API. I wanted to know if there is a way to restrict matches to a certain vocabulary. This is the set of vocabularies  options I can select in Batch Metamap (https://ii.nlm.nih.gov/vocabs/1718/RsourceFrame_USAbase.html).

Thanks,
Deepak"
"fixed warnings using ""pyton setup.py check --resturcutredtext""",It should fix the problem of README.
Metamap Installation Issue,"
Metamap version: 2014 for linux
System: Ubuntu 16.04 LTS
JAVA:1.8 jre
I was following the metamap installation instruction from  **https://metamap.nlm.nih.gov/Installation.shtml**. 
But when I test **echo ""lung cancer"" | ./bin/metamap -I** ,  I got 
Control options:
  composite_phrases=4
  lexicon=db
  mm_data_year=2014AA
  show_cuis
Processing 00000000.tx.1: lung cancer

Phrase: lung cancer
Meta Mapping (1000):
  1000   C0684249:LUNG CANCER (Carcinoma of lung) [Neoplastic Process]
Meta Mapping (1000):
  1000   C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]
Unlike what I looks like in the instruction:
Phrase: ""lung cancer""
Meta Candidates (8):
  1000 C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]
  1000 C0684249:Lung Cancer (Carcinoma of lung) [Neoplastic Process]
   861 C0006826:Cancer (Malignant Neoplasms) [Neoplastic Process]
   861 C0024109:Lung [Body Part, Organ, or Organ Component]
   861 C0998265:Cancer (Cancer Genus) [Invertebrate]
   861 C1278908:Lung (Entire lung) [Body Part, Organ, or Organ Component]
   861 C1306459:Cancer (Primary malignant neoplasm) [Neoplastic Process]
   768 C0032285:Pneumonia [Disease or Syndrome]
Meta Mapping (1000):
  1000 C0684249:Lung Cancer (Carcinoma of lung) [Neoplastic Process]
Meta Mapping (1000):
  1000 C0242379:Lung Cancer (Malignant neoplasm of lung) [Neoplastic Process]

And my servers works fine:
wei@wei-OptiPlex-790:/opt/public_mm$ sudo ./bin/skrmedpostctl start
$Starting skrmedpostctl: 
started.
wei@wei-OptiPlex-790:/opt/public_mm$ sudo ./bin/wsdserverctl start
$Starting wsdserverctl: 
started.
wei@wei-OptiPlex-790:/opt/public_mm$ loading properties file /opt/public_mm/WSD_Server/config/disambServer.cfg
WSD Server initializing disambiguation methods.
WSD Server databases and disambiguation methods have been initialized.

I totally dont understand.
Please Help!
"
Need help in running python wrapper for metamap,"> mm = MetaMap.get_instance('/opt/public_mm/bin/metamap12')

I cannot understand what do you mean by metamap binary? Can you share the link for downloading metamap binary?

I have following files in /public_mm/bin/ directory but couldn't figure out which one is binary:

>db_access.so metamap16.BINARY.Linux   nls_signal.so	 SKRrun.16.in
ginstall.tcl  metamap16.in	       qp_lexicon.so	 uninstall.sh.in
install.log   metamap16.sav	       qp_morph.so	 wsdserverctl.in
install.sh    metamap2016.TEMPLATE.in  SKRenv.16.in
install.tcl   metamap.in	       skrmedpostctl.in
"
dyld: Library not loaded,"When I run the example code, I got this error below. 
Any idea?

> dyld: Library not loaded: /usr/local/BerkeleyDB.4.8/lib/libdb-4.8.dylib
  Referenced from: /users/gracelee/documents/metamap/public_mm/bin/metamap14.BINARY.Darwin
  Reason: image not found
/users/gracelee/documents/metamap/public_mm/bin/metamap2014.TEMPLATE: line 146: 95257 Abort trap: 6           $COMMAND"
Add mm data version,
metamap,"<img width=""1280"" alt=""screen shot 2017-07-28 at 5 12 40 pm"" src=""https://user-images.githubusercontent.com/30526383/28715966-4e05c090-73b8-11e7-9ec6-59231773d624.png"">
I am getting output as empty list instead i should get list containing concepts could you please help me out with this . I want the similar output as yours.Does this code need any servers in metamap to run in the background?"
metamap,I have run the same code but the output it is giving is empty that is if i try to print the concepts it is giving empty list([]) for the text containing heart attack .
Update .gitignore,"I installed this as a submodule to my project. After running `python setup.py install`, it created two directories: 

```
build
dist
```

<img width=""662"" alt=""screen shot 2017-07-21 at 3 46 19 pm"" src=""https://user-images.githubusercontent.com/4664374/28479848-37fdf628-6e2c-11e7-81fa-1114a8e04b97.png"">


Adding this to the `.gitignore` and removed the `dists` directory"
Python 3 support,"Hi,

This pull requests fixes #11 
I tested it on python 3.4.3 and python 2.7.6.

In addition, the `str(metamap_filename)` conversion makes it possible to work with Pathlib's filenames in python 3.4"
How to use commands for Human Readable and Negation Detection with wrapper,"Hi there,

For MetaMap I know that the commands for Human Readable output and Negation Detection are -I and -negex respectively. How would I implement this with the python wrapper? I looked through the code and did not see a command that addresses these two options.

Thanks!"
Any support for MetaMapLite version?,"Hi, Thanks for your great program.

I wonder if you can make your program work with MetaMapLite version. MetaMap tends to be rigorous but slower than the lite version, and it would be nicer if you can support both versions. "
how to set options,"Dear Rios,

thanks for your great work, I can run it in my Mac computer now. 
Here I have one more questions:
 where can we set the output options like the metamap originally have, eg: set semtype to dysn only..."
extract_concepts not running," from pymetamap import MetaMap
>>> mm = MetaMap.get_instance('/Documents/public_mm/bin/metamap13')
>>> sents = ['Heart Attack', 'John had a huge heart attack']
>>> **concepts,error = mm.extract_concepts(sents,[1,2])**
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pymetamap/SubprocessBackend.py"", line 117, in extract_concepts
    metamap_process = subprocess.Popen(command, stdout=subprocess.PIPE)
  File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1343, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory

Getting this error for some time...Please help
"
String format issue with python3.4,"I ran into the following error with PyMetaMap on python 3.4:

> /SubprocessBackend.py"", line 80, in extract_concepts
 input_file.write(b'%r|%r\n' % (identifier, sentence))
TypeError:

 unsupported operand type(s) for %: 'bytes' and 'tuple'

And solved it by replacing the input string format with this:
`input_file.write(bytes(str(identifier) + '|' + sentence + '\n', encoding='UTF-8'))`

Bit of a hack, but works! "
Add Windows Support,#9 
Running error,"I have already installed the metamap13 and when i run the example under windows 7, i get the following error: 
![image](https://cloud.githubusercontent.com/assets/17524879/23212519/12852d3a-f942-11e6-8640-47010f42d5bb.png)
I have also already import the pymetamp and replace the address in the metamap.get_instance method:
![image](https://cloud.githubusercontent.com/assets/17524879/23212628/8f15431c-f942-11e6-841b-5d6647254e15.png)
I just don't know what to do, please help me ^^"
Added python 3.x support,"Hey @AnthonyMRios,

Can you please check this out? This pull request should be able to fix your issue #6 .

Best,
J"
installation error/Running error,"Installed the package fine but it wouln't run, i am not sure if this was an installation error;
`from pymetamap import MetaMap`
it return the error

> from Concept import ConceptMMI
> ImportError: No module named 'Concept'

Any idea what went wrong and how I could fix it?"
Add Compatibility with Python 3.5,
"TypeError: a bytes-like object is required, not 'str'","Hi,
I just discovered the Pymetamap package today and I am new to python.
I am using this package to analyze clinical trial inclusion criteria retrieved from mysql database in the form of dict object.
This is the object I took for experimental analysis:
`result_set={'criteria': 'Male physicians, ages 40 to 84. No history of stroke, myocardial infarction, cancer, or renal disease. No contraindications to aspirin or beta-carotene. No current usage of aspirin or Vitamin A tables greater than once per week.'}`

I first converted the dict object to string:
`str_json = json.dumps(result_set)
`
When I followed the example usage code and tried to run the line 
`     
concepts,error = mm.extract_concepts(str_json)
`

it returns the error: 

> TypeError: a bytes-like object is required, not 'str'

Then I tried to convert to bytes format by running:
`data=str.encode(str_json)
`
And checked the type of the newly generated object:
`type(data)`

It shows that data is of type 'bytes' already.

Thus I ran the concept extraction code again:
`concepts,error = mm.extract_concepts(data)
`

And it still returns the same error asking for a 'bytes-like object'.

Could you please help me figure out what is wrong here? Is there anything I should look into other than the data type conversion (since I already converted the data type)?

I am currently using Python 3 (Anaconda environment).

Thank you so much!!



Tianran
"
Added AA and UA concepts,
 __new__() takes exactly 11 arguments (9 given),"Hi, this is similar to the closed Issue, but I am also having trouble getting a particular list of sentences to return MetaMap concepts. I tried debugging this myself, but couldn't figure it out. I did make a little progress though:

With the following list of sentences: 

`test = ['The patient stated that her general practitioner (GP) and specialist did not think that the problems were the side effects of domperidone.', 'No further information was reported at the time of the report.', 'Other reference number include - MHRA: 01001307081; 01001307081: GB-JNJFOC-20150407438.', 'Duration of suspect drug ""Domperidone"" was reported as 5 years.', '""No medical assessment"".']` :

`concepts, error = mm.extract_concepts(test[0])` 

and 

`concepts, error = mm.extract_concepts(test[1])`

both work, but 

`concepts, error = mm.extract_concepts(test[0:1])` throws the error that I mentioned in the title. Full stack track reproduced below:

``` python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/SubprocessBackend.py"", line 132, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 39, in load
    corpus.append(Concept.from_mmi(line))
  File ""/work/vsocrates/venv/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 30, in from_mmi
    return this_class(**dict(zip(FIELD_NAMES, fields)))
TypeError: __new__() takes exactly 11 arguments (9 given)
__new__() takes exactly 11 arguments (9 given)
```

Thank you for your help! 
"
TypeError: __new__() takes exactly 11 arguments (10 given),"Hi Antony, 

I am trying to use your wrapper but I am having some issues with, but only when I submit my own text to extract the concepts from text. 

When I use the example text to run over MetaMap, everyting works just fine. But when I pass my own text, made out of title, abstract and meshcode for articles I get: 

```
concepts,error = mm.extract_concepts(sents,[1,2])
```

  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/SubprocessBackend.py"", line 132, in extract_concepts
    concepts = Corpus.load(output.splitlines())
  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 39, in load
    corpus.append(Concept.from_mmi(line))
  File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 30, in from_mmi
    return this_class(**dict(zip(FIELD_NAMES, fields)))
TypeError: **new**() takes exactly 11 arguments (10 given)

Any help would be welcome. 

[MetaMap_originalText.txt](https://github.com/AnthonyMRios/pymetamap/files/231943/MetaMap_originalText.txt)
"
Unable to use pymetamap in windows,"I installed the pymetamap setup in windows and try to use it, but the metamap instance was unable to extract concepts from the sentences. I tried to run the same code given i.e

> > > from pymetamap import MetaMap
> > > mm = MetaMap.get_instance('...../public_mm/bin/metamap14')
> > > sents = ['Heart Attack', 'John had a huge heart attack']
> > > concepts,error = mm.extract_concepts(sents,[1,2])
> > > but after this I get the error:
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""C:\Anaconda2\lib\site-packages\pymetamap-0.1-py2.7.egg\pymetamap\Subproc
> > > essBackend.py"", line 127, in extract_concepts
> > >     os.remove(input_file.name)
> > > WindowsError: [Error 32] The process cannot access the file because it is being
> > > used by another process: 'c:\users\abhila~1.kas\appdata\local\temp\tmp00rn
> > > u0'
> > > I have metamap14 installed on my pc.
"
What dataset does the pretrained weight train from?,Please tell us which dataset did the pretrained weight train from?
Updated Readme,"Changed the Run Section 
old
`python main.py config/test.json`

this one will not work with python2 also shows to give --config in python3 so changed to 
`python3 main.py --config config/test.json`"
Running Issue,"As I am not used to python, I need some help running the MobileNet. Please help :)

When I do 'python main.py config/test.json'
It prints out 
'main.py: error: unrecognized arguments: config/test.json
Add a config file using '--config file_name.json'

What do I have to do?

Thank you.
"
loss can't down,"Thank for your share? I have a question is as follows:
The loss function cannot be reduced and all samples are predicted to be a certain category

I am looking forward for your answer, thank you very much!"
how can i convert other pre-train weights to pkl format?,"i want to use MobileNet_v1_0.25_224 weights,but i don't know how to convert ckpt to pkl format. can you help me? Thanks.."
Some questions About mobilenet_v1.pkl,"Thank you for your job, Could you tell me How to get the pre-trained model. Is this that you use the ckpt  format file in Tensorflow/model  to convert or retrain with Imagenet data set   "
I have a question during code review.,"Hello. I'm studying about parameter reduction and newbie in deep learning stuffs.

I have a question about data_loader.py

I've ran main.py with config.json that included originally, and it works well i think.

but during data loading, there is only jpeg images and loaded only images without any kinds of class information.

So my question is,

How it(code) calculates predicted accuracy without image's original class information?

I can not find any kinds of loading class information in data_loader.py

plz help me guyz :)"
how to convert mobilenet_v1.pkl to tensorflow pb file?,"Could you guide how to convert mobilenet_v1.pkl to tensorflow pb file?

Thanks!"
How to use the pre-trained model?,"A little new to tf.
I get mobilenet_v1.pkl follow the link you give and load it with pickle.load, then I get a dict with 137 keys.
But how to restore the model form this dict?

Actually, I have implement a net follow this: https://arxiv.org/pdf/1712.07168.pdf.
In this paper, only part of the mobilenet is used as the encoder part. In this case, the pre-trained model can be used?"
Change: Small fixes,Slightly changes of not longer supported tf.to_float() and tf.diag_part() in equivalent tf2.10.0 supported functions
"Revert ""Modifications for TensorFlow 2.0 compatibility """,Reverts guillaumegenthial/tf_metrics#10
issue with installing the package (command errored out with exit status 1),"I attempted to install the package from Jupyter Notebook, using the code suggested in the raeadme file. Unfortunately, I obtain the following error. I also tried to install it from the terminal but nothing changes. Any clue about it? Any mistake in the code? Am I missing some required packages? Thanks 

<img width=""1135"" alt=""Schermata 2020-09-07 alle 17 50 45"" src=""https://user-images.githubusercontent.com/67710089/92464294-394a9400-f1cd-11ea-8a79-8327b3e0cb90.png"">
"
Modifications for TensorFlow 2.0 compatibility ,"Accessing TensorFlow 1.x functions from a TensorFlow 2.x environment requires the prefix `tf.compat.v1.`. This PR implements those changes.

Affected files: `tf_metrics/__init__.py`."
TypeError: '>' not supported between instances of 'NoneType' and 'int',"
/usr/local/lib/python3.6/dist-packages/tf_metrics/__init__.py in precision(labels, predictions, num_classes, pos_indices, weights, average)
     40     """"""
     41     cm, op = _streaming_confusion_matrix(
---> 42         labels, predictions, num_classes, weights)
     43     pr, _, _ = metrics_from_confusion_matrix(
     44         cm, pos_indices, average=average)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py in _streaming_confusion_matrix(labels, predictions, num_classes, weights)
    262 
    263   # Flatten the input if its rank > 1.
--> 264   if predictions.get_shape().ndims > 1:
    265     predictions = array_ops.reshape(predictions, [-1])
    266 

TypeError: '>' not supported between instances of 'NoneType' and 'int'

In python 3, the comparison operators raise an error. Refer to -> https://docs.python.org/3/whatsnew/3.0.html#ordering-comparisons
The ordering comparison operators (<, <=, >=, >) raise a TypeError exception when the operands don‚Äôt have a meaningful natural ordering. Thus, expressions like 1 < '', 0 > None or len <= len are no longer valid, and e.g. None < None raises TypeError instead of returning False. A corollary is that sorting a heterogeneous list no longer makes sense ‚Äì all the elements must be comparable to each other. Note that this does not apply to the == and != operators: objects of different incomparable types always compare unequal to each other."
remove tensorflow from install requires,
enable gpu only if nvidia-smi succeeded,"What about 

def has_gpu():
    try:
        p = Popen([""nvidia-smi""], stdout=PIPE)
        stdout, stderror = p.communicate()
        if p.returncode != 0:
          return False
        return True

?"
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"Hi there, thanks for the very useful lib. 

As soon as I installed tf_metrics on my azure VM, I started getting the libcublas error for importing tensorflow which never occurred before. What's the installation  requirements for tf_metrics? Is there any caution for installing tf_metrics within a specific environment?

I use tensorflow '1.12.0-rc0' gpu with cuda-9.2"
Explicitly specify the encoding while reading the readme file.,"Since the readme file contains a Unicode character, on some systems such as docker container when the terminal encoding is not proper, the setup fails with the following error:

```
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 300: ordinal not in range(128)
```

The error is generated while reading the readme file in setup.py

This pull request will resolve this issue. The proposed solution is tested for Python 2.7 and Python 3.5."
f1 scores are unstable with different evaluation batch size,"Hello~

i used `tf_metrics` in https://github.com/dsindex/BERT-BiLSTM-CRF-NER/blob/master/bert_lstm_ner.py

```
    estimator = tf.contrib.tpu.TPUEstimator(
        use_tpu=FLAGS.use_tpu,
        model_fn=model_fn,
        config=run_config,
        train_batch_size=FLAGS.train_batch_size,
        eval_batch_size=FLAGS.eval_batch_size,
        predict_batch_size=FLAGS.predict_batch_size)

...

        def metric_fn(label_ids, pred_ids, per_example_loss, input_mask):
                    # ['<pad>'] + [""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""B-MISC"", ""I-MISC"", ""X""]
                    indices = [2, 3, 4, 5, 6, 7, 8, 9]
                    precision = tf_metrics.precision(label_ids, pred_ids, num_labels, indices, input_mask)
                    recall = tf_metrics.recall(label_ids, pred_ids, num_labels, indices, input_mask)
                    f = tf_metrics.f1(label_ids, pred_ids, num_labels, indices, input_mask)
                    accuracy = tf.metrics.accuracy(label_ids, pred_ids, input_mask)
                    loss = tf.metrics.mean(per_example_loss)
                    return {
                        'eval_precision': precision,
                        'eval_recall': recall,
                        'eval_f': f,
                        'eval_accuracy': accuracy,
                        'eval_loss': loss,
                    }
                eval_metrics = (metric_fn, [label_ids, pred_ids, per_example_loss, input_mask])
                output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                    mode=mode,
                    loss=total_loss,
                    eval_metrics=eval_metrics,
                    scaffold_fn=scaffold_fn)

...
```

and the training script https://github.com/dsindex/BERT-BiLSTM-CRF-NER/blob/master/train.sh

```
python bert_lstm_ner.py   \
        --task_name=""NER""  \
        --do_train=True   \
        --use_feature_based=False \
        --do_predict=True \
        --use_crf=True \
        --data_dir=${CDIR}/NERdata  \
        --vocab_file=${bert_model_dir}/vocab.txt  \
        --do_lower_case=${lowercase} \
        --bert_config_file=${bert_model_dir}/bert_config.json \
        --init_checkpoint=${bert_model_dir}/bert_model.ckpt   \
        --max_seq_length=150   \
        --lstm_size=256 \
        --train_batch_size=32   \
        --eval_batch_size=128   \
        --predict_batch_size=128   \
        --bert_dropout_rate=0.1 \
        --bilstm_dropout_rate=0.1 \
        --learning_rate=2e-5   \
        --num_train_epochs=100   \
        --data_config_path=${CDIR}/data.conf \
        --output_dir=${CDIR}/output/result_dir/
```

the situation is, 

when i set the `train_batch_size` 32 -> 6 and `eval_batch_size` 128 to 6, `eval_f` scores are not same.
exactly speaking, the lower batch_size the higher `eval_f` score we have.
is it normal?"
support  auc?,support  auc?
what's the meaning of positive class in multi-class task?,"Hey man, I want to ask the meaning of parameter `pos_indices`.

Which is the `pos_indices` in a multi-class task like 'Image Classify'? You can not say that cat or dog is positive if they have no opposite class, like good and bad."
Add GPU supoprt,"Currently, tf_metrics install tensorflow CPU as the dependency.
I think it would be better that this module can be used in GPU enable environment.
So I edited `setup.py` to be check whether GPU is enable and install tensorflow according to it.

(thank for sharing awesome NER tool!! tf_ner and tf_metrics is very useful for me!!)"
