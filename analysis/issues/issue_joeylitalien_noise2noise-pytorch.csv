title,body
fit error  help,"model = Sequential()

model.add(Conv2D(32, kernel_size=(5, 9),
                 activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 4)))

model.add(Conv2D(16, kernel_size=(5, 7), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 3)))

model.add(Flatten())

#num_classes*4 =104 

model.add(Dense(num_classes*4, activation='sigmoid'))


model.compile(loss=keras.losses.binary_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

ValueError: Error when checking target: expected dense_1 to have shape (104,) but got array with shape (26,)

why？ help me~~~~thks~"
验证码重叠,
add a font file and fix the writing style,
4-6个字符能用一个模型实现吗？,你好，你的模型我成功复制了，4个字符，6个字符单独测试都识别率很高，有没有办法一个模型既支持4个字符又支持6个字符。
write labels err,"    writer.writerows(labels)
TypeError: a bytes-like object is required, not 'str'"
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,按照你上面的代码运行发现报这个错误，查了下说明超出了范围，不大理解为什么连接相加的时候出错了，谢谢告知
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1,"装好依赖，生成图片后，直接运行，报如下错误
picnum :  6000
6000 (20, 80)
6000 4
Traceback (most recent call last):
  File ""cnn_end2end_ocr.py"", line 76, in <module>
    c = np.concatenate((c0,c1,c2,c3),axis=1)
numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1"
Only extract one word from gumbel softmax,"In the code https://github.com/yala/text_nn/blob/master/rationale_net/utils/learn.py#L71-L85

```python
def get_hard_mask(z, return_ind=False):
    '''
        -z: torch Tensor where each element probablity of element
        being selected
        -args: experiment level config
        returns: A torch variable that is binary mask of z >= .5
    '''
    max_z, ind = torch.max(z, dim=-1)
    if return_ind:
        del z
        return ind
    masked = torch.ge(z, max_z.unsqueeze(-1)).float()
    del z
    return masked

```

because we take the max, usually, only one position will have the max value. 
In this case, if we have 100 words in the sentence, we only select one word as the rationale?
I thought we should select independently and choose those words with >0.5 probability.

Maybe we should change
```python
masked = torch.ge(z, max_z.unsqueeze(-1)).float()
```
to 
```python
masked = torch.ge(z, 0.5).float()
```
instead?"
Multiple GPUs is broken,"Hi Yala! 

Great package. Just letting you know, though, that computation on multiple GPU's is broken for two reasons:

1. The `model.py` file does not import 
``import torch.nn as nn``
that's an easy fix.

2. You have some class-attribute dependencies that are single-thread bound.
https://github.com/pytorch/pytorch/issues/8637

I'm not sure exactly what they are, but here is my error message, which matches the one in the issue I linked to above:

```
Traceback (most recent call last):
  File ""scripts/main.py"", line 35, in <module>
    epoch_stats, model, gen = train.train_model(train_data, dev_data, model, gen, args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 59, in train_model
    args=args)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/learn/train.py"", line 198, in run_epoch
    mask, z = gen(x_indx)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/_utils.py"", line 369, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker
    output = module(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/generator.py"", line 55, in forward
    activ = self.cnn(x)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 55, in forward
    activ = self._conv(x)
  File ""/auto/rcf-proj/ef/spangher/newspaper-pages/text_nn/rationale_net/models/cnn.py"", line 41, in _conv
    next_activ.append( conv(padded_activ) )
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rcf-40/spangher/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 200, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)```

Alex"
Gumbel softmax instead of Reinforce,"Hi,

Thank you for your code ! I have seen you wrote Gumbel instead of Reinforce but I don't understand exactly how Reinforce was implemented before as I don't see any average over Z samples. Here I see that Z is sampled only once https://github.com/yala/text_nn/blob/master/rationale_net/models/generator.py#L40 .

Did I miss something ?

Thank you very much for your help"
Is log_softmax function missing in the Line 39 of generator.py?,"I think log_softmax function should be added in Line 39 if we want to use the Gumbel Softmax.

`logits = F.log_softrmax(self.hidden(activ))`

If I'm wrong, please let me know. 
Thanks.

![image](https://user-images.githubusercontent.com/17995697/54492275-0d17c500-4900-11e9-8bee-addd81d35934.png)
"
Can this be applied to a regression problem?,Can this be applied to a regression problem?
Is it possible to share the pre-trained embedding for beer reviews? ,"From https://github.com/yala/text_nn/blob/master/rationale_net/utils/embedding.py#L38 , it's trying to find `review+wiki.filtered.200.txt.gz`, is it possible to share the pre-trained embedding for beer reviews? "
Tutorial,Added tutorial
gpu-trained on cpu fix,
Refactor,Add rationale export in results file and clean up code
FileNotFoundError: [Errno 2] No such file or directory: 'pickle_files/embeddings.p',"In dataset.py we read
    embedding_path = 'pickle_files/embeddings.p'
    word_to_indx_path = 'pickle_files/vocabIndxDict.p'
    embedding_tensor = pickle.load(open(embedding_path,'rb'))
    word_to_indx = pickle.load(open(word_to_indx_path,'rb'))
However, the two files are not in the zip-code?"
AttributeError: 'Namespace' object has no attribute 'use_as_tagger',"Running from command line:
`CUDA_VISIBLE_DEVICES=2 python -u scripts/main.py  --batch_size 64 --cuda --dataset full_beer --embedding 
glove --dropout 0.05 --weight_decay 5e-06 --num_layers 1 --model_form cnn --hidden_dim 100 --epochs 50 --init_lr 0.0001 --num_workers
 0 --objective cross_entropy --patience 5 --save_dir snapshot --train --test --results_path logs/adhoc_0.results  --gumbel_decay 1e-5 --get_rationales
 --aspect aroma --selection_lambda .005 --continuity_lambda .01`

Gives an error:

`
Traceback (most recent call last):
  File ""scripts/main.py"", line 30, in <module>
    gen, model = model_utils.get_model(args, embeddings, train_data)
  File ""/home/mglowacki/Desktop/RNN_yala_pytorch_2/rationale_net/utils/model.py"", line 12, in get_model
    if args.use_as_tagger == True:
AttributeError: 'Namespace' object has no attribute 'use_as_tagger'
`
I've checked sourcecode but there is no --use_as_tagger parameter in args.
Btw. in the same file `model.py` there is an additional problem because `nn` is not imported.

"
fixing 0-dim tensor (scalar) access,"New versions of PyTorch will require proper usage for accessing 0 dimensional Tensors:
`tensor.item()` instead of `tensor.data[0]`"
Tagging,
added confusion matrix info,
How can I be able to learn with your repository,"How can I be able to learn with your repository

Note: I am a complete Noobie in the world of C# Programming but with Knowledge in JavaScript"
HPO wiki page,"Small edit - in the code example on the wiki page for hyperparameter optimization, `transform.Logarithmic` should now be `transform.Log10`"
how can i train the Neural Network with my own Training Pictures?,"let's say i have a list of Images ..how do i convert my images into F64matrix Form so i can train them with my Neural Network ..and how do i test the Network with an Image at the End
could you please make an example of this because you didn't mention that in the examples, you also didn't mention how to test the Network using a real Image..

Thanks in Advance "
Access OOB data and OOB error calculations of Random Forest,"Hi
Can you add access to the Out-of-Bag data and/or Out-of-Data error calculations for Random Forests?

Love this project,
Thanks

"
A way to Save Bayesian Optimizer progress and continue later.,"Hello, 
I would like to use the Bayesian Optimizer for Hyperparameter tuning.
Is there a way to save the current status of the optimizer and then resume later. I could not find one... 
Also I could not figure out a way to pass a cancellation token.
Great project.
Thank you."
Code sharing,"Hi @mdabros,

For about 15 years, I was the main maintainer of a project called the Accord.NET Framework, which was mainly a machine learning/statistics processing framework for .NET. I have recently archived the project as I couldn't keep up updating it. If there is anything that you would ever find useful in Accord/its codebase, I just wanted to let you know that I have granted license to anyone who would like to, to reuse any piece of code I have written myself (as per noted in the headers of each file of the project) under the MIT or BSD licenses.

I still continue to receive requests on how to use/adapt existing features, even after the project has been archived, so I guess there are still useful things that have been implemented in that framework. If you would like to adapt any of those into SharpLearning, please let me know.

All the best and merry Christmas if you celebrate it!

Cesar

"
Exception when serializing neural net to XML,"Hello,

I am using SharpLearning.Neural version 0.31.8 in a .Net Core 3.1 project. After training a neural network and obtaining the predictor model, I try to serialize it to an XML file using the suggested procedure:

```csharp
GenericXmlDataContractSerializer xmlSerializer = new GenericXmlDataContractSerializer();
xmlSerializer.Serialize<IPredictorModel<double>>(_model, () => new StreamWriter(filePath));
```

It fails with the following SerializationException:

System.Runtime.Serialization.SerializationException: An object of type 'SharpLearning.InputOutput.Serialization.GenericXmlDataContractSerializer+GenericResolver' which derives from DataContractResolver returned false from its TryResolveType method when attempting to resolve the name for an object of type 'MathNet.Numerics.LinearAlgebra.Storage.DenseVectorStorage`1[[System.Single, System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]]', indicating that the resolution failed. Change the TryResolveType implementation to return true.

Any help?
"
SharpLearning.XGBoost.dll is not compatible with .net core,"Hi,
SharpLearning.XGBoost.dll is not compatible with .net core due to PicNet.XGBoost (0.2.1) dependency.
Checked on 0.31.8 version. And  Core 2.1 version
Do you plan to fix it?"
Continuously improving a neural network over time using small batches.,"Hey, first of thanks for a fantastic library!

The library is really easy and simple to use if you have a large dataset and want to train a network in one go.

But I'm building a DQN and I want to continuously improve a neural network from small batches of training data, with as little overhead as possible. Is that something that's easily possible in SharpLearning?

Right now, the only way I see it can be achive, is by doing something like this:

    var net = new NeuralNet();
    // ...

    while (true)
    {
        var learner = new NeuralNetLearner(net, new CopyTargetEncoder(), new SquareLoss());
        // ...
        net = learner.Learn(observations, targets);
    }

However there's a lot of overhead and data copying going on there. Are there better ways to go about it?

Thanks  :)

**Edit 1** : Seems like my example doesn't work either since the weights are randomized when a learning begins.

**Edit 2**: My seccond attempt, throws a nullreference exception on net.Forward(input, output). (Allthough I imagine that this is not a very good way to go about it either? And probably wrong on many levels 😊)

    var delta = Matrix<float>.Build.Dense(1, 1);
    var input = Matrix<float>.Build.Dense(inputCount, 1);
    var output = Matrix<float>.Build.Dense(1, 1);

    while (true)
    {
            PopulateInput(input)
            net.Forward(input, output);
            var expected = GetExpected();
            delta[0, 0] = (float)(expected - output[0, 0]);
            net.Backward(delta);
    }"
Serialization Exception,"Hi. 

Not sure what happened, but something happened :) 

1. I updated all projects from .NET Core 3.0 to 3.1 
2. Made some changes in MySQL DB 

Now, I get various errors from `GenericXmlDataContractSerializer`. Surprisingly, exception happens only when I build project the second time, after the first build it works fine. I mentioned DB, because I serialize trained model using `MemoryStream` and save it as a byte[] to the MySQL column of type LongBlob. I'm also using models from `ML.NET` and export / import them from DB the same way and they work fine, so probably DB is not an issue. All projects in the solution are built as x64. Serializer fails on any model, either `RandomForest` or `AdaBoost`, with the same exception. 

**The issue** 

1. build the project and start debugging 
2. create, train model, and save it to DB as byte array using `GetPredictor` method below 
3. select byte array from DB, deserialize to a model, provide test data and get estimate - **OK** 
4. stop debugging, repeat steps 1-3, now prediction method fails with the exception below - **NOT OK**

**The question**

Maybe somebody knows what could be the reason for serializer to fall with the exception? Also, can I serialize trained model to `MemoryStream` using different serializer, without `GenericXmlDataContractSerializer`? 

**Most common exception**
```
System.Runtime.Serialization.SerializationException: Element 'http://schemas.datacontract.org/2004/07/Core.Learners.SharpLearning.EngineSpace:Model' contains data from a type that maps to the name 'SharpLearning.RandomForest.Models:ClassificationForestModel'. The deserializer has no knowledge of any type that maps to this name. Consider changing the implementation of the ResolveName method on your DataContractResolver to return a non-null value for name 'ClassificationForestModel' and namespace 'SharpLearning.RandomForest.Models' 
```

**After updating all Nuget packages I got another exception only once** 
```
Invalid XML at line 1 or something like that
```

**Serializing trained model to byte array and save to DB** 

```C#
public virtual ResponseModel<byte> GetPredictor(IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<byte>();

  using (var memoryStream = new MemoryStream())
  {
    var processor = GetInput(columns, inputs, nameof(PredictorLabelsEnum.Emotion));
    var learner = new ClassificationRandomForestLearner();
    var serializer = new GenericXmlDataContractSerializer();
    var container = new MapModel<int, string>
    {
      Map = processor.Map,
      Model = learner.Learn(processor.Input.Observations, processor.Input.Targets)
    };
    
    serializer.Serialize(container, () => new StreamWriter(memoryStream));
    responseModel.Items = memoryStream.ToArray().ToList();
  }

  return responseModel;
}
```

**Deserializing model from DB stream and getting prediction** 

```C#
public virtual ResponseModel<string> GetEstimate(IEnumerable<byte> predictor, IDictionary<int, string> columns, IDataView inputs)
{
  var responseModel = new ResponseModel<string>();

  using (var memoryStream = new MemoryStream(predictor.ToArray()))
  {
    var processor = GetInput(columns, inputs);
    var serializer = new GenericXmlDataContractSerializer();
    var model = serializer.Deserialize<MapModel<int, string>>(() => new StreamReader(memoryStream));
    var predictions = model.Predict(processor.Input.Observations);

    responseModel.Items.Add(predictions.OrderByDescending(o => o.Key).First().Value);
  }

  return responseModel;
}
```

Method `GetInput` in the code above is just a conversion from `IDataView` format in ML.NET to `ObservationSet` format in `SharpLearning`. `MapModel` is a [wrapper](https://github.com/mdabros/SharpLearning/issues/132) that allows to save text labels along with numeric ones. "
[PR] The proj files have been updated to enable SourceLink,"CSProj files have been updated to enable SourceLink in your nuget
---

*[This pull request was created with an automated workflow]*

I noticed that your repository and Nuget package are important for our .NET community, but you still haven't enabled SourceLink.

**We have to take 2 steps:**
1) Please approve this pull request and make .NET a better place for .NET developers and their debugging.
2) **Then just upload the .snupkg file** to https://www.nuget.org/ (now you can find the snupkg file along with the .nuget file)

You can find more information about SourceLine at the following links  
https://github.com/dotnet/sourcelink
https://www.hanselman.com/blog/ExploringNETCoresSourceLinkSteppingIntoTheSourceCodeOfNuGetPackagesYouDontOwn.aspx

If you are interesting about this automated workflow and how it works  
https://github.com/JTOne123/GitHubMassUpdater

*If you notice any flaws, please comment and I will try to make fixes manually*
"
Is there a way to keep textual labels / targets as a part of the trained model?,"First of all, thank you for sharing this library. 
Second, would be great to make mapping between columns and feature names mode obvious. 
If model was serialized and saved on one computer and deserialized and loaded on the other one, then second computer will have no idea what's the meaning of labels / targets, because model keeps them as double values. 

**Save model**

```C#

var labels = new[] { ""Good"", ""Bad"", ""Average"" ...  };
var labelKeys = labels.Select((v, i) => (double) i);  // take label key instead of name 
var learner = new ClassificationDecisionTreeLearner();
var model = learner.Learn(items, labelKeys); // is there any reason not to use string labels instead of doubles?

using (var memoryStream = new MemoryStream())
{
  var serializer = new GenericXmlDataContractSerializer();
  serializer.Serialize<IPredictorModel<double>>(model, () => new StreamWriter(memoryStream));
  db.Save(memoryStream.ToArray());  // convert XML to byte[] and save as Blob to DB
}
```

**Load model**

```C#
var xmlModel = db.Get(...).AsBlob().GetBytes(); // load saved model from blob column in DB

using (var memoryStream = new MemoryStream(xmlModel))
{
  var serializer = new GenericXmlDataContractSerializer();
  var xml = serializer.Deserialize<IPredictorModel<double>>(() => new StreamReader(memoryStream));
}
```

As a result, loaded model has property `Targets` that contains some double values, like 1, 2, 3, 4 and there is no way to understand that initially they meant ""Good"", ""Bad"", etc 

**Question**

Is there a way to save original **string labels / targets** as a part of the model to make prediction results human-readable? "
0.31.8.0: Ensure deterministic order of results from multithreaded optimizers,"Fix #130 and make order of results deterministic for all optimizers when running with parallel execution. Results will now also be the same between single threaded and multithreaded execution.

This affects all optimizers supporting parallel execution:
 - `BayesianOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`
 - `GridSearchOptimizer`
 - `ParticleSwarmOptimizer`
 - `RandomSearchOptimizer`
"
Order of results from RandomSearch is not deterministic with different iteration counts.,"Running the `RandomSearchOptimizer` with `runPrallel=false`, so not multithreading, with 100 iterations and 120 iterations seem to provide different order of results. Expectation would be that the fist 100 iteration would be the same, and this is not currently the case.
This is most likely caused by the use of `ConcurrentBag` to collect the results, which does not guarantee order.

This might also affect other Optimizers supporting parallel execution,"
Issue with loading model using GenericXmlDataContractSerializer: The deserializer has no knowledge of any type that maps to this name,"Thank you so much for developing this package. It's been working smoothly on my computer, but I might need some help on generating a dll file for others to run on their computers. I tried using this nuget package [https://github.com/Fody/Costura/graphs/contributors](url) to compile sharplearning dlls into the project dll and adding all sharplearning xml files to embedded resources. But I keep getting the same error saying that ""Element 'http://schemas.microsoft.com/2003/10/Serialization/:anyType' contains data from a type that maps to the name 'SharpLearning.GradientBoost.Models:RegressionGradientBoostModel'. The deserializer has no knowledge of any type that maps to this name. "" I was wondering where the deserializer knowledge is stored and how do I add them to the project dll file. Any comment or suggestion is appreciated. Thank you!"
0.31.7.0: Refactor BaysianOptimizer and add parallel computation,"This pull request refactors the `BayesianOptimizer` implementation to be use the same principles as the `SMACOptimizer`. The two optimizers are both model based optimizers, and should therefore be very similar in implementation. The `BayesianOptimizer` can be viewed a basic implementation of model based optimization, which the `SMACOptimizer` builds a few tricks on top of.
A base class for model based optimizers seems to be the next logical step, but that will follow in a later pull request.

The refactoring enables use of the `BayesianOptimizer` in an ""open loop"" style just like the `SMACOptimizer`. See the unit tests for an example.

This pull request also adds the option of parallel computation to the `BayesianOptimizer`. This work was originally added in #119.

Note that when running in parallel, and using the `Optimize(Func<double[], OptimizerResult> functionToMinimize)` method, the order of the results will not be reproducible. The individuel results will remain the same, but the order of the results will vary between runs.

I recommend only using the parallel version if the provided `functionToMinimize` is running serial computation, and is slow to compute."
"Minor typo fixes in layers (release postponed, so no version incrementation)",Fix typo in comment of DropoutLayer and SoftMaxLayer. Misspelled gradients.
0.31.5.0: Add sigmoid activation function,"Added sigmoid activation function, sigmoid short derivative, sigmoid test.
"
"TrimSplitLineTrimColumnsToDictionary throws a ""key already exists"" exception","Hi guys!
First off, great job! SharpLearning is very useful and well built.

One small bug I found while mistakenly creating a dataset based on a CSV file without the headers line (when calling 'ToF64Matrix()').

In SharpLearning.InputOutput.Csv.CsvParser -> Dictionary<string, int> TrimSplitLineTrimColumnsToDictionary(string line)
there's an iteration over the headers line, but it assumes all headers are distinct (and also that it is the headers line) - therefore an exception of ""key already exists in dictionary"" is thrown.
I think it should check if there's a duplication and throw a more explanatory error message in such case.

Let me know if you want me to fix it and add a pull request."
0.31.6.0: Adds implicit and explicit conversions from double[][] to F64Matrix,"This is a simple way to address the need to expose an API surface that accepts double[][] rather than an F64Matrix. Instead of changing all interfaces and implementations, I have made double[][] implicitly convertible to F64Matrix. This can be considered a convenience and a temporary workaround for [#20](https://github.com/mdabros/SharpLearning/issues/20) and [#115 ](https://github.com/mdabros/SharpLearning/issues/115)."
Add parallelism to Bayesian Optimizer. Also allow resampling non-deterministic algorithms,"[https://github.com/mdabros/SharpLearning/pull/119](https://github.com/mdabros/SharpLearning/pull/119)
"
retrain a Model,"How to Retain a model with new data ?
"
For Multiple files ,"is there a way of loading multiple files of same schema ,or do i have to combine  all files in to one big giant file ?"
Excellent Work,Thanks for x boost GPU learners .
"Adds parellelism to Bayesian optimizer by default and adds support for non-deterministic algorithms (release postponed, so no version incrementation)","I've had a second look at the Bayesian Optimizer and have introduced parallelism by default as with other optimizers. I've also introduced support for non-deterministic algorithms that may return different results for identical parameters. This also entailed a change to the standard serial behaviour so that instead of skipping an evaluation if the parameters did not change from the previous run, it will now store the results for all evaluations and skip any that have been run before. This should result in a performance improvement for the serial behaviour but will consume some memory."
"Hard dependency from Microsoft.IdentityModel.Clients.ActiveDirectory, version 3.17.2.31801",In my test project by saving of gradient boosting model I’ve encounter strange implicit dependency from Microsoft.IdentityModel.Clients.ActiveDirectory. It doesn’t work (load assembly exception) in all referenced versions of this assembly except I reference pretty old one version 3.17.2. Even if I use dependentAssembly construction it doesn’t help. My project is net461.
"0.31.4.0: Add CrossValidationUtilities.GetKFoldCrossValidationIndexSets, Refactor CrossValidation.","Extract the internal `GetKFoldCrossValidationIndexSets` method form the `CrossValidation<T>` class. 
This enables calculation of KFold CrossValidation IndexSets for use outside the `CrossValidation<T>` it self.

Usage: 

```csharp
// Targets to create KFold Index Sets from.
var targets = new double[] { 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3 };
// Sampler to control the sampling of the sets. In this case Stratified.
var sampler = new StratifiedIndexSampler<double>(seed: 242);

var indexSets = CrossValidationUtilities.GetKFoldCrossValidationIndexSets(sampler,
    foldCount: 4, targets: targets);

foreach (var (trainingIndices, validationIndices) in indexSets)
{
    // Do model training and accumulate predictions,
    // to form a fully k-fold cross validated prediction array.
}
```

Note, that in the case of remainders from `samplesPerFold = targets.Length / foldCount`, the last validationIndices will contain the remaining values (making it larger compared to the others), and the last trainingIndices will exclude these (making it smaller than the others)."
SharpLearning.Core: Merge SharpLearning.Containers and SharpLearning.Common.Interfaces,"Once #20, and #112 has been completed, the code in SharpLearning.Containers should be reduced quite a bit, and only contain a few basic types and support extension methods. For this reason it would make sense to merge this with the common interfaces assembly into a SharpLearning.Core project that holds the basic essentials for SharpLearning."
Add multidimensional array extensions to replace the current properties and methods on the Matrix class,Switching from the matrix class to multidimensional arrays requires adding extension methods to provide to expose the same members that the current matrix class does.
Consolidate common project settings into Directory.Build.props,"Where possible, add common project settings to Directory.Build.props"
Update projects to target c# 7.3 to enable new language features,
Remove/clean unused types and classes,"Currently there are a few which contains functionality that is rarely used, or that doesn't quite fit into the current direction of the package. This includes:
 - `SharpLearning.Containers.Arithmetic`: MatrixF64 is mostly used as a container, and more efficient matrix arithmetic can found in other libraries, like mathnet.numerics.
 - `SharpLearning.Containers.ObservationTargetSet`: This can be replaced by using a value tuple instead.
 - `SharpLearning.Containers.ArrayExtensions`: Several methods are unused.
 - `SharpLearning.CrossValidation.ContinuousMungeAugmentator`: 
 - `SharpLearning.CrossValidation.NominalMungeAugmentator`:
"
"Update code style, line length, member order, etc.","Part of 2019 sommer cleaning to get the code base cleaned and more up to date. This pull request contains mainly code style changes. This includes:
 - Line lengths around 100 chars (unless specific circumstances, like constructor checks, and expected test results)
 - Remove and order usings.
 - Class member ordering to follow standard guidelines.
 - Use expression bodied methods where applicable.

The pull request also includes code changes to avoid duplicated code in a few places.

A few more exotic metrics has also been deleted:
 - DiscreteTargetMeanErrorRegressionMetric 
 - RocAucRegressionMetric

If anybody uses these, let me know, and I can readd them."
Make SharpLearning.Neural support .net core 2.0/.net standard 2.0 (update mathnet numerics to latest version (.NETStandard 2.0 compatible) ),"This pull request updates mathnet numerics to latest version (.NETStandard 2.0 compatible). This enables SharpLearning.Neural to have support for .net core 2.0/.net standard 2.0, and solves #26. "
Add azure pipelines yaml configuration,"This pull request adds `azure-pipelines.yaml` configuration to control CI and PR validation. This also adds both debug and release validation. Previously, only the release version was build and tested via CI. 

This addition is the bare minimum of the yaml configuration. BuildPlatform has not been added to the configuration yet. I could not make it work together with the dotnet build command. Information on this can be found here: [dotnet issue 10421](https://github.com/dotnet/cli/issues/10421). So currently, the build platform from the project files is used.

I am currently using 'tasks' for the individual steps, but it seems that 'scripts' are generally more popular for dotnet core pipelines. Here is a guide using scripts: [yaml-build-pipeline-net-core-azure-devops-tutorial)](https://www.nankov.com/posts/yaml-build-pipeline-net-core-azure-devops-tutorial)

Another example is the [TorchSharp pipeline](https://github.com/xamarin/TorchSharp/blob/master/azure-pipelines.yml), but that also handles additional steps for getting external resources etc.. 'Scripts' definitely seems more flexible than 'tasks', so I might switch to 'scripts' in the future when I have some more experience with them."
"Can this project be called under xamarin forms and run on android? I tried, loading the model failed.",
Wow! Amazing work! Thanks a lot!,"Hi!

I also wanted to thank you for this wonderful library! The API is super clean and love it so far.

One question I have is regarding the accuracy score like the accuracy_score() function in scikit learn that can be seen here: https://www.kaggle.com/mathvv/prediction-of-red-wine-quality-93-215
Like this:
`print('Random Forest:', accuracy_score(y_test, rf_pred)*100,'%')`
Which outputs this:
`Random Forest: 91.875 %`

Many thanks and congrats again!

Flo"
Understand class prediction results,"Hi,
maybe some silly questions...
I'm trying to train a random forest model with **two different classes**. I think I understood that the number of rows of the target vector must be equal to observations matrix (therefore regardless of the number of classes). So in the rows of the output vector I set the value 0 for the first class and 1 for the second class. **This is right?** 
I would also like to understand how to interpret the results, for example if for a set of features I have the prediction value 0.6 I must consider it a class of type ""0"" or a class of type ""1""? Do I have to cast to integer or I must to round it? Finally the ""variance"" values ​​contained in CertaintyPrediction indicates the probability of the prediction (greater is better)?
many thanks
"
Add support for simple linear/logistic regression,"You've done a great job with the more sophisticated algorithms: would it be possible, for completeness, to throw in linear/logistic regression? I imagine it would be fairly quick comparatively."
"0.31.1.0: Add SmacOptimizer, update argument names on HyperbandOptimizer and BayesianOptimizer, clean up SharpLearning.Optimization.Test project","This pull request adds the `SmacOptimizer` to `SharpLearning.Optimization`. The implementation is based on the paper: [Sequential Model-Based Optimization for General Algorithm Configuration](https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf).

The algorithm combines bayesian optimization with greedy local search based on the current top solutions.

The `SmacOptimizer` implements the regular `IOptimizer` interface, but also surfaces the two primary methods for running the algorithm `ProposeParameterSets` and `RunParameterSets`. This makes it possible to use the optimizer in an ""open-loop"" style, and allows the optimizer to be easily used from or combined with other optimizers. An examples could be using the scheduling technique from the `HyberbandOptimizer` together with the model based sampling from the `SmacOptimizer`.

An example showing ""open-loop"" use can be found in the `SmacOptimizerTest` class."
Remove resources and add culture initialiser for test projects,"This should fix issue #34, and fix the current issue with azure dev ops pipelines.

This adds an `AssemblyInitializeCultureTest` class for all unit test project, which sets the culture settings to `CultureInfo.InvariantCulture`. This should make the unit tests pass on machines with different culture settings.
```CSharp
    [TestClass]
    public class AssemblyInitializeCultureTest
    {
        [AssemblyInitialize]
        public static void AssemblyInitializeCultureTest_InvariantCulture(TestContext c)
        {
            CultureInfo culture = CultureInfo.InvariantCulture;
            CultureInfo.DefaultThreadCurrentCulture = culture;
            CultureInfo.DefaultThreadCurrentUICulture = culture;
            Thread.CurrentThread.CurrentCulture = culture;
            Thread.CurrentThread.CurrentUICulture = culture;
        }
    }
```

This pull request also removes the use of resources in all test projects, and instead uses a `DataSetUtilities` class to handle small test datasets. This also cleans up a lot of CsvParser code for loading the data."
Make individual Tree models public on ForestModels. This is a breaking change. Also clean up SharpLearning.RandomForest.Test,"This pull request makes the individual tree models public on the forest models: `RegressionForestModel` and `ClassificationForestModel`. This makes it possible to use the individual trees from the model for custom predictions, or for calculating statistics. For instance, using a different ensemble strategy than average for regression and majority vote for classification. Getting the predictions for the individual trees also makes it possible to calculate statistics on the predictions to see how much the ensemble of models agrees or disagress.

The tree models are accessed through the `.Trees` property of the forest models:

```CSharp
var learner = new RegressionRandomForestLearner();
var forest = learner.Learn(observations, targets);
var trees = forest.Trees;
```

The trees can then be used individually afterwards:

```CSharp
var prediction = trees.Select(t => t.Predict(observation)).Average();
```

Note that this is a **breaking change**, since the trees have been promoted from a private member to a public property. This means that it will not be possible to load ForestModels trained with earlier versions of SharpLearning into this version. So retraining of models from the `SharpLearning.RandomForest` project is mandatory if updating to 0.31.0.0 and newer versions. This is sadly the downside of serializing the model code directly instead of using a custom format, which is the current strategy in SharpLearning.

This should solve #101 and #94 "
 Individual trees prediction of the classification RF model," #94
Add an additional methods which provides individual trees prediction of the classification RF model.

I ‘m not sure the code is right .Please help me to check it.I need the methods a little hurry.Thanks very much!
JinHJ"
0.30.2: Add HyperbandOptimizer.OptimizeBest method,Add missing `OptimizeBest` method for the `HyperbandOptimizer`. This method returns the best result found by the optimizer
"0.30.1.0: Add HyperbandOptimizer, and update test adapters","This pull request adds the `HyperbandOptimizer` to `SharpLearning.Optimization`. The implementation is based on the original article [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) and the implementation by [fastml](http://fastml.com/tuning-hyperparams-fast-with-hyperband/).

Compared to the other optimizers from SharpLearning, Hyperband includes an extra parameter in the objective function, `unitsOfCompute`. Hyperband uses the `unitsOfCompute` parameter to control a budget of compute for each set of hyperparameters. Initially it will run each parameter set with very little compute budget to get a taste of how they perform. Then it takes the best performers and runs them on a larger budget.

The `unitOfCompute` parameter is used in the objective function, and could for instance be used to control the size of the training set, the number of trees in gradient boost, or the number of epochs for neural nets. One unit of compute could for instance be defined as 1000 samples in the training set. The `maximumUnitsOfCompute` is provided as an argument to the `HyperbandOptimizer`, and the optimzer will define a schedule for evaluating the hyperparameters on a budget.

A small experiment comparing the results and runtime of the `HyperbandOptimizer` vs. the `RandomSearchOptimizer`, optimizing a neural net (using CNTK) on the CIFAR-10 dataset:

**System**
**CPU: i7-4770**
**GPU: GTX1070**

The Hyperband optimizers uses the default parameters except for the `skipLastIterationOfEachRound` which is enabled for one of the runs. The default parameters result in a total of 209 different parameter sets tried with the Hyberband optimizers. The `unitsOfCompute` parameter in the objective function is used to control the size of the training set, where 1 unit of compute is set to 740 samples, which corresponds to the full training set size of 60.000 samples, when `maximumUnitsOfCompute` is set to 81, which is the default maximum. The full test set is used to track the test loss/accuracy in all rounds/iterations.

| Optimizer        | Time (hours)           | Test accuracy (%)  |
| ------------- |:-------------:| -----:|
| `RandomSearchOptimizer(iterations=100)`|  45.34 |  88.47 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=false)` |  10.46  |   87.93 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=true)`|  6.56  | 87.93 |

As can be seen the RandomSearchOptimizer finds a slightly better parameter set. However, the two Hyperband runs uses significant less time to find a solution that is nearly as good. In this case, skipping the last iteration of each round results in finding the same parameter set as when including all iterations. This will not be the case for all problem types, but it does provide a nice speed up for large problems.

In the end, Hyperband finds a solution that is almost as good, and reduces the time required by a factor of 4-7.

A future extension to the hyperband optimizer could be to use bayesian optimization instead of random search to select the parameter sets. This combination has proven very useful in: [Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/pdf/1807.01774.pdf)."
"Add support for Reinforcement Learning algorithms: QLearning, Sarsa etc.","There is currently support for most of the common (and some less common) ML algorithms in Sharp Learning. However, there does appear to be a lack in the area of Reinforcement Leaning and some might observe these algorithms are beginning to gain some traction.

If there is any appetite for extending into this area, I would propose as an initial baseline provision for QLearning and Sarsa, backed by Epsilon Greedy and Boltzmann approaches. A second stage could then continue with Thompson and UCB1 exploration, and finally the existing Neural Net and ensemble interfaces could probably produce a compound that resembled Deep Q Networks."
0.30.0: Optimizers Optimize method returns unfiltered results in chronological order,"The `Optimize ` method on the optimizers from `SharpLearning.Optimization` will now return all results, unfiltered and in chronological order. Before, the results would be filtered for `NaN` values and ordered from smallest to largest error. This change makes it easier to compare the iterations required to get a good solution between the optimizers. The `OptimizeBest` method, which returns the single best result from the optimizers, is unchanged and will provide exactly the same result as previously.

This is a potential breaking change, so if relaying on the order of the results from the `Optimize` method, these should now be sorted and/or filtered after the call to the optimizer. Like it is also done in the `OptimizeBest` method:

```CSharp
Optimize(functionToMinimize).Where(v => !double.IsNaN(v.Error)).OrderBy(r => r.Error).First();
```

Note, that the particle swarm optimizer only returns the latest result from each particle."
"0.29.1: Rename logarithmic transform to Log10 transform, and release contribution of parallel ParticleSwarm and parallel GlobalizedBoundedNelderMeadOptimizer.","This pull request renames the `Logarithmic` transform to `Log10` transform. This should fix issue #93.

This pull request also releases the contributions made by @jameschch in pull request #95, which adds parallel execution to the following optimizers:

 - `ParticleSwarmOptimizer`
 - `GlobalizedBoundedNelderMeadOptimizer`

and adds selection of the degree of parallelism to:

 - `GridSearchOptimizer`
 - `RandomSearchOptimizer`"
"Adds parellelism to Particle Swarm optimizer, Adds configurable maxim…",This introduces parallel evaluation to the particle swarm and Nelder Mead optimizers. This also introduces a configurable maximum degree of parallelism to Random and Grid optimizers. The automatic scheduler is not effective for long-running problems that themselves are multi-threaded. This setting allows the user to strictly control the number of parallel operations.
Random Forest: how can I get each tree's prediction ?,"hi,Mads,
I hope to know how can I get each tree's prediction or which category it choose when I finished trained a RF model for my classify task？It is useful for my task.
thanks！
"
Optimization: Rename Transforms.Logarithmic to Transforms.Log10,"Currently, the name of the logarithmic transform is misleading, since in the .Net world log refers to log2, and the [LogarithmicTransform](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.Optimization/Transforms/LogarithmicTransform.cs) from SharpLearning uses Log10. 

So the transform, and the enum should be renamed to illustrate the use of Log10. For instance:
 - `Transforms.Log10`
 - `Log10Transform`"
Add IParameterSpec to SharpLearning.Optimization,"Primary change is the addition of an `IParameterSpec` interface, that replaces the previous `ParameterBounds` type. Two concrete implementations have been added:
 - *GridParameterSpec*:  Usable when a fixed set of parameters, needs to be searched.
 - *MinMaxParameterSpec*: Direct replacement of `ParameterBounds`. used for sampling values in the range [min;max].

The addition of the `IParameterSpec` and the  `GridParameterSpec`, makes the `GridSearchOptimizer` use a similar set of bounds as all the other optimizers. This makes it easier to switch to the `GridSearchOptimizer` in scenarios where autofac or other dependency injection frameworks are used.

The addition of the `GridParameterSpec`, also makes it possible to limit the sampling of the `RandomSearchOptimizer` to a fixed set of values. For instance:

```csharp
var parameterSpecs = new IParameterSpec[] 
{
    new GridParameterSpec(1, 10, 15, 20, 25),
    new MinMaxParameterSpec(0.0, 100.0, Transform.Linear)
};
var optimizer = new RandomSearchOptimizer(parameterSpecs, iterations: 100);
var actual = optimizer.OptimizeBest(Minimize);
```

In the above, the `RandomSearchOptimizer` will only sample randomly between the fixed values `1, 10, 15, 20, 25`, for the first parameter."
Add serializable feature transforms,"This pull request enables serialization of the features transforms available in the *SharpLearning.FeatureTransformations* project.

This is related to issue #90"
Serialisation of MinMaxTransformer,"Firstly, thanks for this excellent library - it is pretty well exactly what i have been looking for.

I have not found a way to serialise a MinMaxTransformer after use during training of a regression model 

Is there some other approach i should be using to normalise new data points for prediction in subsequent processing?"
"Error when training with GPU -  ""Unknown linear updater grow_gpu_hist""","I have downloaded and installed CUDA, my GPU is benchmark ""ASUS ROG strix OC 1080TI"",
when the learning starts some console prints are shown (attached screenshots) some error messages and sometimes the program even crashes,
should i build the program in certain way/download CUB or something else i missed?
Please help!

thx,
![capture](https://user-images.githubusercontent.com/13719129/47250162-af9d4c00-d425-11e8-8f29-ed3c8f3526ba.PNG)

"
Add Activation Functions other than ReLU please!,
SEHException when using xgboost with gpu,"I have installed latest Cuda version (9.2), and when setting the tree method to GPU* i get an SEHException ""external component has thrown an exception"", should i compile the program in a certain way after cuda installation? What should i do?, thanks!"
"Add overload for CsvRowExtensions.ToF64Matrix and ToF64Vector, to support other converters ",
Multiple output regression,"Most regression models are hardcoded with learning method: Learn(F64Matrix observations, double[] targets), what if we had multiple predicted variables.
I want to add method Learn(F64Matrix observations, F64Matrix targets)"
Full access to trained model structures,"Currently the trained models only expose members from IPredictorModel. For NNs, its impossible to get to the layered structures of the best model. Extend models to expose this"
GBM prediction confidence,"Hi, is it possible to add an option for getting the confidence of a prediction of a GBM?
To know how much the prediction ""can be trusted""
How can i do it by my self?
Thanks!"
Sample weight for XGBoost,"In Python XGBoost one can provide weights for each row of the data, see http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit. I tried to look for a way to specify such weights in SharpLearning, but could not find it. Is this possible?"
 Add check to GBMDecisionTreeLearner so we wont use more features than we can,
CrossValidation CrossValidate ProbabilityPredictions error means?,"Hi,
when I run ""CrossValidation_CrossValidate_ProbabilityPredictions"" example with my data, it gives these values:
**Cross-validation error: 6.88921441267999**
**Training error: 6.86281040890985**
I searched a lot but couldn't find any documantation abput it? Could you explain what's this values mean, and for best prediction what they should be?
Thanks.
Regards!"
F# unit tests,Let’s „contaminate“ SharpLearning with some F# code. For data processing F# is quite good alternative to c#  
Text Classification,"I am pretty new to Machine Learning and all of these things
I am want to ask a question about text classification: 
Can I use this library for text classification and how? 



P.S I need to split one string and classify substrings by Categories\Groups\etc, for example ""Ray's Potato Chips with Ketchup taste 80g"" I need to split into 
Category ""Potato Chips"" 
Groups:""Ketchup"" 
Brand(or smthng):""Ray's"""
How to vectorize text?,"Hi, thanks for the great library!

My CSV has text in some of the columns. Some of them are categorical (e.g. month of the year) and some have free text (e.g., book title). Looks like `SharpLearning.InputOutput.Csv.CsvRowExtensions.ToF64Matrix` is trying to parse stringified numbers. What if my CSV consists of non-number values? Is there a recommended way or should I wire another lib to do TF-IDF/word2vec/char embedding/etc?"
How to load data from SQL server table,"All given samples contain CSV method only.

```
            #region Read data

            // Use StreamReader(filepath) when running from filesystem
            var parser = new CsvParser(() => new StringReader(Resources.winequality_white));
            var targetName = ""quality"";

            // read feature matrix (all columns different from the targetName)
            F64Matrix observations = parser.EnumerateRows(c => c != targetName).ToF64Matrix();

            // read targets
            var targets = parser.EnumerateRows(targetName).ToF64Vector();
```

I would like to load data from a List<Dto> object; how it can be possible?
Thanks!

edit:

```
    /// <summary>
    /// Parses the CsvRows to a double array. Only CsvRows with a single column can be used
    /// </summary>
    /// <param name=""dataRows""></param>
    /// <returns></returns>
    public static double[] ToF64Vector(this IEnumerable<CsvRow> dataRows)
    {
      if (dataRows.First<CsvRow>().ColumnNameToIndex.Count != 1)
        throw new ArgumentException(""Vector can only be genereded from a single column"");
      return dataRows.SelectMany<CsvRow, double>((Func<CsvRow, IEnumerable<double>>) (values => (IEnumerable<double>) values.Values.AsF64())).ToArray<double>();
    }
```

This code only works with CsvRow list."
Which model type should I use for financial price prediction?,"First of all thank you for the great library!
My question is simple: I want to predict next period price with pre-computed history values.
I have over 30 rows data for each price.
Price and datas are decimal.

**For example history:**
**Indicator1** - **Indicator 2** - **Indicator 3** - **Price**         - **Trend**
10,01121  - 23,56540    - 12.00001     - 12,23321   - UP
9,00001    - 3,00040    -   2.00001       - 1,23300     - DOWN
...
...
**And data to predict coming like** 
8,11211    - 1,00020    -   0.00021       - 3,5555     - **?**
I want to get TREND field.

Which model should I use? Any example will be perfect?
Regards!
"
Implement Coefficient of Determination (r-squared) metric,
Add ParameterType to optimization ParameterBound,"This pull request adds a `ParameterType` to the `ParameterBound` class in `SharpLearning.Optimization`. The parameter type specifies if the parameter is discrete or continous. This enables the parameter samplers used in the optimizers to sample from either a continous range or from a discrete range. Before this, all samplers would sample from a continous range.

For instance, during hyperparameter optimization of a gradient boost model, it makes more sense to sample the number of trees on a discrete range, and the learning rate on a continous range. If the number of trees is sampled on a continous range, the optimizer will try many continous values which will be cast to the same integer values in the learner. For instance 15.2325, 15.9343, and so on, will all be cast to 15. Using the discrete range will make the search more efficient.

The parameter type is specified on the `ParameterBound` class, together with the other options:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 10, max: 1000, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // iterations
    
    new ParameterBounds(min: 0.001, max:  0.2, transform: Transform.Logarithmic, 
         parameterType: ParameterType.Continuous), // learning rate
    
    new ParameterBounds(min: 1, max: 25, transform: Transform.Linear, 
         parameterType: ParameterType.Discrete), // maximumTreeDepth
    
    new ParameterBounds(min: 0.5, max: 1.0, transform: Transform.Linear, 
         parameterType: ParameterType.Continuous), // subSampleRatio
};
```

The default value of the `ParameterType` on `ParameterBounds` is still `Continous`, so the default behavior of the optimizers will not change. 
"
Add proper optimizer seeding to all optimizers in SharpLearning.Optimization,"This pull request will add proper seeding to all optimizer algorithms in `SharpLearning.Optimization`. The technique used is a single seed is set through the constructor, this seed is used to create a `random` generator, which will then create seeds for all underlying algorithms used in the given optimizer.

Note, that since this PR will change default seeding of the optimizers, the default behavior of the optimizers will not be the same as before this addition.

This should solve issue #69 . "
Nuget package for SharpLearning.XGBoost can't install,"
Currently there is an issue when installing the SharpLearning.XGBoost package, Nuget will try to add a reference for the native xgboost dll:

![image](https://user-images.githubusercontent.com/9001637/40295861-298b3238-5cdb-11e8-9426-f7646974f015.png)

The reason for this is how the nuget package for PicNet.XGBoost.Net has been created. I have openened an issue to get this solved: https://github.com/PicNet/XGBoost.Net/issues/24
"
[Question] How would you use the model to make predictions on new data?,"I have read all the examples and gone through the source code, but haven't been able to answer the question.

I have setup a data set, trained and tested the model, but now I would like to use the model to make predictions on new data. How would I achieve this?

Example:
Target value has 3 classifications: good, bad, average

New data comes in -> use trained/tested model to make a prediction on the target value. Also, would it be possible to get a probability/confidence of the prediction of the target value i.e. 25% good, 50% average, 25% bad.
"
Most implementations of IOptimizer don't properly pass on the random seed to all internally used algorithms,"Hi mdabros,

I love the Optimizer classes of SharpDevelop, I use them heavily for hyperparameter tuning.
But I would like to use differend random seeds and parts of the Optimizer classes don't allow using them like that.
Example:
If you look at the constructor of your BayesianOptimizer you can see that it doesn't pass on the ""seed"" parameter to all other classes that BayesianOptimizer creates instances of, sometimes it will just forward a hardcoded 42 instead. 
I know, 42 is the answer, but I would prefer ""seed"" in this case... ;)
The other IOptimizer implementations have similar issues.
Would be nice if you could modify that some time... 

Thank you!

Best regards
Florian
"
Add SharpLearning.XGBoost project,"Add a more efficient alternative to `SharpLearning.GradientBoost`. XGBoost is faster on CPU and also supports GPU learning. However, it does have native dependencies, so might not be ideal for all platforms and situations.

A small test comparing the `RegressionXGBoostLearner` and the `RegressionGradientBoostLearner` from SharpLearning on a medium sized regression task. 

Dataset: [YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)
Rows: 515345
Cols: 90

Hardware:
CPU: Core i7-4770
GPU: GTX-1070

Model parameters:
`MaximumTreeDepth`: 7
`Estimators`: 152
`colSampleByTree`: 0.45
`colSampleByLevel`: 0.77

Training time compared using XGBoost in `histogram` and `exact` mode on GPU and CPU:

![image](https://user-images.githubusercontent.com/9001637/39971261-51276bf6-56f8-11e8-9d59-b50c46d4af7f.png)


As can be seen, XGBoost can be up to 70 times faster, when using the histogram based tree method. Using the exact method, which is more similar to the method from SharpLearning.GradientBoost, the speed up is still around 10 when using GPU, and 5 when using CPU. 

Missing tasks before the PR can be completed:
 - [x] Add argument checks to learners.
 - [x] Add unit test of conversion class.
 - [x] Add index support for learners, and input checks.
 - [x] Add probability support for classifier model.
 - [x] Add more learner and model tests.
 - [x] In the classification model, consider removing the targetNameToTargetIndex member, and adopt XGBoost´s requirement of sequntial class labels starting at 0. Checks can be added to alert users before learning starts.
 - [x] Complete pull request to XGBoost.Net to enable GPU use and Booster selection.
 - [x] Add VariableImportance support to XGBoost models.
 - [x] Split objectives into regression and classification, so only compatible objectives are available for the learners .
 - [x] Consider splitting learners into `Linear`, `Tree` and `Dart`, to only show relevant hyperparameters for each in the constructors.
 - [x] Add enums for the DART specific parameters. 
 - [x] Complete pull request to XGBoost.Net to add DART parameters.
 - [x] Complete pull request to XGBoost.Net to fix `Booster.Dispose()`.
 - [x] Get XGBoost.Net to publish new nuget package.
 - [x] Change from local reference to updated XGBoost.Net package.
 - [x] Update readme.
 - [x] Check cross-validation and learning curves loops with XGBoost models (disposable).
 - [x] Package SharpLearing.XGBoost during build to avoid issue with ""dotnet pack"" and how the native dll is included in picnet.xgboost.net
 - [x] Add probability interfaces to xgboost classification learner.
 - [x] Add model converter from XGBoost to SharpLearing.GradientBoost. (Added but not completed).
 - [x] Consider using the SharpLearing.GradientBoost.Models instead of the XGBoost equivilants. This would enable standard serialization and features, and avoid having to deal with native resources when using the XGBoost models. (For now, it has been decided to use the XGBoostModels, and leave the conversion for another pull request)."
Fix the non-deterministic behaviour of the random forest and extra trees learners when running multi-threaded.,"This pull request will fix the non-deterministic behaviour of the random forest and extra trees learners. It will also ensure that the learners produce the same models when running single threaded and multi threaded.

The main changes to the learners are:
 - The random generator used for each tree is created before the learning process starts.
 - Each random generator is associated with a specific tree index.
 - The final order of the learned trees is made using the tree index.    

This pull request should solve issue #65 and is related to the previous pull request on that issue: #66 "
correct random seed problems in random forest learners,fix bug described in #65 
Random Forest: m_random and parallel RNG,"Hello, I would first like to thank you for making such an excellent and accessible repo. We're getting quite a bit of use out of it in multiple projects.

I've noticed that despite setting the seed in the Random Forest learner, I get slightly varying results from run-to-run given identical inputs. I suspect the problem is in the following code block:

```
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, new Random(m_random.Next())));
});
```

The parallel random number generation is a problem; the same random numbers are generated because the seed is set in the constructor, but there is a ""race"" between the threads to grab the next random number. Locking m_random would not help, I think. This should be fixed by generating a random number for each tree prior to entering the Parallel.ForEach loop, like so:

```
int[] randomNumbers = new int[m_trees];
for(int i = 0; i < randomNumbers.Length; i++)
{
   randomNumbers[i] = m_random.Next();
}
Parallel.ForEach(rangePartitioner, (work, loopState) =>
{
   results.Add(CreateTree(observations, targets, indices, randomNumbers[work]));
});
```

Let me know if I'm overlooking something. I'd be happy to fork and make a pull request.

Rob"
An item with the same key has already been added,"Hello,

Might not be an actual issue, but more of a question on how to handle my error. I want to feed data into the parser that is in the SharpLearning.InputOutput.Csv package. Below is my code:

`string rawTarget = Transformations.ReturnColumnAsCSVString(Data, OutputColumn);

System.Windows.Forms.Clipboard.SetText(rawTarget);

var targetparser = new CsvParser(() => new StringReader(rawTarget));

var targets2 = targetparser.EnumerateRows(OutputColumn).ToF64Vector();`

So, first I pull my data into a string, this results in a string looking like this:
`""Vwap"";7049.4;6983.3;6981.8;6871.0;6846.7;6811.0`
(obviously there is a lot more)

Then I use the Stringreader to read my string and parse it to an F64 vector.  The error I get is:
`An item with the same key has already been added.`

I have also tried to convert the string to a stream, then use the Streamreader but this results in the exact same error. I am at a loss at how to solve this. 

Hope anyone can provide a solution! Thanks in advance!
"
Strongly-named assemblies,"First, let me say thanks for the great package: it works much better in our app than our previous (non-learning) solution.

We've got one issue to report: in our next release, all the various subsystems need to be in signed assemblies, which means we can't at the moment use SharpLearning since it would need to be called by a signed assembly and is itself unsigned. Any chance we could see signed versions of the SharpLearning Nuget packages?

Thanks,

Alistair
"
Possible evolution: trained neuralnet predict C output,"Hello,
I created a NeuralNet and trained it on my computer. 
I wish to port it on an embedded target now and i asked myself if a C code output of predict for a trained network could be possible. Is that the case ? If yes could you tell me some advises where to look at ?

Thank you"
change unsafe code to safe code,
Add support for enabling/disabling messages from learners during training.,"Currently, some of the learners in SharpLearning will output information during the training period. This includes the early stopping with gradient boost and neural networks. It should be possible to enable and disable these messages, and preferably, also possible to choose where the messages should be outputted. For instance to  `Console`, `Trace`, or alternatively a log file. 

This could be made by adding an `Action` to receive the message. This should probably be part of a separate interface, for learners supporting this. Something like:

```csharp
public interface ILogger
{
   public Action<string> Log { get; set }
}
```

The learners would then add the message to the log. Likewise, other algorithms like the optimizers could also implement this interface."
OutOfMemory Exception in F64Matrix constructor - maximum array bounds exceeded,"Hi there!

First: Thanks for the great work, excellent design you have there!

I am experiencing an OutOfMemory Exception in the constructor of F64Matrix that does not really come from memory shortage but from the fact that F64Matrix internally uses a single one dimensional double array that can quite easily exceed .NETs internal boundaries of maximum array dimensions.

In my case I tried to create a F64Matrix with 10 mio rows and 55 columns.

My preferred suggestion would be to either abstract the matrix to an IF64Matrix interface that probably only consists of the At() method overloads. This would enable users to provide a custom implementation that is capable of handling larger amounts of data, if needed even by swapping data from and to disk.
Another solution could be to change the internal implementation of F64Matrix to use an array of double arrays, which I believe could also help.

Thanks for your help and keep up the excellent work!

Best regards

Florian"
Add ParameterBounds and hyper-parameter scale transform to optimization ,"This pull request adds a ParameterBounds type to optimization to make it more clear to the user how to setup min and max bounds for the optimization parameters. This is illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
    new ParameterBounds(min: -10.0, max: 10.0),
};
 ```
Note that the `GridSearchOptimizer` still uses jagged arrayes for defining parameter ranges. This is intended since a grid search typically invovles outlining all hyperparameters for the grid, and jagged arrays fits this purpose nicely.

It is now also possible to add a scale transform to the sampling of the hyper-parameters. This can be usefull when dealing with hyper-parameters like learning rate, that covers a large range close to zero, like 0.0001 to 1.0. In this case it would be better to sample using the logarithmic scale to get a better distribution of values across the entire range. Default transform is linear, and logarithmic scale can be applied like illustrated below:

```csharp
var parameters = new ParameterBounds[]
{
    new ParameterBounds(min: 100, max: 300, transform: Transform.Linear),
    new ParameterBounds(min: 0.001, max: 1.0, transform: Transform.Logarithmic),
};
 ```

Running the hyper-parameter tuning example from [SharpLearning.Examples](https://github.com/mdabros/SharpLearning.Examples/blob/master/src/Guides/HyperparameterTuningGuide.cs) before and after introducing logarithmic scale for the learning rate and subSampleRatio shows improved results:

**With linear scale**
Train error: 0.0174 - Test error: 0.3905

**With logarithmic scale**
Train error: 0.0011 - Test error: 0.3843

This will solve issue #57 .


"
Optimization: Add option for how to sample hyper parameters,"Currently, all optimizers in SharpLearning.Optimization use random uniform sampling for sampling hyper parameters from the provided min/max boundaries. This is not always optimal, for instance when dealing with a hyper parameters like learning rate that can span a large range of values, like 0.0001 to 1.0. Using random uniform sampling in this case might result in only sampling values in a small part of the range. In this case it would be much better to sample at uniform in the log space. Hence, it should be possible to select which space to sample from for each hyper parameter when setting og an optimizer. 

This should include at least random uniform form:
 - Linear (current method)
 - Logarithmic
 - Exponential

At the same time, setup of the hyper-parameter ranges could be changed from setting op an array of arrays to using a type to guide the user better:

**Current method**
```csharp
var parameters = new double[][]
{
   new double[] { 80, 300 }, // iterations (min: 80, max: 300)
   new double[] { 0.02, 0.2 }, // learning rate (min: 0.02, max: 0.2)
};
 ```

**Proposed method**
```csharp
var parameters = new OptimizerParameter[]
{
   new OptimizerParameter(min: 80, max: 300,  SamplingMethod.Linear), // iterations (min: 80, max: 300)
   new OptimizerParameter(min: 0.02, max: 0.2, SamplingMethod.Logarithmic),, // learning rate (min: 0.02, max: 0.2)
};
 ```"
[WIP] Backends testing and C# TF batch running,
[WIP] Add cntk cnn example (C# and Python),"Adding cntk convolutional neural network example in C# and Python [WIP].
 - [x] Examples should return the same result
 - [x] Examples should be as similar as possible


"
Add preserveObjectReferences option to GenericXmlDataContractSerializer,"The default settings for the GenericXmlDataContractSerializer is to perserve object references. This is required for the SharpLearning.Neural.Models to be serialized and deserialized correctly. However, for serializing simple types like a list of items, the resulting xml can be slightly more complicated. Therefore this pull request adds the option to not preserve object references when using the GenericXmlDataContractSerializer.

The recommended setting for serializing SharpLearning models is the default (true).

**XML with  perserveObjectReferences = true (default):**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" z:Id=""1"" z:Size=""5"" xmlns:z=""http://schemas.microsoft.com/2003/10/Serialization/"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key z:Id=""2"">Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""3"">Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""4"">Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""5"">Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key z:Id=""6"">Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```

**XML with  perserveObjectReferences = false:**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
  <KeyValueOfstringint>
    <Key>Test1</Key>
    <Value>0</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test2</Key>
    <Value>1</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test3</Key>
    <Value>2</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test4</Key>
    <Value>3</Value>
  </KeyValueOfstringint>
  <KeyValueOfstringint>
    <Key>Test5</Key>
    <Value>4</Value>
  </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```"
Random Forest Regression generates constant values for prediction,"Hi,

I am using random Forest for learning. The output I get results into the initial 40 or so values varying (Float values) but after that it's just constant. 

I was wondering if you have seen this behavior. The data has 24 features and 500 observations. I get the 42 first prediction varying but after that it's just constant. 

I can provide the data and the code if you would like that.

regards,
Avi"
Add cntk python simple mnist examples,"As a starting point for simple python mnist example using cntk, copied the original example from [cntk](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)

Currently, I have not found a method for running/debugging this from visual studio, only from command prompt. More information can be found in the [readme](https://github.com/mdabros/SharpLearning/blob/backends-cntk-python/python/src/CntkPython/README.md)

@nietras Currently, the simple mnist example from CNTK is quite different from corresponding example in Tensorflow. We should decide how much we want to modify the CNTK example, and/or the tensorflow example, to make them as similar as possible.

Some differences: 
 - The CNTK example uses text files as input and tensorflow the raw data.
 - The CNTK example also uses the Dense operator from the layer API.
 - The CNTK exmaple uses all 60k examples for training and tensorflow only 10k."
Consider switching to XorShift for RNG,"XorShift is a simple alternative to the built-in Random class in C#, which provide better performance and randomness. I benchmarked the RNG implementations of Math.net as seen in the picture. The XorHome in the list is my own quick port of xoroshiro128+ from here: http://xoroshiro.di.unimi.it/xoroshiro128plus.c

![image](https://user-images.githubusercontent.com/657616/35411685-ec14329a-0219-11e8-8e19-82716a4361ef.png)

As seen in the picture, the Math.Net XorShift is much faster than the built-in random."
Backends Tensorflow Deep Mnist Raw (to backends!),
[WIP] Backends Tensorflow Deep Mnist Raw,
Backends TensorFlow prototyping,"Create PR to more easily follow changes.

I made the mistake of merging with master, so perhaps ""backends"" should merge with master too?"
Add CntkRawMnistTest. Showing a simple cnn using the cntk api,Training a simple cnn using the cntk api. 
Backends new cntk test project,"Changing the `SharpLearning.Backend.Cntk.Test` project format from new csproj to old style .net framework csproj. Also, change platform target for the project to x64 to work with CNTK.

"
*NOMERGE* Cntk experiment,Create PR to easier review the changes in this.
Failing unit test ClassificationGradientBoostLearner_LearnWithEarlyStopping,"This always fails on when I run `all.ps1`
cc: @mdabros 
```
Failed   ClassificationGradientBoostLearner_LearnWithEarlyStopping
Error Message:
   Assert.AreEqual failed. Expected a difference no greater than <1E-06> between expected value <0.162790697674419> and actual value <0.13953488372093>.
Stack Trace:
   at SharpLearning.GradientBoost.Test.Learners.ClassificationGradientBoostLearnerTest.ClassificationGradientBoostLearner_LearnWithEarlyStopping() in E:\oss\SharpLearning\src\SharpLearning.GradientBoost.Test\Learners\ClassificationGradientBoostLearnerTest.cs:line 120
Standard Output Messages:


Debug Trace:
Iteration 1 Validation Error: 0.674418604651163
   Iteration 11 Validation Error: 0.290697674418605
   Iteration 21 Validation Error: 0.244186046511628
   Iteration 31 Validation Error: 0.22093023255814
   Iteration 41 Validation Error: 0.186046511627907
   Iteration 51 Validation Error: 0.186046511627907
   Iteration 61 Validation Error: 0.197674418604651
   Iteration 71 Validation Error: 0.174418604651163
   Iteration 81 Validation Error: 0.13953488372093
   Iteration 91 Validation Error: 0.162790697674419
```
Both for `Debug` and `Release`.
"
0.26.7.0: TimeSeriesCrossValidation,"With most time series data, it is not possible to use traditional cross-validation methods, like the CrossValidators available in SharpLearning. The reason for this is, that shuffling the data will result in the learner and model using future data to predict past observations.
While it is possible to use the NoShuffleTrainingTestSplitter to create a single split without chainging the order, this will limit the size of test set and for smaller datasets reduce the robustness of the test set error/generalization error.

For this reason, this pull request introduces the TimeSeriesCrossValidation<T> class. Time series cross-validation is based on rolling validation, where the original order of the data is kept, and new observations in the test interval are predicted as hold-out samples and following included in the model one at a time. A nice illustration of this can be found here: [Cross-validation for time series](https://robjhyndman.com/hyndsight/tscv/). 

The TimeSeriesCrossValidation<T> class supports the following features:
 - InitialTrainingSetSize: Specify how much data the initial learner/model should use.
 - maxTrainingSetSize: Specify a max size for the training interval. If no max size is specified, this will correspond to an expading training interval. If a max is specified, this will correspond to a sliding training interval.
 - retrainInterval: Specify how often the model being validated should be retrained. If no interval is specified, the model will be retrained each time a new time step is predicted and included. If an interval is specified, the model will only be retrained at the specified interval and the existing model will be used for validation predictions inbetween the retrain intervals.

More information can be found in the documentation of the code and the unit tests.

A short example showing how to measure the mean square error using The TimeSeriesCrossValidation<T>class:

```c#
var tsCv = new TimeSeriesCrossValidation<double>(initialTrainingSize: 30);

// Calculate the validated predictions.
var timeSeriesPredictions = tsCv.Validate(new RegressionDecisionTreeLearner(), observations, targets);
// Get the targets corresponding to the validation predictions. 
var timeSeriesTargets = tsCv.GetValidationTargets(targets);

// Measure the mean square error
var metric = new MeanSquaredErrorRegressionMetric();
var mse = metric.Error(timeSeriesTargets, timeSeriesPredictions);
```


 "
Improve SequentialModelBasedOptimizer (now BayesianOptimizer),"While attending the NIPS 2017 conference I was lucky enough to have Frank Hutter, the author and co-author of several [Bayesian Optimization papers](http://ml.informatik.uni-freiburg.de/people/hutter/publications.html), explain me some insightful details about this type of optimization.

This pull request contains improvements to the SequentialModelBasedOptimizer, now renamed to BayesianOptimizer, based on some of the insights provided by Frank Hutter.

The main changes are:

- Model type changed from RandomForest to ExtraTrees. The important change here is how the split in the decision trees are calculated. RF: (v1 + v2)/ 2 vs. ET: random * (max - min)  + min, where random is between 0 and 1.
- Optimizer type for finding the maximum of the acquisition function changed to RandomSearchOptimizer. This should matter less, but compared to the ParticleSwarmOptimizer, this seemed to work better in practice.
- Refactored the BayesianOptimizer to be easier to extend with other model types, optimizers and acquisition functions in the future.
  - Currently the model type, optimizer type and acquisition function is hardcoded:
    - Model type: ExtraTrees
    - Optimizer type: RandomSearchOptimizer
    - Acquisition function: Expected improvement
  - A natural extension would be to make it possible to inject other types. Like a Gaussian Process instead of the ExtraTrees model and so forth. These changes will be included in a later pull request.

I ran a small test to illustrate the improvements, optimizing the hyper parameters of a classification decision tree learner on the [landsat satellite dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)) from UCI. As can be seen on the attached image, the updated BayesianOptimizer uses significant less function evaluations compared with the old implementation (SequentialModelBasedOptimizer), and the RandomSearchOptimizer, while also finding a better minimum.

![image](https://user-images.githubusercontent.com/9001637/34309947-faac473c-e754-11e7-8c50-e51d42c00670.png)
"
Fix for AccessViolationException in F64MatrixView and F64MatrixColumnView for large matrices,"#38 

Changed the pointer offsets applied in `F64MatrixColumnView.RowPtr(row)` and `F64MatrixView.this[row]` to use `long` instead of `int` - integer overflows were occurring when the length of the underlying `double[]` in `F64Matrix` passed `int.MaxValue / sizeof(double)`."
0.26.5.1: Add .net461 to target frameworks,"When using .net standard 2.0 class libraries from a .net framework application, all system dependencies will be copied to the build output. To avoid this, a net461 build has been added to the target frameworks. This means that future SharpLearning nuget packages will contain both a .netstandard2.0 build and a .net461 build.

This might change in the future if .net standard changes the way dependencies are handled."
Add contribution guide,"Adding a contribution guide to make it easier for other developers to contribute to SharpLearning. Some parts of the guide could use more details, but this should provide a start and make the contribution process more clear."
System.AccessViolationException when retrieving data from a large dataset,"In `F64MatrixColumnView.RowPtr`, integer overflow can occur when `row * m_strideInBytes` is larger than `int.MaxValue`, resulting in an invalid offset being applied to `m_dataPtr`:

    double* RowPtr(int row)
    {
        return (double*)((byte*)m_dataPtr + row * m_strideInBytes);
    }

I am happy to fix this (it just requires a cast to `long`), but I'm not sure how to contribute - do I branch from master, then push and create a pull request from the GitHub website? In the future, if I find a simple bug like this, should I raise an issue, or can I just push with details and let you decide whether it's a good fix?

I haven't contributed to an open source project before!"
Unnecessary System Files Generated,"For our project [MetaMorpheus](https://github.com/smith-chem-wisc/MetaMorpheus), after adding Sharplearning NuGet Package to our EngineLayer and TaskLayer, in the GUI WPF project, there is an excessive amount of unnecessary system dll files generated in the output folder after building (No matter release or debug). Here is a list of these [files](https://github.com/smith-chem-wisc/MetaMorpheus/issues/767#issuecomment-349014733). We really couldn't determine where is the problem since there is no trace in the .csproj files and references of GUI nor related projects. So please help us if you have any idea! Thanks a lot.
"
"CS0012	The type 'Object' is defined in an assembly that is not referenced. You must add a reference to assembly 'netstandard, Version=2.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'.","Fresh download, after restoring packages via NuGet.

How to solve it????"
Backends prototyping,"Creating a PR for the backends stuff to more easily see the changes.

cc: @mdabros "
Unittests fail because of localization settings,"Some unittests compare against hardcoded strings written in the test method. These fail on systems that use ,(comma) as decimal separator instead of .(dot). These strings should probably be loaded from a resource or the entire library should work with invariant culture unless otherwise specified."
Predict overload for multiple observations added to IPredictor<TPrediction>,
Add TPrediction[] Predict(F64Matrix observations) to public interface IPredictor<TPrediction>,"I think it would be a good idea to add the `F64Matrix `overload to the `IPredictor `interface as it would make it easier to use the `IPredictorModel `interface in your code. The models seems to implement it already. 

It would add a dependency for `SharpLearning.Containers.Matrices` in `SharpLearning.Common.Interfaces`, but I think it is unlikely that you would use the SharpLearning library without referencing `SharpLearning.Containers.Matrices` anyway."
Duplicate efforts,"Hi @mdabros!

I've just found your library a couple days ago and couldn't help but notice the similarity between both of our projects, SharpLearning and [Accord.NET](https://github.com/accord-net/framework). Since we both share the same goal (bring serious machine learning to .NET), and instead of duplicating our efforts, wouldn't you be willing to join the Accord.NET project as well? 

Seeing your extremely well-organized repository and coding skills, you would be more than welcome in joining Accord.NET as one of its authors. 

Regards,
Cesar"
Change to unified project versioning,"After the migration from .net framework/desktop to .net core, I decided to switch to individual versioning for each project in SharpLearning. However, since vsts continuous integration/delivery currently does not support skipping already published nuget packages, I have decided to switch back to unified versioning across all projects. This is done differently with .net core projects compared to .net framework projects. I followed the advice from this answer on stackoverflow: [sharedassemblyinfo-equivalent-in-net-core-projects](https://stackoverflow.com/questions/42790536/sharedassemblyinfo-equivalent-in-net-core-projects). 

This solution also allows to have assembly attributes shared among the projects in one location."
Added dictionary version of KeyCombine which is a lot faster,moved unittests from CsvParserTests to CsvRowExtensionsTests
Add better error messages to learners when input data is not valid,"This pull request should provide better error messages and feedback from the learners when invalid input data is provided and solve issue #27. 

Added checks for all learners includes: 
 - Observations: Verify that row and column count is larger than zero.
 - Targets: Verify that row count is larger than zero
 - Observations and Targets: Verify that the row count of observations and targets are equal
 - Indices: Verify that there are no negative indices provided. Verify that the max index does not exceed the row count of observations and targets.  "
Better error messages from learners in case of dimensionality mismatch,"Currently, there are no checks to verify that the dimensions of the observation matrix and the target array matches before learning is started. This results in error messages from somewhere in the learner implementation, providing poor error messages and feedback to the user.

Checks should be added to all learners to ensure that the provided arguments and data is valid, before starting the learning process."
SharpLearning.Neural full .net core2.0/standard 2.0 support,"SharpLearning.Neural depends on [math.net](https://github.com/mathnet/mathnet-numerics), which does not currently support .net core 2.0/.net standard 2.0. Hence, SharpLearning.Neural will only work on .NET Desktop/Windows. 

Support for .net core 2.0/.net standard 2.0 is planned for math.net, so full support for SharpLearning.Neural will also be possible once this is implemented. Eventually, #9 might also solve this. "
Vsts continuous integration,"Merging the move to vsts continuous integration. This will also add continuous delivery, packing and pushing nuget packages with each commit to the master branch. The nuget steps are disabled for pull requests against the master branch."
Migrate everything to .NET Core 2 (.NET Standard 2.0),"In the end it was easier for me to start from scratch, so I didn't use the branch you had prepared (hopefully that's ok).

Here are a few things from the migration that I thought I should mention:
* I disabled the ""auto-generate AssemblyInfo"" feature of the new .csproj files so that existing `AssmeblyInfo.cs` continue to be used.
* I left the old `.nuspec` files in place.  These take precedence over the NuGet info in the new .csproj files.
* Updated the `SerializationString` value used in the `GenericBinarySerializer_Serialize` and `GenericBinarySerializer_Deserialize` tests to reflect the .NET Core behaviour of the `BinaryFormatter`."
AdaBoostLearners: Add subsample ratio pr. tree as hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, subSampleRatio, which controls how many training samples are forwarded to each tree in the ensemble. When subsampling is active, samples from the training data will be drawn with replacement, leading to more variation among the trees in the ensemble. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

In the RandomForest implementation of this feature, there is sampling with replacement, even if subSamplingRatio=1.0, this is part of the algorithms design. However, for the AdaBoost implementation of this feature, if subsampling is off (subSampleRatio=1.0), no sampling with replacement should be introduced, and the whole training set should be considered in each tree of the ensemble. This will result in the 'classic', AdaBoost algorithm, if the subSamplingRatio is set to 1.0. 

Besides the difference when subSampleRatio=1.0, the AdaBoost implementation should be very similar to the RandomForest implementation, which can be found here [RandomForest](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.RandomForest/Learners/ClassificationRandomForestLearner.cs)."
AdaBoostLearners: Add features pr. split as regularization hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, featuresPrSplit, which controls how many randomly selected features are considered during the decision trees search for a new split. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

Sine the DecisionTreeLearner used in adaboost already supports 'featuresPrSplit', the implementation should simply add the hyper parameter to the adaboost learner contructors and forward the parameter to the DecisionTreeLearner."
Better default parameters for DecisionTreeLearners,"Currently, the DecisionTreeLearners ([ClassificationDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/ClassificationDecisionTreeLearner.cs) and [RegressionDecisionTreeLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.DecisionTrees/Learners/RegressionDecisionTreeLearner.cs)) does not have very good default parameters. With a maximumTreeDepth=2000, using the default paramters will, in most cases, result in a model that overfits the problem. Hence, a better set of default paramters should be found, that, in more cases results in a better regularized model."
Replace SharpLearning.Containers.Matrices.F64Matrix with multidimensional array,"In SharpLearning, the F64Matrix class, which is part of the [Learner interfaces](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Common.Interfaces), is mostly used as a container for holding the features for a learning problem. While SharpLearning does contain some [arithmetic extensions](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Containers/Arithmetic) for the F64Matrix, the arithmetic is not used by any of the learners. Also, more efficient implementations can be found in [Math.net](https://github.com/mathnet/mathnet-numerics). 

Therefore it might indicate, that the primary container for features in SharpLearning should rather be a standard .net type like multidimensional array (double[,]) or jagged array (double[][]), with some extension methods to add the current functionality of the F64Matrix. 

An alternative, also suggested in #6, would be to replace the F64Matrix directly by using Math.net as the matrix provider. However, since only the SharpLearning.Neural project is using matrix arithmetic and with the plan of using [CNTK as backend](https://github.com/mdabros/SharpLearning/issues/9), math.net is a large dependency to take, if only using the matrix class as a feature container. So currently, I am leaning more towards replacing F64Matrix with a standard .net type. However, to better handle integration between Math.Net and SharpLearning, maybe a separate project, SharpLearning.MathNet, could be added with efficient conversions between Math.net and SharpLearning containers (both copy and shared memory). This of course depends on what data structure ends up replacing F64Matrix, if any.

These are my current thoughts, and people are very welcome to discuss and pitch in with suggestions. 
"
Metrics: Consider adding support for sample weighted metrics,"When dealing with imbalanced data sets, it can be beneficial to use sample weighted metrics. This task should be split into several tasks, one for each metric, if sample weights are to be supported in the [metrics project](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Metrics).  "
LearningCurves: Add support for weighted learners,"Extend the [ILearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/ILearningCurvesCalculator.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
Implement support for sample weights in [LearningCurvesCalculator](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/LearningCurves/LearningCurvesCalculator.cs). "
CrossValidation: Add support for weighted learners,"- Extend the [ICrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/ICrossValidation.cs) interface to support the IWeightedIndexedLearner interface. This depends on #14 being completed.
- Implement support for sample weights in [CrossValidation](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.CrossValidation/CrossValidators/CrossValidation.cs)."
Ensemble learners: Add support for sample weights,"Add support for sample weights to the Ensemble learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This task requires #14 to be done first, since the ensemble learners needs to be extended to also support weighted learners in the constructor.

Following the learners must implement weighted learner interfaces and should simply forward the sample weights the learners in the ensemble. The ensemble learners can be found here in the ensemble project: [Ensemble learners](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Ensemble/Learners)"
NeuralNet: Add support for sample weights,"Add support for sample weights to the NeuralNet learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This is currently on-hold until #9 has been decided."
Add IWeigtedLearner and IWeigtedIndexedLearner interfaces to Common.Interfaces,"Add interface for learners supporting sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights);
```

Add interface for learners supporting sample indices and sample weights:

```csharp
IPredictorModel<TPrediction> Learn(F64Matrix observations, double[] targets, 
double[] sampleWeights, int[] indices);
```
"
GradientBoost: Add support for sample weights,"Add support for sample weights to the GradientBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

The GBMDecisionTreeLearner, used by GradientBoost does not support sample weights, so adding sample weight support to the GradientBoost learners requires first adding it to the GBMDecisionTreeLearner. Adding sample weight support to the GBMDecisionTreeLearner, primarely requires using the wieghts in the loss functions: [GradientBoost Loss](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Loss)

Following, sample wieght support can be added to the GradientBoost learners. The learners can be found here in the GradientBoost project: [GradientBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.GradientBoost/Learners)"
ExtremelyRandomizedTrees: Add support for sample weights,"Add support for sample weights to the ExtremelyRandomizedTrees learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by ExtremelyRandomizedTrees, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project:
 [ExtremelyRandomizedTrees](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
RandomForest: Add support for sample weights,"Add support for sample weights to the RandomForest learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by RandomForest, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project: [RandomForest](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)"
AdaBoost: Add support for sample weights,"Add support for sample weights to the AdaBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by AdaBoost, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner in each boosting iteration. The learners can be found here in the AdaBoost project: [AdaBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.AdaBoost/Learners)

The work is currently in progress in the branch [adaboost-sample-weight-support](https://github.com/mdabros/SharpLearning/tree/adaboost-sample-weight-support)"
CNTK as backend for SharpLearning.Neural,"The Microsoft team working on [CNTK](https://github.com/Microsoft/CNTK) has recently released the initial version of the C#/.Net API with support for both evaluation and training of neural networks. A more feature complete version, with support for layers and other helpful features, should arrive before the end of the year. Currently, there seems to be a few performance related issues (https://github.com/Microsoft/CNTK/issues/2374 and https://github.com/Microsoft/CNTK/issues/2386) but hopefully these will be also be solved in the next release.

Using CNTK as backend for SharpLearning.Neural will add operators for more layer and network types, while also enabling GPU training and evaluation. Using a well supported deep learning toolkit as backend will also help to ensure that future operator, layer and network types will be available faster.

This task will require a large rewrite of SharpLearning.Neural, most likely only keeping the top level interface. However, since all the core operations are availible from CNTK, most of the hard work is already completed.

This task should be split into multiple others when a design of how CNTK should be integrated has been completed. A few considerations:
- Should the integrations be ""simple"", i.e. only have a NeuralNetLearner and NeuralNetModel in SharpLearning and use the layer construction and related functionality from CNTK directly?
- Should the integration hide CNTK behind an adapter to make it easier to support other deep learning toolkits like TensorFlow(Sharp)? "
.Net Core and .Net Standard support,"Add .NET Core and .Net Standard support to make SharpLearning available on more platforms. Porting to .Net Standard involves the following tasks:

- Retargeting the projects .NET Framework version to .NET Framework 4.6.2.
- Determining the portability of the code using API Portability Analyzer. This has been done, and only the GenericXmlDataContractSerializer from SharpLearning.InputOutput uses unsupported API calls.
- Change the implementation of GenericXmlDataContractSerializer to conform with .net standard 2.0. This is possible with the available API calls, however there are issues with serializing some of the Math.net containers used in the NeuralNet models. This might be solved together with #9, since CNTK will most likely replace math.net in the SharpLearning.Neural project.
- Change project format to .net core.

After the porting process the continuous integration on appveyor must be updated."
"Exception: Source array was not long enough. Check srcIndex and length, and the array's lower bounds","The following code throws a System.IndexOutOfRangeException on line 328 in GBMDecisionTreeLearner.cs

```
            var sut = new RegressionSquareLossGradientBoostLearner();

            Random rnd = new Random(42);
            var rows = 10000;
            var columns = 1;
            double[] values = new double[rows * columns];
            for (int i = 0; i < rows * columns; i++)
                values[i] = rnd.NextDouble();
            Containers.Matrices.F64Matrix observations = new Containers.Matrices.F64Matrix(values, 1, 10000);
            double[] targets = new double[rows];
            for (int i = 0; i < rows; i++)
                targets[i] = rnd.NextDouble();

            var model = sut.Learn(observations, targets);
```"
Math.NET Matrices,"This is a really great library. Was there a specific reason why you chose to roll your own Matrix class, rather than leveraging Math.NET?

Ideally I'd like to marry the two (not only for consistency with modules I've already written, but even for smaller things like using Matrix<float> rather than Matrix<double>). Before I jump in and start changing anything, though, I thought I'd check with the author to see if there was a specific reason behind it.

If I do proceed with integrating the two, more than happy to submit back a PR too, just let me know."
Thanks for developing and sharing this brilliant machine learning package in C#,
Please share your vision of .NET deep Learning,"@mdabros Pls apologize if I hijack your excellent work here.

Daniel from MSFT is gathering [a broad vision for .NET Deep Learning here.](https://github.com/Microsoft/CNTK/issues/960#issuecomment-315049580)  I think you may have unique view on this.  "
[Question] 2d - 3d output in neural networks,"Is it possible ? 
And if it's possible, how to train the network ?"
Feature Suggestion,"
First of all, I want to congratulate you for this project. I have a suggestion, couldn't figure where to write it other than issues on GitHub.

My suggestion is, number of observations (or better their indexes, one can count them) that fall to the left and right child of a node.
"
"Restructure repo, add scripts and nuget packaging",
请问数据集来源是？,
python demo_single.py ---->TypeError: can't pickle weakref objects,"Traceback (most recent call last):
  File ""demo_single.py"", line 36, in <module>
    pickle.dump(clf, f)
TypeError: can't pickle weakref objects
"
抱歉看错了,为什么二分类任务用categorical_crossentropy，多分类任务用binary_creossentory
predict的label严重不准确的问题,"道友你好~
使用DEMO的法律数据进行测试发现,predict的label几乎(95%以上)全部是一样的
我查看了前三位的索引值
prediction.argsort()[-3:][::-1]
类似:
[139,34,55]
[139,34,55]
........
[139,55,34]

基本完全一样

我发现prediction内的值是不一样的,但最高的几个值都是在固定位置
换成我自己用的数据(标签更丰富)进行测试,同样的问题,每次都还是几个固定的label
不明白是哪里出了问题





"
验证,可以画个roc-auc曲线对模型的性能做个评估
请问一下，Ali打标签中，org_code指的是什么？,是regionID还是？
Gutenberg file naming changed,"I believe the files changed a little bit on the Gutenberg website. 
The ASCII versions are now (mostly) found under an ""old"" folder. 
Also, this might be due to how my internet is set-up locally, but downloading via http creates empty files, hence I changed to https.

For the Pride and Prejudice book I couldn't find the same file, This one seems to be in UTF-8. Nonetheless everything seems to be fine with the pushed code. 

Best,"
Add soft K means,"http://www.inference.org.uk/mackay/itprnn/ps/284.292.pdf
http://www.inference.org.uk/mackay/itprnn/code/kmeans/

Example code:
```q
\l funq.q
\l iris.q
X:iris.X
\S -314159i
c:.ml.forgy[3] X
/ 
these all return the same answer (within epsilon 5e-5)
5.006    3.428    1.462    0.246   
5.901613 2.748387 4.393548 1.433871
6.85     3.073684 5.742105 2.071053
\
asc flip .ml.kmeans[X] over c
asc flip .ml.kmeanss[X] over c
asc flip .ml.kmeans1[100;X] over c1
```"
How to compile the code?,"I work under win10, win32-bit q . Useing MSYS2 to Compile., which is armed by the following packages:

- tar
- make
- patch
- mingw-w64-i686-gcc-fortran

under the target folder, type make much errors appear, as the attach file. NEED HELP!
[error.txt](https://github.com/psaris/funq/files/6866296/error.txt)
"
possible typo in book,"First congrats on the book! Looks great, very excited to go through it. Not sure if this is the right place to report typos but on p 16, section 2.4 in it refers to ""3...independent features (petal and sepal measurements..."" and ""4...dependent feature (species label)"" and then at the bottom of the page it says ""we have access to the independent (species labels) and dependent (sepal and petal measurements)""  The independent/dependent labels are reversed. I believe the first labels are correct but figured i'd let you know."
fixed weighted odds,Added missing multiplication operation.
Take advantage of symmetry of cNk (Pascal),More stable and faster for large n
fixing typos,
fix typos in comments,
fix typo in comments,
fix two typos in comments,
fix two typos,
Remove trailing whitespace,
fix correlated matrix in linreg.q,"was `X[0]` which produced results for `rho` as if it was `1-rho`. 
Test: plot for `rho:1f` should show a line but was showing circle"
Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5×3 positions and 5×4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5×3 positions and 5×4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4×3 positions and 4×4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter network。
"
Problem in training the segmentation network using the provided dataset,
Pose estimation has huge position and orientation error for one object and does not appear to decrease,"Hello,

So I generated my own training set and am attempting to train the pose estimation portion with just one object, I was able to go through all the steps to get the CAD model properly setup, the PCD files and do all the stuff with the redis-server. I ended up attempting to train the model, and after 3000 epochs, I find the error is quite large, roughly the same as what it was at the start (37889.62 m for position, 127.81 m for orientation). Any advice on what I might be doing incorrectly?
"
How can I get ground truth pose and the segmentation mask?,"Hi Jimmy

I have already downloaded the oil change dataset, I would like to ask how to get the ground truth poses and the segmentation masks from the .json file? (specifically, I want to get the ground truth pose and the segmentation mask of the blue oil funnel)

Thank you so much!"
Why do we need blender for demo?,"Hi, thanks for your great work. I wanted to see the results of your work, i.e. I will give a RGB image and expect 6dposes. I believe that blender was used to create the synthetic dataset for training Pose interpreter network. My question is why do we need blender for seeing the results.

My other question is if we require blender then how to use it on some online platform like google collab? "
end_to_end_eval position estimation (own dataset),"Hey there,

when I evaluate the data using eval.ipynb on the small images, which where used for training the pose estimation network, the position error is abot 1.5 cm. So that's good.

But when I use the end to end evaluation, the position error ist about 2 m. The error is only this big in the z coordinate. The x and y coordinates are fine.

Do you have any idea why it is estimating the position an the small pictures right, but on the masks it is getting from the segmentation network wrong.

Thanks in advance!

Kind regard!"
steps to train model again with another DRN,"Hello, 
I want to train another drn model on the same dataset. Please guide me through steps for end to end evaluation. Also, I can see DRNSeg being called during training. But where is drn22 being called by DRNSeg ?"
I think [this line](https://github.com/jimmyyhwu/pose-interpreter-networks/blob/master/pose_estimation/render_pose.py#L6) is supposed to take care of the import problem. I did not have to add any workaround to run the notebooks. Maybe you could try opening `jupyter notebook` in the same directory that the notebook is in?,
Rendering folder calling issue,"the code for segmentation works fine but creates a problem for me in end_to_end_visualization.py and pose_estimation/demo.py in importing 
from dataset import oilchange_scene

I think when at folder pose_estimation it needs to access oilchange.py file which is in dataset folder, and from folder I cannot import this way. Hence, it shows and error. Please suggest a work around for this."
pose estimation demo runtime error,"While running demo.ipynb the following error comes up. I am unsure is it the sound card error or CUDA error ? Please help.

ALSA lib confmisc.c:768:(parse_card) cannot find card '0'
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name
ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory
ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory
ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default
AL lib: (EE) ALCplaybackAlsa_open: Could not open playback device 'default': No such file or directory
found bundled python: /opt/blender/2.79/python
Import finished in 1.6221 sec.
Fra:1 Mem:56.16M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Preparing Scene data
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Creating Shadowbuffers
Fra:1 Mem:56.18M (0.00M, Peak 64.88M) | Time:00:00.06 | Raytree.. preparing
Fra:1 Mem:91.02M (0.00M, Peak 91.02M) | Time:00:00.10 | Raytree.. building
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Raytree finished
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Creating Environment maps
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Caching Point Densities
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Loading voxel datasets
Fra:1 Mem:89.18M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Volume preprocessing
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:89.19M (0.00M, Peak 143.57M) | Time:00:01.00 | Sce: Scene Ve:126775 Fa:253749 La:1
Fra:1 Mem:91.04M (0.00M, Peak 143.57M) | Time:00:01.32 | Scene, Part 13-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 3-20
Fra:1 Mem:91.37M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 11-20
Fra:1 Mem:91.30M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 9-20
Fra:1 Mem:91.21M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 7-20
Fra:1 Mem:91.48M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 19-20
Fra:1 Mem:91.46M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 15-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 5-20
Fra:1 Mem:91.05M (0.00M, Peak 143.57M) | Time:00:01.39 | Scene, Part 18-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.40 | Scene, Part 10-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 1-20
Fra:1 Mem:91.44M (0.00M, Peak 143.57M) | Time:00:01.41 | Scene, Part 4-20
Fra:1 Mem:91.91M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 17-20
Fra:1 Mem:91.82M (0.00M, Peak 143.57M) | Time:00:01.42 | Scene, Part 2-20
Fra:1 Mem:91.36M (0.00M, Peak 143.57M) | Time:00:01.43 | Scene, Part 14-20
Fra:1 Mem:90.95M (0.00M, Peak 143.57M) | Time:00:01.44 | Scene, Part 8-20
Fra:1 Mem:90.57M (0.00M, Peak 143.57M) | Time:00:01.45 | Scene, Part 12-20
Fra:1 Mem:90.19M (0.00M, Peak 143.57M) | Time:00:01.46 | Scene, Part 20-20
Fra:1 Mem:89.99M (0.00M, Peak 143.57M) | Time:00:01.49 | Scene, Part 16-20
Fra:1 Mem:89.28M (0.00M, Peak 143.57M) | Time:00:01.56 | Scene, Part 6-20
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Determining resolution
Fra:1 Mem:32.74M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Initializing execution
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.57 | Compositing | Tile 1-2
Fra:1 Mem:34.21M (0.00M, Peak 143.57M) | Time:00:01.58 | Compositing | Tile 2-2
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Compositing | De-initializing execution
Fra:1 Mem:34.20M (0.00M, Peak 143.57M) | Time:00:01.69 | Sce: Scene Ve:126775 Fa:253749 La:1
Saved: '/tmp/tmpz3aqpi1j/render.png'
 Time: 00:01.75 (Saving: 00:00.05)


Blender quit
Traceback (most recent call last):
  File ""<stdin>"", line 14, in <module>
  File ""<stdin>"", line 3, in visualize_batch
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 112, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 52, in forward
    x = self.resnet18(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/pose-interpreter-networks/pose_estimation/models.py"", line 199, in forward
    x = self.conv1(x)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/meghal/python36/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDNN_STATUS_MAPPING_ERROR"
No such file or directory: 'data/OilChangeDataset\\annotations\\val_20171103_OilChange.json',"I can not unzip the oil change dataset. Its showing corrupted file. 
Kindly help, please !!!!"
blender linking issue,"It seems to be easy but I am facing trouble with the process of linking blender. Kindly let me know where am I going wrong.
Step 1:  I downloaded blender-2.79b-linux-glibc219-x86_64.tar.bz2  from the link : https://download.blender.org/release/Blender2.79/

Step 2: created an empty directory blender in /usr/local/bin/blender 

Step 3: extracted .tar file at the downloaded location.

Step 4: to link the binary file : lm -s /blender_download_folder/blender /usr/local/bin/blender

This gives me an error ln: failed to create symbolic link '/usr/local/bin/blender/blender': File exists. I could not find any executable file in the downloaded folders. Please let me know if I am wrong or missing out something.

"
end_to_end_visualize.ipynb HTTP 403 forbidden error ,"I loaded all the pretrained models and dataset required. While running end_to_end_visualize.ipynb I get an error at line 
segm_model = segm_models.DRNSeg(segm_cfg.arch, segm_cfg.data.classes, None, pretrained=True)

Downloading: ""https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth"" 
 line 65, in load_url
    _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
in _download_url_to_file
    u = urlopen(url)
  File ""/usr/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.6/urllib/request.py"", line 532, in open
    response = meth(req, response)
  File ""/usr/lib/python3.6/urllib/request.py"", line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/usr/lib/python3.6/urllib/request.py"", line 570, in error
    return self._call_chain(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.6/urllib/request.py"", line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
"
Issue with Segmentation Eval 403 Forbidden,"Hi Jimmy,

I have downloaded the pretrained datasets, however when I attempt to run the segmentation eval Jupyter notebook. I receive an error in the third block. It seems that the code is downloading a checkpoint from a princeton website. Attempting to visit this site gives me a 403 Forbidden error. The address is: https://tigress-web.princeton.edu/~fy/drn/models/drn_d_22-4bd2f8ea.pth. 

Do users need special permission for this?

Thanks"
ROS Dependencies,"in order to run end_to_end_visualize.ipynb  I installed all the packages from conda and tried to run the ipynb file. Earlier it gave me pypcd error. I cloned the github repo of the same. But again got stuck somewhere and got an error 
RuntimeError: invalid hash value (expected ""4bd2f8ea"", got ""f22d7c4219e69774cf0e6a9ebcaeb1a200420ae084036bf122712107be6576bb"")

Please let me know how to resolve these ROS dependency and make the code run just to see how it works."
Problem about evaluation with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object and I trained it, I had problems because my images hadn't the same size as others images used for models pretained, so I changed the train code and it worked. But now I have problems when I try to use eval.ipynb, at the line model.load_state_dict(checkpoint['state_dict']) I have this error :
RuntimeError: Error(s) in loading state_dict for DataParallel:
size mismatch for module.fc1.weight: copying a param of torch.Size([256, 40960]) from checkpoint, where the shape is torch.Size([256, 86528]) in current model.
size mismatch for module.fc_p1.weight: copying a param of torch.Size([15, 256]) from checkpoint, where the shape is torch.Size([3, 256]) in current model.
size mismatch for module.fc_p1.bias: copying a param of torch.Size([15]) from checkpoint, where the shape is torch.Size([3]) in current model.
size mismatch for module.fc_o1.weight: copying a param of torch.Size([20, 256]) from checkpoint, where the shape is torch.Size([4, 256]) in current model.
size mismatch for module.fc_o1.bias: copying a param of torch.Size([20]) from checkpoint, where the shape is torch.Size([4]) in current model.

I don't understand how I could change these sizes.

Thanks in advance.

"
Problem about pose_estimation training with my own dataset,"Hi, Jimmy.

I use my own dataset which contains one object to train the pose estimation. When I ran train.py there is the error : 
RuntimeError: invalid argument 2: size '[-1 x 40960]' is invalid for input with 2768896 elements at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THStorage.cpp:80

I went in the debug mode and I found that the problem is that the matrix x in models.py file at line 52 don't have the right size. Normally I should have matrix of shape [32,512,8,10] but I have [32,512,13,13] when I look x.shape. I don't understand how this matrix is created and so where does the problem come from. 

Could you give me any suggestion?

Thanks in advance."
Problem about orientation_error of validation set is extremely high compare to train set.,"Hi!
I used my own dataset which contains four objects to train the pose_interpreter_network and I found the orientation_error of val dosen't decline well when error of train set converged as the chart below.

![火狐截图_2019-05-07T11-28-27 356Z](https://user-images.githubusercontent.com/38148405/57296727-44d10b00-7100-11e9-9665-a0cd94afaaa1.png)

Consequently, the result of end_to_end_eval is unsatisfactory as well. Despite the positions of object are predicted correctly, most the orientations are wrong.
Could you give me any suggestion?
Thanks in advance."
Error when training pose_estimation about load pcd file.,"Hi, Jimmy. 
I have made my own dataset that contains 4 objects and pcd files. When I run:

> python train.py config/kinect1_mask.yml

There is an error occured:

> Traceback (most recent call last):
  File ""train.py"", line 295, in <module>
    main(cfg)
  File ""train.py"", line 215, in main
    criterion = PointsL1Loss(numpy_pcs).cuda()
  File ""train.py"", line 63, in __init__
    self._pcs = torch.from_numpy(np.array(numpy_pcs)).cuda()
ValueError: could not broadcast input array from shape (4,38720) into shape (4)

There are 38720 points in my first pcd file, and the format is same with your pcd files. It's like:

> VERSION 0.7
FIELDS x y z
SIZE 4 4 4
TYPE F F F
COUNT 1 1 1
WIDTH 38720
HEIGHT 1
VIEWPOINT 0 0 0 1 0 0 0
POINTS 38720
DATA ascii
0 0.00193 0
0 0 0
0.0019499999 0 0
0.0038999999 0 0
0.0058499998 0 0
0.0077999998 0 0
0.0097500002 0 0
...

My config file is like :

> data:
        root: data/kinect1_mask
        pcd_root: objects
        num_subsets: 10
        val_subset_num: 101
        objects: [
            box,
            bottle,
            cola_can,
            power_bank
        ]
        batch_size: 32
        workers: 4
arch:
        num_input_channels: 1
        num_shared_fc_layers: 1
        num_shared_fc_nodes: 256
        num_position_fc_layers: 1
        num_position_fc_nodes: 256
        num_orientation_fc_layers: 1
        num_orientation_fc_nodes: 256
loss: points # l1 | posecnn | points_simple | points
optimizer:
        lr: 0.01
        lr_decay_epochs: [700, 1400]
        momentum: 0.9
        weight_decay: 0.0001
training:
        logs_dir: logs/
        checkpoints_dir: checkpoints/
        experiment_name: kinect1_mask
        print_freq: 10
        checkpoint_epochs: 100
        epochs: 2100
        log_dir:
        resume:

Could you give me any suggestions?
Thanks in advance."
About ‘end to end eval’ error,"I add get_filtered_cat_ids() in the ‘datasets.py':

> def get_filtered_cat_ids(coco, img_ids):
    object_instances = coco.loadAnns(coco.getAnnIds(imgIds=img_ids))
    def catgory_filter():
        return lambda x: x['category_id'] in [1, 2, 4, 5, 6]
    return [object_instances for object_instances in filter(catgory_filter(), object_instances)]

Then I changed the __init__() in ’EvalDataset‘class:

> def __init__(self, data_root, ann_file, camera_name, object_names, transform):
        self.data_root = data_root
        self.coco = COCO(os.path.join(self.data_root, 'annotations', ann_file))

        img_ids = get_filtered_img_ids(self.coco, camera_name)
        self.object_instances = get_filtered_cat_ids(self.coco, img_ids)

        self.object_names_map = {cat['id']: cat['name'] for cat in self.coco.dataset['categories']}
        #self.object_indices_map = {object_name: i for i, object_name in enumerate(object_names)}
        self.object_indices_map = {'blue_funnel':6,'funnel':4,'oil_bottle':1,'fluid_bottle':2,'engine':5}
        self.object_ids_map = {cat['name']: cat['id'] for cat in self.coco.dataset['categories']}
        #self.object_ids_map = self.object_indices_map

        self.transform = transform

The 'end to end eval' can read the dataset:
> loading annotations into memory...
Done (t=0.72s)
creating index...
index created!
using camera: kinect2

But, I get thsi error:

> RuntimeError                              Traceback (most recent call last)
<ipython-input-11-fed55fe84c4c> in <module>
      3 with torch.no_grad():
      4     for input, target, object_index, object_id in tqdm(val_loader):
----> 5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)
      7         orientation_errors.extend(orientation_error)

<ipython-input-10-ebee32e8a8cf> in forward_batch(model, input, target, object_index, object_id)
      5 
      6     position, orientation = model(input, object_index, object_id)
----> 7     print(target)
      8     position_error = (target[:, :3] - position).pow(2).sum(dim=1).sqrt()
      9     orientation_error = 180.0 / np.pi * pose_utils.batch_rotation_angle(target[:, 3:], orientation)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/tensor.py in __repr__(self)
     55         # characters to replace unicode characters with.
     56         if sys.version_info > (3,):
---> 57             return torch._tensor_str._str(self)
     58         else:
     59             if hasattr(sys.stdout, 'encoding'):

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in _str(self)
    254             suffix += ', dtype=' + str(self.dtype)
    255 
--> 256         formatter = _Formatter(get_summarized_data(self) if summarize else self)
    257         tensor_str = _tensor_str(self, indent, formatter, summarize)
    258 

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/_tensor_str.py in __init__(self, tensor)
     80 
     81         else:
---> 82             copy = torch.empty(tensor.size(), dtype=torch.float64).copy_(tensor).view(tensor.nelement())
     83             copy_list = copy.tolist()
     84             try:

RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorCopy.cpp:70
"
a question about making own datasets ,"Hello, I now mark each image in my dataset with ”labelme“ and convert the annotation.json format to coco format, but these files are one by one, how to properly merge a annotation file, just like the annotations file you provide in Oilchangedatasets?
![image](https://user-images.githubusercontent.com/34231078/56178671-a805db00-6035-11e9-95d3-7ed905db812d.png)

"
"Error about ""end to end pose estimator"" use ""kinect2"" in Oilchange datasets ","When testing “end to end system” in Oilchange dataset，I used “kinect2“，but  I met  the key error. 

> KeyError                                  Traceback (most recent call last)
<ipython-input-169-fed55fe84c4c> in <module>
      2 orientation_errors = []
      3 with torch.no_grad():
----> 4     for input, target, object_index, object_id in tqdm(val_loader):
      5         position_error, orientation_error = forward_batch(model, input, target, object_index, object_id)
      6         position_errors.extend(position_error)

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py in __iter__(self, *args, **kwargs)
    219     def __iter__(self, *args, **kwargs):
    220         try:
--> 221             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    222                 # return super(tqdm...) will not catch exception
    223                 yield obj

~/.conda/envs/poseIN/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)
   1020                 """"""), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1021 
-> 1022             for obj in iterable:
   1023                 yield obj
   1024                 # Update and possibly print the progressbar.

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/.conda/envs/poseIN/lib/python3.6/site-packages/torch/utils/data/dataloader.py in <listcomp>(.0)
    312         if self.num_workers == 0:  # same-process loading
    313             indices = next(self.sample_iter)  # may raise StopIteration
--> 314             batch = self.collate_fn([self.dataset[i] for i in indices])
    315             if self.pin_memory:
    316                 batch = pin_memory_batch(batch)

~/codedisk/dsl_py/pose-interpreter-networks/datasets.py in __getitem__(self, index)
     50 
     51         object_name = self.object_names_map[ann['category_id']]
---> 52         object_index = self.object_indices_map[object_name]
     53         object_id = self.object_ids_map[object_name]
     54 

KeyError: 'oilfilter'

How to read  annotations in datasets which only  used for pose estimation?"
About trainning pose estimator ,"Hello, I changed the model in pose estimator from  resnet18 to  resnet50, I use the points loss, but when I train the estimator on th mask dataset, the orientation loss does not converge，could you give me some suggestion？
![image](https://user-images.githubusercontent.com/34231078/56078641-fb511100-5e1c-11e9-9bcd-8bac773f74c1.png)
![image](https://user-images.githubusercontent.com/34231078/56078642-fe4c0180-5e1c-11e9-9361-346168bec827.png)
![image](https://user-images.githubusercontent.com/34231078/56078643-0015c500-5e1d-11e9-81e7-800a13289c46.png)
"
Error about ros package,"Hello, when I run ""roslaunch pose_interpreter_networks pose_estimator.launch"", I met this error, as follow:

> [ERROR] [1554901200.082931]: bad callback: <bound method Subscriber.callback of <message_filters.Subscriber object at 0x7fb60d16e910>>
Traceback (most recent call last):
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/topics.py"", line 750, in _invoke_callback
    cb(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 75, in callback
    self.signalMessage(msg)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 224, in add
    self.signalMessage(*msgs)
  File ""/opt/ros/kinetic/lib/python2.7/dist-packages/message_filters/__init__.py"", line 57, in signalMessage
    cb(*(msg + args))
  File ""/home/dsl/catkin_ws/src/pose_interpreter_networks/src/pose_estimator.py"", line 117, in callback
    segm, object_names, positions, orientations = self.model(input)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 124, in forward
    return self.gather(outputs, self.output_device)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 136, in gather
    return gather(outputs, output_device, dim=self.dim)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 67, in gather
    return gather_map(outputs)
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File ""/home/dsl/.conda/envs/pose/lib/python2.7/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
TypeError: zip argument #1 must support iteration

How to do to solve this problem?
"
How to use the pretrained object pose estimation model for end to end eval.,"Hi, Jimmy.
When I loaded the checkpoint of floating_kinect1_object in the end_to_end_eval.ipynb. There is a error 

> RuntimeError: Given groups=1, weight[64, 3, 7, 7], so expected input[1, 1, 240, 320] to have 3 channels, but got 1 channels instead

So I wander could the object chekpoint be used to end_to_end_eval/visualize.
Thanks you."
Where to download provided val?,"Hello, in poseprocess_wraper.py:
“parser.set_defaults(process_val=False)  # should download provided val set to match numbers in paper”
Where to download val set??
I set process_val=True to create val dataset ， but when I train my pose estimator：
![image](https://user-images.githubusercontent.com/34231078/55678340-6d5fbc80-592a-11e9-99bc-62128ddebdf1.png)
I use kinect2.  I just foget to change ""experiment_name"""
Issue about  training pose estimation models.,"Hi, Jimmy.
When I ran 

> python train.py config/floating_kinect1_mask.yml

There was a error

> File ""/home/huo/virtuallist/py36/lib/python3.6/site-packages/pypcd/pypcd.py"", line 282, in point_cloud_from_fileobj
> if ln.startswith('DATA'):
> TypeError: startswith first arg must be bytes or a tuple of bytes, not str 

came out.

It seems because of loading pcd file incorrectly. I have tried to use the two fomat of pcd file in binary and ascii, but the same error still occurs. And if I modified the code `ln.startswith('DATA')` with `ln.startswith(b'DATA')`, there will be a error 

> File ""/home/huo/virtuallist/py36/lib/python3.6/re.py"", line 172, in match
>     return _compile(pattern, flags).match(string)
> TypeError: cannot use a string pattern on a bytes-like object

come out subsequently.

Could you please give some adcices to solve the peoblems above?
Thank you."
Segmentation training issues: StopIteration,"HI! Thank you for the great job.
I created a json file of my own annotated data by following the coco mask.py.
The part of the json file as below: 

> {...,
> ""image"": [{""license"": 0, ""file_name"": ""0000007149_rgb.png"", ""coco_url"": """", ""height"": 480, ""width"": 640, ""date_captured"": 1544011401.0, ""camera_id"": 0, ""flickr_url"": """", ""id"": 0},...],
> ""annotations"": [{""segmentation"": {""size"": [480, 640], ""counts"": ""QgY37h>f0ZOe0\\Od0\\Od0\\Od0[Of0[Oc0]O`0@3M2N2N3M2N2N2N3L3N2N2N3M2N2N2N2N1O001O0O2O0000001O000000O1000O10000000O10000O100000O0100000000O100000000O1000O10O1000000O100000000O0100000O100000000O1000O10O1000000O1000000O1000O1000O1000000O1000000O10O100000O1000000O10000000O0100000000O1000000O1000O10O100000000O100000O0100000000O10000O100000001N1000001N10001O0O101O000O2O0000001N101hMfFmLn0a0m;]OUDa0U=01N2O1O1O1N101O1O1N2O1O1N2N2MQeW3""}, ""area"": 34310, ""pose"": {""position"": {""x"": -0.0007623627074502259, ""y"": 0.34552469760243687, ""z"": 1.0687215939143497}, ""orientation"": {""x"": 0.7472581634432386, ""y"": -0.14419430357527752, ""z"": 0.10985900520900949, ""w"": 0.6393310871202543}}, ""iscrowd"": 0, ""image_id"": 0, ""bbox"": [225.0, 242.0, 193.0, 219.0], ""category_id"": 1, ""id"": 0},...],
> ""categories"": [{""supercategory"": ""objects"", ""mesh"": ""charge_pile.stl"", ""id"": 1, ""name"": ""charge_pile""}]}

Then I modified the config file(drn_d_22_ChargePile.yml) and utils.py related to the object classes in ""segmentation"" folder.
But when I run

>  python train.py config/drn_d_22_ChargePile.yml

There is a error ""StopIteration"" came out at:
`first_input_batch, first_target_batch = iter(val_loader).next()` in the train.py:217
and RuntimeWarning: invalid value encountered in true_divide at:
`return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))` in the utils.py:25
It seems my dataset wasn't loaded correctly.
Can you give me any suggestions about how to solve these problem？Thanks."
How to Create a RGB segmentation dataset for your environment,"How, I am a tiro，I don‘t konw how to create a RGB segmentation datase for translate learning. I just use labelme to buile .json file for  a single RGB image, just like this  
![image](https://user-images.githubusercontent.com/34231078/54470325-cd0df080-47e0-11e9-9c0b-be1a38439b1d.png)
However, It didn't like your annotation file

![image](https://user-images.githubusercontent.com/34231078/54470333-e747ce80-47e0-11e9-8502-fcfb4adc1b4a.png)
"
blender in pose estimation,"Hello, when I run the demo, visualization, and evaluation code in ""pose estimation"", the program has been rendering in the background of the blender, unable to proceed to the next step, and there has been no result. I encountered this problem for the first time and put the “mesh” folder in the Oil Change dataset. The permissions of the .stl file in the file are changed to executable files, which can run normally. 
However, after a few days, I met the same problem that the program has been running can not produce results, I don't know how to solve this time."
Problem about training on LineMod dataset,"I want to train your network on linemod dataset, however, I have question writing the config file such as 
20171103_OilChange.json. I cannot understand the messages in the json file like the following...

{""segmentation"":{""counts"":""mYVe08]Q14M4L3L4N1N3M3L3N3N1M4N2L3N3mLZOPUOh0jj0ElTO?Qk0HhTO;Tk0LfTO7Wk02aTO0\\k08\\TOKbk0?STODjk0j0hSOYOUl0W3O1N2O00001O00O100O1O1O1O1O1O1N2O1O1O1O1O1O100O1O1O1O1O1O100O1O1O1O1O1O1O1O1O1O1O1O1O1000000O10000000000O100000000000000001O000000001O000000000000001O0000000000000000O10000O100O1O1O1N2O1O1O1O1O1N2O1O1N2N2O1O1O1O1O1N2O1O1N2O1O1O1N2O1O1O1N2O1O1O1O1O1N2N2O1O1N200O100O100O100O100O100O1O1N3O2J6D<E`0Ca0\\Oc0BQ[\\T1"",""size"":[1080,1920]},""area"":21399,""pose"":{""position"":{""y"":-0.3352892126408012,""x"":-0.1564572835692801,""z"":0.7772331158642385},""orientation"":{""y"":0.9502802730465292,""x"":-0.303411278174998,""z"":-0.01268932429862474,""w"":0.06890558746337358}},""iscrowd"":0,""image_id"":0,""bbox"":[643.0,0.0,173.0,179.0],""category_id"":6,""id"":0},

Could you please explain how did you create these .json files and what should I do to my own dataset such as LineMod to get these segmentation ground truth?
Thank you !!!


"
A bug,"File ""train.py"", line 52, in train
    output = model(input)
RuntimeError: CuDNN error: CUDNN_STATUS_MAPPING_ERROR
I run the code and It seems a bug here."
A BUG,"I run the code and there's a  ""CUDNN_STATUS_MAPPING_ERROR"" 
File ""train.py"", line 52, in train
    output = model(input)
it seems from here.

"
PermissionError: Permission denied: '/usr/local/bin/blender',"When I used anaconda3's python to run end_to_end_visualize.ipynb, this problem occurred. I copied the system's installed Blender library file to my anaconda3/share/ folder, but it didn't work. I don't know how to solve this problem.

The error message is as follows：
---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
<ipython-input-11-a2f62c5336e7> in <module>()
     21         all_objects = np.zeros_like(resized_image)
     22         for i, object_name in enumerate(object_names):
---> 23             single_object = pose_renderers[object_names[i]].render(positions[i], orientations[i])
     24             all_objects = np.maximum(all_objects, single_object * object_colors[object_names[i]])
     25         rendered_pose = (1 - alpha) * resized_image + alpha * all_objects

~/pose-interpreter-networks/pose_estimation/utils.py in render(self, position, orientation)
     89                                    str(self.camera_parameters['p_x']), str(self.camera_parameters['p_y']),
     90                                    str(self.camera_scale),
---> 91                                    position, orientation])
     92             assert ret == 0
     93             image = np.asarray(Image.open(output_path))

~/anaconda3/lib/python3.6/subprocess.py in call(timeout, *popenargs, **kwargs)
    265     retcode = call([""ls"", ""-l""])
    266     """"""
--> 267     with Popen(*popenargs, **kwargs) as p:
    268         try:
    269             return p.wait(timeout=timeout)

~/anaconda3/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)
    707                                 c2pread, c2pwrite,
    708                                 errread, errwrite,
--> 709                                 restore_signals, start_new_session)
    710         except:
    711             # Cleanup if the child failed starting.

~/anaconda3/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)
   1342                         if errno_num == errno.ENOENT:
   1343                             err_msg += ': ' + repr(err_filename)
-> 1344                     raise child_exception_type(errno_num, err_msg, err_filename)
   1345                 raise child_exception_type(err_msg)
   1346 

PermissionError: [Errno 13] Permission denied: '/usr/local/bin/blender'"
Support for kernel mean embeddings and kernels on distributions,"My work involves embedding distributions in RKHS using [kernel mean embeddings](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions) or equivalently kernels on distributions [Ch. 2, especially the generalized RBF kernel](http://reports-archive.adm.cs.cmu.edu/anon/2016/CMU-CS-16-128.pdf).

Are there any plans of extending this library to this case (or other structured domains for that matter)?

In the near future I'll be implement algorithms that uses this and if this sounds interesting I'd be happy to clean up and contribute the resulting code!

Thanks for the hard work :)"
Using GPU to accelerate the eigen decomposition in the psuedo-inverse calculation in nystrom.jl,"Here is my implementation:
```julia
using CuArrays, CUDAnative, LinearAlgebra, MLKernel

function make_symmetric(A::Mat, uplo::Char='U') where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    return LinearAlgebra.copytri!(A |> Matrix{T}, uplo)
end

function nystrom_inv_gpu!(A::Mat) where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
    A = cu(A)
    vals, vectors = CuArrays.CUSOLVER.syevd!('V', 'U', A)
    tol = eps(T)*size(A,1)
    max_eig = maximum(vals)
    # for i in eachindex(vals)
    #     vals[i] = abs(vals[i]) <= max_eig * tol ? zero(T) : one(T) / sqrt(vals[i])
    # end
    predicate = one(T) .* (vals .>= max_eig * tol)
    vals .= predicate .* CUDAnative.rsqrt.(vals .^ predicate)
    QD = CuArrays.CUBLAS.dgmm!('R', vectors, vals, vectors)
    W = CuArrays.CUBLAS.syrk('U', 'N', QD)
    return make_symmetric(W)
end
```"
nystrom factorization for sparse matrix,"``` julia
fac = nystrom(k, X)
```
```
ERROR: MethodError: no method matching nystrom(::PolynomialKernel{Float64,Int64}, ::SparseMatrixCSC{Float64,Int64})
Closest candidates are:
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}) where T<:Union{Float32, Float64} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
  nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}, ::Array{U<:Integer,1}) where {T<:Union{Float32, Float64}, U<:Integer} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
```"
add constructor for RBFKernel,"Hi, folks

I'm just wondering if it make more sense to use the convention in wiki ([here](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)) instead of just define it as an alias

I found it a little bit confusing (not very intuitive) while searching wiki, if this name does refer to RBF kernel, since it is completely the same to `SquaredExponentialKernel`..."
Add missing negative signs in the documentation,In the documentation of the exponential kernels some negative signs are missing.
ForwardDiff.jl Compatibility,"[ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) has the following constraints:

  - [ ] The target function can only be composed of generic Julia functions. ForwardDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of Ax_mul_Bx-style functions.
  - [ ] The target function must be written generically enough to accept numbers of type T<:Real as input (or arrays of these numbers). The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well."
Implement Automatic Relevance Determination,"Implement Automatic Relevance Determination as described in Rasmussen (page 2 of pdf, 106 of text):

> http://www.gaussianprocess.org/gpml/chapters/RW5.pdf"
ARD Implementation and Automatic Differentation compatibility,"Hello,

I made relatively heavy changes to your package with the objective to be able to have Automatic Relevance Determination and to be compatible with Automatic Differentation packages such as ForwardDiff.jl.

I changed the way distances were computed by directly applying the scale factor (appropriately) to the vectors. It might make the algorithm slower, I have not made benchmarks so far.
However it is still retrocompatible and using a scalar scaling work with everything.

I did not add tests yet for the ARD case, and I had to remove some cause of the needed transformation of the parameters (in the ""Testing constructors"" part) cause I did not know if the best option is to keep a copy of the original parameters or ignore this issue. Let me know if you would like other modifications.

Cheers, "
Remove hyperparameters,Addresses #65 
Remove integer parameter from PolynomialKernel,"Having a second parameter complicates conversions and adds little value since parameter `d` is typically small (usually 3). Therefore, the practical set of values that would be used can be represented by a floating point number."
Remove LinearKernel,The LinearKernel is used infrequently (never) and can be modelled with the PolynomialKernel with d=1.
Remove PeriodicKernel,"This is a construction of the form k(f(x), f(y)) which can be left to data preprocessing."
Rename GammaRationalKernel to GammaRationalQuadraticKernel,Keep naming consistent with Rasmussen's Gaussian Processes book where the extension of the Exponential Kernel is referred to as the Gamma Exponential Kernel
Error tagging new release,"The tag name ""0.4.0"" is not of the appropriate SemVer form (vX.Y.Z).
cc: @trthatcher"
Dev,
Wrong metric for Matern kernel,"I am only half sure about what I advanced but I think the metric used for the Matern Kernel is not correct. Currently it is using a Squared Euclidean (hence ||x-x'||^2).
But in the documentation (and in the definition of the Matern kernel according to Rasmussen and Williams) the distance used is the Euclidean distance (||x-x'||), also in the documentation the equation of Matern kernel should have the 2's in the square roots. 
"
Issue with new release,"I tried to `add` the package with Julia 1.0.1 but get the following error :
```julia
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ├─possible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ├─restricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 └─restricted by julia compatibility requirements to versions: uninstalled — no versions left
```

I think it has to do with the tag version in Project.toml compared to the actual github release tag"
Dev,
Dev 2,
Dev,Refactored code to support Documenter.jl
Update dependencies for 1.0 compatibility.,"Installing MLKernels on Julia 1.0 currently fails due to a dependency issue.
```
(v1.0) pkg> add MLKernels
  Updating registry at `~/.julia/registries/General`
  Updating git-repo `https://github.com/JuliaRegistries/General.git`
 Resolving package versions...
ERROR: Unsatisfiable requirements detected for package MLKernels [6899632a]:
 MLKernels [6899632a] log:
 ├─possible versions are: [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0] or uninstalled
 ├─restricted to versions * by an explicit requirement, leaving only versions [0.0.1-0.0.3, 0.1.0, 0.2.0, 0.3.0]
 └─restricted by julia compatibility requirements to versions: uninstalled — no versions left
```

With the newest release of SpecialFunctions.jl (https://github.com/JuliaLang/METADATA.jl/pull/19167) we can now update the dependencies of MLKernels.jl. This enables full compatibility for Julia 1.0 (CI passes).

Please trigger a new release on METADATA.jl after merging this to allow users to finally use this great package with Julia 1.0.

A test coverage drop may occur, see the bug and discussion here: https://github.com/JuliaLang/julia/issues/28192."
Update requirements for doc build,
Move HyperParameters module into deprecated file. Add deprecation war…,…nings.
Dev,
Remove HyperParameter?,"Hej!

I like this package, and in principle I think it could be a nice building block for different GP packages such as https://github.com/PieterjanRobbe/GaussianRandomFields.jl or https://github.com/STOR-i/GaussianProcesses.jl. But I'm not sure whether the use of `HyperParameter`s is actually needed; to me it seems it adds an additional layer of complexity that might not always be desired. My suggestion would be to follow the same approach as https://github.com/JuliaStats/Distributions.jl: restrict parameters `<:Real` of immutable kernel functions to certain intervals such as positive real line with the help of inner constructors. This would provide users with a very simple and completely flexible way of updating parameters by just creating new kernel functions with updated parameter values. For increased convenience one could even think about providing default transformations from https://github.com/tpapp/ContinuousTransformations.jl that would provide an easy way to map real values to required intervals and transforming likelihoods etc in a correct way."
Add Julia 1.0 to CI.,"Add testing for Julia 1.0.
Add windows testing."
more Julia 0.7 compat,Test has to be a dependency now
increase version to 0.4.0,was 0.3.9999
Julia 07 compat,
0.7 compat,Is there any work for 0.7 compatibility on the way?
Kernel combination implementation and derivatives,"Hello,
It would be amazing for the package to feature simple kernel combination such as kernel sum and product.
I wrote my own kernel function module because I needed it imperatively but yours is a lot more robust, maybe I could implement it given some directions.
Thanks!"
Kernels for HAC estimators?,"I am unsure if this package is the right place to implement these, but wanted to ask. For robust variance covariance estimators sometimes kernels are implemented to account for spatial or temporal correlation of an assumed or estimated form (Heteroscedastic and Autocorrelation Consistent / HAC variance covariance estimators). Would this package be a good place to have these kernels implemented? Here is a reference of the R implementation of these [Sandwich](https://www.jstatsoft.org/article/view/v011i10/v11i10.pdf) and here is another implementation in Julia [CovarianceMatrices.jl](https://github.com/gragusa/CovarianceMatrices.jl)."
Dev,
WIP: Fixes for Julia v0.6,"- [x] Fix deprecation warnings
- [ ] Fix kernel macros"
Using Distances.jl and automatic differentiation,"I just made a PR JuliaStats/Distances.jl#72 to make distances compatible with automatic differentiation. By using Distances.jl we could get

- ARD kernels using the weighted metrics
- easy higher order derivatives with automatic differentiation

Using a more general `Metric` type in the kernel computations could also enable kernels to be computed over other metric spaces than R^n (e.g. graphs).

Relevant issues:
#46 #53 #52 "
threading support,"I am doing some tests with the `@threads` macro, in case you are interested:

https://github.com/gdkrmr/MLKernels.jl/tree/threads

For the Gaussian kernel I get around 60% speedup for two threads."
DomainError in tests,"in `v0.2.0`
```
INFO: Testing MLKernels.kernelmatrix!
ERROR: LoadError: LoadError: DomainError:
 in exponentialkernel at /home/me/.julia/v0.5/MLKernels/src/kernel.jl:110 [inlined]
```
`z` can be smaller than zero, I think this is due to calculating the squared euclidean distance as x^2 - 2xy + y^2"
pairwise_aggregate inconsistencies,"sometimes you use `f::Type` and sometimes only `::Type` in `pairwise_aggregate`:

https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L19
https://github.com/trthatcher/MLKernels.jl/blob/master/src/PairwiseFunctions/pairwise.jl#L27"
julia 0.6 compatibility,What are your plans on supporting Julia 0.6?
tag a new version,"I am developing a package that depends on MLKernels, but the latest tagged version is 2 years old, which makes it hard to specify dependencies.

If you are interested: https://github.com/gdkrmr/KernelRidgeRegression.jl"
More detailed categorization of kernels,"I think it would be useful to have a more detailed categorization of the kernels. In chapter 4 of [this book](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) kernels (covariance functions) are categorized by the types of their inputs. This would enable specifying the pairwise functions in the type hierarchy instead of specifying them for each kernel separately.

For example:
- stationary kernels satisfy `K(x, y) = k(x-y)`
- isotropic kernels satisfy `K(x, y) = k(|x-y|)`"
Random Fourier Features,"Random Fourier features is a technique for approximating inner products in the RKHS using a randomized feature map. It would be great to have this in MLKernels.

Here's a paper introducing them:
[Random Features for Large-Scale Kernel Machines](http://pages.cs.wisc.edu/~brecht/papers/07.rah.rec.nips.pdf)

A blog post demonstrating their use:
[Random Fourier Features for Kernel Density Estimation](https://mlstat.wordpress.com/2010/10/04/random-fourier-features-for-kernel-density-estimation/)

The kernel being approximated needs to have some specific properties. Mainly it needs to be stationary (shift-invariant) and scaled properly.

I need to look into this a bit more, but it seems like it might not be too difficult to implement. The main part being a function `spectraldensity`, which takes a kernel as an argument and returns a distribution to sample from."
LoadError with 0.5,"I get the following syntax error and deprecation warnings with version 0.5.1 of Julia.

```julia
julia> Pkg.free(""MLKernels"")
INFO: Freeing MLKernels
INFO: No packages to install, update or remove

julia> using MLKernels
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:14
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
WARNING: super(T::DataType) is deprecated, use supertype(T) instead.
 in super(::DataType) at ./deprecated.jl:50
 in supertypes(::DataType) at .../.julia/v0.5/MLKernels/src/meta.jl:17
 in macro expansion; at .../.julia/v0.5/MLKernels/src/kernels.jl:164 [inlined]
 in anonymous at ./<missing>:?
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/kernels.jl, in expression starting on line 161
ERROR: LoadError: LoadError: syntax: space before ""("" not allowed in ""? (""
 in eval_user_input(::Any, ::Base.REPL.REPLBackend) at ./REPL.jl:64
 in macro expansion at ./REPL.jl:95 [inlined]
 in (::Base.REPL.##3#4{Base.REPL.REPLBackend})() at ./event.jl:68
while loading .../.julia/v0.5/MLKernels/src/pairwise.jl, in expression starting on line 45
while loading .../.julia/v0.5/MLKernels/src/MLKernels.jl, in expression starting on line 57
```
These do not appear in the `dev` branch or when using version 0.4.5 of Julia."
Kernel Derivatives,"There's two components to this enhancement.

### Optimization
Define a `theta` and `eta` (inverse `theta`) function to transform parameters between an open bounded interval to a closed bounded interval (or eliminate the bounds entirely) for use in optimization methods. This is similar to how link functions work in logistic regression - unconstrained optimization is used to set a parameter value in the interval (0,1) using the logit link function.
- [x] `theta` - given an interval and a value, applies a transformation that eliminates finite open bounds
- [x] `eta` - given an interval and a value, reverses the value back to the original parameter space
- [x] `gettheta` returns the theta transformed variable when applied to `HyperParameters` and a vector of theta transformed variables when used on a  `Kernel`
- [x] `settheta!` this function is used to update `HyperParameter`s or `Kernel`s given a vector of theta-transformed variables
- [x] `checktheta` used to check if the provided vector (or scalar if working with a HyperParameter) is a valid update
- [x] `upperboundtheta` returns the theta-transformed upper bound. For example, in the case that a parameter is restricted to (0,1], the transformed upper bound will be log(1)
- [x] `lowerboundtheta` returns the theta-transformed lower bound. For example, in the case that a parameter is restricted to (0,1], the transformed lower bound will be -Infinity

### Derivatives
Derivatives will be with respect to `theta` as described above.
- [ ] `gradeta` derivative of `eta` function. Using chain rule, this is applied to `gradkappa` to get the derivative with respect to theta. Not exported.
- [ ] `gradkappa` derivative of the scalar part of a `Kernel`. This must be defined for each kernel. It will be manual, so the derivative will be analytical or a hand coded numerical derivative. It will only be defined for parameters of the kernel. Not exported. Ex. `dkappa(k, Val{:alpha}, z)`
- [ ] `gradkernel` derivative of `kernel`. Second argument will be the variable the derivative is with respect to. A value type with the field name as a parameter will be used. Ex. `dkernel(k, Val{:alpha}, x, y)`
- [ ] `gradkernelmatrix` derivative matrix."
Equality test for Kernels,"```julia
julia> GaussianKernel(1.0) == GaussianKernel(1.0)
false
```

my best guess:

```julia
julia> isimmutable(GaussianKernel(1.0))
true

julia> isimmutable(GaussianKernel(1.0).alpha)
false
```
confuses julia"
performance,"The following code shaves off more than 0.5 seconds when X has `20_000` columns and `10` rows:

```julia
n = 20000
x = randn(10, n)

function gauss{T}(X::Matrix{T}, alpha::T)
    n = size(X, 2)
    xy = LinAlg.BLAS.syrk('U', 'T', one(T), X)
    x2 = [ sum(X[:, i] .^ 2) for i in 1:n ]
    
    LinAlg.BLAS.syr2k!('U', 'N', one(T), x2, ones(T, n), T(-2), xy)
    
    @inbounds for i in 1:n
        for j in 1:i
            xy[j, i] = exp(-alpha * xy[j, i])
        end
    end
    LinAlg.copytri!(xy, 'U')
end
gauss(x, 1.0)
```

compared to

```julia
kernelmatrix!(
    ColumnMajor(),
    Matrix{Float64}(n, n), 
    GaussianKernel(1.0), 
    x, true
)
```
I think one of the reasons is that `LinAlg.syrk_wrapper!` always copies the triangle in the matrix, even though you try to avoid that. I am not sure how much using `syr2k` saves.
"
MLKernels with julia v0.5,"`MLKernels` is nice, but there are a few issues when using with julia 0.5.
Here is what I've found:
- In `kernelapproximation.jl`: replace `Base.blasfunc` with `Base.LinAlg.BLAS.@basefunc` and `chksquare` with `checksquare`
- In `meta.jl`: replace `super` with `supertype`
- In`pairwise.jl`: replace text like `i = store_upper ? (1:j) : (j:n)` with `i = (store_upper ? (1:j) : (j:n))`
- In `kernelfunctions.jl`: in the first 4 `call` statements, there is the issue of adding methods to an abstract type in Julia 0.5 e.g. sisl/BayesNets.jl#28 . What functionality in `MLKernels` would be lost if these statements were excluded? 
"
kernelmatrix function,"<img width=""1087"" alt=""screenshot 2016-06-30 01 26 14"" src=""https://cloud.githubusercontent.com/assets/1314675/16481762/c665400c-3e61-11e6-8ea9-cc8db4caaba7.png"">

I also get this error with `kernel` function sometimes.
"
kernel function,"I just started using your latest version of MLKernels.  I'm having an issue which may just be a syntax problem on my end.  I'm attempting to use the kernel function like I have in the past but here's the result:

``` julia
julia> kernel(GaussianKernel(), 2., 4.)
```

```
ERROR: type KernelComposition has no field k
 in kernel at /Users/joshualeond/.julia/v0.4/MLKernels/src/kernelfunction.jl:14
```

I usually use the kernel to compare arrays and have the same result except the error is at line 16.  The version that isn't working for me is v0.1.0+ which I cloned from Github and the previous version I was using was v0.1.0 installed from Metadata. 
"
Hyperparameters,"The parameters for Kernels should be abstracted to a `HyperParameter` type that can be used to aid in optimization and ensure consistency of constraints.
"
Update to 0.4 compatibility,"This PR makes the full move to work in 0.4+
"
Positive Definite vs Mercer,"I see that you treat positive definite kernels as being synonymous to mercer kernels, while as far as I understand there is a slight difference in that a mercer is more restrictive (e.g. continuity).

Was this an oversight, or a conscious design decision (for simplification maybe)?

source: [here at 36:40](http://videolectures.net/mlss09uk_schoelkopf_km/)
"
Use of triangular dispatch invalid,"This definition for ARD seems to invalid in Julia 0.4, and I think should be invalid in 0.3 too but might not be due to a change.

https://github.com/trthatcher/MLKernels.jl/blob/c1532ce1d3b43806664296b82b4a0bd0b281190c/src/kernels.jl#L62

```
immutable ARD{T<:FloatingPoint,K<:StandardKernel{T}} <: SimpleKernel{T}
```

In particular the `K` part. I can't find exactly where it was changed, but here is related discussion

https://github.com/JuliaLang/julia/issues/6620
https://github.com/JuliaLang/julia/issues/3766
https://github.com/JuliaLang/julia/issues/8974#issuecomment-62552208

I'm a bit confused on how this all works internally, might be worth posting on julia-users if you want to get it working on 0.4 at some point.
"
Kernel/Kernel Matrix Computation,"@st-- 

So I was thinking a bit about the approach to the kernel/kernel matrix computation.

If you have a function f:RxR -> R, then you can apply it element-wise to two vectors and sum the results. So for x = [x1,x2] and y = [y1,y2] the dot product the function would be f(x,y) = xy. Then k(x,y) = f(x1,y1) + f(x2,y2) = x1y1 + x2y2. The squared distance is just f(x,y) = (x-y)^2

If f is a valid positive or negative definite kernel in RxR, then we have a way to extend the kernel to R^n x R^2 by summing the element-wise results.

I was thinking we could take a more modular approach. Technically, the kernels we have now are a composition of a positive definite kernel (the polynomial kernel takes the dot product) or a negative definite positive-valued kernel (the euclidean distance in what we have implemented) and we could define these 'input' kernels in terms of `f`.

I was thinking we could abstract `kernelmatrix` into two functions: generic `kernelize` (the way it currently operates) and a generic `pairwise` which operates on two vectors/matrices. We would create a new class of kernels and `pairwise` would dispatch on those types. Examples of those new base kernels would be `SquaredDistance` and `ScalarProduct`. However, we can easily extend it:
- `f(x,y) = sin(x-y)^2` (sine squared kernel - also negative definite)
- `f(x,y) = (x-y)^2/(x+y)` (chi squared kernel for R+ x R+)

These can all be extended naturally to cover the periodic kernel weights... I was thinking two weight vectors and `f` could be defined like this:

k(x,y) = u1_sin(w1_x1 - w1_y1)^2 + u2_sin(w2_x2 - w2_y2)^2

Where u and w are your weight vectors. (I know this could add redundancy - just an idea)

Long story short, three levels:
- 'Base' simple kernels that will be defined for `pairwise` - ARD would be defined at this level
- Derived kernels that are a function of the matrix that pairwise returns
- Composite kernels as we have them now.

Anyway, I think an approach along these lines would also give us a nice modular approach to the derivatives. We can always ensure there's generic fall-back methods basically exactly how we have them now. Let me know your thoughts
"
Consistency in Exceptions: ArgumentError vs DimensionMismatch,"`kernelmatrix!` for StandardKernel throws ArgumentError, `kernelmatrix!` for SquaredDistanceKernel or ScalarProductKernel throws DimensionMismatch (by `scprodmatrix!`/`sqdistmatrix!`). `matrix_prod!` and `matrix_sum!` for generic Arrays call error(), but for matrices with is_upper argument the former throws DimensionMismatch and the latter throws ArgumentError. I think this should all be made a bit more consistent.

Is there a specific reason to use error() rather than throw(<some exception>) in some cases ?

(NB. there should be `@test_throw`s as well, which I'm about to add.)
"
Merge Development into Master,
Kernel derivatives need to be a separate package,"@st-- 

I'm trying to be pragmatic when it comes to this package. I originally envisioned that this package would provide the following:
- A vetted set of mainstream machine learning kernels (with some simple combination rules)
- The ability to compute a kernel matrix quickly
- The ability to compute a kernel matrix approximation

Unfortunately with the kernel derivatives, I feel the package moves too far away from that. My concerns are that: 
- Derivatives may not be defined and vector parameters add a layer of complexity
- Many derivatives that do exist are intractable - reliance on analytic derivatives is complex/unfeasible in many cases and raises floating point accuracy in others
- Derivatives appear to be catering to a very specific need - it's a specialized component being added to a generic package. 

As such, it's bespoke code and necessitates its own package. 

That being said, I appreciate all the help and I'm more than willing to help you out where ever I can.
"
MultiQuadraticKernel missing,"Maybe that's intentional, but I just noticed that the MultiQuadraticKernel in master doesn't have any equivalent in developments. The InverseMultiQuadraticKernel maps to RationalQuadraticKernel with beta=0.5, but the MultiQuadraticKernel would be beta=-0.5 and you only allow positive values for beta...
"
@inbounds,"Various functions, particularly in vectorfunctions.jl and kernelderiv.jl, use `@inbounds` without making sure all the arguments have the right length etc. Is this fine, because we know how we call them, and we do check array dimensions etc. in the parent routines? Or should we add additional bounds checks in there?
"
ARD parameter (weights) derivative,"Just realised that `kernelmatrix_dp` was missing, and added it in ab3c09258fdcc322f0748ac9467f0b8ac86f518c -- piggy-backing on `kernelmatrix` seemed easiest. The only issue with this is that, currently, `kernel_dp(k::ARD, :weights, x, y)` returns an array with the length of `k.weights`. From a computational POV it makes sense to calculate all dk/dw[i]s at once (and usually you'd want all derivatives, not just one of them, I think), but from an interface/API POV it would be better if `kernel_dp` always returned a scalar... how to reconcile that ?
"
Segfaults,"I was going to add `@test_throw` tests for the various ArgumentErrors e.g. in kernelmatrix, and to figure out which way around things should go I played around in the REPL with things like `MLKernels.kernelmatrix!(zeros(4,4), ExponentialKernel(), ones(4,6), true)`, transposed dimensions for X, different values for is_trans, leaving out is_trans entirely - which sometimes led to instant segfaults, sometimes with a bit of delay. Don't have the time anymore to investigate, but I suspect it's something where we used `@inbounds` but didn't have enough checks to make sure we weren't going to exceed the array bounds!
"
"Limits on parameter ranges: PolynomialKernel,","PolynomialKernel required d to be integer. ExponentialKernel, RationalQuadraticKernel, PowerKernel, LogKernel all require gamma to be <= 1. What's the reason for that ? (Maybe a kernel would not be Mercer if gamma > 1 (or d real, for PolynomialKernel), but it might still be useful. We could just amend ismercer() so that it actually checks the parameter value?)
"
Approach to derivatives,"Types can be parameterized by symbols, which means we can take a different approach to derivatives. I've included a [gist](https://gist.github.com/trthatcher/acd86f5a9b911737d419) to illustrate how we could take advantage of this (""Approach 2"") 

It would cut down on the number of functions and it seems to perform better on my machine (and definitely no worse).
"
Definity of kernels,"I had assumed the Sigmoid kernel was c.p.d., but according to the [paper I just read](http://vip.uwaterloo.ca/files/publications/Carrington%20et%20al%20-%20A%20New%20Mercer%20Sigmoid%20Kernel.pdf) about the Mercer Sigmoid kernel apparently that's only true for some parameter values, depending on the data set... so I suppose we should have iscondposdef(::SigmoidKernel) = false?

Also, I think we might've been working with `isposdef(::Kernel)` meaning ""positive semi-definite"", but Base.isposdef is ""strictly positive definite"" (e.g. isposdef(0) = false...), and if we extend a Base function we should follow that. For kernels it doesn't seem to make much difference whether it's positive semi-definite or strictly positive definite, so maybe move to our own function? We could have ismercer()... as positive semi-definity is sufficient and necessary for a kernel being a Mercer kernel.
"
Remove Separable Kernel (Mercer Sigmoid Kernel),"The separable kernel is the dot product kernel applied to an element-wise transformation of the original data vectors. The element-wise transformation is 100% equivalent to pre-processing one's data.

I don't think it's adding any value - any opposition to removal of this type/kernel?
"
Nystrom kernel approximation,"I believe the Nystrom kernel approximation should satisfy the following invariant:
`nystrom(kernel, X, [1:size(X,1)]) == kernelmatrix(kernel, X)` (up to floating point inaccuracies)
The current implementation is definitely broken.
If I use just Base routines, as follows, I get what I believe is the correct approximation:

```
function basenystrom(kernel::Kernel, X::Matrix, xs::Vector)
    C = kernelmatrix(kernel, X, X[xs,:])
    D = C[xs,:]
    SVD = svdfact(D)
    DVC = diagm(1./sqrt(SVD[:S])) * SVD[:Vt] * C'
    MLKernels.syml(BLAS.syrk('U', 'T', 1, DVC))
end
```

But I don't understand your optimised version well enough to figure out where it's going wrong...

I've added this as test/test_approx.jl; have a look and have fun fiddling with your implementation until that test passes. ;-) It's not yet added to runtests.jl so it doesn't break the overall test.
"
Edge cases,"LogKernel is not pos.def. anyway; constructor says gamma has to be in (0,1] but only checks for gamma>0. Is there a reason for gamma<=1? If not, error message should be adjusted, otherwise code & test...
"
GammaRationalQuadraticKernel parameters,"So GammaRationalQuadraticKernel(alpha,beta,gamma) is a generalised form of the RationalQuadraticKernel(alpha,beta) is a generalised form of the InverseQuadraticKernel(alpha).
InverseQuadraticKernel(alpha) = RationalQuadraticKernel(alpha,beta=1), and
RationalQuadraticKernel(alpha,beta) = GammaRationalQuadraticKernel(alpha,beta,gamma=1).
But the default value of GammaRationalQuadraticKernel is gamma=0.5. Just checking if that is intentional?
"
Why split up test_standardkernels?,"The kernels are all very similar in their tests - wouldn't splitting it up risk missing some tests for some of them / testing them vaguely differently? It's a lot of code duplication for which I can't see the gain... maybe you've got a good reason though ? Mainly curious.
"
Periodic Kernel does not appear to be a Squared Distance kernel,"I was trying to find some documentation on the periodic kernel. In all the papers I look at, it seems like the sin() function is applied to the element-wise distances and squared rather than taking the squared sine of the distance.

sum sin(xi - yi)^2 for all i

rather than sin(||x-y||)^2

Here's a paper for example:

http://jmlr.org/proceedings/papers/v33/hajighassemi14.pdf
"
Release,"@st-- 

I would _really_ like to finish up the non-derivative portion of the package ASAP (next 10 days or so) so that I may use it in my other projects and so that I can release a version of this package to http://pkg.julialang.org/. Originally, this would have happened last week, but the derivatives and ARD obviously added to that time. That's okay (:

I'm going through all the standard kernel definitions and correcting/refining all definitions to ensure they are clean and correct. I'm also going to revisit the standard kernel tests (broken on my last commit - don't worry, will fix up). The composite kernels will need to be sorted for the recursive definition.

Regarding the kernel derivatives, there's a couple options since they probably won't be ready. First, exclude them from the first release and just keep them in a development branch. Alternatively, they could be included in the package code, but not extracted/documented. You would need to explicitly import them. Lastly, they could be broken out into their own package with this one as a dependency (you would be the owner of the deriv package - but I could still co-maintain).

How would you like to approach it?
"
sqdist_d*() and scprod_d*(),"It seems like each version is used in only one place (the derivative for the corresponding class of kernel including ARD). Making use of these functions uses at least two - currently three - scans of the array that the kernel function returns.

Are these functions providing any real value? It seems like there's two definitions with multiple array scans when only a single definition and one scan is required.
"
Only integer exponents?,"Does the exponent e.g. for PowerKernel need to be 
integer? Can it not interpolate with real exponents?
"
Re-implementing Base functions,"In one of your recent commits you made use of (Base.)scale!() which I hadn't come across before, but it looks like it's basically the same functionality as gdmm!/dgmm! in matrixfunctions.jl - the only difference is that the Base version doesn't use `@inbounds`... does that macro make things so much faster that it's worth keeping an in-package version of matrix-vector scaling ?
"
N vs. T,"How useful is it to basically duplicate have the code to allow both row-wise and column-wise data matrices ? From an implementation point of view it'd be a lot simpler to just decide on one, and require users to call e.g. kernelmatrix(k, X', Y') when needed, which is a one-off overhead.

The only difference is whether the access is X[i,:] and Y[j,:] or X[:,i] and Y[:,j]... so I played around with macros and came up with one which takes a condition, a tuple of symbols, and a code block and transforms all array references to objects listed in the tuple of symbols (84ca8bffcc825aaea48708b4d8fc9e9c6984c446, 1564f8a3c4a68ac97bcd5eeda4464a6120fa4e7b). But that doesn't work so well for the cases where you have e.g. N_sqdist and T_sqdist...
"
scprod and sqdist including weighted versions and derivatives and tests,"Sorry, I had added this as well but hadn't put it in there. Merging now...
"
"kernelmatrix(k, X, Y) calculates full matrix, not only upper-right triangle as commented","Now the question is, are there going to be any kernels which are non-symmetric, for which k(x, y) != k(y, x)? If not, then kernelmatrix(k, X, Y) should only calculate half the matrix, as already the case for kernelmatrix(k, X) [which calculates kernelmatrix(k, X, X)].

On second thought, why is there a special case for kernelmatrix(k, X, X)? Should be the same code as for kernelmatrix(k, X, Y)...
"
Roadmap,"@st-- 

I think we should establish a roadmap. For the most part, I already have what I need from this package. All I need to do is expand/finish the approximations.

We have a good foundation for the kernel derivatives defined for the Gaussian kernel. I can expand it to include all the other kernels, it's just a matter of sinking some time into it. 

What are you priorities in terms of the features? How do you want to approach it?
"
Change norm2 -> sqdist & add derivatives for ScaledKernel,
positive definiteness of composite kernels,"Are you sure that a kernel sum of one positive definite and one NOT positive definite kernel is still positive definite ? That's what the code says: `isposdef(ψ::KernelSum) = isposdef(ψ.k1) | isposdef(ψ.k2)`, but I really wouldn't've thought so..
"
Kernel function discussion/kernels with variable-dimensional parameters (ARD),"I just pushed an implementation of integer-based parameter derivatives based on calling names() on the kernel object to get its field. After all that I remembered an important use case in Gaussian Processes: the ""Automatic Relevance Determination"" (ARD) kernel. This is basically a Gaussian kernel, but with a sigma _vector_ (same length as data dimension):

Instead of the 1D Gaussian, `exp(- vecnorm(x - y)^2 / 2k.sigma^2)`, you would use `exp(- vecnorm((x - y) ./ k.sigmas)^2 / 2)`, where k.sigmas is a vector of the same dimensions as x and y.

This crops up not just for euclidean distance kernels, but also scalar product kernels - e.g. a LinearKernel with different scaling for each dimension...

Any idea on how to best implement that ?
"
Memory usage / in-place covariance matrix calculations,"Reading up on Coverage.jl I was curious to check the memory allocation behaviour of the code. Most of it is constructing Kernels, which I think is fine. But it occurred to me that e.g. the second derivatives construct n x n matrices, and memory allocation is one of the biggest speed hits. So in the future it might be worth rewriting the kernel functions such that they can write directly into the covariance matrix. (This will require some more careful thought, and isn't urgent, but I wanted to bring it up now before I forget again!)
"
Composited composite kernels,"Currently, a CompositeKernel can only be made out of StandardKernels, which means something like GaussianKernel()+GaussianKernel()*GaussianKernel() doesn't work. Is there a good reason for that?
"
Fix GaussianKernel kernelize* functions,"Parentheses are significant :)
"
Speed vs code clarity,"So quite a bit of the code looks like it's rather optimised for some things - e.g. euclidean_distance() with BLAS.axpy! and BLAS.dot - is that actually that much faster than Base.dot(x, y) that it's worth it ? And what happens if x and y happen to have different lengths (_shouldn't_ happen, but you never know)? Base.dot() would just throw a DimensionMismatch. The BLAS stuff doesn't complain, just computes a different number...

On a very similar note, is it worth writing convert(T,2) instead of just 2? Doesn't the compiler automatically convert types?

And on a not so related note, but too small to bother opening a separate issue: the Coveralls thing seems to think that kernelize_scalar() for the MercerSigmoidKernel (separablekernels.jl) isn't covered - but I think it should be ? When I add a println() statement to the function, it gets printed out lots of times. Any idea what's going on?
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
First stab at derivatives,"Automatically testing derivatives is a bit tricky. What you'd really want to do is run epsilon from say 1e-2 to 1e-8, and print the results, and what you want to see is the finite difference derivative first getting closer to the analytic derivative as epsilon decreases, and then become worse again as you get into floating point rounding errors...
"
Derivatives (wrt parameters and input values),"Hi,

to be able to use this kernel module in my Gaussian Process regression code - instead of rolling my own covariance kernels, which do basically the same, just not as fancy as your code - I will need the derivatives of kernels, both with respect to the parameters (for optimizing log marginal likelihoods) and with respect to input values (for training from and prediction of derivatives of function values, not just the function values themselves). What are your thoughts (if any) on the interface for that ?

Have a look at https://github.com/st--/juliagp/blob/master/covariances.jl for how I've implemented it so far...

-ST
"
Refactor conversion loop to reduce code redundancy,"Hi,
I just came across your Kernel package for Julia, which I'd like to incorporate into my Gaussian Process code - rather than duplicating all the covariance kernels! To get a bit used to things I just refactored the loops at the bottom of the kernel files, removing code redundancy. All tests running fine.
"
can I use this module in virtual environment?,"Now, I try to install in virtual-env.
but I couldn't find the way for install.
could you help me?"
Fix absl compatibility issues in TF 1.13,"Ref https://github.com/tensorflow/tensorflow/issues/22766
Ref https://github.com/tensorflow/tensorflow/issues/22113"
How to use extra loss with Keras model?,"Hi

I am using Keras api for model training and testing. May I know how to use the loss function in this project together with Keras api?

Thank you!"
error in 'make',"When I run 'make' in build directory , there some errors:

[100%] Linking CXX shared library libextra_losses.so
/usr/bin/ld: 找不到 -ltensorflow_framework
collect2: error: ld returned 1 exit status
CMakeFiles/extra_losses.dir/build.make:166: recipe for target 'libextra_losses.so' failed
make[2]: *** [libextra_losses.so] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/extra_losses.dir/all' failed
make[1]: *** [CMakeFiles/extra_losses.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2

do you know the reason? thank you for your help.

ubuntu16.04+python3.6(anaconda3)+tensorflow1.6
thank you very much."
Where is cuda_config.h?,"      In the ""readme.md"", there is ""copy the header file ""cuda\_config.h"" from ""your\_python\_path/site-packages/external/local\_config\_cuda/cuda/cuda/cuda\_config.h"" to ""your\_python\_path/site-packages/tensorflow/include/tensorflow/stream\_executor/cuda/cuda\_config.h"".
      But  I can't find the path or ""cuda_config.h"" anywhere. It may lead to the failure of ""make"" command.  Can you fix it?"
 undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv,"I have Ubuntu 16.04, Python 3.5.2 in a virtual env, and Tensorflow 1.6 GPU.
I have 2 1080ti.
I have run  the ""cd build && cmake .."" and the output is:

> `-- The C compiler identification is GNU 5.4.0
> -- The CXX compiler identification is GNU 5.4.0
> -- Check for working C compiler: /usr/bin/cc
> -- Check for working C compiler: /usr/bin/cc -- works
> -- Detecting C compiler ABI info
> -- Detecting C compiler ABI info - done
> -- Detecting C compile features
> -- Detecting C compile features - done
> -- Check for working CXX compiler: /usr/bin/c++
> -- Check for working CXX compiler: /usr/bin/c++ -- works
> -- Detecting CXX compiler ABI info
> -- Detecting CXX compiler ABI info - done
> -- Detecting CXX compile features
> -- Detecting CXX compile features - done
> -- Looking for pthread.h
> -- Looking for pthread.h - found
> -- Looking for pthread_create
> -- Looking for pthread_create - not found
> -- Looking for pthread_create in pthreads
> -- Looking for pthread_create in pthreads - not found
> -- Looking for pthread_create in pthread
> -- Looking for pthread_create in pthread - found
> -- Found Threads: TRUE  
> -- Found CUDA: /usr/local/cuda-9.0 (found version ""9.0"") 
> -- Found CWD: /home/vision/matthew/tf.extra_losses/build
> -- Found GPU_CAPABILITY: gencode arch=compute_61,code=sm_61
> 
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> /home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> -- Found TF_INC: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include
> -- Found TF_INC_EXTERNAL: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/external/nsync/public
> -- Found TF_LIB: /home/vision/tf_16/lib/python3.5/site-packages/tensorflow
> -- Configuring done
> -- Generating done
> -- Build files have been written to: /home/vision/matthew/tf.extra_losses/build
> `

**And Also 'make' command:**

> `[ 16%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_grad_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cu:181:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> [ 33%] Building NVCC (Device) object CMakeFiles/cuda_compile.dir/cuda_compile_generated_l_softmax_op.cu.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cu:131:1: warning: multi-line comment [-Wcomment]
>  // #define DEFINE_GPU_SPECS(T)   \
>  ^
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(57): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(304): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(305): warning: integer conversion resulted in a change of sign
> 
> /home/vision/tf_16/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(685): warning: variable ""unused"" was set but never used
> 
> /home/vision/matthew/tf.extra_losses/common.h(32): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(46): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> /home/vision/matthew/tf.extra_losses/common.h(52): warning: the ""always_inline"" attribute is ignored on non-inline functions
> 
> Scanning dependencies of target extra_losses
> [ 50%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:274:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:268:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:291:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:285:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:62:40: warning: variable ‘num_dimensions’ set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc: In lambda function:
> /home/vision/matthew/tf.extra_losses/l_softmax_op.cc:93:40: warning: variable ‘num_dimensions’ set but not used [-Wunused-but-set-variable]
>        shape_inference::DimensionHandle num_dimensions = c->Dim(features_shape, 1);
>                                         ^
> [ 66%] Building CXX object CMakeFiles/extra_losses.dir/l_softmax_grad_op.cc.o
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:293:0: warning: ""REGISTER_CPU"" redefined
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:287:0: note: this is the location of the previous definition
>  #define REGISTER_CPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:310:0: warning: ""REGISTER_GPU"" redefined
>  #define REGISTER_GPU(T)                                          \
>  ^
> /home/vision/matthew/tf.extra_losses/l_softmax_grad_op.cc:304:0: note: this is the location of the previous definition
>  #define REGISTER_GPU(T)                                          \
>  ^
> [ 83%] Building CXX object CMakeFiles/extra_losses.dir/common.cc.o
> In file included from /home/vision/matthew/tf.extra_losses/common.cc:22:0:
> /home/vision/matthew/tf.extra_losses/common.h:46:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_pow2(const T x)
>                                      ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) Target binary_cast(Source s)
>                                        ^
> /home/vision/matthew/tf.extra_losses/common.h:32:39: warning: always_inline function might not be inlinable [-Wattributes]
> /home/vision/matthew/tf.extra_losses/common.h:52:37: warning: always_inline function might not be inlinable [-Wattributes]
>  __attribute__((always_inline)) bool is_aligned(const T ptr, const size_t alignment)
>                                      ^
> [100%] Linking CXX shared library libextra_losses.so
> [100%] Built target extra_losses
> `

**but unfortunately  got an error in test_op.py execution:**

 

> 
> `/home/vision/tf_16/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> Traceback (most recent call last):
>   File ""test_op.py"", line 36, in <module>
>     op_module = load_op_module(LIB_NAME)
>   File ""test_op.py"", line 33, in load_op_module
>     oplib = tf.load_op_library(lib_path)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
>     lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
>   File ""/home/vision/tf_16/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.NotFoundError: /home/vision/matthew/tf.extra_losses/build/libextra_losses.so: undefined symbol: _ZNK10tensorflow8OpKernel11type_stringEv
> `
> 

What should  do?!"
How to get results from embedding training?,"Hi

 I've successfully made the model train, but I'm having trouble printing results, can you please elaborate on how to:

""use these embeddings, place them in the same folder as main.py, load the embeddings and use them.""

Specifically, what does ""loading and using them mean"" in the context of getting H@1, MRR etc.?"
How is background information used in the model ?,"Your paper mentions about using background information for filtering training dataset and tying parameters of relation in ""SimplE-bk"".  Is the code of this model available?"
Can I use GPU accelerate the training process?,"The training process (except the early stop process and test process) take me 7 hours on wn18 dataset using the default parameters on a Linux machine (One GTX 1080 ti). Is this normal?

And I find the early stop process also need 6 hours on wn18 dataset, I guess the time consuming is mainly from ""self.reader.replace_raw"" and ""get_rank"" step. Is this normal?"
a question about simplE_ignr.py,"I really like this work, but I have one thing confused.
In the 34th line of simplE_ignr.py, you use the code ""self.labels = tf.tile(self.y, [2])"", why do you use this code (what's its effect), and why don't you use the code in simplE_avg.py?
Looking forward to your kind feedback."
The evaluations,"I really appreciate your work but I'm not sure about one thing: In `Reader.py next_batch()`, if the `neg_ratio` is larger than 1, the size of `all_triples_and_labels` would be `(1+neg_ratio) * len(bp_triples)`，then the evaluations should all be divided by  `(1+neg_ratio) * len(triples)`. Looking forward to your reply.
"
GPU not being used,i used --cuda on google colab but gpu is not being used how to fix it
Hi! the training can not convergence!,"I train the model on val2017 dataset and my own dataset, but the valid result is about 21db, have you tested them ?Thank you!"
Methods for adding poission noise in official implementations,"Hi Authors,

I  found the official implementations of adding poission noise use the following methods:
https://github.com/NVlabs/noise2noise/blob/c40a0481198bb524d0b70c2cc452f21bd7aec85c/train.py#L47

        return np.random.poisson(chi*(x+0.5))/chi - 0.5

Do you think we can utilize this method?"
Using my own noisy-noisy pairs,"Dear Authors,

There are no instructions on how to use the code for my own noisy-noisy pairs. The current code takes a dataset of images as input and applies two independent noises on each instance, leading to pairs of noisy-noisy train set. But what if I already have my own pairs of noisy-noisy images and want to train the network on them? Is this possible with current architecture?

Best,
Ali. "
just for the test.py,"Hello, I would like to ask, why the test image I input is 1024 * 1024, but the output is indeed 256 * 256, and I did not see where the picture was cropped, I look forward to your reply, thank you."
"Confused about the unet.py code, is it reasonable?","Hi I checked the code feel confused with the code in unet.py:
```
    def forward(self, x):
        """"""Through encoder, then decoder by adding U-skip connections. """"""

        # Encoder
        pool1 = self._block1(x)
        pool2 = self._block2(pool1)
        pool3 = self._block2(pool2)
        pool4 = self._block2(pool3)
        pool5 = self._block2(pool4)
```

pool2-pool5 is computed by self._block2, So it means pool2-pool5 re-use the same conv weight and bias. Does it accepted? I think the u-net should use different conv weight of different layer."
Sharing Results from Monto Carlo Renderings,"Hi ,

Can you please share the results of Monte Carlo Renderings N2N ? 

Regards
Muzahid"
OpenEXR Error while running the train files.,"Traceback (most recent call last):
  File ""train.py"", line 7, in <module>
    from datasets import load_dataset
  File ""/Users/snehagathani/Desktop/noise/src/datasets.py"", line 9, in <module>
    from utils import load_hdr_as_tensor
  File ""/Users/snehagathani/Desktop/noise/src/utils.py"", line 12, in <module>
    import OpenEXR
ImportError: dlopen(/anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so, 2): Symbol not found: __ZN7Imf_2_314TypedAttributeISsE13readValueFromERNS_7IStreamEii
  Referenced from: /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so
  Expected in: flat namespace
 in /anaconda3/lib/python3.6/site-packages/OpenEXR.cpython-36m-darwin.so


This error comes for all the training files."
datasets.py  issues?,"Hello,there may be problems with this place
`raise ValueError('Invalid noise type: {}'.format(noise_type))` 
 should be 
`raise ValueError('Invalid noise type: {}'.format(self.noise_type))`"
something wrong with your endcoder?,"https://github.com/joeylitalien/noise2noise-pytorch/blob/7942c06f924e2244d91fc8c1aff1c2e3991e0eae/src/unet.py#L82
I think enconv(i), i=2,...,5 should be defined separatedly or they will share the same weights.
"
About U-Net model and data pre-processing,"I am curious about the differences from the paper model
1) Why did you choose to use ConvTranspose instead of Upsampling
2) You used RELU in all parts and Leaky RELU after the last layer. But from the paper I think the author meant Leaky RELU everywhere except the last layer. And In the last layer only linear activation

Unrelated question
3) Did you use any pre-processing for the images e.g. means subtraction, normalization etc. I think that would be needed since we don't have BN layers


I am trying to implement the model in Tensorflow and having the problem of INF loss that starts in the 2nd epoch. So I hope your answers would help me. Thank you in advance!

Update: I seem to have solved the problem by adding Batch Norm layers to the UNET model. But still I am puzzled how the authors managed to get a stable training without Batch Norm"
Create LICENSE,
