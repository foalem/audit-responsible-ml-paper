id,issue_title,issue_body,AI_principle_Patric,AI_principle_Leuson,Conflct
15091,Metrics: Consider adding support for sample weighted metrics,"When dealing with imbalanced data sets, it can be beneficial to use sample weighted metrics. This task should be split into several tasks, one for each metric, if sample weights are to be supported in the [metrics project](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Metrics).",Fairness,Fairness,
7313,Bump tensorflow from 1.15.2 to 1.15.4,"Bumps [tensorflow](https://github.com/tensorflow/tensorflow) from 1.15.2 to 1.15.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 1.15.4</h2>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses both NumPy and TensorFlow headers.</li>
</ul>
<h2>TensorFlow 1.15.3</h2>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Updates <code>sqlite3</code> to <code>3.31.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880"">CVE-2019-19880</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244"">CVE-2019-19244</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a></li>
<li>Updates <code>curl</code> to <code>7.69.1</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601"">CVE-2019-15601</a></li>
<li>Updates <code>libjpeg-turbo</code> to <code>2.0.4</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664"">CVE-2018-19664</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330"">CVE-2018-20330</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960"">CVE-2019-13960</a></li>
<li>Updates Apache Spark to <code>2.4.5</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099"">CVE-2019-10099</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190"">CVE-2018-17190</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770"">CVE-2018-11770</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow's changelog</a>.</em></p>
<blockquote>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>,
and
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses
both NumPy and TensorFlow headers.</li>
</ul>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/df8c55ce12b5cfc6f29b01889f7773911a75e6ef""><code>df8c55c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43442"">#43442</a> from tensorflow-jenkins/version-numbers-1.15.4-31571</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e8cbcb0b1756de4afda8677add8a55355720ab7""><code>0e8cbcb</code></a> Update version numbers to 1.15.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5b65bf202a00f558784e61b7dba5063195cce0f5""><code>5b65bf2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43437"">#43437</a> from tensorflow-jenkins/relnotes-1.15.4-10691</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/814e8d83f5966af55168bc1141dc8ba68561556f""><code>814e8d8</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/757085e3e62197ab5ad6a10c667aae08a8929556""><code>757085e</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e99e53dda53644e49f4b8b4ec16ef92f6399fc3b""><code>e99e53d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43410"">#43410</a> from tensorflow/mm-fix-1.15</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bad36df000e97cfe0a271e08778a81db4ce8834a""><code>bad36df</code></a> Add missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f3f1835aed4ab1874c0891c487cd6d0340fed67b""><code>f3f1835</code></a> No <code>disable_tfrt</code> present on this branch</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7ef5c62a21f2c03483c21566dd6c048218dced26""><code>7ef5c62</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43406"">#43406</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/abbf34a5885400f81620df23d9da70f30630e699""><code>abbf34a</code></a> Remove import that is not needed</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.15.2...v1.15.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow&package-manager=pip&previous-version=1.15.2&new-version=1.15.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/hedrox/ecg-classification/network/alerts).

</details>",Security & Safety,Security & Safety,FIXED
41135,Ubuntu Installation problem,"Trying to install this in Ubuntu 22.04, I get these types of errors. Any suggestions on how to fix this? 
`error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1
 [end of output]
 
 note: This error originates from a subprocess, and is likely not a problem with pip.
 ERROR: Failed building wheel for blis
 Running setup.py clean for blis
 Failed to build thinc preshed blis
 Installing collected packages: wrapt, wasabi, srsly, plac, murmurhash, cymem, wheel, tqdm, six, setuptools, preshed, numpy, Cython, thinc_gpu_ops, blis, thinc
 Running setup.py install for preshed: started
 Running setup.py install for preshed: finished with status 'error'
 error: subprocess-exited-with-error
 
 Ã Running setup.py install for preshed did not run successfully.
 â exit code: 1
 â°â> [14 lines of output]
 /home/sdspieg/.env/lib/python3.10/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.
 warnings.warn(
 running install
 /home/sdspieg/.env/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
 warnings.warn(
 running build
 running build_py
 running build_ext
 building 'preshed.maps' extension
 x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.10 -I/home/sdspieg/.env/include -I/usr/include/python3.10 -c preshed/maps.cpp -o build/temp.linux-x86_64-cpython-310/preshed/maps.o -O3 -Wno-strict-prototypes -Wno-unused-function
 cc1plus: warning: command-line option â-Wno-strict-prototypesâ is valid for C/ObjC but not for C++
 cc1plus: fatal error: preshed/maps.cpp: No such file or directory
 compilation terminated.
 error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1
 [end of output]
 
 note: This error originates from a subprocess, and is likely not a problem with pip.
 error: legacy-install-failure
 
 Ã Encountered error while trying to install package.
 â°â> preshed
 
 note: This is an issue with the package mentioned above, not pip.
 hint: See above for output from the failure.
 [end of output]
 
`",Other,Other,
38479,Add examples with page range after colon.,From journal FOLD&R.,Other,Other,
966,Update sbt-scoverage to 1.9.2,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 1.9.1 to 1.9.2.
[GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v1.9.2) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v1.9.1...v1.9.2)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/8e4970db22ed448af068371a1e7cac4f1ea594b8/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]
```
</details>

labels: sbt-plugin-update, early-semver-patch, semver-spec-patch",Other,Other,
18430,Bump pillow from 6.0.0 to 9.0.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@âkmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@âDWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@âaclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@âm-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@âcmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@âilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@âinfmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix pushes_fd method signature <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5833"">#5833</a> [<a href=""https://github.com/hoodmane""><code>@âhoodmane</code></a>]</li>
<li>Add support for pickling TrueType fonts <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5826"">#5826</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Only prefer command line tools SDK on macOS over default MacOSX SDK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5828"">#5828</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix compilation on 64-bit Termux <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5793"">#5793</a> [<a href=""https://github.com/landfillbaby""><code>@âlandfillbaby</code></a>]</li>
<li>Replace 'setup.py sdist' with '-m build --sdist' <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5785"">#5785</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Use declarative package configuration <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5784"">#5784</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Use title for display in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5788"">#5788</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix for PyQt6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5775"">#5775</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
<li>
<p>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a>
[m-shinder, radarhere]</p>
</li>
<li>
<p>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a>
[cmbruns, radarhere]</p>
</li>
<li>
<p>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/82541b6dec8452cb612067fcebba1c5a1a2bfdc8""><code>82541b6</code></a> 9.0.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cae5ac495badd7c7ecfad8223a08f55f5d2eaacb""><code>cae5ac4</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5924"">#5924</a> from radarhere/cves</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ed4cf7813777ad8478cac46f448bc45416a2a99e""><code>ed4cf78</code></a> CVEs TBD</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d7f60d1d5a746eb01d4cb3c7fb05b6593f46b0f5""><code>d7f60d1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> from radarhere/imagemath_eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8531b01d6cdf0b70f256f93092caa2a5d91afc11""><code>8531b01</code></a> Restrict builtins for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1efb1d9fabd1dfdbf7982035eca0dae7306abef1""><code>1efb1d9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5922"">#5922</a> from radarhere/releasenotes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f6c78713a491764dfac576f6c42127755f2c62b3""><code>f6c7871</code></a> Added release notes for <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> and <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a></li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/032d2dc3658f94718109068ac70799313e440754""><code>032d2dc</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/baae9ec4b67c68e3adaf1208cf54e8de5e38a6fd""><code>baae9ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> from radarhere/jpeg_eoi</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1059eb537639925c96d3245dcd73c106d4266c83""><code>1059eb5</code></a> If appended EOI did not work, do not keep trying</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
17313,Image files with periods in file name might share masks,"For example, two image files to segment: a.1.jpg and a.2.jpg

They will end up sharing the same mask files (ie a_mask.png) because extension is assumed to be following the first period and not the last (QT baseName vs completeBaseName).",Other,Other,
8867,Cannot process Supplementary Character in Java / æ æ³å¨Javaä¸­å¤çSupplementaryå­ç¬¦,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter. 
ä»¥ä¸å¿å¡«ï¼å¦åæä¸åçã
-->

**Describe the bug**
A clear and concise description of what the bug is.

When handling supplementary character, HanLP(I tested with pinyin and word segmentation) couldn't handle the supplementary character properly. For short. Java represent a unicode character > 0xFFFF as two sepeprate char, thus HanLP treat them as two seperate Chinese character when getting Pinyin on it. However, those chars not assigned to any validate charset, so the pinyin result would be two 'none', rather than one 'none'.

Word segmentation cannot recognize it, but would always keep them as a word.

å¤ç[Supplementaryå­ç¬¦](https://www.oracle.com/technical-resources/articles/javase/supplementary.html)æ¶ï¼HanLPï¼ææµè¯äºæ¼é³æ æ³¨ååè¯ï¼ä¼¼ä¹æ²¡æ³æ°å½çå¤çSupplementaryå­ç¬¦ãç®åå°è¯´ï¼Javaå°0xFFFFä»¥ä¸çUnicodeå­ç¬¦è¡¨ç¤ºä¸ºä¸¤ä¸ªcharï¼å æ­¤HanLPå¨æ æ³¨æ¼é³çæ¶åä¼å°å¶è§ä¸ºä¸¤ä¸ªç¬ç«çæ±å­ãç¶èè¿äºcharçå¼ç¹æçæ²¡ææå®ç»ä»»æææçå­ç¬¦éï¼å æ­¤æ¼é³æ æ³¨çç»ææ¯ä¸¤ä¸ª'none'ï¼èä¸æ¯ä¸ä¸ª'none'

åè¯ä¹å¹¶ä¸è½è¯å«è¿ç§å­ç¬¦ï¼ä½æ¯åè¯æ»ä¼ç¡®ä¿è¿äºå­ç¬¦æ¯ä¸ä¸ªè¯ï¼ç»æä¸­ä¸ä¼äº§çç ´ç¢çcharã

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```kotlin
println(HanLP.convertToPinyinList(""ð¯¨"").map { it.pinyinWithToneMark })
```

**Describe the current behavior**
A clear and concise description of what happened.

Get: `[none, none]`
`ð¯¨` would be represent as `\uD87E\uDE1B` in Java, none of them is validate Chinese character. So got 2 `none`.
`ð¯¨`å¨Javaä¸­è¢«è¡¨ç¤ºä¸º `\uD87E\uDE1B`ï¼æ¯ä¸ä¸ªåç¬çcharé½ä¸æ¯ææçä¸­æå­ç¬¦ï¼å æ­¤å¾å°äºä¸¤ä¸ª`none`ã

**Expected behavior**
A clear and concise description of what you expected to happen.

Should get: `[fÃ©n]`, or at least a `[none]`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004
- Python version: Not available
- HanLP version: 1.7.8
- Java Version: Java 11

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.",Other,Other,
10350,Remove hyperparameters,Addresses #65,Other,Other,
41545,Bump tensorflow-gpu from 1.4.0 to 1.15.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.2.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.2
> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
> * Tensors are no longer hashable.
> * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
> * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
> * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
> * Expand support for TensorFlow operators in TensorRT conversion (e.g.
> `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
> * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
> significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
> * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
> * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
> * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
> * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
> * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
> * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
> * Fix tests in canned estimators.
> * Expose Head as public API.
> * Fixes critical bugs that help with `DenseFeatures` usability in TF2
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> 
> # Release 2.1.0
> 
> TensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.
> 
> ## Major Features and Improvements
> * The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install ""Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019"", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).
> * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.
> * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.
> * The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.
> * `tf.keras`
> * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).
> * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).
> * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.
> * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).
> * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.
> * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.
> * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.
> * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).
> * `tf.data`
> * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.
> * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.
> * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`
> * `tf.debugging`
> * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.
> * `tf.distribute`
> * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.
> * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.
> * `TensorRT`
> * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.
> * Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to ""true"" or ""1"", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\*D and MaxPool\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.
> 
> ## Breaking Changes
> * Deletes `Operation.traceback_with_start_lines` for which we know of no usages.
> * Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.
> * Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> * The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.
> * `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.
> * `tf.config.experimental_list_devices` has been removed, please use
> `tf.config.list_logical_devices`.
> 
> ## Bug Fixes and Other Changes
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`5d80e1e`](https://github.com/tensorflow/tensorflow/commit/5d80e1e8e6ee999be7db39461e0e79c90403a2e4) Merge pull request [#36215](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36215) from tensorflow-jenkins/version-numbers-1.15.2-8214
- [`71e9d8f`](https://github.com/tensorflow/tensorflow/commit/71e9d8f8eddfe283943d62554d4c676bdaf79372) Update version numbers to 1.15.2
- [`e50120e`](https://github.com/tensorflow/tensorflow/commit/e50120ee34e1e29252f4cbc8ac4cd328e9a9840c) Merge pull request [#36214](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36214) from tensorflow-jenkins/relnotes-1.15.2-2203
- [`1a7e9fb`](https://github.com/tensorflow/tensorflow/commit/1a7e9fbf670ef9d03b2f8fdf1ae2276b2d100fab) Releasing 1.15.2 instead of 1.15.1
- [`85f7aab`](https://github.com/tensorflow/tensorflow/commit/85f7aab93b65ed1fcc589f54d40793b1afb65bf4) Insert release notes place-fill
- [`e75a6d6`](https://github.com/tensorflow/tensorflow/commit/e75a6d6e6e20df83f19e72e04c7984587d768bd3) Merge pull request [#36190](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36190) from tensorflow/mm-r1.15-fix-v2-build
- [`a6d8973`](https://github.com/tensorflow/tensorflow/commit/a6d897351e483dfd0418e5cad2900ad9ef24188c) Use `config=v1` as this is `r1.15` branch.
- [`fdb8589`](https://github.com/tensorflow/tensorflow/commit/fdb85890df5df1e6b3867c842aabb44f561b446d) Merge pull request [#35912](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/35912) from tensorflow-jenkins/relnotes-1.15.1-31298
- [`a6051e8`](https://github.com/tensorflow/tensorflow/commit/a6051e8094c5e7d26ec9573a740246c92e4057a2) Add CVE number for main patch
- [`360b2e3`](https://github.com/tensorflow/tensorflow/commit/360b2e318af2db59152e35be31c8aab1fb164088) Merge pull request [#34532](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/34532) from ROCmSoftwarePlatform/r1.15-rccl-upstream-patch
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.2)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.2)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
39176,i want to create with a starmask,how can i create sparate the photo to one starless photo and one starmask photo,Other,Other,
39944,Datasets cannot be downloaded,"I cannot download the datasets for cyclegan for testing the model. Please Help!
![error](https://user-images.githubusercontent.com/41199211/66584126-318c0500-eba2-11e9-96dd-9a3b0dfe023c.png)",Privacy,Privacy,
9239,Dies on certain input texts,"I inadvertently fed anystyle-parse some weird input text, and it choked. Granted, it's not supposed to be able to parse this properly, but it probably shouldn't choke either?

Input text ""@misc{70213094902020,\n doi = {DOI:10.1503/jpn.100140}\n}\n""

result:
/Users/Stian/.rbenv/versions/1.9.3-rc1/lib/ruby/gems/1.9.1/gems/wapiti-0.0.5/lib/wapiti/model.rb:47:in `label': missing tokens, cannot apply pattern (Wapiti::NativeError)
 from /Users/Stian/.rbenv/versions/1.9.3-rc1/lib/ruby/gems/1.9.1/gems/wapiti-0.0.5/lib/wapiti/model.rb:47:in`label'
 from /Users/Stian/.rbenv/versions/1.9.3-rc1/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.0.9/lib/anystyle/parser/parser.rb:59:in `label'
 from /Users/Stian/.rbenv/versions/1.9.3-rc1/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.0.9/lib/anystyle/parser/parser.rb:50:in`parse'
 from /Users/Stian/.rbenv/versions/1.9.3-rc1/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.0.9/lib/anystyle/parser/utility.rb:4:in `parse'
 from anystyle-import.rb:34:in`<main>'",Other,Other,
7493,AdaBoost: Add support for sample weights,"Add support for sample weights to the AdaBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by AdaBoost, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner in each boosting iteration. The learners can be found here in the AdaBoost project: [AdaBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.AdaBoost/Learners)

The work is currently in progress in the branch [adaboost-sample-weight-support](https://github.com/mdabros/SharpLearning/tree/adaboost-sample-weight-support)",Fairness,Fairness,
10483,urllib.error.URLError: <urlopen error unknown url type: https>,"python 3.6
pyhanlp 0.1.44
å¨è¿è¡å¦ä¸ä»£ç æ¶ï¼æ¥é

from pyhanlp import *
text = """"
print(HanLP.segment(text))

errorï¼File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 116, in <module>
 _start_jvm_for_hanlp()
 File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 38, in _start_jvm_for_hanlp
 from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
 File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 305, in <module>
 install_hanlp_jar()
 File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 191, in install_hanlp_jar
 version = hanlp_latest_version()[0]
 File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 68, in hanlp_latest_version
 return hanlp_releases()[0]
 File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 76, in hanlp_releases
 content = urllib.urlopen(""https://api.github.com/repos/hankcs/HanLP/releases"").read()
 File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
 return opener.open(url, data, timeout)
 File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 526, in open
 response = self._open(req, data)
 File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 549, in _open
 'unknown_open', req)
 File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
 result = func(*args)
 File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 1388, in unknown_open
 raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: https>",Other,Other,
19060,Update sbt to 1.6.0,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from 1.5.8 to 1.6.0.
[GitHub Release Notes](https://github.com/sbt/sbt/releases/tag/v1.6.0) - [Version Diff](https://github.com/sbt/sbt/compare/v1.5.8...v1.6.0)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/6b0742a3e7dfd9bb294190b1db3d605d9556cea3/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",Other,Other,
23088,How to train pix2pix model using CycleGAN?,"Does it train pix2pix model? If it can do so, how to prepare the train sets? Thanks",Other,Other,FIXED
25650,python demo_single.py ---->TypeError: can't pickle weakref objects,"Traceback (most recent call last):
 File ""demo_single.py"", line 36, in <module>
 pickle.dump(clf, f)
TypeError: can't pickle weakref objects",Other,Other,
17467,0.31.7.0: Refactor BaysianOptimizer and add parallel computation,"This pull request refactors the `BayesianOptimizer` implementation to be use the same principles as the `SMACOptimizer`. The two optimizers are both model based optimizers, and should therefore be very similar in implementation. The `BayesianOptimizer` can be viewed a basic implementation of model based optimization, which the `SMACOptimizer` builds a few tricks on top of.
A base class for model based optimizers seems to be the next logical step, but that will follow in a later pull request.

The refactoring enables use of the `BayesianOptimizer` in an ""open loop"" style just like the `SMACOptimizer`. See the unit tests for an example.

This pull request also adds the option of parallel computation to the `BayesianOptimizer`. This work was originally added in #119.

Note that when running in parallel, and using the `Optimize(Func<double[], OptimizerResult> functionToMinimize)` method, the order of the results will not be reproducible. The individuel results will remain the same, but the order of the results will vary between runs.

I recommend only using the parallel version if the provided `functionToMinimize` is running serial computation, and is slow to compute.",Transparency & Explainability ,Transparency & Explainability ,FIXED
11747,Add support for simple linear/logistic regression,"You've done a great job with the more sophisticated algorithms: would it be possible, for completeness, to throw in linear/logistic regression? I imagine it would be fairly quick comparatively.",Other,Other,
41402,Derivatives (wrt parameters and input values),"Hi,

to be able to use this kernel module in my Gaussian Process regression code - instead of rolling my own covariance kernels, which do basically the same, just not as fancy as your code - I will need the derivatives of kernels, both with respect to the parameters (for optimizing log marginal likelihoods) and with respect to input values (for training from and prediction of derivatives of function values, not just the function values themselves). What are your thoughts (if any) on the interface for that ?

Have a look at https://github.com/st--/juliagp/blob/master/covariances.jl for how I've implemented it so far...

-ST",Other,Other,
5893,[Security] Bump lodash from 4.17.11 to 4.17.14,"Bumps [lodash](https://github.com/lodash/lodash) from 4.17.11 to 4.17.14. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from The GitHub Security Advisory Database.*

> **High severity vulnerability that affects lodash, lodash-es, lodash-amd, lodash.template, lodash.merge, lodash.mergewith, and lodash.defaultsdeep**
> Affected versions of lodash are vulnerable to Prototype Pollution. 
> The function defaultsDeep could be tricked into adding or modifying properties of Object.prototype using a constructor payload.
> 
> Affected versions: < 4.17.13

</details>
<details>
<summary>Commits</summary>

- [`be87d30`](https://github.com/lodash/lodash/commit/be87d303941222b97c482755afc0f4a77ce46c30) Bump to v4.17.14.
- [`a6fe6b1`](https://github.com/lodash/lodash/commit/a6fe6b1e174fd02b5e60eb2664405f4c1262c300) Rebuild lodash and docs.
- [`e371828`](https://github.com/lodash/lodash/commit/e37182845f16715a0d1c391c8662d83c55609cee) Bump to v4.17.13.
- [`357e899`](https://github.com/lodash/lodash/commit/357e899e685872b4af5403ecc4b2a928f961ae63) Rebuild lodash and docs.
- [`fd9a062`](https://github.com/lodash/lodash/commit/fd9a062d57646450b61f74029315abd4cc834b08) Bump to v4.17.12.
- [`e77d681`](https://github.com/lodash/lodash/commit/e77d68121ff00ba86b53eed5893d35adfe94c9dd) Rebuild lodash and docs.
- [`629d186`](https://github.com/lodash/lodash/commit/629d1865793182cd967196716f4beff223aa4a91) Update OpenJS references.
- [`2406eac`](https://github.com/lodash/lodash/commit/2406eac542b2a1282be8d812a6d8a45433ade80a) Fix minified build.
- [`17a34bc`](https://github.com/lodash/lodash/commit/17a34bc5854bb982ef333bfe7ae469f4dfcee0ec) Fix test bootstrap for core build.
- [`53838a3`](https://github.com/lodash/lodash/commit/53838a38f8e4f6204ef2f837fecc4e07d09afe77) Fix tests in older browsers.
- Additional commits viewable in [compare view](https://github.com/lodash/lodash/compare/4.17.11...4.17.14)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.11&new-version=4.17.14)](https://dependabot.com/compatibility-score.html?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.11&new-version=4.17.14)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it). To ignore the version in this PR you can just close it
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>",Security & Safety,Security & Safety,FIXED
33312,Computational performance - optimize for speed,"Hi, 

I am using the ScikitLearn.jl library to train Random Forest classifiers. After the training, I note that re-applying the trained models to new datapoints take about 0.2 seconds. After some tests, it seems that this amount of time is un-related to the number of trees and features. Instead, it seems to be latency time. 

I had a look at the scikit-learn webpage here: https://scikit-learn.org/0.15/modules/computational_performance.html
Here they mention that the computational performance of scikitlearn heavily relies on Numpy/Scipy and linear algebra and that it makes sense to take care of these libraries. So they propose to check that Numpy is built using an optimized BLAS/LAPACK library, as follows: 

`from numpy.distutils.system_info import get_info
print(get_info('blas_opt'))
print(get_info('lapack_opt'))`

Any idea of how I can check for this in Julia? 
Else, do you have any suggestion to speed-up the ScikitLearn.jl predictions?",Other,Other,
25844,sqdist_d*() and scprod_d*(),"It seems like each version is used in only one place (the derivative for the corresponding class of kernel including ARD). Making use of these functions uses at least two - currently three - scans of the array that the kernel function returns.

Are these functions providing any real value? It seems like there's two definitions with multiple array scans when only a single definition and one scan is required.",Other,Other,
7028,"kernelmatrix(k, X, Y) calculates full matrix, not only upper-right triangle as commented","Now the question is, are there going to be any kernels which are non-symmetric, for which k(x, y) != k(y, x)? If not, then kernelmatrix(k, X, Y) should only calculate half the matrix, as already the case for kernelmatrix(k, X) [which calculates kernelmatrix(k, X, X)].

On second thought, why is there a special case for kernelmatrix(k, X, X)? Should be the same code as for kernelmatrix(k, X, Y)...",Other,Other,
4335,"0.31.4.0: Add CrossValidationUtilities.GetKFoldCrossValidationIndexSets, Refactor CrossValidation.","Extract the internal `GetKFoldCrossValidationIndexSets` method form the `CrossValidation<T>` class. 
This enables calculation of KFold CrossValidation IndexSets for use outside the `CrossValidation<T>` it self.

Usage: 

```csharp
// Targets to create KFold Index Sets from.
var targets = new double[] { 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3 };
// Sampler to control the sampling of the sets. In this case Stratified.
var sampler = new StratifiedIndexSampler<double>(seed: 242);

var indexSets = CrossValidationUtilities.GetKFoldCrossValidationIndexSets(sampler,
 foldCount: 4, targets: targets);

foreach (var (trainingIndices, validationIndices) in indexSets)
{
 // Do model training and accumulate predictions,
 // to form a fully k-fold cross validated prediction array.
}
```

Note, that in the case of remainders from `samplesPerFold = targets.Length / foldCount`, the last validationIndices will contain the remaining values (making it larger compared to the others), and the last trainingIndices will exclude these (making it smaller than the others).",Fairness,Fairness,FIXED
26370,How do I use different input?,"I'm sorry, but I am not very familiar with digital image processing.
Could you explain to me further how can I use different input, eg, my own video?
It didn't detect the car plate properly, sometimes it gave me index out of range, I am guessing this is because it completely did not detect something at all in the video.
Is it because I have to train the program to detect the font of my car plate in my video?
If so, how can I do that?

Sorry and thank you, would really appreciate your help because this is very important to me.",Other,Other,
24436,signup not possible - recaptcha wrong after new installation,"How can I enter the correct recaptcha API keys? settings.py ? and how?
Thanks",Other,Other,
31109,Update sbt-pgp to 2.1.2,"Updates com.jsuereth:sbt-pgp from 2.1.1 to 2.1.2.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/75eb8d9cc5ec5ca2bf794e20ce72baa7b907f181/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.jsuereth"", artifactId = ""sbt-pgp"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
8964,Results don't match paper,"Hi,
I downloaded the pretrained models and after running run ./eval_models.sh and got these numbers
CNN-VOL IOU = 0.500177PTN-COMB IOU = 0.509016PTN-PROJ IOU = 0.503761
they are not far off, but still don't correspond to the paper numbers, any idea why I am getting different results?",Other,Other,FIXED
21826,Mujoco Soccer Observation Space,"Hi! i am doing a research with mujoco soccer and right now im having trouble understanding the observation space orientation, the observations given by dm_control are in a (x, y, z) or in a (z, x, y) orientation? 
Regards!",Other,Other,
11200,Update sbt-release to 1.0.15,"Updates com.github.gseitz:sbt-release from 1.0.13 to 1.0.15.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5dafbcd522a0465af7fbbaf4304d48a7b25e3516/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.github.gseitz"", artifactId = ""sbt-release"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
17431,KeyError when load tracks in baseline ipynb,"```AUDIO_DIR = os.environ.get('AUDIO_DIR')

tracks = utils.load(r'data\fma_metadata\tracks.csv')
features = utils.load(r'data\fma_metadata\features.csv')
echonest = utils.load(r'data\fma_metadata\echonest.csv')

np.testing.assert_array_equal(features.index, tracks.index)
assert echonest.index.isin(tracks.index).all()

tracks.shape, features.shape, echonest.shape
```
This is the second block in the baseline.ipynb, 
I get this KeyError:
```KeyError Traceback (most recent call last)
<ipython-input-9-ed98c1f7f0d0> in <module>()
 1 AUDIO_DIR = os.environ.get('AUDIO_DIR')
 2 
----> 3 tracks = utils.load(r'data\fma_metadata\tracks.csv')
 4 features = utils.load(r'data\fma_metadata\features.csv')
 5 echonest = utils.load(r'data\fma_metadata\echonest.csv')

G:\www\fma\utils.py in load(filepath)
 201 ('track', 'genres_top')]
 202 for column in COLUMNS:
--> 203 tracks[column] = tracks[column].map(ast.literal_eval)
 204 
 205 COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
 1960 return self._getitem_frame(key)
 1961 elif is_mi_columns:
-> 1962 return self._getitem_multilevel(key)
 1963 else:
 1964 return self._getitem_column(key)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\frame.py in _getitem_multilevel(self, key)
 2004 
 2005 def _getitem_multilevel(self, key):
-> 2006 loc = self.columns.get_loc(key)
 2007 if isinstance(loc, (slice, Series, np.ndarray, Index)):
 2008 new_columns = self.columns[loc]

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\multi.py in get_loc(self, key, method)
 1998 key = _values_from_object(key)
 1999 key = tuple(map(_maybe_str_to_time_stamp, key, self.levels))
-> 2000 return self._engine.get_loc(key)
 2001 
 2002 # -- partial selection or non-unique index

pandas\_libs\index.pyx in pandas._libs.index.MultiIndexObjectEngine.get_loc (pandas\_libs\index.c:12722)()

pandas\_libs\index.pyx in pandas._libs.index.MultiIndexObjectEngine.get_loc (pandas\_libs\index.c:12643)()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5280)()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5126)()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20523)()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20477)()

KeyError: ('track', 'genres_top')```",Other,Other,
44159,TypeError: __new__() takes exactly 11 arguments (10 given),"Hi Antony, 

I am trying to use your wrapper but I am having some issues with, but only when I submit my own text to extract the concepts from text. 

When I use the example text to run over MetaMap, everyting works just fine. But when I pass my own text, made out of title, abstract and meshcode for articles I get: 

```
concepts,error = mm.extract_concepts(sents,[1,2])
```

 File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/SubprocessBackend.py"", line 132, in extract_concepts
 concepts = Corpus.load(output.splitlines())
 File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 39, in load
 corpus.append(Concept.from_mmi(line))
 File ""/home/ubuntu/canopy/lib/python2.7/site-packages/pymetamap-0.1-py2.7.egg/pymetamap/Concept.py"", line 30, in from_mmi
 return this_class(**dict(zip(FIELD_NAMES, fields)))
TypeError: **new**() takes exactly 11 arguments (10 given)

Any help would be welcome. 

[MetaMap_originalText.txt](https://github.com/AnthonyMRios/pymetamap/files/231943/MetaMap_originalText.txt)",Other,Other,
34913,Help for using the code in practice,"Hi man, great job! 
I'm interested in using your code in my project, but although I'm used to the theory behind RNN's and specially GRU's, I'm not skilled enough for doing the things work in practice. Do you know any other project that has used your code as baseline, performing steps like training and inference? If so, it would be very helpful. Thanks you so much!",Other,Other,
20964,Where are the cv2 file?,"Hey Apoorva Dave

Where are the cv2 which is imported in DetectPlate.py?",Other,Other,
44643,Update google-api-services-bigquery to v2-rev20221217-2.0.0,"Updates com.google.apis:google-api-services-bigquery from v2-rev20221209-2.0.0 to v2-rev20221217-2.0.0.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/0f9c24c264796a82577bce0471e822f32c4ae447/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""@monthly"" },
 dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" }
}]
```
</details>

labels: library-update, commit-count:1",Other,Other,
40290,Update sbt-sonatype to 3.0,"Updates org.xerial.sbt:sbt-sonatype from 2.6 to 3.0.

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>",Other,Other,
21131,about the number of sample,"hello,thanks for your contribution.I want ask one question,as follow:
how do you sellect the number of sample which is in the getPatchImageAndMask.py.
I print the number of sample,which is 5917.I use the dataset of LITS.
Looking forward to hearing from you!",Transparency & Explainability ,Transparency & Explainability ,FIXED
34006,Can't enable NEGEX ('--negex') option,"How can I use custom options? I rewrote function extract_concepts from SubprocessBackend class and add some custom options. I added line ""command.append('--negex')"", but there is no difference in result between this approach and the first one. How can I enable negation detection display?",Other,Other,
20098,ruby 2.7 deprecation warnings,"```
ruby/2.7.0/gems/anystyle-1.3.12/lib/anystyle.rb:73: warning: Using the last argument as keyword parameters is deprecated; maybe ** should be added to the call
ruby/2.7.0/gems/anystyle-1.3.12/lib/anystyle/finder.rb:48: warning: The called method `find' is defined here
```",Other,Other,
8351,attention implementation help for OCR,"Hi,

how can i add the attention model fot keras image_ocr implementation",Other,Other,
23439,Move from Azure to AWS - tracking,"We have a bunch of container images in Azure's container registry, and want to move these to AWS ECR. Repository structure and so on should stay the same. 

Stretch goal is to automate it using [this style of script](https://medium.com/@pjbgf/moving-docker-images-from-one-container-registry-to-another-2f1f1631dc49).",Other,Other,
7682,éç¨é»è®¤åè¯ç»æä¸åç¡®,"éç¨é»è®¤åè¯
è¾å¥åå®¹ä¸ºï¼ éé³è§é³å±±æ¯è¡ï¼åè¯ç»æï¼
[éé³/ns, è§é³å±±/nz, æ¯è¡/n]
ä½æ¯éç¨ NShortSegmentåè¯ï¼ç»æä¸º
[éé³è§é³å±±æ¯è¡/nt]

é»è®¤çåè¯å®ä¾ä¸ºï¼
public static Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
Nshortåè¯å®ä¾ä¸ºï¼
private static Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);

å¦å¤ä¸ä¸ªé®é¢æ¯ï¼ ä¸ºä»ä¹Nshortåè¯ä¸æ¯æèªå®ä¹å­å¸ï¼",Other,Other,
33208,Update jackson-databind to 2.10.1,"Updates [com.fasterxml.jackson.core:jackson-databind](http://github.com/FasterXML/jackson-databind) from 2.10.0 to 2.10.1.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.fasterxml.jackson.core"", artifactId = ""jackson-databind"" } ]
```
</details>

labels: semver-patch",Other,Other,
28295,Respect ENFORCE_SIGNUP_CAPTCHA in profile signup,"Tested with Cypress [here](https://github.com/JasonGhent/NewsBlur/blob/cypress/Makefile), which will produce a video output:

![index_spec js](https://user-images.githubusercontent.com/986620/91789226-36074500-ebdc-11ea-88c9-8f5bde53088e.gif)",Other,Other,
6223,I change to my word embeding and my data but not work help me.,"i train work embedding 300 dimension with 12000 word from https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/.
but not work.. help me tks you very much.
---------------------------------------------------------------------------
Started Training
.
---------------------------------------------------------------------------
InvalidArgumentError Traceback (most recent call last)
~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
 1360 try:
-> 1361 return fn(*args)
 1362 except errors.OpError as e:

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
 1339 return tf_session.TF_Run(session, options, feed_dict, fetch_list,
-> 1340 target_list, status, run_metadata)
 1341 

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
 515 compat.as_text(c_api.TF_Message(self.status.status)),
--> 516 c_api.TF_GetCode(self.status.status))
 517 # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: indices[5] = 12036 is not in [0, 12000)
 [[Node: embedding_lookup_16 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@Const""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Const, _arg_enc_train_inputs_16_0_154)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError Traceback (most recent call last)
<ipython-input-37-0f438758adc0> in <module>()
 51 # ======================= OPTIMIZATION ==========================
 52 if step < 10000:
---> 53 _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)
 54 else:
 55 _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
 903 try:
 904 result = self._run(None, fetches, feed_dict, options_ptr,
--> 905 run_metadata_ptr)
 906 if run_metadata:
 907 proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
 1135 if final_fetches or final_targets or (handle and feed_dict_tensor):
 1136 results = self._do_run(handle, final_targets, final_fetches,
-> 1137 feed_dict_tensor, options, run_metadata)
 1138 else:
 1139 results = []

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
 1353 if handle is None:
 1354 return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1355 options, run_metadata)
 1356 else:
 1357 return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
 1372 except KeyError:
 1373 pass
-> 1374 raise type(e)(node_def, op, message)
 1375 
 1376 def _extend_graph(self):

InvalidArgumentError: indices[5] = 12036 is not in [0, 12000)
 [[Node: embedding_lookup_16 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@Const""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Const, _arg_enc_train_inputs_16_0_154)]]

Caused by op 'embedding_lookup_16', defined at:
 File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
 ""__main__"", mod_spec)
 File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
 exec(code, run_globals)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel_launcher.py"", line 16, in <module>
 app.launch_new_instance()
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/traitlets/config/application.py"", line 658, in launch_instance
 app.start()
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/kernelapp.py"", line 486, in start
 self.io_loop.start()
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tornado/platform/asyncio.py"", line 127, in start
 self.asyncio_loop.run_forever()
 File ""/usr/lib/python3.5/asyncio/base_events.py"", line 345, in run_forever
 self._run_once()
 File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1312, in _run_once
 handle._run()
 File ""/usr/lib/python3.5/asyncio/events.py"", line 125, in _run
 self._callback(*self._args)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
 handler_func(fileobj, events)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
 return fn(*args, **kwargs)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
 self._handle_recv()
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
 self._run_callback(callback, msg)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
 callback(*args, **kwargs)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
 return fn(*args, **kwargs)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
 return self.dispatch_shell(stream, msg)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
 handler(stream, idents, msg)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
 user_expressions, allow_stdin)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
 res = shell.run_cell(code, store_history=store_history, silent=silent)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
 return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
 raw_cell, store_history, silent, shell_futures)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
 interactivity=interactivity, compiler=compiler, result=result)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
 if self.run_code(code, result):
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
 exec(code_obj, self.user_global_ns, self.user_ns)
 File ""<ipython-input-32-82d33b2003d4>"", line 23, in <module>
 encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]
 File ""<ipython-input-32-82d33b2003d4>"", line 23, in <listcomp>
 encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py"", line 327, in embedding_lookup
 transform_fn=None)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py"", line 151, in _embedding_lookup_and_transform
 result = _clip(_gather(params[0], ids, name=name), ids, max_norm)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py"", line 55, in _gather
 return array_ops.gather(params, ids, name=name)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 2667, in gather
 params, indices, validate_indices=validate_indices, name=name)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1777, in gather
 validate_indices=validate_indices, name=name)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
 op_def=op_def)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
 op_def=op_def)
 File ""/root/tensorflow-dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
 self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): indices[5] = 12036 is not in [0, 12000)
 [[Node: embedding_lookup_16 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@Const""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Const, _arg_enc_train_inputs_16_0_154)]]",Other,Other,
818,modify photo enhancement description,"Hello, this library looks very cool.

Reading the readme.md i was struck by the description of `iPhone photo to DSLR photo`.
It feel like wront to associate a depth of field from some lenses to a particualr phone or a DSLR.

the generic DSLR lenses cannot make that blurry picture and the new iphone looks like it can.
I would stay generic with a simpler description.

Regards",Other,Other,
42311,Why split up test_standardkernels?,The kernels are all very similar in their tests - wouldn't splitting it up risk missing some tests for some of them / testing them vaguely differently? It's a lot of code duplication for which I can't see the gain... maybe you've got a good reason though ? Mainly curious.,Other,Other,
15709,Remove hyperparameters,Addresses #65,Other,Other,
21385,Add a Gitter chat badge to README.md,"### uetchy/juno now has a Chat Room on Gitter

@uetchy has just created a chat room. You can visit it here: [https://gitter.im/uetchy/juno](https://gitter.im/uetchy/juno?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&content=body_link).

This pull-request adds this badge to your README.md:

[![Gitter](https://badges.gitter.im/uetchy/juno.svg)](https://gitter.im/uetchy/juno?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=body_badge)

If my aim is a little off, please [let me know](https://github.com/gitterHQ/readme-badger/issues).

Happy chatting.

PS: [Click here](https://gitter.im/settings/badger/opt-out) if you would prefer not to receive automatic pull-requests from Gitter in future.",Other,Other,
33051,Update google-api-services-bigquery to v2-rev20230408-2.0.0,"## About this PR
ð¦ Updates com.google.apis:google-api-services-bigquery from `v2-rev20230401-2.0.0` to `v2-rev20230408-2.0.0`

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/ebc36a3096e62ae3b05745b5c7dce0fe45bfe343/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" }
}]
```
</details>

<sup>
labels: library-update, commit-count:1
</sup>",Other,Other,
5442,added documentations for all modules and cleaned up,"<!-- Reviewable:start -->
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mogeng/iohmm/10)
<!-- Reviewable:end -->",Other,Other,
16163,"Exception: Source array was not long enough. Check srcIndex and length, and the array's lower bounds","The following code throws a System.IndexOutOfRangeException on line 328 in GBMDecisionTreeLearner.cs

```
 var sut = new RegressionSquareLossGradientBoostLearner();

 Random rnd = new Random(42);
 var rows = 10000;
 var columns = 1;
 double[] values = new double[rows * columns];
 for (int i = 0; i < rows * columns; i++)
 values[i] = rnd.NextDouble();
 Containers.Matrices.F64Matrix observations = new Containers.Matrices.F64Matrix(values, 1, 10000);
 double[] targets = new double[rows];
 for (int i = 0; i < rows; i++)
 targets[i] = rnd.NextDouble();

 var model = sut.Learn(observations, targets);
```",Other,Other,
5949,For 0.6.0 release,"- [x] Update **project.toml** file to include support for latest packages (DataFrames [0.20](https://github.com/JuliaData/DataFrames.jl/releases/tag/v0.20.2))
- [x] Drop support for julia 0.7.
- [x] Add travis ci test for julia 1.4
- [x] fix issue #68",Other,Other,
24579,iOS: Add link to privacy policy and terms of use to premium upgrade dialog,"Apple rejected the app stating that those two links need to be present. Can we design an attributed string that says ""See NewsBlur's [privacy policy] and [terms of use] for details."" and link to newsblur.com/privacy and newsblur.com/tos.",Privacy,Privacy,FIXED
8040,correct random seed problems in random forest learners,fix bug described in #65,Other,Other,
30286,Why split up test_standardkernels?,The kernels are all very similar in their tests - wouldn't splitting it up risk missing some tests for some of them / testing them vaguely differently? It's a lot of code duplication for which I can't see the gain... maybe you've got a good reason though ? Mainly curious.,Other,Other,
7996,Excellent Work,Thanks for x boost GPU learners .,Other,Other,
10130,Why only use a single image per batch?,"Hello!

Thanks for your wonderful project for the FID score. 

I have an issue with [this line of code](https://github.com/mseitzer/pytorch-fid/blob/3316d52ec3bfa2aa80c206007b22200b3a7ce279/fid_score.py#L121).
Line 121 
```
pred = model(batch)[0]
# If model output is not scalar, apply global spatial average pooling.
# This happens if you choose a dimensionality not equal 2048.
if pred.shape[2] != 1 or pred.shape[3] != 1:
pred = adaptive_avg_pool2d(pred, output_size=(1, 1))
pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)
```

It seems that only a single image per batch is used for calculating FID. I am confused by this. Please help me solve this issue. Thank youï¼

Best,

Simon",Transparency & Explainability ,Transparency & Explainability ,FIXED
34402,Out of memory,"Hi, 
When running the pretrained style_monet, I get the following error:

THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-3656/cutorch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
/root/torch/distro/install/bin/luajit: /root/torch/distro/install/share/lua/5.1/nn/Container.lua:67: 
In 29 module of nn.Sequential:
/root/torch/distro/install/share/lua/5.1/nn/THNN.lua:110: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-3656/cutorch/lib/THC/generic/THCStorage.cu:66
stack traceback:
[C]: in function 'v'
/root/torch/distro/install/share/lua/5.1/nn/THNN.lua:110: in function 'SpatialConvolutionMM_updateOutput'
...h/distro/install/share/lua/5.1/nn/SpatialConvolution.lua:79: in function <...h/distro/install/share/lua/5.1/nn/SpatialConvolution.lua:76>
[C]: in function 'xpcall'
/root/torch/distro/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
/root/torch/distro/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
./models/one_direction_test_model.lua:52: in function 'Forward'
test.lua:100: in main chunk
[C]: in function 'dofile'
...rch/distro/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
[C]: at 0x00405d50

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
[C]: in function 'error'
/root/torch/distro/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
/root/torch/distro/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
./models/one_direction_test_model.lua:52: in function 'Forward'
test.lua:100: in main chunk
[C]: in function 'dofile'
...rch/distro/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
[C]: at 0x00405d50",Other,Other,
38716,th: command not found,"WHen I run DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model th train.lua in the terminal,
it always remind me of ""th: command not found""",Other,Other,
36991,Delete parsed references from list,"User request:

It would be very handy to have an option to delete an entry entirely from the edit section. Sometimes references are simply broken to begin with but you notice it only after the parsing. For example I just had this bibliographical entry in one of the lists I fed into AnyStyle:

Graddol, David, Dick Leith, et al., eds. Changing English. Milton Park, New York: Routledge > The spread of English within the British Isles, p. 125 ff.

This entry is irreparably broken, but I cannot take it out of the list without having to parse it again, thus losing all the edits I already made. Now I had to remember, find and delete that entry after the import into Zotero.",Other,Other,
37415,Update sbt-sonatype to 3.9.3,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.2 to 3.9.3.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.3) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.2...3.9.3)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/d4cd7d9f31adba210d2c7d88301b6531bcd4af01/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
39120,Build steps for PI module,Is there a build process for making our own release of a PI module to experiment with?,Other,Other,
13540,The output is not correct,"@apoorva-dave 
the car number is not given acquired in console. see the frist picture .Whats wrong actually.
![2019-07-23 (1)](https://user-images.githubusercontent.com/27545384/61711960-15cd4b80-ad77-11e9-953f-d91e789f6795.png)
![2019-07-23](https://user-images.githubusercontent.com/27545384/61711961-1665e200-ad77-11e9-98a1-9ba28b2ee472.png)",Transparency & Explainability ,Transparency & Explainability ,FIXED
13181,TensorFlow implement,"I am sorry to bother you . Recently i want to reproduct the PTN with tensorflow implementation . I want to use my own data , but having problems in making tfrecords format data. Could you please show me the codes to make tfrecords format data with the following features(float representations) : image , mask, vox . Looking forward to your reply , thanks.",Other,Other,
26602,Update sbt-sonatype to 3.9.13,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.12 to 3.9.13.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.13) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.12...3.9.13)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/960ae4bc18b810654be1e11f8848eb3b280cac64/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""@monthly"" },
 dependency = { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" }
}]
```
</details>

labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1",Other,Other,
13425,Identified an issue related with the parser or training,"I've identified an issue with the parser. Can you tell me how can i work with getting the expected output. As, of now main issue is **Author** should be ""**Social Security Administration**"" other than ""**Administration, Social Security**""

INPUT:

> Social Security Administration. Social Security Programs Throughout the World: Asia and the Pacific, 2008. Vol. 13. No. 11801. Government Printing Office, 2002.

TRAIN DATA:
`<author>Social Security Administration</author><title>Social Security Programs Throughout the World: Asia and the Pacific, 2008. </title><volume>Vol. 13. No. 11801. </volume><publisher>Government Printing Office, </publisher><date>2002.</date>`
```
Anystyle.parser.train '/var/www/volume/temp_dta/train_test.txt', false
Anystyle.parser.model.save
```

Expected Output:

> @book{**social2002social**,
> **author = {Social Security Administration},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

What i got output other than expected?

> @book{**administration2002a**,
> **author = {Administration, Social Security},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

REF: 
https://scholar.google.com/scholar?hl=en&q=security&btnG=&oq=sec",Security & Safety,Security & Safety,FIXED
33052,Update google-api-services-bigquery to v2-rev20230311-2.0.0,"## About this PR
ð¦ Updates com.google.apis:google-api-services-bigquery from `v2-rev20230210-2.0.0` to `v2-rev20230311-2.0.0`

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/f1f2e5919fe043fe940d5909880475a863a6458b/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" }
}]
```
</details>

<sup>
labels: library-update, commit-count:1
</sup>",Other,Other,
4397,Unnecessary System Files Generated,"For our project [MetaMorpheus](https://github.com/smith-chem-wisc/MetaMorpheus), after adding Sharplearning NuGet Package to our EngineLayer and TaskLayer, in the GUI WPF project, there is an excessive amount of unnecessary system dll files generated in the output folder after building (No matter release or debug). Here is a list of these [files](https://github.com/smith-chem-wisc/MetaMorpheus/issues/767#issuecomment-349014733). We really couldn't determine where is the problem since there is no trace in the .csproj files and references of GUI nor related projects. So please help us if you have any idea! Thanks a lot.",Other,Other,
16829,Still using CUDA tensor when set to use CPU,"```
use InstanceNormalization
loading previously trained model (/home/dev/CycleGAN/checkpoints/style_cezanne_pretrained/latest_net_G.t7)
use InstanceNormalization
/home/dev/torch/install/bin/luajit: /home/dev/torch/install/share/lua/5.1/torch/File.lua:343: unknown Torch class <torch.CudaTensor>
stack traceback:
[C]: in function 'error'
/home/dev/torch/install/share/lua/5.1/torch/File.lua:343: in function 'readObject'
/home/dev/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
/home/dev/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
/home/dev/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
/home/dev/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'
/home/dev/CycleGAN/util/util.lua:192: in function 'load_test_model'
./models/one_direction_test_model.lua:24: in function 'Initialize'
test.lua:72: in main chunk
[C]: in function 'dofile'
...dev/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
[C]: at 0x5620d7133460

```
Not entirely sure why it's trying to use a CUDA tensor when it should be using a Float tensor (because my machine has a CPU), and I set cudnn and gpu to 0. For some reason, that causes an error when I'm trying to run the pretrained model (style_cezanne_pretrained) on ae_photos.",Other,Other,
34513,Update sbt-sonatype to 3.9.19,"## About this PR
ð¦ Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from `3.9.18` to `3.9.19`

ð [GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/v3.9.19) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/v3.9.18...v3.9.19)

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/0642f3c1f70f08a4e493683c38ebaf92310ab7fb/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" }
}]
```
</details>

<sup>
labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1
</sup>",Other,Other,
18353,pyhanlpå¤è¿ç¨é®é¢,"**Describe the bug**
pyhanlpå¤è¿ç¨å¼å¸¸. ä¸è½ååå©ç¨cpu,èä¸æè§ ä»£ç åæ­¢/""å¡ä½""

**Code to reproduce the issue**
```
!pip3 install pyhanlp
from multiprocessing import Pool
from tqdm import tqdm
from pyhanlp import HanLP
print(HanLP.segment('hello'))

def test_process(tmp: int):
 for i in range(10000):
 HanLP.segment(""åååæå¡"")

pool_ = Pool(2)
result = pool_.map(test_process, tqdm(range(10)))
pool_.close()
pool_.join()
# print(result)
print('END')
```

**Describe the current behavior**
åæ ·çå¤è¿ç¨ä»£ç ,å°±åçº¯çåè¯ä»£ç æ¹æå¶å®åè¯å·¥å·æ¯æ²¡æé®é¢ç
```HanLP.segment``` -> ```jieba.cut```
ä½æ¯hanlpè¿è¡çæ¶åcpuä½¿ç¨çå¨130%å·¦å³(æºå¨æ¯2é¢ E5-2620 v4,æ¯é¢æ¯8æ ¸16çº¿ç¨,åå­å©ä½30G)
æä¸ç¥éæ¯ççå¡ä½è¿æ¯,éåº¦æ¢.



**Expected behavior**
æå¸æè½å¤å¤è¿ç¨,é«éè¿è¡hanlpåè¯


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core) 
- Python version:3.6.5
- HanLP version:
hanlp 2.1.0a36
hanlp-common 0.0.6
hanlp-downloader 0.0.20
hanlp-trie 0.0.2
pyhanlp 0.1.77

**Other info / logs**
è¿æ¯ä½¿ç¨
https://play.hanlp.ml/run/hanlp-zh
è¿è¡çç»æ

cpuä½¿ç¨çä¾æ§æ¯0,ç­äºå¥½ä¹,ä¸ç´å¨è½¬.
å¾çé¾æ¥
https://sm.ms/image/acSxlnBwpG49ehJ

ä»£ç æ¹èª**https://github.com/hankcs/HanLP/issues/1625**

å¨ç½ä¸æ¾å°çå¯è½ç¸å³çé®é¢
https://bbs.hankcs.com/t/topic/2128

* [x] I've completed this form and searched the web for solutions.",Other,Other,
40619,Bump pillow from 6.2.0 to 8.3.2,"[//]: # (dependabot-start)
â ï¸ **Dependabot is rebasing this PR** â ï¸ 

Rebasing might not happen immediately, so don't worry if this takes some time.

Note: if you make any changes to this PR yourself, they will take precedence over the rebase.

---

[//]: # (dependabot-end)

Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.2.0 to 8.3.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.3.2</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html</a></p>
<h2>Security</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
</ul>
<h2>Python 3.10 wheels</h2>
<ul>
<li>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</li>
</ul>
<h2>Fixed regressions</h2>
<ul>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a> [<a href=""https://github.com/t-vi""><code>@ât-vi</code></a>]</li>
</ul>
<h2>8.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Use snprintf instead of sprintf <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a> [<a href=""https://github.com/kmilos""><code>@âkmilos</code></a>]</li>
<li>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a> [<a href=""https://github.com/newpanjing""><code>@ânewpanjing</code></a>]</li>
<li>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a> [<a href=""https://github.com/gofr""><code>@âgofr</code></a>]</li>
<li>Do not use background or transparency index for new color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5564"">#5564</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Simplified code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5315"">#5315</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.3.2 (2021-09-02)</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
<li>
<p>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</p>
</li>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1 (2021-07-06)</h2>
<ul>
<li>
<p>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a>
[radarhere]</p>
</li>
<li>
<p>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a>
[radarhere]</p>
</li>
<li>
<p>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a>
[t-vi, radarhere]</p>
</li>
</ul>
<h2>8.3.0 (2021-07-01)</h2>
<ul>
<li>
<p>Use snprintf instead of sprintf. CVE-2021-34552 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a>
[radarhere]</p>
</li>
<li>
<p>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a>
[kmilos]</p>
</li>
<li>
<p>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a>
[baletu, radarhere, newpanjing, hugovk]</p>
</li>
<li>
<p>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a>
[gofr, radarhere]</p>
</li>
<li>
<p>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8013f130a5077b238a4346b73e149432b180a8ea""><code>8013f13</code></a> 8.3.2 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/23c7ca82f09df6ba1047d2d96714eb825f0d7948""><code>23c7ca8</code></a> Update CHANGES.rst</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8450366be331762ae327036e3c6658c517b05638""><code>8450366</code></a> Update release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/a0afe89990f5ba40a019afc2f22e1b656f8cfd03""><code>a0afe89</code></a> Update test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/9e08eb8f78fdfd2f476e1b20b7cf38683754866b""><code>9e08eb8</code></a> Raise ValueError if color specifier is too long</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bd5cf7db87c6abf7c3510a50170851af5538249f""><code>bd5cf7d</code></a> FLI tests for Oss-fuzz crash.</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/94a0cf1b14f09626c7403af83fa9fef0dfc9bb47""><code>94a0cf1</code></a> Fix 6-byte OOB read in FliDecode</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cece64f4be10ab28b12a83a3555af579dad343a5""><code>cece64f</code></a> Add 8.3.2 (2021-09-02) [CI skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e42238637651f191c2fc6e3f4024348c126e0ccc""><code>e422386</code></a> Add release notes for Pillow 8.3.2</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/08dcbb873217874eee0830fc5aaa1f231c5af4fa""><code>08dcbb8</code></a> Pillow 8.3.2 supports Python 3.10 [ci skip]</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.2.0...8.3.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.2.0&new-version=8.3.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/Ranlot/single-parameter-fit/network/alerts).

</details>",Other,Other,
40522,the emb18311.json file,"What is the file emb18311.json under the data file?can you help me,thank you in advance",Other,Other,
9946,EncoderRNN' object is not callable,"TRADE.py line 135 - you trying to call an object. Must be you mean not 
`encoded_outputs, encoded_hidden = self.encoder(story.transpose(0, 1), data['context_len'])`
but 
`encoded_outputs, encoded_hidden = self.encoder.forward(story.transpose(0, 1), data['context_len'])`
?
I wonder how this error exists in all versions of file TRADE.py and how it's even worked at all",Other,Other,
32672,"Anystyle with dense, complex, non-standard bibliography entries","First and foremost this thing is awesome, huge props.

I am working with pretty dense, non-standard bibliographic data that looks like this

`Di kafeterye [The cafeteria]. Tsukunft 1968 March-April, vol. 73, no. 3-4, pp. 121-129; signed Bashevis. Reprinted in the Forverts 1977 Oct. 13, 14, 20, 21, 27, 28, Nov. 3, p. 3; also reprinted in the Forverts 1979 June 3, p. B32. Appears in Yiddish in book form in: Mayses fun hintern oyvn (B7). Translated into English as âThe cafeteriaâ in: A friend of Kafka (C18).`

The standard model seems unable to parse this well. If trained on this, do you think a custom model might be able to? If so, how would you recommend going about training?",Other,Other,
17269,ScikitLearn.jl cannot find scikit-learn pkg,"On Linux, Julia 0.5.1, I:

```
Pkg.add(""ScikitLearn"")
Conda.add(""scikit-learn"")
```

Then 

```
using ScikitLearn
@sk_import linear_model: LinearRegression
```

yields:

```
ERROR: PyError (:PyImport_ImportModule) <type 'exceptions.ImportError'>
ImportError('No module named sklearn.linear_model',)
```

Are there some environment variables I need to assert are set up correctly?",Other,Other,
5134,Interpreting attention weights for more than one input features.,"How can we get attention weights for each input feature when our input consists of multiple inputs?
I am getting only one array of attention weights and I am not sure how to interpret it for multiple inputs.

shape of attention weights (attached as fig) is:
(300, 6) 
where 6 is the sequence_length/lookback steps/time steps. 

![attention_weight](https://user-images.githubusercontent.com/52854229/110059164-0877a000-7da7-11eb-84d1-4687600b6889.png)",Transparency & Explainability ,Transparency & Explainability ,
3708,HanLP2.1åè¯æ¶å¤§éææ¬ä¼æ¥éï¼RuntimeError: expected scalar type Float but found Long,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter. 
ä»¥ä¸å¿å¡«ï¼å¦åæä¸åçã
-->

**Describe the bug**
æå¨ç¨HanLP2.1åè¯æ¶ä¼éå°å¤§éææ¬æ¥éï¼RuntimeError: expected scalar type Float but found Long
å¾å¤é¿ææ¬åè¯æ¶ä¼æ¥è¿ä¸ªéãã
è°·æ­æäºåå¤©ï¼é½è¯´æ¯pytorchéçä»£ç é®é¢ï¼ä¸ç¡®å®æ¯ä¸æ¯æºç çé®é¢ï¼è¿æ¯æè¿è¾¹çåå å¯¼è´ãã

**Code to reproduce the issue**
å®æ´ä»£ç å¦ä¸

```python
import hanlp

HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # ä¸çæå¤§ä¸­æè¯­æåº
tok = HanLP[âtok/coarseâ]

# è¯»åèªå®ä¹è¯è¡¨
beauty_vocab = set()
with open(ââ¦/data/userdict.txtâ, ârâ, encoding=âutf-8â) as fin:
 for line in fin:
 beauty_vocab.add(line.strip())
tok.dict_combine = beauty_vocab

text = âææå¿æ°´çé£äº çº¢ï¼å¯¹å£çº¢è¿ç§ä¸è¥¿ççæ¯æ¯«æ æµæåå æ¯æ¬¡å¥äºæ°è²å°±å¨èæåæèä¸è¯ ä»æ»ä¼è¯´ å¥³çå°åºéè¦æå¤å°å£çº¢å èº«ä¸ºä¸ä¸ªç´ç·å¯è½ççä¸æ çº¢æä¸ç¾ç§ åååå è¨å½æ­£ä¼ é¿çå°¼çº¢ç®¡501 503 504 é¿çå°¼çº¢ç®¡ççæ¯ææç±çåéäº å¥çç¬¬ä¸æ¯æ¯504 ç¬¬ä¸æ¬¡ç¨å°±è¢«æè³å°äº ç«ç¶æè¿ä¹ä¸æ»çåé å®å¨æ¯å¥¶æ²¹ææ¯è´¨å° è¿åç°å¾é«çº§çååæ ççæ¯è¶ç± ç±ä¸éæ åæ¥åç¸ç»§æ¶äº501å503 ä»¥åä¸å®è¿ä¼ç»§ç»­æ¶ åç±ç¨åº¦ yslæ¹ç®¡17 13 17æ¯ææ¥å¤ç¨çæå¤æ¬¡çä¸æ¯ å ä¹æ¯å¤©é½å¨ç¨ ä¸ç¥éä»å¤©æ¦ä»ä¹çæ¶åå°±ç¨å® åæ²¡é ä¸ä¸ªå¤å¤©è¿å»å°±ä¸å»äºä¸å¤§æª åç±ç¨åº¦ 13æ¯åæå¥ç ç¸æ¯17å©ç¨çå°±æ²¡æé£ä¹é« æä¹è¯´å¢ 13è¿æ¯æäºæç®ç ç´ é¢æ¶è¯å®æ¯åçäº åç±ç¨åº¦ macå­å¼¹å¤´ see sheerä¹æ¯æçå¿å¤´ç± åä¹°çæ¶åæ¯å¤©é½æ¾åé éæ¶è¡¥ è¶çº§æ»æ¶¦ é¢è²çé¸¡æ¥å¸¸ ä¸ç­æ¶å®å¨ä¸ä¼æ¾å¾çªå è¶ç± åç±ç¨åº¦ chillå°è¾£æ¤ ä¹æ¯è¶ç±çä¸æ¯ æä¹æ¶é½å¥½ç æ¯«ä¸å¤¸å¼ ç§å¬ä¸ç¥éæ¶å¥æ¶ æ¦å®æ»ä¸ä¼åºé åç±ç¨åº¦ éè¯å°é»loveç³»å 300æ©çº¢è² 310æ¢å­è² é½æ¯æç±çé¢è² å¾æ¾ç½ åç±ç¨åº¦ é¿çå°¼å°èä¸504 å¤§åé¼é¼çå¥¶æ²¹æ© åç§è¢«å¤¸ æä¸ä½é£ç§èç ä½æ¯ï¼ççæ²¡æé£ä¹å¥½ å¥½å æ è®ºåæ¶èæ¶æè§é½ä¸æ¯å¾æ»¡æ æéå¸¸æ¯å å ååå¶ä»å£çº¢ä¸é¢ç¨ åç±ç¨åº¦ tf16 é£èå¤§ççªèè² å¿ä¸ä½å¥ç ä½æ¯ä½¿ç¨çå¾ä½ ä¸ç¥éä¸ºå¥ å¯è½ä¸å¤æ¥å¸¸ ä¹å¯è½æ¯ä¸èå¾ç¨ å·ç¬R åç±ç¨åº¦ é¦å¥å¿58 ä¹æ¯è·é£å¥ç æ®è¯´åç§éåç§å¬ éåé»ç® æ¯«ä¸ç¹è±«æ¿ä¸ butæç¹æ¯æ å¹¶ä¸æ¯å¾å¥½é©¾é©­å èä¸æè§ä¸æ¾ç½ï¼ åç±ç¨åº¦ yslé»ç®¡402 è¿åªæ¯ç²éç éåæ¥å¤ æè§ä¸è¬ ä¸­è§ä¸­ç© åç±ç¨åº¦ ysléé¢åé09 æ°´çº¢è² é¢è²ä¸é ä½æ¯ä½æ¯ è¿ä¸ªåéçè´¨å°æççæ¯ç±ä¸èµ·æ¥ ä¸è½æ¿å´ ç²ç²ç å¾ä¸å¥½ä¸åå åç±ç¨åº¦ 3ce åçè²116 å¹²ï¼å·¨å¹²ï¼çé¸¡å¹²ï¼æ ¹æ¬ä¸è½ç¨ ä¹°ä¹åè¢«é¢è²å¸å¼ å¾å¤äººé½è¯´å®å¾å¹² æå¿æ³ åå¹²è½å¹²æå¥æ · åä¸å¤©æä¸ååè æ©æ¨åç¨åé¨æåºåæ¶ ä¸ç­ä¸å°ä¸¤å°æ¶ å´å°±å¹²å°èµ·ç® åçè²å£çº¢å¾å¤ åä¸å«ä¹°è¿ä¸æ¯ åç±ç¨åº¦ æåè¯´ä¸å¥ æ¬äººé»ç® åè²æµ å¹²èµ·ç®åçº¹é åèç¥¨çè³èè¯è² ä»ä¾åè ç¬¬ä¸æ¬¡åè¿ä¹é¿çç¬è®° æææ­äº å¥½äºå°±é± å¹æ°Râ

cut_res = HanLP([text])[âtok/coarseâ][0]
print(cut_res)
```

**Describe the current behavior**
ä»£ç å¦ä¸ï¼ç¢°å°éå¸¸å¤å¦ä¸çtextææ¬ï¼åè¯å°±ä¼æ¥é

**Expected behavior**
è¿è¯·æåä½èï¼å¦ä½è§£å³è¿ä¸ªé®é¢ããä¸åæè°¢ï¼

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
mac OS 10.15.7

- Python version:
python 3.9.2

- HanLP version:
hanlp==2.1.0a20
hanlp-common==0.0.6
hanlp-trie==0.0.2
torch==1.7.1

**Other info / logs**
å®æ´çæ¥éä¿¡æ¯å¦ä¸ï¼

Traceback (most recent call last):
File â/Users/river/Desktop/1-tag-recall/note-tag/optimization/src/preprocess.pyâ, line 27, in
cut_res = HanLP([text])
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.pyâ, line 768, in call
return super().call(data, batch_size, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.pyâ, line 26, in decorate_context
return func(*args, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/torch_component.pyâ, line 629, in call
return super().call(data, **merge_dict(self.config, overwrite=True,
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/common/component.pyâ, line 36, in call
return self.predict(data, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.pyâ, line 512, in predict
output_dict = self.predict_task(self.tasks[task_name], task_name, batch, results, output_dict,
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.pyâ, line 590, in predict_task
output_dict, batch = self.feed_batch(batch, output_key, output_dict, run_transform, cls_is_bos, sep_is_eos,
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/multi_task_learning.pyâ, line 682, in feed_batch
âoutputâ: task.feed_batch(h,
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/init.pyâ, line 182, in feed_batch
return decoder(h, batch=batch, mask=mask)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.pyâ, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/components/mtl/tasks/ner/tag_ner.pyâ, line 35, in forward
contextualized_embeddings = self.secondary_encoder(contextualized_embeddings, mask=mask)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.pyâ, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.pyâ, line 310, in forward
x = layer(x, mask)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.pyâ, line 727, in _call_impl
result = self.forward(*input, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.pyâ, line 264, in forward
x = self.self_attn(x, mask)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.pyâ, line 727, in call_impl
result = self.forward(*input, **kwargs)
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/hanlp/layers/transformers/relative_transformer.pyâ, line 150, in forward
D = torch.einsum(ând,ld->nlâ, self.r_w_bias, pos_embed)[None, :, None] # head x 2max_len, æ¯ä¸ªheadå¯¹ä½ç½®çbias
File â/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/functional.pyâ, line 344, in einsum
return _VF.einsum(equation, operands) # type: ignore
RuntimeError: expected scalar type Float but found Long

* [x] I've completed this form and searched the web for solutions.",Other,Other,
27838,Add proper optimizer seeding to all optimizers in SharpLearning.Optimization,"This pull request will add proper seeding to all optimizer algorithms in `SharpLearning.Optimization`. The technique used is a single seed is set through the constructor, this seed is used to create a `random` generator, which will then create seeds for all underlying algorithms used in the given optimizer.

Note, that since this PR will change default seeding of the optimizers, the default behavior of the optimizers will not be the same as before this addition.

This should solve issue #69 .",Other,Other,
27459,"pip install and numpy, keras packages are forced to be uninstalled","Hi,

As I install the keras-attention-mechanism to my conda3 by pip, the essential packages of numpy and keras are unexpectedly being uninstalled. Do you know why? 

Bests,
Peiwan",Other,Other,
41843,Which kind of model is better for keyword-set classification?,"There exists a similar task that is named text classification.

But I want to find a kind of model that the inputs are keyword set. And the keyword set is not from a sentence.

For example:
```
input [""apple"", ""pear"", ""water melon""] --> target class ""fruit""
input [""tomato"", ""potato""] --> target class ""vegetable""
```

Another example:
```
input [""apple"", ""Peking"", ""in summer""] --> target class ""Chinese fruit""
input [""tomato"", ""New York"", ""in winter""] --> target class ""American vegetable""
input [""apple"", ""Peking"", ""in winter""] --> target class ""Chinese fruit""
input [""tomato"", ""Peking"", ""in winter""] --> target class ""Chinese vegetable""
```
Thank you.",Other,Other,
23356,Update sbt to 1.3.1,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) [from 1.3.0 to 1.3.1](https://github.com/sbt/sbt/compare/v1.3.0...v1.3.1).
[Release Notes/Changelog](https://github.com/sbt/sbt/releases/tag/v1.3.1)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: semver-patch",Other,Other,
28211,iOS: Switch domain to beta.newsblur.com,"We'll change it back to newsblur.com once everything's shipped, but I need to get the apps on the new infrastructure to stress test it.",Other,Other,
1228,Use NBInclude to test notebooks,We could modify NBInclude to be similar to nosebook.,Other,Other,
35959,Why the kernel size of discriminator is 4?,"I don't understand, is there any special role?
thanksï¼",Other,Other,
21097,Fix dataset reader for graphs with no node features,"### Reference Issues/PR
#37 input JSON

### what does this implement/fix? Explain your changes
The degree function used when there are no features in nodes returns a list, not a tuple.
 so I have made the required changes to fix that issue by simply updating the features in the conditional statements themselves.",Other,Other,
1312,Is it possible to share the pre-trained embedding for beer reviews?,"From https://github.com/yala/text_nn/blob/master/rationale_net/utils/embedding.py#L38 , it's trying to find `review+wiki.filtered.200.txt.gz`, is it possible to share the pre-trained embedding for beer reviews?",Other,Other,
13586,"How many epochs were required to create a model ""horse2zebra""?","How many epochs were required to create a model ""horse2zebra""?",Other,Other,
29711,å è½½æ¨¡åhanlp.pretrained.cws.LARGE_ALBERT_BASEæ¶åºé,"<!--
Thank you for reporting a possible bug in HanLP.
Please fill in the template below to bypass our spam filter. 
ä»¥ä¸å¿å¡«ï¼å¦åæä¸åçã
-->

**Describe the bug**
A clear and concise description of what the bug is.
å è½½cwsæ¨¡åalbertæ¶åºé
```bash
Downloading https://file.hankcs.com/hanlp/embeddings/albert_base_zh.tar.gz to /home/yihuazhou/.hanlp/embeddings/albert_base_zh.tar.gz

97.69%, 37.0 MB/37.9 MB, 92 KB/s, ETA 10 s Failed to load https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip. See traceback below:
================================ERROR LOG BEGINS================================
Traceback (most recent call last):
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 201, in download
 urlretrieve(url, tmp_path, reporthook)
 File ""/home/yihuazhou/miniconda3/lib/python3.8/urllib/request.py"", line 286, in urlretrieve
 raise ContentTooShortError(
urllib.error.ContentTooShortError: <urlopen error retrieval incomplete: got only 38807364 out of 39731739 bytes>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/component_util.py"", line 48, in load_from_meta_file
 obj.load(save_dir, **load_kwargs)
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 244, in load
 self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/common/component.py"", line 254, in build
 self.model = self.build_model(**merge_dict(self.config, training=kwargs.get('training', None),
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 34, in build_model
 model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/layers/transformers/loader.py"", line 39, in build_transformer
 bert_dir = get_resource(model_url)
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 340, in get_resource
 path = download(url=path, save_path=realpath)
 File ""/home/yihuazhou/miniconda3/lib/python3.8/site-packages/hanlp/utils/io_util.py"", line 214, in download
 installed_version, latest_version, latest_version_str = check_outdated()
ValueError: not enough values to unpack (expected 3, got 2)
=================================ERROR LOG ENDS=================================
When reporting an issue, make sure to paste the FULL ERROR LOG above.
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tokenizer = hanlp.load(hanlp.pretrained.cws.LARGE_ALBERT_BASE)
```

**Describe the current behavior**
A clear and concise description of what happened.
ç®åä¸è½½å®albertæ¨¡ååå è½½åºé

**Expected behavior**
A clear and concise description of what you expected to happen.
å¸æè½æ­£å¸¸å è½½ALbertæ¨¡å

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Python version: 3.8.3
- HanLP version: '2.0.0-alpha.61'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.",Other,Other,
6569,å¨çº¿çåè¯éè¯¯,"<!--
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼

ä»¥ä¸å¿å¡«ï¼å¦åæä¸åçã
-->

**Describe the bug**
ä¾ï¼é©¬æ¯åçªç¶æ¾è¯æ¶è´­æ¨ç¹ï¼åºä»·2700äº¿è®©å®éå¸ã
![å±å¹å¿«ç§ 2022-04-16 ä¸å1 35 30](https://user-images.githubusercontent.com/10105704/163663066-839e5542-25e5-47e9-8a6d-7e82bebffed1.png)

""**æ¨ç¹ï¼**"" 


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
```

**Describe the current behavior**
ä¸åªè¿ä¸ä¸ªå¥å­ï¼å¨çº¿çä¸å·²è§è¿å¤æ¬¡ **æè¯ä¸æ ç¹** åå¨ä¸èµ·çæåµã

---
ä½¿ç¨æ¬å°çhanlp-2.1.0b24ï¼ tokæ¨¡å **FINE_ELECTRA_SMALL_ZH**ï¼ å¯¹ä¾å¥åè¯ç»æ**æ­£ç¡®**ã

**Expected behavior**
A clear and concise description of what you expected to happen.

**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Python version: 3.9
- HanLP version: 2.1.0b24

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->",Other,Other,
23172,Update sbt-scalafmt to 2.5.1,"## About this PR
ð¦ Updates [org.scalameta:sbt-scalafmt](https://github.com/scalameta/sbt-scalafmt) from `2.5.0` to `2.5.1`

ð [GitHub Release Notes](https://github.com/scalameta/sbt-scalafmt/releases/tag/v2.5.1) - [Version Diff](https://github.com/scalameta/sbt-scalafmt/compare/v2.5.0...v2.5.1)

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/22261658573e0e68bdeca8520f5da78bd61adbdc/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scalameta"", artifactId = ""sbt-scalafmt"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.scalameta"", artifactId = ""sbt-scalafmt"" }
}]
```
</details>

<sup>
labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1
</sup>",Other,Other,
38969,Update sbt-sonatype to 3.9.3,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.2 to 3.9.3.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.3) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.2...3.9.3)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/d4cd7d9f31adba210d2c7d88301b6531bcd4af01/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
17376,License info,"Hi,

I've found MIT license notice at [README](https://github.com/jasonwu0731/trade-dst/blob/master/README.md).
Can you let me get your copyright notice?

Thanks!",Other,Other,
20952,Windows 10 Fresh Install,"Run in cmd:
```
ECHO Python python-3.8.5-amd64
ECHO Git-2.28.0-64-bit
CD /D ""D:\Wide\jetoil_mod""
""D:\DEV\python385winx64\python.exe"" -m pip install --upgrade pip
pip3 install virtualenv
virtualenv --python ""D:\DEV\python385winx64\python.exe"" venv
.\venv\Scripts\activate
REM GOTO method2
git clone https://github.com/apoorva-dave/LicensePlateDetector
CD LicensePlateDetector
pip3 install -r requirements.txt
python PredictCharacters.py
```

Get error:
```
Traceback (most recent call last):
 File ""PredictCharacters.py"", line 1, in <module>
 import SegmentCharacters
 File ""D:\Wide\jetoil_mod\LicensePlateDetector\SegmentCharacters.py"", line 4, in <module>
 from skimage.measure import regionprops
 File ""D:\Wide\jetoil_mod\venv\lib\site-packages\skimage\__init__.py"", line 125, in <module>
 from .util.dtype import (img_as_float32,
 File ""D:\Wide\jetoil_mod\venv\lib\site-packages\skimage\util\__init__.py"", line 17, in <module>
 from ._map_array import map_array
 File ""D:\Wide\jetoil_mod\venv\lib\site-packages\skimage\util\_map_array.py"", line 2, in <module>
 from ._remap import _map_array
ImportError: DLL load failed while importing _remap: The specified module could not be found.
```

Fix by editing SegmentCharacters.py and import matplotlib before skimage
```
import numpy as np
import matplotlib
from skimage.transform import resize
from skimage import measure
from skimage.measure import regionprops
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import DetectPlate
```",Other,Other,
28422,"Disable stories loading animation (if preference), to avoid memory leak","I've had trouble with Newsblur memory leaks for a long time, and recently Newsblur has been crashing its tab in Chrome multiple times a day. I tracked it down to this code. I'm not sure of the ultimate cause, but this seems to fix it for me (I have the preference set to false). I hope you can merge this and put it on the live site.",Other,Other,
637,Bump numpy from 1.17.2 to 1.22.0,"Bumps [numpy](https://github.com/numpy/numpy) from 1.17.2 to 1.22.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>
<blockquote>
<h2>v1.22.0</h2>
<h1>NumPy 1.22.0 Release Notes</h1>
<p>NumPy 1.22.0 is a big release featuring the work of 153 contributors
spread over 609 pull requests. There have been many improvements,
highlights are:</p>
<ul>
<li>Annotations of the main namespace are essentially complete. Upstream
is a moving target, so there will likely be further improvements,
but the major work is done. This is probably the most user visible
enhancement in this release.</li>
<li>A preliminary version of the proposed Array-API is provided. This is
a step in creating a standard collection of functions that can be
used across application such as CuPy and JAX.</li>
<li>NumPy now has a DLPack backend. DLPack provides a common interchange
format for array (tensor) data.</li>
<li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The
new methods provide a complete set of the methods commonly found in
the literature.</li>
<li>A new configurable allocator for use by downstream projects.</li>
</ul>
<p>These are in addition to the ongoing work to provide SIMD support for
commonly used functions, improvements to F2PY, and better documentation.</p>
<p>The Python versions supported in this release are 3.8-3.10, Python 3.7
has been dropped. Note that 32 bit wheels are only provided for Python
3.8 and 3.9 on Windows, all other wheels are 64 bits on account of
Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.
All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix
the occasional problems encountered by folks using truly huge arrays.</p>
<h2>Expired deprecations</h2>
<h3>Deprecated numeric style dtype strings have been removed</h3>
<p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,
and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>
<h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>
<p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that
users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both
deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with
the appropriate value for the <code>usemask</code> parameter.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>
<li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>
<li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>
<li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>
<li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>
<li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>
<li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>
<li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.17.2...v1.22.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.17.2&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/ShawnyXiao/TextClassification-Keras/network/alerts).

</details>",Other,Other,
31341,new Release?,"Hello, 
I'm interested in a version without the pixel boarder bugs. Is there any new release planned in the near future? 
Or could the artifacts from the dev build be kept longer (https://ci.appveyor.com/project/abreheret/pixelannotationtool/build/artifacts) ?",Other,Other,
33209,Update sbt-release to 1.0.12,"Updates com.github.gseitz:sbt-release from 1.0.11 to 1.0.12.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.github.gseitz"", artifactId = ""sbt-release"" } ]
```
</details>

labels: semver-patch",Other,Other,
22626,Please mention the validation set in README.md,"Hi, I think everyone who presented in this competition just played it dirty by not mentioning the validation set. I'd like to request that you mention the images taken in the validation set so that reader will not be misled by the results if one tries to reproduce them. Btw nice code we'll also make our code public soon :)",Privacy,Privacy,FIXED
25838,"Limits on parameter ranges: PolynomialKernel,","PolynomialKernel required d to be integer. ExponentialKernel, RationalQuadraticKernel, PowerKernel, LogKernel all require gamma to be <= 1. What's the reason for that ? (Maybe a kernel would not be Mercer if gamma > 1 (or d real, for PolynomialKernel), but it might still be useful. We could just amend ismercer() so that it actually checks the parameter value?)",Other,Other,
42710,Dictionary fixes,"Small fixes for opening the dictionary file:
1. ""b"" flag is required when opening gzip files on Windows
2. The ""UTF-8"" flag is redundant / misleading when opening the gzip file
3. The ""UTF-8"" flag is necessary, however, on the gzip reader stream

Without (3), accented characters in place/person names are not correctly matched. A test case: ""Weber, M., 1922. Wirtschaft und Gesellschaft: Grundriss der verstehenden Soziologie. JCB Mohr, TÃ¼bingen."" On Anycite.io, TÃ¼bingen is not recognised as a place, even though it is in the dictionary. This fixes that.",Other,Other,
27660,Fix for coding dictionary items,"Some faulty logic meant that items could be incorrectly coded when appearing multiple times, depending on the previously applied codes. Test for, example ""may"" (coded as 4=""place"", instead of correctly 3=2+1=""name+month"")",Other,Other,
39632,Added python 3.x support,"Hey @AnthonyMRios,

Can you please check this out? This pull request should be able to fix your issue #6 .

Best,
J",Other,Other,
9203,On homepage- option to serialize training line after label assignment,"This is basically the XML, but stripped down?

```
When I visit https://anystyle.io/
 And I paste in a single citation (that is poorly processed)
 And I assign many labels
 Then I want XML, but only to serialize a line for my training.txt file (no <reference>)
 And it would be great if there was an option to copy that training line to my clipboard
```",Other,Other,
9142,anystyle.io cli api fails to execute requests within a loop,"https://anystyle.io/ says it cannot run more than 1000 records per request. I have installed ruby gem using command line and trying to read 10 lac records, 1000 records per loop. After 2 iterations, it fails to give me the result. Can anyone help me figure out this issue?",Other,Other,
22136,How to render?,There is a `render` module but I don't know how to use it. Could you please provide some examples of how to render?,Other,Other,
6577,DoubleArrayTrieéçLongestSearcherçnextæ¹æ³ï¼å½ä¼ å¥çtreemapçvalueä¸ºnullæ¶ï¼ä¼å¼åbug,"<!--
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼
æé®è¯·ä¸è®ºåï¼ä¸è¦åè¿éï¼

ä»¥ä¸å¿å¡«ï¼å¦åæä¸åçã
-->

**Describe the bug**

DoubleArrayTrieéçLongestSearcherçnextæ¹æ³ï¼å½ä¼ å¥çtreemapçvalueä¸ºnullæ¶ï¼ä¼å¼åbug
![image](https://user-images.githubusercontent.com/13550295/130592417-32ae4155-0ea3-4c7a-ad17-9fd2c67e92a5.png)


**Code to reproduce the issue**

```java
 public void testLongestSearcherWithNullValue() {
 TreeMap<String, String> buildFrom = new TreeMap<String, String>();
 TreeMap<String, String> buildFromValueNull = new TreeMap<String, String>();
 String[] keys = new String[]{""he"", ""her"", ""his""};
 for (String key : keys) {
 buildFrom.put(key, key);
 buildFromValueNull.put(key, null);
 }
 DoubleArrayTrie<String> trie = new DoubleArrayTrie<String>(buildFrom);
 DoubleArrayTrie<String> trieValueNull = new DoubleArrayTrie<String>(buildFromValueNull);

 String text = ""her3he6his-hers! "";

 DoubleArrayTrie<String>.LongestSearcher searcher = trie.getLongestSearcher(text.toCharArray(), 0);
 DoubleArrayTrie<String>.LongestSearcher searcherValueNull = trieValueNull.getLongestSearcher(text.toCharArray(), 0);

 while (true) {
 boolean next = searcher.next();
 boolean nextValueNull = searcherValueNull.next();

 if (next && nextValueNull) {
 assertTrue(searcher.begin == searcherValueNull.begin && searcher.length == searcherValueNull.length);
 } else if (next || nextValueNull) {
 assert false;
 break;
 } else {
 break;
 }
 }
 }
```

**Describe the current behavior**
ç±äºmapä¼ å¥çvalueä¸ºnullï¼å³ä½¿å­å¨ï¼ä¹æ¥ä¸å°

**Expected behavior**
A clear and concise description of what you expected to happen.
æ ¹æ®lengthæèindexè¿è¡å¤æ­
ä¿®å¤åè§pull request (https://github.com/hankcs/HanLP/pull/1674)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Python version: xxx
- HanLP version: 1.8.2

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

* [x] I've completed this form and searched the web for solutions.
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->
<!-- åè¡¨ååæç´¢ï¼æ­¤å¤ä¸å®è¦å¾éï¼ -->",Other,Other,
39491,ç®ç¹è½¬æ¢éè¯¯ï¼å§åâæ¢é¹âè½¬ä¸ºç¹ä½ååæäºâæ¨é¹â,çæ¬ï¼portable-1.3.4,Other,Other,
27703,Errors related to missing library,"After installing anystyle-parser, and loading it in an irb session as follows gives

<pre>
irb(main):002:0> require 'anystyle/parser'
LoadError: cannot load such file -- language_detector
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:27:in `<class:Parser>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:4:in `<module:Parser>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:2:in `<module:Anystyle>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:1:in `<top (required)>'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser.rb:14:in `<top (required)>'
</pre>


Although the library is loaded all right, the parse method cannot be found

<pre>
irb(main):007:0> Anystyle.parse 
NoMethodError: undefined method `parse' for Anystyle:Module

</pre>


saji",Other,Other,
9680,Update scalacheck to 1.14.2,"Updates [org.scalacheck:scalacheck](http://www.scalacheck.org) from 1.14.1 to 1.14.2.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scalacheck"", artifactId = ""scalacheck"" } ]
```
</details>

labels: semver-patch",Other,Other,
19089,Update shapeless to 2.3.7,"Updates [com.chuusai:shapeless](https://github.com/milessabin/shapeless) from 2.3.6 to 2.3.7.
[GitHub Release Notes](https://github.com/milessabin/shapeless/releases/tag/v2.3.7) - [Version Diff](https://github.com/milessabin/shapeless/compare/v2.3.6...v2.3.7)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9b1175210c902dbb63ea03d1a2136868ffb54f16/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.chuusai"", artifactId = ""shapeless"" } ]
```
</details>

labels: library-update, semver-patch",Other,Other,
18724,Show more recent labels,"User request:

The number of labels that appear below the âAssign labelâ float box in the edit section is too small for my taste. I used about seven different labels regularly to correct entries and â as it goes ;) â always the one I needed had just dropped out of the list. Would it be difficult to just increase the list to seven or eight items? Even five (one more) would be a benefit, IMO.",Other,Other,
28660,Android: More dark theme improvements.,With !important stripped these changes make the HTML based newsletters look reasonable.,Other,Other,
7879,About trainning pose estimator,"Hello, I changed the model in pose estimator from resnet18 to resnet50, I use the points loss, but when I train the estimator on th mask dataset, the orientation loss does not convergeï¼could you give me some suggestionï¼
![image](https://user-images.githubusercontent.com/34231078/56078641-fb511100-5e1c-11e9-9bcd-8bac773f74c1.png)
![image](https://user-images.githubusercontent.com/34231078/56078642-fe4c0180-5e1c-11e9-9361-346168bec827.png)
![image](https://user-images.githubusercontent.com/34231078/56078643-0015c500-5e1d-11e9-81e7-800a13289c46.png)",Other,Other,
18982,Lead over .mp4 creation from predicted results.,"Hi junqiangchen,

Can you provide a lead over the conversion of prediction results to .mp4 format?",Other,Other,
27791,Is there a way to keep textual labels / targets as a part of the trained model?,"First of all, thank you for sharing this library. 
Second, would be great to make mapping between columns and feature names mode obvious. 
If model was serialized and saved on one computer and deserialized and loaded on the other one, then second computer will have no idea what's the meaning of labels / targets, because model keeps them as double values. 

**Save model**

```C#

var labels = new[] { ""Good"", ""Bad"", ""Average"" ... };
var labelKeys = labels.Select((v, i) => (double) i); // take label key instead of name 
var learner = new ClassificationDecisionTreeLearner();
var model = learner.Learn(items, labelKeys); // is there any reason not to use string labels instead of doubles?

using (var memoryStream = new MemoryStream())
{
 var serializer = new GenericXmlDataContractSerializer();
 serializer.Serialize<IPredictorModel<double>>(model, () => new StreamWriter(memoryStream));
 db.Save(memoryStream.ToArray()); // convert XML to byte[] and save as Blob to DB
}
```

**Load model**

```C#
var xmlModel = db.Get(...).AsBlob().GetBytes(); // load saved model from blob column in DB

using (var memoryStream = new MemoryStream(xmlModel))
{
 var serializer = new GenericXmlDataContractSerializer();
 var xml = serializer.Deserialize<IPredictorModel<double>>(() => new StreamReader(memoryStream));
}
```

As a result, loaded model has property `Targets` that contains some double values, like 1, 2, 3, 4 and there is no way to understand that initially they meant ""Good"", ""Bad"", etc 

**Question**

Is there a way to save original **string labels / targets** as a part of the model to make prediction results human-readable?",Other,Other,
37822,Too slow in Firefox,"With `::Self` it barely responses to typing.

Browser: Firefox 62.0b13",Other,Other,
4051,Add update method to GlfwWindow for compatibility with dm_robotics MoMa renderer,Adressing this issue https://github.com/deepmind/dm_robotics/issues/11.,Other,Other,
26430,A pair of 256x256 images and I am already out of memory?,"Just downloaded the zip and ran it in python. I picked 2 images for style transfer and I have put them separately in trainA and trainB in another folder called style. I did not make valA and valB folders. I called:

`python train.py --dataroot ./datasets/style`

everything runs until it hits x + self.conv_block(x) which is part of the residual layer then I get cuda error on running out of memory? GPUs = 2 GTX 980 with 3GB in each so 6GB in total. In the printed info below it shows that I have 11.378 million params. What did I do wrong? Do I need to lower my residual blocks and change the default epoch size?

Edited: Btw how do you calculate param size anways? How big is 11M param in memory?

```
CustomDatasetDataLoader

dataset [UnalignedDataset] was created

#training images = 1

C:\Users\name\Desktop\Painter\models\networks.py:45: UserWarning: nn.init.normal is now 

deprecated in favor of nn.init.normal_.

 init.normal(m.weight.data, 0.0, 0.02)

---------- Networks initialized -------------

[Network G_A] Total number of parameters : 11.378 M

[Network G_B] Total number of parameters : 11.378 M

[Network D_A] Total number of parameters : 2.765 M

[Network D_B] Total number of parameters : 2.765 M

-----------------------------------------------
```

Actual Error

> model [CycleGANModel] was created
> create web directory ./checkpoints\experiment_name\web...
> THCudaCheck FAIL file=c:\programdata\miniconda3\conda-bld\pytorch_1524549877902\work\aten\src\thc\generic/THCStorage.cu line=58 error=2 : out of memory
> Traceback (most recent call last):
> File ""train.py"", line 31, in <module>
> model.optimize_parameters()
> File ""C:\Users\Luke Chen\Desktop\Painter\models\cycle_gan_model.py"", line 157, in optimize_parameters
> self.backward_G()
> File ""C:\Users\Luke Chen\Desktop\Painter\models\cycle_gan_model.py"", line 146, in backward_G
> self.rec_B = self.netG_A(self.fake_A)
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 491, in __call__
> result = self.forward(*input, **kwargs)
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\parallel\data_parallel.py"", line 112, in forward
> return self.module(*inputs[0], **kwargs[0])
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 491, in __call__
> result = self.forward(*input, **kwargs)
> File ""C:\Users\Luke Chen\Desktop\Painter\models\networks.py"", line 199, in forward
> return self.model(input)
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 491, in __call__
> result = self.forward(*input, **kwargs)
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\modules\container.py"", line 91, in forward
> input = module(input)
> File ""C:\Users\Luke Chen\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 491, in __call__
> result = self.forward(*input, **kwargs)
> File ""C:\Users\Luke Chen\Desktop\Painter\models\networks.py"", line 241, in forward
> out = x + self.conv_block(x)
> RuntimeError: cuda runtime error (2) : out of memory at c:\programdata\miniconda3\conda-bld\pytorch_1524549877902\work\aten\src\thc\generic/THCStorage.cu:58",Other,Other,
36906,anystyle.io cli api fails to execute requests within a loop,"https://anystyle.io/ says it cannot run more than 1000 records per request. I have installed ruby gem using command line and trying to read 10 lac records, 1000 records per loop. After 2 iterations, it fails to give me the result. Can anyone help me figure out this issue?",Other,Other,
23098,modify photo enhancement description,"Hello, this library looks very cool.

Reading the readme.md i was struck by the description of `iPhone photo to DSLR photo`.
It feel like wront to associate a depth of field from some lenses to a particualr phone or a DSLR.

the generic DSLR lenses cannot make that blurry picture and the new iphone looks like it can.
I would stay generic with a simpler description.

Regards",Other,Other,
5209,loss can't down,"Thank for your share? I have a question is as follows:
The loss function cannot be reduced and all samples are predicted to be a certain category

I am looking forward for your answer, thank you very much!",Other,Other,
43217,Fail nested type for TensorFlowType at compile time,Right now `TensorFlowType` uses the same traits as `BigQueryType` and `DatastoreType` but doesn't implement methods that handle nested types. We should either duplicate the implicit chain logic or figure out a way to fail it at compile time.,Other,Other,
29586,How can I get ground truth pose and the segmentation mask?,"Hi Jimmy

I have already downloaded the oil change dataset, I would like to ask how to get the ground truth poses and the segmentation masks from the .json file? (specifically, I want to get the ground truth pose and the segmentation mask of the blue oil funnel)

Thank you so much!",Other,Other,
44767,Update sbt-scalafmt to 2.4.0,"Updates [org.scalameta:sbt-scalafmt](https://github.com/scalameta/sbt-scalafmt) from 2.3.4 to 2.4.0.
[GitHub Release Notes](https://github.com/scalameta/sbt-scalafmt/releases/tag/v2.4.0) - [Version Diff](https://github.com/scalameta/sbt-scalafmt/compare/v2.3.4...v2.4.0)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/a11b52fe53f6c44477ff56a4bbb154e09e6c6aea/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scalameta"", artifactId = ""sbt-scalafmt"" } ]
```
</details>

labels: sbt-plugin-update, semver-minor",Other,Other,
14130,Typos and fix suggestions in Example 1 notebook.,"Thx for the awesome tutorial ð 
I have run it on my own, and found some typos and python compatibility issues.

1. this is probably `12 output nodes`?
<img width=""836"" alt=""image"" src=""https://user-images.githubusercontent.com/14329563/32362950-1efb6d28-c0b0-11e7-8cf6-aff39d73ad79.png"">

2. `432 = 36 x 12`, `36 x 12 + 12`
<img width=""616"" alt=""image"" src=""https://user-images.githubusercontent.com/14329563/32362966-37dde550-c0b0-11e7-9b1c-201982584fdf.png"">

3. `next()` in iters as been renamed to `__next__()` in python 3. If proceeded with this, keras model would give `DataGen is not iterable` error. 
<img width=""744"" alt=""image"" src=""https://user-images.githubusercontent.com/14329563/32362984-4e15ef3e-c0b0-11e7-8694-e92321f1d299.png"">

I have tested this with `Keras==2.0.9`

4. Similarly, calling next iteration have been changed in python3.
<img width=""835"" alt=""image"" src=""https://user-images.githubusercontent.com/14329563/32363035-8e7cff40-c0b0-11e7-9337-3ae0b244a89a.png"">

All `datagen.next()` had to be modified to `next(datagen)` in this cell.",Other,Other,
19572,FileNotFoundError: [Errno 2] No such file or directory: 'pickle_files/embeddings.p',"In dataset.py we read
 embedding_path = 'pickle_files/embeddings.p'
 word_to_indx_path = 'pickle_files/vocabIndxDict.p'
 embedding_tensor = pickle.load(open(embedding_path,'rb'))
 word_to_indx = pickle.load(open(word_to_indx_path,'rb'))
However, the two files are not in the zip-code?",Other,Other,
40967,pip install from git fails,"I tested it on several machines in clean conda installations and got exactly the same error message. All machines run Ubuntu 20.04.

```bash
pip install git+git://github.com/deepmind/dm_control.git
```

fails with the following error message

```bash
ERROR: Command errored out with exit status 1:
 command: /home/kostrikov/miniconda3/envs/test/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-e4i_e72j/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-e4i_e72j/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-celpm6j2/install-record.txt --single-version-externally-managed --compile --install-headers /home/kostrikov/miniconda3/envs/test/include/python3.9/dm-control
 cwd: /tmp/pip-req-build-e4i_e72j/
 Complete output (41 lines):
 running install
 running build_mjbindings
 Traceback (most recent call last):
 File ""/tmp/pip-req-build-e4i_e72j/dm_control/autowrap/autowrap.py"", line 149, in <module>
 app.run(main)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/site-packages/absl/app.py"", line 312, in run
 _run_main(main, args)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/site-packages/absl/app.py"", line 258, in _run_main
 sys.exit(main(argv))
 File ""/tmp/pip-req-build-e4i_e72j/dm_control/autowrap/autowrap.py"", line 113, in main
 parser.parse_consts_typedefs(src)
 File ""/tmp/pip-req-build-e4i_e72j/dm_control/autowrap/binding_generator.py"", line 355, in parse_consts_typedefs
 self.recurse_into_conditionals(tokens)
 File ""/tmp/pip-req-build-e4i_e72j/dm_control/autowrap/binding_generator.py"", line 370, in recurse_into_conditionals
 self.typedefs_dict.update({token.name: token.typename})
 File ""/tmp/pip-req-build-e4i_e72j/dm_control/autowrap/codegen_util.py"", line 78, in __setitem__
 raise ValueError(""Key '{}' already exists."".format(k))
 ValueError: Key 'mjtNum' already exists.
 Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""/tmp/pip-req-build-e4i_e72j/setup.py"", line 176, in <module>
 setup(
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/site-packages/setuptools/__init__.py"", line 153, in setup
 return distutils.core.setup(**attrs)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/distutils/core.py"", line 148, in setup
 dist.run_commands()
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/distutils/dist.py"", line 966, in run_commands
 self.run_command(cmd)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/distutils/dist.py"", line 985, in run_command
 cmd_obj.run()
 File ""/tmp/pip-req-build-e4i_e72j/setup.py"", line 140, in run
 self.run_command('build_mjbindings')
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/distutils/cmd.py"", line 313, in run_command
 self.distribution.run_command(command)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/distutils/dist.py"", line 985, in run_command
 cmd_obj.run()
 File ""/tmp/pip-req-build-e4i_e72j/setup.py"", line 115, in run
 subprocess.check_call(command)
 File ""/home/kostrikov/miniconda3/envs/test/lib/python3.9/subprocess.py"", line 373, in check_call
 raise CalledProcessError(retcode, cmd)
 subprocess.CalledProcessError: Command '['/home/kostrikov/miniconda3/envs/test/bin/python', 'dm_control/autowrap/autowrap.py', '--header_paths=/home/kostrikov/.mujoco/mujoco210/include/mjdata.h /home/kostrikov/.mujoco/mujoco210/include/mjmodel.h /home/kostrikov/.mujoco/mujoco210/include/mjrender.h /home/kostrikov/.mujoco/mujoco210/include/mjui.h /home/kostrikov/.mujoco/mujoco210/include/mjvisualize.h /home/kostrikov/.mujoco/mujoco210/include/mjxmacro.h /home/kostrikov/.mujoco/mujoco210/include/mujoco.h', '--output_dir=/tmp/pip-req-build-e4i_e72j/build/lib/dm_control/mujoco/wrapper/mjbindings']' returned non-zero exit status 1.
 ----------------------------------------
ERROR: Command errored out with exit status 1: /home/kostrikov/miniconda3/envs/test/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-e4i_e72j/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-e4i_e72j/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-celpm6j2/install-record.txt --single-version-externally-managed --compile --install-headers /home/kostrikov/miniconda3/envs/test/include/python3.9/dm-control Check the logs for full command output.
```",Other,Other,
6856,0.31.7.0: Refactor BaysianOptimizer and add parallel computation,"This pull request refactors the `BayesianOptimizer` implementation to be use the same principles as the `SMACOptimizer`. The two optimizers are both model based optimizers, and should therefore be very similar in implementation. The `BayesianOptimizer` can be viewed a basic implementation of model based optimization, which the `SMACOptimizer` builds a few tricks on top of.
A base class for model based optimizers seems to be the next logical step, but that will follow in a later pull request.

The refactoring enables use of the `BayesianOptimizer` in an ""open loop"" style just like the `SMACOptimizer`. See the unit tests for an example.

This pull request also adds the option of parallel computation to the `BayesianOptimizer`. This work was originally added in #119.

Note that when running in parallel, and using the `Optimize(Func<double[], OptimizerResult> functionToMinimize)` method, the order of the results will not be reproducible. The individuel results will remain the same, but the order of the results will vary between runs.

I recommend only using the parallel version if the provided `functionToMinimize` is running serial computation, and is slow to compute.",Other,Other,
23031,The number of training dataset,"Hello, I have a question. 
How many images in the training dataset can be used to train an effective model?
For example, domain X has 6000 undistorted images, domain Y has only 145 images with five distortion types. The scenes of Y are included in X . In this case, could generate images with five distortion types, which is similar to Y domain?",Other,Other,
30722,biblatex support & other remarks,"Brilliant, thank you so much. For the most part, AnyStyle already works extremely well compared with any other parser I tried so far.

Just a few observations and remarks:
- ""45(3):23-45"" is not separated, and parsed as ""volume={3}"" and ""issue={23-45}""
- Sometimes, ""ed."" is chopped off of names, e.g. ""Alfred"" -> ""Alfr""
- Frequently, surrounding quotes, and final commas are not removed from titles.
- Frequently, periods are chopped off of initials, e.g. ""author = {Doe, John R}""
- Names containing initials without periods are inverted, e.g. ""author = {JR, Doe}""
- ""Transl."" is not removed from a translator's name.
- ""Accessed"" is not removed from an ""Accessed"" field (also, this field should be named ""Urldate"" instead, at least for biblatex, see below)
- The (field) labels being used seem to based, mostly, on CSL variables, resp., Zotero field names. I'd suggest introducing a few more, e.g., 
 - ""Series Title"" = CSL ""collection-title"", 
 - ""Series Number"" = CSL ""collection-number"", 
 - ""Book Author"" = CSL ""container-author"", 
 - ""No of Volumes"" = CSL ""number-of-volumes"", 
 - ""Report number"" = CSL ""number"", and 
 - CSL ""original-date""; 
 - maybe also the (hopefully) soon-to-be-introduced ""Volume Title"" = CSL ""volume-title"",
 - maybe an option to label authors as a corporate authors.

I'd also suggest adding biblatex as a separate output format. While some might want to continue using classical bibtex, I see huge advantages in using biblatex and its much more comprehensive data model, and it'd be nice if AnyStyle could output the biblatex format directly. The most important differences between bibtex and biblatex include:
- Improved handling of dates:
 - ""date = {YYYY-MM-DD}"" instead of separate year, month, day fields
 - date ranges in the format ""date = {YYYY-MM-DD/YYYY-MM-DD}""
 - ""urldate"" (instead of ""accessed""; also I'm not sure whether there's any bibtex variant that would accept ""accessed"")
 - ""origdate""
- An ""online"" entry type
- An ""institution"" field for report and thesis entries, and an ""organization"" field for manual and online entry types (""authority"", again, is not recognized by biblatex, and probably by no other bibtex variant either)
- An ""incollection"" entry type (AnyStyle uses this, though output for classical bibtex should probably return to ""inbook"" here)
- A ""maintitle"" field, which will have to be used if AnyStyle starts using CSL ""volume-title"" (mapping is a bit complicated here, but I'd be happy to help here, as with all biblatex questions).",Other,Other,
36302,Update jackson-databind to 2.9.9.3,"Updates com.fasterxml.jackson.core:jackson-databind from 2.9.9.2 to 2.9.9.3.

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [{ groupId = ""com.fasterxml.jackson.core"", artifactId = ""jackson-databind"" }]
```
</details>",Other,Other,
26,For Multiple files,"is there a way of loading multiple files of same schema ,or do i have to combine all files in to one big giant file ?",Other,Other,
13495,What if some sequences has only one record in it?,"I was using UnSupervisedIOHMM to learn sequences, but the `predict_log_proba` in `E_step` keeps give me **ValueError**: zero-size array to reduction operation maximum which has no identity. The error arises from `X = self._transform_X(X)` in `predict_log_proba` and my self.inp_transitions[seq]=[] due to the length of these sequences are 1. So they don't have inp_transitions. Is there any way I have get rid of this problem?",Other,Other,
14799,[Security] Bump lodash from 4.17.11 to 4.17.15,"Bumps [lodash](https://github.com/lodash/lodash) from 4.17.11 to 4.17.15. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from The GitHub Security Advisory Database.*

> **High severity vulnerability that affects lodash, lodash-es, lodash-amd, lodash.template, lodash.merge, lodash.mergewith, and lodash.defaultsdeep**
> Affected versions of lodash are vulnerable to Prototype Pollution. 
> The function defaultsDeep could be tricked into adding or modifying properties of Object.prototype using a constructor payload.
> 
> Affected versions: < 4.17.13

</details>
<details>
<summary>Commits</summary>

- [`ddfd9b1`](https://github.com/lodash/lodash/commit/ddfd9b11a0126db2302cb70ec9973b66baec0975) Bump to v4.17.15.
- [`b185fce`](https://github.com/lodash/lodash/commit/b185fcee26b2133bd071f4aaca14b455c2ed1008) Rebuild lodash and docs.
- [`be87d30`](https://github.com/lodash/lodash/commit/be87d303941222b97c482755afc0f4a77ce46c30) Bump to v4.17.14.
- [`a6fe6b1`](https://github.com/lodash/lodash/commit/a6fe6b1e174fd02b5e60eb2664405f4c1262c300) Rebuild lodash and docs.
- [`e371828`](https://github.com/lodash/lodash/commit/e37182845f16715a0d1c391c8662d83c55609cee) Bump to v4.17.13.
- [`357e899`](https://github.com/lodash/lodash/commit/357e899e685872b4af5403ecc4b2a928f961ae63) Rebuild lodash and docs.
- [`fd9a062`](https://github.com/lodash/lodash/commit/fd9a062d57646450b61f74029315abd4cc834b08) Bump to v4.17.12.
- [`e77d681`](https://github.com/lodash/lodash/commit/e77d68121ff00ba86b53eed5893d35adfe94c9dd) Rebuild lodash and docs.
- [`629d186`](https://github.com/lodash/lodash/commit/629d1865793182cd967196716f4beff223aa4a91) Update OpenJS references.
- [`2406eac`](https://github.com/lodash/lodash/commit/2406eac542b2a1282be8d812a6d8a45433ade80a) Fix minified build.
- Additional commits viewable in [compare view](https://github.com/lodash/lodash/compare/4.17.11...4.17.15)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.11&new-version=4.17.15)](https://dependabot.com/compatibility-score.html?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.11&new-version=4.17.15)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it). To ignore the version in this PR you can just close it
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>",Other,Other,
34680,Update jackson-databind to 2.9.10,"Updates [com.fasterxml.jackson.core:jackson-databind](http://github.com/FasterXML/jackson) from 2.9.9.3 to 2.9.10.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.fasterxml.jackson.core"", artifactId = ""jackson-databind"" } ]
```
</details>",Other,Other,
40675,No module named pytorch_fid,"Hi!

I pip installed PyTorch-fid but when I run
`python -m pytorch_fid ~/fid/real_data ~/fid/synthetic_data`
I get 

`/usr/bin/python: No module named pytorch_fid`

And when running

`python3 -m pytorch_fid ~/fid/real_data ~/fid/synthetic_data`
```
Traceback (most recent call last):
 File ""/usr/local/Modules/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
 ""__main__"", mod_spec)
 File ""/usr/local/Modules/lib/python3.7/runpy.py"", line 85, in _run_code
 exec(code, run_globals)
 File ""/home/cdinea/.local/lib/python3.7/site-packages/pytorch_fid/__main__.py"", line 1, in <module>
 import pytorch_fid.fid_score
 File ""/home/cdinea/.local/lib/python3.7/site-packages/pytorch_fid/fid_score.py"", line 40, in <module>
 import torchvision.transforms as TF
 File ""/home/cdinea/.local/lib/python3.7/site-packages/torchvision/__init__.py"", line 5, in <module>
 from torchvision import datasets, io, models, ops, transforms, utils
 File ""/home/cdinea/.local/lib/python3.7/site-packages/torchvision/datasets/__init__.py"", line 1, in <module>
 from ._optical_flow import FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel
 File ""/home/cdinea/.local/lib/python3.7/site-packages/torchvision/datasets/_optical_flow.py"", line 12, in <module>
 from .utils import _read_pfm, verify_str_arg
 File ""/home/cdinea/.local/lib/python3.7/site-packages/torchvision/datasets/utils.py"", line 1, in <module>
 import bz2
 File ""/usr/local/Modules/lib/python3.7/bz2.py"", line 19, in <module>
 from _bz2 import BZ2Compressor, BZ2Decompressor
ModuleNotFoundError: No module named '_bz2'
```",Other,Other,
18408,è¿è¡èªå®ä¹è¯æ§Demoï¼NoSuchFieldException:$VALUES,"æ¨å¥½ï¼ä½¿ç¨MacBook Pro 10.11ï¼Eclipse Mars.1 ï¼JDK1.7ï¼è¿è¡1.2.10çæ¬çDemoCustomNature.javaåå¦ä¸æ¥éï¼
ä¸æ 24, 2016 6:55:03 ä¸å com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
è­¦å: å·²æ¿æ´»èªå®ä¹è¯æ§åè½,ç±äºéç¨äºåå°ææ¯,ç¨æ·éå¯¹æ¬å°ç¯å¢çå¼å®¹æ§åç¨³å®æ§è´è´£!
å¦æç¨æ·ä»£ç X.javaä¸­æswitch(nature)è¯­å¥,éè¦è°ç¨CustomNatureUtility.registerSwitchClass(X.class)æ³¨åXè¿ä¸ªç±»
Exception in thread ""main"" java.lang.IllegalArgumentException: Could not create enum
 at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:99)
 at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
 at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
 at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
 at com.hankcs.demo.DemoCustomNature.main(DemoCustomNature.java:41)
Caused by: java.lang.IllegalArgumentException: Could not create the class
 at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:453)
 at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:439)
 at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:91)
 ... 4 more
Caused by: java.lang.NoSuchFieldException: $VALUES
 at java.lang.Class.getDeclaredField(Class.java:1961)
 at com.hankcs.hanlp.corpus.util.EnumBuster.findValuesField(EnumBuster.java:349)
 at com.hankcs.hanlp.corpus.util.EnumBuster.values(EnumBuster.java:429)
 at com.hankcs.hanlp.corpus.util.EnumBuster.access$0(EnumBuster.java:426)
 at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:443)",Other,Other,
22241,torch<=1.4.0 needed,"Your work, `bert-sklearn`, is a really great piece of code! Thank you! :)

It seems to me, that it does not work with `torch>1.4.0`,
so `torch` version need to be limited to max `1.4.0`.

Please review and merge.",Other,Other,
16837,"base_options.py '--loadSize', '--fineSize', and '--resize_or_crop'","I met some confusion about the setting in base_options.py , as the setting about the '--loadSize', '--fineSize', and '--resize_or_crop'. Firstly, I would like to know how did you get these parameters, and why did you set then as that. And, then , if I don't follow your default setting, there are someone saying that the model couldn't work as well as you, why? Because of coding of "" transforms.RandomCrop(opt.fineSize) "", when I run the test.py for producing large number of images for low-quality image recognition, even though I just input ONE imageï¼ BUT, the results are different for THIS ONE input everytime. How can I deal with it. Thanks for your attention and your great work.",Other,Other,
34543,Update sbt-scoverage to 2.0.1,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 2.0.0 to 2.0.1.
[GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.1) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.0...v2.0.1)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/e94fc77485a2beb9c366015507dcda1393e25cda/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""@monthly"" },
 dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }
}]
```
</details>

labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1",Other,Other,
27287,Fix absl compatibility issues in TF 1.13,"Ref https://github.com/tensorflow/tensorflow/issues/22766
Ref https://github.com/tensorflow/tensorflow/issues/22113",Other,Other,FIXED
21116,How about training an undirected weighted graph?,"How about training an undirected weighted graph? How should I modify the code and input?
And How about dynamic graph input?",Other,Other,
42758,biblatex support & other remarks,"Brilliant, thank you so much. For the most part, AnyStyle already works extremely well compared with any other parser I tried so far.

Just a few observations and remarks:
- ""45(3):23-45"" is not separated, and parsed as ""volume={3}"" and ""issue={23-45}""
- Sometimes, ""ed."" is chopped off of names, e.g. ""Alfred"" -> ""Alfr""
- Frequently, surrounding quotes, and final commas are not removed from titles.
- Frequently, periods are chopped off of initials, e.g. ""author = {Doe, John R}""
- Names containing initials without periods are inverted, e.g. ""author = {JR, Doe}""
- ""Transl."" is not removed from a translator's name.
- ""Accessed"" is not removed from an ""Accessed"" field (also, this field should be named ""Urldate"" instead, at least for biblatex, see below)
- The (field) labels being used seem to based, mostly, on CSL variables, resp., Zotero field names. I'd suggest introducing a few more, e.g., 
 - ""Series Title"" = CSL ""collection-title"", 
 - ""Series Number"" = CSL ""collection-number"", 
 - ""Book Author"" = CSL ""container-author"", 
 - ""No of Volumes"" = CSL ""number-of-volumes"", 
 - ""Report number"" = CSL ""number"", and 
 - CSL ""original-date""; 
 - maybe also the (hopefully) soon-to-be-introduced ""Volume Title"" = CSL ""volume-title"",
 - maybe an option to label authors as a corporate authors.

I'd also suggest adding biblatex as a separate output format. While some might want to continue using classical bibtex, I see huge advantages in using biblatex and its much more comprehensive data model, and it'd be nice if AnyStyle could output the biblatex format directly. The most important differences between bibtex and biblatex include:
- Improved handling of dates:
 - ""date = {YYYY-MM-DD}"" instead of separate year, month, day fields
 - date ranges in the format ""date = {YYYY-MM-DD/YYYY-MM-DD}""
 - ""urldate"" (instead of ""accessed""; also I'm not sure whether there's any bibtex variant that would accept ""accessed"")
 - ""origdate""
- An ""online"" entry type
- An ""institution"" field for report and thesis entries, and an ""organization"" field for manual and online entry types (""authority"", again, is not recognized by biblatex, and probably by no other bibtex variant either)
- An ""incollection"" entry type (AnyStyle uses this, though output for classical bibtex should probably return to ""inbook"" here)
- A ""maintitle"" field, which will have to be used if AnyStyle starts using CSL ""volume-title"" (mapping is a bit complicated here, but I'd be happy to help here, as with all biblatex questions).",Other,Other,
18133,"Hard dependency from Microsoft.IdentityModel.Clients.ActiveDirectory, version 3.17.2.31801",In my test project by saving of gradient boosting model Iâve encounter strange implicit dependency from Microsoft.IdentityModel.Clients.ActiveDirectory. It doesnât work (load assembly exception) in all referenced versions of this assembly except I reference pretty old one version 3.17.2. Even if I use dependentAssembly construction it doesnât help. My project is net461.,Other,Other,
18372,Named Entity Recognition Doesnt work,"I am running this project in Google Colab with GPU, which I run this command
recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)

It gives following error,
Downloading https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip to /root/.hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip
92.72%, 356.6 MB/384.6 MB, 7.3 MB/s, ETA 4 s Executing op Range in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
Downloading https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
100.00%, 388.8 MB/388.8 MB, 7.9 MB/s, ETA 0 s 
Extracting /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip to /root/.hanlp/thirdparty/storage.googleapis.com/bert_models/2018_10_18
Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op TruncatedNormal in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0

Failed to load https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip. See stack trace below
Traceback (most recent call last):
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py"", line 43, in load_from_meta_file
 obj.load(save_dir, **load_kwargs)
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 244, in load
 self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/common/component.py"", line 255, in build
 loss=kwargs.get('loss', None)))
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/components/taggers/transformers/transformer_tagger.py"", line 35, in build_model
 model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/layers/transformers/loader.py"", line 108, in build_transformer
 with stdout_redirected(to=os.devnull):
 File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
 return next(self.gen)
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 520, in stdout_redirected
 stdout_fd = fileno(stdout)
 File ""/usr/local/lib/python3.6/dist-packages/hanlp/utils/io_util.py"", line 501, in fileno
 fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()
io.UnsupportedOperation: fileno
https://file.hankcs.com/hanlp/ner/ner_conll03_bert_base_uncased_en_20200104_194352.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.20. Try to upgrade hanlp with
pip install --upgrade hanlp

---------------------------------------------------------------------------

UnsupportedOperation Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
 42 load_kwargs = {}
---> 43 obj.load(save_dir, **load_kwargs)
 44 obj.meta['load_path'] = load_path

9 frames

UnsupportedOperation: fileno


During handling of the above exception, another exception occurred:

NameError Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/hanlp/utils/component_util.py in load_from_meta_file(save_dir, meta_filename, transform_only, load_kwargs, **kwargs)
 54 f'Try to upgrade hanlp with\n'
 55 f'pip install --upgrade hanlp')
---> 56 exit(1)
 57 
 58 

NameError: name 'exit' is not defined

Any help will be appreciated!
Thank in advance",Other,Other,
17500,Multiple output regression,"Most regression models are hardcoded with learning method: Learn(F64Matrix observations, double[] targets), what if we had multiple predicted variables.
I want to add method Learn(F64Matrix observations, F64Matrix targets)",Other,Other,FIXED
39312,Ensemble learners: Add support for sample weights,"Add support for sample weights to the Ensemble learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it.

This task requires #14 to be done first, since the ensemble learners needs to be extended to also support weighted learners in the constructor.

Following the learners must implement weighted learner interfaces and should simply forward the sample weights the learners in the ensemble. The ensemble learners can be found here in the ensemble project: [Ensemble learners](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.Ensemble/Learners)",Fairness,Fairness,
3131,changed hstack to vstack,"<!-- Reviewable:start -->
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mogeng/iohmm/19)
<!-- Reviewable:end -->",Other,Other,
7231,use attention_3d_block in many to many mapping,"Hi, I'm beginner of Keras and tring to use attention_3d_block in translation module.
I have input of 5 sentences, each sentences has padding to 6 words, each word is presented in 620 dim(as embedding dim). 
And the output is 5 sentences, sentences padding to 9 words, and word is presented in 1-of-k in 30 dim(as vocabulary size)
How to use attention_3d_block in this scenario as the LSTM is many to many?

![s b3v8 0fr ex 3 he0wk](https://user-images.githubusercontent.com/21202514/27814158-4571e454-60ad-11e7-8028-bd8f38d593f0.png)",Other,Other,
15193,Hi! the training can not convergence!,"I train the model on val2017 dataset and my own dataset, but the valid result is about 21db, have you tested them ?Thank you!",Other,Other,
30217,RandomForest: Add support for sample weights,"Add support for sample weights to the RandomForest learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by RandomForest, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner for each tree. The learners can be found here in the RandomForest project: [RandomForest](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.RandomForest/Learners)",Fairness,Fairness,FIXED
736,About 'L1-norm' in cycle-consistency loss function,"Cycle-consistency loss function is using 'L1-norm'. 
I think it is because G(F(x)) is very near to domain x. 
x and x^ should be like this: 
x [1.0, 1.0, 1.0, 0.0, 0.0, ...]
x^ [1.0, 1.0, 0.9, 0.1, 0.1, ...]
Since almost of all values should have same, the distance is near to zero. So I think L1-norm is useful than L2 norm.
Is this idea right?",Other,Other,
24964,Mark newer/older as read extremely likely to destroy everything with now way to rescue,"A was forced to upgrade to the beta version 4.0 of the NewsBlur Android app as my Nexus phone has just been updated to Android 5.0 Lollipop. I like the offline sync function but I am **very scared** by the presence of both mark newer and older as read options in the tap list. I am reading my stories oldest-first from all threads at once (âAll storiesâ list). I am scrolling down regularly marking all the seen (i.e. older) stories as read.

However, in the current beta-version of the app there are **both options to mark older and newer stories as read side by side**. I am quite sure I will **by accident tap on the wrong option** (i.e. mark newer as read instead of the correct older as read option) and I will effectively **completely and irreversibly destroy** my 400 long list of unread stories with **no confirmation or anything**.

Such a **extremely dangerous function** should definitely be protected at least by a confirmation dialogue (at least when marking more then 5 stories or so at once) or **should be removed at all** (at least as a configuration option in the application settings).

I really do not know why to add option to destroy all following stories in the list the user does not have any chance to see. **Function âMark previous as readâ in the 3.6 version of the app was _definitely_ the correct and totally satisfactory implementation of this function.** Please, add that back, do not let users to destroy their valuable long time build lists of stories to read with no chance to rescue.",Other,Other,
25178,Allow only POST when marking stories as read/unread,"I'm not entirely sure this works (so please review it), but I'm hoping this would cause the ""Invalid method. Use POST. You used GET"" to return from the API when using GET.
I think it would be best to support only POST here and let the user know of it.",Other,Other,
38796,comparing with original software package and citation,"Dear author, 
I want to ask, since the software is not the original graph2vec implementation, is this version of implementation generating same result comparing to the original version released by the author of the paper? 
And If I used this work, except that paper, what else can I cite to show that I used your or got some idea from your software?

Thanks",Other,Other,
40854,Regarding the setting of sphere parameters,"Hello, 

I have a question regarding the setting of sphere parameters.

I wanted to use Mujoco to simulate the pouring of marbles from a cup as shown in the first movie, 
and I have created the simulation shown below with a lot of help....!

https://user-images.githubusercontent.com/63595993/192478534-9feb97ac-95ee-47cb-9a39-6e3216c84c70.mp4

https://user-images.githubusercontent.com/63595993/192479910-049820dd-bd61-44f0-a736-1cec58d316cb.mov


Thanks to your many instructions, I was able to complete a rough simulation, but I think I still need to make minor tweaks.

Specifically, the problem is the difference that the simulation shows a **smooth** outflow from the **lower part** of the marble, while in reality the structure of the marble **collapses** and outflow occurs from the **upper** part of the marble.

I thought that the **friction coefficients** of the marbles and cups had something to do with this fine-tuning.
So I have changed the values of friction in geom, but there is no improvement.

I know this is a very detailed question, but I would like to know if there are other parameters that might be involved...!

Also, I read on [this site](https://mujoco.readthedocs.io/en/latest/XMLreference.html) that the values of sliding, torsional, and rolling friction can be changed friction of ""geom"", but is it possible to set the **static friction coefficient**?

Furthermore, I would like to adjust the value of the **surface hardness** of the marble, which seems to be related to it.

If there are any other values that I can change by myself, I would appreciate it if you could let me know.

Thank you in advance.",Other,Other,
13051,Create .travis.yml,"Sorry, I selected the wrong option while following the tutorial! Please discard this change attempt. Thank you.

<!--
Thank you for being interested in contributing to HanLP! You are awesome â¨.
â ï¸Changes must be made on dev branch.
-->

# Title of Your Pull Request

## Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please check any relevant options and delete the rest.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] This change requires a documentation update

## How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

## Checklist

Check all items that apply.

- [ ] â ï¸Changes **must** be made on `dev` branch instead of `master`
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] My code follows the style guidelines of this project
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have checked my code and corrected any misspellings",Other,Other,
30638,Fix optional parts of volume keywords,"I don't know whether `heft` should be `heft?` but it looks to me the `|j(ahrgan)g|heft` was simply inserted *before* `suppl(ement)`s `?` instead of after in the previous patch.

See a489c56092cf9ebe757be8b6fe5e9e85a73c09ed",Other,Other,
27659,Compacter dictionary with Scopus journal names,"A somewhat compacter dictionary with additional journal titles, mostly compiled from the previously used sources:
- duplicate entries in the same mode are not repeated
- American surnames with <0.001 are not included (by my estimation this means <3,000 US population)
- Additional journal names based on the publications indexed by Scopus

Happy to share the simple script used to compile the dictionary file.",Other,Other,
1894,nystrom factorization for sparse matrix,"``` julia
fac = nystrom(k, X)
```
```
ERROR: MethodError: no method matching nystrom(::PolynomialKernel{Float64,Int64}, ::SparseMatrixCSC{Float64,Int64})
Closest candidates are:
 nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}) where T<:Union{Float32, Float64} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
 nystrom(::Kernel{T<:Union{Float32, Float64}}, ::Array{T<:Union{Float32, Float64},2}, ::Array{U<:Integer,1}) where {T<:Union{Float32, Float64}, U<:Integer} at C:\Users\charl\.julia\packages\MLKernels\DqEdF\src\nystrom.jl:97
```",Other,Other,
44293,Fix for coding dictionary items,"Some faulty logic meant that items could be incorrectly coded when appearing multiple times, depending on the previously applied codes. Test for, example ""may"" (coded as 4=""place"", instead of correctly 3=2+1=""name+month"")",Other,Other,
15979,Add proper optimizer seeding to all optimizers in SharpLearning.Optimization,"This pull request will add proper seeding to all optimizer algorithms in `SharpLearning.Optimization`. The technique used is a single seed is set through the constructor, this seed is used to create a `random` generator, which will then create seeds for all underlying algorithms used in the given optimizer.

Note, that since this PR will change default seeding of the optimizers, the default behavior of the optimizers will not be the same as before this addition.

This should solve issue #69 .",Other,Other,
27591,Terminal parenthesis removed from title,"When given a reference similar to that of the following example, in which the end of the work's title is enclosed in parentheses, the final closing parenthesis is not present in the output.

Input:
Author, A. N. (2022). Autism spectrum disorder (ASD). The Journal 1(2), 3-4.

Title line in output:
 title = {Autism spectrum disorder (ASD},",Other,Other,
38311,Use this repository for CNN,"Dear Sir,

Would it be possible to use this repo for CNN network also?

Thanks and regards.",Other,Other,
43330,Headless display?,"I have 8 GPUs on a node and I guess only one of them has a head. So when I run a second job on them, I get a headless display error. The first job works fine.

Following that, I tried loading ""glew"" and now I'm getting this output:

```
[2022-06-01 23:18:45,712][HYDRA] MUJOCO_GL is not set, so an OpenGL backend will be chosen automatically.
[2022-06-01 23:18:45,786][HYDRA] Successfully imported OpenGL backend: glfw
[2022-06-01 23:18:46,023][HYDRA] MuJoCo library version is: 210
python3: /builds/florianrhiem/pyGLFW/glfw-3.3.4/src/posix_thread.c:64: _glfwPlatformGetTls: Assertion `tls->posix.allocated == 1' failed.
/var/spool/slurm/job11038422/slurm_script: line 25: 100365 Aborted 
```",Other,Other,
7425,Optimization: Rename Transforms.Logarithmic to Transforms.Log10,"Currently, the name of the logarithmic transform is misleading, since in the .Net world log refers to log2, and the [LogarithmicTransform](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.Optimization/Transforms/LogarithmicTransform.cs) from SharpLearning uses Log10. 

So the transform, and the enum should be renamed to illustrate the use of Log10. For instance:
 - `Transforms.Log10`
 - `Log10Transform`",Other,Other,
978,Update sbt-sonatype to 3.9.8,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.7 to 3.9.8.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.8) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.7...3.9.8)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/90cd45d1abb030a028e5be42fbf3858daf0a461e/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
7133,add nGram distance function between strings,"fork you code,change jdk1.8 ,do some code optimize,want to join the project.
the function is in [getNGRAMDistance in EditDistance.java ](https://github.com/DusonWang/HanLP/blob/master/src/main/java/com/hankcs/hanlp/algoritm/EditDistance.java)",Other,Other,
12406,Discrete Math: update PNF lecture notes shots,I updated discrete mathematics lecture slides shots about PNF equations by the value in the PDF file which is different than the one represented in the shots.,Other,Other,
42787,Fix travis testings,"- Removed family wrapper since statsmodels already supported loglike_per_sample (loglike_obs). We can just generalize their function a bit
- Fixed some unit tests due to version compatibility
- Updated the requirements

<!-- Reviewable:start -->
---
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mogeng/iohmm/32)
<!-- Reviewable:end -->",Other,Other,
14330,how to do prediction?,How to do prediction for new test data after trained the model?,Other,Other,
27426,Bump tensorflow-gpu from 1.4.0 to 1.15.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.2.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.2
> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
> * Tensors are no longer hashable.
> * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to ""true"" or ""1"" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
> * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
> * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
> * Expand support for TensorFlow operators in TensorRT conversion (e.g.
> `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
> * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
> significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
> * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
> * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
> * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
> * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer ""layer-name"" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
> * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
> * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
> * Fix tests in canned estimators.
> * Expose Head as public API.
> * Fixes critical bugs that help with `DenseFeatures` usability in TF2
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.2
> 
> ## Bug Fixes and Other Changes
> * Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))
> * Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)
> * Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)
> 
> 
> # Release 2.1.0
> 
> TensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.
> 
> ## Major Features and Improvements
> * The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install ""Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019"", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).
> * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.
> * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.
> * The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.
> * `tf.keras`
> * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).
> * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3).
> * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.
> * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).
> * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.
> * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.
> * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.
> * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).
> * `tf.data`
> * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.
> * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.
> * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`
> * `tf.debugging`
> * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.
> * `tf.distribute`
> * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.
> * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.
> * `TensorRT`
> * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.
> * Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to ""true"" or ""1"", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\*D and MaxPool\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.
> 
> ## Breaking Changes
> * Deletes `Operation.traceback_with_start_lines` for which we know of no usages.
> * Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.
> * Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> * The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.
> * `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.
> * `tf.config.experimental_list_devices` has been removed, please use
> `tf.config.list_logical_devices`.
> 
> ## Bug Fixes and Other Changes
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`5d80e1e`](https://github.com/tensorflow/tensorflow/commit/5d80e1e8e6ee999be7db39461e0e79c90403a2e4) Merge pull request [#36215](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36215) from tensorflow-jenkins/version-numbers-1.15.2-8214
- [`71e9d8f`](https://github.com/tensorflow/tensorflow/commit/71e9d8f8eddfe283943d62554d4c676bdaf79372) Update version numbers to 1.15.2
- [`e50120e`](https://github.com/tensorflow/tensorflow/commit/e50120ee34e1e29252f4cbc8ac4cd328e9a9840c) Merge pull request [#36214](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36214) from tensorflow-jenkins/relnotes-1.15.2-2203
- [`1a7e9fb`](https://github.com/tensorflow/tensorflow/commit/1a7e9fbf670ef9d03b2f8fdf1ae2276b2d100fab) Releasing 1.15.2 instead of 1.15.1
- [`85f7aab`](https://github.com/tensorflow/tensorflow/commit/85f7aab93b65ed1fcc589f54d40793b1afb65bf4) Insert release notes place-fill
- [`e75a6d6`](https://github.com/tensorflow/tensorflow/commit/e75a6d6e6e20df83f19e72e04c7984587d768bd3) Merge pull request [#36190](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36190) from tensorflow/mm-r1.15-fix-v2-build
- [`a6d8973`](https://github.com/tensorflow/tensorflow/commit/a6d897351e483dfd0418e5cad2900ad9ef24188c) Use `config=v1` as this is `r1.15` branch.
- [`fdb8589`](https://github.com/tensorflow/tensorflow/commit/fdb85890df5df1e6b3867c842aabb44f561b446d) Merge pull request [#35912](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/35912) from tensorflow-jenkins/relnotes-1.15.1-31298
- [`a6051e8`](https://github.com/tensorflow/tensorflow/commit/a6051e8094c5e7d26ec9573a740246c92e4057a2) Add CVE number for main patch
- [`360b2e3`](https://github.com/tensorflow/tensorflow/commit/360b2e318af2db59152e35be31c8aab1fb164088) Merge pull request [#34532](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/34532) from ROCmSoftwarePlatform/r1.15-rccl-upstream-patch
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.2)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.2)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
5048,do refactor in segment package,"<!--
æè°¢ä½ å¯¹å¼æºäºä¸çè´¡ç®ï¼è¿æ¯ä¸ä»½æ¨¡æ¿ï¼æ¹ä¾¿è®°å½ä½ ååºçåç»©ï¼è°¢è°¢ï¼
-->

## æ³¨æäºé¡¹

* è¿æ¬¡ä¿®æ¹æ²¡æå¼å¥ç¬¬ä¸æ¹ç±»åºã
* ä¹æ²¡æä¿®æ¹JDKçæ¬å·
* ææææ¬é½æ¯UTF-8ç¼ç 
* ä»£ç é£æ ¼ä¸è´
* [x] æå¨æ­¤æ¬å·åè¾å¥xæé©ï¼ä»£è¡¨ä¸è¿°äºé¡¹ç¡®è®¤å®æ¯

## è§£å³äºä»ä¹é®é¢ï¼å¸¦æ¥äºä»ä¹å¥½å¤ï¼

<!-- ä½ çè¡¥ä¸è§£å³äºä»ä¹é®é¢ï¼ç»å¤§å®¶å¸¦æ¥äºä»ä¹å¥½å¤ï¼ -->
Hi.
I do refactor in the ```com.hankcs.hanlp.seg``` package. 

- rename four abstract class with word ```Abstract``` ahead, and put them into the ```com.hankcs.hanlp.seg.base``` package;

- remove ```com.hankcs.hanlp.seg.common.Config``` class into ``com.hankcs.hanlp.seg.common``` package, package permission of this class has also been modified;

- some package permission of ```com.hankcs.hanlp.seg.base.AbstractSegment```'s method has been modified;

- mainly, I write a ```com.hankcs.hanlp.seg.Segment``` class with a nested class ```Builder``` , and a ```com.hankcs.hanlp.seg.common.SegmentTypeEnum``` class, to provide a easier way to know what seg method can be used.

## ç¸å³issue

<!-- å¦æè·å·²æissueç¸å³çè¯ï¼éº»ç¦åä¸ä¸ -->",Other,Other,
2782,Bump numpy from 1.16.4 to 1.22.0,"Bumps [numpy](https://github.com/numpy/numpy) from 1.16.4 to 1.22.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>
<blockquote>
<h2>v1.22.0</h2>
<h1>NumPy 1.22.0 Release Notes</h1>
<p>NumPy 1.22.0 is a big release featuring the work of 153 contributors
spread over 609 pull requests. There have been many improvements,
highlights are:</p>
<ul>
<li>Annotations of the main namespace are essentially complete. Upstream
is a moving target, so there will likely be further improvements,
but the major work is done. This is probably the most user visible
enhancement in this release.</li>
<li>A preliminary version of the proposed Array-API is provided. This is
a step in creating a standard collection of functions that can be
used across application such as CuPy and JAX.</li>
<li>NumPy now has a DLPack backend. DLPack provides a common interchange
format for array (tensor) data.</li>
<li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The
new methods provide a complete set of the methods commonly found in
the literature.</li>
<li>A new configurable allocator for use by downstream projects.</li>
</ul>
<p>These are in addition to the ongoing work to provide SIMD support for
commonly used functions, improvements to F2PY, and better documentation.</p>
<p>The Python versions supported in this release are 3.8-3.10, Python 3.7
has been dropped. Note that 32 bit wheels are only provided for Python
3.8 and 3.9 on Windows, all other wheels are 64 bits on account of
Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.
All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix
the occasional problems encountered by folks using truly huge arrays.</p>
<h2>Expired deprecations</h2>
<h3>Deprecated numeric style dtype strings have been removed</h3>
<p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,
and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>
<h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>
<p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that
users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both
deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with
the appropriate value for the <code>usemask</code> parameter.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>
<li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>
<li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>
<li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>
<li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>
<li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>
<li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>
<li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.16.4...v1.22.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.16.4&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
312,Add new methods in TextRankSentence class,"Add new methods in TextRankSentence class to support the newly added text summary API (which is based on user specified summary length). 

The return type of the method will be String, which is human-readable text based on the original text sequence.",Other,Other,
43716,Only extract one word from gumbel softmax,"In the code https://github.com/yala/text_nn/blob/master/rationale_net/utils/learn.py#L71-L85

```python
def get_hard_mask(z, return_ind=False):
 '''
 -z: torch Tensor where each element probablity of element
 being selected
 -args: experiment level config
 returns: A torch variable that is binary mask of z >= .5
 '''
 max_z, ind = torch.max(z, dim=-1)
 if return_ind:
 del z
 return ind
 masked = torch.ge(z, max_z.unsqueeze(-1)).float()
 del z
 return masked

```

because we take the max, usually, only one position will have the max value. 
In this case, if we have 100 words in the sentence, we only select one word as the rationale?
I thought we should select independently and choose those words with >0.5 probability.

Maybe we should change
```python
masked = torch.ge(z, max_z.unsqueeze(-1)).float()
```
to 
```python
masked = torch.ge(z, 0.5).float()
```
instead?",Other,Other,
26700,Update google-api-services-bigquery to v2-rev20200916-1.30.10,"Updates com.google.apis:google-api-services-bigquery from v2-rev20200827-1.30.10 to v2-rev20200916-1.30.10.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/699e420f7466287c6f3c21ddb906c21b4ab27f3d/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
</details>

labels: library-update",Other,Other,
20198,Show more recent labels,"User request:

The number of labels that appear below the âAssign labelâ float box in the edit section is too small for my taste. I used about seven different labels regularly to correct entries and â as it goes ;) â always the one I needed had just dropped out of the list. Would it be difficult to just increase the list to seven or eight items? Even five (one more) would be a benefit, IMO.",Other,Other,
44317,On homepage- option to serialize training line after label assignment,"This is basically the XML, but stripped down?

```
When I visit https://anystyle.io/
 And I paste in a single citation (that is poorly processed)
 And I assign many labels
 Then I want XML, but only to serialize a line for my training.txt file (no <reference>)
 And it would be great if there was an option to copy that training line to my clipboard
```",Other,Other,
19912,Bump tensorflow-gpu from 1.4.0 to 1.15.4,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 1.15.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 1.15.4</h2>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses both NumPy and TensorFlow headers.</li>
</ul>
<h2>TensorFlow 1.15.3</h2>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Updates <code>sqlite3</code> to <code>3.31.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19880"">CVE-2019-19880</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19244"">CVE-2019-19244</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a></li>
<li>Updates <code>curl</code> to <code>7.69.1</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-15601"">CVE-2019-15601</a></li>
<li>Updates <code>libjpeg-turbo</code> to <code>2.0.4</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-19664"">CVE-2018-19664</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-20330"">CVE-2018-20330</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13960"">CVE-2019-13960</a></li>
<li>Updates Apache Spark to <code>2.4.5</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10099"">CVE-2019-10099</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-17190"">CVE-2018-17190</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-11770"">CVE-2018-11770</a></li>
</ul>
<h2>TensorFlow 1.15.2</h2>
<h1>Release 1.15.2</h1>
<p>Note that this release no longer has a single pip package for GPU and CPU. Please see <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/36347"">#36347</a> for history and details</p>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes a security vulnerability where converting a Python string to a <code>tf.float16</code> value produces a segmentation fault (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215"">CVE-2020-5215</a>)</li>
<li>Updates <code>curl</code> to <code>7.66.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482"">CVE-2019-5482</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481"">CVE-2019-5481</a></li>
<li>Updates <code>sqlite3</code> to <code>3.30.01</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646"">CVE-2019-19646</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645"">CVE-2019-19645</a> and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168"">CVE-2019-16168</a></li>
</ul>
<h2>TensorFlow 1.15.0</h2>
<h1>Release 1.15.0</h1>
<p>This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.</p>
<h2>Major Features and Improvements</h2>
<ul>
<li>As <a href=""https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0"">announced</a>, <code>tensorflow</code> pip package will by default include GPU support (same as <code>tensorflow-gpu</code> now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. <code>tensorflow-gpu</code> will still be available, and CPU-only packages can be downloaded at <code>tensorflow-cpu</code> for users who are concerned about package size.</li>
<li>TensorFlow 1.15 contains a complete implementation of the 2.0 API in its <code>compat.v2</code> module. It contains a copy of the 1.15 main module (without <code>contrib</code>) in the <code>compat.v1</code> module. TensorFlow 1.15 is able to emulate 2.0 behavior using the <code>enable_v2_behavior()</code> function.
This enables writing forward compatible code: by explicitly importing either <code>tensorflow.compat.v1</code> or <code>tensorflow.compat.v2</code>, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.</li>
<li><code>EagerTensor</code> now supports numpy buffer interface for tensors.</li>
<li>Add toggles <code>tf.enable_control_flow_v2()</code> and <code>tf.disable_control_flow_v2()</code> for enabling/disabling v2 control flow.</li>
<li>Enable v2 control flow as part of <code>tf.enable_v2_behavior()</code> and <code>TF2_BEHAVIOR=1</code>.</li>
<li>AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside <code>tf.function</code>-decorated functions. AutoGraph is also applied in functions used with <code>tf.data</code>, <code>tf.distribute</code> and <code>tf.keras</code> APIS.</li>
<li>Adds <code>enable_tensor_equality()</code>, which switches the behavior such that:
<ul>
<li>Tensors are no longer hashable.</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 1.15.4</h1>
<h2>Bug Fixes and Other Changes</h2>
<ul>
<li>Fixes an undefined behavior causing a segfault in <code>tf.raw_ops.Switch</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15190"">CVE-2020-15190</a>)</li>
<li>Fixes three vulnerabilities in conversion to DLPack format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15191"">CVE-2020-15191</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15192"">CVE-2020-15192</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15193"">CVE-2020-15193</a>)</li>
<li>Fixes two vulnerabilities in <code>SparseFillEmptyRowsGrad</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15194"">CVE-2020-15194</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15195"">CVE-2020-15195</a>)</li>
<li>Fixes an integer truncation vulnerability in code using the work sharder API
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15202"">CVE-2020-15202</a>)</li>
<li>Fixes a format string vulnerability in <code>tf.strings.as_string</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15203"">CVE-2020-15203</a>)</li>
<li>Fixes segfault raised by calling session-only ops in eager mode
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15204"">CVE-2020-15204</a>)</li>
<li>Fixes data leak and potential ASLR violation from <code>tf.raw_ops.StringNGrams</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15205"">CVE-2020-15205</a>)</li>
<li>Fixes segfaults caused by incomplete <code>SavedModel</code> validation
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15206"">CVE-2020-15206</a>)</li>
<li>Fixes a data corruption due to a bug in negative indexing support in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15207"">CVE-2020-15207</a>)</li>
<li>Fixes a data corruption due to dimension mismatch in TFLite
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15208"">CVE-2020-15208</a>)</li>
<li>Fixes several vulnerabilities in TFLite saved model format
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15209"">CVE-2020-15209</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15210"">CVE-2020-15210</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15211"">CVE-2020-15211</a>)</li>
<li>Updates <code>sqlite3</code> to <code>3.33.00</code> to handle
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327"">CVE-2020-9327</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655"">CVE-2020-11655</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656"">CVE-2020-11656</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434"">CVE-2020-13434</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435"">CVE-2020-13435</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630"">CVE-2020-13630</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631"">CVE-2020-13631</a>,
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871"">CVE-2020-13871</a>,
and
<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358"">CVE-2020-15358</a>.</li>
<li>Fixes <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/41630"">#41630</a> by including <code>max_seq_length</code> in CuDNN descriptor cache key</li>
<li>Pins <code>numpy</code> to 1.18.5 to prevent ABI breakage when compiling code that uses
both NumPy and TensorFlow headers.</li>
</ul>
<h1>Release 2.3.0</h1>
<h2>Major Features and Improvements</h2>
<ul>
<li><code>tf.data</code> adds two new mechanisms to solve input pipeline bottlenecks and save resources:</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/df8c55ce12b5cfc6f29b01889f7773911a75e6ef""><code>df8c55c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43442"">#43442</a> from tensorflow-jenkins/version-numbers-1.15.4-31571</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/0e8cbcb0b1756de4afda8677add8a55355720ab7""><code>0e8cbcb</code></a> Update version numbers to 1.15.4</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/5b65bf202a00f558784e61b7dba5063195cce0f5""><code>5b65bf2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43437"">#43437</a> from tensorflow-jenkins/relnotes-1.15.4-10691</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/814e8d83f5966af55168bc1141dc8ba68561556f""><code>814e8d8</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/757085e3e62197ab5ad6a10c667aae08a8929556""><code>757085e</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e99e53dda53644e49f4b8b4ec16ef92f6399fc3b""><code>e99e53d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43410"">#43410</a> from tensorflow/mm-fix-1.15</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/bad36df000e97cfe0a271e08778a81db4ce8834a""><code>bad36df</code></a> Add missing import</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f3f1835aed4ab1874c0891c487cd6d0340fed67b""><code>f3f1835</code></a> No <code>disable_tfrt</code> present on this branch</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/7ef5c62a21f2c03483c21566dd6c048218dced26""><code>7ef5c62</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/43406"">#43406</a> from tensorflow/mihaimaruseac-patch-1</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/abbf34a5885400f81620df23d9da70f30630e699""><code>abbf34a</code></a> Remove import that is not needed</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v1.15.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=1.15.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
33512,host on cloud computing provider,"A suggestion - I notice there are a few open issues about outdated data version, so I presume the hosting of this data is inconvenient to update. As such i might be worth hosting the data somewhere else.

according to the [FAQ](https://msropendata.com/faq), Microsoft Research Open Data will host data sets up to 250gb. Amazon ad probably google offer similar schemes.",Security & Safety,Security & Safety,
6336,Is it possible to share the pre-trained embedding for beer reviews?,"From https://github.com/yala/text_nn/blob/master/rationale_net/utils/embedding.py#L38 , it's trying to find `review+wiki.filtered.200.txt.gz`, is it possible to share the pre-trained embedding for beer reviews?",Other,Other,
39792,Identified an issue related with the parser or training,"I've identified an issue with the parser. Can you tell me how can i work with getting the expected output. As, of now main issue is **Author** should be ""**Social Security Administration**"" other than ""**Administration, Social Security**""

INPUT:

> Social Security Administration. Social Security Programs Throughout the World: Asia and the Pacific, 2008. Vol. 13. No. 11801. Government Printing Office, 2002.

TRAIN DATA:
`<author>Social Security Administration</author><title>Social Security Programs Throughout the World: Asia and the Pacific, 2008. </title><volume>Vol. 13. No. 11801. </volume><publisher>Government Printing Office, </publisher><date>2002.</date>`
```
Anystyle.parser.train '/var/www/volume/temp_dta/train_test.txt', false
Anystyle.parser.model.save
```

Expected Output:

> @book{**social2002social**,
> **author = {Social Security Administration},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

What i got output other than expected?

> @book{**administration2002a**,
> **author = {Administration, Social Security},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

REF: 
https://scholar.google.com/scholar?hl=en&q=security&btnG=&oq=sec",Security & Safety,Security & Safety,
39528,Bump opencv-python from 4.1.0.25 to 4.1.1.26,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.1.1.26.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<ul>
<li>FFmpeg has been compiled with https support on Linux builds <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/229"">#229</a></li>
<li>CI build logic related changes <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/197"">#197</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/227"">#227</a>, <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/228"">#228</a></li>
<li>Custom libjepg-turbo removed because it's provided by OpenCV <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/231"">#231</a></li>
<li>64-bit Qt builds are now smaller <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/236"">#236</a></li>
<li>Custom builds should be now rather easy to do locally <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/235"">#235</a>:
<ol>
<li>Clone this repository</li>
<li>Optional: set up ENABLE_CONTRIB and ENABLE_HEADLESS environment variables to 1 if needed</li>
<li>Optional: add additional Cmake arguments to CMAKE_ARGS environment variable</li>
<li>Run <code>python setup.py bdist_wheel</code></li>
</ol>
</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.1.1.26)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
18209,AdaBoostLearners: Add features pr. split as regularization hyper parameter,"RandomForest and GradientBoost learners have a hyper parameter, featuresPrSplit, which controls how many randomly selected features are considered during the decision trees search for a new split. This parameter should also be introduced in the AdaBoostLearners ([ClassificationAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/ClassificationAdaBoostLearner.cs) and [RegressionAdaBoostLearner](https://github.com/mdabros/SharpLearning/blob/master/src/SharpLearning.AdaBoost/Learners/RegressionAdaBoostLearner.cs)), to have more possibilities for reguralizing this type of model .

Sine the DecisionTreeLearner used in adaboost already supports 'featuresPrSplit', the implementation should simply add the hyper parameter to the adaboost learner contructors and forward the parameter to the DecisionTreeLearner.",Other,Other,
20196,Delete parsed references from list,"User request:

It would be very handy to have an option to delete an entry entirely from the edit section. Sometimes references are simply broken to begin with but you notice it only after the parsing. For example I just had this bibliographical entry in one of the lists I fed into AnyStyle:

Graddol, David, Dick Leith, et al., eds. Changing English. Milton Park, New York: Routledge > The spread of English within the British Isles, p. 125 ff.

This entry is irreparably broken, but I cannot take it out of the list without having to parse it again, thus losing all the edits I already made. Now I had to remember, find and delete that entry after the import into Zotero.",Other,Other,
22637,Does it support causal mask?,"The effect is the same as the argument ""use_casual_mask"" in keras official layer class Attention.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention
Thanks for your useful module and expect your reply.",Other,Other,
29927,Tokenizer doesn't parse Volume/issue typeset with no space.,"Germane to #23 : when given such a reference :

```
1.Felson DT. Epidemiology of hip and knee osteoarthritis. Epidemiol Rev. 1988;10:1â28. 
```

the current parser tokenizes `1988;10:1â28` as a whole and assigns it to `Volume/Issue`. It should be approximately 
| Token | Value |
|------|-------|
| Year | 1998 |
| Volume | 10 |
| Pages | 1-28 |

Worse case : 

```
2.Heijink A, Gomoll AH, Madry H, DrobniÄ M, Filardo G, Espregueira-Mendes J, et al. Biomechanical considerations in the pathogenesis of osteoarthritis of the knee. Knee Surg Sports Traumatol Arthrosc. mars 2012;20(3):423â35. 
```

Is parsed as : 

| Token | Value |
|------|-------|
| Citation number | 2 |
[ Author | Heijink A, Gomoll AH, Madry H, DrobniÄ M, Filardo G, Espregueira-Mendes J, et al. |
| Title | Biomechanical considerations in the pathogenesis of osteoarthritis of the knee |
| Journal | Knee Surg Sports Traumatol Arthrosc mars |
| Date | 2012 |
| Volume/Issue | 20(3):423â35 |

Again, the whoele Volume/issue token isn't parsed for punctuation. I would expect : 

| Token | Value |
|------|-------|
| Citation number | 2 |
[ Author | Heijink A, Gomoll AH, Madry H, DrobniÄ M, Filardo G, Espregueira-Mendes J, et al. |
| Title | Biomechanical considerations in the pathogenesis of osteoarthritis of the knee mars|
| Journal | Knee Surg Sports Traumatol Arthrosc |
| Date | mars 2012 |
| Volume/Issue | 20(3)
| Pages | 423â35 |

Recognizing `mars 2012` is probably harder...

HTH,",Other,Other,
31591,CVE-2007-4559 Patch,"# Patching CVE-2007-4559

Hi, we are security researchers from the Advanced Research Center at [Trellix](https://www.trellix.com). We have began a campaign to patch a widespread bug named CVE-2007-4559. CVE-2007-4559 is a 15 year old bug in the Python tarfile package. By using extract() or extractall() on a tarfile object without sanitizing input, a maliciously crafted .tar file could perform a directory path traversal attack. We found at least one unsantized extractall() in your codebase and are providing a patch for you via pull request. The patch essentially checks to see if all tarfile members will be extracted safely and throws an exception otherwise. We encourage you to use this patch or your own solution to secure against CVE-2007-4559. Further technical information about the vulnerability can be found in this [blog](https://www.trellix.com/en-us/about/newsroom/stories/research/tarfile-exploiting-the-world.html).

If you have further questions you may contact us through this projects lead researcher [Kasimir Schulz](mailto:kasimir.schulz@trellix.com).",Security & Safety,Security & Safety,
4696,attention_lstm.py and Tensorflow,"In the attention_3d_block, I have some questions/bug (I think). I am running on Tensorflow. 
(1) inputs doesn't have a shape method. So it crashes. I assume you meant to call the shape function on the numpy array on inputs_1.
(2) Is there a reason for calling Permute?
(3) What is the Reshape layer supposed to do? After the call to Permute, isn't the output of the previous permute layer already in shape (Batch Size, input_dim, TIME_STEPS)?
(4) The next call to Dense expects ndim =2, not 3. So the code crashes for me. I assume you meant the previous Reshape layer to map the 3d input to 2d?
(5) I would just like to point out that APPLY_ATTENTION_BEFORE_LSTM is False iff you call model_attention_applied_before_lstm.",Other,Other,
3040,Dictionary fixes,"Small fixes for opening the dictionary file:
1. ""b"" flag is required when opening gzip files on Windows
2. The ""UTF-8"" flag is redundant / misleading when opening the gzip file
3. The ""UTF-8"" flag is necessary, however, on the gzip reader stream

Without (3), accented characters in place/person names are not correctly matched. A test case: ""Weber, M., 1922. Wirtschaft und Gesellschaft: Grundriss der verstehenden Soziologie. JCB Mohr, TÃ¼bingen."" On Anycite.io, TÃ¼bingen is not recognised as a place, even though it is in the dictionary. This fixes that.",Other,Other,
10650,How to make keep_prob variable during training?,Setting the keep probability is only possible during eager evaluation. How can it be made to work during the training phase?,Other,Other,
6606,Train a LM on wiki + weibo + qa + news + danmaku + reviews + ...,"What makes HanLP different than the majority of OSS projects?
One of the most important factors would be the large scale professional corpora, and the correct way to make use of them.
To have some unique pretrained LM before releasing the beta version would be a cool idea. Don't you think so?",Other,Other,
25757,Vsts continuous integration,"Merging the move to vsts continuous integration. This will also add continuous delivery, packing and pushing nuget packages with each commit to the master branch. The nuget steps are disabled for pull requests against the master branch.",Other,Other,
23890,How to visualise as 2dimensional heatmap?,"lets say we are predicting with timestep of 24, and get 24 result as output. how can we visualise as heatmap like in https://github.com/datalogue/keras-attentiontx#",Other,Other,FIXED
2575,AdaBoost: Add support for sample weights,"Add support for sample weights to the AdaBoost learners. This will make it possible to handle imbalanced datasets directly in the learners, instead of under/oversampling the dataset inorder to balance it. 

The DecisionTreeLearners, used by AdaBoost, already support sample weights, so implementing it only involves setting up the sample weights and forwarding the weights to the DecisionTreeLearner in each boosting iteration. The learners can be found here in the AdaBoost project: [AdaBoost](https://github.com/mdabros/SharpLearning/tree/master/src/SharpLearning.AdaBoost/Learners)

The work is currently in progress in the branch [adaboost-sample-weight-support](https://github.com/mdabros/SharpLearning/tree/adaboost-sample-weight-support)",Fairness,Fairness,
29587,Why do we need blender for demo?,"Hi, thanks for your great work. I wanted to see the results of your work, i.e. I will give a RGB image and expect 6dposes. I believe that blender was used to create the synthetic dataset for training Pose interpreter network. My question is why do we need blender for seeing the results.

My other question is if we require blender then how to use it on some online platform like google collab?",Other,Other,
41570,FileNotFoundError: [Errno 2] No such file or directory: 'pickle_files/embeddings.p',"In dataset.py we read
 embedding_path = 'pickle_files/embeddings.p'
 word_to_indx_path = 'pickle_files/vocabIndxDict.p'
 embedding_tensor = pickle.load(open(embedding_path,'rb'))
 word_to_indx = pickle.load(open(word_to_indx_path,'rb'))
However, the two files are not in the zip-code?",Other,Other,
28197,"iOS: On iPad landscape, the third column is offset","From TestFlight feedback:

![original-4](https://user-images.githubusercontent.com/44229/117200658-b8728500-adb9-11eb-92b9-faccb61f01e8.jpg)",Other,Other,
24656,Android: Prod Release Candidate,A few very small bugs caught in Beta. App should be ready for immediate release to Prod.,Other,Other,
40251,Update sbt to 1.3.11,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from 1.3.10 to 1.3.11.
[GitHub Release Notes](https://github.com/sbt/sbt/releases/tag/v1.3.11) - [Version Diff](https://github.com/sbt/sbt/compare/v1.3.10...v1.3.11)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/3cc49f29e2f6d005f39969112aa6daeba90cb5f6/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: library-update, semver-patch",Other,Other,
3753,add nGram distance function between strings,"fork you code,change jdk1.8 ,do some code optimize,want to join the project.
the function is in [getNGRAMDistance in EditDistance.java ](https://github.com/DusonWang/HanLP/blob/master/src/main/java/com/hankcs/hanlp/algoritm/EditDistance.java)",Other,Other,
33351,Error loading ScikitLearn,"I get the following error when inputting `using ScikitLearn` after installing the package.
using Julia 1.0.0
```
julia> using ScikitLearn
[ Info: Precompiling ScikitLearn [3646fa90-6ef7-5e7e-9f22-8aca16db6324]
ERROR: LoadError: LoadError: syntax: extra token ""ScikitLearnBase"" after end of expression
Stacktrace:
 [1] include at ./boot.jl:317 [inlined]
 [2] include_relative(::Module, ::String) at ./loading.jl:1038
 [3] include at ./sysimg.jl:29 [inlined]
 [4] include(::String) at /Users/kartikeygupta/.julia/packages/ScikitLearn/ILHSi/src/ScikitLearn.jl:10
 [5] top-level scope at none:0
 [6] include at ./boot.jl:317 [inlined]
 [7] include_relative(::Module, ::String) at ./loading.jl:1038
 [8] include(::Module, ::String) at ./sysimg.jl:29
 [9] top-level scope at none:2
 [10] eval at ./boot.jl:319 [inlined]
 [11] eval(::Expr) at ./client.jl:389
 [12] top-level scope at ./none:3
in expression starting at /Users/kartikeygupta/.julia/packages/ScikitLearn/ILHSi/src/Skcore.jl:15
in expression starting at /Users/kartikeygupta/.julia/packages/ScikitLearn/ILHSi/src/ScikitLearn.jl:12
ERROR: Failed to precompile ScikitLearn [3646fa90-6ef7-5e7e-9f22-8aca16db6324] to /Users/kartikeygupta/.julia/compiled/v1.0/ScikitLearn/tbUuI.ji.
Stacktrace:
 [1] error(::String) at ./error.jl:33
 [2] macro expansion at ./logging.jl:313 [inlined]
 [3] compilecache(::Base.PkgId, ::String) at ./loading.jl:1184
 [4] macro expansion at ./logging.jl:311 [inlined]
 [5] _require(::Base.PkgId) at ./loading.jl:941
 [6] require(::Base.PkgId) at ./loading.jl:852
 [7] macro expansion at ./logging.jl:311 [inlined]
 [8] require(::Module, ::Symbol) at ./loading.jl:834
```",Other,Other,
44057,Pre-trained model link,"The pre-trained model link seems broken:

https://drive.google.com/file/d/1U21Bwchcm96-Nz1SVEo0IvSoPyqqA7gm/view?usp=sharing

Any chance you could upload it again?

Thank you for sharing your work!",Other,Other,
14650,Update sbt-release to 1.1.0,"Updates [com.github.sbt:sbt-release](https://github.com/sbt/sbt-release) from 1.0.15 to 1.1.0.
[GitHub Release Notes](https://github.com/sbt/sbt-release/releases/tag/v1.1.0) - [Version Diff](https://github.com/sbt/sbt-release/compare/v1.0.15...v1.1.0)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/fcb3205568718165f2edd88599e603ee21886132/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""sbt-release"" } ]
```
</details>

labels: sbt-plugin-update, semver-minor",Other,Other,
42551,what is the utility of backend='subprocess',"Hi Dear Anthony,

Thanks for your great software, I have another two quick questions that, 

1. what is the utility of 

> backend='subprocess'?

Because when I using pymetamap to extract concepts, the screen will show a lot of intermediate processes, is this 'backend='subprocess'' used to make the process silent?

2. Is it possible to get concepts by multi-threading?

Because I am working on a pretty large dataset, thus if I extract their concepts one by one, it will take pretty long time, thus I am thinking is there any chance to use the python multi-threading feature, like this:

> from multiprocessing import Pool
> p = Pool(5)
> def f(x): <--------------in the function I will extract concepts
... return x*x 
...
> p.map(f, [1,2,3])

Thanks a million in advance!",Other,Other,
35653,Added python 3.x support,"Hey @AnthonyMRios,

Can you please check this out? This pull request should be able to fix your issue #6 .

Best,
J",Other,Other,
4798,SharpLearning.Core: Merge SharpLearning.Containers and SharpLearning.Common.Interfaces,"Once #20, and #112 has been completed, the code in SharpLearning.Containers should be reduced quite a bit, and only contain a few basic types and support extension methods. For this reason it would make sense to merge this with the common interfaces assembly into a SharpLearning.Core project that holds the basic essentials for SharpLearning.",Other,Other,
31678,Get Soccer players orientation,"Hi, Im running the multi agent soccer environment and I want to get the coordinate system of each player with respect to the world. Is that possible? if it is, how can i do it?
Regards",Other,Other,
587,Name parser issue,"```Ruby
require 'namae'
Namae.options[:prefer_comma_as_separator] = true
Namae.parse ""I. F. Senturk, S. Yilmaz and K. Akkaya""
#=> [#<Name family=""Senturk"" given=""I. F."">, #<Name family=""Yilmaz"" given=""S."">, #<Name family=""Akkaya"" given=""K."">]
```

But parsing the a reference with those names using anystyle, parses the last two names as a single name (notably containing `S.Yilmaz` without a space, so some of the name [normalization patterns](https://github.com/inukshuk/anystyle-parser/blob/master/lib/anystyle/parser/parser.rb#L6) are likely the cause).",Other,Other,
13333,One word got lost in the title,"In this PDF https://iase-web.org/documents/papers/icots5/Topic1m.pdf

![image](https://user-images.githubusercontent.com/3487994/172229259-a814ff2a-7f13-4d63-bf9c-251e44c74638.png)",Other,Other,
34964,Known issues (and next release),"Below are issues affecting the [`rc1` data release](https://github.com/mdeff/fma#history) that cannot be fixed without a data update. As updating is disruptive (it'll break code and make results non-comparable), it should be done sparingly, e.g., to fix a fatal flaw or many small ones discovered over time.

* zip decompression fails because of unsupported bzip2 compression (#5)
 * [x] workaround (`master`): note in README to try with 7zip (5700859)
 * [ ] fix (`next`): zip with deflate (instead of bzip2) (#5) or zstd (#32)
* excerpts shorter than 30s and erroneous audio length metadata (#4, #8, #36, #44)
 * [x] workaround (`master`): [small subset's list](https://github.com/mdeff/fma/issues/8#issue-264055623), [medium subset's list](https://github.com/mdeff/fma/issues/8#issuecomment-344081080) (#8)
 * [x] fix (`next`): metadata from mp3 not API, ensure 30s (8077afe, 00d5b71, 840b337)
* erroneous ID3 tags (#27)
 * [x] workaround (`master`): [list](https://github.com/mdeff/fma/files/2207308/bad-fma-id3tags.txt) (#27)
 * [ ] fix (`next`): dump ID3 tags with technical metadata and remove from mp3
* exact duplicate tracks (#23)
 * [ ] workaround (`master`): list the 937 duplicates
 * [ ] fix (`next`): remove them (try other methods and detect near duplicates)

Workarounds are explained in more details in the [wiki](https://github.com/mdeff/fma/wiki).",Other,Other,
3359,Cannot exit thread because queue block forever.,"In [dehaze_transmission.py](https://github.com/Seanforfun/Deep-Learning/blob/master/DehazeNet/dehazenet_transmission.py), the task queue will block forever.",Other,Other,
20920,SemiSupervisedIOHMM couldn't load model,"On line:
`SHMM_from_json = SemiSupervisedIOHMM.from_json(json_dict)`
i get the following error:

> ---------------------------------------------------------------------------
> ValueError Traceback (most recent call last)
> <ipython-input-13-8fe9a1d9af1b> in <module>
> ----> 1 SHMM_from_json = SemiSupervisedIOHMM.from_json(json_dict)
> 
> ~/.local/lib/python3.5/site-packages/IOHMM/IOHMM.py in from_json(cls, json_dict)
> 539 model_initial=getattr(
> 540 LinearModelLoader, json_dict['properties']['model_initial']['data_type']).from_json(
> --> 541 json_dict['properties']['model_initial']),
> 542 model_transition=[getattr(
> 543 LinearModelLoader, model_transition_json['data_type']
> 
> ~/.local/lib/python3.5/site-packages/IOHMM/linear_models.py in from_json(cls, json_dict)
> 316 l1_ratio=json_dict['properties']['l1_ratio'],
> 317 coef=np.load(json_dict['properties']['coef']['path']),
> --> 318 stderr=np.load(json_dict['properties']['stderr']['path']))
> 319 
> 320 
> 
> ~/.local/lib/python3.5/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)
> 445 else:
> 446 return format.read_array(fid, allow_pickle=allow_pickle,
> --> 447 pickle_kwargs=pickle_kwargs)
> 448 else:
> 449 # Try a pickle
> 
> ~/.local/lib/python3.5/site-packages/numpy/lib/format.py in read_array(fp, allow_pickle, pickle_kwargs)
> 694 # The array contained Python objects. We need to unpickle the data.
> 695 if not allow_pickle:
> --> 696 raise ValueError(""Object arrays cannot be loaded when ""
> 697 ""allow_pickle=False"")
> 698 if pickle_kwargs is None:
> 
> ValueError: Object arrays cannot be loaded when allow_pickle=False",Other,Other,
25143,Android font sizes/scaling and large tablet,"I use the NewsBlur app on both my phone and tablet. On my phone, the latest update (2.5) looks great. On my large tablet (Samsung Galaxy Tab 10.1""), not so much. 

The main issue is in the article view. With version 2.1, I used the largest (XL) font size on my 10.1"" tablet. In version 2.5, the XL size has decreased to about the same size as M was on 2.1. This is too small for me -- sure, I can read it, but line length is too long, and the font size is a bit uncomfortable for my middle-aged eyes to read with for long periods.

I attempted to address this by changing my system's font size to a larger setting (even though that would mean adjusting all my other apps downward), but things are not scaling properly in the NewsBlur app. It did increase the font size in the article list, and in the header portion (title etc.) of the article view. The article body text sizes, however, remained the same as before the system change, as did the text in the folder/feed list (though the font for the unread count on that screen did increase).

I have reverted to using a backed-up copy of 2.1 on my tablet, but that's obviously a stopgap.

I'd like to request a change in the font handling, preferably increasing the range of font sizes available in the app so I may choose a larger size for the article text. If that's not feasible at this point, would you please ensure that the fonts used in the app scale properly with the device's font size setting -- as noted above, it is inconsistent currently and doesn't work for my most important use: the article text.

In case it's relevant, the tablet is running Android 4.1 at 1200x800 resolution.",Other,Other,
30867,weird results with pretrained models,"I can't reproduce the results even with the pretrained models. I get similar results for all data sets on gpu and cpu. A screen shot of the results below.

![screenshot-2018-4-17 screenshot](https://user-images.githubusercontent.com/16942719/38874395-40bbcf64-4258-11e8-8174-30fbac095d93.png)",Transparency & Explainability ,Transparency & Explainability ,
1742,Only extract one word from gumbel softmax,"In the code https://github.com/yala/text_nn/blob/master/rationale_net/utils/learn.py#L71-L85

```python
def get_hard_mask(z, return_ind=False):
 '''
 -z: torch Tensor where each element probablity of element
 being selected
 -args: experiment level config
 returns: A torch variable that is binary mask of z >= .5
 '''
 max_z, ind = torch.max(z, dim=-1)
 if return_ind:
 del z
 return ind
 masked = torch.ge(z, max_z.unsqueeze(-1)).float()
 del z
 return masked

```

because we take the max, usually, only one position will have the max value. 
In this case, if we have 100 words in the sentence, we only select one word as the rationale?
I thought we should select independently and choose those words with >0.5 probability.

Maybe we should change
```python
masked = torch.ge(z, max_z.unsqueeze(-1)).float()
```
to 
```python
masked = torch.ge(z, 0.5).float()
```
instead?",Other,Other,
21151,Update sbt to 1.9.0,"## About this PR
ð¦ Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from `1.8.3` to `1.9.0`

ð [GitHub Release Notes](https://github.com/sbt/sbt/releases/tag/v1.9.0) - [Version Diff](https://github.com/sbt/sbt/compare/v1.8.3...v1.9.0)

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/113884ac11d639ed8b70fc368f6a69895b14f8ba/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.scala-sbt"", artifactId = ""sbt"" }
}]
```
</details>

<sup>
labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1
</sup>",Other,Other,
9258,_rescale_data returns 3 outputs instead of 2 now,"Solves issue #39

<!-- Reviewable:start -->
- - -
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/Mogeng/IOHMM/40)
<!-- Reviewable:end -->",Other,Other,
20252,remove threee files related to circle ci,"<!-- Reviewable:start -->
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mogeng/io-hmm/4)
<!-- Reviewable:end -->",Other,Other,
22411,Pose estimation has huge position and orientation error for one object and does not appear to decrease,"Hello,

So I generated my own training set and am attempting to train the pose estimation portion with just one object, I was able to go through all the steps to get the CAD model properly setup, the PCD files and do all the stuff with the redis-server. I ended up attempting to train the model, and after 3000 epochs, I find the error is quite large, roughly the same as what it was at the start (37889.62 m for position, 127.81 m for orientation). Any advice on what I might be doing incorrectly?",Other,Other,
6563,m1ç³»ç»å®è£hanlpåºé can't find Rust compiler,"å®è£å¤±è´¥

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac m1 
- Python version:3.8.9
- HanLP version:1.8.3

**Other info / logs**

```python
 Building wheel for tokenizers (pyproject.toml) ... error
 error: subprocess-exited-with-error
 
 Ã Building wheel for tokenizers (pyproject.toml) did not run successfully.
 â exit code: 1
 â°â> [51 lines of output]
 running bdist_wheel
 running build
 running build_py
 creating build
 creating build/lib.macosx-10.14-arm64-cpython-38
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers
 copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
 copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
 copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
 copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
 copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
 copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
 copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/implementations
 creating build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
 copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
 copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
 copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers
 copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/models
 copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/decoders
 copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/normalizers
 copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/pre_tokenizers
 copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/processors
 copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/trainers
 copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.14-arm64-cpython-38/tokenizers/tools
 running build_ext
 running build_rust
 error: can't find Rust compiler
 
 If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
 
 To update pip, run:
 
 pip install --upgrade pip
 
 and then retry package installation.
 
 If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
 [end of output]
 
 note: This error originates from a subprocess, and is likely not a problem with pip.
 ERROR: Failed building wheel for tokenizers
Successfully built pyyaml
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects

```
* [x] I've completed this form and searched the web for solutions.",Other,Other,
12777,Getting This Error When Running on a graph with 1304 nodes,"![graph2vec error](https://user-images.githubusercontent.com/61021277/133727809-3a097487-158f-41a9-aaf5-569b94be2507.PNG)

Does anyone know how to solve this?",Other,Other,
18367,"was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38.","Failed to load https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip. See stack trace below
Traceback (most recent call last):
 File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 70, in get_tensor
 self, compat.as_bytes(tensor_str))
RuntimeError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
 File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\utils\component_util.py"", line 48, in load_from_meta_file
 obj.load(save_dir, **load_kwargs)
 File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 244, in load
 self.build(**merge_dict(self.config, training=False, logger=logger, **kwargs, overwrite=True, inplace=True))
 File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\common\component.py"", line 255, in build
 loss=kwargs.get('loss', None)))
 File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\components\taggers\transformers\transformer_tagger.py"", line 36, in build_model
 model, tokenizer = build_transformer(transformer, max_seq_length, len(self.transform.tag_vocab), tagging=True)
 File ""E:\PyCharmWorkspace\pwlp\pwlp-judged\hanlp\layers\transformers\loader.py"", line 141, in build_transformer
 skipped_weight_value_tuples = bert.load_bert_weights(l_bert, ckpt)
 File ""H:\Anaconda3\envs\py36\lib\site-packages\bert_for_tf2-0.12.7-py3.6.egg\bert\loader.py"", line 216, in load_stock_weights
 ckpt_value = ckpt_reader.get_tensor(stock_name)
 File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 74, in get_tensor
 error_translator(e)
 File ""H:\Anaconda3\envs\py36\lib\site-packages\tensorflow_core\python\training\py_checkpoint_reader.py"", line 48, in error_translator
 raise errors_impl.OpError(None, None, error_message, errors_impl.UNKNOWN)
tensorflow.python.framework.errors_impl.OpError: Checksum does not match: stored 114360999 vs. calculated on the restored bytes 3891682082
https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20200104_185735.zip was created with hanlp-2.0.0-alpha.5, while you are running 2.0.0-alpha.38. Try to upgrade hanlp with
pip install --upgrade hanlp
If the problem persists, please submit an issue to https://github.com/hankcs/HanLP/issues .
* [ ] I've completed this form and searched the web for solutions.",Other,Other,
39277,OutOfMemory Exception in F64Matrix constructor - maximum array bounds exceeded,"Hi there!

First: Thanks for the great work, excellent design you have there!

I am experiencing an OutOfMemory Exception in the constructor of F64Matrix that does not really come from memory shortage but from the fact that F64Matrix internally uses a single one dimensional double array that can quite easily exceed .NETs internal boundaries of maximum array dimensions.

In my case I tried to create a F64Matrix with 10 mio rows and 55 columns.

My preferred suggestion would be to either abstract the matrix to an IF64Matrix interface that probably only consists of the At() method overloads. This would enable users to provide a custom implementation that is capable of handling larger amounts of data, if needed even by swapping data from and to disk.
Another solution could be to change the internal implementation of F64Matrix to use an array of double arrays, which I believe could also help.

Thanks for your help and keep up the excellent work!

Best regards

Florian",Other,Other,
25559,Remove/clean unused types and classes,"Currently there are a few which contains functionality that is rarely used, or that doesn't quite fit into the current direction of the package. This includes:
 - `SharpLearning.Containers.Arithmetic`: MatrixF64 is mostly used as a container, and more efficient matrix arithmetic can found in other libraries, like mathnet.numerics.
 - `SharpLearning.Containers.ObservationTargetSet`: This can be replaced by using a value tuple instead.
 - `SharpLearning.Containers.ArrayExtensions`: Several methods are unused.
 - `SharpLearning.CrossValidation.ContinuousMungeAugmentator`: 
 - `SharpLearning.CrossValidation.NominalMungeAugmentator`:",Other,Other,
27621,Failed to build WAPITI gem native extension on linux,"Hello, I'm just looking to install this project on my ubuntu env but I get this error :(

```
# gem install anystyle
Fetching: latex-decode-0.3.1.gem (100%)
Successfully installed latex-decode-0.3.1
Fetching: bibtex-ruby-5.1.4.gem (100%)
Successfully installed bibtex-ruby-5.1.4
Fetching: anystyle-data-1.2.0.gem (100%)
Successfully installed anystyle-data-1.2.0
Fetching: builder-3.2.4.gem (100%)
Successfully installed builder-3.2.4
Fetching: wapiti-1.0.7.gem (100%)
Building native extensions. This could take a while...
ERROR: Error installing anystyle:
 ERROR: Failed to build gem native extension.

 current directory: /var/lib/gems/2.5.0/gems/wapiti-1.0.7/ext/wapiti
/usr/bin/ruby2.5 -r ./siteconf20200520-21321-daxumw.rb extconf.rb
checking for -lpthread... *** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers. Check the mkmf.log file for more details. You may
need configuration options.

Provided configuration options:
 --with-opt-dir
 --without-opt-dir
 --with-opt-include
 --without-opt-include=${opt-dir}/include
 --with-opt-lib
 --without-opt-lib=${opt-dir}/lib
 --with-make-prog
 --without-make-prog
 --srcdir=.
 --curdir
 --ruby=/usr/bin/$(RUBY_BASE_NAME)2.5
 --with-pthreadlib
 --without-pthreadlib
/usr/lib/ruby/2.5.0/mkmf.rb:456:in `try_do': The compiler failed to generate an executable file. (RuntimeError)
You have to install development tools first.
 from /usr/lib/ruby/2.5.0/mkmf.rb:541:in `try_link0'
 from /usr/lib/ruby/2.5.0/mkmf.rb:559:in `try_link'
 from /usr/lib/ruby/2.5.0/mkmf.rb:777:in `try_func'
 from /usr/lib/ruby/2.5.0/mkmf.rb:1004:in `block in have_library'
 from /usr/lib/ruby/2.5.0/mkmf.rb:947:in `block in checking_for'
 from /usr/lib/ruby/2.5.0/mkmf.rb:350:in `block (2 levels) in postpone'
 from /usr/lib/ruby/2.5.0/mkmf.rb:320:in `open'
 from /usr/lib/ruby/2.5.0/mkmf.rb:350:in `block in postpone'
 from /usr/lib/ruby/2.5.0/mkmf.rb:320:in `open'
 from /usr/lib/ruby/2.5.0/mkmf.rb:346:in `postpone'
 from /usr/lib/ruby/2.5.0/mkmf.rb:946:in `checking_for'
 from /usr/lib/ruby/2.5.0/mkmf.rb:999:in `have_library'
 from extconf.rb:20:in `<main>'

To see why this extension failed to compile, please check the mkmf.log which can be found here:

 /var/lib/gems/2.5.0/extensions/x86_64-linux/2.5.0/wapiti-1.0.7/mkmf.log

extconf failed, exit code 1

Gem files will remain installed in /var/lib/gems/2.5.0/gems/wapiti-1.0.7 for inspection.
Results logged to /var/lib/gems/2.5.0/extensions/x86_64-linux/2.5.0/wapiti-1.0.7/gem_make.out
```

So I looked for some fixes on Google but nothing seemed to help.
I tried to install _ruby-dev_ instead _ruby-full_, I tried to update gem, I tried to install wapiti directly from git source, but nothing work :(

If anyone can help me... A big thank you!",Other,Other,
17822,increase version to 0.4.0,was 0.3.9999,Other,Other,
43886,DomainError in tests,"in `v0.2.0`
```
INFO: Testing MLKernels.kernelmatrix!
ERROR: LoadError: LoadError: DomainError:
 in exponentialkernel at /home/me/.julia/v0.5/MLKernels/src/kernel.jl:110 [inlined]
```
`z` can be smaller than zero, I think this is due to calculating the squared euclidean distance as x^2 - 2xy + y^2",Other,Other,
37237,Correct Multiset Calculation,"Thanks for your work on this library. I was looking through the code after I applied this to my own dataset and noticed a potential issue with the calculation of the label multiset in the WeisfeilerLehmanMachine.

In the [original paper](https://people.mpi-inf.mpg.de/~mehlhorn/ftp/genWLpaper.pdf) they provide this figure to explain one iteration of the relabeling process(p. 10): 
<img width=""780"" alt=""Screen Shot 2019-07-28 at 2 12 43 PM"" src=""https://user-images.githubusercontent.com/5107405/62011148-9064e500-b142-11e9-8067-32678cea8278.png"">

In particular, the node with label `4` in the left graph has a pre-hashed label of `4,1135` - meaning it's original label is `4`, and the multiset representation of its neighbors labels is `1135`

I think there's an issue in how this multiset is calculated in the `WeisfeilerLehmanMachine` class, specifically this line: https://github.com/benedekrozemberczki/graph2vec/blob/master/src/graph2vec.py#L42 
 
I've got a simple reproduction of this logic here. I am expecting the output to be `4_1_1_3_5`, however, I get:

```python
>>> node_label = ""4""
>>> neighbor_labels = [""5"", ""3"", ""1"", ""1""]
>>> features = ""_"".join([node_label] + list(set(sorted([str(n) for n in neighbor_labels]))))
>>> print(features)
4_5_1_3
```

I believe you can fix this by removing the creation of the set, I'm not sure that's doing what we want. Since sets are [unordered collections of objects](https://docs.python.org/3.7/library/stdtypes.html#set-types-set-frozenset), we are effectively ordering the objects with `sorted` and then unordering them with `set`.

I could be wrong as I didn't evaluate the full pipliene here - there maybe some other change in the input to this function that does give you the correct answer, but I think this part of the implementation is worth another look.",Other,Other,
22761,ttx parser breaks if line doesn't end with space,"If in a `.ttx` file, a line does not end with a space (or is empty), the following error occurs:

```
Error: undefined method `gsub' for nil:NilClass
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/utils.rb:42:in `display_chars'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:10:in `block in parse'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:9:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:9:in `parse'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/document.rb:62:in `pages'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/document.rb:67:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:33:in `with_index'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:33:in `block in expand'
 /var/lib/gems/3.0.0/gems/wapiti-2.0.0/lib/wapiti/dataset.rb:61:in `each'
 /var/lib/gems/3.0.0/gems/wapiti-2.0.0/lib/wapiti/dataset.rb:61:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:32:in `expand'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/parser.rb:86:in `prepare'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:88:in `prepare'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/parser.rb:59:in `train'
 /app/cgi-bin/train.rb:55:in `<main>'
```

for example, the following line causes this to happen:

`blank |\n` 

This is a problem because IDEs routinely strip trailing whitespace.

I've been working around this by programmatically adding spaces but I think it would be much better to fix the source of the problem, given that it should be really straightforward to make the parser more robust in this respect.

Thank you.",Other,Other,
12322,How to implement Multi-Hop Attention using Keras?,MultiHopAttention was proposed by Fackbook.,Other,Other,
41190,"Train jSRE model for ""has_property"" relation","* [x] Train jSRE model for ""has_property"" relation using all labeled MPF and PHX docs
* [x] It would be good to create a script for training a new jSRE model, instead of referring to the wiki page where the process is documented.
* We need a separate jSRE model for each relation used. This means we'll also need to update `jsre_parser.py` to apply more than one model (see #34 )",Other,Other,
5728,Update sbt-scoverage to 1.9.1,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 1.9.0 to 1.9.1.
[GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v1.9.1) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v1.9.0...v1.9.1)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/4a2fab910a505ca6ac6000b3608e2f20e833e596/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
28592,"Android: Bugfixes, Custom Server Support","* Added a fix so users don't lose all of their state when the phantom-cookie bug hits.

* Implemented custom server endpoints.

* Rolled back the notification smoke test to work out some more bugs now that it has had some time to cook.

Should be good for quick beta and then release.",Other,Other,
44082,Attention not working for MLP,"I need to add attention to my following model. It works perfectly for LSTM model but I get the below error : 
```
def get_ANN_attention_model(num_hidden_layers, num_neurons_per_layer, dropout_rate, activation_func, train_X):
 with tf.device('/gpu:0'):
 model_input = tf.keras.Input(shape=(train_X.shape[1])) # input layer.
 for i in range(num_hidden_layers):
 x = layers.Dense(num_neurons_per_layer,activation=activation_func,bias_regularizer=L1L2(l1=0.0, l2=0.0001),activity_regularizer=L1L2(1e-5,1e-4))(model_input)
 x = layers.Dropout(dropout_rate)(x)
 x = Attention(num_hidden_layers)(x)
 outputs = layers.Dense(1, activation='linear')(x)
 model = tf.keras.Model(inputs=model_input, outputs=outputs)
 model.summary()
 return model
```

**ERROR**
 hidden_size = int(hidden_states.shape[2])
 File ""C:\Users\bhask\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\tensor_shape.py"", line 896, in __getitem__
 return self._dims[key].value
IndexError: list index out of range",Other,Other,
30907,Loading `latest_net_G.t7 ` model requires CUDA from util.lua in (CPU_MODE) Mac,"```bash
------------------- Options -------------------
 DATA_ROOT: ./datasets/ae_photos
 align_data: 0
 aspect_ratio: 1
 batchSize: 1
 checkpoints_dir: ./checkpoints
 continue_train: 1
 cudnn: 0
 display: 1
 display_id: 200
 fineSize: 256
 flip: 0
 gpu: 0
 how_many: all
 input_nc: 3
 loadSize: 256
 model: one_direction_test
 nThreads: 1
 name: style_cezanne_pretrained
 norm: instance
 output_nc: 3
 phase: test
 resize_or_crop: scale_width
 results_dir: ./results/
 serial_batches: 1
 test: 1
 which_direction: AtoB
 which_epoch: latest
-----------------------------------------------
CPU Mode
{
 cudnn : 0
 results_dir : ""./results/""
 resize_or_crop : ""scale_width""
 name : ""style_cezanne_pretrained""
 which_direction : ""AtoB""
 phase : ""test""
 fineSize : 256
 batchSize : 1
 continue_train : 1
 aspect_ratio : 1
 loadSize : 256
 gpu : 0
 nThreads : 1
 DATA_ROOT : ""./datasets/ae_photos""
 test : 1
 which_epoch : ""latest""
 align_data : 0
 model : ""one_direction_test""
 norm : ""instance""
 how_many : ""all""
 input_nc : 3
 display : 1
 output_nc : 3
 flip : 0
 checkpoints_dir : ""./checkpoints""
 display_id : 200
 serial_batches : 1
}
DataLoader UnalignedDataLoader was created.
Starting donkey with id: 1 seed: 6108
table: 0x0836e0a0
table: 0x084acfc8
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
now combine all the files to a single large file
load the large concatenated list of sample paths to self.imagePath
cmd..gwc -L '/tmp/lua_RBFY3t' |gcut -f1 -d' '
205 samples found.............. 0/205 .............................] ETA: 0ms | Step: 0ms 
Updating classList and imageClass appropriately
 [============================= 1/1 ==============================>] Tot: 0ms | Step: 0ms 
Cleaning up temporary files
Dataset Size A: 205
Starting donkey with id: 1 seed: 3950
table: 0x08905270
table: 0x08855168
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
now combine all the files to a single large file
load the large concatenated list of sample paths to self.imagePath
cmd..gwc -L '/tmp/lua_YTSNnD' |gcut -f1 -d' '
205 samples found.............. 0/205 .............................] ETA: 0ms | Step: 0ms 
Updating classList and imageClass appropriately
 [============================= 1/1 ==============================>] Tot: 0ms | Step: 0ms 
Cleaning up temporary files
Dataset Size B: 205
use InstanceNormalization
loading previously trained model (~/Documents/BigDataLab/Lotus/CycleGAN/checkpoints/style_cezanne_pretrained/latest_net_G.t7)
use InstanceNormalization
~/torch/install/bin/luajit: ~/torch/install/share/lua/5.1/torch/File.lua:343: unknown Torch class <torch.CudaTensor>
stack traceback:
[C]: in function 'error'
/Users/HANEL/torch/install/share/lua/5.1/torch/File.lua:343: in function 'readObject'
/Users/HANEL/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
/Users/HANEL/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
/Users/HANEL/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
/Users/HANEL/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'
...~/Documents/BigDataLab/Lotus/CycleGAN/util/util.lua:192: in function 'load_test_model'
./models/one_direction_test_model.lua:23: in function 'Initialize'
test.lua:79: in main chunk
[C]: in function 'dofile'
...ANEL/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
[C]: at 0x0107710360
```",Other,Other,
40747,New complementary tool,"My name is Luis, I'm a big-data machine-learning developer, I'm a fan of your work, and I usually check your updates. 

I was afraid that my savings would be eaten by inflation. I have created a powerful tool that based on past **technical patterns** (volatility, moving averages, statistics, trends, candlesticks, support and resistance, stock index indicators).
All the ones you know (RSI, MACD, STOCH, Bolinger Bands, SMA, DEMARK, Japanese candlesticks, ichimoku, fibonacci, williansR, balance of power, murrey math, etc) and more than 200 others.

The tool creates prediction models of correct trading points (buy signal and sell signal, every stock is good traded in time and direction).
For this I have used big data tools like pandas python, stock market libraries like: tablib, TAcharts ,pandas_ta... For data collection and calculation.
And powerful machine-learning libraries such as: Sklearn.RandomForest , Sklearn.GradientBoosting, XGBoost, Google TensorFlow and Google TensorFlow LSTM.

With the models trained with the selection of the best technical indicators, the tool is able to predict trading points (where to buy, where to sell) and send real-time alerts to Telegram or Mail. The points are calculated based on the learning of the correct trading points of the last 2 years (including the change to bear market after the rate hike).

I think it could be useful to you, to improve, I would like to give it to you, and if you are interested in improving and collaborating I am also willing, and if not I would like to file it in the drawer.",Other,Other,
27954,iOS: Allow usernames up to 128 characters,"From a user:

![Unknown](https://user-images.githubusercontent.com/44229/202526674-f2f62631-275e-4e77-873c-2eadd21bf307.jpeg)",Other,Other,
13673,how can i train the Neural Network with my own Training Pictures?,"let's say i have a list of Images ..how do i convert my images into F64matrix Form so i can train them with my Neural Network ..and how do i test the Network with an Image at the End
could you please make an example of this because you didn't mention that in the examples, you also didn't mention how to test the Network using a real Image..

Thanks in Advance",Other,Other,
29488,Consolidate common project settings into Directory.Build.props,"Where possible, add common project settings to Directory.Build.props",Other,Other,
11274,[datastore]: StackOverflow for Case Class with over 22 fields,"I have a 4 converters where I'm calling `toEntityBuilder` and if I call `sbt clean compile` I consistently get the following StackOverflowError - https://gist.github.com/grahamar/f9100091d61f0313327a07e6aba06435#file-stack-txt

But If I don't `clean` it will work, which makes me think it's a race condition... All my code looks like - https://gist.github.com/grahamar/f9100091d61f0313327a07e6aba06435#file-authorconverter-scala, nothing complex.

My types are all very straight forward too:

```
case class Author(
 id: Long = 0L,
 source_id: Option[Long] = None,
 name: Option[String] = None,
 enabled: Option[Boolean] = None,
 avatar_url: Option[String] = None,
 extracted_name: Option[String] = None,
 item_name: Option[String] = None,
 unique_id: Option[String] = None,
 deleted_at: Option[String] = None,
 created_at: Option[String] = None,
 updated_at: Option[String] = None
)
```

Any help is appreciated, I don't have enough knowledge of shapeless to know where too look either...",Other,Other,
27644,Add Mediawiki Template output format support,"I actually already wrote this, but am adding it here as an issue so someone will bug be if I don't get around to committing the tree. For several reasons, I'd rather rebase my working tree to commit a cleaner tree. Hopefully will have this done within a week.",Other,Other,
2035,com.hankcs.hanlp.corpus.io.ByteArrayOtherStream.ensureAvailableBytes ä¸­ int availableBytes = is.available(),,Other,Other,
36495,Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5Ã3 positions and 5Ã4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5Ã3 positions and 5Ã4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4Ã3 positions and 4Ã4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter networkã",Transparency & Explainability ,Transparency & Explainability ,FIXED
35810,Identified an issue related with the parser or training,"I've identified an issue with the parser. Can you tell me how can i work with getting the expected output. As, of now main issue is **Author** should be ""**Social Security Administration**"" other than ""**Administration, Social Security**""

INPUT:

> Social Security Administration. Social Security Programs Throughout the World: Asia and the Pacific, 2008. Vol. 13. No. 11801. Government Printing Office, 2002.

TRAIN DATA:
`<author>Social Security Administration</author><title>Social Security Programs Throughout the World: Asia and the Pacific, 2008. </title><volume>Vol. 13. No. 11801. </volume><publisher>Government Printing Office, </publisher><date>2002.</date>`
```
Anystyle.parser.train '/var/www/volume/temp_dta/train_test.txt', false
Anystyle.parser.model.save
```

Expected Output:

> @book{**social2002social**,
> **author = {Social Security Administration},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

What i got output other than expected?

> @book{**administration2002a**,
> **author = {Administration, Social Security},**
> title = {Social Security Programs Throughout the World: Asia and the Pacific, 2008},
> volume = {13},
> publisher = {Government Printing Office},
> date = {2002},
> number = {11801},
> language = {}
> }

REF: 
https://scholar.google.com/scholar?hl=en&q=security&btnG=&oq=sec",Security & Safety,Security & Safety,
33846,import hanlp bug on hanlp-2.1.0a55,"import hanlpçæ¶åï¼åºç°AttributeError 
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
/var/folders/31/7mbjndl966d92drq1zvt9t3c0000gn/T/ipykernel_30433/3724283222.py in <module>
----> 1 import hanlp #Successfully installed hanlp-2.1.0a55

~/anaconda3/lib/python3.7/site-packages/hanlp/__init__.py in <module>
 8 from hanlp.version import __version__
 9 
---> 10 hanlp.utils.ls_resource_in_module(hanlp.pretrained)
 11 
 12 
AttributeError: module 'hanlp' has no attribute 'utils' 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS montery 
- Python version: Python 3.7.3
- HanLP version:hanlp-2.1.0a55

**Other info / logs** 

* [x] I've completed this form and searched the web for solutions.",Other,Other,
35496,Deploy on the production server. Invite users for beta test,"Only a handful models are able to serve. Those subclass models are not likely to fit with tensorflow serving. Need more time to investigate.

Will need an authentication service to identify users for test purpose and rate limiting. I don't have much computation resource for all the users. Might invite 10 lucky users.",Other,Other,
11074,About *.nii to *.bmp for test,"æ¨å¥½ï¼è¿æ®µniiè½¬bmpçä»£ç æ¯ææ¨¡ä»¿çæè®­ç»æ°æ®åçï¼æµè¯çææå¾å·®ï¼æ³ç¥éé®é¢æ¯åºå¨äºåªé ï¼ï¼

```
def prepareTestData():
 for i in range(0, 70, 1):
 filepath = testImage + str(i) + "".nii""
 src = load_itk(filepath)
 data = sitk.GetArrayFromImage(src)
 data = data.astype(np.float32)
 data = np.clip(data, 0, 255).astype('uint8')
 channel, width, height = data.shape[0], data.shape[1], data.shape[2]
 for j in range(channel):
 savePath = outputPath + str(i)
 if not os.path.exists(savePath):
 os.makedirs(savePath)
 v_flip = cv2.flip(data[j, :, :], 0) # ä¸ä¸ç¿»è½¬å¾ç
 cv2.imwrite(savePath + ""/"" + str(j) + "".bmp"", v_flip)
```",Other,Other,
19906,Bump pillow from 6.0.0 to 8.3.2,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.3.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.3.2</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.2.html</a></p>
<h2>Security</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
</ul>
<h2>Python 3.10 wheels</h2>
<ul>
<li>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</li>
</ul>
<h2>Fixed regressions</h2>
<ul>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.1.html</a></p>
<h2>Changes</h2>
<ul>
<li>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a> [<a href=""https://github.com/t-vi""><code>@ât-vi</code></a>]</li>
</ul>
<h2>8.3.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.3.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Use snprintf instead of sprintf <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a> [<a href=""https://github.com/kmilos""><code>@âkmilos</code></a>]</li>
<li>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a> [<a href=""https://github.com/newpanjing""><code>@ânewpanjing</code></a>]</li>
<li>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a> [<a href=""https://github.com/gofr""><code>@âgofr</code></a>]</li>
<li>Do not use background or transparency index for new color <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5564"">#5564</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Simplified code <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5315"">#5315</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.3.2 (2021-09-02)</h2>
<ul>
<li>
<p>CVE-2021-23437 Raise ValueError if color specifier is too long
[hugovk, radarhere]</p>
</li>
<li>
<p>Fix 6-byte OOB read in FliDecode
[wiredfool]</p>
</li>
<li>
<p>Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5569"">#5569</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5570"">#5570</a>
[hugovk, radarhere]</p>
</li>
<li>
<p>Ensure TIFF <code>RowsPerStrip</code> is multiple of 8 for JPEG compression <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5588"">#5588</a>
[kmilos, radarhere]</p>
</li>
<li>
<p>Updates for <code>ImagePalette</code> channel order <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5599"">#5599</a>
[radarhere]</p>
</li>
<li>
<p>Hide FriBiDi shim symbols to avoid conflict with real FriBiDi library <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5651"">#5651</a>
[nulano]</p>
</li>
</ul>
<h2>8.3.1 (2021-07-06)</h2>
<ul>
<li>
<p>Catch OSError when checking if fp is sys.stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5585"">#5585</a>
[radarhere]</p>
</li>
<li>
<p>Handle removing orientation from alternate types of EXIF data <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5584"">#5584</a>
[radarhere]</p>
</li>
<li>
<p>Make Image.<strong>array</strong> take optional dtype argument <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5572"">#5572</a>
[t-vi, radarhere]</p>
</li>
</ul>
<h2>8.3.0 (2021-07-01)</h2>
<ul>
<li>
<p>Use snprintf instead of sprintf. CVE-2021-34552 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5567"">#5567</a>
[radarhere]</p>
</li>
<li>
<p>Limit TIFF strip size when saving with LibTIFF <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5514"">#5514</a>
[kmilos]</p>
</li>
<li>
<p>Allow ICNS save on all operating systems <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4526"">#4526</a>
[baletu, radarhere, newpanjing, hugovk]</p>
</li>
<li>
<p>De-zigzag JPEG's DQT when loading; deprecate convert_dict_qtables <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4989"">#4989</a>
[gofr, radarhere]</p>
</li>
<li>
<p>Replaced xml.etree.ElementTree <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5565"">#5565</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8013f130a5077b238a4346b73e149432b180a8ea""><code>8013f13</code></a> 8.3.2 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/23c7ca82f09df6ba1047d2d96714eb825f0d7948""><code>23c7ca8</code></a> Update CHANGES.rst</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8450366be331762ae327036e3c6658c517b05638""><code>8450366</code></a> Update release notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/a0afe89990f5ba40a019afc2f22e1b656f8cfd03""><code>a0afe89</code></a> Update test case</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/9e08eb8f78fdfd2f476e1b20b7cf38683754866b""><code>9e08eb8</code></a> Raise ValueError if color specifier is too long</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bd5cf7db87c6abf7c3510a50170851af5538249f""><code>bd5cf7d</code></a> FLI tests for Oss-fuzz crash.</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/94a0cf1b14f09626c7403af83fa9fef0dfc9bb47""><code>94a0cf1</code></a> Fix 6-byte OOB read in FliDecode</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cece64f4be10ab28b12a83a3555af579dad343a5""><code>cece64f</code></a> Add 8.3.2 (2021-09-02) [CI skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e42238637651f191c2fc6e3f4024348c126e0ccc""><code>e422386</code></a> Add release notes for Pillow 8.3.2</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/08dcbb873217874eee0830fc5aaa1f231c5af4fa""><code>08dcbb8</code></a> Pillow 8.3.2 supports Python 3.10 [ci skip]</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.3.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.3.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
12593,Emissions Probability Distribution,"Hi!

I have adjusted an unsupervised IOHMM with 3 states and an emission covariate (see code below).

`SHMM = UnSupervisedIOHMM(num_states=3, max_EM_iter=200, EM_tol=1e-4)`

`SHMM.set_models(model_emissions = [OLS(est_stderr=True)], model_transition=CrossEntropyMNL(solver='lbfgs'), model_initial=CrossEntropyMNL(solver='lbfgs'))`

`SHMM.set_inputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [['var']])`

`SHMM.set_outputs([['out']])`

`SHMM.set_data([df])`

`SHMM.train()`

I am trying to get the Emissions Probability Distribution in each state based on the emission covariate value. How can I model this probability? I am getting two coefficients and two estimated standard error coefficients of the emission model in each state. 

How can I use these coefficients?

Appreciate your answers!",Other,Other,
16664,Bibetex Error when running program,"[root@localhost anystyle]# ruby parser.rb 
/usr/share/rubygems/rubygems/core_ext/kernel_require.rb:55:in `require': cannot load such file -- bibtex (LoadError)
 from /usr/share/rubygems/rubygems/core_ext/kernel_require.rb:55:in`require'
 from parser.rb:4:in `<main>'
[root@localhost anystyle]#",Other,Other,
38504,Multiple Reference Parsing,"Hi, 
I am trying to parse multiple references simultaneously, currently iterating through array of citations. Can I pass the algorithm one string of multiple citations instead?
Thanks",Other,Other,
35164,a question about making own datasets,"Hello, I now mark each image in my dataset with âlabelmeâ and convert the annotation.json format to coco format, but these files are one by one, how to properly merge a annotation file, just like the annotations file you provide in Oilchangedatasets?
![image](https://user-images.githubusercontent.com/34231078/56178671-a805db00-6035-11e9-95d3-7ed905db812d.png)",Other,Other,
24108,Android: Premium Archive subscription,"We have a new subscription tier launching soon. It's on beta.newsblur.com. Here's the extra text we need to add (found in: https://github.com/samuelclay/NewsBlur/blob/e09c97ef6f60d018eb3ab040203ebfe24cf7c367/media/js/newsblur/reader/reader_feedchooser.js#L184-L217) 


<img width=""1483"" alt=""Screen Shot 2022-06-01 at 2 43 18 PM"" src=""https://user-images.githubusercontent.com/44229/171479030-112b5a65-0547-472c-87f7-47addcf91fc0.png"">",Other,Other,
15183,can I use this module in virtual environment?,"Now, I try to install in virtual-env.
but I couldn't find the way for install.
could you help me?",Other,Other,
20579,Bump pillow from 6.0.0 to 9.0.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 9.0.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>9.0.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Restrict builtins for ImageMath.eval() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed ImagePath.Path array handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Removed redundant part of condition <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5915"">#5915</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Explicitly enable strip chopping for large uncompressed TIFFs <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5517"">#5517</a> [<a href=""https://github.com/kmilos""><code>@âkmilos</code></a>]</li>
<li>Use the Windows method to get TCL functions on Cygwin <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5807"">#5807</a> [<a href=""https://github.com/DWesl""><code>@âDWesl</code></a>]</li>
<li>Changed error type to allow for incremental WebP parsing <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5404"">#5404</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Ensure that BMP pixel data offset does not ignore palette <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5899"">#5899</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Use latin1 encoding to decode bytes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5870"">#5870</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Image.NONE is only used for resampling and dithers <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5908"">#5908</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Add Tidelift alignment action and badge <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5763"">#5763</a> [<a href=""https://github.com/aclark4life""><code>@âaclark4life</code></a>]</li>
<li>Replaced further direct invocations of setup.py <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5906"">#5906</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a> [<a href=""https://github.com/m-shinder""><code>@âm-shinder</code></a>]</li>
<li>Fixed typo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5902"">#5902</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Switched from deprecated &quot;setup.py install&quot; to &quot;pip install .&quot; <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5896"">#5896</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a> [<a href=""https://github.com/cmbruns""><code>@âcmbruns</code></a>]</li>
<li>Fixed raising OSError in _safe_read when size is greater than SAFEBLOCK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5872"">#5872</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>WebP: Fix memory leak during decoding on failure <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5798"">#5798</a> [<a href=""https://github.com/ilai-deutel""><code>@âilai-deutel</code></a>]</li>
<li>Do not prematurely return in ImageFile when saving to stdout <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5665"">#5665</a> [<a href=""https://github.com/infmagic2047""><code>@âinfmagic2047</code></a>]</li>
<li>Added support for top right and bottom right TGA orientations <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5829"">#5829</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Corrected ICNS file length in header <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5845"">#5845</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Block tile TIFF tags when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5839"">#5839</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added line width argument to ImageDraw polygon <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5694"">#5694</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Do not redeclare class each time when converting to NumPy <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5844"">#5844</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Only prevent repeated polygon pixels when drawing with transparency <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5835"">#5835</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix pushes_fd method signature <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5833"">#5833</a> [<a href=""https://github.com/hoodmane""><code>@âhoodmane</code></a>]</li>
<li>Add support for pickling TrueType fonts <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5826"">#5826</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Only prefer command line tools SDK on macOS over default MacOSX SDK <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5828"">#5828</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix compilation on 64-bit Termux <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5793"">#5793</a> [<a href=""https://github.com/landfillbaby""><code>@âlandfillbaby</code></a>]</li>
<li>Replace 'setup.py sdist' with '-m build --sdist' <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5785"">#5785</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Use declarative package configuration <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5784"">#5784</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Use title for display in ImageShow <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5788"">#5788</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix for PyQt6 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5775"">#5775</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>9.0.0 (2022-01-02)</h2>
<ul>
<li>
<p>Restrict builtins for ImageMath.eval(). CVE-2022-22817 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a>
[radarhere]</p>
</li>
<li>
<p>Ensure JpegImagePlugin stops at the end of a truncated file <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a>
[radarhere]</p>
</li>
<li>
<p>Fixed ImagePath.Path array handling. CVE-2022-22815, CVE-2022-22816 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a>
[radarhere]</p>
</li>
<li>
<p>Remove consecutive duplicate tiles that only differ by their offset <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>
[radarhere]</p>
</li>
<li>
<p>Improved I;16 operations on big endian <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5901"">#5901</a>
[radarhere]</p>
</li>
<li>
<p>Limit quantized palette to number of colors <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5879"">#5879</a>
[radarhere]</p>
</li>
<li>
<p>Fixed palette index for zeroed color in FASTOCTREE quantize <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5869"">#5869</a>
[radarhere]</p>
</li>
<li>
<p>When saving RGBA to GIF, make use of first transparent palette entry <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5859"">#5859</a>
[radarhere]</p>
</li>
<li>
<p>Pass SAMPLEFORMAT to libtiff <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5848"">#5848</a>
[radarhere]</p>
</li>
<li>
<p>Added rounding when converting P and PA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5824"">#5824</a>
[radarhere]</p>
</li>
<li>
<p>Improved putdata() documentation and data handling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5910"">#5910</a>
[radarhere]</p>
</li>
<li>
<p>Exclude carriage return in PDF regex to help prevent ReDoS <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5912"">#5912</a>
[hugovk]</p>
</li>
<li>
<p>Fixed freeing pointer in ImageDraw.Outline.transform <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5909"">#5909</a>
[radarhere]</p>
</li>
<li>
<p>Added ImageShow support for xdg-open <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5897"">#5897</a>
[m-shinder, radarhere]</p>
</li>
<li>
<p>Support 16-bit grayscale ImageQt conversion <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5856"">#5856</a>
[cmbruns, radarhere]</p>
</li>
<li>
<p>Convert subsequent GIF frames to RGB or RGBA <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5857"">#5857</a>
[radarhere]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/82541b6dec8452cb612067fcebba1c5a1a2bfdc8""><code>82541b6</code></a> 9.0.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/cae5ac495badd7c7ecfad8223a08f55f5d2eaacb""><code>cae5ac4</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5924"">#5924</a> from radarhere/cves</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ed4cf7813777ad8478cac46f448bc45416a2a99e""><code>ed4cf78</code></a> CVEs TBD</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/d7f60d1d5a746eb01d4cb3c7fb05b6593f46b0f5""><code>d7f60d1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5923"">#5923</a> from radarhere/imagemath_eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8531b01d6cdf0b70f256f93092caa2a5d91afc11""><code>8531b01</code></a> Restrict builtins for ImageMath.eval</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1efb1d9fabd1dfdbf7982035eca0dae7306abef1""><code>1efb1d9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5922"">#5922</a> from radarhere/releasenotes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/f6c78713a491764dfac576f6c42127755f2c62b3""><code>f6c7871</code></a> Added release notes for <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5919"">#5919</a>, <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5920"">#5920</a> and <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a></li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/032d2dc3658f94718109068ac70799313e440754""><code>032d2dc</code></a> Update CHANGES.rst [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/baae9ec4b67c68e3adaf1208cf54e8de5e38a6fd""><code>baae9ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5921"">#5921</a> from radarhere/jpeg_eoi</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/1059eb537639925c96d3245dcd73c106d4266c83""><code>1059eb5</code></a> If appended EOI did not work, do not keep trying</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...9.0.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=9.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
44749,Update sbt-sonatype to 3.9.5,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.4 to 3.9.5.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.5) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.4...3.9.5)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b4241535fa3ead05e4a17193048fdd1a7d59e9bd/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
28091,iOS: Account deletion,"Don't think we support this yet but the [App Store Guidelines 5.1.1](https://developer.apple.com/app-store/review/guidelines/#5.1.1) states that we need to support it. 

Kick the user over to /profile/delete_account in a webview. Launch it from the bottom of the Preferences dialog. Also, let's do this after we ship the testflight branch in February. We have until June 2022.",Other,Other,
14180,Series and Series Number Merged by Web Interface,"When I import the following through the web interface at anystyle.io, and use the ""Collection title"" label for ""Men at Arms 373,""

`Brzeinski, Richard / Mielczarek, Mariusz (2002) The Sarmatians 600 BC-AD 450. Men at Arms 373 (Osprey Publishing: Botley)`

the Zotero entry has ""Series: Men at Arms 373"" not ""Series: Men at Arms"" and ""Series Number: 373."" I am new to Zotero and BibTex but I think name and number are logically two separate entities. I don't see an appropriate ""collection number"" label in the web interface.",Other,Other,
40788,"SyntaxError: invalid syntax in File ""/workspace/mxnet_benchmarks/benchmarks.py"", line 390","hi,Here I come again.I used the mxnet framework, there was a syntax error.
my commandï¼
```
 python $experimenter run --log-level=warning -Pexp.framework='""mxnet""' -Pexp.gpus='0' \
-Pexp.docker=true -Pmonitor.frequency=0.1 -Vexp.replica_batch='[16]' -Pexp.num_warmup_batches=10\
-Pexp.num_batches=10 -Pmxnet.cudnn_autotune='""false""' -Vexp.model='[""alexnet""]' \
-Pexp.phase='""training""' \
-Pexp.log_file='""${BENCH_ROOT}/mxnet/${exp.model}_${exp.effective_batch}.log""'\
-Pmxnet.docker_image='""nvcr.io/nvidia/mxnet:18.11-py3""'
```

error:
```
__results.start_time__= ""2018-12-18:08:54:20:839""
 File ""/workspace/mxnet_benchmarks/benchmarks.py"", line 390
 except Exception, e:
 ^
SyntaxError: invalid syntax
```
@sergey-serebryakov",Other,Other,
8738,possible typo in book,"First congrats on the book! Looks great, very excited to go through it. Not sure if this is the right place to report typos but on p 16, section 2.4 in it refers to ""3...independent features (petal and sepal measurements..."" and ""4...dependent feature (species label)"" and then at the bottom of the page it says ""we have access to the independent (species labels) and dependent (sepal and petal measurements)"" The independent/dependent labels are reversed. I believe the first labels are correct but figured i'd let you know.",Other,Other,
19403,How to create a new mask?,"Suppose I want to create another mask for fishes, ships, etc. How to create those masks?
thank you very much in advance.

warmest regards,
suryadi",Other,Other,
36421,Full access to trained model structures,"Currently the trained models only expose members from IPredictorModel. For NNs, its impossible to get to the layered structures of the best model. Extend models to expose this",Security & Safety,Security & Safety,FIXED
31724,"How to better avoid ""Error: free joint can only be used on top level"" when combining two models","I want to create separate xml files for robot, object, arena and combine them together later.
Because all the objects are very similar, I decided to put all of them in one xml file. Each object are should move freely. 
Something like this:
```XML
<mujoco model=""objects"">
 <worldbody>
 <body >
 <freejoint/>
 <geom>
 </body>

 <body >
 <freejoint/>
 <geom>
 </body>
 </worldbody>
```

Then I use .attach to combine two models: modelArena.attach(modelObject)

But, this will create an outside <body that wraps all <body from modelObject.
Something like this:
```XML
<mujoco model=""arena>
 <worldbody>

<body name=""objects""> 
 <body>
 <freejoint/>
 <geom>
 </body>
 <body>
 <freejoint/>
 <geom>
 </body>
</body>
 </worldbody>
```
Then I will get a problem ""Error: free joint can only be used on top level"".

I know one solution would be to create xml file for each object individually and let the attachment frame add('freejoint').
But this is not very elegant, and it will create many redundant attributes (like <default, because each xml file has the same default setting) 

So is there any better way to achieve the same result? I tried to find a way to do without creating another <body that wraps all these but still haven't found one. Thanks!",Other,Other,
28048,Show opml imported & exported activity,"When we subscribe to any RSS it shows on our ACTIVITY that ""`You subscribed to ABCD`"". Similarly show opml import/export message on our activity. It will help users to check when we imported/exported the file last time. Easy to keep track things ; for an example if you are using multiple RSS services/apps...",Other,Other,
2228,"Add stripping of ""by ..."" to names normalizer","I suggest to add a regular expression to strip occurrences of leading ""by"" in the names normalizer

https://github.com/inukshuk/anystyle/blob/master/lib/anystyle/normalizer/names.rb#L43 

so that the ""by"" can be included in the ""author"" annotation and be pattern-recognized in contexts such as

![grafik](https://user-images.githubusercontent.com/75390/201177237-454446b2-e474-4283-9315-71d01f59da4a.png)",Other,Other,
3275,Fix typo in options.lua,The comment above opt_train incorrectly stated that it's the options for test,Other,Other,
37272,Update sbt to 1.9.1,"## About this PR
ð¦ Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from `1.9.0` to `1.9.1`

ð [GitHub Release Notes](https://github.com/sbt/sbt/releases/tag/v1.9.1) - [Version Diff](https://github.com/sbt/sbt/compare/v1.9.0...v1.9.1)

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5a5bbecb93b85b71cf7fd58670266ca5fb5e7bbf/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.scala-sbt"", artifactId = ""sbt"" }
}]
```
</details>

<sup>
labels: library-update, early-semver-patch, semver-spec-patch, version-scheme:early-semver, commit-count:1
</sup>",Other,Other,
31276,Consider removing dependency over downstream packages,"My idea for ScikitLearn.jl at the start was that each package would implement the `ScikitLearnBase.jl` interface, so that `ScikitLearn.jl` users wouldn't have everything installed. This is [not true anymore](https://github.com/cstjean/ScikitLearn.jl/blob/master/Project.toml) with the `DecisionTree` / `GaussianProcesses` / ... dependencies. 

@OkonSamuel Did you have a specific reason for putting them in the Project.toml? I'm not necessarily against it, but I think it should be discussed at least.",Other,Other,
37628,How to build for Mac,I dont see any build instructions for Mac. Is it compatible? If yes can you please add the mac build instructions.,Other,Other,
39351,Using GPU to accelerate the eigen decomposition in the psuedo-inverse calculation in nystrom.jl,"Here is my implementation:
```julia
using CuArrays, CUDAnative, LinearAlgebra, MLKernel

function make_symmetric(A::Mat, uplo::Char='U') where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
 return LinearAlgebra.copytri!(A |> Matrix{T}, uplo)
end

function nystrom_inv_gpu!(A::Mat) where {T<:AbstractFloat, Vec<:AbstractVector{T}, Mat<:AbstractMatrix{T}}
 A = cu(A)
 vals, vectors = CuArrays.CUSOLVER.syevd!('V', 'U', A)
 tol = eps(T)*size(A,1)
 max_eig = maximum(vals)
 # for i in eachindex(vals)
 # vals[i] = abs(vals[i]) <= max_eig * tol ? zero(T) : one(T) / sqrt(vals[i])
 # end
 predicate = one(T) .* (vals .>= max_eig * tol)
 vals .= predicate .* CUDAnative.rsqrt.(vals .^ predicate)
 QD = CuArrays.CUBLAS.dgmm!('R', vectors, vals, vectors)
 W = CuArrays.CUBLAS.syrk('U', 'N', QD)
 return make_symmetric(W)
end
```",Other,Other,
17844,Edge cases,"LogKernel is not pos.def. anyway; constructor says gamma has to be in (0,1] but only checks for gamma>0. Is there a reason for gamma<=1? If not, error message should be adjusted, otherwise code & test...",Other,Other,
4157,Do you plan to add support for open simulator ?,"dm_control is becoming a reference as far as evaluating RL benchmark is concerned. Unfortunately it is based on a Mujoco which is a closed and not easily accessible for every one. Furthermore, the development of MuJoCo seems to be dead, last release 3 years ago, no support for such an expensive product.
With the research reproducibility in mind, why did Deepmind not choose an open simulator as a backend for dm_control ? Or add possibilities to use several backend ?",Other,Other,
39117,Merge pull request #12 from nekitmm/tf2,TF2 version of StarNet,Other,Other,
32481,Pre-trained model link,"The pre-trained model link seems broken:

https://drive.google.com/file/d/1U21Bwchcm96-Nz1SVEo0IvSoPyqqA7gm/view?usp=sharing

Any chance you could upload it again?

Thank you for sharing your work!",Security & Safety,Security & Safety,FIXED
13379,Pubmed ID additions,"`Horifffuchi M, Offfkita K. Blood flow resgggtricted exffercise anffd vasggcular funcggtion. Int J Vasc Med 2012;2012. [PMID:23133756].`

Nowadays bibliography has PMID (Pubmed) as i mentioned above in a sample,
Is it possible to add support for that",Other,Other,
25835,Consistency in Exceptions: ArgumentError vs DimensionMismatch,"`kernelmatrix!` for StandardKernel throws ArgumentError, `kernelmatrix!` for SquaredDistanceKernel or ScalarProductKernel throws DimensionMismatch (by `scprodmatrix!`/`sqdistmatrix!`). `matrix_prod!` and `matrix_sum!` for generic Arrays call error(), but for matrices with is_upper argument the former throws DimensionMismatch and the latter throws ArgumentError. I think this should all be made a bit more consistent.

Is there a specific reason to use error() rather than throw(<some exception>) in some cases ?

(NB. there should be `@test_throw`s as well, which I'm about to add.)",Other,Other,
32287,Problem about orientation_error of validation set is extremely high compare to train set.,"Hi!
I used my own dataset which contains four objects to train the pose_interpreter_network and I found the orientation_error of val dosen't decline well when error of train set converged as the chart below.

![ç«çæªå¾_2019-05-07T11-28-27 356Z](https://user-images.githubusercontent.com/38148405/57296727-44d10b00-7100-11e9-9665-a0cd94afaaa1.png)

Consequently, the result of end_to_end_eval is unsatisfactory as well. Despite the positions of object are predicted correctly, most the orientations are wrong.
Could you give me any suggestion?
Thanks in advance.",Other,Other,
32669,"issue with the ""find"" command","Some issue with find. Maybe I'm missing something obvious.

$ anystyle find booklet.txt
Error processing `BrochureBCSDL.tex'
 undefined method `split' for nil:NilClass
 /Library/Ruby/Gems/2.6.0/gems/anystyle-1.3.12/lib/anystyle/document.rb:11:in `parse'
 /Library/Ruby/Gems/2.6.0/gems/anystyle-1.3.12/lib/anystyle/document.rb:40:in `open'
 ...",Other,Other,
18237,steps to train model again with another DRN,"Hello, 
I want to train another drn model on the same dataset. Please guide me through steps for end to end evaluation. Also, I can see DRNSeg being called during training. But where is drn22 being called by DRNSeg ?",Other,Other,
7269,Added python 3.x support,"Hey @AnthonyMRios,

Can you please check this out? This pull request should be able to fix your issue #6 .

Best,
J",Other,Other,
12433,Wrong result for find command,"In this PDF https://iase-web.org/documents/papers/icots5/Topic1m.pdf

![image](https://user-images.githubusercontent.com/3487994/172229984-445ac97f-4d18-4d72-8dd3-2c5505f5592e.png)",Other,Other,
44488,Size mismatch RuntimeError on training with FINE_SIZE more than 256.,"Using LOAD_SIZE=640, FINE_SIZE=256 for training works fine. However, using LOAD_SIZE=640, FINE_SIZE=512 or LOAD_SIZE=512, FINE_SIZE=512 throws a RunTimeError shown below:
<pre>Traceback (most recent call last):
 File &quot;./train.py&quot;, line 34, in &lt;module&gt;
 model.optimize_parameters()
 File &quot;bicyclegan/BicycleGAN/models/bicycle_gan_model.py&quot;, line 209, in optimize_parameters
 self.forward()
 File &quot;bicyclegan/BicycleGAN/models/bicycle_gan_model.py&quot;, line 106, in forward
 self.z_encoded, self.mu, self.logvar = self.encode(self.real_B_encoded)
 File &quot;bicyclegan/BicycleGAN/models/bicycle_gan_model.py&quot;, line 83, in encode
 mu, logvar = self.netE.forward(input_image)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py&quot;, line 121, in forward
 return self.module(*inputs[0], **kwargs[0])
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 477, in __call__
 result = self.forward(*input, **kwargs)
 File &quot;bicyclegan/BicycleGAN/models/networks.py&quot;, line 602, in forward
 output = self.fc(conv_flat)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 477, in __call__
 result = self.forward(*input, **kwargs)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py&quot;, line 91, in forward
 input = module(input)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 477, in __call__
 result = self.forward(*input, **kwargs)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py&quot;, line 55, in forward
 return F.linear(input, self.weight, self.bias)
 File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py&quot;, line 1024, in linear
 return torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [1 x 1024], m2: [256 x 8] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249
</pre>

Could you please help me find out how can I train the model for FINE_SIZE values of 512 or more ?
Thanks!",Other,Other,
23422,Bump werkzeug from 0.10.4 to 2.2.3 in /docker/corenlp/app,"Bumps [werkzeug](https://github.com/pallets/werkzeug) from 0.10.4 to 2.2.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/werkzeug/releases"">werkzeug's releases</a>.</em></p>
<blockquote>
<h2>2.2.3</h2>
<p>This is a fix release for the 2.2.x release branch.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-3"">https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-3</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/26?closed=1"">https://github.com/pallets/werkzeug/milestone/26?closed=1</a></li>
</ul>
<p>This release contains security fixes for:</p>
<ul>
<li><a href=""https://github.com/pallets/werkzeug/security/advisories/GHSA-xg9f-g7g7-2323"">https://github.com/pallets/werkzeug/security/advisories/GHSA-xg9f-g7g7-2323</a></li>
<li><a href=""https://github.com/pallets/werkzeug/security/advisories/GHSA-px8h-6qxv-m22q"">https://github.com/pallets/werkzeug/security/advisories/GHSA-px8h-6qxv-m22q</a></li>
</ul>
<h2>2.2.2</h2>
<p>This is a fix release for the <a href=""https://github.com/pallets/werkzeug/releases/tag/2.2.0"">2.2.0</a> feature release.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-2"">https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-2</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/25?closed=1"">https://github.com/pallets/werkzeug/milestone/25?closed=1</a></li>
</ul>
<h2>2.2.1</h2>
<p>This is a fix release for the <a href=""https://github.com/pallets/werkzeug/releases/tag/2.2.0"">2.2.0</a> feature release.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-1"">https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-1</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/24?closed=1"">https://github.com/pallets/werkzeug/milestone/24?closed=1</a></li>
</ul>
<h2>2.2.0</h2>
<p>This is a feature release, which includes new features and removes previously deprecated features. The 2.2.x branch is now the supported bugfix branch, the 2.1.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-0"">https://werkzeug.palletsprojects.com/en/2.2.x/changes/#version-2-2-0</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/20?closed=1"">https://github.com/pallets/werkzeug/milestone/20?closed=1</a></li>
</ul>
<h2>2.1.2</h2>
<p>This is a fix release for the <a href=""https://github.com/pallets/werkzeug/releases/tag/2.1.0"">2.1.0</a> feature release.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-2"">https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-2</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/22?closed=1"">https://github.com/pallets/werkzeug/milestone/22?closed=1</a></li>
</ul>
<h2>2.1.1</h2>
<p>This is a fix release for the <a href=""https://github.com/pallets/werkzeug/releases/tag/2.1.0"">2.1.0</a> feature release.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-1"">https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-1</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/19?closed=1"">https://github.com/pallets/werkzeug/milestone/19?closed=1</a></li>
</ul>
<h2>2.1.0</h2>
<p>This is a feature release, which includes new features and removes previously deprecated features. The 2.1.x branch is now the supported bugfix branch, the 2.0.x branch will become a tag marking the end of support for that branch. We encourage everyone to upgrade, and to use a tool such as <a href=""https://pypi.org/project/pip-tools/"">pip-tools</a> to pin all dependencies and control upgrades.</p>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0"">https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/16?closed=1"">https://github.com/pallets/werkzeug/milestone/16?closed=1</a></li>
</ul>
<h2>2.0.3</h2>
<ul>
<li>Changes: <a href=""https://werkzeug.palletsprojects.com/en/2.0.x/changes/#version-2-0-3"">https://werkzeug.palletsprojects.com/en/2.0.x/changes/#version-2-0-3</a></li>
<li>Milestone: <a href=""https://github.com/pallets/werkzeug/milestone/18?closed=1"">https://github.com/pallets/werkzeug/milestone/18?closed=1</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pallets/werkzeug/blob/main/CHANGES.rst"">werkzeug's changelog</a>.</em></p>
<blockquote>
<h2>Version 2.2.3</h2>
<p>Released 2023-02-14</p>
<ul>
<li>Ensure that URL rules using path converters will redirect with strict slashes when
the trailing slash is missing. :issue:<code>2533</code></li>
<li>Type signature for <code>get_json</code> specifies that return type is not optional when
<code>silent=False</code>. :issue:<code>2508</code></li>
<li><code>parse_content_range_header</code> returns <code>None</code> for a value like <code>bytes */-1</code>
where the length is invalid, instead of raising an <code>AssertionError</code>. :issue:<code>2531</code></li>
<li>Address remaining <code>ResourceWarning</code> related to the socket used by <code>run_simple</code>.
Remove <code>prepare_socket</code>, which now happens when creating the server. :issue:<code>2421</code></li>
<li>Update pre-existing headers for <code>multipart/form-data</code> requests with the test
client. :issue:<code>2549</code></li>
<li>Fix handling of header extended parameters such that they are no longer quoted.
:issue:<code>2529</code></li>
<li><code>LimitedStream.read</code> works correctly when wrapping a stream that may not return
the requested size in one <code>read</code> call. :issue:<code>2558</code></li>
<li>A cookie header that starts with <code>=</code> is treated as an empty key and discarded,
rather than stripping the leading <code>==</code>.</li>
<li>Specify a maximum number of multipart parts, default 1000, after which a
<code>RequestEntityTooLarge</code> exception is raised on parsing. This mitigates a DoS
attack where a larger number of form/file parts would result in disproportionate
resource use.</li>
</ul>
<h2>Version 2.2.2</h2>
<p>Released 2022-08-08</p>
<ul>
<li>Fix router to restore the 2.1 <code>strict_slashes == False</code> behaviour
whereby leaf-requests match branch rules and vice
versa. :pr:<code>2489</code></li>
<li>Fix router to identify invalid rules rather than hang parsing them,
and to correctly parse <code>/</code> within converter arguments. :pr:<code>2489</code></li>
<li>Update subpackage imports in :mod:<code>werkzeug.routing</code> to use the
<code>import as</code> syntax for explicitly re-exporting public attributes.
:pr:<code>2493</code></li>
<li>Parsing of some invalid header characters is more robust. :pr:<code>2494</code></li>
<li>When starting the development server, a warning not to use it in a
production deployment is always shown. :issue:<code>2480</code></li>
<li><code>LocalProxy.__wrapped__</code> is always set to the wrapped object when
the proxy is unbound, fixing an issue in doctest that would cause it
to fail. :issue:<code>2485</code></li>
<li>Address one <code>ResourceWarning</code> related to the socket used by
<code>run_simple</code>. :issue:<code>2421</code></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pallets/werkzeug/commit/22a254fca2ad0130adbbcbd11d3de51bcb04a08b""><code>22a254f</code></a> release version 2.2.3</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/517cac5a804e8c4dc4ed038bb20dacd038e7a9f1""><code>517cac5</code></a> Merge pull request from GHSA-xg9f-g7g7-2323</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/babc8d9e8c9fa995ef26050698bc9b5a92803664""><code>babc8d9</code></a> rewrite docs about request data limits</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/09449ee77934a0c883f5959785864ecae6aaa2c9""><code>09449ee</code></a> clean up docs</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/fe899d0cdf767a7289a8bf746b7f72c2907a1b4b""><code>fe899d0</code></a> limit the maximum number of multipart form parts</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/cf275f42acad1b5950c50ffe8ef58fe62cdce028""><code>cf275f4</code></a> Merge pull request from GHSA-px8h-6qxv-m22q</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/8c2b4b82d0cade0d37e6a88e2cd2413878e8ebd4""><code>8c2b4b8</code></a> don't strip leading = when parsing cookie</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/7c7ce5cb73f3f7d3b9c09340e4f322aeb583dbc5""><code>7c7ce5c</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2585"">#2585</a>)</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/19ae03e6a39b3f63fd08fef4fddae4385cdddf25""><code>19ae03e</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>
<li><a href=""https://github.com/pallets/werkzeug/commit/a83d3b8bf070810874c8e8d03dcce270666e10fe""><code>a83d3b8</code></a> [pre-commit.ci] pre-commit autoupdate</li>
<li>Additional commits viewable in <a href=""https://github.com/pallets/werkzeug/compare/0.10.4...2.2.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=werkzeug&package-manager=pip&previous-version=0.10.4&new-version=2.2.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/stanfordnlp/chirpycardinal/network/alerts).

</details>",Other,Other,
44475,The number of training dataset,"Hello, I have a question. 
How many images in the training dataset can be used to train an effective model?
For example, domain X has 6000 undistorted images, domain Y has only 145 images with five distortion types. The scenes of Y are included in X . In this case, could generate images with five distortion types, which is similar to Y domain?",Fairness,Fairness,FIXED
30378,Add unit tests for com.hankcs.hanlp.utility.MathUtilityTest,"## æ³¨æäºé¡¹

* è¿æ¬¡ä¿®æ¹æ²¡æå¼å¥ç¬¬ä¸æ¹ç±»åºã
* ä¹æ²¡æä¿®æ¹JDKçæ¬å·
* ææææ¬é½æ¯UTF-8ç¼ç 
* ä»£ç é£æ ¼ä¸è´
* [x] æå¨æ­¤æ¬å·åè¾å¥xæé©ï¼ä»£è¡¨ä¸è¿°äºé¡¹ç¡®è®¤å®æ¯

Hi,

I've analysed your code base and noticed that `com.hankcs.hanlp.utility.MathUtility` in the `hanlp` module is not fully tested.

I've written some tests that cover this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests should help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other particular classes that you consider important.",Other,Other,
26843,ScikitLearn.jl - JLD.jl segmentation fault error,"Hi, 

I am using Julia v. 1.6.3 and I have a problem using JLD to save a RandomForestClassifier() model, trained with ScikitLearn. Namely, when the number of features and labels are too large, I get a segmentation fault error. 

Here a working example to reproduce the error: 
`
using ScikitLearn
using ScikitLearn.Pipelines
using PyCall, JLD, PyCallJLD
using Random
@sk_import ensemble: (RandomForestClassifier)

#working example with 100 features and 100 labels
x_vals=rand(100,45)
y_vals=vec(rand([0,1],100,1))

clf_model=RandomForestClassifier(n_estimators=500,bootstrap=true,oob_score=true,n_jobs=-1,class_weight=""balanced_subsample"",)
fit!(clf_model,x_vals,y_vals)
oob_score_value = clf_model.oob_score_
println(""Oob score: $oob_score_value"")

JLD.save(""clf_model_100.jld"", ""clf_model"", clf_model)

#NOT working example with 10,000 features and 100,000 labels
x_vals=rand(10000,45)
y_vals=vec(rand([0,1],10000,1))

clf_model=RandomForestClassifier(n_estimators=500,bootstrap=true,oob_score=true,n_jobs=-1,class_weight=""balanced_subsample"",)
fit!(clf_model,x_vals,y_vals)
oob_score_value = clf_model.oob_score_
println(""Oob score: $oob_score_value"")

JLD.save(""clf_model_10000.jld"", ""clf_model"", clf_model)
`
Here the error: 
> signal (11): Segmentation fault: 11
in expression starting at /Users/admin/Desktop/Online2/Train_ML_new.jl:998
jl_exit_thread0_cb at /Applications/Julia-1.6.app/Contents/Resources/julia/lib/julia/libjulia-internal.1.dylib (unknown line)
Allocations: 86747943 (Pool: 86717037; Big: 30906); GC: 83

Could anyone help understanding what is going on? 

UPDATE: I have downgraded Julia to v. 1.0.5 and this has solved the segmentation fault for the working example, although I get the following warning: 

> â Warning: JLD incorrectly extends FileIO functions (see FileIO documentation)
â @ FileIO ~/.julia/packages/FileIO/DNKwN/src/loadsave.jl:217",Other,Other,
6341,Access OOB data and OOB error calculations of Random Forest,"Hi
Can you add access to the Out-of-Bag data and/or Out-of-Data error calculations for Random Forests?

Love this project,
Thanks",Privacy,Other,
9733,chore(deps): [security] bump ini from 1.3.5 to 1.3.8,"Bumps [ini](https://github.com/isaacs/ini) from 1.3.5 to 1.3.8. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-qqgx-2p2h-9c37"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution</strong></p>
<h3>Overview</h3>
<p>The <code>ini</code> npm package before version 1.3.6 has a Prototype Pollution vulnerability.</p>
<p>If an attacker submits a malicious INI file to an application that parses it with <code>ini.parse</code>, they will pollute the prototype on the application. This can be exploited further depending on the context.</p>
<h3>Patches</h3>
<p>This has been patched in 1.3.6</p>
<h3>Steps to reproduce</h3>
<p>payload.ini</p>
<pre><code>[__proto__]
polluted = &quot;polluted&quot;
</code></pre>
<p>poc.js:</p>
<pre><code>var fs = require('fs')
&lt;/tr&gt;&lt;/table&gt; ... (truncated)
<p>Affected versions: &lt; 1.3.6
</code></pre></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/npm/ini/commit/a2c5da86604bc2238fe393c5ff083bf23a9910eb""><code>a2c5da8</code></a> 1.3.8</li>
<li><a href=""https://github.com/npm/ini/commit/af5c6bb5dca6f0248c153aa87e25bddfc515ff6e""><code>af5c6bb</code></a> Do not use Object.create(null)</li>
<li><a href=""https://github.com/npm/ini/commit/8b648a1ac49e1b3b7686ea957e0b95e544bc6ec1""><code>8b648a1</code></a> don't test where our devdeps don't even work</li>
<li><a href=""https://github.com/npm/ini/commit/c74c8af35f32b801a7e82a8309eab792a95932f6""><code>c74c8af</code></a> 1.3.7</li>
<li><a href=""https://github.com/npm/ini/commit/024b8b55ac1c980c6225607b007714c54eb501ba""><code>024b8b5</code></a> update deps, add linting</li>
<li><a href=""https://github.com/npm/ini/commit/032fbaf5f0b98fce70c8cc380e0d05177a9c9073""><code>032fbaf</code></a> Use Object.create(null) to avoid default object property hazards</li>
<li><a href=""https://github.com/npm/ini/commit/2da90391ef70db41d10f013e3a87f9a8c5d01a72""><code>2da9039</code></a> 1.3.6</li>
<li><a href=""https://github.com/npm/ini/commit/cfea636f534b5ca7550d2c28b7d1a95d936d56c6""><code>cfea636</code></a> better git push script, before publish instead of after</li>
<li><a href=""https://github.com/npm/ini/commit/56d2805e07ccd94e2ba0984ac9240ff02d44b6f1""><code>56d2805</code></a> do not allow invalid hazardous string as section name</li>
<li>See full diff in <a href=""https://github.com/isaacs/ini/compare/v1.3.5...v1.3.8"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~isaacs"">isaacs</a>, a new releaser for ini since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)](https://dependabot.com/compatibility-score/?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",Other,Other,
5924,How to avoid using Conda when using @sk_import,"Hi, 

I'm using HPC clusters that are incompatible with internet so I'd like to avoid using Conda when using @sk_import in ScikitLearn.jl. However, all my trials have been unsuccessful so far. 

For example, when I use ScikitLearn, e.g.: 
```
 using ScikitLearn
 @sk_import decomposition: PCA
```
I get: 

> [ Info: Running `conda install -y -c conda-forge llvm-openmp` in root environment
> Collecting package metadata (current_repodata.json): done
> Solving environment: done
> All requested packages already installed.
> Retrieving notices: ...working... done

Could anyone help me understanding what is going on? 
Thanks!",Other,Other,
5359,"Model data got replaced in training even after setting ""false"" as second parameter","@inukshuk Impressive work by the tool. I've played around. 
As, i'm new to ruby i can't able to debug. Can you help me on the issue. Please view below for the issue recorded.

![output](https://user-images.githubusercontent.com/10484307/26925714-7cde988c-4c68-11e7-8c03-5bb4febb8207.gif)",Other,Other,FIXED
10646,Explicitly specify the encoding while reading the readme file.,"Since the readme file contains a Unicode character, on some systems such as docker container when the terminal encoding is not proper, the setup fails with the following error:

```
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 300: ordinal not in range(128)
```

The error is generated while reading the readme file in setup.py

This pull request will resolve this issue. The proposed solution is tested for Python 2.7 and Python 3.5.",Other,Other,
22602,Bump numpy from 1.16.4 to 1.22.0,"Bumps [numpy](https://github.com/numpy/numpy) from 1.16.4 to 1.22.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>
<blockquote>
<h2>v1.22.0</h2>
<h1>NumPy 1.22.0 Release Notes</h1>
<p>NumPy 1.22.0 is a big release featuring the work of 153 contributors
spread over 609 pull requests. There have been many improvements,
highlights are:</p>
<ul>
<li>Annotations of the main namespace are essentially complete. Upstream
is a moving target, so there will likely be further improvements,
but the major work is done. This is probably the most user visible
enhancement in this release.</li>
<li>A preliminary version of the proposed Array-API is provided. This is
a step in creating a standard collection of functions that can be
used across application such as CuPy and JAX.</li>
<li>NumPy now has a DLPack backend. DLPack provides a common interchange
format for array (tensor) data.</li>
<li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The
new methods provide a complete set of the methods commonly found in
the literature.</li>
<li>A new configurable allocator for use by downstream projects.</li>
</ul>
<p>These are in addition to the ongoing work to provide SIMD support for
commonly used functions, improvements to F2PY, and better documentation.</p>
<p>The Python versions supported in this release are 3.8-3.10, Python 3.7
has been dropped. Note that 32 bit wheels are only provided for Python
3.8 and 3.9 on Windows, all other wheels are 64 bits on account of
Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.
All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix
the occasional problems encountered by folks using truly huge arrays.</p>
<h2>Expired deprecations</h2>
<h3>Deprecated numeric style dtype strings have been removed</h3>
<p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,
and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>
<h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>
<p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that
users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both
deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with
the appropriate value for the <code>usemask</code> parameter.</p>
<p>(<a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>
<li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>
<li><a href=""https://github.com/numpy/numpy/commit/125304b035effcd82e366e601b102e7347eaa9ba""><code>125304b</code></a> wip</li>
<li><a href=""https://github.com/numpy/numpy/commit/c283859128b1a4b57014581570a23ed7950a24ea""><code>c283859</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20682"">#20682</a> from charris/backport-20416</li>
<li><a href=""https://github.com/numpy/numpy/commit/5399c03d4a069fe81a1616be0184c9749d7271ee""><code>5399c03</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20681"">#20681</a> from charris/backport-20954</li>
<li><a href=""https://github.com/numpy/numpy/commit/f9c45f8ebf31340b1a5a0371bfca25afcfc4794e""><code>f9c45f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/numpy/numpy/issues/20680"">#20680</a> from charris/backport-20663</li>
<li><a href=""https://github.com/numpy/numpy/commit/794b36f7e1bf2a8c42774ab0db86a74bd32f674b""><code>794b36f</code></a> Update armccompiler.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/d93b14e3d7abaa1d837825e51671f817788e120f""><code>d93b14e</code></a> Update test_public_api.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/7662c0789cc6a70d5ad4d950ee2e95f3afef7df6""><code>7662c07</code></a> Update <strong>init</strong>.py</li>
<li><a href=""https://github.com/numpy/numpy/commit/311ab52488a7d096ac3bc4c2de0fdae17ecd13ef""><code>311ab52</code></a> Update armccompiler.py</li>
<li>Additional commits viewable in <a href=""https://github.com/numpy/numpy/compare/v1.16.4...v1.22.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=numpy&package-manager=pip&previous-version=1.16.4&new-version=1.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
33612,Access OOB data and OOB error calculations of Random Forest,"Hi
Can you add access to the Out-of-Bag data and/or Out-of-Data error calculations for Random Forests?

Love this project,
Thanks",Privacy,Privacy,FIXED
18909,Memory usage increases with epochs,"Hi junyanz,

Thanks for releasing the code, very cool work. Awesome!

I am having a memory issue, or perhaps it is normal that you know of. With each epoch, GPU memory usage seems to increase. After a while, the limit (12GB) is reached and the learning terminates. 

Is there a way to circumvent this? I currently just reduce the number of filters, but it is hard to estimate how many epoch I can run until it crashes.

Best,
Ruud",Other,Other,
25177,Android: No longer runs on Android 4.0.4 (LG Lucid),"Automatic update of the app occurred today; now the app no longer runs, and instead has immediate failure (""Unfortunately, NewsBlur has stopped."")",Other,Other,
26551,Update sbt to 1.9.4,"## About this PR
ð¦ Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from `1.9.3` to `1.9.4`

ð [GitHub Release Notes](https://github.com/sbt/sbt/releases/tag/v1.9.4) - [Version Diff](https://github.com/sbt/sbt/compare/v1.9.3...v1.9.4)

## Usage
â **Please merge!**

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/fd2970bb455e62fab7a419c29a354de3ae727b4d/docs/repo-specific-configuration.md) file.

_Have a fantastic day writing Scala!_

<details>
<summary>â Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.scala-sbt"", artifactId = ""sbt"" }
}]
```
</details>

<sup>
labels: library-update, early-semver-patch, semver-spec-patch, version-scheme:early-semver, commit-count:1
</sup>",Other,Other,
660,Handling latent observations in IOHMM,"Hi,

I need to modify your code to be able to handle missing observations controlled by a binary vector R that controls missed or provided observations by considering probability 1(log probability 0 ) for missed observations such as suggested solution on 
[yeh2012 (1).pdf](https://github.com/Mogeng/IOHMM/files/4538047/yeh2012.1.pdf)

can you guide me on this?
thnx",Other,Other,
13644,train.lua crashes with batchSize bigger than 1,"[...]/torch/install/bin/luajit: [...]/torch/install/share/lua/5.1/image/init.lua:176: [write_png_file] Depth must be 1, 3 or 4
stack traceback: 
 [C]: in function 'save'
 /data/software/torch/install/share/lua/5.1/image/init.lua:176: in function 'saver' 
 /data/software/torch/install/share/lua/5.1/image/init.lua:442: in function 'save' 
 ./util/visualizer.lua:50: in function 'save_results' 
 train.lua:96: in function 'save_current_results' 
 train.lua:154: in main chunk 
 [C]: in function 'dofile' 
 [...]/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk 
 [C]: at 0x00405d50",Other,Other,
28879,Android: Loading indicators on reading overlays do not dissappear on scroll,"I happened to scroll while text view was loading and noticed that the loading indicator did not disappear along with the overlay.

I think I may have also noticed this with the loading indicator on the next overlay in the past too but I'm not sure.",Other,Other,
4251,get camera information in viewer?,Is there any way to get the pos and quaternion of the free camera in the viewer? Only the statically defined camera can be seen in physics.model.cam_pos,Other,Other,
27029,Error while loading tracks.csv,"Hello,

I am getting an error while trying to load tracks.csv. 

![image](https://user-images.githubusercontent.com/13437655/84540297-5e616080-accb-11ea-98ed-1306974fec80.png)

Any ideas?",Other,Other,
40285,Update sbt to 1.3.1,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) [from 1.3.0 to 1.3.1](https://github.com/sbt/sbt/compare/v1.3.0...v1.3.1).
[Release Notes/Changelog](https://github.com/sbt/sbt/releases/tag/v1.3.1)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: semver-patch",Other,Other,
24300,iOS: Swiping to next story sends you back to story list,"I've seen this personally, but this came from TestFlight feedback:

If you are looking at the second story in the feed on an iPhone, the swipe to the right to go to the first story, it instead goes back to the story list.",Other,Other,
24873,Added a workaround for Linux webkit browsers,"This is the issue referred to from https://getsatisfaction.com/newsblur/topics/does-not-open-links-in-new-tabs-in-various-webkit-browsers

Essentially, WebKit-based browsers on Linux can't handle the way NewsBlur attempts to open tabs in the background. This workaround detects those browsers and disables backgrounding for them.",Other,Other,
21573,How to generate words beyond the vocabulary ?,"As the paper says in the end of Section 2.2 State Generator
```
Note that due to Eq (2), our model is able to generate words even if they are not pre-defined in the vocabulary.
```
but as the [code](https://github.com/jasonwu0731/trade-dst/blob/master/models/TRADE.py#L453) shows, `p_context_ptr = torch.zeros(p_vocab.size())`, does that mean it **restrict** the words must be in pre-defined vocabulary ? 

For example, the vocabulary size is 100, but we have a word id in `story` is 103.

Thanks a lot",Other,Other,
7439,Add ParameterType to optimization ParameterBound,"This pull request adds a `ParameterType` to the `ParameterBound` class in `SharpLearning.Optimization`. The parameter type specifies if the parameter is discrete or continous. This enables the parameter samplers used in the optimizers to sample from either a continous range or from a discrete range. Before this, all samplers would sample from a continous range.

For instance, during hyperparameter optimization of a gradient boost model, it makes more sense to sample the number of trees on a discrete range, and the learning rate on a continous range. If the number of trees is sampled on a continous range, the optimizer will try many continous values which will be cast to the same integer values in the learner. For instance 15.2325, 15.9343, and so on, will all be cast to 15. Using the discrete range will make the search more efficient.

The parameter type is specified on the `ParameterBound` class, together with the other options:

```csharp
var parameters = new ParameterBounds[]
{
 new ParameterBounds(min: 10, max: 1000, transform: Transform.Linear, 
 parameterType: ParameterType.Discrete), // iterations
 
 new ParameterBounds(min: 0.001, max: 0.2, transform: Transform.Logarithmic, 
 parameterType: ParameterType.Continuous), // learning rate
 
 new ParameterBounds(min: 1, max: 25, transform: Transform.Linear, 
 parameterType: ParameterType.Discrete), // maximumTreeDepth
 
 new ParameterBounds(min: 0.5, max: 1.0, transform: Transform.Linear, 
 parameterType: ParameterType.Continuous), // subSampleRatio
};
```

The default value of the `ParameterType` on `ParameterBounds` is still `Continous`, so the default behavior of the optimizers will not change.",Other,Other,FIXED
4488,Approach to derivatives,"Types can be parameterized by symbols, which means we can take a different approach to derivatives. I've included a [gist](https://gist.github.com/trthatcher/acd86f5a9b911737d419) to illustrate how we could take advantage of this (""Approach 2"") 

It would cut down on the number of functions and it seems to perform better on my machine (and definitely no worse).",Other,Other,
16086,"0.30.1.0: Add HyperbandOptimizer, and update test adapters","This pull request adds the `HyperbandOptimizer` to `SharpLearning.Optimization`. The implementation is based on the original article [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) and the implementation by [fastml](http://fastml.com/tuning-hyperparams-fast-with-hyperband/).

Compared to the other optimizers from SharpLearning, Hyperband includes an extra parameter in the objective function, `unitsOfCompute`. Hyperband uses the `unitsOfCompute` parameter to control a budget of compute for each set of hyperparameters. Initially it will run each parameter set with very little compute budget to get a taste of how they perform. Then it takes the best performers and runs them on a larger budget.

The `unitOfCompute` parameter is used in the objective function, and could for instance be used to control the size of the training set, the number of trees in gradient boost, or the number of epochs for neural nets. One unit of compute could for instance be defined as 1000 samples in the training set. The `maximumUnitsOfCompute` is provided as an argument to the `HyperbandOptimizer`, and the optimzer will define a schedule for evaluating the hyperparameters on a budget.

A small experiment comparing the results and runtime of the `HyperbandOptimizer` vs. the `RandomSearchOptimizer`, optimizing a neural net (using CNTK) on the CIFAR-10 dataset:

**System**
**CPU: i7-4770**
**GPU: GTX1070**

The Hyperband optimizers uses the default parameters except for the `skipLastIterationOfEachRound` which is enabled for one of the runs. The default parameters result in a total of 209 different parameter sets tried with the Hyberband optimizers. The `unitsOfCompute` parameter in the objective function is used to control the size of the training set, where 1 unit of compute is set to 740 samples, which corresponds to the full training set size of 60.000 samples, when `maximumUnitsOfCompute` is set to 81, which is the default maximum. The full test set is used to track the test loss/accuracy in all rounds/iterations.

| Optimizer | Time (hours) | Test accuracy (%) |
| ------------- |:-------------:| -----:|
| `RandomSearchOptimizer(iterations=100)`| 45.34 | 88.47 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=false)` | 10.46 | 87.93 |
| `HyperbandOptimizer(skipLastIterationOfEachRound=true)`| 6.56 | 87.93 |

As can be seen the RandomSearchOptimizer finds a slightly better parameter set. However, the two Hyperband runs uses significant less time to find a solution that is nearly as good. In this case, skipping the last iteration of each round results in finding the same parameter set as when including all iterations. This will not be the case for all problem types, but it does provide a nice speed up for large problems.

In the end, Hyperband finds a solution that is almost as good, and reduces the time required by a factor of 4-7.

A future extension to the hyperband optimizer could be to use bayesian optimization instead of random search to select the parameter sets. This combination has proven very useful in: [Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/pdf/1807.01774.pdf).",Other,Other,
31727,How to know the physical meaning of each dimension of state space?,"I would like to write a wrapper for dm control environment and zero out some specific dimensions of the state. But I don't know the detailed physical meaning (i.e., position / velocity) of each individual dimension for each environment. I wonder where could I find these information?",Other,Other,FIXED
910,Update sbt-scoverage to 2.0.7,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 2.0.6 to 2.0.7.
[GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.7) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.6...v2.0.7)


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/71a67d63f74149774a2a6e021dfb354771078b87/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Adjust future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]
```
Or, add this to slow down future updates of this dependency:
```
dependencyOverrides = [{
 pullRequests = { frequency = ""30 days"" },
 dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }
}]
```
</details>

labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1",Other,Other,
31677,Fixed pip install git URL,"Original URL with `git+git` fails as shown
```
> pip install git+git://github.com/deepmind/dm_control.git
Collecting git+git://github.com/deepmind/dm_control.git
 Cloning git://github.com/deepmind/dm_control.git to /tmp/pip-req-build-p9lzhp7w
 Running command git clone --filter=blob:none --quiet git://github.com/deepmind/dm_control.git /tmp/pip-req-build-p9lzhp7w
 fatal: unable to connect to github.com:
 github.com[0: 140.82.113.4]: errno=Connection timed out
```

Successfully installs with `git+https`
```
> pip install git+https://github.com/deepmind/dm_control.git
Collecting git+https://github.com/deepmind/dm_control.git
 Cloning https://github.com/deepmind/dm_control.git to /tmp/pip-req-build-3qzfvyvw
 Running command git clone --filter=blob:none --quiet https://github.com/deepmind/dm_control.git /tmp/pip-req-build-3qzfvyvw
 Resolved https://github.com/deepmind/dm_control.git to commit 5dec8898a4539589d4989f6553673e588e6a6b06
 Installing build dependencies ... done
 ...
```",Other,Other,
34967,Unable to download fma_full.zip,"## Overview

The download of `fma_full.zip` stops before completion.

## Steps to Reproduce

```bash
# From the README
curl -O https://os.unil.cloud.switch.ch/fma/fma_full.zip
```

which, after a while, gives something like this:

```bash
transfer closed with n bytes remaining to read
```

Any advice/help would be greatly appreciated :)",Other,Other,
861,Variance in resulting embeddings,"Hello!
I tried to calculate your embeddings for my own dataset of 300 graphs. The features are nide degrees as suggested in the paper, I only modified the number of epochs making it 40. 
Let's say I now have embeddings 1, 2 and 3 of the same set of graphs (so graph2vec applied 3 times on the same data).
Then I tried to calculate map@5 for retrieval of the most similar embeddings between the embeddings 1 and 3, 1 and 2, 3 and 2. And I got really bad results, precisely map@5: 0.249222, 0.260222, 0.163278.
I got similar negative results using the dataset provided and default parameters. 
Please, can you comment on that or let me know what I might be doing wrong? 
Thank you!",Other,Other,FIXED
14707,Update google-api-services-bigquery to v2-rev20200827-1.30.10,"Updates com.google.apis:google-api-services-bigquery from v2-rev20200818-1.30.10 to v2-rev20200827-1.30.10.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/a7d4a20023a46ac28c61ae0f151a7f234d737251/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
</details>

labels: library-update",Other,Other,
38444,How to proccess a raw string input?,"Nice job dude. I want to proccess a raw string input, not from pdf or txt file. And I find some related code from AnyStyle and wapiti-ruby source code, but it didn`t work. Can u give me an example?
```ruby
require 'anystyle'

# `str` is the full text of pdf
str = ""xxx""

finder = AnyStyle.finder

dataset = Wapiti::Dataset.prepare(str)

output = finder.model.label dataset

result = Wapiti::Dataset.new(dataset.map.with_index { |doc, idx|
 doc.label(output[idx])
})

print result[0].references
```

it case some bugs like:
```
Traceback (most recent call last):
 2: from parser_string.rb:1102:in `<main>'
 1: from /Library/Ruby/Gems/2.6.0/gems/wapiti-1.0.7/lib/wapiti/model.rb:44:in `label'
/Library/Ruby/Gems/2.6.0/gems/wapiti-1.0.7/lib/wapiti/model.rb:44:in `label': missing tokens at 1: 11/8, cannot apply pattern (Wapiti::NativeError)
```",Other,Other,
273,Train a LM on wiki + weibo + qa + news + danmaku + reviews + ...,"What makes HanLP different than the majority of OSS projects?
One of the most important factors would be the large scale professional corpora, and the correct way to make use of them.
To have some unique pretrained LM before releasing the beta version would be a cool idea. Don't you think so?",Other,Other,
27800,Excellent Work,Thanks for x boost GPU learners .,Other,Other,
3376,GIMP Plugin for StarNet - gimp-plugin-starnet,"Hi @nekitmm 

I just wanted to report that I've found a nice/simple plugin to integrate StarNet with GIMP (have the starless automatically imported as layer). I think it's totally worth the mention in your website.

You may find it here:
https://github.com/pigeond/gimp-plugin-starnet

PS: I didn't know about this repo... is this the source for the binaries in the website (I don't see releases here, hence my question)? If it's all Python, then the plugin could actually be importing the code - one thing that could definitely improved is the progress completion feedback to GIMP....",Other,Other,
2057,BaseNodeç¼è¯éè¯¯,"åç°BaseNodeçwalkToLoadæ¹æ³æç¼è¯éè¯¯ã
éè¯¯ä¿¡æ¯ï¼

> The method walkToLoad(ByteArray, BaseNode.ValueArray) in the type BaseNode is not applicable for the arguments (ByteArray, BaseNode<V>.ValueArray)",Other,Other,
30066,Errors related to missing library,"After installing anystyle-parser, and loading it in an irb session as follows gives

<pre>
irb(main):002:0> require 'anystyle/parser'
LoadError: cannot load such file -- language_detector
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:27:in `<class:Parser>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:4:in `<module:Parser>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:2:in `<module:Anystyle>'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser/parser.rb:1:in `<top (required)>'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
 from /usr/local/lib/ruby/gems/1.9.1/gems/anystyle-parser-0.7.1/lib/anystyle/parser.rb:14:in `<top (required)>'
</pre>


Although the library is loaded all right, the parse method cannot be found

<pre>
irb(main):007:0> Anystyle.parse 
NoMethodError: undefined method `parse' for Anystyle:Module

</pre>


saji",Other,Other,
44127,Resource error: insufficient memory,"I have the problem of insufficient memory. The sever memory is 256 GB, so ideally, the size is enough. I added `-Xmx16g -Xms2g` to the metamap server scripts to start the services. But I still encounter the error:
```
0it [00:00, ?it/s]! Resource error: insufficient memory
0it [02:27, ?it/s]
Traceback (most recent call last):
 File ""get_concepts.py"", line 133, in <module>
 process_json(task_path, task)
 File ""get_concepts.py"", line 110, in process_json
 results = extract_entities(documents)
 File ""get_concepts.py"", line 87, in extract_entities
 concepts = mm.extract_concepts(
 File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/SubprocessBackend.py"", line 243, in extract_concepts
 concepts = Corpus.load(output.splitlines())
 File ""/home/xiaolei/anaconda3/lib/python3.8/site-packages/pymetamap-0.2-py3.8.egg/pymetamap/Concept.py"", line 75, in load
 if fields[1] == 'MMI':
IndexError: list index out of range
```
Here is my function:
```python
def extract_entities(docs):
 results = []
 mm = MetaMap.get_instance('/data/tools/public_mm/bin/metamap')
 for doc in docs:
 concepts = mm.extract_concepts(
 [doc], word_sense_disambiguation=True, unique_acronym_variants=True,
 ignore_stop_phrases=True, no_derivational_variants=True,
 )
 results.append(concepts)

 # below implementation will cause insuficient memory error, even with 16G XMx
 # results = [[]] * len(docs)
 # concepts = mm.extract_concepts(docs, ids=list(range(len(docs))), word_sense_disambiguation=True)
 # for concept in concepts:
 # results[int(concept.index)].append(concept)

 return results
```",Other,Other,
2474,[PR] The proj files have been updated to enable SourceLink,"CSProj files have been updated to enable SourceLink in your nuget
---

*[This pull request was created with an automated workflow]*

I noticed that your repository and Nuget package are important for our .NET community, but you still haven't enabled SourceLink.

**We have to take 2 steps:**
1) Please approve this pull request and make .NET a better place for .NET developers and their debugging.
2) **Then just upload the .snupkg file** to https://www.nuget.org/ (now you can find the snupkg file along with the .nuget file)

You can find more information about SourceLine at the following links 
https://github.com/dotnet/sourcelink
https://www.hanselman.com/blog/ExploringNETCoresSourceLinkSteppingIntoTheSourceCodeOfNuGetPackagesYouDontOwn.aspx

If you are interesting about this automated workflow and how it works 
https://github.com/JTOne123/GitHubMassUpdater

*If you notice any flaws, please comment and I will try to make fixes manually*",Other,Other,
2310,Compacter dictionary with Scopus journal names,"A somewhat compacter dictionary with additional journal titles, mostly compiled from the previously used sources:
- duplicate entries in the same mode are not repeated
- American surnames with <0.001 are not included (by my estimation this means <3,000 US population)
- Additional journal names based on the publications indexed by Scopus

Happy to share the simple script used to compile the dictionary file.",Other,Other,
19099,Update google-api-services-bigquery to v2-rev20210422-1.31.0,"Updates com.google.apis:google-api-services-bigquery from v2-rev20210410-1.31.0 to v2-rev20210422-1.31.0.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/6daa53b4ad73f87c6fecf65525d53b596a31f15b/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-bigquery"" } ]
```
</details>

labels: library-update",Other,Other,
39136,Bump certifi from 2018.1.18 to 2022.12.7,"Bumps [certifi](https://github.com/certifi/python-certifi) from 2018.1.18 to 2022.12.7.
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/certifi/python-certifi/commit/9e9e840925d7b8e76c76fdac1fab7e6e88c1c3b8""><code>9e9e840</code></a> 2022.12.07</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/b81bdb269f1edb791bcd4ec8a9d0c053758f961a""><code>b81bdb2</code></a> 2022.09.24</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/939a28ffc57b1613770f572b584745c7b6d43e7d""><code>939a28f</code></a> 2022.09.14</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/aca828a78e73235a513dff9ebc181a47ef7dbf7b""><code>aca828a</code></a> 2022.06.15.2</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/de0eae12a6d5794a4c1e33052af6717707ce1fcc""><code>de0eae1</code></a> Only use importlib.resources's new files() / Traversable API on Python â¥3.11 ...</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/b8eb5e9af9143b22b7f651942b393e369ed4c52a""><code>b8eb5e9</code></a> 2022.06.15.1</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/47fb7ab715965684e035292d2ad3386aabdc4d25""><code>47fb7ab</code></a> Fix deprecation warning on Python 3.11 (<a href=""https://github-redirect.dependabot.com/certifi/python-certifi/issues/199"">#199</a>)</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/b0b48e059995f455ac1e79b3ad373ad4ef355516""><code>b0b48e0</code></a> fixes <a href=""https://github-redirect.dependabot.com/certifi/python-certifi/issues/198"">#198</a> -- update link in license</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/9d514b4cad79357071c89d7dc4dc1b4df72bb997""><code>9d514b4</code></a> 2022.06.15</li>
<li><a href=""https://github.com/certifi/python-certifi/commit/4151e8849481f396537c34812068e89b32731e52""><code>4151e88</code></a> Add py.typed to MANIFEST.in to package in sdist (<a href=""https://github-redirect.dependabot.com/certifi/python-certifi/issues/196"">#196</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/certifi/python-certifi/compare/2018.01.18...2022.12.07"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=certifi&package-manager=pip&previous-version=2018.1.18&new-version=2022.12.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/cjratcliff/ml-compiled/network/alerts).

</details>",Other,Other,
27673,Latest Wapiti,"I noted that AP uses the gem `wapiti (0.1.1)`. This is very old I guess. Is there any plans to upgrade?
Grobid uses Wapiti version 1.5.0. 

Just to check the latest capability of Wapiti, is it possible to run AP outside of gem. 

PS: I am NOT a ruby person :)",Other,Other,
1252,Library versions,"Qt and OpenCV has EOL versions as minimum requirements. 
This PR updates the documentation and build_scripts. 

edit: description of #72:
I updated the code to match more CLang Tidy hints. Those helpt to write more up to date code. For example, NULL --> nullptr.",Other,Other,
19879,TraditionalChineseToker cannot use CustomerDictionary,"I am testing different tokenizer performance, I found that the TraditionalChineseTokenizer does not apply the CustomDicionary as the others do.

```
CustomDictionary.add(""éä¸»ç¤"", ""nz 1024 n 1"");
 Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true);

 for (String sentence : testCase)
 {
 System.out.println( ""\n\n"" + ""Sentence"" + sentence);
 System.out.println(""HanLP"");
 List<Term> termList = segment.seg(sentence);
 System.out.println(termList);

 termList = TraditionalChineseTokenizer.segment(sentence);
 System.out.println(""TraditionalChineseTokenizer"");
 System.out.println(termList);


 termList = NLPTokenizer.segment(sentence);
 System.out.println(""NLPTokenizer"");
 System.out.println(termList);
 }
```

Results
Sentence (Only TraditionalChineseTokenizer does not apply the ""éä¸»ç¤"" new word in CustomDictionary.)

```

éä¸»ç¤,å¥½å¤äººç­ç·æ¨å¸å¤§è·!
HanLP
[éä¸»ç¤/nz, ,/w, å¥½å¤/mq, äºº/n, ç­/udeng, ç·/n, æ¨/n, å¸/n, å¤§è·/v, !/w]
TraditionalChineseTokenizer
[é/ng, ä¸»ç¤/n, ,/w, å¥½å¤/mq, äºº/n, ç­/udeng, ç·/d, æ¨å¸/n, å¤§è·/v, !/w]
NLPTokenizer
[éä¸»ç¤/nz, ,/w, å¥½å¤äºº/nt, ç­/udeng, ç·/n, æ¨/n, å¸/n, å¤§è·/v, !/w]
```",Other,Other,
43996,æ§è½æµè¯ DoubleArrayTrie åå­ä½¿ç¨ > AhoCorasickDoubleArrayTrie > Bintrie,"## çæ¬å·
<!-- åè¡çè¯·æ³¨æjaræä»¶åå»ææå±åçé¨åï¼GitHubä»åºçè¯·æ³¨æmasterè¿æ¯portableåæ¯ -->

å½åææ°çæ¬å·æ¯ï¼
æä½¿ç¨ççæ¬æ¯ï¼1.7.2

## æçé®é¢
æç§åç 
 å¯¹DAT ã ACDat ãBintrie è¿è¡æ§è½æµè¯

æµè¯ææ¬ : 

""æè¿æ­£å¨åä¸ä¸ªèªå·±çNLPåºï¼åèµ·æ­¥çç¬¬ä¸ä¸ªé®é¢å°±æ¯å­å¸çå¨å­ä¸æ¥è¯¢ãæ¯«æ çé®ï¼æä½³çæ°æ®ç»ææ¯Trieæ ï¼åæ¶ä¸ºäºå¹³è¡¡æçåç©ºé´ï¼å³å®ä½¿ç¨åæ°ç»Trieæ ãç°å¨çé®é¢æ¯ï¼åæ°ç»Trieæ æ¯ä¸ä¸ªåç¼©çTrieæ "";

æµè¯è¯å¸: 
CoreNatureDictionary.txt

JVM å è®¾ç½®: -Xms2g Xmx2g 

æ§è½åæå·¥å·: JConsole 

æµè¯ç»æ : 
æç§åç DAT çåå­ä½¿ç¨åºè¯¥æ¯ä¸èæå°ç
ä½ç®åæµå°çç»æ DAT > ACDAT > Bintrie 
åè Bintrie å åå­ä½¿ç¨æå°

æ§è½æ¹é¢ : DAT æ§è½çº¦ç­äº ACDAT > Bintrie 

### è§¦åä»£ç 

```
 String text = ""æè¿æ­£å¨åä¸ä¸ªèªå·±çNLPåºï¼åèµ·æ­¥çç¬¬ä¸ä¸ªé®é¢å°±æ¯å­å¸çå¨å­ä¸æ¥è¯¢ãæ¯«æ çé®ï¼æä½³çæ°æ®ç»ææ¯Trieæ ï¼åæ¶ä¸ºäºå¹³è¡¡æçåç©ºé´ï¼å³å®ä½¿ç¨åæ°ç»Trieæ ãç°å¨çé®é¢æ¯ï¼åæ°ç»Trieæ æ¯ä¸ä¸ªåç¼©çTrieæ "";

public void testACDAT() throws InterruptedException {
 Thread.sleep(10000);
 Runtime.getRuntime().gc();
 TreeMap<String, CoreDictionary.Attribute> map = new TreeMap<String, CoreDictionary.Attribute>();
 IOUtil.LineIterator iterator = new IOUtil.LineIterator(""data/dictionary/CoreNatureDictionary.txt"");
 while (iterator.hasNext())
 {
 String line = iterator.next().split(""\\s"")[0];
 map.put(line, new CoreDictionary.Attribute(Nature.n));
 }
 Runtime.getRuntime().gc();
 System.out.println("" Map æå»ºæå !!!!!"");
 Thread.sleep(10000);

 //AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute> act = new AhoCorasickDoubleArrayTrie<CoreDictionary.Attribute>();
 //DoubleArrayTrie<CoreDictionary.Attribute> act = new DoubleArrayTrie<CoreDictionary.Attribute>();
 BinTrie<CoreDictionary.Attribute> act = new BinTrie<CoreDictionary.Attribute>();
 long timeMillis = System.currentTimeMillis();
 act.build(map);
 System.out.println(""æå»ºèæ¶ï¼"" + (System.currentTimeMillis() - timeMillis) + "" ms"");
 timeMillis = System.currentTimeMillis();
 for(int i = 0 ; i < 30000000; i ++){
 act.parseText(text, new AhoCorasickDoubleArrayTrie.IHit<CoreDictionary.Attribute>() {
 @Override
 public void hit(int begin, int end, CoreDictionary.Attribute value) {

 }
 });
 }
 System.out.println(""åè¯èæ¶ï¼"" + (System.currentTimeMillis() - timeMillis) + "" ms"");
 }
```
### ææè¾åº

<!-- ä½ å¸æè¾åºä»ä¹æ ·çæ­£ç¡®ç»æï¼-->

```
ææè¾åº
```

### å®éè¾åº

<!-- HanLPå®éè¾åºäºä»ä¹ï¼äº§çäºä»ä¹ææï¼éå¨åªéï¼-->

```
å®éè¾åº
```

## å¶ä»ä¿¡æ¯

<!-- ä»»ä½å¯è½æç¨çä¿¡æ¯ï¼åæ¬æªå¾ãæ¥å¿ãéç½®æä»¶ãç¸å³issueç­ç­ã-->",Other,Other,
1375,[Question] How would you use the model to make predictions on new data?,"I have read all the examples and gone through the source code, but haven't been able to answer the question.

I have setup a data set, trained and tested the model, but now I would like to use the model to make predictions on new data. How would I achieve this?

Example:
Target value has 3 classifications: good, bad, average

New data comes in -> use trained/tested model to make a prediction on the target value. Also, would it be possible to get a probability/confidence of the prediction of the target value i.e. 25% good, 50% average, 25% bad.",Other,Other,
21435,@pyimport foo is deprecated,"I get this warning:

```
â Warning: `@pyimport foo` is deprecated in favor of `foo = pyimport(""foo"")`.
```

when running the example https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison.ipynb.",Other,Other,
37866,Update setup.py to support Windows Installation,"Fixed up the setup.py file to be compatible with Windows.
The windows installation doesn't appreciate the '/' at the end of file paths.",Other,Other,
41549,NameError: name 'iou' is not defined in unet.py,"Hi, It seems like variable/function 'iou' is missing in unet.py

My stack trace: 
File ""/macierz/home/s174520/eye-in-the-sky/unet.py"", line 84, in UNet
 model.compile(optimizer = Adam(lr = 0.000001), loss = 'categorical_crossentropy', metrics = ['accuracy', iou])
NameError: name 'iou' is not defined

Edit:
I checked commit history. I think you have added iou function to test_unet.py file istead of unet.py and you're not passing it to UNet model.",Other,Other,
24003,isPartOf Mapping not used in functional_class.py,"https://github.com/pathwayforte/pathway-forte/blob/6acfcbb3b09d3ee7064f9365da5160d81c53e94a/src/pathway_forte/pathway_enrichment/functional_class.py#L371

Why is this here if it doesn't get used?",Other,Other,
4000,Create Experimenting notes,This files tracks the progress over the md-control.,Other,Other,
7219,why add a Dense(64) layer after the attention layer,what's the point of adding another `attention_mul = Dense(units=64)(attention_mul)` ?,Other,Other,FIXED
13437,Normalizes unpunctuated Vancouver-style names,"- Adds punctuation separator between family name and initials for Vancouver-style author names
- Example: `Rang HP, Dale MM, Ritter JM, Moore PK` normalizes to `Rang, HP, Dale, MM, Ritter, JM, Moore, PK` so that these can be parsed correctly into family name/given name pairs

NB. Alternative is to normalize them by switching the order of surname-initials to initials-surname",Other,Other,
4795,"Adds parellelism to Bayesian optimizer by default and adds support for non-deterministic algorithms (release postponed, so no version incrementation)","I've had a second look at the Bayesian Optimizer and have introduced parallelism by default as with other optimizers. I've also introduced support for non-deterministic algorithms that may return different results for identical parameters. This also entailed a change to the standard serial behaviour so that instead of skipping an evaluation if the parameters did not change from the previous run, it will now store the results for all evaluations and skip any that have been run before. This should result in a performance improvement for the serial behaviour but will consume some memory.",Other,Other,FIXED
1902,Error tagging new release,"The tag name ""0.4.0"" is not of the appropriate SemVer form (vX.Y.Z).
cc: @trthatcher",Other,Other,
6167,The decodevalues do not reproduce the original image,I have tried to replicate the code given along with installing the required libraries but i am not able to reproduce the original image from the decoded values. Any help is appreciated?,Transparency & Explainability ,Transparency & Explainability ,
3029,Getting the entire set of labels (along with the sub-labels) used in the model,"Dear Mr. @inukshuk 

I am analyzing Anystyle parser and I would like to know the labels used in the model.
With Anystyle.parser.model.labels the output is:
`[""accessed"", ""author"", ""authority"", ""booktitle"", ""date"", ""director"", ""doi"", ""edition"", ""editor"", ""event"", ""genre"", ""isbn"", ""journal"", ""location"", ""medium"", ""note"", ""pages"", ""producer"", ""publisher"", ""section"", ""source"", ""title"", ""translator"", ""unknown"", ""url"", ""volume""]`

However, when I set the format = :citeproc, there are more labels than the ones specified above.
`irb(main):033:0> Myprs.parse(""Lombard, M.-A. 1989. âTchador Ã lâÃ©cole: deux mois dâun dÃ©bat qui divise la France.â Le Figaro, 27 November, p. 10."", format = :citeproc) => [{""author""=>[{""family""=>""Lombard"", ""given""=>""M.-A.""}], ""title""=>""Tchador Ã lâÃ©cole: deux mois dâun dÃ©bat qui divise la France."", ""container-title""=>""Le Figaro"", ""volume""=>""27"", ""page""=>""10"", ""language""=>"""", ""id""=>""lombard1989a"", ""issued""=>{""date-parts""=>[[1989, 11]]}, ""type""=>""article-journal""}]`
Is there a way that I can view/extract all the labels (along with its sub-labels) used in the model?

Your help will aid me in my school project.",Other,Other,FIXED
2413,BSD license,"<!-- Reviewable:start -->
This change isâ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/mogeng/iohmm/15)
<!-- Reviewable:end -->",Other,Other,
1407,Added dictionary version of KeyCombine which is a lot faster,moved unittests from CsvParserTests to CsvRowExtensionsTests,Other,Other,
23624,Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5Ã3 positions and 5Ã4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5Ã3 positions and 5Ã4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4Ã3 positions and 4Ã4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter networkã",Transparency & Explainability ,Transparency & Explainability ,FIXED
37987,Random Forest: how can I get each tree's prediction ?,"hi,Mads,
I hope to know how can I get each tree's prediction or which category it choose when I finished trained a RF model for my classify taskï¼It is useful for my task.
thanksï¼",Other,Other,
12671,Question about training GAN,"I found that it only forward once before update discriminator and generator per iteration in your code. However, in other papers, some forward respectively for D and G, e.g. MUNIT. Personally, I think the latter one is more reasonable. Could you please tell me what's the difference between this two kinds of pipeline? Or why do you forward only once per iteration? Thanks a lot.",Other,Other,
19183,Update sbt to 1.3.2,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) [from 1.3.1 to 1.3.2](https://github.com/sbt/sbt/compare/v1.3.1...v1.3.2).
[Release Notes/Changelog](https://github.com/sbt/sbt/releases/tag/v1.3.2)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention @scala-steward in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: semver-patch",Other,Other,
28623,"iOS: ""No unread stories"" message showing up in Saved stories view","Screenshot says it all.
![img_0061](https://cloud.githubusercontent.com/assets/44229/17415407/9b14c1ea-5a3e-11e6-9963-42a145e4324c.PNG)",Other,Other,
370,Use this repository for CNN,"Dear Sir,

Would it be possible to use this repo for CNN network also?

Thanks and regards.",Other,Other,
29087,More font choices for story list,"Currently the web client supports only 2 typefaces: Lucida Grande and Georgia. These are fine typefaces, but a typical Newsblur user will be scanning _large_ amounts of boldface text (i.e., unread item titles), and the boldface variations of these faces were designed to provide emphasis to short phrases, not for typesetting large amounts of text. Lucida Grande bold is heavy and does not scan very quickly.

Of course, reasonable people may disagree about which fonts look better in boldface, but I think some additional font choices (or perhaps styling choices for unread items?) are warranted.",Other,Other,
37482,chore(deps-dev): bump electron from 9.3.1 to 15.5.5,"Bumps [electron](https://github.com/electron/electron) from 9.3.1 to 15.5.5.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/electron/electron/releases"">electron's releases</a>.</em></p>
<blockquote>
<h2>electron v15.5.5</h2>
<h1>Release Notes for v15.5.5</h1>
<h2>Other Changes</h2>
<ul>
<li>Backported fix for CVE-2022-1482. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34040"">#34040</a></li>
<li>Backported fix for CVE-2022-1483. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34009"">#34009</a></li>
<li>Backported fix for CVE-2022-1497. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34075"">#34075</a></li>
</ul>
<h2>electron v15.5.4</h2>
<h1>Release Notes for v15.5.4</h1>
<h2>Other Changes</h2>
<ul>
<li>Backported fix for CVE-2022-1138. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33682"">#33682</a></li>
<li>Backported fix for CVE-2022-1478. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34045"">#34045</a></li>
<li>Backported fix for CVE-2022-1479. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34037"">#34037</a></li>
<li>Backported fix for CVE-2022-1480. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34019"">#34019</a></li>
<li>Backported fix for CVE-2022-1492. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/34051"">#34051</a></li>
</ul>
<h2>electron v15.5.3</h2>
<h1>Release Notes for v15.5.3</h1>
<h2>Fixes</h2>
<ul>
<li>Fixed a network service crash that could occur when using setCertificateVerifyProc. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33256"">#33256</a> <!-- raw HTML omitted -->(Also in <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33255"">16</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33254"">17</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33253"">18</a>)<!-- raw HTML omitted --></li>
<li><code>shell.openExternal()</code> now reports more detailed errors on Windows. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33656"">#33656</a> <!-- raw HTML omitted -->(Also in <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33657"">16</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33658"">17</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33705"">18</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33660"">19</a>)<!-- raw HTML omitted --></li>
</ul>
<h2>Other Changes</h2>
<ul>
<li>Backported fix for CVE-2022-1134. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33763"">#33763</a></li>
<li>Backported fix for CVE-2022-1305. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33860"">#33860</a></li>
<li>Backported fix for CVE-2022-1310. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33831"">#33831</a></li>
<li>Backported fix for CVE-2022-1314. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33884"">#33884</a></li>
<li>Backported fix for CVE-2022-1364. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33836"">#33836</a></li>
<li>Backported fix for chromium:1286816. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33679"">#33679</a></li>
<li>Backported fix for chromium:1291482. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33676"">#33676</a></li>
<li>Backported fix for chromium:1310761. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33856"">#33856</a></li>
<li>Security: backported fix for CVE-2022-0116 and CVE-2022-1306. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33852"">#33852</a></li>
<li>Security: backported fix for CVE-2022-23308. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33670"">#33670</a></li>
<li>Security: backported fix for chromium:1280743. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33715"">#33715</a></li>
<li>Security: backported fix for chromium:1280852. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33673"">#33673</a></li>
</ul>
<h2>electron v15.5.2</h2>
<h1>Release Notes for v15.5.2</h1>
<h2>Fixes</h2>
<ul>
<li>Fixed behavior of BrowserWindow.maximize on macOS for not shown windows. <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33523"">#33523</a> <!-- raw HTML omitted -->(Also in <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33535"">16</a>, <a href=""https://github-redirect.dependabot.com/electron/electron/pull/33537"">18</a>)<!-- raw HTML omitted --></li>
</ul>
<h2>Other Changes</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/electron/electron/commit/f98885312827f3f111cfac80d71dc9a2c2d2cbc9""><code>f988853</code></a> Bump v15.5.5</li>
<li><a href=""https://github.com/electron/electron/commit/8bc25e8a9de07496db6b884b228ca12d0e161f07""><code>8bc25e8</code></a> build: change upload-to-s3 vars to upload-to-storage (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34142"">#34142</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/a8f8b507aa01c794c698f72be7cc9d0f9bb9fa5b""><code>a8f8b50</code></a> build: use azure function to hash assets instead of lambda (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34119"">#34119</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/eb320cbbf9f887ff2ecf3c76ef8b1eff7143e49f""><code>eb320cb</code></a> build: stop uploading assets to S3 (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34112"">#34112</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/37eab5c66e609f9d98203c58feb740d23265ea2e""><code>37eab5c</code></a> chore: cherry-pick 5be8e065f43e from chromium (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34040"">#34040</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/fc912026e2c383c24949e4bb381eb7a07121a657""><code>fc91202</code></a> chore: cherry-pick 5361d836ae from chromium (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34009"">#34009</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/6aa0609a9707a81611b6dceb2dbd9d779c83a040""><code>6aa0609</code></a> chore: cherry-pick 6b66a45021 from chromium (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34075"">#34075</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/620d615fe026d654178be0a462dab3580429f55a""><code>620d615</code></a> test: fix nativeModulesEnabled in spec/webview-spec.js (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34063"">#34063</a>)</li>
<li><a href=""https://github.com/electron/electron/commit/ec7baba40166e9f11046cff31de180e4f652ec23""><code>ec7baba</code></a> Bump v15.5.4</li>
<li><a href=""https://github.com/electron/electron/commit/f8ba4d6012c2c6a853c0b9b31e2a3816ac8b3c0c""><code>f8ba4d6</code></a> chore: cherry-pick d27d9d059b51 from angle (<a href=""https://github-redirect.dependabot.com/electron/electron/issues/34045"">#34045</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/electron/electron/compare/v9.3.1...v15.5.5"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=electron&package-manager=npm_and_yarn&previous-version=9.3.1&new-version=15.5.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/uetchy/juno/network/alerts).

</details>",Other,Other,
7456,Add preserveObjectReferences option to GenericXmlDataContractSerializer,"The default settings for the GenericXmlDataContractSerializer is to perserve object references. This is required for the SharpLearning.Neural.Models to be serialized and deserialized correctly. However, for serializing simple types like a list of items, the resulting xml can be slightly more complicated. Therefore this pull request adds the option to not preserve object references when using the GenericXmlDataContractSerializer.

The recommended setting for serializing SharpLearning models is the default (true).

**XML with perserveObjectReferences = true (default):**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" z:Id=""1"" z:Size=""5"" xmlns:z=""http://schemas.microsoft.com/2003/10/Serialization/"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
 <KeyValueOfstringint>
 <Key z:Id=""2"">Test1</Key>
 <Value>0</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key z:Id=""3"">Test2</Key>
 <Value>1</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key z:Id=""4"">Test3</Key>
 <Value>2</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key z:Id=""5"">Test4</Key>
 <Value>3</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key z:Id=""6"">Test5</Key>
 <Value>4</Value>
 </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```

**XML with perserveObjectReferences = false:**
```csharp
<?xml version=""1.0"" encoding=""utf-16""?>
<ArrayOfKeyValueOfstringint xmlns:i=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://schemas.microsoft.com/2003/10/Serialization/Arrays"">
 <KeyValueOfstringint>
 <Key>Test1</Key>
 <Value>0</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key>Test2</Key>
 <Value>1</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key>Test3</Key>
 <Value>2</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key>Test4</Key>
 <Value>3</Value>
 </KeyValueOfstringint>
 <KeyValueOfstringint>
 <Key>Test5</Key>
 <Value>4</Value>
 </KeyValueOfstringint>
</ArrayOfKeyValueOfstringint>
```",Other,Other,
27790,[PR] The proj files have been updated to enable SourceLink,"CSProj files have been updated to enable SourceLink in your nuget
---

*[This pull request was created with an automated workflow]*

I noticed that your repository and Nuget package are important for our .NET community, but you still haven't enabled SourceLink.

**We have to take 2 steps:**
1) Please approve this pull request and make .NET a better place for .NET developers and their debugging.
2) **Then just upload the .snupkg file** to https://www.nuget.org/ (now you can find the snupkg file along with the .nuget file)

You can find more information about SourceLine at the following links 
https://github.com/dotnet/sourcelink
https://www.hanselman.com/blog/ExploringNETCoresSourceLinkSteppingIntoTheSourceCodeOfNuGetPackagesYouDontOwn.aspx

If you are interesting about this automated workflow and how it works 
https://github.com/JTOne123/GitHubMassUpdater

*If you notice any flaws, please comment and I will try to make fixes manually*",Other,Other,
8457,ttx parser breaks if line doesn't end with space,"If in a `.ttx` file, a line does not end with a space (or is empty), the following error occurs:

```
Error: undefined method `gsub' for nil:NilClass
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/utils.rb:42:in `display_chars'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:10:in `block in parse'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:9:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/page.rb:9:in `parse'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/document.rb:62:in `pages'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/document.rb:67:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:33:in `with_index'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:33:in `block in expand'
 /var/lib/gems/3.0.0/gems/wapiti-2.0.0/lib/wapiti/dataset.rb:61:in `each'
 /var/lib/gems/3.0.0/gems/wapiti-2.0.0/lib/wapiti/dataset.rb:61:in `each'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:32:in `expand'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/parser.rb:86:in `prepare'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/finder.rb:88:in `prepare'
 /var/lib/gems/3.0.0/gems/anystyle-1.3.14/lib/anystyle/parser.rb:59:in `train'
 /app/cgi-bin/train.rb:55:in `<main>'
```

for example, the following line causes this to happen:

`blank |\n` 

This is a problem because IDEs routinely strip trailing whitespace.

I've been working around this by programmatically adding spaces but I think it would be much better to fix the source of the problem, given that it should be really straightforward to make the parser more robust in this respect.

Thank you.",Other,Other,
27289,Where is cuda_config.h?,"In the ""readme.md"", there is ""copy the header file ""cuda\_config.h"" from ""your\_python\_path/site-packages/external/local\_config\_cuda/cuda/cuda/cuda\_config.h"" to ""your\_python\_path/site-packages/tensorflow/include/tensorflow/stream\_executor/cuda/cuda\_config.h"".
 But I can't find the path or ""cuda_config.h"" anywhere. It may lead to the failure of ""make"" command. Can you fix it?",Other,Other,
25457,[Feature Request] Ability to save media enclosed in feed items (eg podcasts),"I'm subscribed to a number of MP3 / MP4 / OGG audio and video podcasts that enclose media in the RSS feed. When one of those is in NewsBlur, it gives no indication that a media file exists in the feed item. I'd love it if there was some kind of visual indication that enclosed media existed and a ""Save media file"" button that let you save the enclosed file to a location of your choice.",Other,Other,
29537,Add cntk python simple mnist examples,"As a starting point for simple python mnist example using cntk, copied the original example from [cntk](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)

Currently, I have not found a method for running/debugging this from visual studio, only from command prompt. More information can be found in the [readme](https://github.com/mdabros/SharpLearning/blob/backends-cntk-python/python/src/CntkPython/README.md)

@nietras Currently, the simple mnist example from CNTK is quite different from corresponding example in Tensorflow. We should decide how much we want to modify the CNTK example, and/or the tensorflow example, to make them as similar as possible.

Some differences: 
 - The CNTK example uses text files as input and tensorflow the raw data.
 - The CNTK example also uses the Dense operator from the layer API.
 - The CNTK exmaple uses all 60k examples for training and tensorflow only 10k.",Other,Other,
17679,"Minor typo fixes in layers (release postponed, so no version incrementation)",Fix typo in comment of DropoutLayer and SoftMaxLayer. Misspelled gradients.,Other,Other,
35552,Bump pillow from 6.0.0 to 8.2.0,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.0.0 to 8.2.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/releases"">pillow's releases</a>.</em></p>
<blockquote>
<h2>8.2.0</h2>
<p><a href=""https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html"">https://pillow.readthedocs.io/en/stable/releasenotes/8.2.0.html</a></p>
<h2>Changes</h2>
<ul>
<li>Security fixes for 8.2.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> [<a href=""https://github.com/hugovk""><code>@âhugovk</code></a>]</li>
<li>Move getxmp() to JpegImageFile <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5376"">#5376</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a> [<a href=""https://github.com/UrielMaD""><code>@âUrielMaD</code></a>]</li>
<li>Compile LibTIFF with CMake on Windows <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5359"">#5359</a> [<a href=""https://github.com/nulano""><code>@ânulano</code></a>]</li>
<li>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a> [<a href=""https://github.com/latosha-maltba""><code>@âlatosha-maltba</code></a>]</li>
<li>Tiff crash fixes in TiffDecode.c <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5372"">#5372</a> [<a href=""https://github.com/wiredfool""><code>@âwiredfool</code></a>]</li>
<li>Remove redundant check (addition to <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>) <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5366"">#5366</a> [<a href=""https://github.com/kkopachev""><code>@âkkopachev</code></a>]</li>
<li>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a> [<a href=""https://github.com/raygard""><code>@âraygard</code></a>]</li>
<li>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a> [<a href=""https://github.com/elejke""><code>@âelejke</code></a>]</li>
<li>Only import numpy when necessary <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5323"">#5323</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Add support for reading TIFFs with PlanarConfiguration=2 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a> [<a href=""https://github.com/wiredfool""><code>@âwiredfool</code></a>]</li>
<li>More OSS-Fuzz support <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5328"">#5328</a> [<a href=""https://github.com/wiredfool""><code>@âwiredfool</code></a>]</li>
<li>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a> [<a href=""https://github.com/nulano""><code>@ânulano</code></a>]</li>
<li>Use quantization method attributes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5353"">#5353</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a> [<a href=""https://github.com/nulano""><code>@ânulano</code></a>]</li>
<li>Removed build_distance_tables return value <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5363"">#5363</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Improved efficiency when creating GIF disposal images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5326"">#5326</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Replaced tiff_deflate with tiff_adobe_deflate compression when saving TIFF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5343"">#5343</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Save ICC profile from TIFF encoderinfo <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5321"">#5321</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Moved RGB fix inside ImageQt class <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5268"">#5268</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Fix -Wformat error in TiffDecode <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5305"">#5305</a> [<a href=""https://github.com/lukegb""><code>@âlukegb</code></a>]</li>
<li>Allow alpha_composite destination to be negative <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5313"">#5313</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Ensure file is closed if it is opened by ImageQt.ImageQt <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5260"">#5260</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added ImageDraw rounded_rectangle method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5208"">#5208</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Added IPythonViewer <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5289"">#5289</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Only draw each rectangle outline pixel once <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5183"">#5183</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Use mmap instead of built-in Win32 mapper <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5224"">#5224</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Handle PCX images with an odd stride <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5214"">#5214</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
<li>Only read different sizes for &quot;Large Thumbnail&quot; MPO frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5168"">#5168</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>Updated harfbuzz to 2.8.0 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5334"">#5334</a> [<a href=""https://github.com/radarhere""><code>@âradarhere</code></a>]</li>
</ul>
<h2>Deprecations</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst"">pillow's changelog</a>.</em></p>
<blockquote>
<h2>8.2.0 (2021-04-01)</h2>
<ul>
<li>
<p>Added getxmp() method <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5144"">#5144</a>
[UrielMaD, radarhere]</p>
</li>
<li>
<p>Add ImageShow support for GraphicsMagick <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5349"">#5349</a>
[latosha-maltba, radarhere]</p>
</li>
<li>
<p>Do not load transparent pixels from subsequent GIF frames <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5333"">#5333</a>
[zewt, radarhere]</p>
</li>
<li>
<p>Use LZW encoding when saving GIF images <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5291"">#5291</a>
[raygard]</p>
</li>
<li>
<p>Set all transparent colors to be equal in quantize() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5282"">#5282</a>
[radarhere]</p>
</li>
<li>
<p>Allow PixelAccess to use Python <strong>int</strong> when parsing x and y <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5206"">#5206</a>
[radarhere]</p>
</li>
<li>
<p>Removed Image._MODEINFO <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5316"">#5316</a>
[radarhere]</p>
</li>
<li>
<p>Add preserve_tone option to autocontrast <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5350"">#5350</a>
[elejke, radarhere]</p>
</li>
<li>
<p>Fixed linear_gradient and radial_gradient I and F modes <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5274"">#5274</a>
[radarhere]</p>
</li>
<li>
<p>Add support for reading TIFFs with PlanarConfiguration=2 <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5364"">#5364</a>
[kkopachev, wiredfool, nulano]</p>
</li>
<li>
<p>Deprecated categories <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5351"">#5351</a>
[radarhere]</p>
</li>
<li>
<p>Do not premultiply alpha when resizing with Image.NEAREST resampling <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5304"">#5304</a>
[nulano]</p>
</li>
<li>
<p>Dynamically link FriBiDi instead of Raqm <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5062"">#5062</a>
[nulano]</p>
</li>
<li>
<p>Allow fewer PNG palette entries than the bit depth maximum when saving <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5330"">#5330</a>
[radarhere]</p>
</li>
<li>
<p>Use duration from info dictionary when saving WebP <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5338"">#5338</a>
[radarhere]</p>
</li>
<li>
<p>Stop flattening EXIF IFD into getexif() <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4947"">#4947</a>
[radarhere, kkopachev]</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/python-pillow/Pillow/commit/e0e353c0ef7516979a9aedce3792596649ce4433""><code>e0e353c</code></a> 8.2.0 version bump</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ee635befc6497f1c6c4fdb58c232e62d922ec8b7""><code>ee635be</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/python-pillow/Pillow/issues/5377"">#5377</a> from hugovk/security-and-release-notes</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/694c84f88f4299319bac49b20bd9baae82ca41b8""><code>694c84f</code></a> Fix typo [ci skip]</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/8febdad8dd51ad5c75a1db78492973588c7cbf6b""><code>8febdad</code></a> Review, typos and lint</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/fea419665b75f11910e44cfe6f89622fda63e78b""><code>fea4196</code></a> Reorder, roughly alphabetic</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/496245aa4365d0827390bd0b6fbd11287453b3a1""><code>496245a</code></a> Fix BLP DOS -- CVE-2021-28678</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/22e9bee4ef225c0edbb9323f94c26cee0c623497""><code>22e9bee</code></a> Fix DOS in PSDImagePlugin -- CVE-2021-28675</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/ba65f0b08ee8b93195c3f3277820771f5b62aa52""><code>ba65f0b</code></a> Fix Memory DOS in ImageFont</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/bb6c11fb889e6c11b0ee122b828132ee763b5856""><code>bb6c11f</code></a> Fix FLI DOS -- CVE-2021-28676</li>
<li><a href=""https://github.com/python-pillow/Pillow/commit/5a5e6db0abf4e7a638fb1b3408c4e495a096cb92""><code>5a5e6db</code></a> Fix EPS DOS on _open -- CVE-2021-28677</li>
<li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/6.0.0...8.2.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.0.0&new-version=8.2.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
7865,Questions about the output layer of the pose interpreter network,"Hello, after reading your paper, what I understand is that the pose interpreter network is to output the position and orientation of 5 objects at once, so we need to set the final output of the network to be 5Ã3 positions and 5Ã4 orientations. (Take 5 types of objects as an example).

But after looking at the code of the pose interpreter network(pose-interpreter-networks/pose_estimation/models.py: line 68-75), I found that the logic of this network is that the network inputs a mask of one object and the corresponding object id. The network first outputs 5Ã3 positions and 5Ã4 orientations, and then determines which one to choose as the final predicted value according to the object id. In the end-to-end model(pose-interpreter-networks/models.py: line 45-55) number of times the pose interpreter network runs is equivalent to numbers of objects segmentation network output. 

When training this network(pose-interpreter-networks/pose_estimation/train.py: line 111-118), only 3 positions and 4 orientations are compared with the target value, which is equivalent to the remaining 4Ã3 positions and 4Ã4 orientations are meaningless. (I don't know if my understanding is correct)

If my understanding is correct, then why does the final output of the network need to be related to the number of object types , can it be directly set to output 3 positions and 4 orientations?
If my understanding is wrong, I would appreciate it if you could explain the posture interpreter networkã",Transparency & Explainability ,Transparency & Explainability ,FIXED
16377,Bump opencv-python from 4.1.0.25 to 4.2.0.32,"Bumps [opencv-python](https://github.com/skvark/opencv-python) from 4.1.0.25 to 4.2.0.32.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/skvark/opencv-python/releases"">opencv-python's releases</a>.</em></p>
<blockquote>
<h2>4.2.0.32</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.2.0.</p>
<p>Changes:</p>
<ul>
<li>macOS environment updated from xcode8.3 to xcode 9.4</li>
<li>macOS uses now Qt 5 instead of Qt 4</li>
<li>Nasm version updated to Docker containers</li>
<li>multibuild updated</li>
</ul>
<p>Fixes:</p>
<ul>
<li>don't use deprecated brew tap-pin, instead refer to the full package name when installing <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/267"">#267</a></li>
<li>replace get_config_var() with get_config_vars() in setup.py <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/274"">#274</a></li>
<li>add workaround for DLL errors in Windows Server <a href=""https://github-redirect.dependabot.com/skvark/opencv-python/issues/264"">#264</a></li>
</ul>
<h2>4.1.2.30</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.2.</p>
<p>Changes:</p>
<ul>
<li>Python 3.8 builds added to the build matrix</li>
<li>Support for Python 3.4 builds dropped (Python 3.4 is in EOL)</li>
<li>multibuild updated</li>
<li>minor build logic changes</li>
<li>Docker images rebuilt</li>
</ul>
<p>Notes:</p>
<p>Please note that Python 2.7 enters into EOL phase in January 2020. <code>opencv-python</code> Python 2.7 wheels won't be provided after that.</p>
<h2>4.1.1.26</h2>
<ul>
<li>opencv-python: <a href=""https://pypi.org/project/opencv-python/"">https://pypi.org/project/opencv-python/</a></li>
<li>opencv-contrib-python: <a href=""https://pypi.org/project/opencv-contrib-python/"">https://pypi.org/project/opencv-contrib-python/</a></li>
<li>opencv-python-headless: <a href=""https://pypi.org/project/opencv-python-headless/"">https://pypi.org/project/opencv-python-headless/</a></li>
<li>opencv-contrib-python-headless: <a href=""https://pypi.org/project/opencv-contrib-python-headless/"">https://pypi.org/project/opencv-contrib-python-headless/</a></li>
</ul>
<p>OpenCV version 4.1.1.</p>
<p>Changes:</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/skvark/opencv-python/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=opencv-python&package-manager=pip&previous-version=4.1.0.25&new-version=4.2.0.32)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,
43209,BigQueryType.toTableRow does not serialize properly in lambda when there are custom types,"This serializes, but `BigQuerySchema` does not know about the custom types. Note: `releases` is a `SCollection[Release]`.
```
 val bqType: BigQueryType[Release] = BigQueryType[Release]
 releases
 .map { a =>
 implicit val enumType
 : BaseBigQueryMappableType[MyEnum] =
 BigQueryType.at[MyEnum](""STRING"")(
 a => MyEnum.fromName(a.toString).orNull,
 _.name)
 bqType.toTableRow(a)
 }
 .saveAsBigQuery(
 table,
 schema = BigQuerySchema[Release]
 )
```

This does not serialize:
```
 implicit val enumType
 : BaseBigQueryMappableType[MyEnum] =
 BigQueryType.at[MyEnum](""STRING"")(
 a => MyEnum.fromName(a.toString).orNull,
 _.name)
 val bqType: BigQueryType[Release] = BigQueryType[Release]
 val schema = BigQuerySchema[Release]

 releases
 .map { a =>
 bqType.toTableRow(a)
 }
 .saveAsBigQuery(
 table,
 schema = schema
 )
```

```
Exception in thread ""main"" java.lang.IllegalArgumentException: unable to serialize anonymous function map ...
Caused by: java.io.NotSerializableException ...
```",Other,Other,
22946,Constraints,Is it possible to impose constraints on the parameters?,Other,Other,
30537,Unable to use pymetamap in windows,"I installed the pymetamap setup in windows and try to use it, but the metamap instance was unable to extract concepts from the sentences. I tried to run the same code given i.e

> > > from pymetamap import MetaMap
> > > mm = MetaMap.get_instance('...../public_mm/bin/metamap14')
> > > sents = ['Heart Attack', 'John had a huge heart attack']
> > > concepts,error = mm.extract_concepts(sents,[1,2])
> > > but after this I get the error:
> > > Traceback (most recent call last):
> > > File ""<stdin>"", line 1, in <module>
> > > File ""C:\Anaconda2\lib\site-packages\pymetamap-0.1-py2.7.egg\pymetamap\Subproc
> > > essBackend.py"", line 127, in extract_concepts
> > > os.remove(input_file.name)
> > > WindowsError: [Error 32] The process cannot access the file because it is being
> > > used by another process: 'c:\users\abhila~1.kas\appdata\local\temp\tmp00rn
> > > u0'
> > > I have metamap14 installed on my pc.",Other,Other,
22886,Normalizes unpunctuated Vancouver-style names,"- Adds punctuation separator between family name and initials for Vancouver-style author names
- Example: `Rang HP, Dale MM, Ritter JM, Moore PK` normalizes to `Rang, HP, Dale, MM, Ritter, JM, Moore, PK` so that these can be parsed correctly into family name/given name pairs

NB. Alternative is to normalize them by switching the order of surname-initials to initials-surname",Other,Other,
22413,Why do we need blender for demo?,"Hi, thanks for your great work. I wanted to see the results of your work, i.e. I will give a RGB image and expect 6dposes. I believe that blender was used to create the synthetic dataset for training Pose interpreter network. My question is why do we need blender for seeing the results.

My other question is if we require blender then how to use it on some online platform like google collab?",Other,Other,FIXED
17944,reloadå è½½éè¯¯ArrayIndexOutOfBoundsException,"<!--
æ³¨æäºé¡¹åçæ¬å·å¿å¡«ï¼å¦åä¸åå¤ãè¥å¸æå°½å¿«å¾å°åå¤ï¼è¯·ææ¨¡æ¿è®¤çå¡«åï¼è°¢è°¢åä½ã
-->

## æ³¨æäºé¡¹
è¯·ç¡®è®¤ä¸åæ³¨æäºé¡¹ï¼

* æå·²ä»ç»éè¯»ä¸åææ¡£ï¼é½æ²¡ææ¾å°ç­æ¡ï¼
 - [é¦é¡µææ¡£](https://github.com/hankcs/HanLP)
 - [wiki](https://github.com/hankcs/HanLP/wiki)
 - [å¸¸è§é®é¢](https://github.com/hankcs/HanLP/wiki/FAQ)
* æå·²ç»éè¿[Google](https://www.google.com/#newwindow=1&q=HanLP)å[issueåºæ£ç´¢åè½](https://github.com/hankcs/HanLP/issues)æç´¢äºæçé®é¢ï¼ä¹æ²¡ææ¾å°ç­æ¡ã
* ææç½å¼æºç¤¾åºæ¯åºäºå´è¶£ç±å¥½èéèµ·æ¥çèªç±ç¤¾åºï¼ä¸æ¿æä»»ä½è´£ä»»æä¹å¡ãæä¼ç¤¼è²åè¨ï¼åæ¯ä¸ä¸ªå¸®å©æçäººè¡¨ç¤ºæè°¢ã
* [x] æå¨æ­¤æ¬å·åè¾å¥xæé©ï¼ä»£è¡¨ä¸è¿°äºé¡¹ç¡®è®¤å®æ¯

## çæ¬å·
<!-- åè¡çè¯·æ³¨æjaræä»¶åå»ææå±åçé¨åï¼GitHubä»åºçè¯·æ³¨æmasterè¿æ¯portableåæ¯ -->

å½åææ°çæ¬å·æ¯ï¼1.7.1
æä½¿ç¨ççæ¬æ¯ï¼1.7.1

<!--ä»¥ä¸å±äºå¿å¡«é¡¹ï¼ä»¥ä¸å¯èªç±åæ¥-->

## æçé®é¢
å¨è¯åºè¿è¡reloadæä½æ¶åºç°æ°ç»æº¢åºï¼å¨1.7.0çæ¬ä¸­æ²¡æè¯¥é®é¢ã
æ¯å¦å1.7.1ä¿®æ¹reloadçæèªå®ä¹è¯å¸binæä»¶å¨éæ°è½½å¥æ¶æåºå¼å¸¸ArrayIndexOutOfBoundsException #1028è¯¥é®é¢æå³

## å¤ç°é®é¢

### æ­¥éª¤
`å¨CustomDictionaryç±»çloadMainDictionaryæ¹æ³ä¸­dat.build(map);æ¶åçå¼å¸¸ã
ç»è¿è·è¸ªåç°å¨DoubleArrayTrieç

```
 private int resize(int newSize)
 {
 int[] base2 = new int[newSize];
 int[] check2 = new int[newSize];
 if (allocSize > 0)
 {
 System.arraycopy(base, 0, base2, 0, allocSize);
 System.arraycopy(check, 0, check2, 0, allocSize);
 }

 base = base2;
 check = check2;

 return allocSize = newSize;
 }
```
å¨System.arraycopy(base, 0, base2, 0, allocSize);ä¸­baseåä¸ºæ·è´æºæ°æ®é¿åº¦ä¸º1426181
base2çé¿åº¦ä¸º2097152ï¼èallocSizeå¤å¶é¿åº¦ä¹ä¸º2097152ï¼å¯¼è´æ°æ®æº¢åº
### è§¦åä»£ç 

```
 HanLP.segment(""èªç¶è¯­è¨å¤ç"");
 CustomDictionary.reload();
```
### ææè¾åº

æ­£å¸¸éè½½


### å®éè¾åº

<!-- HanLPå®éè¾åºäºä»ä¹ï¼äº§çäºä»ä¹ææï¼éå¨åªéï¼-->

```
è­¦å: èªå®ä¹è¯å¸./work/data/dictionary/custom/CustomDictionary.txtç¼å­å¤±è´¥ï¼
java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 2097152 out of bounds for int[1426181]
at java.base/java.lang.System.arraycopy(Native Method)
at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.resize(DoubleArrayTrie.java:94)
at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:403)
at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:338)
at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:365)
at com.hankcs.hanlp.collection.trie.DoubleArrayTrie.build(DoubleArrayTrie.java:378)
at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:107)
at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:157)
at com.hankcs.hanlp.dictionary.CustomDictionary.reload(CustomDictionary.java:657)
at com.gildata.removal.Test_Hanlp.main(Test_Hanlp.java:21)
```

## å¶ä»ä¿¡æ¯
å¸æè½å¤ææä¸ä¸
<!-- ä»»ä½å¯è½æç¨çä¿¡æ¯ï¼åæ¬æªå¾ãæ¥å¿ãéç½®æä»¶ãç¸å³issueç­ç­ã-->",Other,Other,
17755,Added dictionary version of KeyCombine which is a lot faster,moved unittests from CsvParserTests to CsvRowExtensionsTests,Other,Other,
5059,Translating docs and codebase to English,Are there any forks of this repository that translate it into English ?,Other,Other,
11164,Update sbt-sonatype to 3.9.10,"Updates [org.xerial.sbt:sbt-sonatype](https://github.com/xerial/sbt-sonatype) from 3.9.9 to 3.9.10.
[GitHub Release Notes](https://github.com/xerial/sbt-sonatype/releases/tag/3.9.10) - [Release Notes](https://github.com/xerial/sbt-sonatype/blob/master/ReleaseNotes.md) - [Version Diff](https://github.com/xerial/sbt-sonatype/compare/3.9.9...3.9.10)

I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5033773a27cb5120a7728023ce082d2f24b93eaf/docs/repo-specific-configuration.md) file.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.xerial.sbt"", artifactId = ""sbt-sonatype"" } ]
```
</details>

labels: sbt-plugin-update, semver-patch",Other,Other,
400,get_activations not producing list,"Thanks for uploading this to github! Great for learning more about attention models. When I run attention_dense.py, however, I get this error (after the model finishing training):

> ---------------------------------------------------------------------------
> IndexError Traceback (most recent call last)
> <ipython-input-9-5fdd7f84f2d8> in <module>()
> 37 # Attention vector corresponds to the second matrix.
> 38 # The first one is the Inputs output.
> ---> 39 attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()
> 40 print('attention =', attention_vector)
> 41 
> 
> IndexError: list index out of range

Any idea why the get_activations function isn't working properly?",Other,Other,
19157,Update sbt to 1.3.9,"Updates [org.scala-sbt:sbt](https://github.com/sbt/sbt) from 1.3.8 to 1.3.9.


I'll automatically update this PR to resolve conflicts as long as you don't change it yourself.

If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below.

Have a fantastic day writing Scala!

<details>
<summary>Ignore future updates</summary>

Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:
```
updates.ignore = [ { groupId = ""org.scala-sbt"", artifactId = ""sbt"" } ]
```
</details>

labels: library-update, semver-patch",Other,Other,
17734,Optimization: Add option for how to sample hyper parameters,"Currently, all optimizers in SharpLearning.Optimization use random uniform sampling for sampling hyper parameters from the provided min/max boundaries. This is not always optimal, for instance when dealing with a hyper parameters like learning rate that can span a large range of values, like 0.0001 to 1.0. Using random uniform sampling in this case might result in only sampling values in a small part of the range. In this case it would be much better to sample at uniform in the log space. Hence, it should be possible to select which space to sample from for each hyper parameter when setting og an optimizer. 

This should include at least random uniform form:
 - Linear (current method)
 - Logarithmic
 - Exponential

At the same time, setup of the hyper-parameter ranges could be changed from setting op an array of arrays to using a type to guide the user better:

**Current method**
```csharp
var parameters = new double[][]
{
 new double[] { 80, 300 }, // iterations (min: 80, max: 300)
 new double[] { 0.02, 0.2 }, // learning rate (min: 0.02, max: 0.2)
};
 ```

**Proposed method**
```csharp
var parameters = new OptimizerParameter[]
{
 new OptimizerParameter(min: 80, max: 300, SamplingMethod.Linear), // iterations (min: 80, max: 300)
 new OptimizerParameter(min: 0.02, max: 0.2, SamplingMethod.Logarithmic),, // learning rate (min: 0.02, max: 0.2)
};
 ```",Other,Other,
30666,Getting the entire set of labels (along with the sub-labels) used in the model,"Dear Mr. @inukshuk 

I am analyzing Anystyle parser and I would like to know the labels used in the model.
With Anystyle.parser.model.labels the output is:
`[""accessed"", ""author"", ""authority"", ""booktitle"", ""date"", ""director"", ""doi"", ""edition"", ""editor"", ""event"", ""genre"", ""isbn"", ""journal"", ""location"", ""medium"", ""note"", ""pages"", ""producer"", ""publisher"", ""section"", ""source"", ""title"", ""translator"", ""unknown"", ""url"", ""volume""]`

However, when I set the format = :citeproc, there are more labels than the ones specified above.
`irb(main):033:0> Myprs.parse(""Lombard, M.-A. 1989. âTchador Ã lâÃ©cole: deux mois dâun dÃ©bat qui divise la France.â Le Figaro, 27 November, p. 10."", format = :citeproc) => [{""author""=>[{""family""=>""Lombard"", ""given""=>""M.-A.""}], ""title""=>""Tchador Ã lâÃ©cole: deux mois dâun dÃ©bat qui divise la France."", ""container-title""=>""Le Figaro"", ""volume""=>""27"", ""page""=>""10"", ""language""=>"""", ""id""=>""lombard1989a"", ""issued""=>{""date-parts""=>[[1989, 11]]}, ""type""=>""article-journal""}]`
Is there a way that I can view/extract all the labels (along with its sub-labels) used in the model?

Your help will aid me in my school project.",Other,Other,
42458,Bump tensorflow-gpu from 1.4.0 to 2.5.2,"Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.4.0 to 2.5.2.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/releases"">tensorflow-gpu's releases</a>.</em></p>
<blockquote>
<h2>TensorFlow 2.5.2</h2>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory during shape inference in <code>Cudnn*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by <code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to <code>nullptr</code> in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code> ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">CVE-2021-41207</a>)</li>
<li>Fixes FPE issues in convolutions with zero size filters (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
<li>Fixes a segfault produced while copying constant resource tensor (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41204"">CVE-2021-41204</a>)</li>
<li>Fixes a vulnerability caused by unitialized access in <code>EinsumHelper::ParseEquation</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41201"">CVE-2021-41201</a>)</li>
<li>Fixes several vulnerabilities and segfaults caused by missing validation during checkpoint loading (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41203"">CVE-2021-41203</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.range</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41202"">CVE-2021-41202</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.image.resize</code> when size is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41199"">CVE-2021-41199</a>)</li>
<li>Fixes an overflow producing a crash in <code>tf.tile</code> when tiling tensor is large (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41198"">CVE-2021-41198</a>)</li>
<li>Fixes a vulnerability produced due to incomplete validation in <code>tf.summary.create_file_writer</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41200"">CVE-2021-41200</a>)</li>
<li>Fixes multiple crashes due to overflow and <code>CHECK</code>-fail in ops with large tensor shapes (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197"">CVE-2021-41197</a>)</li>
<li>Fixes a crash in <code>max_pool3d</code> when size argument is 0 or negative (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41196"">CVE-2021-41196</a>)</li>
<li>Fixes a crash in <code>tf.math.segment_*</code> operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41195"">CVE-2021-41195</a>)</li>
<li>Updates <code>curl</code> to <code>7.78.0</code> to handle <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22922"">CVE-2021-22922</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22923"">CVE-2021-22923</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22924"">CVE-2021-22924</a>, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22925"">CVE-2021-22925</a>, and <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22926"">CVE-2021-22926</a>.</li>
</ul>
<h2>TensorFlow 2.5.1</h2>
<h1>Release 2.5.1</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635"">CVE-2021-37635</a>)</li>
<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636"">CVE-2021-37636</a>)</li>
<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637"">CVE-2021-37637</a>)</li>
<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638"">CVE-2021-37638</a>)</li>
<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639"">CVE-2021-37639</a>)</li>
<li>Fixes an integer division by 0 in sparse reshaping (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640"">CVE-2021-37640</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md"">tensorflow-gpu's changelog</a>.</em></p>
<blockquote>
<h1>Release 2.5.2</h1>
<p>This release introduces several vulnerability fixes:</p>
<ul>
<li>Fixes a code injection issue in <code>saved_model_cli</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228"">CVE-2021-41228</a>)</li>
<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225"">CVE-2021-41225</a>)</li>
<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223"">CVE-2021-41223</a>)</li>
<li>Fixes an arbitrary memory read in <code>ImmutableConst</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227"">CVE-2021-41227</a>)</li>
<li>Fixes a heap OOB in <code>SparseBinCount</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226"">CVE-2021-41226</a>)</li>
<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224"">CVE-2021-41224</a>)</li>
<li>Fixes a segfault due to negative splits in <code>SplitV</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222"">CVE-2021-41222</a>)</li>
<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory
during shape inference in <code>Cudnn*</code> ops
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221"">CVE-2021-41221</a>)</li>
<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by
<code>Enter</code> op (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217"">CVE-2021-41217</a>)</li>
<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218"">CVE-2021-41218</a>)</li>
<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix
multiplication (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219"">CVE-2021-41219</a>)</li>
<li>Fixes a heap buffer overflow in <code>Transpose</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216"">CVE-2021-41216</a>)</li>
<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213"">CVE-2021-41213</a>)</li>
<li>Fixes a null pointer exception in <code>DeserializeSparse</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215"">CVE-2021-41215</a>)</li>
<li>Fixes an undefined behavior arising from reference binding to
<code>nullptr</code> in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214"">CVE-2021-41214</a>)</li>
<li>Fixes a heap OOB read in <code>tf.ragged.cross</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212"">CVE-2021-41212</a>)</li>
<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code>
ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205"">CVE-2021-41205</a>)</li>
<li>Fixes an FPE in <code>ParallelConcat</code> ([CVE-2021-41207]
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207"">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207</a>))</li>
<li>Fixes FPE issues in convolutions with zero size filters
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209"">CVE-2021-41209</a>)</li>
<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code>
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210"">CVE-2021-41210</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code
(<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208"">CVE-2021-41208</a>)</li>
<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple
TF ops (<a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206"">CVE-2021-41206</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/957590ea15cc03ee2e00fc61934647d54836676f""><code>957590e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52873"">#52873</a> from tensorflow-jenkins/relnotes-2.5.2-20787</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2e1d16d7aac34983e4ff0d55f434e4d07fea7bce""><code>2e1d16d</code></a> Update RELEASE.md</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2fa6dd95659985a9ee9429d146c29c27f12e342c""><code>2fa6dd9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52877"">#52877</a> from tensorflow-jenkins/version-numbers-2.5.2-192</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/480748994b151d28818cdfc659f4332bce8a97b2""><code>4807489</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52881"">#52881</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/d398bdfd5d2190a3274141416c54f3e7207e96f3""><code>d398bdf</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/857ad5ef1eb23bbeb378b1f3c3cbe6f38286c2d0""><code>857ad5e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52878"">#52878</a> from tensorflow/fix-build-1-on-r2.5</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/6c2a215be0afd10008046bd072daaf81228a19a1""><code>6c2a215</code></a> Disable failing test</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/f5c57d495753bbc166abc928430ea808aa8aa6b3""><code>f5c57d4</code></a> Update version numbers to 2.5.2</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/e51f9495418eb373074a652b5bf4bba1c41aa132""><code>e51f949</code></a> Insert release notes place-fill</li>
<li><a href=""https://github.com/tensorflow/tensorflow/commit/2620d2cd5c4e03df6d02ae18fda2a9fdc2466738""><code>2620d2c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52863"">#52863</a> from tensorflow/fix-build-3-on-r2.5</li>
<li>Additional commits viewable in <a href=""https://github.com/tensorflow/tensorflow/compare/v1.4.0...v2.5.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.4.0&new-version=2.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/manideep2510/eye-in-the-sky/network/alerts).

</details>",Other,Other,